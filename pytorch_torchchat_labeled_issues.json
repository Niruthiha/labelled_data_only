{
  "repository": "pytorch/torchchat",
  "repository_info": {
    "repo": "pytorch/torchchat",
    "stars": 3594,
    "language": "Python",
    "description": "Run PyTorch LLMs locally on servers, desktop and mobile",
    "url": "https://github.com/pytorch/torchchat",
    "topics": [
      "llm",
      "local",
      "pytorch"
    ],
    "created_at": "2024-03-22T18:15:54Z",
    "updated_at": "2025-06-21T13:22:35Z",
    "search_query": "local llm language:python stars:>2",
    "total_issues_estimate": 50,
    "labeled_issues_estimate": 46,
    "labeling_rate": 92.9,
    "sample_labeled": 13,
    "sample_total": 14,
    "has_issues": true,
    "repo_id": 776122617,
    "default_branch": "main",
    "size": 9426
  },
  "extraction_date": "2025-06-22T00:43:45.773181",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 140,
  "issues": [
    {
      "issue_number": 1273,
      "title": "GeneratorArgs.is_torchtune_model is a misnomer",
      "body": "### 🚀 The feature, motivation and pitch\n\n`is_torchtune_model` is a misnomer and can result in buggy code. It gates logic for models that have [`tune` suffix](https://github.com/pytorch/torchchat/blob/d0993b3508f802e81a6917b8959907a9abff827a/torchchat/generate.py#L143), but not all torchtune models end with this suffix. For example Flamingo (Llama3.2 11B) is also a torchtune model\r\n\r\nThis results in code like this: \r\nhttps://github.com/pytorch/torchchat/blob/d0993b3508f802e81a6917b8959907a9abff827a/torchchat/generate.py#L611-L614\r\n\r\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\nWhile the logic around torchtune models itself needs some refactor, the short term solution is some combination of:\r\n* Rename the field to be more accurate\r\n* Refactor the logic such that it does account for torchtune models that do not end in `tune`",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-10-05T03:26:16Z",
      "updated_at": "2025-05-16T17:49:39Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "actionable",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1273/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1273",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1273",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:04.120181",
      "comments": [
        {
          "author": "krammnic",
          "body": "@Jack-Khuu Hey! If it is still relevant I can take it. (We want to have better integration with torchtune)",
          "created_at": "2025-05-16T17:49:38Z"
        }
      ]
    },
    {
      "issue_number": 1536,
      "title": "Improve Tokenizer New Type Onboarding",
      "body": "### 🚀 The feature, motivation and pitch\n---\nAs a sequel to https://github.com/pytorch/torchchat/issues/1518 where we added an enum for tokenizer types to simplify `TokenizerArgs __post_init__`, we need to further improve it to simplify new tokenizer type onboarding:\n\n### Tasks\n---\n- Move TokenizerType to a centralized place\n  - We now have two of them: https://github.com/pytorch/torchchat/blob/0299a37a342348803763e37e9f4823c5bcb12c92/dist_run.py#L67-L69 https://github.com/pytorch/torchchat/blob/0299a37a342348803763e37e9f4823c5bcb12c92/torchchat/cli/builder.py#L241-L245\n- Check all getters of tokenizer types\n  - It may be able to be simplified as inline https://github.com/pytorch/torchchat/blob/0299a37a342348803763e37e9f4823c5bcb12c92/torchchat/generate.py#L368\n- Add documentation for future tokenizer onboard.\n  - We may need to point people to update the model validation logic: https://github.com/pytorch/torchchat/blob/0299a37a342348803763e37e9f4823c5bcb12c92/torchchat/cli/builder.py#L290-L322\n---\nTo test, run a model with each tokenizer type:\n- python torchchat.py generate llama2\n- python torchchat.py generate llama3\n- python torchchat.py generate granite-code\n\ncc @Jack-Khuu @byjlw ",
      "state": "open",
      "author": "zhenyan-zhang-meta",
      "author_type": "User",
      "created_at": "2025-04-28T18:31:33Z",
      "updated_at": "2025-05-13T17:54:18Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "actionable",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1536/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "srikary12"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1536",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1536",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:04.367268",
      "comments": [
        {
          "author": "srikary12",
          "body": "Would like to take this up.",
          "created_at": "2025-05-05T02:46:48Z"
        },
        {
          "author": "zhenyan-zhang-meta",
          "body": "@srikary12 Nice, thanks for taking this up. I've just assigned you to this issue. Let us know when there's any PR to review, and chat in [#torchchat-contributors](https://discord.com/channels/1334270993966825602/1350125825206255657) if there's any questions.",
          "created_at": "2025-05-05T17:26:26Z"
        },
        {
          "author": "srikary12",
          "body": "@zhenyan-zhang-meta I've made changes. Documentation changes are pending. If the PR looks looks okay, I'll add documentation changes.",
          "created_at": "2025-05-11T07:40:38Z"
        }
      ]
    },
    {
      "issue_number": 1281,
      "title": "Llama 3.2 11B Currently Only Supports Single Image",
      "body": "### 🐛 Describe the bug\r\n\r\nCurrently, **Llama 3.2 11B** only supports a single optional image prompt in torchchat. The base torchtune model backing Llama3.2 11B should* be capable of supporting multiturn with:\r\n* Multiple Simultaneous Images\r\n* Replacing the previous image\r\n\r\nThis Issue acts as a tracker for the development of these 2 extensions to Llama 3.2 11B functionality\r\n\r\nE.g. Via OpenAI API/Browser you can currently provide text prompts similar to LLama3.1 8B, but you are unable to replace the image once one is provided\r\n\r\n*Should being the operative word as it may require additional changes to the torchtune repo\r\n\r\n### Versions\r\n\r\nNA",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-10-08T03:37:55Z",
      "updated_at": "2025-05-11T08:01:55Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Known Gaps",
        "Llama 3.2- Multimodal",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1281/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1281",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1281",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:04.565096",
      "comments": [
        {
          "author": "srikary12",
          "body": "Would like to work on this. Would love some guidance on the approach.",
          "created_at": "2025-05-11T08:01:53Z"
        }
      ]
    },
    {
      "issue_number": 1519,
      "title": "torchtune as an optional dependency: Lazy Import",
      "body": "### 🚀 The feature, motivation and pitch\n\nWe would like to make `torchtune` an optional dependency. The first step towards that is to avoid importing `torchtune` unless it is actively used. \n\nTo make this migration easier, let's move the top level imports into the functions/classes that require them. \n\n> We explicitly acknowledge that this initial step isn't a best practice, but will make the following work simpler.\n\nHere's an example where we delay imports here: https://github.com/pytorch/torchchat/blob/1384f7d3d7af0847d8364fe7b300a8b49f2213c2/torchchat/usages/eval.py#L216-L225\n\n**Task:** Update all imports of `torchtune` in the repo, such that imports are only done when necessary\n- Suggestion: Feel free to send out PR's that only update a subset of all use cases\n\nTo test your changes, run: \n- With torchtune installed: https://github.com/pytorch/torchchat/blob/main/docs/multimodal.md#generation\n- With torchtune uninstalled: `python torchchat.py generate llama3.2-1B`\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-03-31T20:22:46Z",
      "updated_at": "2025-05-05T16:49:37Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "actionable",
        "triaged",
        "torchtune"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1519/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krammnic"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1519",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1519",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:04.763551",
      "comments": [
        {
          "author": "krammnic",
          "body": "Hey, I'm from torchtune. Would you mind me to take this issue?",
          "created_at": "2025-05-03T08:57:59Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Totally, give it a shot",
          "created_at": "2025-05-05T16:49:32Z"
        }
      ]
    },
    {
      "issue_number": 1518,
      "title": "Simplify TokenizerArgs __post_init__: Unnecessarily verbose",
      "body": "### 🚀 The feature, motivation and pitch\n\n`TokenizerArgs.__post_init__` has grown quite verbose/redundant and could use a bit of simplification\n\nhttps://github.com/pytorch/torchchat/blob/1384f7d3d7af0847d8364fe7b300a8b49f2213c2/torchchat/cli/builder.py#L244-L289\n\nTask: Simplify the logic in __post_init__ to reduce redundancy\n---\nTo test, run a model with each tokenizer type:\n- python torchchat.py generate llama2\n- python torchchat.py generate llama3\n- python torchchat.py generate granite-code\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-03-31T19:50:34Z",
      "updated_at": "2025-04-28T18:33:59Z",
      "closed_at": "2025-04-28T18:33:59Z",
      "labels": [
        "good first issue",
        "actionable",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1518/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "zhenyan-zhang-meta"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1518",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1518",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:05.001924",
      "comments": [
        {
          "author": "mu-lippy",
          "body": "I'd like to work on this, please.",
          "created_at": "2025-03-31T19:58:09Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Give it a shot 😃 \n\nAlso if you haven't already, definitely join the discord https://discord.com/invite/hm2Keduk3v",
          "created_at": "2025-03-31T20:58:52Z"
        },
        {
          "author": "mu-lippy",
          "body": "I'm sorry if this question is silly, but do I need to write tests after I make changes? Thanks!",
          "created_at": "2025-04-01T15:18:29Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Usually yes, but in this case the integration tests should catch it so you should be good without (bad tokenizer casting fails fast)",
          "created_at": "2025-04-01T17:59:05Z"
        },
        {
          "author": "mu-lippy",
          "body": "I think dealing with the different parameter naming conventions for each tokenizer makes this hard to deal with. When I try to make it more concise, it looks a lot uglier and harder to read. I may be wrong, but it might be better to leave this method as is?\nIf not, I'd love to see a more experienced",
          "created_at": "2025-04-02T13:23:54Z"
        }
      ]
    },
    {
      "issue_number": 1520,
      "title": "Enable torchao.experimental EmbeddingQuantization",
      "body": "### 🚀 The feature, motivation and pitch\n\nQuantization is a technique used to reduce the speed, size, or memory requirements of a model and [torchao](https://github.com/pytorch/ao) is PyTorch's native quantization library for inference and training\n\nThere are new experimental quantizations in torchao that we would like to enable in torchchat. Specifically this task is for enabling [EmbeddingQuantizer](https://github.com/pytorch/ao/blob/42e1345f0bc451383bcd27e39e93d4ae673eabe0/torchao/experimental/quant_api.py#L585) and [SharedEmbeddingQuantizer](https://github.com/pytorch/ao/blob/42e1345f0bc451383bcd27e39e93d4ae673eabe0/torchao/experimental/quant_api.py#L882).\n\n**Entrypoint**: https://github.com/pytorch/torchchat/blob/1384f7d3d7af0847d8364fe7b300a8b49f2213c2/torchchat/utils/quantize.py#L101\n\n**Task**: Using ExecuTorch as a reference (https://github.com/pytorch/executorch/pull/9548) add support for EmbeddingQuantizer and SharedEmbeddingQuantizer.\n\ncc: @metascroy, @manuelcandales \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-03-31T20:48:15Z",
      "updated_at": "2025-04-15T17:03:14Z",
      "closed_at": null,
      "labels": [
        "Quantization",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1520/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dillondesilva"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1520",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1520",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:05.204331",
      "comments": [
        {
          "author": "dillondesilva",
          "body": "@Jack-Khuu I'd love to have a crack at this if possible! Would you mind assigning it to me?",
          "created_at": "2025-04-01T09:52:27Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Totally, give it a shot",
          "created_at": "2025-04-01T17:00:33Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Hi @dillondesilva, how's the task going? Any questions?",
          "created_at": "2025-04-07T21:06:51Z"
        },
        {
          "author": "dillondesilva",
          "body": "Hey @Jack-Khuu - I've just been busy with mid-semester exams in the past week. Should have time to start sometime this week and will send questions soon 👍 Thanks for checking in!",
          "created_at": "2025-04-08T01:52:08Z"
        },
        {
          "author": "dillondesilva",
          "body": "@Jack-Khuu Good news! Here's the PR -> https://github.com/pytorch/torchchat/pull/1525\n\nI don't know if I've oversimplified it so feel free to correct me if I'm wrong but to enable the above experimental quantizers, I'm assuming all that was needed was:\n\n1. A mapping in `quantizer_class_dict` to enab",
          "created_at": "2025-04-13T11:40:47Z"
        }
      ]
    },
    {
      "issue_number": 1380,
      "title": "What is the future plan of model expansion?",
      "body": "### 🚀 The feature, motivation and pitch\n\nI see current torchchat only support a few kinds of model, like llama based(liked) architecture, or pre-defined Transformer architecture models. Is there any plan to support other kinds of model architecture in the future? which kinds of model you're considering to add? If there is a new model whose architecture is not in the supporting list, is there a way to run it?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "jenniew",
      "author_type": "User",
      "created_at": "2024-11-15T23:33:01Z",
      "updated_at": "2025-03-31T20:39:15Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Question",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1380/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1380",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1380",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:05.458734",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "My *personal* take on how to tc might support a broader set of models:\r\n\r\nBecause the model description is part of the torchchat tree, there's a natural limit to the types of models that can be supported to those that can fit the general infra that torchchat supports.  \r\n\r\nOf course, the model.py co",
          "created_at": "2024-11-18T06:19:31Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Great Question @jenniew. \r\n\r\nLike you mentioned, model support is currently biased towards Llama/Transformer architectures, but we intend for the inference pipeline to be built model agnostic. The upcoming models are Llava and Granite Code Models (though both are Transformer based), with Mamba's (SS",
          "created_at": "2024-11-18T21:29:02Z"
        },
        {
          "author": "byjlw",
          "body": "Like @Jack-Khuu mentioned. We need to make some architecture changes and create a model adding flow so it's easy for anyone to add models. \r\n\r\nIn the meantime, feel free to ask for a specific model.",
          "created_at": "2024-11-19T21:38:05Z"
        },
        {
          "author": "dillondesilva",
          "body": "Hey there @byjlw @Jack-Khuu - I came across this thread and found it quite interesting. Has there been any progress on ideas for architecture changes to support a flow for adding new models? I've just cloned torchchat and enjoyed playing around with it. \n\nWhen someone desires to add a new model, is ",
          "created_at": "2025-03-31T12:13:13Z"
        },
        {
          "author": "byjlw",
          "body": "@dillondesilva thanks, for 1 the term we use here is \"export\" having a better export flow for developers is a must for sure. \n@Jack-Khuu and I are discussing this very thing right now. \n\nOnce we have our rough thoughts together we'll discuss and collaborate people in the Edge Discord Channel\nhttps:/",
          "created_at": "2025-03-31T19:56:08Z"
        }
      ]
    },
    {
      "issue_number": 1504,
      "title": "Update CI Jobs in anticipation for Cuda 12.4 deprecation",
      "body": "### 🚀 The feature, motivation and pitch\n\nPT is deprecating CUDA 12.4 nightly builds: https://github.com/pytorch/test-infra/pull/6333\n\nThis repo will need to update CI and testing in response to this\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-02-25T20:50:42Z",
      "updated_at": "2025-03-27T23:44:17Z",
      "closed_at": "2025-03-27T23:44:16Z",
      "labels": [
        "Cuda",
        "CI Infra",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1504/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1504",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1504",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:05.666357",
      "comments": [
        {
          "author": "HonestDeng",
          "body": "Hi. I want to solve this issue and to be a long-time contributor. Is there any procedure I need go through? Or is there any help? Thanks",
          "created_at": "2025-03-23T13:05:07Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Turns out we don't need to bump the cuda configs in the ci per https://github.com/pytorch/test-infra/issues/6465",
          "created_at": "2025-03-27T23:44:16Z"
        }
      ]
    },
    {
      "issue_number": 1179,
      "title": "convert_hf_checkpoint only relies on model_name to resolve TransformerArgs",
      "body": "### 🐛 Describe the bug\n\n[`convert_hf_checkpoint`](https://github.com/pytorch/torchchat/blob/main/torchchat/cli/convert_hf_checkpoint.py#L37) transforms a HF checkpoint into a torchchat format. \r\n\r\nAs part of this process, `ModelArgs` is created for the newly downloaded model. Currently it constructs ModelArgs based on model_name instead of checking  `transformer_params_key` first:\r\n* `config_args = ModelArgs.from_name(model_name).transformer_args['text']`\r\n\r\nWhile this is correct most of the time, [model_configs](https://github.com/pytorch/torchchat/blob/main/torchchat/model_config/models.json) defines a `transformer_params_key` to allow specifying an alternative model_params.\r\n\r\n**Task**: Update [`convert_hf_checkpoint`](https://github.com/pytorch/torchchat/blob/main/torchchat/cli/convert_hf_checkpoint.py#L37) to check for a `transformer_params_key` defined model_params before searching with model_name\r\n* i.e. Attempt to construct ModelArgs with [from_table](https://github.com/pytorch/torchchat/blob/b037b71205145a46e3f2db02a391f6372dfb6c91/torchchat/model.py#L357) before `from_name`\r\n\n\n### Versions\n\nN/a",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-09-23T15:19:26Z",
      "updated_at": "2025-03-27T23:39:04Z",
      "closed_at": null,
      "labels": [
        "actionable",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1179/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1179",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1179",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:05.899560",
      "comments": [
        {
          "author": "sanafatima612",
          "body": "hi i want to work on it",
          "created_at": "2024-10-29T21:26:14Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "We welcome the help!! Please let me know if you require more details or assistance",
          "created_at": "2024-10-30T21:27:35Z"
        }
      ]
    },
    {
      "issue_number": 1334,
      "title": "Multimodal Eval Enablement (Looking for Developer to Implement Design)",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\n***Please note that since the actual implementation is going to be simple, and the design has already been reviewed, the purpose of this GitHub Issue is to look for a developer to implement this feature ASAP.***\r\n\r\nLLM eval stands for the process of assessing the perplexity, performance and capabilities of LLMs, usually by having the model complete one or a series of tasks and assigning them scores. Torchchat is already using EleutherAI’s [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) to do eval on text LLM ([code pointer](https://github.com/pytorch/torchchat/blob/11dcbebe6bd2ee933f7302b4e14baa23761abc0c/torchchat/usages/eval.py#L198)). Recently, torchtune has worked with EleutherAI to enable eval on text-image models in the harness, and has integrated this feature into torchtune ([code pointer](https://github.com/pytorch/torchtune/blob/d0c6460b51fc18245b3da0220568e10b3de06b63/recipes/eleuther_eval.py#L40)). Torchchat wants to just copy that solution from torchtune for text-image models.\r\n\r\nWithout the ability to do eval on multimodal LLMs, the enablement of multimodal LLMs on torchchat is incomplete. It’s critical to understand how well torchchat performs with image inputs. \r\n\r\n### Additional context\r\n\r\n## Assumptions\r\n\r\n\r\n\r\n* The eval for text LLMs is already enabled on torchchat. Code pointer to the [core eval function](https://github.com/pytorch/torchchat/blob/11dcbebe6bd2ee933f7302b4e14baa23761abc0c/torchchat/usages/eval.py#L172) and the [main function](https://github.com/pytorch/torchchat/blob/11dcbebe6bd2ee933f7302b4e14baa23761abc0c/torchchat/usages/eval.py#L226).\r\n* The Llama 3.2-11b multimodal model has been onboarded to torchchat, and in the future there will be more multimodal LLMs on torchchat. \r\n* EleutherAI’s [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) has enabled eval on llama3.2-11b, thus we don’t need to make code changes in EleutherAI repo.\r\n\r\n\r\n## The Main Goal\r\nA torchchat user can run eval on the llama 3.2-11b model (which image-text-in, text-out). Note that we don’t need to worry about the internals of how the eval happens because we will only be calling the EleutherAI’s eval libraries and report the metrics it returns. \r\n\r\nThe user interface will be a commandline `python torchchat.py eval <model-name>` with additional arguments specifying detailed requirements for the eval tasks.\r\n\r\nThe result will be printed out on the terminal which include the following metrics:\r\n * Tasks that have been run \r\n * The score to each task \r\n * The time it took to run each task\r\n\r\n\r\n### RFC (Optional)\r\n\r\n# Design\r\n\r\n\r\n## Overview\r\n\r\nIn this design, the multimodal eval in torchchat will borrow from the implementation of multimodal eval in torchtune which utilizes EleutherAI’s [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness). The reason we can do this is that torchchat uses the same Llama 3.2-11b model definition as torchtune. \r\n\r\n## Details\r\n\r\n\r\n### The Core Eval Implementation\r\n\r\n\r\n#### [Preferred] Approach A: import the implementation of `HFMultimodalLM` from torchtune directly \r\nThe easiest implementation is to import the implementation of <code>HFMultimodalLM </code>directly from torchtune, then call <code>evaluate()</code> with this wrapper class passed in. </em>\r\n\r\nHere’s torchtune’s implementation of `HFMultimodalLM`: [code pointer](https://github.com/pytorch/torchtune/blob/ced1a840300b1ab550dac4fc2054b187f5b45c8c/recipes/eleuther_eval.py#L68).\r\n\r\n*Pseudocode:*\r\n```\r\n# In eval.py\r\nfrom torchtune.recipes.eleuther_eval import _VLMEvalWrapper\r\n\r\nif model is text-based:\r\n   do the existing text-based model eval\r\nelif model is text-image-based:\r\n   eval_results = evaluate(_VLMEvalWrapper(...))\r\n```\r\n\r\nThe pros and cons of this solution is discussed in the following “Alternatives Discussion” section. This solution should be the one to start with given how quick it can enable multimodal eval on torchchat. If for some unforeseen reason that it doesn’t work, then take the following approach that requires more work.\r\n\r\n\r\n#### Approach B: copy the implementation of `HFMultimodalLM` from torchtune\r\n\r\n\r\n\r\n1. Creating a wrapper class that overrides class <code>[HFMultimodalLM](https://github.com/EleutherAI/lm-evaluation-harness/blob/0845b588303f1f59af98dd1c5bdbd78a9e75a1e2/lm_eval/models/hf_vlms.py#L30)</code>, which is an abstract Hugging Face model class for multimodal models. The implementation of this class can be copied from torchtune, [code pointer](https://github.com/pytorch/torchtune/blob/ced1a840300b1ab550dac4fc2054b187f5b45c8c/recipes/eleuther_eval.py#L68).\r\n2. Then call <code>evaluate()</code> with this wrapper class passed in. \r\n\r\n*Pseudocode:*\r\n```\r\n# In eval.py\r\nfrom lm_eval.models.hf_vlms import HFMultimodalLM\r\nfrom lm_eval.evaluator import evaluate\r\n\r\nclass VLMEvalWrapper(HFMultimodalLM):\r\n   ...# implementation\r\n\r\nif model is text-based:\r\n   do the existing text-based model eval\r\nelif model is text-image-based:\r\n   eval_results = evaluate(VLMEvalWrapper(...))\r\n```\r\n\r\n### The Commandline Arguments\r\n\r\nUser command should be `python torchchat.py eval llama3.2-11b` + some optional arguments.\r\n\r\nIn terms of implementation, reuse the same cli entry point as the text eval: [torchchat.py](https://github.com/pytorch/torchchat/blob/77774d2345dee150d7bdc2dbd22529cbde388ed7/torchchat.py#L89), [eval.py](https://github.com/pytorch/torchchat/blob/77774d2345dee150d7bdc2dbd22529cbde388ed7/torchchat/usages/eval.py#L226). Then in [def eval()](https://github.com/pytorch/torchchat/blob/77774d2345dee150d7bdc2dbd22529cbde388ed7/torchchat/usages/eval.py#L172), have an if-else to decide which eval wrapper (`GPTFastEvalWrapper` or the new `VLMEvalWrapper`) to use based on model type. \r\n\r\n\r\n## Alternatives Discussion\r\n\r\nDiscuss the pros and cons of importing torchtune’s implementation directly\r\n\r\nPro: \r\n\r\n\r\n\r\n1. Easy to implement because it’s just an import\r\n2. Consistency between torchchat and torchtune\r\n3. Easy maintenance for us\r\n4. Torchtune has a better relationship with EleutherAI \r\n\r\nCons:\r\n\r\n\r\n\r\n1. Hard to customize the implementation for torchchat’s needs\r\n2. For some models, we use model definitions that are different from torchtune’s\r\n3. We rely on the compatibility on their side\r\n4. We have more dependency on torchtune\r\n\r\n\r\n## Testing & Tooling Plan\r\n\r\nRun command `python torchchat.py eval llama3.2-11b` with different parameter combinations. \r\n\r\nThe expected output is the tasks that have been run, their scores and the time it took to run each task. \r\n\r\n",
      "state": "closed",
      "author": "Olivia-liu",
      "author_type": "User",
      "created_at": "2024-10-29T01:01:50Z",
      "updated_at": "2025-03-25T06:24:18Z",
      "closed_at": "2025-03-25T06:24:18Z",
      "labels": [
        "enhancement",
        "good first issue",
        "actionable",
        "Llama 3.2- Multimodal",
        "triaged"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 26,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1334/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "anirudhs001"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1334",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1334",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:06.181817",
      "comments": []
    },
    {
      "issue_number": 1497,
      "title": "Llama3.2 vision model AOTI integration",
      "body": "### 🚀 The feature, motivation and pitch\n\nNeed work to enable:\n\n```\npython3 torchchat.py export llama3.2-11B --output-aoti-package-path exportedModels/llama3_2_artifacts.pt2\n```\n\nFeatures to be added:\n\n* Run AOTI export on preprocess, vision encoder and text decoder separately.\n  * This requires dependency on torchtune's exportable modules and perform module transformation needed.\n* Package them into a single pt2 file.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "larryliu0820",
      "author_type": "User",
      "created_at": "2025-02-21T21:13:21Z",
      "updated_at": "2025-03-23T13:09:09Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Compile / AOTI",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1497/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1497",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1497",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:06.181838",
      "comments": [
        {
          "author": "HonestDeng",
          "body": "I want to be a long-time contributor. This issue looks very interesting and challenging for me. Can I get any help and solve this issue?",
          "created_at": "2025-03-23T13:09:08Z"
        }
      ]
    },
    {
      "issue_number": 1484,
      "title": "Move tokenizer information into pte to reduce ExecuTorch runner args",
      "body": "### 🚀 The feature, motivation and pitch\n\nAfter an ExecuTorch model is exported to a `pte`, tokenization information must be passed in as an arg (`-l <#>`) to the runner. This can be avoided by writing this information into the `pte` file itself since the tokenizer is known at export time (sentencepiece => 2, tiktoken =>3). Tokenization information can be stored during export as a [constant_method](https://github.com/pytorch/executorch/blob/073397357118feef0fca91326ed612ce5c60d53b/exir/program/_program.py#L1188).\n\nFor example: https://github.com/pytorch/torchchat?tab=readme-ov-file#deploy-and-run-on-android\n```\ncmake-out/et_run llama3.1.pte -z `python3 torchchat.py where llama3.1`/tokenizer.model -l 3 -i \"Once upon a time\"\n```\n---\n**Task:**\n1) Update ExecuTorch exporting to save tokenization information in the pte artifact\n2) Update the ExecuTorch runner to read the newly saved metadata\n\n\nFor a similar optimization made for aoti: https://github.com/pytorch/torchchat/pull/1159.\nSee https://github.com/pytorch/torchchat/pull/1439 for conversation/more context\n\n\n\n### Alternatives\n\nContinue to pass tokenizer arguments to the runner\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-01-30T23:05:19Z",
      "updated_at": "2025-03-14T00:07:01Z",
      "closed_at": "2025-03-14T00:07:01Z",
      "labels": [
        "enhancement",
        "good first issue",
        "actionable",
        "ExecuTorch",
        "triaged"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1484/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "silverguo"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1484",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1484",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:06.403341",
      "comments": []
    },
    {
      "issue_number": 1482,
      "title": "Update ExecuTorch install requirements to new APIs",
      "body": "### 🚀 The feature, motivation and pitch\n\nAs a follow up from https://github.com/pytorch/torchchat/pull/1475, torchchat should verify whether the ExecuTorch install instructions properly reflect their new instructions. \n\nSpecifically ET:\n* renamed the original `./install_requirements` to `./install_executorch` (installs dependencies + pip installs)\n* Introduced a new `./install_requirements` command that only installs the ET requirements \n\nSee https://github.com/pytorch/executorch/issues/7730 for the relevant stack.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-01-28T02:29:06Z",
      "updated_at": "2025-03-11T00:44:51Z",
      "closed_at": "2025-03-11T00:44:51Z",
      "labels": [
        "good first issue",
        "actionable",
        "ExecuTorch",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1482/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "silverguo"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1482",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1482",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:06.403362",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Calling out that this may be a no-op: \n\nIn that case justify and verify that the existing instructions are sufficient",
          "created_at": "2025-02-21T02:00:51Z"
        },
        {
          "author": "silverguo",
          "body": "It seems that most of works were done by https://github.com/pytorch/executorch/issues/7730.\nFor installing ET from source, torchchat is doing the correct step by calling ./install_executorch.sh after cloning ET source code.\nInstalling with `--pybind off` is currently broken and sent a PR as a fix ht",
          "created_at": "2025-03-11T00:01:12Z"
        }
      ]
    },
    {
      "issue_number": 1506,
      "title": "export mobile model with 4bit failed, but 8bit was ok.",
      "body": "### 🐛 Describe the bug\n\nI just follow the tutorial to export an mobile model:\n`python3 torchchat.py export llama3.1 --quantize torchchat/quant_config/mobile.json --output-pte-path llama3.1.pte`\n\nin the torchchat page(it's using executorch to export the mobile model, please refer to: https://github.com/pytorch/torchchat/blob/main/torchchat/export.py#L145), I changed the model's name to stories110m:\n`python3 torchchat.py export llama3.1 --quantize torchchat/quant_config/mobile.json --output-pte-path llama3.1.pte`\n```\ntorch._dynamo.exc.TorchRuntimeError: Failed running call_function quantized_decomposed.embedding_4bit.dtype(*(FakeTensor(..., size=(32000, 384), dtype=torch.uint8), FakeTensor(..., size=(32000, 24)), None, 0, 0, FakeTensor(..., size=(1, 1), dtype=torch.int64)), **{'dtype': torch.float32}):\nembedding_4bit_dtype in ExecuTorch expects weight_quant_min == -8\n```\n\nthe steps:\n```\ngit clone https://github.com/pytorch/torchchat\ncompile and install torchchat\nrun the above command.\n```\n\nPlease note: if change \"bitwidth\": 4 to \"bitwidth\": 8, the error isn't reproduced.\n{\n    \"embedding\": {\"bitwidth\": 4, \"groupsize\" : 32},\n    \"linear:a8w4dq\": {\"groupsize\" : 256}\n}\n\nthanks for your work.\n\n### Versions\n\nCollecting environment information...\nPyTorch version: 2.6.0+cpu\nIs debug build: False\nCUDA used to build PyTorch: Could not collect\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 14.0.0-1ubuntu1.1\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: N/A\nNvidia driver version: 560.94\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.7.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.7.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.7.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.7.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.7.1\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.7.1\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.7.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.7.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nVersions of relevant libraries:\n[pip3] executorch==0.6.0a0+791472d\n[pip3] numpy==2.0.0\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pytorch-triton==3.2.0+gitb2684bf3\n[pip3] torch==2.6.0+cpu\n[pip3] torchao==0.8.0+git11333ba2\n[pip3] torchaudio==2.6.0+cpu\n[pip3] torchsr==1.0.4\n[pip3] torchtune==0.6.0.dev20250131+cu124\n[pip3] torchvision==0.21.0+cpu\n[conda] Could not collect\n",
      "state": "closed",
      "author": "TheBetterSolution",
      "author_type": "User",
      "created_at": "2025-02-27T00:33:27Z",
      "updated_at": "2025-02-28T20:07:48Z",
      "closed_at": "2025-02-28T20:07:47Z",
      "labels": [
        "ExecuTorch",
        "Quantization",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1506/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1506",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1506",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:06.623618",
      "comments": [
        {
          "author": "guangy10",
          "body": "cc: @Jack-Khuu @jerryzh168 ",
          "created_at": "2025-02-27T02:13:42Z"
        },
        {
          "author": "jerryzh168",
          "body": "looks like it's valid error: https://github.com/pytorch/executorch/blob/68042847fd0eb6aac94ab2ffad8e1440fca865f4/exir/passes/_quant_patterns_and_replacements.py#L351\n\nprobably should replace https://github.com/pytorch/torchchat/blob/4251a54fd1bd28374933e1e3c2cc22d400ba153d/torchchat/utils/quantize.p",
          "created_at": "2025-02-27T02:51:53Z"
        },
        {
          "author": "TheBetterSolution",
          "body": "> looks like it's valid error: https://github.com/pytorch/executorch/blob/68042847fd0eb6aac94ab2ffad8e1440fca865f4/exir/passes/_quant_patterns_and_replacements.py#L351\n> \n> probably should replace\n> \n> [torchchat/torchchat/utils/quantize.py](https://github.com/pytorch/torchchat/blob/4251a54fd1bd2837",
          "created_at": "2025-02-27T04:08:04Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thanks for the PR!!",
          "created_at": "2025-02-28T20:07:47Z"
        }
      ]
    },
    {
      "issue_number": 1399,
      "title": "Executorch Slows Down after second or third response in Torchchat",
      "body": "### 🐛 Describe the bug\n\nAs discussed earlier in https://github.com/pytorch/executorch/issues/3674, to increase the size of max_seq_len we have to both increase it in the export scripts as well as bump up the hardcoded max_seq_len in  runner.cpp.  We're using Executorch in our Proof-of-Concept demo that we're looking to release at NeurIPS and we discovered this bug when using the LlamaRunner with Ktor.  We also notice it with torchchat, BUT since Torchchat is local, it won't just time out if Llama fails to generate in time.\r\n\r\nStep 1. Update the export.py, as done on this forked repo here: https://github.com/baseweight/torchchat/blob/hardcoded_default/torchchat/export.py#L393\r\nStep 2. Update the runner, as done on this forked repo here:\r\nhttps://github.com/baseweight/executorch/blob/baseweight_demo/examples/models/llama/runner/runner.cpp#L53\r\nStep 3. Follow the instructions to export the model and build the AAR.  I used Llama-3.2-3b-instruct, since it produces actual good demo results about Vancouver (because NeurIPS)\r\nStep 4. Copy the model onto a phone and load in torchchat.  I used a Pixel 9 running Android 15, but I also confirmed this on a OnePlus 12R\r\n\r\nStep 4.  Type a prompt (i.e. \"Tell me about Nardwuar\")\r\nStep 5.  Type a follow up prompt (i.e. \"And the Evaporators?\")\r\nStep 6.  Attempt to type another follow up prompt.\r\n\r\nIt seems that this MIGHT be the limit for actual chat on an Android phone on Executorch, since the device starts to overheat.  Maybe it's not the case and I'm just missing something?\n\n### Versions\n\nHere's the info from my Gaming PC that I'm using to build Executorch.  I have a conda environment setup for this.\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241007+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39\r\nIs CUDA available: False\r\nCUDA runtime version: 12.6.77\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Ti SUPER\r\nNvidia driver version: 555.58.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               16\r\nOn-line CPU(s) list:                  0-15\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD Ryzen 7 7800X3D 8-Core Processor\r\nCPU family:                           25\r\nModel:                                97\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   8\r\nSocket(s):                            1\r\nStepping:                             2\r\nCPU(s) scaling MHz:                   52%\r\nCPU max MHz:                          5050.0000\r\nCPU min MHz:                          545.0000\r\nBogoMIPS:                             8383.77\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\r\nVirtualization:                       AMD-V\r\nL1d cache:                            256 KiB (8 instances)\r\nL1i cache:                            256 KiB (8 instances)\r\nL2 cache:                             8 MiB (8 instances)\r\nL3 cache:                             96 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-15\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.5.0a0+72b3bb3\r\n[pip3] flake8==6.0.0\r\n[pip3] flake8-breakpoint==1.1.0\r\n[pip3] flake8-bugbear==23.6.5\r\n[pip3] flake8-comprehensions==3.12.0\r\n[pip3] flake8-plugin-utils==1.3.3\r\n[pip3] flake8-pyi==23.5.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241007+cpu\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.5.0.dev20241007+cpu\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchtune==0.4.0.dev20241010+cu121\r\n[pip3] torchvision==0.20.0.dev20241007+cpu\r\n[conda] executorch                0.5.0a0+aa67cd9          pypi_0    pypi\r\n[conda] numpy                     2.0.2                    pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241112+cpu          pypi_0    pypi\r\n[conda] torch-stoi                0.2.3                    pypi_0    pypi\r\n[conda] torchaudio                2.5.0.dev20241112+cpu          pypi_0    pypi\r\n[conda] torchgen                  0.0.1                    pypi_0    pypi\r\n[conda] torchsr                   1.0.4                    pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241112+cpu          pypi_0    pypi\r\n",
      "state": "open",
      "author": "infil00p",
      "author_type": "User",
      "created_at": "2024-12-04T09:56:02Z",
      "updated_at": "2025-02-27T19:48:10Z",
      "closed_at": null,
      "labels": [
        "bug",
        "Mobile - Android",
        "ExecuTorch",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1399/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "kirklandsign"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1399",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1399",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:06.864545",
      "comments": [
        {
          "author": "dbort",
          "body": "I transferred this over to the torchchat repo since it seems TC-related on its surface",
          "created_at": "2024-12-04T22:05:07Z"
        },
        {
          "author": "JacobSzwejbka",
          "body": "I havent looked at the ET repos runner in a while, but do our apps actually have a chat function or is it just calling generate each time and having to repopulate the cache with every new message? I remember having to fix that issue in the torchchat cli chat command. \r\n\r\nedit: \r\n\r\nlooks like jni has",
          "created_at": "2024-12-04T22:17:37Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thanks for spinning this up @infil00p. \r\nDid you get a chance to test out exporting with the ET script btw? \r\n\r\nThe export in TC is based on that of ET, so my gut says either:\r\n(a) exporting in ET is bugged (and torchchat by extension) => Fix in ET and port to TC\r\n(b) exporting in ET works, but fail",
          "created_at": "2024-12-04T22:21:53Z"
        },
        {
          "author": "JacobSzwejbka",
          "body": "Ok yeah it looks like the demo app effectively starts from scratch every chat message and treats the entire chat history as a new context from zero instead of just prefilling the new user message from a start_pos == length of chat history so far. https://github.com/baseweight/executorch/blob/basewei",
          "created_at": "2024-12-04T22:39:28Z"
        },
        {
          "author": "JacobSzwejbka",
          "body": "@ infil00p\r\n \r\n If you run the model with generate instead of chat do you still hit the same performance throttling? \r\n \r\n Is generate(4096) significantly faster then N chats summing up to 4096?",
          "created_at": "2024-12-04T22:48:49Z"
        }
      ]
    },
    {
      "issue_number": 1025,
      "title": "Dataclass Type Enforcement ",
      "body": "### 🚀 The feature, motivation and pitch\n\nSelect a method for enforcing types in Python dataclasses (i.e. pydantic)\n\n### Alternatives\n\nDon't enforce types - may cause type errors when decoding JSON requests.\n\n### Additional context\n\nOriginal discussion in comments on https://github.com/pytorch/torchchat/pull/1021\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "vmpuri",
      "author_type": "User",
      "created_at": "2024-08-12T18:32:19Z",
      "updated_at": "2025-02-21T20:32:33Z",
      "closed_at": "2025-02-21T20:32:32Z",
      "labels": [
        "enhancement",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1025/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1025",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1025",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:07.085181",
      "comments": []
    },
    {
      "issue_number": 1333,
      "title": "RFC: Code sharing for ET export, C++ runner and tokenizer, with ExecuTorch",
      "body": "### 🚀 The feature, motivation and pitch\n\nCurrently torchchat is having its own implementation for these features:\r\n* Utils to optimize, quantize and export eager model to ExecuTorch.\r\n* A LLM runner for AOTI and ExecuTorch.\r\n* Tokenizers (sentencepiece and tiktoken) used by both AOTI runner and ET runner.\r\n\r\nThe problem for this design is that it is not bringing in new features checked-in into the [export flow in ExecuTorch](https://github.com/pytorch/executorch/tree/main/extension/llm/export). What's worse is that the [demo apps hosted in torchchat](https://github.com/pytorch/torchchat/tree/main/torchchat/edge/android) is expecting a `.pte` file from the export flow in ExecuTorch instead of the one from torchchat and that will easily break if changes happen to one or the other.\r\n\r\nSimilar story happens to the C++ implementations of the tokenizers. If we look at [tokenizers in ExecuTorch](https://github.com/pytorch/executorch/tree/main/extension/llm/tokenizer) it is a lot similar to what [tokenizers in torchchat](https://github.com/pytorch/torchchat/tree/main/tokenizer) and the code should be unified to avoid duplication.\n\n### Alternatives\n\nAn alternative is do nothing. If we keep the status quo, DevX will deteriorate due to constant changes from ExecuTorch that we need to incorporate into torchchat.  \n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n## Proposal\r\n\r\nOn a high level we want to:\r\n* Reuse export flow in ExecuTorch's `extension/llm` directory.\r\n* Setup a new repo for tokenizers/samplers under `pytorch-labs`.\r\n* Let runner code depend on the new tokenizer repo.\r\n\r\n## Details\r\n![llm stack](https://github.com/user-attachments/assets/f923c5dd-6923-4e21-a9d8-e5158aba7f1f)\r\n\r\n### Export flow:\r\nCurrently torchchat uses [`export.py`](https://github.com/pytorch/torchchat/blob/main/torchchat/export.py) to export a model to ET's .pte file. \r\nProposal: fully migrate to ET’s `extension/llm`.\r\nNew dependency: ET nightly build in pip.\r\n\r\n### Runner:\r\nTorchchat C++ runner needs to work for both AOTI and ET so it’s quite complicated. \r\nProposal 1 (preferred):\r\n* Setup a separate repo for runner and tokenizer code. Both ET and torchchat depend on it.\r\n    * Add a public repo under [`pytorch-labs`](https://github.com/pytorch-labs) organization, say `pytorch-labs/tokenizers`\r\n    * Split existing run.cpp into `et_run.cpp` and `aoti_run.cpp`\r\n        * et_run.cpp depends on ExecuTorch as well as `pytorch-labs/tokenizers`,\r\n        * aoti_run.cpp only depends on `pytorch-labs/tokenizers`.\r\n    * Pros: no code duplication, clear dependencies.\r\n    * Cons: maintenance cost for a new repo.\r\n\r\nProposal 2 (short term?):\r\n* Use runner building blocks and tokenizer from ET. Refactor existing run.cpp to reuse those components. Add ET as a git submodule.\r\n    * Pros: no code duplication.\r\n    * Cons: if a user only wants to build an AOTI runner, it’s weird to pull in tokenizer code from ET.\r\n\r\n### Model definition:\r\nTorchchat depends on torchtune for model definition. All the source transformations will come from the ET `extension/llm` library. Modules that are modified to be `torch.export`able will be hosted in ET `extension/llm`, torchchat should use those as well.\r\n\r\nExample: torchtune’s `MultiHeadAttention` has an input dependent condition that needs to be rewritten into torch.cond so that it’s exportable. This lives in `extension/llm/modules` and should be used by torchchat. [Pending discussion] If torchtune is open to host these exportable modules, torchchat should depend on torchtune to get them.\r\n\r\n### Demo app:\r\nFor both Android and iOS, we want to build runner and tokenizer as libraries, package them into artifacts and distribute them to torchchat. \r\nWe are already doing this for [Android demo app](https://github.com/pytorch/torchchat/tree/main/torchchat/edge/android)\r\niOS demo app code should live in torchchat as well and both demo apps should be removed from ET in the future.\r\n\r\n",
      "state": "closed",
      "author": "larryliu0820",
      "author_type": "User",
      "created_at": "2024-10-28T21:12:35Z",
      "updated_at": "2025-02-21T02:18:04Z",
      "closed_at": "2025-02-21T02:18:04Z",
      "labels": [
        "ExecuTorch",
        "RFC",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1333/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [
        "larryliu0820"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1333",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1333",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:07.085199",
      "comments": [
        {
          "author": "shoumikhin",
          "body": "I like this direction to modularize the components and sharing them for different use-cases beyond those backed by ET.\r\n\r\nIt seems like what we could think on further is how to actually modularize it even more, ie. not just reuse the Tokenizer and Sampler, but also most of the runner logic, if possi",
          "created_at": "2024-11-14T18:31:40Z"
        },
        {
          "author": "gabe-l-hart",
          "body": "I also really like the idea of unifying these c++ layers! I've been working on extending the existing c++ tokenizer support to handle parts of the [tokenizers](https://github.com/huggingface/tokenizers) library from huggingface. I have a [draft PR](https://github.com/pytorch/torchchat/pull/1379) up ",
          "created_at": "2024-11-18T15:58:22Z"
        },
        {
          "author": "larryliu0820",
          "body": "@shoumikhin @gabe-l-hart thanks for chiming in. I spawn up https://github.com/pytorch-labs/tokenizers as our first step to enforcing code sharing on tokenizers. I'm still waiting for legal approval to open this repo up but would love to collaborate.",
          "created_at": "2024-11-26T21:34:21Z"
        },
        {
          "author": "metascroy",
          "body": "In the short term I think this is a good direction, but long-term I do not like how ExecuTorch does not just \"work\" well with an LLM exported from torch.export.\r\n\r\nExecuTorch having its own export_llama_lib script and special source transformations is not a good user experience in my opinion.  My on",
          "created_at": "2024-11-26T21:59:13Z"
        },
        {
          "author": "larryliu0820",
          "body": "> In the short term I think this is a good direction, but long-term I do not like how ExecuTorch does not just \"work\" well with an LLM exported from torch.export.\r\n> \r\n> ExecuTorch having its own export_llama_lib script and special source transformations is not a good user experience in my opinion. ",
          "created_at": "2024-11-26T22:13:30Z"
        }
      ]
    },
    {
      "issue_number": 1430,
      "title": "[KNOWN BUG] Broken Support for TextOnly Models from torchtune ",
      "body": "### 🐛 Describe the bug\n\nOriginally added in https://github.com/pytorch/torchchat/pull/1123, leveraging torchtune model definitions is something that torchchat is gradually moving towards (in contrast to locally hosting model definitions), but has been lost through pin bump/inactivity. \r\n\r\nFor example, commands like the following `python3 torchchat.py generate llama3.1-tune --prompt \"write me a story about a boy and his bear\"` shuold load the model definition using torchtune then pass back to torchchat for inference, but currently errors out on construction due to outdated function signatures. \r\n\r\n---\r\n**Task**: Re-enabling the ability to perform inference with: `python3 torchchat.py generate llama3.1-tune --prompt \"write me a story about a boy and his bear\"` \r\n\r\nI imagine the process being iterative via a combination of tracing signature changes in torchchat and torchtune\r\n\r\n---\r\nA good gauge of this being fixed is that changes like: https://github.com/pytorch/torchchat/commit/69da96c72d41e7ce51aac7ffde9e47fc845cdc58, should be sufficient to support a new torchtune model in torchchat.\n\n### Versions\n\nCurrent Main Hash: https://github.com/pytorch/torchchat/commit/90749d280bbc116fcc121a1eda1b60f1dba5b675",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-12-19T16:41:44Z",
      "updated_at": "2025-02-21T02:04:00Z",
      "closed_at": null,
      "labels": [
        "bug",
        "Known Gaps",
        "triaged",
        "torchtune"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1430/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1430",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1430",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:07.337830",
      "comments": []
    },
    {
      "issue_number": 1491,
      "title": "What's the future plan for torchchat serving",
      "body": "I see current torchchat serving provides basic serving function. I'm wondering what the future plan for serving. What's the target of torchchat serve? Will it provide more optimized and high performance serving features(like Continuous batching, prefix-caching, chunked prefill, etc.)",
      "state": "open",
      "author": "jenniew",
      "author_type": "User",
      "created_at": "2025-02-08T00:30:52Z",
      "updated_at": "2025-02-21T00:18:16Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1491/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1491",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1491",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:07.337853",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "I plan on sharing a directional RFC in the coming week (or next). As a sneak peek, we plan to grow torchchat via 2 vectors:\n* Optimizing the core inference logic \n* Leveraging PyTorch libraries (torchtune, torchao, ExecuTorch, etc.)\n\nI believe your list will fall into the former\n\n> Will it provide m",
          "created_at": "2025-02-10T21:21:19Z"
        },
        {
          "author": "jenniew",
          "body": "Thank you so your response. Could you also provide some benchmarks so we can easy test performance on our platform?",
          "created_at": "2025-02-20T00:52:33Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Totally, I'll spin up a script",
          "created_at": "2025-02-21T00:18:15Z"
        }
      ]
    },
    {
      "issue_number": 1033,
      "title": "Streamlit Browser Issue",
      "body": "### 🐛 Describe the bug\r\n\r\nI am using the streamlit version of streamlit 1.37.1 and while I run the browser version of torchat for llama2. Getting the following error\r\n\r\n`2024-08-15 17:04:46.768 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified`\r\n\r\n`2024-08-15 17:07:17.349 Uncaught app exception\r\nTraceback (most recent call last):`\r\n  \r\n  `File \"/home/ubuntu/new_repo/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 85, in exec_func_with_error_handling\r\n    result = func()`\r\n  \r\n `File \"/home/ubuntu/new_repo/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 576, in code_to_exec\r\n    exec(code, module.__dict__)`\r\n  \r\n  `File \"/home/ubuntu/new_repo/torchchat/torchchat.py\", line 78, in <module>\r\n    browser_main(args)`\r\n  \r\n  `File \"/home/ubuntu/new_repo/torchchat/browser/browser.py\", line 62, in main\r\n    req = CompletionRequest(\r\nTypeError: CompletionRequest.__init__() got an unexpected keyword argument 'prompt' `\r\n\r\nAny idea on it or any one of you faced it before?\r\n\r\n### Versions\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240814+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (aarch64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.2.0-1017-aws-aarch64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       aarch64\r\nCPU op-mode(s):                     64-bit\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          ARM\r\nModel name:                         Neoverse-V2\r\nModel:                              1\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 32\r\nSocket(s):                          1\r\nStepping:                           r0p1\r\nBogoMIPS:                           2000.00\r\nFlags:                              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 flagm2 frint svei8mm svebf16 i8mm bf16 dgh rng bti\r\nL1d cache:                          2 MiB (32 instances)\r\nL1i cache:                          2 MiB (32 instances)\r\nL2 cache:                           64 MiB (32 instances)\r\nL3 cache:                           36 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:           Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240814+cpu\r\n[pip3] torchao==0.4.0+git2e7db12\r\n[conda] Could not collect",
      "state": "closed",
      "author": "nobelchowdary",
      "author_type": "User",
      "created_at": "2024-08-15T17:18:03Z",
      "updated_at": "2025-02-10T18:32:34Z",
      "closed_at": "2024-08-26T17:12:40Z",
      "labels": [
        "bug",
        "Browser"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1033/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1033",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1033",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:07.556850",
      "comments": [
        {
          "author": "nobelchowdary",
          "body": "The main issues are \r\n\r\n`2024-08-16 16:35:46.438 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_`\r\n\r\n`TypeError: CompletionRequest.__init__() got an unexpected keyword argument 'prompt'`\r\n\r",
          "created_at": "2024-08-16T16:38:11Z"
        },
        {
          "author": "nobelchowdary",
          "body": "The api.py file contain the CompletionRequest class. It does not contain the argument `prompt` which is used in the browser.py. How this issue can be resolved?\r\n\r\nAlso when tried to solve the prompt issue, there is another issue raised \r\n`AttributeError: 'OpenAiApiGenerator' object has no attribute ",
          "created_at": "2024-08-16T17:08:01Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "@vmpuri Can you take a look at this?\r\n\r\nSeems like you have a few PR's in ready in this area. Are those related?\r\nhttps://github.com/pytorch/torchchat/pull/1034 \r\nhttps://github.com/pytorch/torchchat/pull/1035",
          "created_at": "2024-08-19T16:33:45Z"
        },
        {
          "author": "vmpuri",
          "body": "Just merged both of the PRs\r\n\r\nhttps://github.com/pytorch/torchchat/pull/1034\r\nhttps://github.com/pytorch/torchchat/pull/1035\r\n\r\n@nobelchowdary Try pulling and trying the browser again. I've changed how the browser works - now, the UI queries the server backend. \r\n\r\nRun the server in one terminal:\r\n",
          "created_at": "2024-08-19T20:36:46Z"
        },
        {
          "author": "nobelchowdary",
          "body": "Hi @vmpuri, when I try to run the current repo - getting this error on the browser \r\n\r\n`InternalServerError: <!doctype html> <html lang=en> <title>500 Internal Server Error</title> <h1>Internal Server Error</h1> <p>The server encountered an internal error and was unable to complete your request. Eit",
          "created_at": "2024-08-20T15:07:05Z"
        }
      ]
    },
    {
      "issue_number": 521,
      "title": "[FEATURE REQUEST] Clang vectoriation on ARM: `warning: loop not vectorized`",
      "body": "```\r\n(py311) mikekg@mikekg-mbp torchchat % python torchchat.py export --output-dso s.so  --quant '{\"embedding\": {\"bitwidth\":8, \"groupsize\": 32}}' --checkpoint-path ${MODEL_PATH} --temperature 0\r\nUsing device=cpu\r\nLoading model...\r\nTime to load model: 0.04 seconds\r\nQuantizing the model with: {'embedding': {'bitwidth': 8, 'groupsize': 32}}\r\nTime to quantize model: 0.05 seconds\r\nExporting model using AOT Inductor to /Users/mikekg/memory/x/z/a/b/torchchat/s.so\r\n/Users/mikekg/memory/x/z/a/b/torchchat/cjks6zm6fxtuhqcxm7zrxesso4ksap62pjzfrfjhak7h5djxutyu.cpp:523:17: warning: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\r\nextern \"C\" void cpp_fused_index_put_stack_1(const float* in_ptr0,\r\n                ^\r\n/Users/mikekg/memory/x/z/a/b/torchchat/cjks6zm6fxtuhqcxm7zrxesso4ksap62pjzfrfjhak7h5djxutyu.cpp:1112:17: warning: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\r\nextern \"C\" void cpp_fused_index_put_stack_6(const float* in_ptr0,\r\n                ^\r\n/Users/mikekg/memory/x/z/a/b/torchchat/cjks6zm6fxtuhqcxm7zrxesso4ksap62pjzfrfjhak7h5djxutyu.cpp:1645:17: warning: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\r\nextern \"C\" void cpp_fused_index_put_stack_11(const float* in_ptr0,\r\n                ^\r\n/Users/mikekg/memory/x/z/a/b/torchchat/cjks6zm6fxtuhqcxm7zrxesso4ksap62pjzfrfjhak7h5djxutyu.cpp:2197:17: warning: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\r\nextern \"C\" void cpp_fused_index_put_stack_16(const float* in_ptr0,\r\n                ^\r\n/Users/mikekg/memory/x/z/a/b/torchchat/cjks6zm6fxtuhqcxm7zrxesso4ksap62pjzfrfjhak7h5djxutyu.cpp:2758:17: warning: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\r\nextern \"C\" void cpp_fused_index_put_stack_21(const float* in_ptr0,\r\n                ^\r\n/Users/mikekg/memory/x/z/a/b/torchchat/cjks6zm6fxtuhqcxm7zrxesso4ksap62pjzfrfjhak7h5djxutyu.cpp:3310:17: warning: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\r\nextern \"C\" void cpp_fused_index_put_stack_26(const float* in_ptr0,\r\n                ^\r\n6 warnings generated.\r\nThe generated DSO model can be found at: /Users/mikekg/memory/x/z/a/b/torchchat/s.so\r\n(py311) mikekg@mikekg-mbp torchchat % \r\n```\r\n\r\ncc: @manuelcandales @malfet @swolchok ",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-27T04:11:01Z",
      "updated_at": "2025-02-08T01:02:26Z",
      "closed_at": null,
      "labels": [
        "triaged"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/521/reactions",
        "total_count": 5,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [
        "nadavrot",
        "helloguo"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/521",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/521",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:07.919989",
      "comments": [
        {
          "author": "nadavrot",
          "body": "@mikekgfb could you please upload the generated .cpp file? If the content is confidential you may be able to reduce the code with \"creduce\"",
          "created_at": "2024-04-30T03:59:42Z"
        },
        {
          "author": "WenleiHe",
          "body": "Yes having the generated .cpp file would help us get right into the investigation (as we're not super familiar with all the setup yet).",
          "created_at": "2024-04-30T04:01:36Z"
        },
        {
          "author": "helloguo",
          "body": "I was able to reproduce the warning with the toy model stories15M.pt (not the exact cpp source though). The whole cpp file is a bit too large to share. One of the loops looks like this. The warning is about the line `#pragma omp simd simdlen(4)`. Just scanning the code without looking at the compile",
          "created_at": "2024-04-30T06:27:20Z"
        },
        {
          "author": "nadavrot",
          "body": "Good catch @helloguo, the LLVM loop vectorizer only vectorizes innermost loops. Mystery solved. ",
          "created_at": "2024-04-30T17:56:57Z"
        },
        {
          "author": "cccclai",
          "body": "@Jack-Khuu do you know the context?",
          "created_at": "2025-02-07T21:22:20Z"
        }
      ]
    },
    {
      "issue_number": 1304,
      "title": "--compile 3x slower using RTX4090",
      "body": "### 🐛 Describe the bug\n\nEager gives me 31 tokens a second\r\n`python3 torchchat.py generate llama3.1 --prompt \"what's the most recent historical event\" --device cuda`\r\n\r\ntorch.compile gives me 10 tokens a second\r\n`python3 torchchat.py generate llama3.1 --prompt \"what's the most recent historical event\" --device cuda --compile`\r\n\r\nI would expect compile at worst not to be slower than eager.\n\n### Versions\n\nOperating System Information\r\nLinux Vikander 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nPRETTY_NAME=\"Ubuntu 24.04.1 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"24.04\"\r\nVERSION=\"24.04.1 LTS (Noble Numbat)\"\r\nVERSION_CODENAME=noble\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=noble\r\nLOGO=ubuntu-logo\r\n\r\nPython Version\r\nPython 3.11.10\r\n\r\nPIP Version\r\npip 24.0 from /home/warden/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n\r\nInstalled Packages\r\nabsl-py==2.1.0\r\naccelerate==1.0.1\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.10\r\naiosignal==1.3.1\r\naltair==5.4.1\r\nannotated-types==0.7.0\r\nantlr4-python3-runtime==4.9.3\r\nanyio==4.6.2.post1\r\nattrs==24.2.0\r\nblinker==1.8.2\r\nblobfile==3.0.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\nchardet==5.2.0\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncmake==3.30.4\r\ncolorama==0.4.6\r\nDataProperty==1.0.1\r\ndatasets==3.0.1\r\ndill==0.3.8\r\ndistro==1.9.0\r\nevaluate==0.4.3\r\nfilelock==3.16.1\r\nFlask==3.0.3\r\nfrozenlist==1.4.1\r\nfsspec==2024.6.1\r\ngguf==0.10.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\nh11==0.14.0\r\nhttpcore==1.0.6\r\nhttpx==0.27.2\r\nhuggingface-hub==0.25.2\r\nidna==3.10\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.6.1\r\njoblib==1.4.2\r\njsonlines==4.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2024.10.1\r\nlm_eval==0.4.2\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==3.0.1\r\nmbstrdecoder==1.1.3\r\nmdurl==0.1.2\r\nmore-itertools==10.5.0\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nmultiprocess==0.70.16\r\nnarwhals==1.9.3\r\nnetworkx==3.4.1\r\nninja==1.11.1.1\r\nnltk==3.9.1\r\nnumexpr==2.10.1\r\nnumpy==1.26.4\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==9.1.0.70\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-nccl-cu12==2.21.5\r\nnvidia-nvjitlink-cu12==12.6.77\r\nnvidia-nvtx-cu12==12.1.105\r\nomegaconf==2.3.0\r\nopenai==1.51.2\r\npackaging==24.1\r\npandas==2.2.3\r\npathvalidate==3.2.1\r\npeft==0.13.2\r\npillow==10.4.0\r\nportalocker==2.10.1\r\npropcache==0.2.0\r\nprotobuf==5.28.2\r\npsutil==6.0.0\r\npyarrow==17.0.0\r\npybind11==2.13.6\r\npycryptodomex==3.21.0\r\npydantic==2.9.2\r\npydantic_core==2.23.4\r\npydeck==0.9.1\r\nPygments==2.18.0\r\npytablewriter==1.2.0\r\npython-dateutil==2.9.0.post0\r\npytorch-triton==3.1.0+cf34004b8a\r\npytz==2024.2\r\nPyYAML==6.0.2\r\nreferencing==0.35.1\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrich==13.9.2\r\nrouge-score==0.1.2\r\nrpds-py==0.20.0\r\nsacrebleu==2.4.3\r\nsafetensors==0.4.5\r\nscikit-learn==1.5.2\r\nscipy==1.14.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsnakeviz==2.2.0\r\nsniffio==1.3.1\r\nsqlitedict==2.1.0\r\nstreamlit==1.39.0\r\nsympy==1.13.1\r\ntabledata==1.3.3\r\ntabulate==0.9.0\r\ntcolorpy==0.1.6\r\ntenacity==9.0.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.8.0\r\ntokenizers==0.20.1\r\ntoml==0.10.2\r\ntorch==2.6.0.dev20241002+cu121\r\ntorchao==0.5.0\r\ntorchtune==0.3.0.dev20240928+cu121\r\ntorchvision==0.20.0.dev20241002+cu121\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntqdm-multiprocess==0.0.11\r\ntransformers==4.45.2\r\ntypepy==1.3.2\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nurllib3==2.2.3\r\nwatchdog==5.0.3\r\nWerkzeug==3.0.4\r\nword2number==1.1\r\nxxhash==3.5.0\r\nyarl==1.15.2\r\nzstandard==0.23.0\r\nzstd==1.5.5.1\r\n\r\nPyTorch Version\r\n2.6.0.dev20241002+cu121\r\n\r\n",
      "state": "closed",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-10-16T18:47:58Z",
      "updated_at": "2025-02-07T22:44:58Z",
      "closed_at": "2025-02-07T22:44:35Z",
      "labels": [
        "Compile / AOTI",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1304/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1304",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1304",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:08.171956",
      "comments": [
        {
          "author": "desertfire",
          "body": "You need to add `--num-samples 3`. The default value is 1, which means the compile overhead will exceed the compile perf gain.\r\n\r\n@Jack-Khuu , we should standardize the perf measurement script and check it in.",
          "created_at": "2024-10-16T21:07:14Z"
        },
        {
          "author": "byjlw",
          "body": "I don't recall having this issue with samples = 1 in the past. We should make sure we're breaking out time spent compiling and time spent during generation so users can see how time is being spent and what the real t/s is. \r\n\r\nUsers are not going to want to have more than one sample unless they're b",
          "created_at": "2024-10-17T16:24:47Z"
        },
        {
          "author": "desertfire",
          "body": "Because --compile is using torch.compile, which is JIT compilation, there is no easy way to separate compile time and execution time. If the cold start time is a concern for user, they should choose the AOTI path.",
          "created_at": "2024-10-25T19:38:34Z"
        },
        {
          "author": "kimishpatel",
          "body": "@Jack-Khuu assigning to you",
          "created_at": "2025-02-07T21:55:52Z"
        }
      ]
    },
    {
      "issue_number": 1311,
      "title": "Eval fails on CUDA with AOTI exported model",
      "body": "### 🐛 Describe the bug\n\n\r\n```\r\n$ python torchchat.py export stories110M --dtype float16 --output-dso-path stories.so\r\n\r\nUsing device=cuda\r\nSetting max_seq_length to 300 for DSO export.\r\nLoading model...\r\nTime to load model: 0.44 seconds\r\n-----------------------------------------------------------\r\nExporting model using AOT Inductor to /content/torchchat-1/stories.so\r\nW1017 20:10:20.554000 7389 torch/_export/__init__.py:225] +============================+\r\nW1017 20:10:20.554000 7389 torch/_export/__init__.py:226] |     !!!   WARNING   !!!    |\r\nW1017 20:10:20.555000 7389 torch/_export/__init__.py:227] +============================+\r\nW1017 20:10:20.555000 7389 torch/_export/__init__.py:228] torch._export.aot_compile() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export()) instead.\r\nThe generated DSO model can be found at: /content/torchchat-1/stories.so\r\n2024-10-17 20:12:01.733978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-10-17 20:12:01.753928: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-10-17 20:12:01.759899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-10-17 20:12:01.774061: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-10-17 20:12:02.820101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nUsing device=cuda\r\nLoading model...\r\nTime to load model: 0.48 seconds\r\n-----------------------------------------------------------\r\n\r\n$ python torchchat.py eval stories110M --dtype float16 --dso-path stories.so --limit 5\r\n2024-10-17:20:12:09,610 INFO     [huggingface.py:162] Using device 'cuda'\r\nconfig.json: 100% 665/665 [00:00<00:00, 3.09MB/s]\r\nmodel.safetensors: 100% 548M/548M [00:05<00:00, 101MB/s]\r\ngeneration_config.json: 100% 124/124 [00:00<00:00, 733kB/s]\r\ntokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 132kB/s]\r\nvocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.67MB/s]\r\nmerges.txt: 100% 456k/456k [00:00<00:00, 1.09MB/s]\r\ntokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 2.14MB/s]\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\r\n  warnings.warn(\r\n2024-10-17:20:12:27,047 WARNING  [task.py:763] [Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\r\n2024-10-17:20:12:27,047 WARNING  [task.py:775] [Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\r\n2024-10-17:20:12:27,047 WARNING  [task.py:763] [Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\r\n2024-10-17:20:12:27,047 WARNING  [task.py:775] [Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\r\n2024-10-17:20:12:27,047 WARNING  [task.py:763] [Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\r\n2024-10-17:20:12:27,047 WARNING  [task.py:775] [Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\r\nwikitext_document_level.py: 100% 10.7k/10.7k [00:00<00:00, 39.4MB/s]\r\nREADME.md: 100% 7.78k/7.78k [00:00<00:00, 32.7MB/s]\r\nRepo card metadata block was not found. Setting CardData to empty.\r\n2024-10-17:20:12:29,949 WARNING  [repocard.py:107] Repo card metadata block was not found. Setting CardData to empty.\r\nDownloading data: 100% 4.72M/4.72M [00:00<00:00, 7.37MB/s]\r\nGenerating test split: 62 examples [00:00, 656.90 examples/s]\r\nGenerating train split: 629 examples [00:00, 1999.28 examples/s]\r\nGenerating validation split: 60 examples [00:00, 2830.26 examples/s]\r\n2024-10-17:20:12:32,165 INFO     [task.py:395] Building contexts for wikitext on rank 0...\r\n100% 5/5 [00:00<00:00, 420.70it/s]\r\n2024-10-17:20:12:32,178 INFO     [evaluator.py:362] Running loglikelihood_rolling requests\r\n  0% 0/5 [00:01<?, ?it/s]\r\nTime to run eval: 23.69s.\r\nTraceback (most recent call last):\r\n  File \"/content/torchchat-1/torchchat.py\", line 92, in <module>\r\n    eval_main(args)\r\n  File \"/content/torchchat-1/torchchat/usages/eval.py\", line 271, in main\r\n    result = eval(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/torchchat-1/torchchat/usages/eval.py\", line 217, in eval\r\n    eval_results = evaluate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/lm_eval/utils.py\", line 288, in _wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/lm_eval/evaluator.py\", line 373, in evaluate\r\n    resps = getattr(lm, reqtype)(cloned_reqs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/lm_eval/models/huggingface.py\", line 840, in loglikelihood_rolling\r\n    string_nll = self._loglikelihood_tokens(\r\n  File \"/usr/local/lib/python3.10/dist-packages/lm_eval/models/huggingface.py\", line 1074, in _loglikelihood_tokens\r\n    logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\r\nRuntimeError: Size does not match at dimension 1 expected index [1, 1537, 1] to be smaller than self [1, 1, 32000] apart from dimension 2\r\n```\r\n\n\n### Versions\n\n```\r\n--2024-10-17 20:26:14--  https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 23357 (23K) [text/plain]\r\nSaving to: ‘collect_env.py’\r\n\r\ncollect_env.py      100%[===================>]  22.81K  --.-KB/s    in 0s      \r\n\r\n2024-10-17 20:26:14 (156 MB/s) - ‘collect_env.py’ saved [23357/23357]\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241002+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: version 3.30.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 535.104.05\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               2\r\nOn-line CPU(s) list:                  0,1\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\r\nCPU family:                           6\r\nModel:                                85\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   1\r\nSocket(s):                            1\r\nStepping:                             3\r\nBogoMIPS:                             4000.28\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            32 KiB (1 instance)\r\nL1i cache:                            32 KiB (1 instance)\r\nL2 cache:                             1 MiB (1 instance)\r\nL3 cache:                             38.5 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0,1\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Mitigation; PTE Inversion\r\nVulnerability Mds:                    Vulnerable; SMT Host state unknown\r\nVulnerability Meltdown:               Vulnerable\r\nVulnerability Mmio stale data:        Vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Vulnerable\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Vulnerable\r\nVulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.13.0\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241002+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.4.1+cu121\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtune==0.4.0.dev20241010+cu121\r\n[pip3] torchvision==0.20.0.dev20241002+cu121\r\n[conda] Could not collect\r\n```",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-10-17T20:26:57Z",
      "updated_at": "2025-02-07T22:43:12Z",
      "closed_at": null,
      "labels": [
        "bug",
        "Compile / AOTI",
        "triaged",
        "Evaluation/Benchmarking"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1311/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1311",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1311",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:08.368411",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Errr thanks for flagging, thought we fixed this before",
          "created_at": "2024-10-21T17:24:34Z"
        },
        {
          "author": "mikekgfb",
          "body": "> Errr thanks for flagging, thought we fixed this before\r\n\r\nI think I saw a fix for PTE models previously (#1053 ), but not for DSO?  Also, once these work, would be great to have tests for both backends (and device={cuda, cpu} for AOTI) running for a few batches?  \r\n\r\nPS: This is one of the benefit",
          "created_at": "2024-11-07T01:18:30Z"
        },
        {
          "author": "mikekgfb",
          "body": "Added PR #1411 to include testing eval with AOTI now that basic eval test works.  Establish if it works, and if so, have test for future changes\r\n",
          "created_at": "2024-12-10T05:04:12Z"
        },
        {
          "author": "kimishpatel",
          "body": "@Jack-Khuu  can you close if this is fixed",
          "created_at": "2025-02-07T21:55:18Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "This one is live",
          "created_at": "2025-02-07T22:43:05Z"
        }
      ]
    },
    {
      "issue_number": 1325,
      "title": "RFC: Quantization Evaluation",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\nWith a single command, quantize the same model across every available quant scheme and configuration and output a table that compares the results. This will allow users to make better decisions about what quant scheme to use\r\n\r\nExtend the eval command to quant and allow users to compare the performance and correctness of different quantization schemes. \r\n\r\nCommand would be something like this\r\n```\r\ntorchchat.py eval llama3.1  --quantize linear dynamic --device mps --outputFormat table\r\n```\r\n\r\nIn this instance the command would evaluate each available option dtype or configuration for each scheme specified. \r\n\r\nAnd outputs a table that compares them all\r\n\r\nModel: Llama 3.1 Instruct 8B\r\nDevice: M1 Max 64GB MPS\r\nExecutionMode: Eager\r\n\r\n| scheme | weights | activations | group size | embeddings | weight | group size | model size | t/s | peak memory | perplexity |\r\n| --------|---------| -----------| ----------- | ------------ | ------- | ----------- | ---------- | --- | -------------| -----------|\r\n| none | bf16 | - | - | no | - | - | 16.2GB | 17 t/s | N/A |  WikiText2: 7.1 |\r\n| linear | 4bit | - | 256 | yes | 4bit | 256 | 4.3GB | 32 t/s | N/A |  WikiText2: 8.1 |\r\n| linear | 8bit | - | 256 | yes | 8bit | 256 | 8.3GB | 27 t/s | N/A |  WikiText2: 7.3 |\r\n\r\nNew flags for eval\r\n`--quantize (optional and default to none)`\r\n  Description: provide a set of quantization schemes to use and the command will try to run quantization on every permutation based on the dtypes available. single scheme sample: `{linear}`, multi-scheme sample: `{linear, dynamic, embedding}`. If embedding is specified, there will be an m*n of every quant + embedding available.\r\nAll available options: `linear, dynamic, embedding, embedding:wx` \r\n\r\n`--outputFormat (optional and default to table)`\r\nDescription: the format of the output. Either `table` or `json`\r\n\r\n\r\n## Design\r\n\r\nAll available options for a particular scheme, model, device and execution mode stored in a json object.\r\nThis will be the source of truth and the command can iterate through \r\ndictionary to run eval on every possible permutation\r\nThis will be stored in quant_config/quant.json\r\nFormat of the object should be something like:\r\n\r\n```\r\n\"devices\": [\r\n      {\r\n        \"name\": \"cpu\",\r\n        \"model_types\": [\r\n          {\r\n            \"type\": \"textOnly\",\r\n            \"execution_modes\": [\r\n              {\r\n                \"mode\": \"eager\",\r\n                \"quantization_options\": {\r\n                  \"quant_schemes\": [\r\n                    {\r\n                      \"scheme\": \"linear\",\r\n                      \"weight_dtypes\": [4, 8]\r\n                    },\r\n                    {\r\n                      \"scheme\": \"dynamic\",\r\n                      \"weight_dtypes\": [4, 8],\r\n                      \"activation_dtypes\": [4, 8]\r\n                    }\r\n                  ],\r\n                  \"embedding_quant_schemes\": [\r\n                    {\r\n                      \"scheme\": \"linear\",\r\n                      \"weight_dtipes\": [4, 8]\r\n                    }\r\n                  ],\r\n                  \"weight_group_sizes\": [256],\r\n                  \"embedding_group_sizes\": [256]\r\n                }\r\n              },\r\n              {\r\n                \"mode\": \"compile\",\r\n                \"quantization_options\": {\r\n                  \"quant_schemes\": [\r\n                    {\r\n                      \"scheme\": \"linear\",\r\n                      \"weight_dtypes\": [4, 8]\r\n                    },\r\n                    {\r\n                      \"scheme\": \"dynamic\",\r\n                      \"weight_dtypes\": [4, 8],\r\n                      \"activation_dtypes\": [4, 8]\r\n                    }\r\n                  ],\r\n                  \"embedding_quant_schemes\": [\r\n                    {\r\n                      \"scheme\": \"linear\",\r\n                      \"weight_dtipes\": [4, 8]\r\n                    }\r\n                  ],\r\n                  \"weight_group_sizes\": [256],\r\n                  \"embedding_group_sizes\": [256]\r\n                }\r\n              }\r\n            ]\r\n          }\r\n```\r\nmodeltype options: `textOnly`, `llamaTextOnly`, `llamaVision`, `llava`\r\n\r\nThe model definitions in mode_config/models.json will be extended to include a new property `model_type`\r\n\r\nThe eval.py needs to be extended so that evaluation can be run multiple times and generate the list of runs to make based on the flag values that came in.\r\n\r\nWhen the model comes in we can look up the model type and device and then run all the configs in the set that match the schemes present in the `--quantize` flag\r\nIf `--quantization` is not present we do a single run using the specified params\r\n",
      "state": "closed",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-10-24T17:58:14Z",
      "updated_at": "2025-02-07T22:39:43Z",
      "closed_at": "2025-02-07T22:39:43Z",
      "labels": [
        "enhancement",
        "RFC"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1325/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1325",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1325",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:08.581512",
      "comments": [
        {
          "author": "byjlw",
          "body": "Work has begun in this branch\r\nhttps://github.com/pytorch/torchchat/tree/quant_eval",
          "created_at": "2024-10-24T22:35:42Z"
        }
      ]
    },
    {
      "issue_number": 1341,
      "title": "OOM Llama 3.2-11B on RTX 4090 ",
      "body": "### 🐛 Describe the bug\n\ntorchchat will OOM when using the Llama 3.2 11b model almost right away. If it doesn't OOM on the first request it will on the second. \r\n\r\nWhen spinning up the server it'll immediately take 22GB of memory (expected)\r\n\r\nFirst request will take it close to the 24GB limit or over in some cases. \r\nSecond request will definitely will\r\n\r\n## Instructions\r\n`python3 torchchat.py server llama3.2-11B`\r\n\r\n`streamlit run torchchat/usages/browser.py`\r\n\r\nAdd an image and provide a prompt\r\n\r\nOutput\r\n\r\n```\r\n2024-11-02:13:30:48,748 INFO     [_internal.py:97] Press CTRL+C to quit\r\n === Completion Request ===\r\nThis2024-11-02:13:32:18,977 INFO     [_internal.py:97] 127.0.0.1 - - [02/Nov/2024 13:32:18] \"POST /v1/chat/completions HTTP/1.1\" 200 -\r\n image shows a large spaceship on a barren planet with three astronauts in the foreground. One astronaut is carrying what looks like a dolly. In the background, there is another spaceship that looks like ruins and two large moons. The overall mood is creepy, with the astronauts in spacesuits and the desolate surroundings. === Completion Request ===\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,311 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,345 INFO     [_internal.py:97] 127.0.0.1 - - [02/Nov/2024 13:33:45] \"POST /v1/chat/completions HTTP/1.1\" 500 -\r\n2024-11-02:13:33:45,348 ERROR    [_internal.py:97] Error on request:\r\nTraceback (most recent call last):\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/serving.py\", line 370, in run_wsgi\r\n    execute(self.server.app)\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/serving.py\", line 333, in execute\r\n    for data in application_iter:\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/wsgi.py\", line 256, in __next__\r\n    return self._next()\r\n           ^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/wrappers/response.py\", line 32, in _iter_encoded\r\n    for item in iterable:\r\n  File \"/home/warden/source/torchchat/torchchat/usages/server.py\", line 77, in chunk_processor\r\n    for chunk in chunked_completion_generator:\r\n  File \"/home/warden/source/torchchat/torchchat/usages/openai_api.py\", line 392, in chunked_completion\r\n    for y, _ in self.generate(\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 645, in generate\r\n    next_token = self.prefill(\r\n                 ^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 382, in prefill\r\n    logits = model(\r\n             ^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/model.py\", line 568, in forward\r\n    return self.model(\r\n           ^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/model_fusion/_fusion.py\", line 446, in forward\r\n    encoder_embed = self.encoder(**encoder_input)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/models/llama3_2_vision/_encoder.py\", line 123, in forward\r\n    x, hidden_states = self.clip(images, aspect_ratio)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/vision_transformer.py\", line 389, in forward\r\n    x = transformer_layer(x)\r\n        ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/transformer.py\", line 120, in forward\r\n    mlp_out = self.mlp(self.mlp_norm(h))\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/feed_forward.py\", line 50, in forward\r\n    h = self.activation(self.w1(x))\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 734, in forward\r\n    return F.gelu(input, approximate=self.approximate)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 40.31 MiB is free. Including non-PyTorch memory, this process has 22.79 GiB memory in use. Of the allocated memory 21.99 GiB is allocated by PyTorch, and 333.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n === Completion Request ===\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,872 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:45,893 INFO     [_internal.py:97] 127.0.0.1 - - [02/Nov/2024 13:33:45] \"POST /v1/chat/completions HTTP/1.1\" 500 -\r\n2024-11-02:13:33:45,894 ERROR    [_internal.py:97] Error on request:\r\nTraceback (most recent call last):\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/serving.py\", line 370, in run_wsgi\r\n    execute(self.server.app)\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/serving.py\", line 333, in execute\r\n    for data in application_iter:\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/wsgi.py\", line 256, in __next__\r\n    return self._next()\r\n           ^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/wrappers/response.py\", line 32, in _iter_encoded\r\n    for item in iterable:\r\n  File \"/home/warden/source/torchchat/torchchat/usages/server.py\", line 77, in chunk_processor\r\n    for chunk in chunked_completion_generator:\r\n  File \"/home/warden/source/torchchat/torchchat/usages/openai_api.py\", line 392, in chunked_completion\r\n    for y, _ in self.generate(\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 645, in generate\r\n    next_token = self.prefill(\r\n                 ^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 382, in prefill\r\n    logits = model(\r\n             ^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/model.py\", line 568, in forward\r\n    return self.model(\r\n           ^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/model_fusion/_fusion.py\", line 446, in forward\r\n    encoder_embed = self.encoder(**encoder_input)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/models/llama3_2_vision/_encoder.py\", line 123, in forward\r\n    x, hidden_states = self.clip(images, aspect_ratio)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/vision_transformer.py\", line 381, in forward\r\n    x = self.ln_pre(x)\r\n        ^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/layer_norm.py\", line 30, in forward\r\n    output = nn.functional.layer_norm(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 2910, in layer_norm\r\n    return torch.layer_norm(\r\n           ^^^^^^^^^^^^^^^^^\r\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 38.31 MiB is free. Including non-PyTorch memory, this process has 22.80 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 221.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n === Completion Request ===\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,010 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,011 WARNING  [attention.py:156] Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping.\r\n2024-11-02:13:33:47,031 INFO     [_internal.py:97] 127.0.0.1 - - [02/Nov/2024 13:33:47] \"POST /v1/chat/completions HTTP/1.1\" 500 -\r\n2024-11-02:13:33:47,032 ERROR    [_internal.py:97] Error on request:\r\nTraceback (most recent call last):\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/serving.py\", line 370, in run_wsgi\r\n    execute(self.server.app)\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/serving.py\", line 333, in execute\r\n    for data in application_iter:\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/wsgi.py\", line 256, in __next__\r\n    return self._next()\r\n           ^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/werkzeug/wrappers/response.py\", line 32, in _iter_encoded\r\n    for item in iterable:\r\n  File \"/home/warden/source/torchchat/torchchat/usages/server.py\", line 77, in chunk_processor\r\n    for chunk in chunked_completion_generator:\r\n  File \"/home/warden/source/torchchat/torchchat/usages/openai_api.py\", line 392, in chunked_completion\r\n    for y, _ in self.generate(\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 645, in generate\r\n    next_token = self.prefill(\r\n                 ^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 382, in prefill\r\n    logits = model(\r\n             ^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/model.py\", line 568, in forward\r\n    return self.model(\r\n           ^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/model_fusion/_fusion.py\", line 446, in forward\r\n    encoder_embed = self.encoder(**encoder_input)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/models/llama3_2_vision/_encoder.py\", line 123, in forward\r\n    x, hidden_states = self.clip(images, aspect_ratio)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/vision_transformer.py\", line 381, in forward\r\n    x = self.ln_pre(x)\r\n        ^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torchtune/modules/layer_norm.py\", line 30, in forward\r\n    output = nn.functional.layer_norm(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 2910, in layer_norm\r\n    return torch.layer_norm(\r\n           ^^^^^^^^^^^^^^^^^\r\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 38.31 MiB is free. Including non-PyTorch memory, this process has 22.80 GiB memory in use. Of the allocated memory 22.10 GiB is allocated by PyTorch, and 222.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n```\r\n\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241002+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 13.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.77\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 4090\r\nGPU 1: NVIDIA GeForce RTX 3090\r\n\r\nNvidia driver version: 560.35.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               32\r\nOn-line CPU(s) list:                  0-31\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD Ryzen 9 7950X3D 16-Core Processor\r\nCPU family:                           25\r\nModel:                                97\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            1\r\nStepping:                             2\r\nCPU(s) scaling MHz:                   29%\r\nCPU max MHz:                          5759.0000\r\nCPU min MHz:                          545.0000\r\nBogoMIPS:                             8400.06\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\r\nVirtualization:                       AMD-V\r\nL1d cache:                            512 KiB (16 instances)\r\nL1i cache:                            512 KiB (16 instances)\r\nL2 cache:                             16 MiB (16 instances)\r\nL3 cache:                             128 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-31\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241002+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.4.0.dev20241010+cu121\r\n[pip3] torchvision==0.20.0.dev20241002+cu121\r\n",
      "state": "open",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-11-02T20:44:33Z",
      "updated_at": "2025-02-07T21:35:10Z",
      "closed_at": null,
      "labels": [
        "bug",
        "performance",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1341/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1341",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1341",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:08.798519",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for the flag, this'll be a good snapshot as we work on quant/reducing overhead",
          "created_at": "2024-11-12T02:15:56Z"
        }
      ]
    },
    {
      "issue_number": 522,
      "title": "[TORCHAO] Handle non-multiple group sizes, support padding as appropriate in torchao and kernels",
      "body": "@jerryzh168  Please add consistent padding support in torchao to make models quantizable\r\n@digantdesai what's the best way to implement this - just round up and ignore part of the result?  \r\nI can't imagine it's worthwhile to write a kernel for partial groups.  Presumably this needs to be done before \r\nallocation?  How \r\n\r\nhttps://github.com/pytorch/torchchat/actions/runs/8857009260/job/24323964701?pr=519\r\n\r\n```\r\n******************************************\r\n*** --quantize config/data/mobile.json ***\r\n******************************************\r\nINFO:datasets:PyTorch version 2.4.0.dev20240422 available.\r\nUsing device=cpu\r\nLoading model...\r\nTime to load model: 0.01 seconds\r\nQuantizing the model with: {'embedding': {'bitwidth': 4, 'groupsize': 32}, 'linear:a8w4dq': {'groupsize': 256}}\r\n\r\nDownloading builder script:   0%|          | 0.00/5.67k [00:00<?, ?B/s]\r\nDownloading builder script: 100%|██████████| 5.67k/5.67k [00:00<00:00, 4.87MB/s]\r\nTraceback (most recent call last):\r\nlinear: layers.0.attention.wq, in=288, out=288\r\n  File \"/Users/runner/work/torchchat/torchchat/export.py\", line 111, in <module>\r\n    main(args)\r\n  File \"/Users/runner/work/torchchat/torchchat/export.py\", line 61, in main\r\n    model = _initialize_model(\r\n            ^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/runner/work/torchchat/torchchat/build/builder.py\", line 406, in _initialize_model\r\n    quantize_model(model, builder_args.device, quantize, tokenizer)\r\n  File \"/Users/runner/work/torchchat/torchchat/quantize.py\", line 52, in quantize_model\r\n    ).quantized_model()\r\n      ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/runner/work/torchchat/torchchat/quantize.py\", line 99, in quantized_model\r\n    return self.quantizer.quantize(self.model_)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchao/quantization/GPTQ.py\", line 1256, in quantize\r\n    state_dict = self._create_quantized_state_dict(model)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchao/quantization/GPTQ.py\", line 1193, in _create_quantized_state_dict\r\n    in_features % self.groupsize == 0\r\nAssertionError: require in_features:288 % self.groupsize:256 == 0\r\nError: Process completed with exit code 1.\r\n```",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-27T08:59:33Z",
      "updated_at": "2025-02-07T21:23:28Z",
      "closed_at": null,
      "labels": [
        "triaged"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/522/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "digantdesai",
        "supriyar",
        "jisaacso",
        "jerryzh168"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/522",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/522",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:09.042713",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "@digantdesai any plan to solve this by April 30?\r\n\r\ncc: @cbilgin @orionr ",
          "created_at": "2024-04-30T06:53:25Z"
        },
        {
          "author": "jerryzh168",
          "body": "can we add padding a bit later, after the tensor subclass refactor? we can include this as an item for torchao 0.3 release",
          "created_at": "2024-04-30T20:54:53Z"
        }
      ]
    },
    {
      "issue_number": 520,
      "title": "Add Android test for #491",
      "body": "Please add a test for #491, to build model,\r\nplus also using the ability to launch android tests from OSS to confirm they work",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-27T03:31:15Z",
      "updated_at": "2025-02-07T21:21:43Z",
      "closed_at": "2025-02-07T21:21:43Z",
      "labels": [
        "triaged"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/520/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "kit1980",
        "shoumikhin",
        "malfet",
        "kirklandsign"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/520",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/520",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:09.267257",
      "comments": [
        {
          "author": "kirklandsign",
          "body": "Instrumentation test is merged. Need to add to OSS CI.\r\n\r\nSo far blocked on\r\n1. Need to have a unified 2/3 runner to try with both 2 and 3 model. \r\n2. Add test part to workflow.",
          "created_at": "2024-05-15T23:10:34Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Linking initial work: https://github.com/pytorch/torchchat/pull/807",
          "created_at": "2024-07-01T17:18:41Z"
        }
      ]
    },
    {
      "issue_number": 513,
      "title": "[FEATURE REQUEST] connect browser to native (Python-free) execution environment",
      "body": "provide ability to hook up browser / flask app with native execution binary runner/run.cpp",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-27T02:20:11Z",
      "updated_at": "2025-02-07T21:21:21Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/513/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/513",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/513",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:09.494939",
      "comments": []
    },
    {
      "issue_number": 1014,
      "title": "torchchat generate requires network connection, even if models are cached",
      "body": "### 🐛 Describe the bug\n\nRun `python3 torchchat.py generate stories110M` on a system with a bad network connection will hang for 90+ sec before it starts generatin anything\n\n### Versions\n\nNot sure why pytorch collect_env is of any use here...",
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-08-06T17:12:51Z",
      "updated_at": "2025-02-06T17:21:40Z",
      "closed_at": "2025-02-06T17:21:39Z",
      "labels": [
        "bug",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1014/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1014",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1014",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:09.494959",
      "comments": [
        {
          "author": "malfet",
          "body": "This happens while one `import torchao.quantization.quant_api`",
          "created_at": "2024-08-06T17:34:42Z"
        },
        {
          "author": "kirklandsign",
          "body": "Issue hasn't been updated for 6 months. Please re-open if it's still valid.",
          "created_at": "2025-02-06T17:21:39Z"
        }
      ]
    },
    {
      "issue_number": 872,
      "title": "requirements setup fails to install/configure triton properly, yielding broken install",
      "body": "Running ./install_requirements.sh runs but has this warning:\r\n~~~\r\nWARNING: Skipping triton as it is not installed.\r\n~~~\r\nWhich  then results in failing when it attemps to locate Triton:\r\n~~~\r\nSuccessfully installed torch-2.5.0.dev20240624+cu121\r\nTraceback (most recent call last):\r\n  File \"/data/users/less/local/torchchat_distributed/scripts/patch_triton.py\", line 20, in <module>\r\n    jit_py = Path(triton.__file__).parent / \"runtime\" / \"jit.py\"\r\n  File \"/home/less/local/miniconda3/envs/gocuda/lib/python3.10/pathlib.py\", line 958, in __new__\r\n    self = cls._from_parts(args)\r\n  File \"/home/less/local/miniconda3/envs/gocuda/lib/python3.10/pathlib.py\", line 592, in _from_parts\r\n    drv, root, parts = self._parse_args(args)\r\n  File \"/home/less/local/miniconda3/envs/gocuda/lib/python3.10/pathlib.py\", line 576, in _parse_args\r\n    a = os.fspath(a)\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n~~~\r\nI'll try to install triton directly to see if that resolves, but given that triton is installed with PyTorch (at least a pinned version, and note that it installed the PyTorch version it wanted above) this is a problem.\r\n",
      "state": "closed",
      "author": "lessw2020",
      "author_type": "User",
      "created_at": "2024-06-28T03:13:22Z",
      "updated_at": "2025-02-04T20:32:22Z",
      "closed_at": "2025-02-04T20:32:22Z",
      "labels": [
        "triaged",
        "Installing"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/872/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/872",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/872",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:09.748986",
      "comments": [
        {
          "author": "lessw2020",
          "body": "installing triton directly first, simply results in the script uninstalling triton:\r\n~~~\r\nFound existing installation: triton 2.3.1\r\nUninstalling triton-2.3.1:\r\n  Successfully uninstalled triton-2.3.1\r\n  ~~~\r\n  and then failing out with the same error...so appears it's trying to find the PyTorch ver",
          "created_at": "2024-06-28T03:15:59Z"
        },
        {
          "author": "lessw2020",
          "body": "note that starting in clean environment succeeds.  \r\nNot sure if you want to just chalk it up to complicated environment regarding not patching triton, or try to robustify the patch_triton.py. \r\n",
          "created_at": "2024-06-28T03:26:24Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thanks Less for flagging\r\nFor the sake of repro, can you elaborate what you mean by \r\n\r\n> that starting in clean environment succeeds.\r\n\r\nWhat is clean environment in this case?",
          "created_at": "2024-07-01T04:53:07Z"
        }
      ]
    },
    {
      "issue_number": 1425,
      "title": "lm-eval pip package version issue",
      "body": "### 🐛 Describe the bug\n\nI tried to install torchchat on a new machine, w/ a new clone, and following instructions ran into this issue.\r\n\r\n```\r\n$ git describe --all --long\r\nheads/main-0-g56be609\r\n[...]\r\n\r\n$ ./install/install_requirements.sh # in venv\r\n\r\n[...]\r\n\r\nINFO: pip is looking at multiple versions of lm-eval to determine which version is compatible with other requirements. This could take a while.\r\nERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5; 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.26.0 Requires-Python <3.13,>=3.9; 1.26.1 Requires-Python <3.13,>=3.9\r\nERROR: Could not find a version that satisfies the requirement torch>=1.8 (from lm-eval) (from versions: none)\r\nERROR: No matching distribution found for torch>=1.8\r\n```\r\n\r\nCommenting this package out works.\r\n```\r\ngit diff\r\ndiff --git a/install/requirements.txt b/install/requirements.txt\r\nindex 8fb1832..993c966 100644\r\n--- a/install/requirements.txt\r\n+++ b/install/requirements.txt\r\n@@ -31,4 +31,4 @@ streamlit\r\n flask\r\n\r\n # eval\r\n-lm_eval==0.4.2\r\n+# lm_eval==0.4.2\r\n```\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241213\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 15.1 (arm64)\r\nGCC version: Could not collect\r\nClang version: 16.0.0 (clang-1600.0.26.4)\r\nCMake version: version 3.31.2\r\nLibc version: N/A\r\n\r\nPython version: 3.13.1 (main, Dec  3 2024, 17:59:52) [Clang 16.0.0 (clang-1600.0.26.4)] (64-bit runtime)\r\nPython platform: macOS-15.1-arm64-arm-64bit-Mach-O\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M4 Pro\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.6.0.dev20241213\r\n[pip3] torchao==0.8.0+git2f97b095\r\n[pip3] torchtune==0.5.0.dev20241126+cpu\r\n[pip3] torchvision==0.22.0.dev20241213\r\n[conda] Could not collect",
      "state": "closed",
      "author": "digantdesai",
      "author_type": "User",
      "created_at": "2024-12-17T18:20:39Z",
      "updated_at": "2025-02-04T20:30:43Z",
      "closed_at": "2025-02-04T20:30:42Z",
      "labels": [
        "bug",
        "Triage review",
        "triaged",
        "Evaluation/Benchmarking"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1425/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1425",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1425",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:10.002914",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Appears to be fixed in main",
          "created_at": "2025-02-04T20:30:42Z"
        }
      ]
    },
    {
      "issue_number": 932,
      "title": "Update CLI arg builders to check for only args that the subcommand uses: Export/Generate",
      "body": "### 🚀 The feature, motivation and pitch\n\nCurrently, the CLI arg parsing in torchchat is too general and overzealous; subcommands indirectly access cli args that it doesn't actually use (e.g. [BuilderArgs](https://github.com/pytorch/torchchat/blob/main/build/builder.py#L31)). This results in supporting an extremely bloated [argparser](https://github.com/pytorch/torchchat/blob/main/cli.py)\r\n\r\nThe fix is simple, either update the parsing to \r\n* not attempt to access fields that a subcommand doesn't directly need\r\n* not crash the execution when the unused field isn't provided\r\n\r\n---\r\nThis issue is specific to the Export and Generate Subcommands\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-07-19T19:24:59Z",
      "updated_at": "2025-02-04T18:58:13Z",
      "closed_at": "2025-02-04T18:57:58Z",
      "labels": [
        "actionable",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/932/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/932",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/932",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:10.224629",
      "comments": []
    },
    {
      "issue_number": 938,
      "title": "Improve the scope of Model Evaluation to AOTI and ET",
      "body": "### 🚀 The feature, motivation and pitch\n\nCurrently, model evaluation is a WIP and mostly focused on pure PyTorch and compile. \r\nThis is planned work to improve PT support and expand to evaluation for AOTI and ET as well\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-07-22T16:27:07Z",
      "updated_at": "2025-02-04T18:47:49Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Known Gaps",
        "triaged",
        "Evaluation/Benchmarking"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/938/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/938",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/938",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:10.224650",
      "comments": []
    },
    {
      "issue_number": 973,
      "title": "Open AI API Maturity",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\nThe OpenAI API support in torchchat is actively in development and will be one of the main entry point for interacting with torchchat\r\n\r\nhttps://github.com/pytorch/torchchat?tab=readme-ov-file#server\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\r\n\r\n### RFC (Optional)\r\n\r\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-07-30T16:46:26Z",
      "updated_at": "2025-02-04T18:42:11Z",
      "closed_at": "2025-02-04T18:42:11Z",
      "labels": [
        "enhancement",
        "Known Gaps",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/973/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/973",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/973",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:10.224657",
      "comments": [
        {
          "author": "vmpuri",
          "body": "[PR 995](https://github.com/pytorch/torchchat/pull/995) addresses some initial concerns. Responses should now be formatted in JSON using the API dataclasses. Here are our functional gaps so far:\r\n\r\n- **system_fingerprint/seed** implementation is incomplete. We need to figure out a good way to create",
          "created_at": "2024-08-02T20:40:22Z"
        },
        {
          "author": "ywang96",
          "body": "Hello there! I'm curious if supporting OpenAI Vision API is also something you're interested in since it looks like the team does plan to support vision language models from the discussion in #988\r\n\r\nIf you do, happy to help!",
          "created_at": "2024-08-04T07:24:39Z"
        },
        {
          "author": "vmpuri",
          "body": "Hey Roger, thanks for reaching out! We do plan on adding LLaVA support in the coming weeks, and we'd appreciate your help on the API/server components. \r\n\r\nI'm still working on bringing the initial version of the API and server up to spec, but I'll keep this issue updated with my progress.",
          "created_at": "2024-08-06T00:18:05Z"
        },
        {
          "author": "vmpuri",
          "body": "Landed pulls #1035 #1034 #1042  which prove that the basic completion API works as expected with the Python OpenAI API.",
          "created_at": "2024-08-19T22:35:09Z"
        }
      ]
    },
    {
      "issue_number": 978,
      "title": "Android App Should Crash gracefully when the tokenizer in the aar doesn't match the model",
      "body": "### 🚀 The feature, motivation and pitch\n\nA crash is encountered when the tokenizer in the Android .aar doesn't line up with the tokenizer needed by the model.\r\nWe should instead handle this crash gracefully with a message guiding users to switch their aar.\r\n\r\nExample PR: https://github.com/pytorch/torchchat/issues/977 (@iceychris)\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-07-30T22:50:45Z",
      "updated_at": "2025-02-04T18:23:18Z",
      "closed_at": null,
      "labels": [
        "actionable",
        "Mobile - Android",
        "ExecuTorch",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/978/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/978",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/978",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:10.486037",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "This might be best fixed in the ExecuTorch repo\r\nWe should coordinate with the folks over there\r\n\r\ncc: @larryliu0820 ",
          "created_at": "2024-07-31T17:28:57Z"
        }
      ]
    },
    {
      "issue_number": 1041,
      "title": "Improve support for and documentation of custom models",
      "body": "### 🚀 The feature, motivation and pitch\n\ntorchchat supports adding models to the \"known_model\" list and has CLI support for local models not hosted in torchchat's, but this can be better documented. \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\nSome PR's Related to this theme:\r\n* https://github.com/pytorch/torchchat/issues/1038 \r\n* https://github.com/pytorch/torchchat/issues/1040\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-08-19T16:43:48Z",
      "updated_at": "2025-02-04T18:22:48Z",
      "closed_at": "2025-02-04T18:22:34Z",
      "labels": [
        "documentation",
        "enhancement",
        "Known Gaps",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1041/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1041",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1041",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:10.741478",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "https://github.com/pytorch/torchchat/blob/main/docs/local-model.md\n\nMore work to come this half, closing issue ",
          "created_at": "2025-02-04T18:22:34Z"
        }
      ]
    },
    {
      "issue_number": 1058,
      "title": "Slimming down torchchat: Replace replace_attention_with_custom_sdpa_attention() with ET's implementation",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\nFirst surfaced in https://github.com/pytorch/torchchat/pull/1057, the `replace_attention_with_custom_sdpa_attention` function, used when exporting models in torchchat, can be replaced with the equivalent API provided in the Excecutorch https://github.com/pytorch/executorch/blob/main/examples/models/llama2/source_transformation/sdpa.py\r\n\r\n**Task**: Swap the torchchat implementation with that of ExecuTorch's. Delete the then defunct code from torchchat\r\n\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\r\n\r\n### RFC (Optional)\r\n\r\n_No response_",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-08-23T23:30:53Z",
      "updated_at": "2025-02-04T18:20:48Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue",
        "ExecuTorch",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1058/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1058",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1058",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:12.909787",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "I think #1057 resolved this.  Can we close?",
          "created_at": "2024-11-08T03:03:09Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Not quite, #1057 was the Pr the flagged it\r\n\r\nShould be easy PR, just needs testing",
          "created_at": "2024-11-09T01:43:09Z"
        }
      ]
    },
    {
      "issue_number": 1315,
      "title": "False Positive on CI tests: test-readme",
      "body": "### 🐛 Describe the bug\n\nIn CI, there are a few tests that should be flagged as failing, but are currently marked as green. \r\n\r\nSpecifically they seem to revolve around the test-readme unittests: see surfacing PR for examples (https://github.com/pytorch/torchchat/pull/1309)\r\n\r\n* https://github.com/pytorch/torchchat/actions/runs/11375390717/job/31652547554?pr=1309\r\n* https://github.com/pytorch/torchchat/actions/runs/11375390708/job/31652543330?pr=1309\n\n### Versions\n\nNA",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-10-18T20:31:51Z",
      "updated_at": "2025-02-04T18:16:53Z",
      "closed_at": null,
      "labels": [
        "bug",
        "actionable",
        "CI Infra",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1315/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1315",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1315",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:13.174366",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "@seemethere @malfet @kit1980 can you please have a look as to why these tests are not flagged as failing? \r\n\r\nditto, from yesterday - https://github.com/pytorch/torchchat/actions/runs/11634438558/job/32413340991?pr=1339 is shown as passed even though commands in the test failed and aborted with an e",
          "created_at": "2024-11-05T18:22:39Z"
        },
        {
          "author": "mikekgfb",
          "body": "And still happening.\r\n\r\nOne possible explanation might be that somewhere the code is catching the exception, pretty printing it, and then exit with a non-error code because the fails such as https://github.com/pytorch/torchchat/actions/runs/12243820522/job/34154220414?pr=1404 show that the code cont",
          "created_at": "2024-12-11T04:09:39Z"
        }
      ]
    },
    {
      "issue_number": 1321,
      "title": "Bump numpy version in torchchat to match that of PyTorch",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\nCurrently torchchat uses a restricted numpy version https://github.com/pytorch/torchchat/blob/0e37b9ad903bf26d3812d5f58f50cf46427b0e2f/install/requirements.txt#L16 due to a dependency on gguf-py (https://github.com/ggerganov/llama.cpp/blob/master/gguf-py/pyproject.toml).\r\n\r\nWe should move to matching the same numpy requirements as pytorch/pytorch. This allows for better continuity across repos. \r\n\r\nhttps://github.com/pytorch/pytorch/blame/main/requirements.txt#L5\r\n\r\nTo do so, we must first bump the gguf-py requirements, then bump the torchchat requirements\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\r\n\r\n### RFC (Optional)\r\n\r\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-10-21T17:16:24Z",
      "updated_at": "2025-02-04T18:16:20Z",
      "closed_at": "2025-02-04T18:16:18Z",
      "labels": [
        "enhancement",
        "Known Gaps",
        "actionable"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1321/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1321",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1321",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:13.382091",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Related initial attempt: https://github.com/pytorch/torchchat/issues/1296\r\n* reverted due to GGUF dep",
          "created_at": "2024-10-21T17:19:28Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "https://github.com/pytorch/torchchat/pull/1479",
          "created_at": "2025-02-04T18:16:18Z"
        }
      ]
    },
    {
      "issue_number": 1483,
      "title": "Model execution runs excruciatingly slow/doesn't run at all",
      "body": "### 🐛 Describe the bug\n\nWhen running any inferences, the model doesn't load anything except maybe the first line. I've tried using Ollama and everything runs instantaneously, but I haven't gotten anything out of running any model on torchchat. Any idea why this is happening?\n\n```bash\n(.venv) (base) jakemalis@Jakes-MacBook-Pro torchchat % python3 torchchat.py generate llama3.1 --prompt \"What's your favorite color?\"              \nDownloading meta-llama/Meta-Llama-3.1-8B-Instruct from HuggingFace...\noriginal/params.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 1.91MB/s]\nconfig.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 855/855 [00:00<00:00, 1.96MB/s]\n.gitattributes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 1.65MB/s]\ngeneration_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184/184 [00:00<00:00, 245kB/s]\nREADME.md: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.0k/44.0k [00:00<00:00, 3.38MB/s]\nUSE_POLICY.md: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.69k/4.69k [00:00<00:00, 15.7MB/s]\nLICENSE: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.63k/7.63k [00:00<00:00, 49.4MB/s]\nspecial_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 3.53MB/s]\ntokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55.4k/55.4k [00:00<00:00, 3.53MB/s]\ntokenizer.model: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.18M/2.18M [00:00<00:00, 3.05MB/s]\ntokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:02<00:00, 3.63MB/s]\nconsolidated.00.pth: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16.1G/16.1G [08:07<00:00, 32.9MB/s]\nFetching 12 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [08:07<00:00, 40.66s/it]\nConverting meta-llama/Meta-Llama-3.1-8B-Instruct to torchchat format...███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:02<00:00, 3.64MB/s]NumExpr defaulting to 10 threads.\nPyTorch version 2.7.0.dev20250124 available.\nWarning: PTEModel (ExecuTorch) not available with exception: No module named 'executorch'\nknown configs: ['llava-1.5', '13B', '70B', 'CodeLlama-7b-Python-hf', 'Meta-Llama-3.1-70B-Tune', 'Granite-3B-Code', '34B', 'Meta-Llama-3.1-8B', 'stories42M', 'Llama-Guard-3-1B', '30B', 'Meta-Llama-3.1-8B-Tune', 'stories110M', 'Granite-3.1-8B-Instruct', 'Llama-3.2-11B-Vision', 'Meta-Llama-3.2-3B', 'Meta-Llama-3.1-70B', 'Meta-Llama-3.2-1B', 'Granite-3.0-2B-Instruct', 'Granite-3.0-8B-Instruct', '7B', 'stories15M', 'Llama-Guard-3-1B-INT4', 'Mistral-7B', 'Granite-8B-Code', 'Meta-Llama-3-70B', 'Granite-3.1-2B-Instruct', 'Meta-Llama-3-8B']\nModel config {'block_size': 131072, 'vocab_size': 128256, 'n_layers': 32, 'n_heads': 32, 'dim': 4096, 'hidden_dim': 14336, 'n_local_heads': 8, 'head_dim': 128, 'rope_base': 500000.0, 'norm_eps': 1e-05, 'multiple_of': 1024, 'ffn_dim_multiplier': 1.3, 'use_tiktoken': True, 'use_hf_tokenizer': False, 'tokenizer_prepend_bos': True, 'max_seq_length': 8192, 'rope_scaling': {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192}, 'n_stages': 1, 'stage_idx': 0, 'attention_bias': False, 'feed_forward_bias': False, 'tie_word_embeddings': False, 'embedding_multiplier': None, 'attention_multiplier': None, 'residual_multiplier': None, 'logits_scaling': None}\nMoving checkpoint to /Users/jakemalis/.torchchat/model-cache/downloads/meta-llama/Meta-Llama-3.1-8B-Instruct/model.pth.\nDone.\nMoving model to /Users/jakemalis/.torchchat/model-cache/meta-llama/Meta-Llama-3.1-8B-Instruct.\nUnable to import torchao experimental quant_api with error:  [Errno 2] No such file or directory: '/Users/jakemalis/Downloads/torchchat/torchao-build/src/ao/torchao/experimental/quant_api.py'\nUsing device=mps \nLoading model...\nTime to load model: 23.71 seconds\n-----------------------------------------------------------\nWhat's your favorite color? - A simple\n```\n\n### Versions\n\n```bash\nCMakeLists.txt\t\tCONTRIBUTING.md\t\tREADME.md\t\tcollect_env.py\t\tdocs\t\t\trunner\t\t\ttokenizer\t\ttorchchat.py\nCODE_OF_CONDUCT.md\tLICENSE\t\t\tassets\t\t\tdist_run.py\t\tinstall\t\t\ttests\t\t\ttorchchat\n(.venv) (base) jakemalis@Jakes-MacBook-Pro torchchat % python collect_env.py\nCollecting environment information...\nPyTorch version: 2.7.0.dev20250124\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: macOS 15.3 (arm64)\nGCC version: Could not collect\nClang version: 16.0.0 (clang-1600.0.26.6)\nCMake version: version 3.31.4\nLibc version: N/A\n\nPython version: 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ] (64-bit runtime)\nPython platform: macOS-15.3-arm64-arm-64bit\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nApple M1 Pro\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.2\n[pip3] torch==2.7.0.dev20250124\n[pip3] torchao==0.8.0+git2f97b095\n[pip3] torchtune==0.6.0.dev20250124+cpu\n[pip3] torchvision==0.22.0.dev20250124\n[conda] numpy                     2.2.1                    pypi_0    pypi\n[conda] numpy-base                2.1.3           py312he047099_0  \n[conda] pytorch                   2.6.0.dev20241112        py3.12_0    pytorch-nightly\n[conda] torchaudio                2.5.0.dev20241118       py312_cpu    pytorch-nightly\n[conda] torchvision               0.20.0.dev20241118       py312_cpu    pytorch-nightly\n```",
      "state": "closed",
      "author": "JakeMalis",
      "author_type": "User",
      "created_at": "2025-01-28T16:35:15Z",
      "updated_at": "2025-02-01T00:09:04Z",
      "closed_at": "2025-01-29T14:27:35Z",
      "labels": [
        "performance",
        "MPS/Metal",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1483/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1483",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1483",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:13.637144",
      "comments": [
        {
          "author": "manuelcandales",
          "body": "You are running the 8B model with no quantization. I think this is happening because of high Memory Pressure. How much RAM does your M1 Pro have?\nThe same thing happens to me on an M1 Pro with 16GB of RAM, even when nothing else is open at the same time. And it also happens to me on an M1 Pro with 3",
          "created_at": "2025-01-28T17:38:32Z"
        },
        {
          "author": "manuelcandales",
          "body": "cc: @malfet @swolchok this user seems to be running into the same issue I was talking about.",
          "created_at": "2025-01-28T17:41:04Z"
        },
        {
          "author": "swolchok",
          "body": "> How much RAM does your M1 Pro have?\n> Have you tried running the 1B model?\n\nif the same model works with another runtime, then the presumption should be that the problem is on our end.",
          "created_at": "2025-01-28T18:04:39Z"
        },
        {
          "author": "swolchok",
          "body": "oh, I see -- ollama uses quantization by default. I ran llama3.2 and llama3.1 and sure enough it was llama.cpp's Q4_K_M. Then yeah ollama's default settings don't seem to be an apples-to-apples comparison with torchchat's default settings as they currently stand.",
          "created_at": "2025-01-28T18:12:42Z"
        },
        {
          "author": "mikekgfb",
          "body": "#1465 adds the ability to save and load snapshots, so we can have prequantized models as well, and users don't have to start up and pay for quantization every time.  That being said, redistributing quantized snapshots is something that Meta employees can do without detailed review, but third parties",
          "created_at": "2025-01-28T19:04:05Z"
        }
      ]
    },
    {
      "issue_number": 1472,
      "title": "[Easy?] Numpy Version Pin Bump: == 2.0",
      "body": "### 🐛 Describe the bug\n\nExecuTorch recently bumped their numpy requirements to numpy == 2.0 in https://github.com/pytorch/executorch/commit/a7b5297f95101c32b9e45fd1ee3e5dfb4a00c96b\nThis puts torchchat in a finicky spot since the current requirements are tied to under 2.0 due to GGUF support requiring < 2.0 (see blame for previous attempts)\nhttps://github.com/pytorch/torchchat/blob/fb65b8bbc405066162fbae1b060eb507687ebabf/install/requirements.txt#L19-L20\n\nWhile not actively an issue, as soon as an ExecuTorch pin bump is required, this will become a hard blocker.\n\n**Task:** Make a requirements version pinbump ~~`numpy >= 2.0`~~ `numpy > 1.17`\n* Considerations:\n  * **1/23**: This may \"just work\" without additional effort; Seems like llama.cpp may have fixed this in December (https://github.com/ggerganov/llama.cpp/pull/9772)\n  * Is updating the GGUF support within torchchat's control (i.e. propogating dependencies)? \nIf not who/what needs coercing?\n  * Should be icebox GGUF support?\n\n### Versions\n\nN/A",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-01-22T23:28:07Z",
      "updated_at": "2025-01-24T21:54:41Z",
      "closed_at": "2025-01-24T21:54:41Z",
      "labels": [
        "good first issue",
        "Known Gaps",
        "actionable",
        "ExecuTorch",
        "triaged"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1472/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1472",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1472",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:13.892873",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "fyi: @Gasoonjia, @vmpuri, @mergennachin (ET)",
          "created_at": "2025-01-22T23:34:22Z"
        },
        {
          "author": "mergennachin",
          "body": "gguf-util says >=1.17 but not explicitly <2, so in theory numpy>=2.0 should work\n\nhttps://github.com/ggerganov/llama.cpp/blame/master/gguf-py/pyproject.toml\n\nwhere does it fail when the version is 2 or above? \n\nlooks like this is the job -- perhaps you could repro it\n\nhttps://github.com/pytorch/torc",
          "created_at": "2025-01-23T16:26:39Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "> where does it fail when the version is 2 or above?\n\nIt was implicit dependency in llama.cpp; the project was using interfaces that were deprecated in numpy 2+\nThat said looks like it got fixed in December (https://github.com/ggerganov/llama.cpp/pull/9772)\n\nIf we're able to just drop it and things ",
          "created_at": "2025-01-23T16:54:21Z"
        },
        {
          "author": "mergennachin",
          "body": "yeah, let's just try updating this commit \n\nhttps://github.com/pytorch/torchchat/blob/main/.github/workflows/pull.yml#L729",
          "created_at": "2025-01-23T17:01:43Z"
        }
      ]
    },
    {
      "issue_number": 985,
      "title": "`scripts/build_native.sh et` errors out",
      "body": "### 🐛 Describe the bug\n\nI am trying to build the llama runner natively on a rasperry pi following the torchchat description, and the post at https://dev-discuss.pytorch.org/t/run-llama3-8b-on-a-raspberry-pi-5-with-executorch/2048\r\n\r\nI was able to build executorch and torchchat so far (I can build a pte and run it with the python driver), but ran into an error with `scripts/build_native et`:\r\n\r\n  [ 22%] Performing download step (git clone) for 'fxdiv'\r\n  Cloning into 'FXdiv-source'...\r\n  Already on 'master'\r\n  Your branch is up to date with 'origin/master'.\r\n  [ 33%] Performing update step for 'fxdiv'\r\n  -- Fetching latest from the remote origin\r\n  [ 44%] No patch step for 'fxdiv'\r\n  [ 55%] No configure step for 'fxdiv'\r\n  [ 66%] No build step for 'fxdiv'\r\n  [ 77%] No install step for 'fxdiv'\r\n  [ 88%] No test step for 'fxdiv'\r\n  [100%] Completed 'fxdiv'\r\n  [100%] Built target fxdiv\r\n  -- Using python executable '/home/sunshine/pt2/bin/python3'\r\n  -- Resolved buck2 as /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/buck2-bin/buck2-49670bee56a7d8a7696409ca6fbf7551d2469787.\r\n  -- Killing buck2 daemon\r\n  -- executorch: Generating source lists\r\n  -- executorch: Generating source file list /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/executorch_srcs.cmake\r\n  Error while generating /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/executorch_srcs.cmake. Exit code: 1\r\n  Output:\r\n\r\n  Error:\r\n  Traceback (most recent call last):\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/buck_util.py\", line 26, in run\r\n      cp: subprocess.CompletedProcess = subprocess.run(\r\n                                        ^^^^^^^^^^^^^^^\r\n    File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\r\n      raise CalledProcessError(retcode, process.args,\r\n  subprocess.CalledProcessError: Command '['/home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/buck2-bin/buck2-49670bee56a7d8a7696409ca6fbf7551d2469787', 'cquery', \"inputs(deps('//runtime/executor:program'))\"]' returned non-zero exit status 2.\r\n\r\n  The above exception was the direct cause of the following exception:\r\n\r\n  Traceback (most recent call last):\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/extract_sources.py\", line 218, in <module>\r\n      main()\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/extract_sources.py\", line 203, in main\r\n      target_to_srcs[name] = sorted(target.get_sources(graph, runner))\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/extract_sources.py\", line 116, in get_sources\r\n      sources: set[str] = set(runner.run([\"cquery\", query]))\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/buck_util.py\", line 31, in run\r\n      raise RuntimeError(ex.stderr.decode(\"utf-8\")) from ex\r\n  RuntimeError: Command failed:\r\n  Error validating working directory\r\n\r\n  Caused by:\r\n      0: Failed to stat `/home/sunshine/torchchat/et-build/src/executorch/buck-out/v2`\r\n      1: ENOENT: No such file or directory\r\n\r\n\r\n  CMake Error at build/Utils.cmake:191 (message):\r\n    executorch: source list generation failed\r\n  Call Stack (most recent call first):\r\n    CMakeLists.txt:327 (extract_sources)\r\n\r\n\r\n  -- Configuring incomplete, errors occurred!\r\n  error: command '/home/sunshine/pt2/bin/cmake' failed with exit code 1\r\n  error: subprocess-exited-with-error\r\n  \r\n  Building wheel for executorch (pyproject.toml) did not run successfully.\r\n  exit code: 1\r\n  \r\n  See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /home/sunshine/pt2/bin/python3 /home/sunshine/pt2/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmpvxjhev2t\r\n  cwd: /home/sunshine/torchchat/et-build/src/executorch\r\n  Building wheel for executorch (pyproject.toml) ... error\r\n  ERROR: Failed building wheel for executorch\r\nFailed to build executorch\r\nERROR: Could not build wheels for executorch, which is required to install pyproject.toml-based projects\r\n\r\n\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240716\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux trixie/sid (aarch64)\r\nGCC version: (Debian 13.3.0-2) 13.3.0\r\nClang version: 16.0.6 (27+b1)\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.38\r\n\r\nPython version: 3.11.2 (main, May  2 2024, 11:59:08) [GCC 12.2.0] (64-bit runtime)\r\nPython platform: Linux-6.6.31+rpt-rpi-v8-aarch64-with-glibc2.38\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         aarch64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nCPU(s):                               4\r\nOn-line CPU(s) list:                  0-3\r\nVendor ID:                            ARM\r\nModel name:                           Cortex-A76\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per cluster:                  4\r\nSocket(s):                            -\r\nCluster(s):                           1\r\nStepping:                             r4p1\r\nCPU(s) scaling MHz:                   100%\r\nCPU max MHz:                          2400.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             108.00\r\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nL1d cache:                            256 KiB (4 instances)\r\nL1i cache:                            256 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             2 MiB (1 instance)\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; CSV2, BHB\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+c757499\r\n[pip3] numpy==1.26.3\r\n[pip3] torch==2.5.0.dev20240716\r\n[pip3] torchao==0.3.1\r\n[pip3] torchaudio==2.4.0.dev20240716\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchvision==0.20.0.dev20240716\r\n[conda] Could not collect\r\n",
      "state": "closed",
      "author": "sunshinesfbay",
      "author_type": "User",
      "created_at": "2024-07-31T20:00:11Z",
      "updated_at": "2025-01-24T21:29:14Z",
      "closed_at": "2025-01-24T21:29:14Z",
      "labels": [
        "bug",
        "need-user-input",
        "ExecuTorch"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/985/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/985",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/985",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:14.091836",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "@sunshinesfbay Thanks for testing out the repo\r\n\r\nTo help us with getting a repro can you share what commands you ran between `git clone ...` up to `scripts/build_native.sh et`? \r\n\r\nI can loop in specific  ExecuTorch folk afterwards",
          "created_at": "2024-07-31T21:57:59Z"
        },
        {
          "author": "WaelShaikh",
          "body": "I'm getting a similar error stack trace in #990 \r\nI followed the instructions in the readme for Android and I got this error when running:\r\n`./scripts/install_et.sh`",
          "created_at": "2024-08-01T21:26:15Z"
        },
        {
          "author": "sunshinesfbay",
          "body": "> @sunshinesfbay Thanks for testing out the repo\r\n> \r\n> To help us with getting a repro can you share what commands you ran between `git clone ...` up to `scripts/build_native.sh et`?\r\n> \r\n> I can loop in specific ExecuTorch folk afterwards\r\n\r\nHere are the commands\r\n\r\n```\r\n   92  git clone https://g",
          "created_at": "2024-08-01T21:59:50Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "The erorr isn't reproing in my side, so we'll need to keep digging\r\n\r\n2 Comments: though they may not be directly tied to the build_native.sh error\r\n\r\n1) Did this command succeed? You should have received an ET EXPORT EXCEPTION error since none of the listed commands installed ExecuTorch\r\n    > pyth",
          "created_at": "2024-08-02T08:00:53Z"
        },
        {
          "author": "sunshinesfbay",
          "body": "> The erorr isn't reproing in my side, so we'll need to keep digging\r\n> \r\n\r\nJust to be sure, you did try on a Raspberry Pi5 with Raspbian, not another system?  This is trying to run everything natively on the Raspberry Pi -- pytorch, executorch, torchchat.\r\n\r\nHere's the Raspbian version  I used:\r\n\r\n",
          "created_at": "2024-08-03T01:19:51Z"
        }
      ]
    },
    {
      "issue_number": 1173,
      "title": "install requirements fails",
      "body": "### 🐛 Describe the bug\r\n\r\n\r\n```\r\n(pt) sunshine@raspberrypi:~/torchchat $ ./install/install_requirements.sh\r\n+ pip3 install -r install/requirements.txt --extra-index-url https://download.pytorch.org/whl/nightly/cu121\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple, https://download.pytorch.org/whl/nightly/cu121\r\nIgnoring tomli: markers 'python_version < \"3.11\"' don't match your environment\r\nRequirement already satisfied: huggingface_hub in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 4)) (0.24.6)\r\nRequirement already satisfied: gguf in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 7)) (0.10.0)\r\nRequirement already satisfied: tiktoken in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 10)) (0.7.0)\r\nRequirement already satisfied: snakeviz in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 13)) (2.2.0)\r\nRequirement already satisfied: sentencepiece in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 14)) (0.2.0)\r\nRequirement already satisfied: numpy<2.0 in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 15)) (1.26.4)\r\nRequirement already satisfied: lm-eval==0.4.2 in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: blobfile in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 18)) (3.0.0)\r\nRequirement already satisfied: openai in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 20)) (1.44.0)\r\nRequirement already satisfied: wheel in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 23)) (0.44.0)\r\nRequirement already satisfied: cmake>=3.24 in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 24)) (3.30.2)\r\nRequirement already satisfied: ninja in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 25)) (1.11.1.1)\r\nRequirement already satisfied: zstd in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 26)) (1.5.5.1)\r\nRequirement already satisfied: streamlit in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 29)) (1.38.0)\r\nRequirement already satisfied: flask in /home/sunshine/pt/lib/python3.11/site-packages (from -r install/requirements.txt (line 32)) (3.0.3)\r\nRequirement already satisfied: accelerate>=0.21.0 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.34.2)\r\nRequirement already satisfied: evaluate in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: datasets>=2.16.0 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.21.0)\r\nRequirement already satisfied: jsonlines in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.0.0)\r\nRequirement already satisfied: numexpr in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: peft>=0.2.0 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.12.0)\r\nRequirement already satisfied: pybind11>=2.6.2 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.13.5)\r\nRequirement already satisfied: pytablewriter in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.2.0)\r\nRequirement already satisfied: rouge-score>=0.0.4 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.2)\r\nRequirement already satisfied: sacrebleu>=1.5.0 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.3)\r\nRequirement already satisfied: scikit-learn>=0.24.1 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.5.1)\r\nRequirement already satisfied: sqlitedict in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: torch>=1.8 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.5.0.dev20240716)\r\nRequirement already satisfied: tqdm-multiprocess in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.0.11)\r\nRequirement already satisfied: transformers>=4.1 in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.42.4)\r\nRequirement already satisfied: zstandard in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.23.0)\r\nRequirement already satisfied: dill in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.3.8)\r\nRequirement already satisfied: word2number in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1)\r\nRequirement already satisfied: more-itertools in /home/sunshine/pt/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (10.5.0)\r\nRequirement already satisfied: filelock in /home/sunshine/pt/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (3.15.4)\r\nRequirement already satisfied: fsspec>=2023.5.0 in /home/sunshine/pt/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2024.6.1)\r\nRequirement already satisfied: packaging>=20.9 in /home/sunshine/pt/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /home/sunshine/pt/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (6.0.2)\r\nRequirement already satisfied: requests in /home/sunshine/pt/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2.32.3)\r\nRequirement already satisfied: tqdm>=4.42.1 in /home/sunshine/pt/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.66.5)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/sunshine/pt/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.12.2)\r\nRequirement already satisfied: regex>=2022.1.18 in /home/sunshine/pt/lib/python3.11/site-packages (from tiktoken->-r install/requirements.txt (line 10)) (2024.7.24)\r\nRequirement already satisfied: tornado>=2.0 in /home/sunshine/pt/lib/python3.11/site-packages (from snakeviz->-r install/requirements.txt (line 13)) (6.4.1)\r\nRequirement already satisfied: pycryptodomex>=3.8 in /home/sunshine/pt/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (3.20.0)\r\nRequirement already satisfied: urllib3<3,>=1.25.3 in /home/sunshine/pt/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (2.2.2)\r\nRequirement already satisfied: lxml>=4.9 in /home/sunshine/pt/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (5.3.0)\r\nRequirement already satisfied: anyio<5,>=3.5.0 in /home/sunshine/pt/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (4.4.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in /home/sunshine/pt/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.9.0)\r\nRequirement already satisfied: httpx<1,>=0.23.0 in /home/sunshine/pt/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.27.2)\r\nRequirement already satisfied: jiter<1,>=0.4.0 in /home/sunshine/pt/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.5.0)\r\nRequirement already satisfied: pydantic<3,>=1.9.0 in /home/sunshine/pt/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (2.9.0)\r\nRequirement already satisfied: sniffio in /home/sunshine/pt/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.3.1)\r\nRequirement already satisfied: altair<6,>=4.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.4.1)\r\nRequirement already satisfied: blinker<2,>=1.0.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (1.8.2)\r\nRequirement already satisfied: cachetools<6,>=4.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.5.0)\r\nRequirement already satisfied: click<9,>=7.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.1.7)\r\nRequirement already satisfied: pandas<3,>=1.3.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (2.2.2)\r\nRequirement already satisfied: pillow<11,>=7.1.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (10.4.0)\r\nRequirement already satisfied: protobuf<6,>=3.20 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.28.0)\r\nRequirement already satisfied: pyarrow>=7.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (17.0.0)\r\nRequirement already satisfied: rich<14,>=10.14.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (13.8.0)\r\nRequirement already satisfied: tenacity<9,>=8.1.0 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.5.0)\r\nRequirement already satisfied: toml<2,>=0.10.1 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.10.2)\r\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (3.1.43)\r\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.9.1)\r\nRequirement already satisfied: watchdog<5,>=2.1.5 in /home/sunshine/pt/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (4.0.2)\r\nRequirement already satisfied: Werkzeug>=3.0.0 in /home/sunshine/pt/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.0.4)\r\nRequirement already satisfied: Jinja2>=3.1.2 in /home/sunshine/pt/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.1.4)\r\nRequirement already satisfied: itsdangerous>=2.1.2 in /home/sunshine/pt/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (2.2.0)\r\nRequirement already satisfied: psutil in /home/sunshine/pt/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.0)\r\nRequirement already satisfied: safetensors>=0.4.3 in /home/sunshine/pt/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.5)\r\nRequirement already satisfied: jsonschema>=3.0 in /home/sunshine/pt/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (4.23.0)\r\nRequirement already satisfied: narwhals>=1.5.2 in /home/sunshine/pt/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (1.6.2)\r\nRequirement already satisfied: idna>=2.8 in /home/sunshine/pt/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r install/requirements.txt (line 20)) (3.8)\r\nRequirement already satisfied: xxhash in /home/sunshine/pt/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: multiprocess in /home/sunshine/pt/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.70.16)\r\nRequirement already satisfied: aiohttp in /home/sunshine/pt/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.10.5)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in /home/sunshine/pt/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (4.0.11)\r\nRequirement already satisfied: certifi in /home/sunshine/pt/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (2024.8.30)\r\nRequirement already satisfied: httpcore==1.* in /home/sunshine/pt/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (1.0.5)\r\nRequirement already satisfied: h11<0.15,>=0.13 in /home/sunshine/pt/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (0.14.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /home/sunshine/pt/lib/python3.11/site-packages (from Jinja2>=3.1.2->flask->-r install/requirements.txt (line 32)) (2.1.5)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/sunshine/pt/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /home/sunshine/pt/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.7 in /home/sunshine/pt/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: annotated-types>=0.4.0 in /home/sunshine/pt/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (0.7.0)\r\nRequirement already satisfied: pydantic-core==2.23.2 in /home/sunshine/pt/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (2.23.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/sunshine/pt/lib/python3.11/site-packages (from requests->huggingface_hub->-r install/requirements.txt (line 4)) (3.3.2)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in /home/sunshine/pt/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sunshine/pt/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (2.18.0)\r\nRequirement already satisfied: absl-py in /home/sunshine/pt/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: nltk in /home/sunshine/pt/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.9.1)\r\nRequirement already satisfied: six>=1.14.0 in /home/sunshine/pt/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.16.0)\r\nRequirement already satisfied: portalocker in /home/sunshine/pt/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: tabulate>=0.8.9 in /home/sunshine/pt/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.9.0)\r\nRequirement already satisfied: colorama in /home/sunshine/pt/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.6)\r\nRequirement already satisfied: scipy>=1.6.0 in /home/sunshine/pt/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.14.1)\r\nRequirement already satisfied: joblib>=1.2.0 in /home/sunshine/pt/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in /home/sunshine/pt/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: sympy in /home/sunshine/pt/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.13.1)\r\nRequirement already satisfied: networkx in /home/sunshine/pt/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.3)\r\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /home/sunshine/pt/lib/python3.11/site-packages (from transformers>=4.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.19.1)\r\nRequirement already satisfied: attrs>=19.2.0 in /home/sunshine/pt/lib/python3.11/site-packages (from jsonlines->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (24.2.0)\r\nRequirement already satisfied: setuptools>=38.3.0 in /home/sunshine/pt/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (70.3.0)\r\nRequirement already satisfied: DataProperty<2,>=1.0.1 in /home/sunshine/pt/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.0.1)\r\nRequirement already satisfied: mbstrdecoder<2,>=1.0.0 in /home/sunshine/pt/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1.3)\r\nRequirement already satisfied: pathvalidate<4,>=2.3.0 in /home/sunshine/pt/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.2.1)\r\nRequirement already satisfied: tabledata<2,>=1.3.1 in /home/sunshine/pt/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.3)\r\nRequirement already satisfied: tcolorpy<1,>=0.0.5 in /home/sunshine/pt/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.6)\r\nRequirement already satisfied: typepy<2,>=1.3.2 in /home/sunshine/pt/lib/python3.11/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.2)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sunshine/pt/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.0)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /home/sunshine/pt/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.1)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /home/sunshine/pt/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/sunshine/pt/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.5)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/sunshine/pt/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.10.0)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in /home/sunshine/pt/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (5.0.1)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/sunshine/pt/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (2023.12.1)\r\nRequirement already satisfied: referencing>=0.28.4 in /home/sunshine/pt/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.35.1)\r\nRequirement already satisfied: rpds-py>=0.7.1 in /home/sunshine/pt/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.20.0)\r\nRequirement already satisfied: mdurl~=0.1 in /home/sunshine/pt/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (0.1.2)\r\nRequirement already satisfied: chardet<6,>=3.0.4 in /home/sunshine/pt/lib/python3.11/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (5.2.0)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sunshine/pt/lib/python3.11/site-packages (from sympy->torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.0)\r\n+ pip3 uninstall -y triton\r\nWARNING: Skipping triton as it is not installed.\r\n+ pip3 install --extra-index-url https://download.pytorch.org/whl/nightly/cpu torch==2.5.0.dev20240814 torchvision==0.20.0.dev20240814 torchtune==0.3.0.dev20240916\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple, https://download.pytorch.org/whl/nightly/cpu\r\nCollecting torch==2.5.0.dev20240814\r\n  Using cached https://download.pytorch.org/whl/nightly/cpu/torch-2.5.0.dev20240814-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (91.4 MB)\r\nERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\r\nERROR: Could not find a version that satisfies the requirement torchvision==0.20.0.dev20240814 (from versions: 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0.dev20231010, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0.dev20240723, 0.20.0.dev20240724, 0.20.0.dev20240725, 0.20.0.dev20240726, 0.20.0.dev20240727, 0.20.0.dev20240728, 0.20.0.dev20240729, 0.20.0.dev20240730, 0.20.0.dev20240731, 0.20.0.dev20240801, 0.20.0.dev20240802, 0.20.0.dev20240803, 0.20.0.dev20240804, 0.20.0.dev20240805, 0.20.0.dev20240806, 0.20.0.dev20240807, 0.20.0.dev20240808, 0.20.0.dev20240809, 0.20.0.dev20240810, 0.20.0.dev20240811, 0.20.0.dev20240812, 0.20.0.dev20240813, 0.20.0.dev20240815, 0.20.0.dev20240816, 0.20.0.dev20240818, 0.20.0.dev20240819, 0.20.0.dev20240820, 0.20.0.dev20240821, 0.20.0.dev20240822, 0.20.0.dev20240823, 0.20.0.dev20240824, 0.20.0.dev20240825, 0.20.0.dev20240826, 0.20.0.dev20240827, 0.20.0.dev20240828, 0.20.0.dev20240829, 0.20.0.dev20240830, 0.20.0.dev20240831, 0.20.0.dev20240901, 0.20.0.dev20240902, 0.20.0.dev20240903, 0.20.0.dev20240904, 0.20.0.dev20240905, 0.20.0.dev20240906, 0.20.0.dev20240907, 0.20.0.dev20240908, 0.20.0.dev20240909, 0.20.0.dev20240910, 0.20.0.dev20240911, 0.20.0.dev20240912, 0.20.0.dev20240913, 0.20.0.dev20240914, 0.20.0.dev20240915, 0.20.0.dev20240916, 0.20.0.dev20240917, 0.20.0.dev20240918, 0.20.0.dev20240919, 0.20.0.dev20240920)\r\nERROR: No matching distribution found for torchvision==0.20.0.dev20240814\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Versions\r\n```\r\n(pt) sunshine@raspberrypi:~/torchchat $ wget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n--2024-09-20 19:43:09--  https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 23357 (23K) [text/plain]\r\nSaving to: â\r\n\r\ncollect_env.py      100%[===================>]  22.81K  --.-KB/s    in 0.007s  \r\n\r\n2024-09-20 19:43:09 (3.30 MB/s) - â saved [23357/23357]\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240716\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux trixie/sid (aarch64)\r\nGCC version: (Debian 13.3.0-6) 13.3.0\r\nClang version: 16.0.6 (27+b1)\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.40\r\n\r\nPython version: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0] (64-bit runtime)\r\nPython platform: Linux-6.6.31+rpt-rpi-v8-aarch64-with-glibc2.40\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         aarch64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nCPU(s):                               4\r\nOn-line CPU(s) list:                  0-3\r\nVendor ID:                            ARM\r\nModel name:                           Cortex-A76\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per cluster:                  4\r\nSocket(s):                            -\r\nCluster(s):                           1\r\nStepping:                             r4p1\r\nCPU(s) scaling MHz:                   100%\r\nCPU max MHz:                          2400.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             108.00\r\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nL1d cache:                            256 KiB (4 instances)\r\nL1i cache:                            256 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             2 MiB (1 instance)\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; CSV2, BHB\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+9129892\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240716\r\n[pip3] torchao==0.4.0+gite11201a\r\n[pip3] torchaudio==2.4.0.dev20240716\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchvision==0.20.0.dev20240716\r\n[conda] Could not collect\r\n```\r\n\r\n",
      "state": "closed",
      "author": "sunshinesfbay",
      "author_type": "User",
      "created_at": "2024-09-21T02:44:49Z",
      "updated_at": "2025-01-24T21:29:02Z",
      "closed_at": "2025-01-24T21:29:02Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1173/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "atalman"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1173",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1173",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:14.292044",
      "comments": [
        {
          "author": "sunshinesfbay",
          "body": "Also (after commenting out torchvision):\r\n\r\n```\r\n+ pip3 install torchao==0.5.0\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nERROR: Could not find a version that satisfies the requirement torchao==0.5.0 (from versions: 0.0.1, 0.0.3, 0.1)\r\nERROR: No matching distribut",
          "created_at": "2024-09-21T03:28:06Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thanks for flagging! \r\n\r\nI haven't tested much on ARM64 machines, so this is a helpful find",
          "created_at": "2024-09-21T06:24:39Z"
        },
        {
          "author": "jerryzh168",
          "body": "> Also (after commenting out torchvision):\r\n> \r\n> ```\r\n> + pip3 install torchao==0.5.0\r\n> Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\n> ERROR: Could not find a version that satisfies the requirement torchao==0.5.0 (from versions: 0.0.1, 0.0.3, 0.1)\r\n> ERROR: No matc",
          "created_at": "2024-09-23T17:32:14Z"
        },
        {
          "author": "atalman",
          "body": "Hi @sunshinesfbay The topmost comment is about nightly builds. torch+torchvision\r\nFor aarch64 please monitor: https://hud2.pytorch.org/hud/pytorch/vision/nightly/1?per_page=50&name_filter=aarch\r\nAfter it completes with the green state, nightly should become available.\r\n\r\nWe do have validation worklf",
          "created_at": "2024-10-23T22:22:50Z"
        },
        {
          "author": "mikekgfb",
          "body": "> We do have validation worklfows that we run after nightly is complete here an example for aarch64: https://github.com/pytorch/builder/actions/runs/11489051491/job/31977088265\r\n\r\nCould we run an aarch64 linux flavor of https://github.com/pytorch/torchchat/blob/main/.github/workflows/run-readme-pr.y",
          "created_at": "2024-11-07T01:07:33Z"
        }
      ]
    },
    {
      "issue_number": 1376,
      "title": "[RFC] Integration of Distributed Inference into TorchChat",
      "body": "### 🚀 The feature, motivation and pitch\n\n\r\n**Overview**\r\nThe goal of this RFC is to discuss the integration of distributed inference into TorchChat. Distributed inference leverages tensor parallelism or pipeline parallelism, or a combination of both to support larger model size which do not fit on a single accelerator. Through parallelization each model shard runs in its own worker process. The processes can either be spawned on the script level (e.g. via torchrun) or from within the main script. For online use cases like chat/server the processes need to coordinate fetching and sharing the user input depending on at which point the processes get spawned. Synchronization points between the processes should be minimized for optimal performance.\r\nThe design goals of the integration are:\r\n* Support all CLI features of TorchChat (generate, chat, server)\r\n* Minimize code duplication\r\n* Maintain TorchChat's copy/pastebility\r\n\n\n### Alternatives\n\n\r\n**Option 1: Integrate at Model Level**\r\nWhile the usage of a tensor parallel model in PyTorchis is very much transparent, the current pipeline parallel API differs significantly from the usage of a local model. This option hides the distributed inference from the Generator class by introducing the distributed inference inside a torchchat.model.Model derivative. The DistributedModel(torchchat.model.Model) class would implement methods like __call__() and forward() and handle distribution to the worker processes inside. \r\n* Pros: \r\n   * Code reuse high\r\n   * Transparent use of distributed model\r\n   * Virtually no changes in main Generator and OpenAiApiGenerator necessary\r\n* Cons:\r\n   * In this scenario, sampling happens in the main script and thus the return value of the model (logits) need to be transferred between processes (i.e. moved to shared GPU memory)\r\n   * As the Generator is unaware of the parallelism the subprocesses would need to be spawned inside the model itself which is kind of ugly\r\n\r\n**Option 2: Abstract Base Class for Generator**\r\nIntroduce a base class Generator which contains the common portions of the implementation generation process like getting and preparing input from the user. LocalGenerator and DistributedGenerator get introduced to handle specifics. The split between base and derivatives can be made at multiple levels, specifically High:Generator.generate, Mid:Generator.decode_n_tokens/prefill, Low: Generator.decode_one_token/prefill\r\n* Pros:\r\n   * Introduces abstraction in the generation process\r\n   * High code reuse\r\n   * Subprocess creation for parallel workers can be on main script level\r\n   * Added complexity stays mostly separate from local generation\r\n* Cons:\r\n   * Splitting up the Generator from main generate.py file will hurt copy/pastebility\r\n   * OpenAiApiGenerator (currently inherits from Generator) will require additional changes to work with distributed inference\r\n\r\n**Option 2b: Integrate at Low Level of Generator without base class**\r\nThis approach skips the creation of a base class and directly inherits DistributedGenerator(Generator) and adds functionality for distributed inference in the main generate.py file. \r\n* Pros:\r\n   * Fully reuses the functionality from existing Generator\r\n   * Subprocess creation for parallel workers can be on main script level\r\n   * Maintains copy/pastebility\r\n* Cons:\r\n   * Some changes necessary in generate.py\r\n   * OpenAiApiGenerator (inherits from Generator) will require additional changes to work with distributed inference\r\n\r\ncc @Jack-Khuu @byjlw @lessw2020\r\n\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "mreso",
      "author_type": "User",
      "created_at": "2024-11-14T23:32:15Z",
      "updated_at": "2025-01-24T17:36:37Z",
      "closed_at": "2025-01-24T17:36:37Z",
      "labels": [
        "Distributed",
        "RFC",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1376/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "mreso"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1376",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1376",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:14.502791",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for spinning this up!! Some initial thoughts (some of which we've chatted about offline, but resharing)\r\n\r\nFor **_Option 1_**, while not requiring changes to the Generator is really tempting, making the Model instance manage distribution/subprocesses themselves is a curious pattern. My gut sa",
          "created_at": "2024-11-16T02:12:49Z"
        },
        {
          "author": "byjlw",
          "body": "I put this in the slack channel but also putting it here.\r\n\r\n I wanted to show something I've been discussing with Jack. This is the direction we want to go. Specific details and what goes exactly where could change, but wanted to open the discussion. I think this will help you make an informed deci",
          "created_at": "2024-11-19T22:00:38Z"
        },
        {
          "author": "byjlw",
          "body": "I think option 2b makes the most sense right now given we have refactoring come down the line. \r\nRight now the API/Server isn't particularly clean since things are duplicated between generate and the API. In the short/medium term we want the CLI and API to be making the exact same calls to the gener",
          "created_at": "2024-11-19T23:00:55Z"
        },
        {
          "author": "mreso",
          "body": "Thanks @byjlw Implementation of 2b lives in this #1381 \r\n",
          "created_at": "2024-11-20T01:07:04Z"
        }
      ]
    },
    {
      "issue_number": 1463,
      "title": "[KNOWN BUG] ExecuTorch Related CI Failing",
      "body": "CI Related to ExecuTorch is currently failing on main (https://github.com/pytorch/torchchat/commit/cbc72a42789a827289478bed6445961a48182756) due to an expired pytorch pin. \n\nAn updated pin will be added in: https://github.com/pytorch/torchchat/pull/1459\n* Currently pending an ET fix https://github.com/pytorch/executorch/pull/7689?fbclid=IwZXh0bgNhZW0CMTEAAR2fLSB75VgNDH54rQF5blz3zdLV8oQ5Di-qZ3o6Y-kWCa91WCys_kX0h0o_aem_AZkvTPJAPZDYS2-vZWTHSA\n\n\ncc: @digantdesai, @Gasoonjia, @metascroy ",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2025-01-16T03:56:50Z",
      "updated_at": "2025-01-24T02:52:32Z",
      "closed_at": "2025-01-24T02:52:32Z",
      "labels": [
        "ExecuTorch",
        "CI Infra",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1463/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1463",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1463",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:14.760892",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Partial pin bump in https://github.com/pytorch/torchchat/pull/1459\n\nFailing on [test-tinystories-executorch](https://github.com/pytorch/torchchat/actions/runs/12820207594/job/35749384005#logs) and [test-build-runner-et-android / linux-job](https://github.com/pytorch/torchchat/actions/runs/1282020759",
          "created_at": "2025-01-17T21:22:50Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Tinystories tests fixed in #1470 ",
          "created_at": "2025-01-22T02:02:04Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Potentially fixes build_runner https://github.com/pytorch/torchchat/pull/1475",
          "created_at": "2025-01-24T01:32:56Z"
        }
      ]
    },
    {
      "issue_number": 982,
      "title": "Add an \"Intro to torchchat\" diagram to the README",
      "body": "### 🚀 The feature, motivation and pitch\n\nThere's a lot of good content in the README, but having a visual component will help reinforce the messaging\r\n\r\nIt'll be the first bit of content, repo visitors see, so it should be simple and depict what we want to showcase:\r\n* One flow for edge\r\n* One flow for AOTI\r\n* Quant as a highlight component\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-07-31T17:27:38Z",
      "updated_at": "2025-01-23T23:06:10Z",
      "closed_at": "2025-01-23T23:06:10Z",
      "labels": [
        "actionable"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/982/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/982",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/982",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:14.981689",
      "comments": []
    },
    {
      "issue_number": 1452,
      "title": "Why Torchchat uses MATH as SDPA backend?",
      "body": "### 🐛 Describe the bug\n\nHi maintainers,\r\n\r\nI find that, Torchchat uses MATH as SDPA backend in https://github.com/pytorch/torchchat/blob/main/torchchat/generate.py#L542.  However, for other libs like vllm, they all accept flash attention as default backend.\r\n\r\nSo why Torchchat uses MATH as a default backend? Is this required for accuracy? If not, I can help to add an argument to let user set the backend. Thanks!\n\n### Versions\n\n*",
      "state": "closed",
      "author": "yanbing-j",
      "author_type": "User",
      "created_at": "2025-01-08T08:40:03Z",
      "updated_at": "2025-01-22T01:57:41Z",
      "closed_at": "2025-01-22T01:57:08Z",
      "labels": [
        "enhancement",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1452/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1452",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1452",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:14.981710",
      "comments": [
        {
          "author": "lucylq",
          "body": "> I can help to add an argument to let user set the backend. \r\n\r\nThis seems like a good idea! cc @Jack-Khuu if there's any history behind using MATH as default backend?",
          "created_at": "2025-01-10T18:04:40Z"
        },
        {
          "author": "lucylq",
          "body": "Hmn, actually, it seems like there's an issue exporting when the default backend is not MATH.\r\n\r\nSee issue: https://github.com/pytorch/pytorch/issues/129418\r\n\r\nIt seems like there's a requirement that decompositions during export must not introduce any mutation ops. `SDPBackend.MATH` is known to wor",
          "created_at": "2025-01-11T00:17:12Z"
        },
        {
          "author": "yanbing-j",
          "body": "cc @mingfeima",
          "created_at": "2025-01-11T02:50:28Z"
        },
        {
          "author": "mingfeima",
          "body": "> Hmn, actually, it seems like there's an issue exporting when the default backend is not MATH.\r\n> \r\n> See issue: [pytorch/pytorch#129418](https://github.com/pytorch/pytorch/issues/129418)\r\n> \r\n> It seems like there's a requirement that decompositions during export must not introduce any mutation op",
          "created_at": "2025-01-13T01:22:42Z"
        },
        {
          "author": "yanbing-j",
          "body": "I draft a PR https://github.com/pytorch/torchchat/pull/1456 to add an argument `attention_backend`. The default value of this argument is `math`. Please take a look, thanks!",
          "created_at": "2025-01-13T06:50:17Z"
        }
      ]
    },
    {
      "issue_number": 1444,
      "title": "We still don't use BFDOT on macOS CPU because Apple compiler used for PyTorch wheel is outdated",
      "body": "### 🚀 The feature, motivation and pitch\n\nSee https://github.com/pytorch/pytorch/issues/143913 . This is blocking improved CPU performance for bfloat16 decoding on Mac; setting up an issue on torchchat side to track.\n\n### Alternatives\n\nrobust setup for decoding using accelerators on Mac\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "swolchok",
      "author_type": "User",
      "created_at": "2024-12-27T17:40:21Z",
      "updated_at": "2025-01-20T20:23:35Z",
      "closed_at": null,
      "labels": [
        "performance",
        "Known Gaps",
        "MPS/Metal",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1444/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1444",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1444",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:15.219238",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Pulling up context from the chain: \n\nPer @huydhn \n\n> We don't plan to take any action for this right now and will wait for Apple to release a new clang/xcode version on MacOS\n\n",
          "created_at": "2025-01-15T19:20:31Z"
        },
        {
          "author": "huydhn",
          "body": "There is a recent update on this https://github.com/pytorch/pytorch/issues/143913#issuecomment-2603180885, it looks like the beta GitHub MacOS 15 runner now has clang 18",
          "created_at": "2025-01-20T20:23:34Z"
        }
      ]
    },
    {
      "issue_number": 1358,
      "title": "Create doc and tests for distributed inference",
      "body": "### 🚀 The feature, motivation and pitch\n\nOnce distributed inference integration into torchchat is functional, let's add a docs/distributed.md with an example, and plumb that example into `.ci/scripts/run-docs distributed`.  (updown.py extracts all commands between triple backticks into a test script.) \r\n\r\ntorchchat has the same runners as pytorch/pytorch, so at least a minimal 2 or 4 GPU setup on a single node would be great.  Not sure whether we can run multi-node testing, you can suppress commands from tests with `[skip default]: begin` and `[skip default]: end` around those commands.  \r\n\r\ncc: @mreso @lessw2020 @kwen2501 \n\n### Alternatives\n\nNone\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-11-08T02:08:33Z",
      "updated_at": "2025-01-18T06:15:01Z",
      "closed_at": "2025-01-18T06:15:00Z",
      "labels": [
        "documentation",
        "actionable",
        "Distributed",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1358/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "mreso"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1358",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1358",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:15.431499",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Definitely something we plan to add!",
          "created_at": "2024-11-12T01:39:12Z"
        },
        {
          "author": "mikekgfb",
          "body": "#1438 ",
          "created_at": "2025-01-18T06:15:00Z"
        }
      ]
    },
    {
      "issue_number": 1402,
      "title": "Distributed Setup is taking up a huge amount of memory",
      "body": "Hello,\r\n\r\nI am running a distributed setup to perform inference with an 8-billion parameter LLaMA model. Despite expecting the workload to fit within two machines (each with 16GB of memory), I had to utilize four machines to avoid memory issues. Even after removing the initialization of the KV cache, for some passes the memory usage still exceeded 9GB per machine.\r\n\r\nCould you please help identify potential reasons for this behavior, or let me know if there is something I might be overlooking in the setup?\r\n\r\nThank you!",
      "state": "closed",
      "author": "bhuvan777",
      "author_type": "User",
      "created_at": "2024-12-07T03:22:30Z",
      "updated_at": "2025-01-15T19:21:34Z",
      "closed_at": "2025-01-15T19:21:33Z",
      "labels": [
        "need-user-input",
        "Distributed"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1402/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1402",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1402",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:15.643160",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Hi @bhuvan777, can you provide a repro?\r\n\r\nAre you using torchchat's distributed flag or handling the distributed aspect locally? ",
          "created_at": "2024-12-09T17:32:27Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Closing Issue awaiting user-input after 30 days",
          "created_at": "2025-01-15T19:21:33Z"
        }
      ]
    },
    {
      "issue_number": 1395,
      "title": "Torchchat on Android crashes on second prompt",
      "body": "### 🐛 Describe the bug\n\nDevice Info:\r\nDevice: Google Pixel 9 \r\nAndroid Version: 15\r\nAPI Level: 35\r\n\r\nSteps to reproduce the bug:\r\n\r\n* Follow the steps in the documentation on llama-3.2-3b-instruct and copy both the llama model and the associated tokenizer  model in the temp directory\r\n* Load torchchat in Android Stuido\r\n* Type a random prompt for the first promt\r\n* After the first prompt, write a second prompt\r\n\r\nExpected: \r\nThe Llama model should produce output\r\n\r\nWhat happened:\r\n```\r\n2024-11-25 14:52:50.659 19932-20110 ExecuTorch              org.pytorch.torchchat                I  RSS after loading model: 2391.855469 MiB (0 if unsupported)\r\n2024-11-25 14:52:50.660 19932-20110 ExecuTorch              org.pytorch.torchchat                A  In function generate(), assert failed (num_prompt_tokens < metadata_.at(kMaxSeqLen)): num_prompt_tokens 140 >= max_seq_len_ 128, Max seq length exceeded - please increase max seq len value in .../llama2/model.py\r\n2024-11-25 14:52:50.661 19932-20110 libc                    org.pytorch.torchchat                A  Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 20110 (pool-3-thread-1), pid 19932 (torch.torchchat)\r\n2024-11-25 14:52:50.782 19932-20004 HWUI                    org.pytorch.torchchat                I  Davey! duration=3084ms; Flags=0, FrameTimelineVsyncId=8776715, IntendedVsync=648189722387971, Vsync=648192674316622, InputEventId=276502475, HandleInputStart=648192688975798, AnimationStart=648192689011239, PerformTraversalsStart=648192689012013, DrawStart=648192795535247, FrameDeadline=648189738987971, FrameInterval=648192688396331, FrameStartTime=16677563, SyncQueued=648192798785491, SyncStart=648192799176116, IssueDrawCommandsStart=648192799894540, SwapBuffers=648192802982390, FrameCompleted=648192807580860, DequeueBufferDuration=332927, QueueBufferDuration=514974, GpuCompleted=648192807580860, SwapBuffersCompleted=648192803706715, DisplayPresentTime=648178625308026, CommandSubmissionCompleted=648192802982390, \r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A  Cmdline: org.pytorch.torchchat\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A  pid: 19932, tid: 20110, name: pool-3-thread-1  >>> org.pytorch.torchchat <<<\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #01 pc 00000000015fdf54  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/lib/arm64/libexecutorch.so (et_pal_abort+8) (BuildId: 87abca08e486390fd661b9f8676b8b0c40ba5d04)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #02 pc 00000000015fdd80  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/lib/arm64/libexecutorch.so (executorch::runtime::runtime_abort()+8) (BuildId: 87abca08e486390fd661b9f8676b8b0c40ba5d04)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #03 pc 0000000001583d9c  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/lib/arm64/libexecutorch.so (example::Runner::generate(std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char>> const&, int, std::__ndk1::function<void (std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char>> const&)>, std::__ndk1::function<void (executorch::extension::llm::Stats const&)>, bool, bool)+3748) (BuildId: 87abca08e486390fd661b9f8676b8b0c40ba5d04)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #04 pc 00000000001e8b18  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/lib/arm64/libexecutorch.so (executorch_jni::ExecuTorchLlamaJni::generate(facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char)+408) (BuildId: 87abca08e486390fd661b9f8676b8b0c40ba5d04)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #05 pc 00000000001e9438  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/lib/arm64/libexecutorch.so (facebook::jni::detail::MethodWrapper<int (executorch_jni::ExecuTorchLlamaJni::*)(facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char), &executorch_jni::ExecuTorchLlamaJni::generate(facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char), executorch_jni::ExecuTorchLlamaJni, int, facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char>::dispatch(facebook::jni::alias_ref<facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*>, facebook::jni::alias_ref<_jintArray*>&&, int&&, int&&, int&&, facebook::jni::alias_ref<_jstring*>&&, int&&, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>&&, unsigned char&&)+156) (BuildId: 87abca08e486390fd661b9f8676b8b0c40ba5d04)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #06 pc 00000000001e9304  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/lib/arm64/libexecutorch.so (facebook::jni::detail::FunctionWrapper<int (*)(facebook::jni::alias_ref<facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*>, facebook::jni::alias_ref<_jintArray*>&&, int&&, int&&, int&&, facebook::jni::alias_ref<_jstring*>&&, int&&, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>&&, unsigned char&&), facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*, int, facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char>::call(_JNIEnv*, _jobject*, _jintArray*, int, int, int, _jstring*, int, facebook::jni::detail::JTypeFor<executorch_jni::ExecuTorchLlamaCallbackJni, facebook::jni::JObject, void>::_javaobject*, unsigned char, int (*)(facebook::jni::alias_ref<facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*>, facebook::jni::alias_ref<_jintArray*>&&, int&&, int&&, int&&, facebook::jni::alias_ref<_jstring*>&&, int&&, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>&&, unsigned char&&))+164) (BuildId: 87abca08e486390fd661b9f8676b8b0c40ba5d04)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #07 pc 00000000001e794c  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/lib/arm64/libexecutorch.so (facebook::jni::detail::MethodWrapper<int (executorch_jni::ExecuTorchLlamaJni::*)(facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char), &executorch_jni::ExecuTorchLlamaJni::generate(facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char), executorch_jni::ExecuTorchLlamaJni, int, facebook::jni::alias_ref<_jintArray*>, int, int, int, facebook::jni::alias_ref<_jstring*>, int, facebook::jni::alias_ref<executorch_jni::ExecuTorchLlamaCallbackJni>, unsigned char>::call(_JNIEnv*, _jobject*, _jintArray*, int, int, int, _jstring*, int, facebook::jni::detail::JTypeFor<executorch_jni::ExecuTorchLlamaCallbackJni, facebook::jni::JObject, void>::_javaobject*, unsigned char)+40) (BuildId: 87abca08e486390fd661b9f8676b8b0c40ba5d04)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #14 pc 0000000000357504  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/base.apk (org.pytorch.executorch.LlamaModule.generate+0)\r\n2024-11-25 14:52:51.146 20135-20135 DEBUG                                                        A        #19 pc 0000000000005d08  /data/app/~~vQVnC2iQW4Ws7d2zTIrIEQ==/org.pytorch.torchchat-ta0utv2u-lYvdiOSyxOdLA==/base.apk (org.pytorch.torchchat.MainActivity$4.run+0)\r\n```\r\n\r\nIt seems that the tokens are miscounted on the second call.  This isn't the case for the iOS application.  I haven't looked at the Android version of the demo located in the executorch repo.  I haven't tested on other models yet, but I can start moving other llama models over to the Android device to see if I can reproduce this tokenizer bug.\n\n### Versions\n\nHere's the info on my MBP.\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241002\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.6.1 (arm64)\r\nGCC version: Could not collect\r\nClang version: 16.0.0 (clang-1600.0.26.4)\r\nCMake version: version 3.30.4\r\nLibc version: N/A\r\n\r\nPython version: 3.10.15 | packaged by conda-forge | (main, Sep 30 2024, 17:48:38) [Clang 17.0.6 ] (64-bit runtime)\r\nPython platform: macOS-14.6.1-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M1 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.5.0a0+72b3bb3\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.6.0.dev20241002\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.5.0.dev20241007\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchtune==0.4.0.dev20241010+cpu\r\n[pip3] torchvision==0.20.0.dev20241002\r\n[conda] numpy                     1.26.4          py312h7f4fdc5_0  \r\n[conda] numpy-base                1.26.4          py312he047099_0  \r\n[conda] numpydoc                  1.7.0           py312hca03da5_0  \r\n```",
      "state": "closed",
      "author": "infil00p",
      "author_type": "User",
      "created_at": "2024-11-25T23:04:28Z",
      "updated_at": "2025-01-15T19:14:21Z",
      "closed_at": "2025-01-15T19:14:19Z",
      "labels": [
        "bug",
        "Mobile - Android",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1395/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "kirklandsign"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1395",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1395",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:15.885907",
      "comments": [
        {
          "author": "HeresMyGit",
          "body": "I'm seeing the same issue.  I get this crash log on the device:\r\n\r\nAbort message: 'In function generate(), assert failed (num_prompt_ tokens < metadata_.at (kMaxSeqLen)): num_prompt_tokens 138 >= max_seq_len_ 128, Max seq length exceeded - please increase max seq len value in .../llama2/model-py'",
          "created_at": "2024-11-26T04:53:21Z"
        },
        {
          "author": "infil00p",
          "body": "I tested with the Llama-3.1-8b model that was used in the instructions, and I'm getting the same behaviour.",
          "created_at": "2024-11-26T05:09:01Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "@kirklandsign Can you take a look at this? Might be related to the tps workaround that Scott looked into a while back",
          "created_at": "2024-11-26T19:49:42Z"
        },
        {
          "author": "infil00p",
          "body": "After testing the Software Maison React Native code and adding Executorch to my project, the issue appears to be with the Android app written for Torchchat and shared with the example app in Executorch.",
          "created_at": "2024-11-28T13:06:37Z"
        },
        {
          "author": "HeresMyGit",
          "body": "Was anybody able to work past this issue?\r\n\r\nI hardcoded the max seq len in several files and rebuilt but it still crashes at the same spot.\r\n\r\n@infil00p if you have any insight please let me know!",
          "created_at": "2025-01-07T17:47:47Z"
        }
      ]
    },
    {
      "issue_number": 1125,
      "title": "int4_weight_only in Cuda compile := RuntimeError: _apply(): Couldn't swap Linear.weight",
      "body": "### 🐛 Describe the bug\r\n\r\nWhen generating multiple samples from a compiled int4 model on CUDA, a runtime error occurs relating to Linear.weight swapping:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 945, in _apply\r\n    torch.utils.swap_tensors(param, param_applied)\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/utils/__init__.py\", line 51, in swap_tensors\r\n    raise RuntimeError(\"Cannot swap t1 because it has weakref associated with it\")\r\nRuntimeError: Cannot swap t1 because it has weakref associated with it\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jackkhuu/oss/torchchat/torchchat.py\", line 83, in <module>\r\n    generate_main(args)\r\n  File \"/home/jackkhuu/oss/torchchat/torchchat/generate.py\", line 934, in main\r\n    for _ in gen.chat(generator_args):\r\n  File \"/home/jackkhuu/oss/torchchat/torchchat/generate.py\", line 826, in chat\r\n    for token_tensor, metrics in generator_func:\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n  File \"/home/jackkhuu/oss/torchchat/torchchat/generate.py\", line 518, in generate\r\n    model = model.to(device=device)\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1340, in to\r\n    return self._apply(convert)\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 900, in _apply\r\n    module._apply(fn)\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 900, in _apply\r\n    module._apply(fn)\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 900, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 2 more times]\r\n  File \"/home/jackkhuu/.conda/envs/99/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 949, in _apply\r\n    raise RuntimeError(\r\nRuntimeError: _apply(): Couldn't swap Linear.weight\r\n```\r\n\r\nThis only occurs for `int4 Quant + CUDA` (which narrows it to [int4_weight_only](https://github.com/pytorch/torchchat/blob/16dbdd782ae9f0ec2ba53c764ded0b80030172a9/torchchat/utils/quantize.py#L85)) with `torch.compile`\r\n\r\nExample Command: \r\n```\r\npython3 torchchat.py generate llama3.1 --quantize '{\"linear:int4\": {\"groupsize\": 256}, \"executor\":{\"accelerator\":\"cuda\"}}' --compile --num-samples 2\r\n```\r\n\r\nCommit: https://github.com/pytorch/torchchat/commit/16dbdd782ae9f0ec2ba53c764ded0b80030172a9\r\n\r\n### Versions\r\n\r\nPyTorch version: 2.5.0.dev20240814+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Stream 9 (x86_64)\r\nGCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.1-3)\r\nClang version: 18.1.6 (CentOS 18.1.6-3.el9)\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.34\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-0_fbk12_hardened_11583_g0bef9520ca2b-x86_64-with-glibc2.34\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA PG509-210\r\nNvidia driver version: 525.105.17\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.9.4\r\n/usr/lib64/libcudnn_adv_infer.so.8.9.4\r\n/usr/lib64/libcudnn_adv_train.so.8.9.4\r\n/usr/lib64/libcudnn_cnn_infer.so.8.9.4\r\n/usr/lib64/libcudnn_cnn_train.so.8.9.4\r\n/usr/lib64/libcudnn_ops_infer.so.8.9.4\r\n/usr/lib64/libcudnn_ops_train.so.8.9.4\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          22\r\nOn-line CPU(s) list:             0-21\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz\r\nCPU family:                      6\r\nModel:                           85\r\nThread(s) per core:              1\r\nCore(s) per socket:              22\r\nSocket(s):                       1\r\nStepping:                        11\r\nBogoMIPS:                        3591.57\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 arat umip pku ospke avx512_vnni md_clear arch_capabilities\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       704 KiB (22 instances)\r\nL1i cache:                       704 KiB (22 instances)\r\nL2 cache:                        88 MiB (22 instances)\r\nL3 cache:                        16 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-21\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Mitigation; Enhanced IBRS\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Vulnerable: eIBRS with unprivileged eBPF\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; TSX disabled\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton==3.0.0+dedb7bdf33\r\n[pip3] torch==2.5.0.dev20240814+cu121\r\n[pip3] torchao==0.4.0+git477ddb6\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+dedb7bdf33          pypi_0    pypi\r\n[conda] torch                     2.5.0.dev20240814+cu121          pypi_0    pypi\r\n[conda] torchao                   0.4.0+git477ddb6          pypi_0    pypi",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-09-10T04:18:50Z",
      "updated_at": "2025-01-06T15:58:17Z",
      "closed_at": "2025-01-06T15:58:17Z",
      "labels": [
        "bug",
        "Compile / AOTI",
        "Quantization"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1125/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1125",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1125",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:16.156136",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "@jerryzh168 Have you seen this before?",
          "created_at": "2024-09-10T04:19:23Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Seems like `model = model.to(device=device)` doesn't play well the second time around. \r\nMaybe the cache being populated makes a difference with this quant? \r\n\r\nhttps://github.com/pytorch/torchchat/blob/main/torchchat/generate.py#L518",
          "created_at": "2024-09-10T16:43:29Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Looks like something that may have gotten picked up in the PT or AO pin bumps: https://github.com/pytorch/torchchat/commit/147c292dd2994be664ea415ea7ff580dcc1fdb3a",
          "created_at": "2024-09-10T23:21:28Z"
        },
        {
          "author": "jerryzh168",
          "body": "sorry just saw this issue, I haven't see the error before, we also have test for `to(device=\"cuda\")` as well for int4_weight_only I think: https://github.com/pytorch/ao/blob/ceec750d7f6f8aefabf6c31e83f139be79ac03b4/test/dtypes/test_affine_quantized.py#L78\r\n\r\nis this still an issue?",
          "created_at": "2024-09-26T17:07:16Z"
        }
      ]
    },
    {
      "issue_number": 1429,
      "title": "[KNOWN BUG] AOTI Inference via cpp Runner bug",
      "body": "### 🐛 Describe the bug\n\nAs of https://github.com/pytorch/torchchat/pull/1426, `torchchat/main` is failing 2 jobs related to AOTI inference via c++ runners. \n\n### Versions\n\nhttps://github.com/pytorch/torchchat/commit/fd1857a6deb9c425527d4fd69fba2c5ca5641f93",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-12-19T03:23:18Z",
      "updated_at": "2025-01-06T01:55:53Z",
      "closed_at": "2025-01-06T01:55:53Z",
      "labels": [
        "bug",
        "Known Gaps",
        "CI Infra",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1429/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1429",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1429",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:16.393566",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "Seems to be a problem in sentencepiece https://github.com/pytorch/torchchat/blob/cc0ffce74f50fd9d47db495aafa50d498dbf447d/tokenizer/sentencepiece.cpp#L39-L43",
          "created_at": "2024-12-20T01:53:05Z"
        }
      ]
    },
    {
      "issue_number": 1440,
      "title": "Migrate Tokenizer Components to utilize pytorch-labs/tokenizers",
      "body": "### 🚀 The feature, motivation and pitch\n\n@larryliu0820 has created a new shared repository for [hosting tokenizer definitions](https://github.com/pytorch-labs/tokenizers). \r\n\r\nThe initial migration attempt was reverted in  https://github.com/pytorch/torchchat/pull/1414 due to a tokenizer issue flagged in https://github.com/pytorch/torchchat/issues/1413, but should be straightforward to debug and reland\r\n\r\n**Task**: Taking inspiration from https://github.com/pytorch/torchchat/pull/1401, reattempt this migration \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-12-23T08:37:17Z",
      "updated_at": "2025-01-06T01:55:42Z",
      "closed_at": "2025-01-06T01:55:42Z",
      "labels": [
        "enhancement",
        "good first issue",
        "actionable",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1440/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1440",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1440",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:16.679471",
      "comments": []
    },
    {
      "issue_number": 1446,
      "title": "Supply Local Weights to an LLM instead of Downloading Weights from HuggingFace",
      "body": "### 🚀 The feature, motivation and pitch\n\nI am having local copy of llama weights and i want to supply those weights to create a chat application.Please include a CLI flag to do so\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "sgupta1007",
      "author_type": "User",
      "created_at": "2024-12-29T20:14:26Z",
      "updated_at": "2025-01-06T01:54:19Z",
      "closed_at": "2025-01-06T01:54:19Z",
      "labels": [
        "documentation",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1446/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1446",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1446",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:16.679492",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "This already exists, but is not documented in the master README.md which is targeted at more novice users.\r\n\r\nThere are some examples how to use local checkpoints in https://github.com/pytorch/torchchat/blob/019f76f4f97e5458cdb7f577f3badfa06943aedd/docs/ADVANCED-USERS.md?plain=1#L245 and #1336 has s",
          "created_at": "2024-12-29T21:48:26Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thanks for answering @mikekgfb and spinning up a doc!! It'll be great for future users\r\n\r\nI'm also planning to send out an RFC after the holidays for an improve flow so stay tuned :) ",
          "created_at": "2025-01-03T03:28:10Z"
        }
      ]
    },
    {
      "issue_number": 1436,
      "title": "If scripts need `bash`, don't say to use `sh`",
      "body": "### 🐛 Describe the bug\n\nOn Debian systems, sh isn't bash, it's [dash](https://en.wikipedia.org/wiki/Almquist_shell#Dash). I haven't tested every script, but https://github.com/pytorch/torchchat/blob/main/docs/quantization.md says to run `sh torchchat/utils/scripts/build_torchao_ops.sh`, but this script fails unless run with bash on my Raspberry Pi 5.\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241218+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 12 (bookworm) (aarch64)\r\nGCC version: (Debian 12.2.0-14) 12.2.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.36\r\n\r\nPython version: 3.11.2 (main, Sep 14 2024, 03:00:30) [GCC 12.2.0] (64-bit runtime)\r\nPython platform: Linux-6.6.51+rpt-rpi-2712-aarch64-with-glibc2.36\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         aarch64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nCPU(s):                               4\r\nOn-line CPU(s) list:                  0-3\r\nVendor ID:                            ARM\r\nModel name:                           Cortex-A76\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per cluster:                  4\r\nSocket(s):                            -\r\nCluster(s):                           1\r\nStepping:                             r4p1\r\nCPU(s) scaling MHz:                   100%\r\nCPU max MHz:                          2400.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             108.00\r\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nL1d cache:                            256 KiB (4 instances)\r\nL1i cache:                            256 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             2 MiB (1 instance)\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; CSV2, BHB\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.6.0.dev20241218+cpu\r\n[pip3] torchao==0.8.0+git2f97b095\r\n[pip3] torchtune==0.5.0.dev20241218+cpu\r\n[pip3] torchvision==0.22.0.dev20241218\r\n[conda] Could not collect",
      "state": "closed",
      "author": "swolchok",
      "author_type": "User",
      "created_at": "2024-12-22T06:43:48Z",
      "updated_at": "2024-12-23T06:49:43Z",
      "closed_at": "2024-12-23T06:49:43Z",
      "labels": [
        "bug",
        "documentation",
        "actionable",
        "Quantization",
        "triaged"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1436/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1436",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1436",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:16.913184",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "Would be useful to run linx + aarch64 as part of regression testing to catch similar items.  #1350 adds running on the linux/aarch64 combo.  I think I have the right config name, but we need to enable the corresponding runner for torchchat to make this happen.  (And confirm the runner name used in t",
          "created_at": "2024-12-22T22:40:30Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "@mikekgfb just fixed in https://github.com/pytorch/torchchat/pull/1437 \r\n\r\nThanks for the flag",
          "created_at": "2024-12-23T06:49:43Z"
        }
      ]
    },
    {
      "issue_number": 1427,
      "title": "INT8 has a poor performance with groupsize > 0 in Torchchat, compared with BF16 and INT8 groupsize == 0",
      "body": "### 🐛 Describe the bug\r\n\r\nHi maintainers,\r\n\r\nWe find that INT8 with groupsize 0 can achieve relatively good performance than BF16 dtype. While INT8 with groupsize > 0 performs even worse than BF16.\r\n\r\nBF16 results:\r\n```\r\nWarning: Excluding compile in calculations\r\n      Average tokens/sec (total): 6.28\r\nAverage tokens/sec (first token): 0.45\r\nAverage tokens/sec (next tokens): 7.35\r\n```\r\n\r\nINT8 with groupsize 0:\r\n```\r\nWarning: Excluding compile in calculations\r\n      Average tokens/sec (total): 6.89\r\nAverage tokens/sec (first token): 0.12\r\nAverage tokens/sec (next tokens): 12.24\r\n```\r\n\r\nINT8 with groupsize 128:\r\n```\r\nWarning: Excluding compile in calculations\r\n      Average tokens/sec (total): 2.54\r\nAverage tokens/sec (first token): 0.46\r\nAverage tokens/sec (next tokens): 2.64\r\n```\r\n\r\nI also investigate INT8 groupsize > 0 in torchao, Only https://github.com/pytorch/ao/pull/1121 has this int8 wo groupwise support with `int8_weight_only(group_size=group_size)` in `_int8wo_groupwise_api`. Unfortunately, it eventually runs into `F.linear`, which is same as torchchat usage and is a slow path. \r\n\r\nIs INT8 woq with groupsize a key point in torchchat? Do you have plan to optimize this feature? Thanks!\r\n\r\nReproducer:\r\n`numactl --physcpubind=120-159 --membind=3 python3 torchchat.py generate llama3.1 --prompt 'It is done, and submitted. You can play '\\''Survival of the Tastiest'\\'' on Android, and on the web. Playing on the web works, but you have to simulate multiple touch for table moving and that can be a bit confusing. There is a lot I'\\''d like to talk about. I will go through every topic, insted of making the typical what went right/wrong list. Concept Working over the theme was probably one of the hardest tasks which I had to face. Originally, I had an idea of what kind of game I wanted to develop, gameplay wise - something with a lot of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident that I could fit any theme around it. In the end, the problem with a theme like '\\''Evolution'\\'' in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game? In a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it'\\''s not evolution anymore - it'\\''s the equivalent of intelligent design, the fable invented by creationists to combat the idea of evolution. Being agnostic and a Pastafarian, that'\\''s not something that rubbed me the right way. Hence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn'\\''t want to create an '\\''intelligent design'\\'' simulator and wrongly call it evolution. This is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I'\\''d say the only real solution was through the use of artificial selection, somehow. So far, I haven'\\''t seen any entry using this at its core gameplay. Alas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out. My initial idea was to create something where humanity tried to evolve to a next level, but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn'\\''t think of compelling (read: serious) mechanics for that. Borgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg? The third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it. Conversations with my inspiring co-worker Roushey (who also created the '\\''Mechanical Underdogs'\\'' signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist - by evolving from a normal dinner table. So the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your '\\''base'\\''. There are 5 other guests at the table, each with their own plate. Your plate can spawn little pieces of pasta. You do so by '\\''ordering'\\'' them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying '\\''costs'\\'', which are debited from your credits (you start with a number of credits). Once spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps). Your pasta doesn'\\''t like other people'\\''s pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill. Once a pasta is in the vicinity of a plate, it starts conquering it for its team. It takes around 10 seconds for a plate to be conquered; less if more pasta from the same team are around. If pasta from other team are around, though, they get locked down in their attempt, unable to conquer the plate, until one of them die (think Battlefield'\\''s standard '\\''Conquest'\\'' mode). You get points every second for every plate you own. Over' --quantize '{\"linear:int8\": {\"bitwidth\": 8, \"groupsize\": 128}}' --num-samples 5 --device cpu --max-new-tokens 128`\r\n\r\n### Versions\r\n\r\ntorch-2.6.0.dev20241124+cpu-cp310\r\ntorchaudio-2.5.0.dev20241121+cpu-cp310\r\ntorchvision-0.20.0.dev20241121+cpu-cp310\r\ntorchao==0.8.0+git039cef4a\r\ntorchchat 4fdbe100c21712da78d67d6b4b80c0eb8dc0b1ed",
      "state": "closed",
      "author": "yanbing-j",
      "author_type": "User",
      "created_at": "2024-12-18T03:26:50Z",
      "updated_at": "2024-12-19T09:12:29Z",
      "closed_at": "2024-12-19T09:12:29Z",
      "labels": [
        "Quantization",
        "triaged"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1427/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu",
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1427",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1427",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:17.106469",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for flagging, @vmpuri and @jerryzh168 are actually working on deprecating int8 implementations in torchchat in favor of AO. \r\n\r\nWhile we have historically provided support for int8 groupsizes in torchchat, I haven't seen this be a popular use case. \r\nI'm going to defer whether we want to opti",
          "created_at": "2024-12-18T19:17:53Z"
        },
        {
          "author": "yanbing-j",
          "body": "@Jack-Khuu Thanks for the confirmation! Will keep a check of the replies of you guys! Thanks so much!",
          "created_at": "2024-12-19T01:16:11Z"
        },
        {
          "author": "jerryzh168",
          "body": "yeah we typically just use per channel int8 weight only quant, but we could check the perf for larger group sizes as well I think, maybe we can check again after the torchao migration is done",
          "created_at": "2024-12-19T04:39:25Z"
        }
      ]
    },
    {
      "issue_number": 1423,
      "title": "[KNOWN BUG] gguf + GPU AOTI Inference bug due to PT version, fix in progress",
      "body": "### 🐛 Describe the bug\n\nAs of https://github.com/pytorch/torchchat/pull/1367/, `torchchat/main` is failing 3-5 CI jobs related to GPU AOTI inference and GGUF inference\r\n\r\nGPU AOTI inference will be fixed with a pinbump to https://github.com/pytorch/pytorch/pull/143236\r\nGGUF AO bug is being addressed in https://github.com/pytorch/torchchat/pull/1404\n\n### Versions\n\nhttps://github.com/pytorch/torchchat/commit/bb72b096b14f0c9753070f3523e43ed58aa55178",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-12-14T00:27:50Z",
      "updated_at": "2024-12-19T03:16:59Z",
      "closed_at": "2024-12-19T03:16:59Z",
      "labels": [
        "bug",
        "Known Gaps",
        "Compile / AOTI",
        "Quantization",
        "CI Infra",
        "triaged"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1423/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1423",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1423",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:17.349744",
      "comments": []
    },
    {
      "issue_number": 653,
      "title": "TorchChat is slower than gpt-fast",
      "body": "Using `torch==2.4.0.dev20240502` on Apple M2 pro I get following numbers for stories110M + float16 dtype\r\n| application | speed (eager) | speed (compile) |\r\n| ---- | ----   | ---- |\r\n| gpt-fast  | 176 tokens/sec | 99 tokens/sec |\r\n| torchchat  | 76 tokens/sec |  33 tokens/sec |\r\n\r\n\r\nCommands to reproduce:\r\n```\r\n% python3 -mpip install --pre torch==2.4.0.dev20240502 --index-url https://download.pytorch.org/whl/nightly/cpu\r\n% git clone https://github.com/pytorch-labs/gpt-fast -b malfet/set-prec-to-float16\r\n% cd gpt-fast\r\n % python3 generate.py --checkpoint_path ~/git/pytorch/torchchat/.model-artifacts/stories110M/stories110M.pt \r\n```\r\n\r\nand for torchchat\r\n```\r\n% python3 torchchat.py generate stories110M --dtype float16 --device cpu\r\n```",
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-05-03T16:19:50Z",
      "updated_at": "2024-12-18T02:46:34Z",
      "closed_at": "2024-12-18T02:46:33Z",
      "labels": [
        "performance"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/653/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/653",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/653",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:17.349768",
      "comments": [
        {
          "author": "mikekgfb",
          "body": " ```\r\n python3 torchchat.py generate stories110M --dtype float16\r\n ```\r\n \r\n This runs --device fast which translates into MPS.  You might specify `--device cpu`. Also if CPU is faster than MPS, we should drop it from the devices selected for device \"fast\"",
          "created_at": "2024-05-04T05:26:34Z"
        },
        {
          "author": "malfet",
          "body": "> ```\r\n> python3 torchchat.py generate stories110M --dtype float16\r\n> ```\r\n> \r\n> This runs --device fast which translates into MPS. You might specify `--device cpu`. Also if CPU is faster than MPS, we should drop it from the devices selected for device \"fast\"\r\n\r\nNo, it was not the case until https:/",
          "created_at": "2024-05-06T19:21:46Z"
        },
        {
          "author": "ezyang",
          "body": "why is the compile tok/sec lower than the eager tok/sec",
          "created_at": "2024-05-07T01:55:29Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Closing stale issue",
          "created_at": "2024-12-18T02:46:33Z"
        }
      ]
    },
    {
      "issue_number": 1424,
      "title": "Misaligned AOTI input; potential perf gains by fixing?",
      "body": "### 🐛 Describe the bug\n\nPicked up in https://github.com/pytorch/torchchat/pull/1367, and worked around via https://github.com/pytorch/pytorch/pull/143236, it appears the input to the torchchat AOTI runner is not 16 byte aligned. \r\n\r\nWhile the PR from pytorch/pytorch eases this constraint, this may be indicative of potential perf losses (common of misalignment)\r\n\r\nhattip to @malfet for suggesting line of investigation\n\n### Versions\n\nhttps://github.com/pytorch/torchchat/commit/bb72b096b14f0c9753070f3523e43ed58aa55178",
      "state": "open",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-12-14T01:11:30Z",
      "updated_at": "2024-12-17T23:35:29Z",
      "closed_at": null,
      "labels": [
        "bug",
        "actionable",
        "Compile / AOTI",
        "triaged"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1424/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1424",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1424",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:17.555913",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "I think most tensors are allocated aligned, but slicing and views may break that in Python/Pytorch.  One solution might be to literally clone every input in flat_inputs.  As an experiment, likely `logits = model(x_sliced.clone(), ip_sliced.clone()` might do the trick - but that would saddle cases wh",
          "created_at": "2024-12-14T02:25:13Z"
        }
      ]
    },
    {
      "issue_number": 1365,
      "title": "AOTI filesize regression *.pt2 filesize is bigger than .*so",
      "body": "### 🐛 Describe the bug\n\nExported model for both pt2 and so.  pt2 file is 2x larger:\r\n\r\n llama31_1bit.pt2 filesize: 3.09GB\r\n  llama31_1bit.so filesize: 1.55GB\r\n \r\n\r\n\r\n\r\npt2 command:\r\n```\r\nOMP_NUM_THREADS=6 python torchchat.py export llama3.1 --device cpu --dtype float32 --quantize '{\"embedding:wx\": {\"bitwidth\": 1, \"groupsize\": 32}, \"linear:a8wxdq\": {\"bitwidth\": 1, \"groupsize\": 256, \"has_weight_zeros\": false}}' --output-aoti-package-path llama31_1bit.pt2\r\n```\r\n\r\nso command:\r\n```\r\nOMP_NUM_THREADS=6 python torchchat.py export llama3.1 --device cpu --dtype float32 --quantize '{\"embedding:wx\": {\"bitwidth\": 1, \"groupsize\": 32}, \"linear:a8wxdq\": {\"bitwidth\": 1, \"groupsize\": 256, \"has_weight_zeros\": false}}' --output-dso llama31_1bit.so\r\n```\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241007\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.7 (arm64)\r\nGCC version: Could not collect\r\nClang version: 16.0.0 (clang-1600.0.26.3)\r\nCMake version: version 3.30.5\r\nLibc version: N/A\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 03:54:28) [Clang 12.0.0 ] (64-bit runtime)\r\nPython platform: macOS-14.7-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M1 Pro\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.5.0a0+72b3bb3\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.6.0.dev20241007\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.5.0.dev20241007\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchtune==0.4.0.dev20241010+cpu\r\n[pip3] torchvision==0.20.0.dev20241007\r\n[conda] executorch                0.5.0a0+72b3bb3          pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241007          pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchaudio                2.5.0.dev20241007          pypi_0    pypi\r\n[conda] torchsr                   1.0.4                    pypi_0    pypi\r\n[conda] torchtune                 0.4.0.dev20241010+cpu          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241007          pypi_0    pypi\r\n",
      "state": "closed",
      "author": "metascroy",
      "author_type": "User",
      "created_at": "2024-11-11T22:45:33Z",
      "updated_at": "2024-12-17T23:14:46Z",
      "closed_at": "2024-12-17T23:14:41Z",
      "labels": [
        "bug",
        "Known Gaps",
        "actionable",
        "Compile / AOTI",
        "triaged"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1365/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1365",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1365",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:17.770107",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Yup yup, this is a known issue/bug in pytorch/pytorch\r\n\r\nIt'll be solved when this lands: https://github.com/pytorch/pytorch/pull/140022",
          "created_at": "2024-11-12T00:52:45Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Will revisit post pin bump https://github.com/pytorch/torchchat/pull/1367",
          "created_at": "2024-11-16T02:28:18Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Picked up in pinbump",
          "created_at": "2024-12-17T23:14:41Z"
        }
      ]
    },
    {
      "issue_number": 1356,
      "title": "`weights_only` default flip for `torch.load`",
      "body": "### 🐛 Describe the bug\r\n\r\nWe've flipped the default for the `weights_only` argument in `torch.load` to `True` in `pytorch/pytorch`, see [here](https://dev-discuss.pytorch.org/t/bc-breaking-change-torch-load-is-being-flipped-to-use-weights-only-true-by-default-in-the-nightlies-after-137602/2573) for details + [documentation](https://pytorch.org/docs/main/notes/serialization.html#torch-load-with-weights-only-true) and this is coming in `torch 2.6`.\r\n\r\nThis is expected to be quite a BC-breaking change, especially if any `torch.load` calls  in `torchchat` are not loading state_dicts of plain tensors.\r\n\r\nWe should make sure that all the `torch.load` calls in `torchchat` are still working.\r\n\r\nFor example, [this one](https://github.com/pytorch/torchchat/blob/e30aaa06511bf490157544054410c468a6382d0f/torchchat/distributed/checkpoint.py#L95) does not explicitly set `weights_only` so might now be failing since `torchchat` doesn't use pytorch nightlies in CI\r\n\r\ncc @Jack-Khuu\r\n\r\n### Versions\r\n\r\nmain",
      "state": "closed",
      "author": "mikaylagawarecki",
      "author_type": "User",
      "created_at": "2024-11-07T23:47:20Z",
      "updated_at": "2024-12-17T23:14:18Z",
      "closed_at": "2024-12-17T23:14:18Z",
      "labels": [
        "enhancement",
        "actionable",
        "triaged"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1356/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1356",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1356",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:17.980957",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Fixed in pin bump",
          "created_at": "2024-12-17T23:14:18Z"
        }
      ]
    },
    {
      "issue_number": 1420,
      "title": "[KNOWN BUG] CPU AOTI Inference bug due to PT version, fix in progress",
      "body": "### 🐛 Describe the bug\n\nAs of https://github.com/pytorch/torchchat/pull/1419, `torchchat/main` is failing 2 CI jobs related to CPU AOTI inference. \r\nSpecifically, a segfault is encountered when performing AOTI inference. This is resolved in https://github.com/pytorch/pytorch/pull/139411\r\n\r\nA pin bump to pick the fix up is in progress: https://github.com/pytorch/torchchat/pull/1367\n\n### Versions\n\nhttps://github.com/pytorch/torchchat/commit/7b86dc30de050ac5f2a992c09c15ce8d7556449c",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-12-12T08:26:16Z",
      "updated_at": "2024-12-14T00:22:53Z",
      "closed_at": "2024-12-14T00:22:53Z",
      "labels": [
        "bug",
        "Known Gaps",
        "Compile / AOTI"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1420/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1420",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1420",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:18.182243",
      "comments": []
    },
    {
      "issue_number": 1416,
      "title": "MacOS test falsely uses MPS, fails and is misreported as passing",
      "body": "### 🐛 Describe the bug\r\n\r\n#1415, #1404 and all other PRs fail on test-readme-macos when torchchat apparently falsely tries to load the model to MPS.  \r\nDue to #1315 , the test don't report as failed, so things get committed anyway.\r\n\r\nI don't know why test-readme-macos would try to load into MPS.  There's a multi-layered story here, where virtualized Mac does not support MPS (I think because most likely there's no MMU for MPS, so you can't virtualize MPS).  MPS is however still reported as available by the OS, and hence a simple check `torch.backends.mps.is_available():` is not sufficient because pytorch thinks that MPS is actually available (but any and all memory allocations should fail).\r\n\r\nWe're trying to fix this by doing an allocation of a tensor in MPS memory and see if that succeeds or fails in `is_mps_available()` here => https://github.com/pytorch/torchchat/blob/main/torchchat/utils/build_utils.py#L269 when `get_device_str()` is looking to check if MPS is available, and the fastest device should be returned as MPS.\r\n\r\nIdeally, this should be fixed in the expansion `get_device_str()` and `is_mps_available()` functions, together with #1315.\r\n\r\n\r\nFail example is here => https://github.com/pytorch/torchchat/actions/runs/12243820522/job/34154220414?pr=1404\r\n\r\n```\r\n## Running via PyTorch \r\n  Downloading https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.pt...\r\n  Downloading https://github.com/karpathy/llama2.c/raw/master/tokenizer.model...\r\n  NumExpr defaulting to 6 threads.\r\n  PyTorch version 2.6.0.dev20241013 available.\r\n  Moving model to /Users/runner/.torchchat/model-cache/stories15M.\r\n  \r\n  Downloading builder script:   0%|          | 0.00/5.67k [00:00<?, ?B/s]\r\n  Downloading builder script: 100%|██████████| 5.67k/5.67k [00:00<00:00, 5.30MB/s]\r\n  Traceback (most recent call last):\r\n    File \"/Users/runner/work/torchchat/torchchat/torchchat.py\", line 96, in <module>\r\n  Using device=mps \r\n  Loading model...\r\n      generate_main(args)\r\n    File \"/Users/runner/work/torchchat/torchchat/torchchat/generate.py\", line 1235, in main\r\n      gen = Generator(\r\n    File \"/Users/runner/work/torchchat/torchchat/torchchat/generate.py\", line 293, in __init__\r\n      self.model = _initialize_model(self.builder_args, self.quantize, self.tokenizer)\r\n    File \"/Users/runner/work/torchchat/torchchat/torchchat/cli/builder.py\", line 603, in _initialize_model\r\n      model = _load_model(builder_args)\r\n    File \"/Users/runner/work/torchchat/torchchat/torchchat/cli/builder.py\", line 465, in _load_model\r\n      model = _load_model_default(builder_args)\r\n    File \"/Users/runner/work/torchchat/torchchat/torchchat/cli/builder.py\", line 427, in _load_model_default\r\n      checkpoint = _load_checkpoint(builder_args)\r\n    File \"/Users/runner/work/torchchat/torchchat/torchchat/cli/builder.py\", line 412, in _load_checkpoint\r\n      checkpoint = torch.load(\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py\", line 1359, in load\r\n      return _load(\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py\", line 1856, in _load\r\n      result = unpickler.load()\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_weights_only_unpickler.py\", line 388, in load\r\n      self.append(self.persistent_load(pid))\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py\", line 1820, in persistent_load\r\n  Time to load model: 0.10 seconds\r\n      typed_storage = load_tensor(\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py\", line 1792, in load_tensor\r\n      wrap_storage=restore_location(storage, location),\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py\", line 1693, in restore_location\r\n      return default_restore_location(storage, map_location)\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py\", line 601, in default_restore_location\r\n      result = fn(storage, location)\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py\", line 467, in _mps_deserialize\r\n      return obj.mps()\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/storage.py\", line 260, in mps\r\n      return torch.UntypedStorage(self.size(), device=\"mps\").copy_(self, False)\r\n  RuntimeError: MPS backend out of memory (MPS allocated: 1.02 GB, other allocations: 0 bytes, max allowed: 15.87 GB). Tried to allocate 256 bytes on shared pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\r\n+ echo ::group::Completion\r\n```\r\n\r\n\r\n### Versions\r\n\r\nproblem occurs in github ci/cd",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-12-11T03:45:46Z",
      "updated_at": "2024-12-12T05:35:40Z",
      "closed_at": null,
      "labels": [
        "bug",
        "CI Infra"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1416/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1416",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1416",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:19.952113",
      "comments": []
    },
    {
      "issue_number": 1413,
      "title": "llama3.2-1b-base gives 'RuntimeError: no tokenizer was found'",
      "body": "### 🐛 Describe the bug\n\nI'm following the basic 5+1 line instructions to get torchchat going with llama3.2-1b-base and it fails with `RuntimeError: no tokenizer was found at /home/tonyr/.torchchat/model-cache/meta-llama/Meta-Llama-3.2-1B/tokenizer.model`.  The 6 lines I'm using are from the README.md:\r\n```text\r\ngit clone https://github.com/pytorch/torchchat.git\r\ncd torchchat\r\npython3 -m venv .venv\r\nsource .venv/bin/activate\r\n./install/install_requirements.sh\r\npython3 torchchat.py generate llama3.2-1b-base --prompt \"write me a story about a boy and his bear\"\r\n```\r\nI also have llama3.2-3b-base which exhibits the same behaviour.  Full logs are below, note I already have llama3.2-1b-base downloaded:\r\n```text\r\n(.venv) think1 tonyr: ls -lta /home/tonyr/.torchchat/model-cache/meta-llama/Meta-Llama-3.2-1B/\r\ntotal 2424908\r\ndrwxrwxr-x 4 tonyr tonyr       4096 Dec  8 20:45 ..\r\ndrwxrwxr-x 4 tonyr tonyr       4096 Dec  8 20:40 .\r\ndrwxrwxr-x 2 tonyr tonyr       4096 Dec  8 20:40 original\r\n-rw-rw-r-- 1 tonyr tonyr 2471677246 Dec  8 20:40 model.pth\r\n-rw-rw-r-- 1 tonyr tonyr    9085657 Dec  8 20:38 tokenizer.json\r\n-rw-rw-r-- 1 tonyr tonyr    2183982 Dec  8 20:38 tokenizer.model\r\n-rw-rw-r-- 1 tonyr tonyr       1519 Dec  8 20:38 .gitattributes\r\n-rw-rw-r-- 1 tonyr tonyr      50500 Dec  8 20:38 tokenizer_config.json\r\n-rw-rw-r-- 1 tonyr tonyr        301 Dec  8 20:38 special_tokens_map.json\r\n-rw-rw-r-- 1 tonyr tonyr        185 Dec  8 20:38 generation_config.json\r\n-rw-rw-r-- 1 tonyr tonyr       7712 Dec  8 20:38 LICENSE.txt\r\n-rw-rw-r-- 1 tonyr tonyr        843 Dec  8 20:38 config.json\r\n-rw-rw-r-- 1 tonyr tonyr      41239 Dec  8 20:38 README.md\r\n-rw-rw-r-- 1 tonyr tonyr       6021 Dec  8 20:38 USE_POLICY.md\r\ndrwxrwxr-x 3 tonyr tonyr       4096 Dec  8 20:38 .cache\r\n```\r\nAlso note, there is a warning in the logs:\r\n```text\r\n+ python3 torchchat/utils/scripts/patch_triton.py\r\n/home/tonyr/torchchat/torchchat/utils/scripts/patch_triton.py:20: SyntaxWarning: invalid escape sequence '\\s'\r\n  new_match = 'self.src = self.src[re.search(r\"^def\\s+\\w+\\s*\\(\", self.src, re.MULTILINE).start():]'\r\n```\r\nand here is the full log, the interesting bit is at the end.\r\n```text\r\nthink1 tonyr: git clone https://github.com/pytorch/torchchat.git\r\nCloning into 'torchchat'...\r\nremote: Enumerating objects: 14722, done.\r\nremote: Counting objects: 100% (2820/2820), done.\r\nremote: Compressing objects: 100% (598/598), done.\r\nremote: Total 14722 (delta 2458), reused 2358 (delta 2211), pack-reused 11902 (from 1)\r\nReceiving objects: 100% (14722/14722), 8.87 MiB | 24.88 MiB/s, done.\r\nResolving deltas: 100% (8755/8755), done.\r\nthink1 tonyr: cd torchchat\r\nthink1 tonyr: python3 -m venv .venv\r\nthink1 tonyr: source .venv/bin/activate\r\n(.venv) think1 tonyr: ./install/install_requirements.sh\r\nUsing python executable: python3\r\nUsing pip executable: pip3\r\n+ pip3 install -r install/requirements.txt --extra-index-url https://download.pytorch.org/whl/nightly/cu121\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu121\r\nIgnoring tomli: markers 'python_version < \"3.11\"' don't match your environment\r\nCollecting huggingface_hub (from -r install/requirements.txt (line 4))\r\n  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\r\nCollecting gguf (from -r install/requirements.txt (line 7))\r\n  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\r\nCollecting tiktoken (from -r install/requirements.txt (line 10))\r\n  Using cached https://download.pytorch.org/whl/nightly/tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\n[ deleted loads more Collecting .. Using cached ]]\r\nUsing cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\r\nUsing cached lm_eval-0.4.2-py3-none-any.whl (1.4 MB)\r\nUsing cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\r\n\r\nUsing cached smmap-5.0.1-py3-none-any.whl (24 kB)\r\nUsing cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\r\nInstalling collected packages: zstd, word2number, sqlitedict, sentencepiece, pytz, mpmath, zstandard, xxhash, wheel, watchdog, urllib3, tzdata, typing-extensions, tqdm, tornado, toml, threadpoolctl, tenacity, tcolorpy, tabulate, sympy, sniffio, smmap, six, setuptools, safetensors, rpds-py, regex, pyyaml, pygments, pycryptodomex, pybind11, pyarrow, psutil, protobuf, propcache, portalocker, pillow, pathvalidate, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, narwhals, multidict, more-itertools, mdurl, MarkupSafe, lxml, joblib, jiter, itsdangerous, idna, h11, fsspec, frozenlist, filelock, distro, dill, colorama, cmake, click, charset-normalizer, chardet, certifi, cachetools, blinker, attrs, annotated-types, aiohappyeyeballs, absl-py, yarl, Werkzeug, triton, tqdm-multiprocess, snakeviz, scipy, sacrebleu, requests, referencing, python-dateutil, pydantic-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numexpr, nltk, multiprocess, mbstrdecoder, markdown-it-py, jsonlines, Jinja2, httpcore, gitdb, gguf, blobfile, anyio, aiosignal, typepy, tiktoken, scikit-learn, rouge-score, rich, pydeck, pydantic, pandas, nvidia-cusolver-cu12, jsonschema-specifications, huggingface_hub, httpx, gitpython, flask, aiohttp, torch, tokenizers, openai, jsonschema, transformers, datasets, DataProperty, altair, accelerate, tabledata, streamlit, peft, evaluate, pytablewriter, lm_eval\r\nSuccessfully installed DataProperty-1.0.1 Jinja2-3.1.4 MarkupSafe-3.0.2 Werkzeug-3.1.3 absl-py-2.1.0 accelerate-1.2.0 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 altair-5.5.0 annotated-types-0.7.0 anyio-4.7.0 attrs-24.2.0 blinker-1.9.0 blobfile-3.0.0 cachetools-5.5.0 certifi-2024.8.30 chardet-5.2.0 charset-normalizer-3.4.0 click-8.1.7 cmake-3.31.1 colorama-0.4.6 datasets-3.1.0 dill-0.3.8 distro-1.9.0 evaluate-0.4.3 filelock-3.16.1 flask-3.1.0 frozenlist-1.5.0 fsspec-2024.9.0 gguf-0.10.0 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface_hub-0.26.5 idna-3.10 itsdangerous-2.2.0 jiter-0.8.2 joblib-1.4.2 jsonlines-4.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 lm_eval-0.4.2 lxml-5.3.0 markdown-it-py-3.0.0 mbstrdecoder-1.1.3 mdurl-0.1.2 more-itertools-10.5.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 narwhals-1.16.0 networkx-3.4.2 ninja-1.11.1.2 nltk-3.9.1 numexpr-2.10.2 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.57.1 packaging-24.2 pandas-2.2.3 pathvalidate-3.2.1 peft-0.14.0 pillow-11.0.0 portalocker-3.0.0 propcache-0.2.1 protobuf-5.29.1 psutil-6.1.0 pyarrow-18.1.0 pybind11-2.13.6 pycryptodomex-3.21.0 pydantic-2.10.3 pydantic-core-2.27.1 pydeck-0.9.1 pygments-2.18.0 pytablewriter-1.2.0 python-dateutil-2.9.0.post0 pytz-2024.2 pyyaml-6.0.2 referencing-0.35.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 rouge-score-0.1.2 rpds-py-0.22.3 sacrebleu-2.4.3 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.14.1 sentencepiece-0.2.0 setuptools-75.6.0 six-1.17.0 smmap-5.0.1 snakeviz-2.2.2 sniffio-1.3.1 sqlitedict-2.1.0 streamlit-1.40.2 sympy-1.13.1 tabledata-1.3.3 tabulate-0.9.0 tcolorpy-0.1.6 tenacity-9.0.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.21.0 toml-0.10.2 torch-2.5.1 tornado-6.4.2 tqdm-4.67.1 tqdm-multiprocess-0.0.11 transformers-4.47.0 triton-3.1.0 typepy-1.3.2 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 watchdog-6.0.0 wheel-0.45.1 word2number-1.1 xxhash-3.5.0 yarl-1.18.3 zstandard-0.23.0 zstd-1.5.5.1\r\n+ pip3 uninstall -y triton\r\nFound existing installation: triton 3.1.0\r\nUninstalling triton-3.1.0:\r\n  Successfully uninstalled triton-3.1.0\r\n+ pip3 install --extra-index-url https://download.pytorch.org/whl/nightly/cu121 torch==2.6.0.dev20241013 torchvision==0.20.0.dev20241013 torchtune==0.4.0.dev20241013\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu121\r\nCollecting torch==2.6.0.dev20241013\r\n  Downloading https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241013%2Bcu121-cp312-cp312-linux_x86_64.whl (783.6 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 783.6/783.6 MB 1.3 MB/s eta 0:00:00\r\nCollecting torchvision==0.20.0.dev20241013\r\n  Downloading https://download.pytorch.org/whl/nightly/cu121/torchvision-0.20.0.dev20241013%2Bcu121-cp312-cp312-linux_x86_64.whl (7.4 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 10.2 MB/s eta 0:00:00\r\nCollecting torchtune==0.4.0.dev20241013\r\n  Downloading https://download.pytorch.org/whl/nightly/cu121/torchtune-0.4.0.dev20241013%2Bcu121-py3-none-any.whl (577 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 577.4/577.4 kB 1.8 MB/s eta 0:00:00\r\nRequirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (3.16.1)\r\nRequirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (4.12.2)\r\nRequirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (3.4.2)\r\nRequirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (3.1.4)\r\nRequirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (2024.9.0)\r\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (9.1.0.70)\r\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (2.21.5)\r\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\nCollecting pytorch-triton==3.1.0+cf34004b8a (from torch==2.6.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp312-cp312-linux_x86_64.whl (239.6 MB)\r\nRequirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (75.6.0)\r\nRequirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0.dev20241013) (1.13.1)\r\nRequirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision==0.20.0.dev20241013) (1.26.4)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision==0.20.0.dev20241013) (11.0.0)\r\nRequirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (3.1.0)\r\nRequirement already satisfied: huggingface-hub in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (0.26.5)\r\nRequirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (0.4.5)\r\nRequirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (0.2.0)\r\nRequirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (0.8.0)\r\nRequirement already satisfied: blobfile>=2 in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (3.0.0)\r\nRequirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (4.67.1)\r\nCollecting omegaconf (from torchtune==0.4.0.dev20241013)\r\n  Using cached https://download.pytorch.org/whl/nightly/omegaconf-2.3.0-py3-none-any.whl (79 kB)\r\nRequirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from torchtune==0.4.0.dev20241013) (6.1.0)\r\nRequirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.6.0.dev20241013) (12.4.127)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0.dev20241013) (1.3.0)\r\nRequirement already satisfied: pycryptodomex>=3.8 in ./.venv/lib/python3.12/site-packages (from blobfile>=2->torchtune==0.4.0.dev20241013) (3.21.0)\r\nRequirement already satisfied: urllib3<3,>=1.25.3 in ./.venv/lib/python3.12/site-packages (from blobfile>=2->torchtune==0.4.0.dev20241013) (2.2.3)\r\nRequirement already satisfied: lxml>=4.9 in ./.venv/lib/python3.12/site-packages (from blobfile>=2->torchtune==0.4.0.dev20241013) (5.3.0)\r\nRequirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (18.1.0)\r\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (0.3.8)\r\nRequirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (2.2.3)\r\nRequirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (2.32.3)\r\nRequirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (3.5.0)\r\nRequirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (0.70.16)\r\nRequirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (3.11.10)\r\nRequirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (24.2)\r\nRequirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets->torchtune==0.4.0.dev20241013) (6.0.2)\r\nRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.6.0.dev20241013) (3.0.2)\r\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune==0.4.0.dev20241013)\r\n  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\r\nRequirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken->torchtune==0.4.0.dev20241013) (2024.11.6)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0.dev20241013) (2.4.4)\r\nRequirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0.dev20241013) (1.3.1)\r\nRequirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0.dev20241013) (24.2.0)\r\nRequirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0.dev20241013) (1.5.0)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0.dev20241013) (6.1.0)\r\nRequirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0.dev20241013) (0.2.1)\r\nRequirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0.dev20241013) (1.18.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune==0.4.0.dev20241013) (3.4.0)\r\nRequirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune==0.4.0.dev20241013) (3.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune==0.4.0.dev20241013) (2024.8.30)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0.dev20241013) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0.dev20241013) (2024.2)\r\nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0.dev20241013) (2024.2)\r\nRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune==0.4.0.dev20241013) (1.17.0)\r\nInstalling collected packages: antlr4-python3-runtime, pytorch-triton, omegaconf, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchvision, torchtune\r\n  Attempting uninstall: nvidia-nvtx-cu12\r\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\r\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\r\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\r\n  Attempting uninstall: nvidia-cusparse-cu12\r\n    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\r\n    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\r\n      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\r\n  Attempting uninstall: nvidia-curand-cu12\r\n    Found existing installation: nvidia-curand-cu12 10.3.5.147\r\n    Uninstalling nvidia-curand-cu12-10.3.5.147:\r\n      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\r\n  Attempting uninstall: nvidia-cufft-cu12\r\n    Found existing installation: nvidia-cufft-cu12 11.2.1.3\r\n    Uninstalling nvidia-cufft-cu12-11.2.1.3:\r\n      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\r\n  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\r\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\r\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\r\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\r\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\r\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\r\n  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\r\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\r\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\r\n  Attempting uninstall: nvidia-cublas-cu12\r\n    Found existing installation: nvidia-cublas-cu12 12.4.5.8\r\n    Uninstalling nvidia-cublas-cu12-12.4.5.8:\r\n      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\r\n  Attempting uninstall: nvidia-cusolver-cu12\r\n    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\r\n    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\r\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\r\n  Attempting uninstall: torch\r\n    Found existing installation: torch 2.5.1\r\n    Uninstalling torch-2.5.1:\r\n      Successfully uninstalled torch-2.5.1\r\nSuccessfully installed antlr4-python3-runtime-4.9.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 pytorch-triton-3.1.0+cf34004b8a torch-2.6.0.dev20241013+cu121 torchtune-0.4.0.dev20241013+cu121 torchvision-0.20.0.dev20241013+cu121\r\n+ pip3 install torchao==0.5.0\r\nCollecting torchao==0.5.0\r\n  Using cached torchao-0.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\nUsing cached torchao-0.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\nInstalling collected packages: torchao\r\nSuccessfully installed torchao-0.5.0\r\n+ python3 torchchat/utils/scripts/patch_triton.py\r\n/home/tonyr/torchchat/torchchat/utils/scripts/patch_triton.py:20: SyntaxWarning: invalid escape sequence '\\s'\r\n  new_match = 'self.src = self.src[re.search(r\"^def\\s+\\w+\\s*\\(\", self.src, re.MULTILINE).start():]'\r\n+ pip3 install evaluate==0.4.3 lm-eval==0.4.2 psutil==6.0.0\r\nRequirement already satisfied: evaluate==0.4.3 in ./.venv/lib/python3.12/site-packages (0.4.3)\r\nRequirement already satisfied: lm-eval==0.4.2 in ./.venv/lib/python3.12/site-packages (0.4.2)\r\nCollecting psutil==6.0.0\r\n  Using cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\r\nRequirement already satisfied: datasets>=2.0.0 in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (3.1.0)\r\nRequirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (1.26.4)\r\nRequirement already satisfied: dill in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (0.3.8)\r\nRequirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (2.2.3)\r\nRequirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (2.32.3)\r\nRequirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (4.67.1)\r\nRequirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (3.5.0)\r\nRequirement already satisfied: multiprocess in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (0.70.16)\r\nRequirement already satisfied: fsspec>=2021.05.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.3) (2024.9.0)\r\nRequirement already satisfied: huggingface-hub>=0.7.0 in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (0.26.5)\r\nRequirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from evaluate==0.4.3) (24.2)\r\nRequirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (1.2.0)\r\nRequirement already satisfied: jsonlines in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (4.0.0)\r\nRequirement already satisfied: numexpr in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (2.10.2)\r\nRequirement already satisfied: peft>=0.2.0 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (0.14.0)\r\nRequirement already satisfied: pybind11>=2.6.2 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (2.13.6)\r\nRequirement already satisfied: pytablewriter in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (1.2.0)\r\nRequirement already satisfied: rouge-score>=0.0.4 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (0.1.2)\r\nRequirement already satisfied: sacrebleu>=1.5.0 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (2.4.3)\r\nRequirement already satisfied: scikit-learn>=0.24.1 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (1.6.0)\r\nRequirement already satisfied: sqlitedict in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (2.1.0)\r\nRequirement already satisfied: torch>=1.8 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (2.6.0.dev20241013+cu121)\r\nRequirement already satisfied: tqdm-multiprocess in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (0.0.11)\r\nRequirement already satisfied: transformers>=4.1 in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (4.47.0)\r\nRequirement already satisfied: zstandard in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (0.23.0)\r\nRequirement already satisfied: word2number in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (1.1)\r\nRequirement already satisfied: more-itertools in ./.venv/lib/python3.12/site-packages (from lm-eval==0.4.2) (10.5.0)\r\nRequirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2) (6.0.2)\r\nRequirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2) (0.4.5)\r\nRequirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate==0.4.3) (3.16.1)\r\nRequirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate==0.4.3) (18.1.0)\r\nRequirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate==0.4.3) (3.11.10)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.3) (4.12.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate==0.4.3) (3.4.0)\r\nRequirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate==0.4.3) (3.10)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate==0.4.3) (2.2.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate==0.4.3) (2024.8.30)\r\nRequirement already satisfied: absl-py in ./.venv/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2) (2.1.0)\r\nRequirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2) (3.9.1)\r\nRequirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2) (1.17.0)\r\nRequirement already satisfied: portalocker in ./.venv/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2) (3.0.0)\r\nRequirement already satisfied: regex in ./.venv/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2) (2024.11.6)\r\nRequirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2) (0.9.0)\r\nRequirement already satisfied: colorama in ./.venv/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2) (0.4.6)\r\nRequirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2) (5.3.0)\r\nRequirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2) (1.14.1)\r\nRequirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2) (3.5.0)\r\nRequirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (3.4.2)\r\nRequirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (3.1.4)\r\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (12.1.105)\r\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (12.1.105)\r\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (12.1.105)\r\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (9.1.0.70)\r\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (12.1.3.1)\r\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (11.0.2.54)\r\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (10.3.2.106)\r\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (11.4.5.107)\r\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (12.1.0.106)\r\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (2.21.5)\r\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (12.1.105)\r\nRequirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (3.1.0+cf34004b8a)\r\nRequirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (75.6.0)\r\nRequirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.8->lm-eval==0.4.2) (1.13.1)\r\nRequirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm-eval==0.4.2) (12.4.127)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.8->lm-eval==0.4.2) (1.3.0)\r\nRequirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers>=4.1->lm-eval==0.4.2) (0.21.0)\r\nRequirement already satisfied: attrs>=19.2.0 in ./.venv/lib/python3.12/site-packages (from jsonlines->lm-eval==0.4.2) (24.2.0)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->evaluate==0.4.3) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->evaluate==0.4.3) (2024.2)\r\nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->evaluate==0.4.3) (2024.2)\r\nRequirement already satisfied: DataProperty<2,>=1.0.1 in ./.venv/lib/python3.12/site-packages (from pytablewriter->lm-eval==0.4.2) (1.0.1)\r\nRequirement already satisfied: mbstrdecoder<2,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from pytablewriter->lm-eval==0.4.2) (1.1.3)\r\nRequirement already satisfied: pathvalidate<4,>=2.3.0 in ./.venv/lib/python3.12/site-packages (from pytablewriter->lm-eval==0.4.2) (3.2.1)\r\nRequirement already satisfied: tabledata<2,>=1.3.1 in ./.venv/lib/python3.12/site-packages (from pytablewriter->lm-eval==0.4.2) (1.3.3)\r\nRequirement already satisfied: tcolorpy<1,>=0.0.5 in ./.venv/lib/python3.12/site-packages (from pytablewriter->lm-eval==0.4.2) (0.1.6)\r\nRequirement already satisfied: typepy<2,>=1.3.2 in ./.venv/lib/python3.12/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.2) (1.3.2)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (2.4.4)\r\nRequirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (1.3.1)\r\nRequirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (1.5.0)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (6.1.0)\r\nRequirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (0.2.1)\r\nRequirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (1.18.3)\r\nRequirement already satisfied: chardet<6,>=3.0.4 in ./.venv/lib/python3.12/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.4.2) (5.2.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.8->lm-eval==0.4.2) (3.0.2)\r\nRequirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk->rouge-score>=0.0.4->lm-eval==0.4.2) (8.1.7)\r\nUsing cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\r\nInstalling collected packages: psutil\r\n  Attempting uninstall: psutil\r\n    Found existing installation: psutil 6.1.0\r\n    Uninstalling psutil-6.1.0:\r\n      Successfully uninstalled psutil-6.1.0\r\nSuccessfully installed psutil-6.0.0\r\n(failed reverse-i-search)`download': llama model ^Cwnload --source meta --model-id Llama3.2-3B \r\n(.venv) think1 tonyr: python3 torchchat.py list\r\n\r\nModel                                        Aliases                                                    Downloaded \r\n-------------------------------------------- ---------------------------------------------------------- -----------\r\nmeta-llama/llama-2-7b-hf                     llama2-base, llama2-7b                                                \r\nmeta-llama/llama-2-7b-chat-hf                llama2, llama2-chat, llama2-7b-chat                                   \r\nmeta-llama/llama-2-13b-chat-hf               llama2-13b-chat                                                       \r\nmeta-llama/llama-2-70b-chat-hf               llama2-70b-chat                                                       \r\nmeta-llama/meta-llama-3-8b                   llama3-base                                                           \r\nmeta-llama/meta-llama-3-8b-instruct          llama3, llama3-chat, llama3-instruct                                  \r\nmeta-llama/meta-llama-3-70b-instruct         llama3-70b                                                            \r\nmeta-llama/meta-llama-3.1-8b                 llama3.1-base                                                         \r\nmeta-llama/meta-llama-3.1-8b-instruct        llama3.1, llama3.1-chat, llama3.1-instruct                            \r\nmeta-llama/meta-llama-3.1-70b-instruct       llama3.1-70b                                                          \r\nmeta-llama/meta-llama-3.1-8b-instruct-tune   llama3.1-tune, llama3.1-chat-tune, llama3.1-instruct-tune             \r\nmeta-llama/meta-llama-3.1-70b-instruct-tune  llama3.1-70b-tune                                                     \r\nmeta-llama/meta-llama-3.2-1b                 llama3.2-1b-base                                           Yes        \r\nmeta-llama/meta-llama-3.2-1b-instruct        llama3.2-1b, llama3.2-1b-chat, llama3.2-1b-instruct                   \r\nmeta-llama/llama-guard-3-1b                  llama3-1b-guard, llama3.2-1b-guard                                    \r\nmeta-llama/meta-llama-3.2-3b                 llama3.2-3b-base                                           Yes        \r\nmeta-llama/meta-llama-3.2-3b-instruct        llama3.2-3b, llama3.2-3b-chat, llama3.2-3b-instruct                   \r\nmeta-llama/llama-3.2-11b-vision              llama3.2-11B-base, Llama-3.2-11B-Vision-base                          \r\nmeta-llama/llama-3.2-11b-vision-instruct     llama3.2-11B, Llama-3.2-11B-Vision, Llama-3.2-mm                      \r\nmeta-llama/codellama-7b-python-hf            codellama, codellama-7b                                               \r\nmeta-llama/codellama-34b-python-hf           codellama-34b                                                         \r\nmistralai/mistral-7b-v0.1                    mistral-7b-v01-base                                                   \r\nmistralai/mistral-7b-instruct-v0.1           mistral-7b-v01-instruct                                               \r\nmistralai/mistral-7b-instruct-v0.2           mistral, mistral-7b, mistral-7b-instruct                              \r\nopenlm-research/open_llama_7b                open-llama, open-llama-7b                                             \r\nstories15m                                                                                                         \r\nstories42m                                                                                                         \r\nstories110m                                                                                                        \r\n\r\n(.venv) think1 tonyr: python3 torchchat.py generate llama3.2-1b-base --prompt \"write me a story about a boy and his bear\"\r\nNumExpr defaulting to 12 threads.\r\nPyTorch version 2.6.0.dev20241013+cu121 available.\r\nUsing device=cuda NVIDIA GeForce RTX 4070 Ti\r\nLoading model...\r\nTime to load model: 0.90 seconds\r\n-----------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/tonyr/torchchat/torchchat.py\", line 96, in <module>\r\n    generate_main(args)\r\n  File \"/home/tonyr/torchchat/torchchat/generate.py\", line 1235, in main\r\n    gen = Generator(\r\n          ^^^^^^^^^^\r\n  File \"/home/tonyr/torchchat/torchchat/generate.py\", line 311, in __init__\r\n    self.tokenizer_args.validate_model(self.model)\r\n  File \"/home/tonyr/torchchat/torchchat/cli/builder.py\", line 264, in validate_model\r\n    raise RuntimeError(f\"no tokenizer was found at {self.tokenizer_path}\")\r\nRuntimeError: no tokenizer was found at /home/tonyr/.torchchat/model-cache/meta-llama/Meta-Llama-3.2-1B/tokenizer.model\r\n```\n\n### Versions\n\n```text\r\n(.venv) think1 tonyr: python3 ~/collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241013+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.1\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Ti\r\nNvidia driver version: 550.127.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               12\r\nOn-line CPU(s) list:                  0-11\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Core(TM) i7-3930K CPU @ 3.20GHz\r\nCPU family:                           6\r\nModel:                                45\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   6\r\nSocket(s):                            1\r\nStepping:                             7\r\nCPU(s) scaling MHz:                   78%\r\nCPU max MHz:                          3800.0000\r\nCPU min MHz:                          1200.0000\r\nBogoMIPS:                             6403.76\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf_lm epb pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid xsaveopt dtherm ida arat pln pts vnmi md_clear flush_l1d\r\nVirtualization:                       VT-x\r\nL1d cache:                            192 KiB (6 instances)\r\nL1i cache:                            192 KiB (6 instances)\r\nL2 cache:                             1.5 MiB (6 instances)\r\nL3 cache:                             12 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-11\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:               Mitigation; PTI\r\nVulnerability Mmio stale data:        Unknown: No mitigations\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241013+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.4.0.dev20241013+cu121\r\n[pip3] torchvision==0.20.0.dev20241013+cu121\r\n[conda] Could not collect\r\n(.venv) think1 tonyr: \r\n```",
      "state": "closed",
      "author": "drtonyr",
      "author_type": "User",
      "created_at": "2024-12-10T11:11:27Z",
      "updated_at": "2024-12-11T09:14:04Z",
      "closed_at": "2024-12-10T19:36:47Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1413/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1413",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1413",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:19.952134",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Hmm looks like the migration https://github.com/pytorch/torchchat/commit/fff956c5c3a68025025c5d906af80eb44e960ce4 messed something up\r\n\r\nTaking a look",
          "created_at": "2024-12-10T17:33:07Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Reverting the cause https://github.com/pytorch/torchchat/pull/1414",
          "created_at": "2024-12-10T18:20:09Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Should work now, feel free to reopen and let us know if it doesn't",
          "created_at": "2024-12-10T19:36:47Z"
        },
        {
          "author": "drtonyr",
          "body": "Fantastic, it now works for me - thanks so much for the quick fix.",
          "created_at": "2024-12-11T09:14:03Z"
        }
      ]
    },
    {
      "issue_number": 1388,
      "title": "eval doc does not pass test",
      "body": "### 🐛 Describe the bug\n\nhttps://github.com/pytorch/torchchat/pull/1383 enables `run-docs evaluation` to extract a test script from eval documentation,\r\nto run evaluation script.  In turn, this extracts the command\r\n\r\n```\r\npython3 torchchat.py eval stories15M --tasks wikitext --limit 10\r\n```\r\n\r\nfrom the eval doc as a test to ensure that the doc is in fact correct.  This appears to be a correct use of eval to me, yet it fails when running as follows:\r\n\r\nhttps://hud.pytorch.org/pr/pytorch/torchchat/1383#33154706429\r\n\r\n```\r\n2024-11-18T18:13:35.1710781Z + python3 torchchat.py eval stories15M --tasks wikitext --limit 10\r\n2024-11-18T18:13:35.1711201Z NumExpr defaulting to 16 threads.\r\n2024-11-18T18:13:35.1711531Z PyTorch version 2.6.0.dev20241002+cu121 available.\r\n2024-11-18T18:13:35.1711768Z \r\n2024-11-18T18:13:35.1711939Z Downloading builder script:   0% 0.00/5.67k [00:00<?, ?B/s]\r\n2024-11-18T18:13:35.1712401Z Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 37.1MB/s]\r\n2024-11-18T18:13:35.1712808Z Traceback (most recent call last):\r\n2024-11-18T18:13:35.1713182Z   File \"/pytorch/torchchat/torchchat.py\", line 100, in <module>\r\n2024-11-18T18:13:35.1713552Z     eval_main(args)\r\n2024-11-18T18:13:35.1713905Z   File \"/pytorch/torchchat/torchchat/usages/eval.py\", line 238, in main\r\n2024-11-18T18:13:35.1714340Z     builder_args = BuilderArgs.from_args(args)\r\n2024-11-18T18:13:35.1714667Z                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-18T18:13:35.1715101Z   File \"/pytorch/torchchat/torchchat/cli/builder.py\", line 169, in from_args\r\n2024-11-18T18:13:35.1715520Z     return cls(\r\n2024-11-18T18:13:35.1715827Z     run_cmd_or_die(f\"docker exec -t {container_name} /exec\")\r\n2024-11-18T18:13:35.1716580Z   File \"/home/ec2-user/actions-runner/_work/torchchat/torchchat/test-infra/.github/scripts/run_with_env_secrets.py\", line 39, in run_cmd_or_die\r\n2024-11-18T18:13:35.1717388Z     raise RuntimeError(f\"Command {cmd} failed with exit code {exit_code}\")\r\n2024-11-18T18:13:35.1718153Z RuntimeError: Command docker exec -t c2e4cff2805edb5848301b09ed712578d726414222642162007e0e16e7c48ba1 /exec failed with exit code 1\r\n2024-11-18T18:13:35.1718786Z            ^^^^\r\n2024-11-18T18:13:35.1719026Z   File \"<string>\", line 24, in __init__\r\n2024-11-18T18:13:35.1719475Z   File \"/pytorch/torchchat/torchchat/cli/builder.py\", line 76, in __post_init__\r\n2024-11-18T18:13:35.1719926Z     raise RuntimeError(\r\n2024-11-18T18:13:35.1720431Z RuntimeError: need to specified a valid checkpoint path, checkpoint dir, gguf path, DSO path, or PTE path\r\n```\r\n\r\n\r\n\r\n\n\n### Versions\n\ngithub runner, environment as configured by pytorch test infra",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-11-19T05:38:54Z",
      "updated_at": "2024-12-10T04:41:51Z",
      "closed_at": "2024-12-10T04:41:50Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1388/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1388",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1388",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:20.230664",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "@vmpuri Can you take a look?\r\n\r\nMaybe the doc is outdated",
          "created_at": "2024-11-19T15:33:34Z"
        },
        {
          "author": "mikekgfb",
          "body": "Resolved by landing #1383 ",
          "created_at": "2024-12-10T04:41:50Z"
        }
      ]
    },
    {
      "issue_number": 1385,
      "title": "Update dead link in https://github.com/pytorch/torchchat/blob/main/docs/quantization.md",
      "body": "### 🐛 Describe the bug\n\nThere is a dead link https://github.com/pytorch/torchchat/blob/main/torchchat/utils/quantize.py#L1260-L1266 in https://github.com/pytorch/torchchat/blob/main/docs/quantization.md like `See the available quantization schemes [here](https://github.com/pytorch/torchchat/blob/main/torchchat/utils/quantize.py#L1260-L1266).`. Could you please help update it to show the quantization schemes examples?\n\n### Versions\n\n#",
      "state": "closed",
      "author": "yanbing-j",
      "author_type": "User",
      "created_at": "2024-11-19T01:34:54Z",
      "updated_at": "2024-12-09T22:37:22Z",
      "closed_at": "2024-12-09T22:37:22Z",
      "labels": [
        "documentation",
        "Quantization"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1385/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1385",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1385",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:20.515039",
      "comments": [
        {
          "author": "yanbing-j",
          "body": "cc @mikekgfb @mingfeima",
          "created_at": "2024-11-19T01:35:41Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Link looks live to me (redirects to code)? \r\n\r\nThat said we doc in the file can be polished\r\n\r\ncc: @vmpuri ",
          "created_at": "2024-11-19T15:36:38Z"
        },
        {
          "author": "yanbing-j",
          "body": "In main branch, torchchat/utils/quantize.py only has 942 lines.",
          "created_at": "2024-11-20T02:09:20Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Ah good catch",
          "created_at": "2024-11-22T06:14:16Z"
        }
      ]
    },
    {
      "issue_number": 1408,
      "title": "install/install_requirements.sh references *_NIGHTLY_VERSION=dev20241010 which is not available",
      "body": "### 🐛 Describe the bug\n\nI followed the install instructions up to and including running install/install_requirements.sh (output below).  This script references three nightly versions called dev20241010.  These versions are not available at https://download.pytorch.org/whl/nightly/torch/ which seems to only have the last month.\r\n```text\r\nthink0 tonyr: git clone https://github.com/pytorch/torchchat.git\r\nCloning into 'torchchat'...\r\nremote: Enumerating objects: 14681, done.\r\nremote: Counting objects: 100% (2463/2463), done.\r\nremote: Compressing objects: 100% (601/601), done.\r\nremote: Total 14681 (delta 2109), reused 1978 (delta 1854), pack-reused 12218 (from 1)\r\nReceiving objects: 100% (14681/14681), 8.73 MiB | 25.77 MiB/s, done.\r\nResolving deltas: 100% (8717/8717), done.\r\nthink0 tonyr: cd torchchat\r\nthink0 tonyr: python3 -m venv .venv\r\nthink0 tonyr: source .venv/bin/activate\r\n(.venv) think0 tonyr: ./install/install_requirements.sh\r\nUsing python executable: python3\r\nUsing pip executable: pip3\r\n+ pip3 install -r install/requirements.txt --extra-index-url https://download.pytorch.org/whl/nightly/cu121\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu121\r\nIgnoring tomli: markers 'python_version < \"3.11\"' don't match your environment\r\nCollecting huggingface_hub (from -r install/requirements.txt (line 4))\r\n  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\r\nCollecting gguf (from -r install/requirements.txt (line 7))\r\n  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\r\nCollecting tiktoken (from -r install/requirements.txt (line 10))\r\n  Using cached https://download.pytorch.org/whl/nightly/tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\nCollecting snakeviz (from -r install/requirements.txt (line 13))\r\n  Using cached snakeviz-2.2.2-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting sentencepiece (from -r install/requirements.txt (line 14))\r\n  Using cached https://download.pytorch.org/whl/nightly/sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\nCollecting numpy<2.0,>=1.17 (from -r install/requirements.txt (line 16))\r\n  Using cached https://download.pytorch.org/whl/nightly/numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\r\nCollecting blobfile (from -r install/requirements.txt (line 17))\r\n  Using cached https://download.pytorch.org/whl/nightly/blobfile-3.0.0-py3-none-any.whl (75 kB)\r\nCollecting openai (from -r install/requirements.txt (line 19))\r\n  Using cached openai-1.57.1-py3-none-any.whl.metadata (24 kB)\r\nCollecting wheel (from -r install/requirements.txt (line 22))\r\n  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\r\nCollecting cmake>=3.24 (from -r install/requirements.txt (line 23))\r\n  Using cached cmake-3.31.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\r\nCollecting ninja (from -r install/requirements.txt (line 24))\r\n  Using cached ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\r\nCollecting zstd (from -r install/requirements.txt (line 25))\r\n  Using cached zstd-1.5.5.1-cp312-cp312-linux_x86_64.whl\r\nCollecting streamlit (from -r install/requirements.txt (line 28))\r\n  Using cached streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\r\nCollecting flask (from -r install/requirements.txt (line 31))\r\n  Using cached flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\r\nCollecting lm_eval==0.4.2 (from -r install/requirements.txt (line 34))\r\n  Using cached lm_eval-0.4.2-py3-none-any.whl.metadata (30 kB)\r\nCollecting accelerate>=0.21.0 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\r\nCollecting evaluate (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\r\nCollecting datasets>=2.16.0 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\r\nCollecting jsonlines (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting numexpr (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached numexpr-2.10.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\r\nCollecting peft>=0.2.0 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting pybind11>=2.6.2 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\r\nCollecting pytablewriter (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\r\nCollecting rouge-score>=0.0.4 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached rouge_score-0.1.2-py3-none-any.whl\r\nCollecting sacrebleu>=1.5.0 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\r\nCollecting scikit-learn>=0.24.1 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\nCollecting sqlitedict (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached sqlitedict-2.1.0-py3-none-any.whl\r\nCollecting torch>=1.8 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\r\nCollecting tqdm-multiprocess (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting transformers>=4.1 (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\r\nCollecting zstandard (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\nCollecting dill (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/dill-0.3.9-py3-none-any.whl (119 kB)\r\nCollecting word2number (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached word2number-1.1-py3-none-any.whl\r\nCollecting more-itertools (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\r\nCollecting filelock (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached https://download.pytorch.org/whl/nightly/filelock-3.16.1-py3-none-any.whl (16 kB)\r\nCollecting fsspec>=2023.5.0 (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached https://download.pytorch.org/whl/nightly/fsspec-2024.10.0-py3-none-any.whl (179 kB)\r\nCollecting packaging>=20.9 (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\r\nCollecting pyyaml>=5.1 (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached https://download.pytorch.org/whl/nightly/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\r\nCollecting requests (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached https://download.pytorch.org/whl/nightly/requests-2.32.3-py3-none-any.whl (64 kB)\r\nCollecting tqdm>=4.42.1 (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\nCollecting typing-extensions>=3.7.4.3 (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached https://download.pytorch.org/whl/nightly/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nCollecting regex>=2022.1.18 (from tiktoken->-r install/requirements.txt (line 10))\r\n  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\r\nCollecting tornado>=2.0 (from snakeviz->-r install/requirements.txt (line 13))\r\n  Using cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\r\nCollecting pycryptodomex>=3.8 (from blobfile->-r install/requirements.txt (line 17))\r\n  Using cached https://download.pytorch.org/whl/nightly/pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\r\nCollecting urllib3<3,>=1.25.3 (from blobfile->-r install/requirements.txt (line 17))\r\n  Using cached https://download.pytorch.org/whl/nightly/urllib3-2.2.3-py3-none-any.whl (126 kB)\r\nCollecting lxml>=4.9 (from blobfile->-r install/requirements.txt (line 17))\r\n  Using cached https://download.pytorch.org/whl/nightly/lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\r\nCollecting anyio<5,>=3.5.0 (from openai->-r install/requirements.txt (line 19))\r\n  Using cached anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\r\nCollecting distro<2,>=1.7.0 (from openai->-r install/requirements.txt (line 19))\r\n  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\nCollecting httpx<1,>=0.23.0 (from openai->-r install/requirements.txt (line 19))\r\n  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\r\nCollecting jiter<1,>=0.4.0 (from openai->-r install/requirements.txt (line 19))\r\n  Downloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\r\nCollecting pydantic<3,>=1.9.0 (from openai->-r install/requirements.txt (line 19))\r\n  Using cached pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\r\nCollecting sniffio (from openai->-r install/requirements.txt (line 19))\r\n  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\r\nCollecting altair<6,>=4.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\r\nCollecting blinker<2,>=1.0.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting cachetools<6,>=4.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\r\nCollecting click<9,>=7.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting pandas<3,>=1.4.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached https://download.pytorch.org/whl/nightly/pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\r\nCollecting pillow<12,>=7.1.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached https://download.pytorch.org/whl/nightly/pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\r\nCollecting protobuf<6,>=3.20 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\r\nCollecting pyarrow>=7.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\nCollecting rich<14,>=10.14.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\r\nCollecting tenacity<10,>=8.1.0 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\r\nCollecting toml<2,>=0.10.1 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\r\nCollecting watchdog<7,>=2.1.5 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\r\nCollecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\r\nCollecting pydeck<1,>=0.8.0b4 (from streamlit->-r install/requirements.txt (line 28))\r\n  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\r\nCollecting Werkzeug>=3.1 (from flask->-r install/requirements.txt (line 31))\r\n  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\r\nCollecting Jinja2>=3.1.2 (from flask->-r install/requirements.txt (line 31))\r\n  Using cached https://download.pytorch.org/whl/nightly/jinja2-3.1.4-py3-none-any.whl (133 kB)\r\nCollecting itsdangerous>=2.2 (from flask->-r install/requirements.txt (line 31))\r\n  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\r\nCollecting psutil (from accelerate>=0.21.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\r\nCollecting safetensors>=0.4.3 (from accelerate>=0.21.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\r\nCollecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\r\nCollecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached narwhals-1.16.0-py3-none-any.whl.metadata (8.3 kB)\r\nCollecting idna>=2.8 (from anyio<5,>=3.5.0->openai->-r install/requirements.txt (line 19))\r\n  Using cached https://download.pytorch.org/whl/nightly/idna-3.10-py3-none-any.whl (70 kB)\r\nCollecting dill (from lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/dill-0.3.8-py3-none-any.whl (116 kB)\r\nCollecting xxhash (from datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\nCollecting multiprocess<0.70.17 (from datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/multiprocess-0.70.16-py312-none-any.whl (146 kB)\r\nCollecting fsspec>=2023.5.0 (from huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\r\nCollecting aiohttp (from datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\nCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\r\nCollecting certifi (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 19))\r\n  Using cached https://download.pytorch.org/whl/nightly/certifi-2024.8.30-py3-none-any.whl (167 kB)\r\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 19))\r\n  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\r\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 19))\r\n  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\r\nCollecting MarkupSafe>=2.0 (from Jinja2>=3.1.2->flask->-r install/requirements.txt (line 31))\r\n  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\r\nCollecting python-dateutil>=2.8.2 (from pandas<3,>=1.4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\r\nCollecting pytz>=2020.1 (from pandas<3,>=1.4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached https://download.pytorch.org/whl/nightly/pytz-2024.2-py2.py3-none-any.whl (508 kB)\r\nCollecting tzdata>=2022.7 (from pandas<3,>=1.4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached https://download.pytorch.org/whl/nightly/tzdata-2024.2-py2.py3-none-any.whl (346 kB)\r\nCollecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 19))\r\n  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\nCollecting pydantic-core==2.27.1 (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 19))\r\n  Using cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\nCollecting charset-normalizer<4,>=2 (from requests->huggingface_hub->-r install/requirements.txt (line 4))\r\n  Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\r\nCollecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\nCollecting pygments<3.0.0,>=2.13.0 (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\r\nCollecting absl-py (from rouge-score>=0.0.4->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\r\nCollecting nltk (from rouge-score>=0.0.4->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting six>=1.14.0 (from rouge-score>=0.0.4->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\r\nCollecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\r\nCollecting tabulate>=0.8.9 (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\nCollecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\nCollecting scipy>=1.6.0 (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\nCollecting joblib>=1.2.0 (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\nCollecting threadpoolctl>=3.1.0 (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting networkx (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/networkx-3.4.2-py3-none-any.whl (1.7 MB)\r\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\nCollecting nvidia-nccl-cu12==2.21.5 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/cu121/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\r\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\r\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\nCollecting triton==3.1.0 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\r\nCollecting setuptools (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\r\nCollecting sympy==1.13.1 (from torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\nCollecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.8->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/mpmath-1.3.0-py3-none-any.whl (536 kB)\r\nCollecting tokenizers<0.22,>=0.21 (from transformers>=4.1->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\nCollecting attrs>=19.2.0 (from jsonlines->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/attrs-24.2.0-py3-none-any.whl (63 kB)\r\nCollecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\r\nCollecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\r\nCollecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\r\nCollecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\r\nCollecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\r\nCollecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\r\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\r\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached https://download.pytorch.org/whl/nightly/multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\r\nCollecting propcache>=0.2.0 (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\r\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\r\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\r\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\r\nCollecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached rpds_py-0.22.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\r\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 28))\r\n  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.2->-r install/requirements.txt (line 34))\r\n  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\r\nUsing cached lm_eval-0.4.2-py3-none-any.whl (1.4 MB)\r\nUsing cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\r\nUsing cached gguf-0.10.0-py3-none-any.whl (71 kB)\r\nUsing cached snakeviz-2.2.2-py3-none-any.whl (183 kB)\r\nUsing cached openai-1.57.1-py3-none-any.whl (389 kB)\r\nUsing cached wheel-0.45.1-py3-none-any.whl (72 kB)\r\nUsing cached cmake-3.31.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\r\nUsing cached ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\r\nUsing cached streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\r\nUsing cached flask-3.1.0-py3-none-any.whl (102 kB)\r\nUsing cached accelerate-1.2.0-py3-none-any.whl (336 kB)\r\nUsing cached altair-5.5.0-py3-none-any.whl (731 kB)\r\nUsing cached anyio-4.7.0-py3-none-any.whl (93 kB)\r\nUsing cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\r\nUsing cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\r\nUsing cached click-8.1.7-py3-none-any.whl (97 kB)\r\nUsing cached datasets-3.1.0-py3-none-any.whl (480 kB)\r\nUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\r\nUsing cached evaluate-0.4.3-py3-none-any.whl (84 kB)\r\nUsing cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\r\nUsing cached GitPython-3.1.43-py3-none-any.whl (207 kB)\r\nUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\r\nUsing cached httpcore-1.0.7-py3-none-any.whl (78 kB)\r\nUsing cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\r\nDownloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.5/345.5 kB 1.1 MB/s eta 0:00:00\r\nUsing cached packaging-24.2-py3-none-any.whl (65 kB)\r\nUsing cached peft-0.14.0-py3-none-any.whl (374 kB)\r\nUsing cached protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\r\nUsing cached pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\r\nUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\r\nUsing cached pydantic-2.10.3-py3-none-any.whl (456 kB)\r\nUsing cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\r\nUsing cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\r\nUsing cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\r\nUsing cached rich-13.9.4-py3-none-any.whl (242 kB)\r\nUsing cached sacrebleu-2.4.3-py3-none-any.whl (103 kB)\r\nUsing cached scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\r\nUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\r\nUsing cached tenacity-9.0.0-py3-none-any.whl (28 kB)\r\nUsing cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\nUsing cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\r\nUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\nUsing cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\nUsing cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\nUsing cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\nUsing cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\nUsing cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\nUsing cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\nUsing cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\nUsing cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\nUsing cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\r\nUsing cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\r\nUsing cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\r\nUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\nUsing cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\r\nUsing cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\r\nUsing cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\r\nUsing cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\r\nUsing cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\r\nUsing cached numexpr-2.10.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (400 kB)\r\nUsing cached pytablewriter-1.2.0-py3-none-any.whl (111 kB)\r\nUsing cached tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\r\nUsing cached zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\r\nUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\nUsing cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\r\nUsing cached DataProperty-1.0.1-py3-none-any.whl (27 kB)\r\nUsing cached aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\nUsing cached gitdb-4.0.11-py3-none-any.whl (62 kB)\r\nUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\r\nUsing cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\r\nUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\nUsing cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\r\nUsing cached mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\r\nUsing cached narwhals-1.16.0-py3-none-any.whl (244 kB)\r\nUsing cached pathvalidate-3.2.1-py3-none-any.whl (23 kB)\r\nUsing cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\r\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\r\nUsing cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\r\nUsing cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\r\nUsing cached six-1.17.0-py2.py3-none-any.whl (11 kB)\r\nUsing cached tabledata-1.3.3-py3-none-any.whl (11 kB)\r\nUsing cached tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nUsing cached tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\r\nUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\r\nUsing cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\nUsing cached typepy-1.3.2-py3-none-any.whl (31 kB)\r\nUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\r\nUsing cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\nUsing cached portalocker-3.0.0-py3-none-any.whl (19 kB)\r\nUsing cached psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\r\nUsing cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\r\nUsing cached chardet-5.2.0-py3-none-any.whl (199 kB)\r\nUsing cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\r\nUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\r\nUsing cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\r\nUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nUsing cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\r\nUsing cached referencing-0.35.1-py3-none-any.whl (26 kB)\r\nUsing cached rpds_py-0.22.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\r\nUsing cached smmap-5.0.1-py3-none-any.whl (24 kB)\r\nUsing cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\r\nInstalling collected packages: zstd, word2number, sqlitedict, sentencepiece, pytz, mpmath, zstandard, xxhash, wheel, watchdog, urllib3, tzdata, typing-extensions, tqdm, tornado, toml, threadpoolctl, tenacity, tcolorpy, tabulate, sympy, sniffio, smmap, six, setuptools, safetensors, rpds-py, regex, pyyaml, pygments, pycryptodomex, pybind11, pyarrow, psutil, protobuf, propcache, portalocker, pillow, pathvalidate, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, narwhals, multidict, more-itertools, mdurl, MarkupSafe, lxml, joblib, jiter, itsdangerous, idna, h11, fsspec, frozenlist, filelock, distro, dill, colorama, cmake, click, charset-normalizer, chardet, certifi, cachetools, blinker, attrs, annotated-types, aiohappyeyeballs, absl-py, yarl, Werkzeug, triton, tqdm-multiprocess, snakeviz, scipy, sacrebleu, requests, referencing, python-dateutil, pydantic-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numexpr, nltk, multiprocess, mbstrdecoder, markdown-it-py, jsonlines, Jinja2, httpcore, gitdb, gguf, blobfile, anyio, aiosignal, typepy, tiktoken, scikit-learn, rouge-score, rich, pydeck, pydantic, pandas, nvidia-cusolver-cu12, jsonschema-specifications, huggingface_hub, httpx, gitpython, flask, aiohttp, torch, tokenizers, openai, jsonschema, transformers, datasets, DataProperty, altair, accelerate, tabledata, streamlit, peft, evaluate, pytablewriter, lm_eval\r\nSuccessfully installed DataProperty-1.0.1 Jinja2-3.1.4 MarkupSafe-3.0.2 Werkzeug-3.1.3 absl-py-2.1.0 accelerate-1.2.0 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 altair-5.5.0 annotated-types-0.7.0 anyio-4.7.0 attrs-24.2.0 blinker-1.9.0 blobfile-3.0.0 cachetools-5.5.0 certifi-2024.8.30 chardet-5.2.0 charset-normalizer-3.4.0 click-8.1.7 cmake-3.31.1 colorama-0.4.6 datasets-3.1.0 dill-0.3.8 distro-1.9.0 evaluate-0.4.3 filelock-3.16.1 flask-3.1.0 frozenlist-1.5.0 fsspec-2024.9.0 gguf-0.10.0 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface_hub-0.26.5 idna-3.10 itsdangerous-2.2.0 jiter-0.8.2 joblib-1.4.2 jsonlines-4.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 lm_eval-0.4.2 lxml-5.3.0 markdown-it-py-3.0.0 mbstrdecoder-1.1.3 mdurl-0.1.2 more-itertools-10.5.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 narwhals-1.16.0 networkx-3.4.2 ninja-1.11.1.2 nltk-3.9.1 numexpr-2.10.2 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.57.1 packaging-24.2 pandas-2.2.3 pathvalidate-3.2.1 peft-0.14.0 pillow-11.0.0 portalocker-3.0.0 propcache-0.2.1 protobuf-5.29.1 psutil-6.1.0 pyarrow-18.1.0 pybind11-2.13.6 pycryptodomex-3.21.0 pydantic-2.10.3 pydantic-core-2.27.1 pydeck-0.9.1 pygments-2.18.0 pytablewriter-1.2.0 python-dateutil-2.9.0.post0 pytz-2024.2 pyyaml-6.0.2 referencing-0.35.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 rouge-score-0.1.2 rpds-py-0.22.3 sacrebleu-2.4.3 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.14.1 sentencepiece-0.2.0 setuptools-75.6.0 six-1.17.0 smmap-5.0.1 snakeviz-2.2.2 sniffio-1.3.1 sqlitedict-2.1.0 streamlit-1.40.2 sympy-1.13.1 tabledata-1.3.3 tabulate-0.9.0 tcolorpy-0.1.6 tenacity-9.0.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.21.0 toml-0.10.2 torch-2.5.1 tornado-6.4.2 tqdm-4.67.1 tqdm-multiprocess-0.0.11 transformers-4.47.0 triton-3.1.0 typepy-1.3.2 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 watchdog-6.0.0 wheel-0.45.1 word2number-1.1 xxhash-3.5.0 yarl-1.18.3 zstandard-0.23.0 zstd-1.5.5.1\r\n+ pip3 uninstall -y triton\r\nFound existing installation: triton 3.1.0\r\nUninstalling triton-3.1.0:\r\n  Successfully uninstalled triton-3.1.0\r\n+ pip3 install --extra-index-url https://download.pytorch.org/whl/nightly/cu121 torch==2.6.0.dev20241010 torchvision==0.20.0.dev20241010 torchtune==0.4.0.dev20241010\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu121\r\nERROR: Could not find a version that satisfies the requirement torch==2.6.0.dev20241010 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0.dev20241011+cu121, 2.6.0.dev20241012+cu121, 2.6.0.dev20241013+cu121, 2.6.0.dev20241014+cu121, 2.6.0.dev20241015+cu121, 2.6.0.dev20241016+cu121, 2.6.0.dev20241017+cu121, 2.6.0.dev20241018+cu121, 2.6.0.dev20241019+cu121, 2.6.0.dev20241020+cu121, 2.6.0.dev20241021+cu121, 2.6.0.dev20241022+cu121, 2.6.0.dev20241023+cu121, 2.6.0.dev20241024+cu121, 2.6.0.dev20241025+cu121, 2.6.0.dev20241026+cu121, 2.6.0.dev20241027+cu121, 2.6.0.dev20241028+cu121, 2.6.0.dev20241029+cu121, 2.6.0.dev20241030+cu121, 2.6.0.dev20241031+cu121, 2.6.0.dev20241101+cu121, 2.6.0.dev20241102+cu121, 2.6.0.dev20241103+cu121, 2.6.0.dev20241104+cu121, 2.6.0.dev20241105+cu121, 2.6.0.dev20241106+cu121, 2.6.0.dev20241107+cu121, 2.6.0.dev20241108+cu121, 2.6.0.dev20241109+cu121, 2.6.0.dev20241111+cu121, 2.6.0.dev20241112+cu121)\r\nERROR: No matching distribution found for torch==2.6.0.dev20241010\r\n```\n\n### Versions\n\n```text\r\nthink0 tonyr: python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\nClang version: Could not collect\r\nCMake version: version 3.28.3\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\r\nIs CUDA available: N/A\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               12\r\nOn-line CPU(s) list:                  0-11\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Core(TM) i7-4930K CPU @ 3.40GHz\r\nCPU family:                           6\r\nModel:                                62\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   6\r\nSocket(s):                            1\r\nStepping:                             4\r\nCPU(s) scaling MHz:                   43%\r\nCPU max MHz:                          3900.0000\r\nCPU min MHz:                          1200.0000\r\nBogoMIPS:                             6803.85\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts vnmi md_clear flush_l1d\r\nVirtualisation:                       VT-x\r\nL1d cache:                            192 KiB (6 instances)\r\nL1i cache:                            192 KiB (6 instances)\r\nL2 cache:                             1.5 MiB (6 instances)\r\nL3 cache:                             12 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-11\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:               Mitigation; PTI\r\nVulnerability Mmio stale data:        Unknown: No mitigations\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.0.0\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] types-flake8-2020==1.8\r\n[pip3] types-flake8-bugbear==23.9.16\r\n[pip3] types-flake8-builtins==2.2\r\n[pip3] types-flake8-docstrings==1.7\r\n[pip3] types-flake8-plugin-utils==1.3\r\n[pip3] types-flake8-rst-docstrings==0.3\r\n[pip3] types-flake8-simplify==0.21\r\n[pip3] types-flake8-typing-imports==1.15\r\n[pip3] types-mypy-extensions==1.0\r\n[conda] Could not collect\r\n```",
      "state": "closed",
      "author": "drtonyr",
      "author_type": "User",
      "created_at": "2024-12-09T19:57:52Z",
      "updated_at": "2024-12-09T21:17:03Z",
      "closed_at": "2024-12-09T21:17:03Z",
      "labels": [
        "bug",
        "Known Gaps"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1408/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1408",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1408",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:20.726524",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for the flag; Updated and shuold work on main \r\n\r\nI'm working on a separately PR to pull us up to nightly so hopefully it's fixed for a while",
          "created_at": "2024-12-09T21:15:51Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Feel free to reopen if it persists: https://github.com/pytorch/torchchat/commit/326c1fe8214cbefdc68fba7a82b358f83526b340",
          "created_at": "2024-12-09T21:17:03Z"
        }
      ]
    },
    {
      "issue_number": 1397,
      "title": "12/3 High Level Message: PyTorch Versioning Bump",
      "body": "### 🐛 Describe the bug\r\n\r\nHeads up we are looking into the install/import related errors that CI/Users are running into\r\n\r\nWIP PR bumping the versions: https://github.com/pytorch/torchchat/pull/1367\r\n\r\n### Versions\r\n\r\nna",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-12-03T19:22:10Z",
      "updated_at": "2024-12-06T23:03:31Z",
      "closed_at": "2024-12-06T23:03:31Z",
      "labels": [
        "bug",
        "Known Gaps"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1397/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1397",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1397",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:20.969049",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Temp Resolve in https://github.com/pytorch/torchchat/pull/1400\r\n\r\n\r\nSee https://github.com/pytorch/torchchat/actions/runs/12204230280/job/34048909674?pr=1367 for a more recent bump\r\n\r\n",
          "created_at": "2024-12-06T21:54:02Z"
        }
      ]
    },
    {
      "issue_number": 969,
      "title": "Running `torchchat export` with just the model name does not error out",
      "body": "### 🐛 Describe the bug\n\nRunning `python torchchat.py export stories15M` does not error out, nor generates any export files, though it should have?\r\n```shell\r\n% python torchchat.py export stories15M; echo $?\r\nlm_eval is not installed, GPTQ may not be usable\r\nUsing device=mps\r\nWarning! Device MPS not supported for export. Exporting for device CPU.\r\nLoading model...\r\nTime to load model: 0.02 seconds\r\n-----------------------------------------------------------\r\n0\r\n```\n\n### Versions\n\nNo idea, where is the torchchat version defined?",
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-07-30T13:56:14Z",
      "updated_at": "2024-11-26T19:43:00Z",
      "closed_at": "2024-11-26T19:43:00Z",
      "labels": [
        "bug",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/969/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/969",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/969",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:21.223604",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Good catch, an export path should be required\r\n\r\n> the torchchat version defined?\r\n\r\nThere is no current torchchat version",
          "created_at": "2024-07-30T15:08:27Z"
        },
        {
          "author": "mike94043",
          "body": "Export was designed to take a set of export options (as in the mathematical definition of set, and including the empty set) and export to all of them. \r\n\r\nThat being said, some transformations only work for one of the backends, and then it would issue an error. Sadly, too many quantization solutions",
          "created_at": "2024-11-25T21:18:54Z"
        }
      ]
    },
    {
      "issue_number": 1357,
      "title": "Fails to export and run llama3.2-1b. RuntimeError: Failed to initialize zip archive: invalid header or archive is corrupted",
      "body": "### 🐛 Describe the bug\r\n\r\n```bash\r\n$ python3 torchchat.py export llama3.2-1b --output-aoti-package-path llama3_2_1b.pt2\r\n# Success\r\n\r\n$ python3 torchchat.py generate llama3.2-1b --aoti-package-path llama3_2_1b.pt2 --prompt \"Hello my name is\"\r\n\r\nWarning: checkpoint path ignored because an exported DSO or PTE path was specified\r\nWarning: checkpoint path ignored because an exported DSO or PTE path was specified\r\nUsing device=cuda NVIDIA A100-PCIE-40GB\r\nLoading model...\r\nTime to load model: 2.64 seconds\r\nTraceback (most recent call last):\r\n  File \"/home/chensf/git/torchchat/torchchat/cli/builder.py\", line 632, in _initialize_model\r\n    aoti_compiled_model = load_package(\r\n  File \"/home/chensf/git/torchchat/env/lib/python3.10/site-packages/torch/_inductor/package/package.py\", line 224, in load_package\r\n    loader = torch._C._aoti.AOTIModelPackageLoader(path, model_name)  # type: ignore[call-arg]\r\nRuntimeError: Failed to initialize zip archive: invalid header or archive is corrupted\r\n```\r\nHi, I tried to export `llama3.2-1b` (aoti) and execute it, but it failed. \r\nI am using the latest version `ac02ffbbb6457aaa27ca05b54b17697d9959e190`\r\n\r\n### Versions\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241002+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~18.04) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.27\r\n\r\nPython version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-150-generic-x86_64-with-glibc2.27\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-PCIE-40GB\r\nGPU 1: NVIDIA A100-PCIE-40GB\r\nGPU 2: NVIDIA A100-PCIE-40GB\r\nGPU 3: NVIDIA A100-PCIE-40GB\r\n\r\nNvidia driver version: 530.30.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              80\r\nOn-line CPU(s) list: 0-79\r\nThread(s) per core:  2\r\nCore(s) per socket:  20\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) Gold 6145 CPU @ 2.00GHz\r\nStepping:            4\r\nCPU MHz:             2127.865\r\nCPU max MHz:         3700.0000\r\nCPU min MHz:         1000.0000\r\nBogoMIPS:            4000.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            28160K\r\nNUMA node0 CPU(s):   0-19,40-59\r\nNUMA node1 CPU(s):   20-39,60-79\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241002+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.4.0.dev20241010+cu121\r\n[pip3] torchvision==0.20.0.dev20241002+cu121\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pytorch-triton            3.1.0+cf34004b8a          pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241002+cu121          pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchtune                 0.4.0.dev20241010+cu121          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241002+cu121          pypi_0    pypi",
      "state": "closed",
      "author": "siahuat0727",
      "author_type": "User",
      "created_at": "2024-11-08T01:24:51Z",
      "updated_at": "2024-11-22T03:33:07Z",
      "closed_at": "2024-11-22T03:33:06Z",
      "labels": [
        "bug",
        "Compile / AOTI"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1357/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "angelayi"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1357",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1357",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:21.475850",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for the patience and trying out the repo. \r\n\r\nThis is a bug, not sure how it slipped past us cc: @angelayi \r\n\r\n",
          "created_at": "2024-11-12T01:45:51Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Was able to repro failure on 1b, but also weirdly enough it works on llama3.1 8b and llama3.2 3b",
          "created_at": "2024-11-12T02:02:19Z"
        },
        {
          "author": "angelayi",
          "body": "I think this issue is fixed in pytorch master with this PR: https://github.com/pytorch/pytorch/pull/140041. Could you try updating your pytorch version? ",
          "created_at": "2024-11-14T05:37:58Z"
        },
        {
          "author": "siahuat0727",
          "body": "Got it. Thanks!",
          "created_at": "2024-11-22T03:33:06Z"
        }
      ]
    },
    {
      "issue_number": 1253,
      "title": "x86 CPU: BF16 should improve decoding performance relative to FP32 on x86, even without hardware BF16",
      "body": "### 🚀 The feature, motivation and pitch\n\nAs you might expect given that decoding is memory-bandwidth-bound, bf16 is roughly twice as fast as fp32 on my M1 Mac: (`python torchchat.py generate llama3.2-1b --device cpu --dtype <as specified>`)\r\n```\r\nmac, dtype bf16:\r\n      Average tokens/sec (total): 16.42\r\nAverage tokens/sec (first token): 0.51\r\nAverage tokens/sec (next tokens): 19.48\r\n\r\nmac, dtype fp32:\r\n      Average tokens/sec (total): 4.72\r\nAverage tokens/sec (first token): 0.66\r\nAverage tokens/sec (next tokens): 4.88\r\n```\r\n\r\nIn contrast, using an x86 machine without hardware bf16 support, bf16 does not improve performance:\r\n```\r\nx86, dtype bf16:\r\n      Average tokens/sec (total): 14.44\r\nAverage tokens/sec (first token): 7.66\r\nAverage tokens/sec (next tokens): 14.50\r\n\r\nx86, dtype fp32:\r\n      Average tokens/sec (total): 14.00\r\nAverage tokens/sec (first token): 7.71\r\nAverage tokens/sec (next tokens): 14.10\r\n```\r\n\r\nThis matches what you would expect given [the documented behavior of MKL's `cblas_gemm_bf16bf16f32`](https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-2/cblas-gemm-bf16bf16f32.html), which is to just upconvert to fp32 and call SGEMM.\n\n### Alternatives\n\nN/A\n\n### Additional context\n\nllama.cpp has a native x86 bfloat16 dot product kernel supporting AVX-512BF16, AVX-512F, AVX2, and scalar: https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml.c#L2149 .\n\n### RFC (Optional)\n\nWe should be able to do better by taking the same approach as PyTorch's ARM bfloat16 kernel (and llama.cpp's x86 bfloat16 kernel) does in the absence of hardware support: convert each loaded vector of bfloat16s to two vectors of float32s and do the arithmetic at float32 precision. Since decoding is memory-bandwidth bound, we should get a ~2x performance win for decoding.",
      "state": "closed",
      "author": "swolchok",
      "author_type": "User",
      "created_at": "2024-10-02T00:03:43Z",
      "updated_at": "2024-11-21T22:26:14Z",
      "closed_at": "2024-11-21T22:26:14Z",
      "labels": [
        "enhancement",
        "performance",
        "actionable"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1253/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1253",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1253",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:21.791217",
      "comments": [
        {
          "author": "swolchok",
          "body": "FP16 is disproportionately slow on x86 as well; a similar approach should improve performance there",
          "created_at": "2024-10-03T17:55:57Z"
        },
        {
          "author": "swolchok",
          "body": "> FP16 on x86\r\n\r\nConcretely, for stories110M: (llama3.2-1b took longer than I was willing to wait for fp16)\r\n```\r\nfp32:\r\n      Average tokens/sec (total): 65.81\r\nAverage tokens/sec (first token): 27.89\r\nAverage tokens/sec (next tokens): 66.27\r\n\r\nbf16:\r\n      Average tokens/sec (total): 45.59\r\nAverag",
          "created_at": "2024-10-03T18:01:56Z"
        },
        {
          "author": "swolchok",
          "body": "I've started work on generalizing the ARM fp16/bf16 gemv fast path code to use at::vec::Vectorized, which will lead to generalizing it to x86 and using it over MKL when cpuinfo says fp16/bf16 hardware support is missing.",
          "created_at": "2024-10-07T18:20:45Z"
        },
        {
          "author": "swolchok",
          "body": "There are inductor issues lower in the stack right now, but https://github.com/pytorch/pytorch/pull/138005 should solve the FP16 portion of this when it's ready, and BF16 is a matter of follow-up.",
          "created_at": "2024-10-16T20:52:02Z"
        },
        {
          "author": "swolchok",
          "body": "https://github.com/pytorch/pytorch/pull/139220 was merged last week, so the only thing left should be to update the pytorch pin. didn't realize this got closed because a commit mentioned it; reopening until verified.",
          "created_at": "2024-11-11T16:06:29Z"
        }
      ]
    },
    {
      "issue_number": 1360,
      "title": "AssertionError: Found multiple weight mapping files",
      "body": "### 🐛 Describe the bug\n\n python3 torchchat.py download mistralai/mistral-7b-instruct-v0.2\r\n.....\r\n.....\r\n File \"/home/dimanodg/myproject/torchchat/torchchat/cli/convert_hf_checkpoint.py\", line 46, in convert_hf_checkpoint\r\n    assert len(model_map_json_matches) <= 1, \"Found multiple weight mapping files\"\r\nAssertionError: Found multiple weight mapping files\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241002+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.52\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti\r\nNvidia driver version: 560.94\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      39 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             20\r\nOn-line CPU(s) list:                0-19\r\nVendor ID:                          GenuineIntel\r\nModel name:                         12th Gen Intel(R) Core(TM) i7-12700H\r\nCPU family:                         6\r\nModel:                              154\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 10\r\nSocket(s):                          1\r\nStepping:                           3\r\nBogoMIPS:                           5376.02\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          480 KiB (10 instances)\r\nL1i cache:                          320 KiB (10 instances)\r\nL2 cache:                           12.5 MiB (10 instances)\r\nL3 cache:                           24 MiB (1 instance)\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241002+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.4.0.dev20241010+cu121\r\n[pip3] torchvision==0.20.0.dev20241002+cu121\r\n[conda] No relevant packages",
      "state": "closed",
      "author": "DemonODG",
      "author_type": "User",
      "created_at": "2024-11-08T08:46:13Z",
      "updated_at": "2024-11-19T23:14:04Z",
      "closed_at": "2024-11-19T23:14:04Z",
      "labels": [
        "bug",
        "Known Gaps",
        "actionable"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1360/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gabe-l-hart"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1360",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1360",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:22.012044",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for the flag, there's a discussion over here: https://github.com/pytorch/torchchat/pull/1346 about how to fix\r\n\r\n@gabe-l-hart, @leseb Y'all have an appetite to push it through? Or we/I can spin up a RFC for someone to follow up",
          "created_at": "2024-11-12T01:14:04Z"
        },
        {
          "author": "gabe-l-hart",
          "body": "Shoot! I meant to open the PR from my branch, but was on my phone and got distracted by the time I got to the keys. I'll open it first thing tomorrow, or you're welcome to open it from my fork tonight. ",
          "created_at": "2024-11-12T01:19:36Z"
        },
        {
          "author": "gabe-l-hart",
          "body": "PR is up: https://github.com/pytorch/torchchat/pull/1366",
          "created_at": "2024-11-12T04:41:57Z"
        },
        {
          "author": "DemonODG",
          "body": "I installed it #1366  but when I run \r\n python3 torchchat.py download mistralai/mistral-7b-instruct-v0.2\r\n it again I get the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/dimanodg/myproject/torchchat/torchchat.py\", line 102, in <module>\r\n    download_main(args)\r\n  File \"/home/",
          "created_at": "2024-11-12T19:38:57Z"
        },
        {
          "author": "gabe-l-hart",
          "body": "Ah, yep, definitely still a bug in there. Good catch (I never made it this far in testing since my home wifi chopped the connection right before the download finished).",
          "created_at": "2024-11-12T19:45:55Z"
        }
      ]
    },
    {
      "issue_number": 996,
      "title": "AOTI/DSO model does not run in Linux",
      "body": "### 🐛 Describe the bug\n\nI am running an Arch Linux system with a 4090/3090 w/ and up-to-date CUDA 12.5 (`Build cuda_12.5.r12.5/compiler.34385749_0`)\r\n\r\nI have created a new mamba env for torchchat and run the install. Regular inferencing (eg with `generate`) works fine.\r\n\r\nI compile an AOTI model per the README:\r\n```\r\n❯ time python3 torchchat.py export llama3.1 --output-dso-path exportedModels/llama3.1.so\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torchao/ops.py:12: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  return torch.library.impl_abstract(f\"{name}\")(func)\r\nNote: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\r\nNumExpr defaulting to 16 threads.\r\nPyTorch version 2.4.0 available.\r\nUsing device=cuda\r\nLoading model...\r\nTime to load model: 2.54 seconds\r\n-----------------------------------------------------------\r\nExporting model using AOT Inductor to /home/local/torchchat/exportedModels/llama3.1.so\r\nW0802 22:25:40.607000 126075654027072 torch/fx/experimental/symbolic_shapes.py:4449] xindex is not in var_ranges, defaulting to unknown range.\r\nIn file included from /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/IListRef.h:631,\r\n                 from /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/DeviceGuard.h:3,\r\n                 from /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/ATen.h:9,\r\n                 from /home/local/torchchat/exportedModels/ca5ydbysfhhoy7a5vyb5c26c642lglqngoqmpxtzrmq77e6kbqqx.cpp:443:\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/IListRef_inl.h: In static member function ‘static c10::detail::IListRefConstRef<at::OptionalTensorRef> c10::detail::IListRefTagImpl<c10::IListRefTag::Boxed, at::OptionalTensorRef>::iterator_get(const c10::List<std::optional<at::Tensor> >::const_iterator&)’:\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/IListRef_inl.h:171:17: warning: possibly dangling reference to a temporary [-Wdangling-reference]\r\n  171 |     const auto& ivalue = (*it).get();\r\n      |                 ^~~~~~\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/IListRef_inl.h:171:35: note: the temporary was destroyed at the end of the full expression ‘(& it)->c10::impl::ListIterator<std::optional<at::Tensor>, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::operator*().c10::impl::ListElementReference<std::optional<at::Tensor>, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::get()’\r\n  171 |     const auto& ivalue = (*it).get();\r\n      |                          ~~~~~~~~~^~\r\nIn file included from /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/dispatch/OperatorEntry.h:12,\r\n                 from /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:6,\r\n                 from /home/local/torchchat/exportedModels/ca5ydbysfhhoy7a5vyb5c26c642lglqngoqmpxtzrmq77e6kbqqx.cpp:444:\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/dispatch/DispatchKeyExtractor.h: In lambda function:\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/dispatch/DispatchKeyExtractor.h:154:32: warning: possibly dangling reference to a temporary [-Wdangling-reference]\r\n  154 |         for (const at::Tensor& tensor : ivalue.toTensorList()) {\r\n      |                                ^~~~~~\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/include/ATen/core/dispatch/DispatchKeyExtractor.h:154:61: note: the temporary was destroyed at the end of the full expression ‘__for_begin .c10::impl::ListIterator<at::Tensor, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::operator*().c10::impl::ListElementReference<at::Tensor, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::operator std::conditional_t<true, const at::Tensor&, at::Tensor>()’\r\n  154 |         for (const at::Tensor& tensor : ivalue.toTensorList()) {\r\n      |                                                             ^\r\nThe generated DSO model can be found at: /home/local/torchchat/exportedModels/llama3.1.so\r\n\r\nreal    2m2.058s\r\nuser    1m24.277s\r\nsys     0m39.165s\r\n```\r\n\r\nWhen I try to run with the exported DSO model it gives an error:\r\n```\r\n python3 torchchat.py generate llama3.1 --dso-path exportedModels/llama3.1.so --prompt \"Hello my name is\"\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torchao/ops.py:12: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  return torch.library.impl_abstract(f\"{name}\")(func)\r\nNote: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\r\nNumExpr defaulting to 16 threads.\r\nPyTorch version 2.4.0 available.\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nUsing device=cuda NVIDIA GeForce RTX 4090\r\nLoading model...\r\nTime to load model: 2.65 seconds\r\nError: CUDA error: out of memory\r\nTraceback (most recent call last):\r\n  File \"/home/local/torchchat/build/builder.py\", line 468, in _initialize_model\r\n    model.forward = torch._export.aot_load(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/_export/__init__.py\", line 425, in aot_load\r\n    runner = torch._C._aoti.AOTIModelContainerRunnerCuda(so_path, 1, device)  # type: ignore[assignment, call-arg]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: create_func_( &container_handle_, num_models, device_str.c_str(), cubin_dir.empty() ? nullptr : cubin_dir.c_str()) API call failed at ../torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 49\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/local/torchchat/torchchat.py\", line 88, in <module>\r\n    generate_main(args)\r\n  File \"/home/local/torchchat/generate.py\", line 838, in main\r\n    gen = Generator(\r\n          ^^^^^^^^^^\r\n  File \"/home/local/torchchat/generate.py\", line 205, in __init__\r\n    self.model = _initialize_model(self.builder_args, self.quantize, self.tokenizer)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/local/torchchat/build/builder.py\", line 472, in _initialize_model\r\n    raise RuntimeError(f\"Failed to load AOTI compiled {builder_args.dso_path}\")\r\nRuntimeError: Failed to load AOTI compiled exportedModels/llama3.1.so\r\n```\r\n\r\nI tried the C++ runner as well but it fails to build:\r\n```\r\n❯ scripts/build_native.sh aoti\r\n+ '[' 1 -eq 0 ']'\r\n+ ((  1  ))\r\n+ case \"$1\" in\r\n+ echo 'Building aoti native runner...'\r\nBuilding aoti native runner...\r\n+ TARGET=aoti\r\n+ shift\r\n+ ((  0  ))\r\n+ '[' -z '' ']'\r\n+++ dirname scripts/build_native.sh\r\n++ cd scripts\r\n++ pwd -P\r\n+ SCRIPT_PATH=/home/local/torchchat/scripts\r\n++ dirname /home/local/torchchat/scripts\r\n+ TORCHCHAT_ROOT=/home/local/torchchat\r\n+ '[' -z '' ']'\r\n+ ET_BUILD_DIR=et-build\r\n+ source /home/local/torchchat/scripts/install_utils.sh\r\n++ set -ex pipefail\r\n++ COMMON_CMAKE_ARGS='    -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON'\r\n+ pushd /home/local/torchchat\r\n~/torchchat ~/torchchat\r\n+ git submodule update --init\r\nSubmodule 'tokenizer/third-party/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'tokenizer/third-party/abseil-cpp'\r\nSubmodule 'tokenizer/third-party/re2' (https://github.com/google/re2.git) registered for path 'tokenizer/third-party/re2'\r\nSubmodule 'tokenizer/third-party/sentencepiece' (https://github.com/google/sentencepiece.git) registered for path 'tokenizer/third-party/sentencepiece'\r\nCloning into '/home/local/torchchat/tokenizer/third-party/abseil-cpp'...\r\nCloning into '/home/local/torchchat/tokenizer/third-party/re2'...\r\nCloning into '/home/local/torchchat/tokenizer/third-party/sentencepiece'...\r\nSubmodule path 'tokenizer/third-party/abseil-cpp': checked out '854193071498f330b71083d7e06a7cd18e02a4cc'\r\nSubmodule path 'tokenizer/third-party/re2': checked out 'ac82d4f628a2045d89964ae11c48403d3b091af1'\r\nSubmodule path 'tokenizer/third-party/sentencepiece': checked out '7dcb541451b1862d73f473b3804ccf8f2a9e10f6'\r\n+ git submodule sync\r\nSynchronizing submodule url for 'tokenizer/third-party/abseil-cpp'\r\nSynchronizing submodule url for 'tokenizer/third-party/re2'\r\nSynchronizing submodule url for 'tokenizer/third-party/sentencepiece'\r\n+ [[ aoti == \\e\\t ]]\r\n+ popd\r\n~/torchchat\r\n+ [[ aoti == \\e\\t ]]\r\n++ python3 -c 'import torch;print(torch.utils.cmake_prefix_path)'\r\n+ cmake -S . -B ./cmake-out -DCMAKE_PREFIX_PATH=/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/share/cmake -DCMAKE_CXX_FLAGS=-D_GLIBCXX_USE_CXX11_ABI=0 -G Ninja\r\n-- The C compiler identification is GNU 14.1.1\r\n-- The CXX compiler identification is GNU 14.1.1\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX17\r\n-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX17 - Success\r\n-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX20\r\n-- Performing Test ABSL_INTERNAL_AT_LEAST_CXX20 - Failed\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n-- Found Threads: TRUE\r\nCMake Warning at tokenizer/third-party/abseil-cpp/CMakeLists.txt:193 (message):\r\n    The default and system-level install directories are unsupported except in LTS   releases of Abseil.  Please set CMAKE_INSTALL_PREFIX to install Abseil in your   source or build tree directly.\r\n\r\n\r\nCMake Deprecation Warning at tokenizer/third-party/sentencepiece/CMakeLists.txt:15 (cmake_minimum_required):\r\n  Compatibility with CMake < 3.5 will be removed from a future version of\r\n  CMake.\r\n\r\n  Update the VERSION argument <min> value or use a ...<max> suffix to tell\r\n  CMake that the project does not need compatibility with older versions.\r\n\r\n\r\n-- VERSION: 0.2.1\r\n-- Found TCMalloc: /usr/lib/libtcmalloc_minimal.so\r\n-- Using ET BUILD DIR: --[et-build]--\r\n-- TORCHCHAT_ROOT=\"/home/local/torchchat\"\r\n-- Looking for excutorch in /home/local/torchchat/et-build/install\r\n-- Could NOT find executorch (missing: executorch_DIR)\r\nCMake Warning at runner/et.cmake:130 (MESSAGE):\r\n  ExecuTorch package not found\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:15 (include)\r\n\r\n\r\nCMake Warning (dev) at runner/aoti.cmake:16 (find_package):\r\n  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\r\n  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\r\n  set the policy and suppress this warning.\r\n\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:21 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found CUDA: /opt/cuda (found version \"12.5\")\r\n-- Found CUDA: /opt/cuda (found version \"12.5\")\r\n-- The CUDA compiler identification is NVIDIA 12.5.82\r\n-- Detecting CUDA compiler ABI info\r\n-- Detecting CUDA compiler ABI info - done\r\n-- Check for working CUDA compiler: /opt/cuda/bin/nvcc - skipped\r\n-- Detecting CUDA compile features\r\n-- Detecting CUDA compile features - done\r\n-- Found CUDAToolkit: /opt/cuda/include (found version \"12.5.82\")\r\n-- Caffe2: CUDA detected: 12.5\r\n-- Caffe2: CUDA nvcc is: /opt/cuda/bin/nvcc\r\n-- Caffe2: CUDA toolkit directory: /opt/cuda\r\n-- Caffe2: Header version is: 12.5\r\n-- /opt/cuda/lib/libnvrtc.so shorthash is a50b0e02\r\n-- USE_CUDNN is set to 0. Compiling without cuDNN support\r\n-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\r\n-- Autodetected CUDA architecture(s):  8.9 8.6\r\n-- Added CUDA NVCC flags for: -gencode;arch=compute_89,code=sm_89;-gencode;arch=compute_86,code=sm_86\r\nCMake Warning at /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\r\n  static library kineto_LIBRARY-NOTFOUND not found.\r\nCall Stack (most recent call first):\r\n  /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\r\n  runner/aoti.cmake:18 (find_package)\r\n  CMakeLists.txt:21 (include)\r\n\r\n\r\n-- Found Torch: /home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torch/lib/libtorch.so (Required is at least version \"2.4.0\")\r\n-- Configuring done (4.2s)\r\n-- Generating done (0.1s)\r\n-- Build files have been written to: /home/local/torchchat/cmake-out\r\n+ cmake --build ./cmake-out --target aoti_run\r\n[63/222] Building CXX object tokenizer/CMakeFiles/tokenizer.dir/tiktoken.cpp.o\r\nFAILED: tokenizer/CMakeFiles/tokenizer.dir/tiktoken.cpp.o\r\n/usr/bin/c++  -I/home/local/torchchat/tokenizer -I/home/local/torchchat/tokenizer/third-party/sentencepiece/src -I/home/local/torchchat/tokenizer/third-party/re2 -I/home/local/torchchat/tokenizer/third-party/abseil-cpp -D_GLIBCXX_USE_CXX11_ABI=0 -MD -MT tokenizer/CMakeFiles/tokenizer.dir/tiktoken.cpp.o -MF tokenizer/CMakeFiles/tokenizer.dir/tiktoken.cpp.o.d -o tokenizer/CMakeFiles/tokenizer.dir/tiktoken.cpp.o -c /home/local/torchchat/tokenizer/tiktoken.cpp\r\nIn file included from /home/local/torchchat/tokenizer/tiktoken.cpp:18:\r\n/home/local/torchchat/tokenizer/base64.h:37:11: error: ‘uint32_t’ does not name a type\r\n   37 | constexpr uint32_t DECODE_TABLE[] = {\r\n      |           ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:29:1: note: ‘uint32_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n   28 | #include <string>\r\n  +++ |+#include <cstdint>\r\n   29 | #include <string_view>\r\n/home/local/torchchat/tokenizer/base64.h:57:13: error: variable or field ‘validate’ declared void\r\n   57 | inline void validate(uint32_t v) {\r\n      |             ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:57:22: error: ‘uint32_t’ was not declared in this scope\r\n   57 | inline void validate(uint32_t v) {\r\n      |                      ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:57:22: note: ‘uint32_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n/home/local/torchchat/tokenizer/base64.h: In function ‘void base64::detail::decode(const std::string_view&, std::string&)’:\r\n/home/local/torchchat/tokenizer/base64.h:70:3: error: ‘uint32_t’ was not declared in this scope\r\n   70 |   uint32_t val = 0;\r\n      |   ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:70:3: note: ‘uint32_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n/home/local/torchchat/tokenizer/base64.h:72:3: error: ‘uint8_t’ was not declared in this scope\r\n   72 |   uint8_t c = input[0];\r\n      |   ^~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:72:3: note: ‘uint8_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n/home/local/torchchat/tokenizer/base64.h:73:12: error: ‘DECODE_TABLE’ was not declared in this scope\r\n   73 |   auto v = DECODE_TABLE[c];\r\n      |            ^~~~~~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:73:25: error: ‘c’ was not declared in this scope\r\n   73 |   auto v = DECODE_TABLE[c];\r\n      |                         ^\r\n/home/local/torchchat/tokenizer/base64.h:74:3: error: ‘validate’ was not declared in this scope\r\n   74 |   validate(v);\r\n      |   ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:75:3: error: ‘val’ was not declared in this scope\r\n   75 |   val = v;\r\n      |   ^~~\r\n/home/local/torchchat/tokenizer/base64.h: In function ‘void base64::detail::decode_1_padding(const std::string_view&, std::string&)’:\r\n/home/local/torchchat/tokenizer/base64.h:105:3: error: ‘uint32_t’ was not declared in this scope\r\n  105 |   uint32_t val = 0;\r\n      |   ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:105:3: note: ‘uint32_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n/home/local/torchchat/tokenizer/base64.h:107:3: error: ‘uint8_t’ was not declared in this scope\r\n  107 |   uint8_t c = input[0];\r\n      |   ^~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:107:3: note: ‘uint8_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n/home/local/torchchat/tokenizer/base64.h:108:12: error: ‘DECODE_TABLE’ was not declared in this scope\r\n  108 |   auto v = DECODE_TABLE[c];\r\n      |            ^~~~~~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:108:25: error: ‘c’ was not declared in this scope\r\n  108 |   auto v = DECODE_TABLE[c];\r\n      |                         ^\r\n/home/local/torchchat/tokenizer/base64.h:109:3: error: ‘validate’ was not declared in this scope\r\n  109 |   validate(v);\r\n      |   ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:110:3: error: ‘val’ was not declared in this scope\r\n  110 |   val = v;\r\n      |   ^~~\r\n/home/local/torchchat/tokenizer/base64.h: In function ‘void base64::detail::decode_2_padding(const std::string_view&, std::string&)’:\r\n/home/local/torchchat/tokenizer/base64.h:131:3: error: ‘uint32_t’ was not declared in this scope\r\n  131 |   uint32_t val = 0;\r\n      |   ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:131:3: note: ‘uint32_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n/home/local/torchchat/tokenizer/base64.h:133:3: error: ‘uint8_t’ was not declared in this scope\r\n  133 |   uint8_t c = input[0];\r\n      |   ^~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:133:3: note: ‘uint8_t’ is defined in header ‘<cstdint>’; this is probably fixable by adding ‘#include <cstdint>’\r\n/home/local/torchchat/tokenizer/base64.h:134:12: error: ‘DECODE_TABLE’ was not declared in this scope\r\n  134 |   auto v = DECODE_TABLE[c];\r\n      |            ^~~~~~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:134:25: error: ‘c’ was not declared in this scope\r\n  134 |   auto v = DECODE_TABLE[c];\r\n      |                         ^\r\n/home/local/torchchat/tokenizer/base64.h:135:3: error: ‘validate’ was not declared in this scope\r\n  135 |   validate(v);\r\n      |   ^~~~~~~~\r\n/home/local/torchchat/tokenizer/base64.h:136:3: error: ‘val’ was not declared in this scope\r\n  136 |   val = v;\r\n      |   ^~~\r\n[96/222] Building CXX object CMakeFiles/aoti_run.dir/runner/run.cpp.o\r\nninja: build stopped: subcommand failed.\r\n```\n\n### Versions\n\n```\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Arch Linux (x86_64)\r\nGCC version: (GCC) 14.1.1 20240720\r\nClang version: 18.1.8\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.40\r\n\r\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-6.10.0-arch1-2-x86_64-with-glibc2.40\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA GeForce RTX 3090\r\nGPU 1: NVIDIA GeForce RTX 4090\r\n\r\nNvidia driver version: 555.58.02\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/libcudnn.so.9.2.1\r\n/usr/lib/libcudnn_adv.so.9.2.1\r\n/usr/lib/libcudnn_cnn.so.9.2.1\r\n/usr/lib/libcudnn_engines_precompiled.so.9.2.1\r\n/usr/lib/libcudnn_engines_runtime_compiled.so.9.2.1\r\n/usr/lib/libcudnn_graph.so.9.2.1\r\n/usr/lib/libcudnn_heuristic.so.9.2.1\r\n/usr/lib/libcudnn_ops.so.9.2.1\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               32\r\nOn-line CPU(s) list:                  0-31\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD Ryzen 9 5950X 16-Core Processor\r\nCPU family:                           25\r\nModel:                                33\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            1\r\nStepping:                             0\r\nFrequency boost:                      enabled\r\nCPU(s) scaling MHz:                   69%\r\nCPU max MHz:                          5083.3979\r\nCPU min MHz:                          2200.0000\r\nBogoMIPS:                             6802.30\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\r\nL1d cache:                            512 KiB (16 instances)\r\nL1i cache:                            512 KiB (16 instances)\r\nL2 cache:                             8 MiB (16 instances)\r\nL3 cache:                             64 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-31\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton==3.0.0+dedb7bdf33\r\n[pip3] torch==2.4.0\r\n[pip3] torchao==0.3.1\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchvideo==0.0.0\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+dedb7bdf33          pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchao                   0.3.1                    pypi_0    pypi\r\n[conda] torchaudio                2.4.0                    pypi_0    pypi\r\n[conda] torchvideo                0.0.0                    pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\n```",
      "state": "closed",
      "author": "lhl",
      "author_type": "User",
      "created_at": "2024-08-02T13:36:47Z",
      "updated_at": "2024-11-18T15:47:29Z",
      "closed_at": "2024-11-18T15:47:29Z",
      "labels": [
        "bug",
        "Compile / AOTI",
        "Cuda"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/996/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/996",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/996",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:22.240114",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for testing out the repo @lhl! \r\n\r\nLooks like we're hitting a `Error: CUDA error: out of memory` here\r\n\r\nCan you check exporting/generating with the stories15M model to verify that the behavior itself is working?\r\n",
          "created_at": "2024-08-02T16:38:30Z"
        },
        {
          "author": "lhl",
          "body": "Looks like `stories15M` works:\r\n```\r\n❯  python3 torchchat.py generate stories15M --dso-path exportedModels/stories15M.so --prompt \"Hello my name is\"\r\n/home/local/.conda/envs/torchchat/lib/python3.11/site-packages/torchao/ops.py:12: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.l",
          "created_at": "2024-08-04T14:01:13Z"
        },
        {
          "author": "sunshinesfbay",
          "body": "I had the same C++ runner issue building runner for ET/PTE models in #985 ",
          "created_at": "2024-08-06T00:35:37Z"
        }
      ]
    },
    {
      "issue_number": 1278,
      "title": "AOTI Export ignores user --device flag - expected behavior? ",
      "body": "### 🐛 Describe the bug\n\nHi all, \r\n\r\nI ran into some confusion when trying to export llama3 on my system. I have a small graphics card (8GB VRAM on an AMD GPU) but a decent amount of RAM (24GB). Obviously, the model won't fit on my GPU un-quantized but it should fit into my RAM + swap.\r\n\r\nI tried running:\r\n```\r\npython3 torchchat.py export llama3 --output-dso-path exportedModels/llama3.so --quantize torchchat/quant_config/desktop.json  --device cpu\r\n```\r\n\r\nHowever, I ran into multiple HIP OOM errors (basically equivalent to CUDA). Why would we try to allocate CUDA memory if the target device is CPU?\r\n\r\nOn further inspection, during export, the device is replaced with whatever is present in the quantize config:\r\nIn `cli.py`\r\nhttps://github.com/pytorch/torchchat/blob/b21715835ab9f61e23dbcf32795b0c0a2d654908/torchchat/cli/cli.py#L491C10-L494C1\r\n```\r\nargs.device = get_device_str(\r\n    args.quantize.get(\"executor\", {}).get(\"accelerator\", args.device)\r\n)\r\n```\r\n\r\nIn this case, the device in `desktop.json` is \"fast\". The `get_device_str` function replaces this with \"cuda\" simply based on `torch.cuda.is_available` without consulting the flag I passed in. \r\n\r\n## Other cases\r\nDoing a quick grep of the repo, I only found one other case in `generate.py` where `torch.cuda.is_available()` is consulted for monitoring memory usage. We should be careful switching based simply on `torch.cuda.is_available()` and make sure to pin to the user's request if we're using ambiguous devices like \"fast\".\r\n\r\nAnother small issue - since I use AMD GPU, the default `install/install_requirements.sh` will download the CPU only version instead of the ROCm version of PyTorch. To use my GPU, I have to re-run the torch installation manually. Luckily, it's quite easy to find this command at https://pytorch.org/get-started/locally/ . Should be straightforward to check of ROCm is available on the system during this script - we can just run `rocminfo` & check if the command is available.\n\n### Versions\n\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n--2024-10-06 12:03:44--  https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 23357 (23K) [text/plain]\r\nSaving to: ‘collect_env.py’\r\n\r\ncollect_env.py                              100%[===========================================================================================>]  22.81K  --.-KB/s    in 0.02s   \r\n\r\n2024-10-06 12:03:44 (1.10 MB/s) - ‘collect_env.py’ saved [23357/23357]\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.4.1+rocm6.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: version 3.30.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.1.4-060104-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Radeon RX 6700S (gfx1030)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.1.40091\r\nMIOpen runtime version: 3.1.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen 9 6900HS with Radeon Graphics\r\nCPU family:                      25\r\nModel:                           68\r\nThread(s) per core:              2\r\nCore(s) per socket:              8\r\nSocket(s):                       1\r\nStepping:                        1\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     4933.8862\r\nCPU min MHz:                     1600.0000\r\nBogoMIPS:                        6587.56\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                  AMD-V\r\nL1d cache:                       256 KiB (8 instances)\r\nL1i cache:                       256 KiB (8 instances)\r\nL2 cache:                        4 MiB (8 instances)\r\nL3 cache:                        16 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton-rocm==3.0.0\r\n[pip3] torch==2.4.1+rocm6.1\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.4.1+rocm6.1\r\n[pip3] torchtune==0.3.0.dev20240928+cpu\r\n[pip3] torchvision==0.19.1+rocm6.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2023.1.0         h213fc3f_46344  \r\n[conda] mkl-service               2.4.0           py311h5eee18b_1  \r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0  \r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0  \r\n[conda] numpy                     1.25.2                   pypi_0    pypi\r\n[conda] numpy-base                1.26.4          py311hf175353_0  \r\n[conda] pytorch                   2.3.0           cpu_py311ha0631a7_0  \r\n[conda] torch                     2.0.1                    pypi_0    pypi\r\n[conda] triton                    2.0.0                    pypi_0    pypi\r\n```",
      "state": "closed",
      "author": "vmpuri",
      "author_type": "User",
      "created_at": "2024-10-06T19:06:51Z",
      "updated_at": "2024-11-16T01:15:38Z",
      "closed_at": "2024-11-16T01:15:37Z",
      "labels": [
        "bug",
        "good first issue",
        "actionable"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1278/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1278",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1278",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:22.470062",
      "comments": [
        {
          "author": "mikekg",
          "body": "The config device in the JSON overrides the command line here => https://github.com/pytorch/torchchat/blob/main/torchchat/cli/cli.py#L492\r\n\r\nIf we want to give commandline priority over config, we can invert the assignment.  If this is done we need to change how default is handled, so that we know w",
          "created_at": "2024-10-07T20:20:29Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "@mikekg hit it on the nose. We should invert the priority and make sure to do so for all the settings (and make a clear warning message that we are doing so in the terminal)",
          "created_at": "2024-10-07T23:22:30Z"
        },
        {
          "author": "mikekg",
          "body": "> @mikekg hit it on the nose. We should invert the priority and make sure to do so for all the settings (and make a clear warning message that we are doing so in the terminal)\r\n\r\nThere's some additional priority logic in quantize, #1282 should make sure we use only one authoritative place for this l",
          "created_at": "2024-10-08T15:15:00Z"
        },
        {
          "author": "desertfire",
          "body": "@byjlw , this is not an AOTI specific issue. Can you find someone else other than me to work on it?",
          "created_at": "2024-10-31T22:04:19Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Fixed (See attached commits)",
          "created_at": "2024-11-16T01:15:37Z"
        }
      ]
    },
    {
      "issue_number": 1373,
      "title": "Torchchat generate cannot work with device=fast",
      "body": "### 🐛 Describe the bug\n\nTorchchat generate and server cannot work with device=fast.\r\n\r\nThe example command: \r\n`python3 torchchat.py generate llama3.1 --prompt \"write me a story about a boy and his bear\" --device fast`\r\n\r\nAnd the error is as below:\r\n```\r\nUsing device=fast\r\nLoading model...\r\nTime to load model: 0.04 seconds\r\nTraceback (most recent call last):\r\n  File \"/home/sdp/jwang/torchchat/torchchat.py\", line 91, in <module>\r\n    server_main(args)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/usages/server.py\", line 127, in main\r\n    app = create_app(args)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/usages/server.py\", line 38, in create_app\r\n    gen: OpenAiApiGenerator = initialize_generator(args)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/usages/server.py\", line 115, in initialize_generator\r\n    return OpenAiApiGenerator(\r\n  File \"/home/sdp/jwang/torchchat/torchchat/usages/openai_api.py\", line 283, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/generate.py\", line 293, in __init__\r\n    self.model = _initialize_model(self.builder_args, self.quantize, self.tokenizer)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/cli/builder.py\", line 603, in _initialize_model\r\n    model = _load_model(builder_args)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/cli/builder.py\", line 465, in _load_model\r\n    model = _load_model_default(builder_args)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/cli/builder.py\", line 427, in _load_model_default\r\n    checkpoint = _load_checkpoint(builder_args)\r\n  File \"/home/sdp/jwang/torchchat/torchchat/cli/builder.py\", line 412, in _load_checkpoint\r\n    checkpoint = torch.load(\r\n  File \"/home/sdp/miniforge3/envs/jiao-pt/lib/python3.10/site-packages/torch/serialization.py\", line 1359, in load\r\n    return _load(\r\n  File \"/home/sdp/miniforge3/envs/jiao-pt/lib/python3.10/site-packages/torch/serialization.py\", line 1856, in _load\r\n    result = unpickler.load()\r\n  File \"/home/sdp/miniforge3/envs/jiao-pt/lib/python3.10/site-packages/torch/_weights_only_unpickler.py\", line 385, in load\r\n    self.append(self.persistent_load(pid))\r\n  File \"/home/sdp/miniforge3/envs/jiao-pt/lib/python3.10/site-packages/torch/serialization.py\", line 1820, in persistent_load\r\n    typed_storage = load_tensor(\r\n  File \"/home/sdp/miniforge3/envs/jiao-pt/lib/python3.10/site-packages/torch/serialization.py\", line 1792, in load_tensor\r\n    wrap_storage=restore_location(storage, location),\r\n  File \"/home/sdp/miniforge3/envs/jiao-pt/lib/python3.10/site-packages/torch/serialization.py\", line 1693, in restore_location\r\n    return default_restore_location(storage, map_location)\r\n  File \"/home/sdp/miniforge3/envs/jiao-pt/lib/python3.10/site-packages/torch/serialization.py\", line 604, in default_restore_location\r\n    raise RuntimeError(\r\nRuntimeError: don't know how to restore data location of torch.storage.UntypedStorage (tagged with fast)\r\n```\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241002+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:24:24) [GCC 13.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        52 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               256\r\nOn-line CPU(s) list:                  0-255\r\nVendor ID:                            GenuineIntel\r\nModel name:                           GENUINE INTEL(R) XEON(R)\r\nCPU family:                           6\r\nModel:                                175\r\nThread(s) per core:                   1\r\nCore(s) per socket:                   128\r\nSocket(s):                            2\r\nStepping:                             0\r\nCPU max MHz:                          2900.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             3200.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            8 MiB (256 instances)\r\nL1i cache:                            16 MiB (256 instances)\r\nL2 cache:                             256 MiB (64 instances)\r\nL3 cache:                             192 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-127\r\nNUMA node1 CPU(s):                    128-255\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS Not affected; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.6.0.dev20241002+cpu\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.4.0.dev20241010+cpu\r\n[pip3] torchvision==0.20.0.dev20241002+cpu\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241002+cpu          pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchtune                 0.4.0.dev20241010+cpu          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241002+cpu          pypi_0    pypi",
      "state": "closed",
      "author": "jenniew",
      "author_type": "User",
      "created_at": "2024-11-14T00:02:01Z",
      "updated_at": "2024-11-14T02:51:31Z",
      "closed_at": "2024-11-14T02:51:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1373/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1373",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1373",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:22.774438",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Looks like a bug from https://github.com/pytorch/torchchat/commit/93f713f12507b5cef18a42c411030c90b9326369\r\n\r\nLooking into it\r\n",
          "created_at": "2024-11-14T00:24:48Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Fixed in https://github.com/pytorch/torchchat/pull/1374",
          "created_at": "2024-11-14T00:51:12Z"
        }
      ]
    },
    {
      "issue_number": 1362,
      "title": "linear:int4 quantization regression testing",
      "body": "### 🚀 The feature, motivation and pitch\n\nIn the past, we padded int4 quantization with non-multiple group size to make things work.  Since we have decided to remove the padding, int4 quantization is now simply skipped for non-multiple groups.  This means, among other things, that int4 quantization is no longer tested because the stories model uses non-multiple-of-256.\r\n\r\n```\r\n  Time to load model: 0.19 seconds\r\n  Quantizing the model with: {'executor': {'accelerator': 'cuda'}, 'precision': {'dtype': 'bf16'}, 'linear:int4': {'groupsize': 256}}\r\n  Skipping quantizing weight with int4 weight only quantization because the shape of weight torch.Size([288, 288]) is not compatible with group_size 256\r\n  Skipping quantizing weight with int4 weight only quantization because the shape of weight torch.Size([288, 288]) is not compatible with group_size 256\r\n  Skipping quantizing weight with int4 weight only quantization because the shape of weight torch.Size([288, 288]) is not compatible with group_size 256\r\n  Skipping quantizing weight with int4 weight only quantization because the shape of weight torch.Size([288, 288]) is not compatible with group_size 256\r\n```\r\n\r\nSome options:\r\n* replace stories with another model that meets the requirement\r\n* add other tests for int4 quantization in tc\r\n\n\n### Alternatives\n\nPut padding back into int4 quantization.  \r\n\r\nYes, it's not ideal, then again, suppressing quantization is not either.  In my own experience, just making things work increases utility for end users, if there's real concern about performance (int4 quantization with padding may still beat non-quantization!), pad and issue a warning to users.\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-11-09T18:28:05Z",
      "updated_at": "2024-11-13T04:45:23Z",
      "closed_at": null,
      "labels": [
        "bug",
        "actionable",
        "Quantization"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1362/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1362",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1362",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:22.999294",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Nice catch, now that we have the 1B models we might be able to switch over to that instead of stories for a more representative model. (or change the groupsize) \r\n\r\nAs for padding vs not padding, I'll need to think about that before a make a call. \r\n- On the one hand, I agree that the UX would be be",
          "created_at": "2024-11-12T01:04:52Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "We should update the test so that this doesn't happen\r\n\r\nhttps://github.com/pytorch/torchchat/blob/main/.github/workflows/pull.yml#L335",
          "created_at": "2024-11-12T01:31:53Z"
        },
        {
          "author": "mikekgfb",
          "body": "> * On the other hand, it adds feature overhead to show a practice that I'm not familiar with whether we should advertise, they should just update the groupsize instead\r\n\r\nWe currently apply a uniform transformation to all layers, so we are limited to the GCD of all layers.  For language-llama (can'",
          "created_at": "2024-11-13T04:30:35Z"
        },
        {
          "author": "mikekgfb",
          "body": "> Nice catch, now that we have the 1B models we might be able to switch over to that instead of stories for a more representative model.\r\n\r\nLet's make sure it does not explode the runtime for tests to not become onerous (when developers will start ignoring or argue for less tests etc)\r\n\r\nAlso, to ge",
          "created_at": "2024-11-13T04:45:22Z"
        }
      ]
    },
    {
      "issue_number": 1351,
      "title": "Periodic runs fail with workflow error: Invalid workflow file: .github/workflows/run-readme-periodic.yml#L50",
      "body": "### 🐛 Describe the bug\n\nhttps://github.com/pytorch/torchchat/actions/runs/11714048971\r\n\r\nInvalid workflow file: .github/workflows/run-readme-periodic.yml#L50\r\nThe workflow is not valid. .github/workflows/run-readme-periodic.yml (Line: 50, Col: 16): Invalid input, secrets is not defined in the referenced workflow.\r\n\n\n### Versions\n\nRepro on torchchat test system => https://github.com/pytorch/torchchat/actions/runs/11714048971",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-11-07T03:15:35Z",
      "updated_at": "2024-11-12T02:17:14Z",
      "closed_at": null,
      "labels": [
        "bug",
        "ciflow/periodic",
        "actionable"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1351/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1351",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1351",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:23.235480",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Good spot, seems like the periodic test have been dead for a while",
          "created_at": "2024-11-12T02:17:01Z"
        }
      ]
    },
    {
      "issue_number": 1363,
      "title": "Specifying dtype flag on export crashes AOTI export",
      "body": "### 🐛 Describe the bug\n\nIf I specify --dtype on torchchat export, the process is killed.  I tried on llama2 with both dtype float32 and bf16.  If I do not specify dtype, the process succeeds.\r\n\r\nSpecifying dtype float32:\r\n\r\n```\r\ntorchchat % OMP_NUM_THREADS=6 python torchchat.py export llama2 --device cpu --dtype float32 --output-dso /tmp/model.so\r\nW1111 09:57:52.876502 25468 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\r\nPyTorch version 2.6.0.dev20241002 available.\r\nLoading custom ops library: /opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/executorch/extension/llm/custom_ops/libcustom_ops_aot_lib.dylib\r\nUsing device=cpu\r\nSetting max_seq_length to 300 for DSO export.\r\nLoading model...\r\nTime to load model: 35.47 seconds\r\n-----------------------------------------------------------\r\nExporting model using AOT Inductor to /tmp/model.so\r\nWARNING!! The path of compiling a dso is deprecated. Please use --output-aoti-package-path to create a .pt2 artifact instead.\r\nW1111 09:58:49.250014 25468 site-packages/torch/_export/__init__.py:225] +============================+\r\nW1111 09:58:49.259787 25468 site-packages/torch/_export/__init__.py:226] |     !!!   WARNING   !!!    |\r\nW1111 09:58:49.259886 25468 site-packages/torch/_export/__init__.py:227] +============================+\r\nW1111 09:58:49.259957 25468 site-packages/torch/_export/__init__.py:228] torch._export.aot_compile() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export()) instead.\r\nclang++: warning: -lc10: 'linker' input unused [-Wunused-command-line-argument]\r\nclang++: warning: -lomp: 'linker' input unused [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/homebrew/opt/libomp/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nzsh: killed     OMP_NUM_THREADS=6 python torchchat.py export llama2 --device cpu --dtype\r\n```\r\n\r\nSpecifying dtype bf16:\r\n\r\n```\r\ntorchchat % OMP_NUM_THREADS=6 python torchchat.py export llama2 --device cpu --dtype bf16 --output-dso /tmp/model.so\r\nW1111 10:13:35.454401 35912 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\r\nPyTorch version 2.6.0.dev20241002 available.\r\nLoading custom ops library: /opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/executorch/extension/llm/custom_ops/libcustom_ops_aot_lib.dylib\r\nUsing device=cpu\r\nSetting max_seq_length to 300 for DSO export.\r\nLoading model...\r\nTime to load model: 13.13 seconds\r\n-----------------------------------------------------------\r\nExporting model using AOT Inductor to /tmp/model.so\r\nWARNING!! The path of compiling a dso is deprecated. Please use --output-aoti-package-path to create a .pt2 artifact instead.\r\nW1111 10:14:06.348782 35912 site-packages/torch/_export/__init__.py:225] +============================+\r\nW1111 10:14:06.349333 35912 site-packages/torch/_export/__init__.py:226] |     !!!   WARNING   !!!    |\r\nW1111 10:14:06.349414 35912 site-packages/torch/_export/__init__.py:227] +============================+\r\nW1111 10:14:06.349486 35912 site-packages/torch/_export/__init__.py:228] torch._export.aot_compile() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export()) instead.\r\nclang++: warning: -lc10: 'linker' input unused [-Wunused-command-line-argument]\r\nclang++: warning: -lomp: 'linker' input unused [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/homebrew/opt/libomp/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nzsh: killed     OMP_NUM_THREADS=6 python torchchat.py export llama2 --device cpu --dtype bf16\r\n(torchchat) scroy@scroy-mbp torchchat % /opt/miniconda3/envs/torchchat/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```\r\n\r\n\r\nNo dtype specified:\r\n\r\n```\r\nOMP_NUM_THREADS=6 python torchchat.py export llama2 --device cpu --output-dso /tmp/model.so \r\nW1111 10:16:24.587299 37753 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\r\nPyTorch version 2.6.0.dev20241002 available.\r\nLoading custom ops library: /opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/executorch/extension/llm/custom_ops/libcustom_ops_aot_lib.dylib\r\nUsing device=cpu\r\nSetting max_seq_length to 300 for DSO export.\r\nLoading model...\r\nTime to load model: 0.07 seconds\r\n-----------------------------------------------------------\r\nExporting model using AOT Inductor to /tmp/model.so\r\nWARNING!! The path of compiling a dso is deprecated. Please use --output-aoti-package-path to create a .pt2 artifact instead.\r\nW1111 10:16:30.370594 37753 site-packages/torch/_export/__init__.py:225] +============================+\r\nW1111 10:16:30.370881 37753 site-packages/torch/_export/__init__.py:226] |     !!!   WARNING   !!!    |\r\nW1111 10:16:30.370961 37753 site-packages/torch/_export/__init__.py:227] +============================+\r\nW1111 10:16:30.371028 37753 site-packages/torch/_export/__init__.py:228] torch._export.aot_compile() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export()) instead.\r\nclang++: warning: -lc10: 'linker' input unused [-Wunused-command-line-argument]\r\nclang++: warning: -lomp: 'linker' input unused [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/homebrew/opt/libomp/lib' [-Wunused-command-line-argument]\r\nclang++: warning: argument unused during compilation: '-L/opt/miniconda3/envs/torchchat/lib/python3.10/site-packages/torch/lib' [-Wunused-command-line-argument]\r\nThe generated packaged model can be found at: /tmp/model.so\r\n```\r\n\r\n\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241002\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.7 (arm64)\r\nGCC version: Could not collect\r\nClang version: 16.0.0 (clang-1600.0.26.3)\r\nCMake version: version 3.30.5\r\nLibc version: N/A\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 03:54:28) [Clang 12.0.0 ] (64-bit runtime)\r\nPython platform: macOS-14.7-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M1 Pro\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.5.0a0+72b3bb3\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.6.0.dev20241002\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.5.0.dev20241007\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchtune==0.4.0.dev20241010+cpu\r\n[pip3] torchvision==0.20.0.dev20241002\r\n[conda] executorch                0.5.0a0+72b3bb3          pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241002          pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchaudio                2.5.0.dev20241007          pypi_0    pypi\r\n[conda] torchsr                   1.0.4                    pypi_0    pypi\r\n[conda] torchtune                 0.4.0.dev20241010+cpu          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241002          pypi_0    pypi",
      "state": "closed",
      "author": "metascroy",
      "author_type": "User",
      "created_at": "2024-11-11T18:27:18Z",
      "updated_at": "2024-11-11T19:33:50Z",
      "closed_at": "2024-11-11T19:33:50Z",
      "labels": [
        "bug",
        "Compile / AOTI"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1363/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1363",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1363",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:23.491708",
      "comments": [
        {
          "author": "metascroy",
          "body": "cc @angelayi ",
          "created_at": "2024-11-11T18:27:32Z"
        },
        {
          "author": "metascroy",
          "body": "Update: it works if I use --output-aoti-package-path instead of --output-dso\r\n",
          "created_at": "2024-11-11T18:39:00Z"
        }
      ]
    },
    {
      "issue_number": 999,
      "title": "MPS: chat command is much slower than generate on Mac",
      "body": "### 🐛 Describe the bug\n\nFor generate on llama3.1, I got 9.1 tok/s, but chat is much slower. I got around 1.4 tok/s. \r\nTest laptop: MacBook Pro with M1 Max, 64 GB memory. Sonoma 14.5\r\n\r\nDetails for both generate and chat: \r\n```\r\n(torchchat) myuan@myuan-mbp torchchat % python3 torchchat.py generate llama3.1 --prompt \"write me a story about a boy and his bear\"\r\nNumExpr defaulting to 10 threads.\r\nPyTorch version 2.5.0.dev20240710 available.\r\nUsing device=mps\r\nLoading model...\r\nTime to load model: 32.03 seconds\r\n-----------------------------------------------------------\r\nwrite me a story about a boy and his bear\r\nOnce upon a time, in a dense forest, there lived a young boy named Timmy. Timmy was a kind and gentle soul, with a heart full of love for all creatures. He had grown up in the forest, surrounded by the sights, sounds, and smells of nature. As he wandered through the woods, he would often talk to the trees, the rabbits, and the birds, and they seemed to respond to his gentle voice.\r\nOne day, while exploring a hidden glade, Timmy stumbled upon a big, fluffy bear. The bear was unlike any other he had seen before - its fur was a soft, golden hue, and its eyes twinkled like the stars on a clear night. Timmy was both startled and fascinated by the bear, and he slowly reached out a hand to touch its fur.\r\nTo his surprise, the bear nuzzled his hand, and Timmy felt a deep connection to the creature. The bear, whose name was Orion, had\r\nTime for inference 1: 21.87 sec total, time to first token 5.20 sec with parallel prefill, 199 tokens, 9.10 tokens/sec, 109.91 ms/token\r\nBandwidth achieved: 146.12 GB/s\r\n*** This first iteration will include cold start effects for dynamic import, hardware caches. ***\r\n\r\n========================================\r\n\r\nAverage tokens/sec: 9.10\r\nMemory used: 0.00 GB\r\n(torchchat) myuan@myuan-mbp torchchat % python3 torchchat.py chat llama3.1\r\nNumExpr defaulting to 10 threads.\r\nPyTorch version 2.5.0.dev20240710 available.\r\nUsing device=mps\r\nLoading model...\r\nTime to load model: 29.87 seconds\r\n-----------------------------------------------------------\r\nStarting Interactive Chat\r\nEntering Chat Mode. Will continue chatting back and forth with the language model until the models max context length of 8192 tokens is hit or until the user says /bye\r\nDo you want to enter a system prompt? Enter y for yes and anything else for no.\r\ny\r\nWhat is your system prompt?\r\nsoftware engineer\r\nUser: Write a python code to solve a maze problem\r\nModel: Here's a simple implementation of a maze solver using Breadth-First Search (BFS) algorithm in Python:\r\n\r\n```python\r\nfrom collections import deque\r\n\r\ndef solve_maze(maze, start, end):\r\n    # Define the possible movements (up, down, left, right)\r\n    movements = [(0, 1), (0, -1), (1, 0), (-1, 0)]\r\n\r\n    # Create a queue for BFS\r\n    queue = deque([(start, [])])\r\n\r\n    # Create a set to store visited cells\r\n    visited = set([start])\r\n\r\n    while queue:\r\n        (x, y), path = queue.popleft()\r\n\r\n        # Check if the current cell is the destination\r\n        if (x, y) == end:\r\n            return path + [(x, y)]\r\n\r\n        # Explore neighbors\r\n        for dx, dy in movements:\r\n            nx, ny = x + dx, y + dy\r\n\r\n            # Check if the neighbor is inside the\r\nTime for inference 1: 142.65 sec total, time to first token 4.86 sec with parallel prefill, 199 tokens, 1.40 tokens/sec, 716.83 ms/token\r\nBandwidth achieved: 22.40 GB/s\r\n*** This first iteration will include cold start effects for dynamic import, hardware caches. ***\r\nUser: How to optimize it?\r\nModel: Here are some ways to optimize the maze solver:\r\n\r\n1. **Using a more efficient data structure**: Currently, we are using a list to represent the maze. We can use a matrix (2D array) to represent the maze, which would be more efficient.\r\n\r\n2. **Avoiding repeated computations**: We are recalculating the `is_valid` function on each iteration. We can pre-calculate the maze and store it in a separate matrix to avoid repeated computations.\r\n\r\n3. **Using a more efficient algorithm**: BFS has a time complexity of O(n*m), where n is the number of rows and m is the number of columns. We can use other algorithms like Dijkstra's algorithm or A\\* algorithm, which have better time complexities.\r\n\r\n4. **Using multithreading or multiprocessing**: If we are dealing with a very large maze, we can use multithreading or multiprocessing to speed up the computation.\r\n\r\nHere's an optimized version of the code:\r\n\r\n```python\r\nfrom collections\r\nTime for inference 2: 138.47 sec total, time to first token 1.54 sec with parallel prefill, 199 tokens, 1.44 tokens/sec, 695.81 ms/token\r\nBandwidth achieved: 23.08 GB/s\r\nUser: Can you show me a code example?\r\nModel: Here's an optimized version of the maze solver using a matrix to represent the maze and avoiding repeated computations:\r\n\r\n```python\r\nfrom collections import deque\r\n\r\ndef solve_maze(maze):\r\n    rows, cols = len(maze), len(maze[0])\r\n    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\r\n\r\n    # Create a matrix to store the distances from the start to each cell\r\n    distances = [[float('inf') for _ in range(cols)] for _ in range(rows)]\r\n    distances[0][0] = 0\r\n\r\n    # Create a queue for BFS\r\n    queue = deque([(0, 0)])\r\n\r\n    while queue:\r\n        x, y = queue.popleft()\r\n\r\n        # Explore neighbors\r\n        for dx, dy in directions:\r\n            nx, ny = x + dx, y + dy\r\n\r\n            # Check if the neighbor is within the maze and is not a wall\r\n\r\nTime for inference 3: 177.72 sec total, time to first token 0.85 sec with parallel prefill, 199 tokens, 1.12 tokens/sec, 893.08 ms/token\r\nBandwidth achieved: 17.98 GB/s\r\n```\n\n### Versions\n\n(torchchat) myuan@myuan-mbp torchchat % python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240710\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.5 (arm64)\r\nGCC version: Could not collect\r\nClang version: 15.0.0 (clang-1500.0.40.1)\r\nCMake version: version 3.30.1\r\nLibc version: N/A\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 03:54:28) [Clang 12.0.0 ] (64-bit runtime)\r\nPython platform: macOS-14.5-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M1 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240710\r\n[pip3] torchao==0.3.1\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.5.0.dev20240710          pypi_0    pypi\r\n[conda] torchao                   0.3.1                    pypi_0    pypi\r\n",
      "state": "open",
      "author": "iseeyuan",
      "author_type": "User",
      "created_at": "2024-08-02T22:33:39Z",
      "updated_at": "2024-11-07T10:25:01Z",
      "closed_at": null,
      "labels": [
        "bug",
        "Known Gaps",
        "actionable",
        "MPS/Metal"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/999/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/999",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/999",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:23.683166",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Seems like a bug with max_seq_len: https://github.com/pytorch/torchchat/blob/0b001b9dc74c12e136f5ee9b3c19427b9acd24ff/generate.py#L631\r\n\r\nWhen I hack it to 200 (compared to the default 8192) the perf for chat is close to that of generate",
          "created_at": "2024-08-02T22:48:21Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "This also seems isolated to MPS as well (I don't see as significant a drop on Cuda)\r\n\r\n@manuelcandales Looks like your PR might solve this problem for free",
          "created_at": "2024-08-02T22:58:18Z"
        },
        {
          "author": "iseeyuan",
          "body": "@Jack-Khuu , would a shorter max_seq_length just a hack? If the chat conversation go beyond the limit the chat will stop, which limit the user experience of long chat history. ",
          "created_at": "2024-08-03T00:37:22Z"
        },
        {
          "author": "malfet",
          "body": "See good old https://github.com/pytorch/torchchat/issues/783",
          "created_at": "2024-08-03T00:53:08Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "> would a shorter max_seq_length just a hack?\r\n\r\nIt would be, which is why we're lucky to have https://github.com/pytorch/torchchat/pull/964\r\nManuel's changes in PT gets picked up by the pin bump and will hopefully resolve the seq_length issues",
          "created_at": "2024-08-03T01:01:41Z"
        }
      ]
    },
    {
      "issue_number": 1298,
      "title": "RuntimeError: CUDA error: named symbol not found",
      "body": "### 🐛 Describe the bug\n\npython torchchat.py generate stories110M --quant torchchat/quant_config/cuda.json --prompt \"It was a dark and stormy night, and\"\r\n\r\nUsing device=cuda Tesla T4\r\nLoading model...\r\nTime to load model: 0.73 seconds\r\nQuantizing the model with: {'executor': {'accelerator': 'cuda'}, 'precision': {'dtype': 'bf16'}, 'linear:int4': {'groupsize': 256}}\r\nTime to quantize model: 0.35 seconds\r\nTraceback (most recent call last):\r\n  File \"/content/torchchat-1/torchchat.py\", line 88, in <module>\r\n    generate_main(args)\r\n  File \"/content/torchchat-1/torchchat/generate.py\", line 1210, in main\r\n    gen = Generator(\r\n  File \"/content/torchchat-1/torchchat/generate.py\", line 290, in __init__\r\n    self.model = _initialize_model(self.builder_args, self.quantize, self.tokenizer)\r\n  File \"/content/torchchat-1/torchchat/cli/builder.py\", line 574, in _initialize_model\r\n    quantize_model(\r\n  File \"/content/torchchat-1/torchchat/utils/quantize.py\", line 114, in quantize_model\r\n    quantize_(model, int4_weight_only(q_kwargs[\"groupsize\"]))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/quant_api.py\", line 462, in quantize_\r\n    _replace_with_custom_fn_if_matches_filter(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/quant_api.py\", line 202, in _replace_with_custom_fn_if_matches_filter\r\n    new_child = _replace_with_custom_fn_if_matches_filter(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/quant_api.py\", line 202, in _replace_with_custom_fn_if_matches_filter\r\n    new_child = _replace_with_custom_fn_if_matches_filter(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/quant_api.py\", line 202, in _replace_with_custom_fn_if_matches_filter\r\n    new_child = _replace_with_custom_fn_if_matches_filter(\r\n  [Previous line repeated 2 more times]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/quant_api.py\", line 198, in _replace_with_custom_fn_if_matches_filter\r\n    model = replacement_fn(model)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/quant_api.py\", line 392, in insert_subclass\r\n    lin.weight = torch.nn.Parameter(constructor(lin.weight, **kwargs), requires_grad=requires_grad)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/quant_api.py\", line 553, in apply_int4_weight_only_quant\r\n    return to_affine_quantized_intx(weight, mapping_type, block_size, target_dtype, quant_min, quant_max, eps, zero_point_dtype=zero_point_dtype, preserve_zero=preserve_zero, zero_point_domain=zero_point_domain, layout_type=layout_type, use_hqq=use_hqq)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/dtypes/affine_quantized_tensor.py\", line 286, in from_hp_to_intx\r\n    layout_tensor = layout_tensor_ctr(data, scale, zero_point, layout_type)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/dtypes/affine_quantized_tensor.py\", line 1033, in from_plain\r\n    scale_and_zero = pack_tinygemm_scales_and_zeros(scale, zero_point)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchao/quantization/utils.py\", line 322, in pack_tinygemm_scales_and_zeros\r\n    torch.cat(\r\nRuntimeError: CUDA error: named symbol not found\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n### Versions\n\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               2\r\nOn-line CPU(s) list:                  0,1\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\r\nCPU family:                           6\r\nModel:                                79\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   1\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             4399.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            32 KiB (1 instance)\r\nL1i cache:                            32 KiB (1 instance)\r\nL2 cache:                             256 KiB (1 instance)\r\nL3 cache:                             55 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0,1\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Mitigation; PTE Inversion\r\nVulnerability Mds:                    Vulnerable; SMT Host state unknown\r\nVulnerability Meltdown:               Vulnerable\r\nVulnerability Mmio stale data:        Vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Vulnerable\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Vulnerable\r\nVulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.13.0\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241002+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.4.1+cu121\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtune==0.3.0.dev20240928+cu121\r\n[pip3] torchvision==0.20.0.dev20241002+cu121\r\n[conda] Could not collect",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-10-14T22:20:52Z",
      "updated_at": "2024-11-06T19:31:12Z",
      "closed_at": null,
      "labels": [
        "Compile / AOTI"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1298/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1298",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1298",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:23.924015",
      "comments": [
        {
          "author": "desertfire",
          "body": "The stack dump suggests this is a torchao issue.  For `RuntimeError: CUDA error: named symbol not found`, does it print out what exact symbol is missing? Can you share your installed CUDA version?",
          "created_at": "2024-10-15T17:04:19Z"
        },
        {
          "author": "mikekgfb",
          "body": "@desertfire:\r\n> The stack dump suggests this is a torchao issue. For `RuntimeError: CUDA error: named symbol not found`, does it print out what exact symbol is missing? Can you share your installed CUDA version?\r\n\r\nNVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2  \r\n",
          "created_at": "2024-10-17T19:54:25Z"
        },
        {
          "author": "mikekgfb",
          "body": "> The stack dump suggests this is a torchao issue. For `RuntimeError: CUDA error: named symbol not found`, does it print out what exact symbol is missing? Can you share your installed CUDA version?\r\n\r\nIt's a torchao issue (see also: https://github.com/pytorch/ao/issues/1110), or more generally a que",
          "created_at": "2024-11-06T19:29:52Z"
        }
      ]
    },
    {
      "issue_number": 1302,
      "title": "Out of memory AOTI using llama 3.1 8b on RTX 4090",
      "body": "### 🐛 Describe the bug\n\nI'm getting an out of memory error when trying to use an exported .so file. The .so is 16GB my GPU has 24GB of memory\r\n\r\n```\r\n(.venv) warden@Vikander:~/source/torchchat$ python3 torchchat.py generate llama3.1 --dso-path exportedModels/llama3.1.so --prompt \"Hello my name is\" --device cuda\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nUsing device=cuda NVIDIA GeForce RTX 4090\r\nLoading model...\r\nTime to load model: 1.85 seconds\r\nError: CUDA error: out of memory\r\nTraceback (most recent call last):\r\n  File \"/home/warden/source/torchchat/torchchat/cli/builder.py\", line 536, in _initialize_model\r\n    model.forward = torch._export.aot_load(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_export/__init__.py\", line 320, in aot_load\r\n    runner = torch._C._aoti.AOTIModelContainerRunnerCuda(so_path, 1, device)  # type: ignore[assignment, call-arg]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: create_func_( &container_handle_, num_models, device_str.c_str(), cubin_dir.empty() ? nullptr : cubin_dir.c_str()) API call failed at ../torch/csrc/inductor/aoti_runner/model_container_runner.cpp, line 85\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/warden/source/torchchat/torchchat.py\", line 88, in <module>\r\n    generate_main(args)\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 1215, in main\r\n    gen = Generator(\r\n          ^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/generate.py\", line 290, in __init__\r\n    self.model = _initialize_model(self.builder_args, self.quantize, self.tokenizer)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/torchchat/cli/builder.py\", line 540, in _initialize_model\r\n    raise RuntimeError(f\"Failed to load AOTI compiled {builder_args.dso_path}\")\r\nRuntimeError: Failed to load AOTI compiled exportedModels/llama3.1.so\r\n```\n\n### Versions\n\nOperating System Information\r\nLinux Vikander 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nPRETTY_NAME=\"Ubuntu 24.04.1 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"24.04\"\r\nVERSION=\"24.04.1 LTS (Noble Numbat)\"\r\nVERSION_CODENAME=noble\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=noble\r\nLOGO=ubuntu-logo\r\n\r\nPython Version\r\nPython 3.11.10\r\n\r\nPIP Version\r\npip 24.0 from /home/warden/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n\r\nInstalled Packages\r\nabsl-py==2.1.0\r\naccelerate==1.0.1\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.10\r\naiosignal==1.3.1\r\naltair==5.4.1\r\nannotated-types==0.7.0\r\nantlr4-python3-runtime==4.9.3\r\nanyio==4.6.2.post1\r\nattrs==24.2.0\r\nblinker==1.8.2\r\nblobfile==3.0.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\nchardet==5.2.0\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncmake==3.30.4\r\ncolorama==0.4.6\r\nDataProperty==1.0.1\r\ndatasets==3.0.1\r\ndill==0.3.8\r\ndistro==1.9.0\r\nevaluate==0.4.3\r\nfilelock==3.16.1\r\nFlask==3.0.3\r\nfrozenlist==1.4.1\r\nfsspec==2024.6.1\r\ngguf==0.10.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\nh11==0.14.0\r\nhttpcore==1.0.6\r\nhttpx==0.27.2\r\nhuggingface-hub==0.25.2\r\nidna==3.10\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.6.1\r\njoblib==1.4.2\r\njsonlines==4.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2024.10.1\r\nlm_eval==0.4.2\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==3.0.1\r\nmbstrdecoder==1.1.3\r\nmdurl==0.1.2\r\nmore-itertools==10.5.0\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nmultiprocess==0.70.16\r\nnarwhals==1.9.3\r\nnetworkx==3.4.1\r\nninja==1.11.1.1\r\nnltk==3.9.1\r\nnumexpr==2.10.1\r\nnumpy==1.26.4\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==9.1.0.70\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-nccl-cu12==2.21.5\r\nnvidia-nvjitlink-cu12==12.6.77\r\nnvidia-nvtx-cu12==12.1.105\r\nomegaconf==2.3.0\r\nopenai==1.51.2\r\npackaging==24.1\r\npandas==2.2.3\r\npathvalidate==3.2.1\r\npeft==0.13.2\r\npillow==10.4.0\r\nportalocker==2.10.1\r\npropcache==0.2.0\r\nprotobuf==5.28.2\r\npsutil==6.0.0\r\npyarrow==17.0.0\r\npybind11==2.13.6\r\npycryptodomex==3.21.0\r\npydantic==2.9.2\r\npydantic_core==2.23.4\r\npydeck==0.9.1\r\nPygments==2.18.0\r\npytablewriter==1.2.0\r\npython-dateutil==2.9.0.post0\r\npytorch-triton==3.1.0+cf34004b8a\r\npytz==2024.2\r\nPyYAML==6.0.2\r\nreferencing==0.35.1\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrich==13.9.2\r\nrouge-score==0.1.2\r\nrpds-py==0.20.0\r\nsacrebleu==2.4.3\r\nsafetensors==0.4.5\r\nscikit-learn==1.5.2\r\nscipy==1.14.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsnakeviz==2.2.0\r\nsniffio==1.3.1\r\nsqlitedict==2.1.0\r\nstreamlit==1.39.0\r\nsympy==1.13.1\r\ntabledata==1.3.3\r\ntabulate==0.9.0\r\ntcolorpy==0.1.6\r\ntenacity==9.0.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.8.0\r\ntokenizers==0.20.1\r\ntoml==0.10.2\r\ntorch==2.6.0.dev20241002+cu121\r\ntorchao==0.5.0\r\ntorchtune==0.3.0.dev20240928+cu121\r\ntorchvision==0.20.0.dev20241002+cu121\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntqdm-multiprocess==0.0.11\r\ntransformers==4.45.2\r\ntypepy==1.3.2\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nurllib3==2.2.3\r\nwatchdog==5.0.3\r\nWerkzeug==3.0.4\r\nword2number==1.1\r\nxxhash==3.5.0\r\nyarl==1.15.2\r\nzstandard==0.23.0\r\nzstd==1.5.5.1\r\n\r\nPyTorch Version\r\n2.6.0.dev20241002+cu121\r\n",
      "state": "closed",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-10-15T20:26:05Z",
      "updated_at": "2024-11-06T16:41:58Z",
      "closed_at": "2024-11-06T16:41:58Z",
      "labels": [
        "Compile / AOTI"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1302/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "desertfire"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1302",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1302",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:24.126974",
      "comments": [
        {
          "author": "desertfire",
          "body": "Just to double check, does torch.compile work?",
          "created_at": "2024-10-15T20:39:01Z"
        },
        {
          "author": "byjlw",
          "body": "> Just to double check, does torch.compile work?\r\n\r\nYes --compile works but it's 3x slower than eager\r\n\r\nalso if i quantize the model during export for AOTI i don't OOM",
          "created_at": "2024-10-16T18:45:11Z"
        },
        {
          "author": "desertfire",
          "body": "also reported in https://github.com/pytorch/torchchat/issues/996",
          "created_at": "2024-10-29T18:40:37Z"
        },
        {
          "author": "desertfire",
          "body": "https://github.com/pytorch/torchchat/pull/1337 should fix the issue. @byjlw , please give it a try and let me know the result.",
          "created_at": "2024-11-01T18:28:48Z"
        }
      ]
    },
    {
      "issue_number": 1224,
      "title": "Llama 3.2 MM Multiturn Browser: Second message errors out ",
      "body": "### 🐛 Describe the bug\r\n\r\nKick off a server (tested on CPU)\r\n` python3 torchchat.py server llama3.2-11B`\r\n\r\nIn a separate terminal open the browser: \r\n`streamlit run torchchat/usages/browser.py`\r\n\r\nFirst send a message with an image. \r\n\r\nWhen that completes\r\n* remove the image from the LHS bar\r\n* Send a new message\r\n---\r\nError when sending the second message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/werkzeug/serving.py\", line 370, in run_wsgi\r\n    execute(self.server.app)\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/werkzeug/serving.py\", line 333, in execute\r\n    for data in application_iter:\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/werkzeug/wsgi.py\", line 256, in __next__\r\n    return self._next()\r\n           ^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/werkzeug/wrappers/response.py\", line 32, in _iter_encoded\r\n    for item in iterable:\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/torchchat/usages/server.py\", line 77, in chunk_processor\r\n    for chunk in chunked_completion_generator:\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/torchchat/usages/openai_api.py\", line 481, in chunked_completion\r\n    for y, _ in self.generate(\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/torchchat/generate.py\", line 633, in generate\r\n    next_token = self.prefill(\r\n                 ^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/torchchat/generate.py\", line 373, in prefill\r\n    logits = model(tokens=x, mask=mask, encoder_input=encoder_input, input_pos=input_pos, encoder_mask=encoder_mask)[:, -1]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/torchchat/model.py\", line 560, in forward\r\n    return self.model(\r\n           ^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torchtune/modules/model_fusion/_fusion.py\", line 448, in forward\r\n    output = self.decoder(\r\n             ^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torchtune/modules/transformer.py\", line 599, in forward\r\n    h = layer(\r\n        ^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torchtune/modules/transformer.py\", line 114, in forward\r\n    attn_out = self.attn(h, h, mask=mask, input_pos=input_pos)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torchtune/modules/attention.py\", line 297, in forward\r\n    output = self._attention_call(\r\n             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jackkhuu/Desktop/oss/torchchat/.venv/lib/python3.12/site-packages/torchtune/modules/attention_utils.py\", line 236, in _attention_call\r\n    return nn.functional.scaled_dot_product_attention(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: The size of tensor a (265) must match the size of tensor b (277) at non-singleton dimension 3\r\n```\r\n\r\n### Versions\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240901+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Stream 9 (x86_64)\r\nGCC version: (GCC) 11.5.0 20240719 (Red Hat 11.5.0-2)\r\nClang version: Could not collect\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.34\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-6.4.3-0_fbk12_hardened_2624_g7d95a0297d81-x86_64-with-glibc2.34\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA PG509-210\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.9.4\r\n/usr/lib64/libcudnn_adv_infer.so.8.9.4\r\n/usr/lib64/libcudnn_adv_train.so.8.9.4\r\n/usr/lib64/libcudnn_cnn_infer.so.8.9.4\r\n/usr/lib64/libcudnn_cnn_train.so.8.9.4\r\n/usr/lib64/libcudnn_ops_infer.so.8.9.4\r\n/usr/lib64/libcudnn_ops_train.so.8.9.4\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             22\r\nOn-line CPU(s) list:                0-21\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz\r\nCPU family:                         6\r\nModel:                              85\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 22\r\nSocket(s):                          1\r\nStepping:                           11\r\nBogoMIPS:                           3591.57\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 arat vnmi umip pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          704 KiB (22 instances)\r\nL1i cache:                          704 KiB (22 instances)\r\nL2 cache:                           88 MiB (22 instances)\r\nL3 cache:                           16 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-21\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Vulnerable\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; TSX disabled\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton==3.0.0+dedb7bdf33\r\n[pip3] torch==2.5.0.dev20240901+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.0.0\r\n[pip3] torchvision==0.20.0.dev20240901+cu121\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+dedb7bdf33          pypi_0    pypi\r\n[conda] torch                     2.5.0.dev20240901+cu121          pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchtune                 0.0.0                    pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20240901+cu121          pypi_0    pypi",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-09-28T00:04:51Z",
      "updated_at": "2024-10-31T21:32:42Z",
      "closed_at": "2024-10-31T21:32:42Z",
      "labels": [
        "bug",
        "Browser",
        "Llama 3.2- Multimodal"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1224/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1224",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1224",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:24.352444",
      "comments": []
    },
    {
      "issue_number": 1032,
      "title": "RFC: Make quantization a first class feature",
      "body": "### 🚀 The feature, motivation and pitch\n\ntorchchat provides quantization functionality but the interaction isn't ideal. \r\n### Currently\r\nCurrently you can run generate with a --quantization flag and it quantize the model before running inference. This means that you have to quantize (expensive) every time you want to run generate. It also means you can't ask for a quantized model for use in another project. \r\n```\r\npython3 torchchat.py generate llama2 --quantize '{\"linear:int4\": {\"groupsize\": 128}, \"precision\": {\"dtype\":\"float16\"}, \"executor\":{\"accelerator\":\"mps\"}}' --prompt \"Once upon a time,\" --max-new-tokens 256 --num-samples 3 --seed 42\r\n```\r\n\r\n### Expectation\r\nIdeally you should be able to save/cache the quantized model during generate *and* run a command to just quantize and get the outputted model\r\n\r\n**Add a quantize command**\r\n```\r\npython3 torchchat.py quantize llama2  '{\"linear:int4\": {\"groupsize\": 128}, \"precision\": {\"dtype\":\"float16\"}, \"executor\":{\"accelerator\":\"mps\"}}' --output-file llama2-4b.pt\r\n``` \r\n- the quantize.md file should enumerate all options available in a table along with a suggested option\r\n- the quantize.md file should provide examples\r\n- the readme.md file should include one example for 4bit (often used for benchmarking)\r\n- optional output-file (if flag isn't present just store it in our default model location with a well defined scheme `{modelname}-{quant}.pt`)\r\n\r\n**Cache the quantized versions when you run using --quantize**\r\n- the `list` command should show the quantized versions\r\n- the `remove` command should delete the quantized version\r\n\r\n### Additionally\r\n- common quantization recipes are stored in a json file and can be passed in\r\n - `torchchat/config/quantization/llama-4bit.json`\r\n - with the contents `{\"linear:int4\": {\"groupsize\": 128}, \"precision\": {\"dtype\":\"float16\"}, \"executor\":{\"accelerator\":\"mps\"}}`\r\n - allowing for the command `python3 torchchat.py quantize llama2 --config config/quantization/llama-4bit.json --output-file llama2-4bit.json`\r\n \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-08-15T16:16:17Z",
      "updated_at": "2024-10-30T20:35:35Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1032/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1032",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1032",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:24.352463",
      "comments": []
    },
    {
      "issue_number": 1277,
      "title": "Android demo app poor model performance",
      "body": "### 🐛 Describe the bug\n\nI wanted to try the new Llama 3.2 1B parameter model on mobile. I downloaded the model and generated the `pte` like so:\r\n\r\n```\r\npython torchchat.py download llama3.2-1b\r\npython torchchat.py export llama3.2-1b --quantize torchchat/quant_config/mobile.json --output-pte-path llama3_2-1b.pte\r\n```\r\n\r\nThen I pushed `llama3_2-1b.pte` file and `tokenizer.model` files to the mobile phone using `adb`. \r\n\r\nI executed the demo app in `torchchat/edge/android/torchchat` using Android Studio with `.aar` file provided on the TorchChat repo readme.\r\n\r\nHowever, when I chat with the AI its responses are very useless and feel quite different than what I get with the same prompt on my computer:\r\n\r\n![example](https://github.com/user-attachments/assets/8e9d7128-6afd-46b5-8c1f-6b03ad3bccbb)\r\n![terminal-interaction](https://github.com/user-attachments/assets/beb9733a-3b23-40e7-9354-43a97cb05fa0)\r\n\r\nIs there a problem with the default quantization parameters? I tried to not quantize but then the app crashed when loading the model.\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240901\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.4 (arm64)\r\nGCC version: Could not collect\r\nClang version: 15.0.0 (clang-1500.3.9.4)\r\nCMake version: version 3.30.4\r\nLibc version: N/A\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 03:54:28) [Clang 12.0.0 ] (64-bit runtime)\r\nPython platform: macOS-14.4-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M2 Pro\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.5.0a0+286799c\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240901\r\n[pip3] torchao==0.5.0+git0916b5b\r\n[pip3] torchaudio==2.5.0.dev20240901\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchtune==0.3.0.dev20240928+cpu\r\n[pip3] torchvision==0.20.0.dev20240901\r\n[conda] executorch                0.5.0a0+286799c          pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.5.0.dev20240901          pypi_0    pypi\r\n[conda] torchaudio                2.5.0.dev20240901          pypi_0    pypi\r\n[conda] torchsr                   1.0.4                    pypi_0    pypi\r\n[conda] torchtune                 0.3.0.dev20240928+cpu          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20240901          pypi_0    pypi",
      "state": "closed",
      "author": "fran-aubry",
      "author_type": "User",
      "created_at": "2024-10-06T15:10:55Z",
      "updated_at": "2024-10-25T08:19:10Z",
      "closed_at": "2024-10-25T08:19:09Z",
      "labels": [
        "actionable",
        "Mobile - Android",
        "ExecuTorch"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1277/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "kirklandsign"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1277",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1277",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:24.352473",
      "comments": [
        {
          "author": "vmpuri",
          "body": "This looks like the type of bug that occurs when we aren't including the proper EOS/BOS and role headers to the messages. The model's trying to \"autocomplete\" your message rather than \"chat\" with you.\r\n\r\ncc. @kirklandsign can you confirm the header formatting is correct for LLaMA3-type models?",
          "created_at": "2024-10-07T16:44:43Z"
        },
        {
          "author": "kirklandsign",
          "body": "Hi @fran-aubry @vmpuri  the app is not updated and doesn't have modes like instruct and doesn't handle EOS/BOS. We need to update to use the same one as ET if we need to handle that",
          "created_at": "2024-10-07T16:50:40Z"
        },
        {
          "author": "fran-aubry",
          "body": "I'm working on a tutorial teaching people how to set-up Llama 3.2 1B on their mobile phone. I thought torchchat would be the easiest way to go.\r\n\r\nWill this be implemented or should I look for another way? ",
          "created_at": "2024-10-08T03:52:29Z"
        },
        {
          "author": "kirklandsign",
          "body": "cc @Jack-Khuu @vmpuri should we update the app?",
          "created_at": "2024-10-08T19:17:08Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Yup, we should update the app. Should be relatively low lift (we already did it locally with Mengwei, just need to push and test)",
          "created_at": "2024-10-09T02:10:25Z"
        }
      ]
    },
    {
      "issue_number": 979,
      "title": "Self-documenting repo: Too many files and folders in root directory",
      "body": "### 🚀 The feature, motivation and pitch\n\nThe repo quite doesn't pass the smell test for being an example in that the number of files and folders in root are too many.\r\nA lot of times, repos are self-documenting, by keeping the file/folder structure simple.\r\n\r\nI couldn't find ton of scope for files, but I think:\r\n\r\n* I think `requirements-lintrunner.txt` can go somewhere else.\r\n* It was annoying that `export.py` literally had no export logic and was pointing to a full folder of files.\r\n  * I think `export.py` should just have two functions, `export_edge` and `export_server`, and all logic we need should be in-lined there, instead of the reader having to jump across multiple files. Also, the `export_edge` code should be simplified (right now its across 3 or 4 files, not sure what its doing).\r\n\r\nThe folders are just too many. We should try to get down to 4 root folders that make sense?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "soumith",
      "author_type": "User",
      "created_at": "2024-07-31T16:34:52Z",
      "updated_at": "2024-10-24T00:57:50Z",
      "closed_at": "2024-10-24T00:57:50Z",
      "labels": [
        "actionable"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/979/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/979",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/979",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:24.603623",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks again for the feedback\r\n\r\nCircling back and closing out suggestions\r\n* Reduced the # of top level directories (will reduce even further with refactor with ET)\r\n* Collapsed related files together to reduce superfluous abstractions",
          "created_at": "2024-10-24T00:57:50Z"
        }
      ]
    },
    {
      "issue_number": 1316,
      "title": "[easy] Outdated Android README Image",
      "body": "### 🐛 Describe the bug\n\nhttps://github.com/pytorch/torchchat/commit/c867660049c6bbceefbaa2341f7d9c79832f2f7f introduced a new and improved Android app, but the README image still shows the old app in https://github.com/pytorch/torchchat?tab=readme-ov-file#deploy-and-run-on-android\r\n\r\nWe should update this\n\n### Versions\n\nNA",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-10-18T20:50:46Z",
      "updated_at": "2024-10-21T18:17:59Z",
      "closed_at": "2024-10-21T18:17:59Z",
      "labels": [
        "actionable",
        "Mobile - Android"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1316/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1316",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1316",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:24.828017",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "cc: @kirklandsign ",
          "created_at": "2024-10-18T20:51:00Z"
        },
        {
          "author": "kirklandsign",
          "body": "https://github.com/pytorch/torchchat/pull/1317",
          "created_at": "2024-10-18T21:03:29Z"
        }
      ]
    },
    {
      "issue_number": 1267,
      "title": "Add changelog generation, releases, publish pypi package",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\n- Would be nice to have automatically **generated changelog** from commit history. Maybe github's\r\n[generated release notes](https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes) or with [git-cliff](https://github.com/orhun/git-cliff) ([example](https://github.com/stanfordnlp/dspy/issues/1455#issuecomment-2338339308)). Then it's **easier to reason about** what **changes** were introduced when.\r\n- Would also be nice to see [torchchat/releases](https://github.com/pytorch/torchchat/releases) populated as new updates are rolled out (e.g. like in [torchtune/releases](https://github.com/pytorch/torchtune/releases)). Then it's **easier to switch between different versions** to test & reproduce functionality.\r\n- Would also be nice to have a possibility to install torchchat via `pip install` as a pypi package. The currently existing [torchcat pypi package](https://pypi.org/project/torchchat/) doesn't represent the latest contents of this repo. Then it's **easier to install torchchat** with just one cli command, as with [torchtune](https://github.com/pytorch/torchtune?tab=readme-ov-file#install-stable-release) `pip install torchtune`\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\r\n\r\n### RFC (Optional)\r\n\r\n_No response_",
      "state": "open",
      "author": "nongrata081",
      "author_type": "User",
      "created_at": "2024-10-04T12:09:36Z",
      "updated_at": "2024-10-18T20:56:28Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1267/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1267",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1267",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:26.954625",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for the suggestions @nongrata081, you've hit on many of the things that I've been meaning to do (or bug others to add)\r\n\r\n> generated changelog\r\n\r\n> releases\r\n\r\nDefinitely something we plan to do when we start doing regular releases. Since we're still aggressively making changes we haven't be",
          "created_at": "2024-10-07T22:44:17Z"
        },
        {
          "author": "nongrata081",
          "body": "@Jack-Khuu awesome! Would love to help. Please let me know if you think this is something I might have a look into?",
          "created_at": "2024-10-08T05:46:55Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "We would more than appreciate the help!!\r\n\r\nIf you want to give spinning up a pyproject a shot, please do!! (feel free to open up an issue if you run into any blockers or questions)\r\n* I can also jot up a more detailed description when i get a sec",
          "created_at": "2024-10-08T18:14:56Z"
        },
        {
          "author": "nongrata081",
          "body": "I've done some work on the [commit to add changelog generation](https://github.com/pytorch/torchchat/compare/main...nongrata081:torchchat:main)\r\n\r\nHere are:\r\n- [Template](https://github.com/nongrata081/torchchat/blob/main/changelog-template.toml) for changelog gen\r\n- generated [changelog](https://gi",
          "created_at": "2024-10-09T19:18:20Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thanks for the links! I'll try to take a look before the weekend",
          "created_at": "2024-10-10T19:01:04Z"
        }
      ]
    },
    {
      "issue_number": 1313,
      "title": "distributed fails with basic test command",
      "body": "### 🐛 Describe the bug\n\n`torchrun --nproc-per-node 2 dist_run.py llama3 --p 1`\r\n\r\n```\r\n10-17 20:27:43.781 - checkpoint_utils:208 - Loading 0 weights into stage dict\r\n10-17 20:27:43.784 - checkpoint_utils:215 - Successfully loaded 0 weights into stage module\r\n10-17 20:27:43.784 - checkpoint_utils:392 - Success - Loaded 0 weights, 291 missing weights\r\n[rank1]: Traceback (most recent call last):\r\n[rank1]:   File \"/home/warden/source/torchchat/dist_run.py\", line 621, in <module>\r\n[rank1]:     main(args)\r\n[rank1]:   File \"/home/warden/source/torchchat/dist_run.py\", line 378, in main\r\n[rank1]:     _load_model_weights(model, distribution, device, config, args.chpt_from)\r\n[rank1]:   File \"/home/warden/source/torchchat/dist_run.py\", line 151, in _load_model_weights\r\n[rank1]:     load_weights_from_hf_format(stage_module, distribution, device, model_config)\r\n[rank1]:   File \"/home/warden/source/torchchat/torchchat/distributed/checkpoint_utils.py\", line 396, in load_weights_from_hf_format\r\n[rank1]:     raise ValueError(f\"Missing {num_missing_weights} weights\")\r\n[rank1]: ValueError: Missing 291 weights\r\n10-17 20:27:43.791 - checkpoint_utils:215 - Successfully loaded 0 weights into stage module\r\n10-17 20:27:43.791 - checkpoint_utils:392 - Success - Loaded 0 weights, 291 missing weights\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/warden/source/torchchat/dist_run.py\", line 621, in <module>\r\n[rank0]:     main(args)\r\n[rank0]:   File \"/home/warden/source/torchchat/dist_run.py\", line 378, in main\r\n[rank0]:     _load_model_weights(model, distribution, device, config, args.chpt_from)\r\n[rank0]:   File \"/home/warden/source/torchchat/dist_run.py\", line 151, in _load_model_weights\r\n[rank0]:     load_weights_from_hf_format(stage_module, distribution, device, model_config)\r\n[rank0]:   File \"/home/warden/source/torchchat/torchchat/distributed/checkpoint_utils.py\", line 396, in load_weights_from_hf_format\r\n[rank0]:     raise ValueError(f\"Missing {num_missing_weights} weights\")\r\n[rank0]: ValueError: Missing 291 weights\r\n[rank1]:[W1017 20:27:44.538472817 ProcessGroupNCCL.cpp:1253] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\n[rank0]:[W1017 20:27:44.639635783 ProcessGroupNCCL.cpp:1253] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\nW1017 20:27:44.376000 96098 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 96131 closing signal SIGTERM\r\nE1017 20:27:44.497000 96098 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 96132) of binary: /home/warden/source/torchchat/.venv/bin/python3.11\r\nTraceback (most recent call last):\r\n  File \"/home/warden/source/torchchat/.venv/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\r\n    run(args)\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\r\n    elastic_launch(\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n============================================================\r\ndist_run.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-10-17_20:27:44\r\n  host      : Vikander\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 96132)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n\r\n```\r\n**Full output is here**\r\n[output.txt](https://github.com/user-attachments/files/17427436/output.txt)\r\n\n\n### Versions\n\nOperating System Information\r\nLinux Vikander 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nPRETTY_NAME=\"Ubuntu 24.04.1 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"24.04\"\r\nVERSION=\"24.04.1 LTS (Noble Numbat)\"\r\nVERSION_CODENAME=noble\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=noble\r\nLOGO=ubuntu-logo\r\n\r\nPython Version\r\nPython 3.11.10\r\n\r\nPIP Version\r\npip 24.0 from /home/warden/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n\r\nInstalled Packages\r\nabsl-py==2.1.0\r\naccelerate==1.0.1\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.10\r\naiosignal==1.3.1\r\naltair==5.4.1\r\nannotated-types==0.7.0\r\nantlr4-python3-runtime==4.9.3\r\nanyio==4.6.2.post1\r\nattrs==24.2.0\r\nblinker==1.8.2\r\nblobfile==3.0.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\nchardet==5.2.0\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncmake==3.30.4\r\ncolorama==0.4.6\r\nDataProperty==1.0.1\r\ndatasets==3.0.1\r\ndill==0.3.8\r\ndistro==1.9.0\r\nevaluate==0.4.3\r\nfilelock==3.16.1\r\nFlask==3.0.3\r\nfrozenlist==1.4.1\r\nfsspec==2024.6.1\r\ngguf==0.10.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\nh11==0.14.0\r\nhttpcore==1.0.6\r\nhttpx==0.27.2\r\nhuggingface-hub==0.25.2\r\nidna==3.10\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.6.1\r\njoblib==1.4.2\r\njsonlines==4.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2024.10.1\r\nlm_eval==0.4.2\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==3.0.1\r\nmbstrdecoder==1.1.3\r\nmdurl==0.1.2\r\nmore-itertools==10.5.0\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nmultiprocess==0.70.16\r\nnarwhals==1.9.3\r\nnetworkx==3.4.1\r\nninja==1.11.1.1\r\nnltk==3.9.1\r\nnumexpr==2.10.1\r\nnumpy==1.26.4\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==9.1.0.70\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-nccl-cu12==2.21.5\r\nnvidia-nvjitlink-cu12==12.6.77\r\nnvidia-nvtx-cu12==12.1.105\r\nomegaconf==2.3.0\r\nopenai==1.51.2\r\npackaging==24.1\r\npandas==2.2.3\r\npathvalidate==3.2.1\r\npeft==0.13.2\r\npillow==10.4.0\r\nportalocker==2.10.1\r\npropcache==0.2.0\r\nprotobuf==5.28.2\r\npsutil==6.0.0\r\npyarrow==17.0.0\r\npybind11==2.13.6\r\npycryptodomex==3.21.0\r\npydantic==2.9.2\r\npydantic_core==2.23.4\r\npydeck==0.9.1\r\nPygments==2.18.0\r\npytablewriter==1.2.0\r\npython-dateutil==2.9.0.post0\r\npytorch-triton==3.1.0+cf34004b8a\r\npytz==2024.2\r\nPyYAML==6.0.2\r\nreferencing==0.35.1\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrich==13.9.2\r\nrouge-score==0.1.2\r\nrpds-py==0.20.0\r\nsacrebleu==2.4.3\r\nsafetensors==0.4.5\r\nscikit-learn==1.5.2\r\nscipy==1.14.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsnakeviz==2.2.0\r\nsniffio==1.3.1\r\nsqlitedict==2.1.0\r\nstreamlit==1.39.0\r\nsympy==1.13.1\r\ntabledata==1.3.3\r\ntabulate==0.9.0\r\ntcolorpy==0.1.6\r\ntenacity==9.0.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.8.0\r\ntokenizers==0.20.1\r\ntoml==0.10.2\r\ntorch==2.6.0.dev20241002+cu121\r\ntorchao==0.5.0\r\ntorchtune==0.4.0.dev20241010+cu121\r\ntorchvision==0.20.0.dev20241002+cu121\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntqdm-multiprocess==0.0.11\r\ntransformers==4.45.2\r\ntypepy==1.3.2\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nurllib3==2.2.3\r\nwatchdog==5.0.3\r\nWerkzeug==3.0.4\r\nword2number==1.1\r\nxxhash==3.5.0\r\nyarl==1.15.2\r\nzstandard==0.23.0\r\nzstd==1.5.5.1\r\n\r\nPyTorch Version\r\n2.6.0.dev20241002+cu121\r\n\r\nCollection Complete\r\n",
      "state": "closed",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-10-18T03:31:01Z",
      "updated_at": "2024-10-18T19:03:21Z",
      "closed_at": "2024-10-18T19:03:21Z",
      "labels": [
        "Distributed"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1313/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lessw2020"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1313",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1313",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:27.173705",
      "comments": [
        {
          "author": "lessw2020",
          "body": "Hi @byjlw \r\nThe error here is that the pointer provided by hf cache via their api does not map to valid files. Can you review your hf cache at this dir and see what is amiss?   \r\n~~~\r\n10-17 20:27:43.774 - checkpoint_utils:193 - ERROR - File not found: /home/warden/.cache/huggingface/hub/models--meta",
          "created_at": "2024-10-18T04:32:18Z"
        },
        {
          "author": "lessw2020",
          "body": "also minor nit but your command has an incorrect param --p 1..should be --pp 1 but since that's the default it doesn't impact anything. \r\n~~~\r\ntorchrun --nproc-per-node 2 dist_run.py llama3 --p 1\r\n~~~\r\nsecondly, ran on my dev server (exact command including --p1 in stead of --pp 1) and all working a",
          "created_at": "2024-10-18T05:33:23Z"
        },
        {
          "author": "byjlw",
          "body": "downloaded the files using the huggingface-cli and things worked. ",
          "created_at": "2024-10-18T19:03:21Z"
        }
      ]
    },
    {
      "issue_number": 1293,
      "title": "export to AOTI using cuda  doesn't work using WSL",
      "body": "### 🐛 Describe the bug\n\n` python3 torchchat.py export llama3.1 --output-dso-path exportedModels/llama3.1.so`\r\n\r\n```\r\nUsing device=cuda\r\nSetting max_seq_length to 300 for DSO export.\r\nLoading model...\r\nTime to load model: 2.74 seconds\r\n-----------------------------------------------------------\r\nExporting model using AOT Inductor to /home/warden/source/torchchat/exportedModels/llama3.1.so\r\nW1010 15:36:44.314000 6252 .venv/lib/python3.11/site-packages/torch/_export/__init__.py:225] +============================+\r\nW1010 15:36:44.314000 6252 .venv/lib/python3.11/site-packages/torch/_export/__init__.py:226] |     !!!   WARNING   !!!    |\r\nW1010 15:36:44.314000 6252 .venv/lib/python3.11/site-packages/torch/_export/__init__.py:227] +============================+\r\nW1010 15:36:44.314000 6252 .venv/lib/python3.11/site-packages/torch/_export/__init__.py:228] torch._export.aot_compile() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export()) instead.\r\n/tmp/tmpie2hawx7/main.c:5:10: fatal error: Python.h: No such file or directory\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\nTraceback (most recent call last):\r\n  File \"/home/warden/source/torchchat/torchchat.py\", line 97, in <module>\r\n    export_main(args)\r\n  File \"/home/warden/source/torchchat/torchchat/export.py\", line 422, in main\r\n    export_for_server(\r\n  File \"/home/warden/source/torchchat/torchchat/export.py\", line 68, in export_for_server\r\n    so = torch._export.aot_compile(\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_export/__init__.py\", line 303, in aot_compile\r\n    so_path = torch._inductor.aot_compile(gm, args, kwargs, options=options)  # type: ignore[arg-type]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/__init__.py\", line 204, in aot_compile\r\n    return compile_fx_aot(\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1150, in compile_fx_aot\r\n    compiled_lib_path = compile_fx(\r\n                        ^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1297, in compile_fx\r\n    return compile_fx(\r\n           ^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1329, in compile_fx\r\n    return compile_fx(\r\n           ^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1551, in compile_fx\r\n    return inference_compiler(unlifted_gm, example_inputs_)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1384, in fw_compiler_base\r\n    return _fw_compiler_base(model, example_inputs, is_inference)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1455, in _fw_compiler_base\r\n    return inner_compile(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/contextlib.py\", line 81, in inner\r\n    return func(*args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 465, in compile_fx_inner\r\n    return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\r\n    inner_compiled_fn = compiler_fn(gm, example_inputs)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 660, in _compile_fx_inner\r\n    compiled_graph = codegen_and_compile(\r\n                     ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 560, in codegen_and_compile\r\n    compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 872, in fx_codegen_and_compile\r\n    compiled_fn = graph.compile_to_fn()\r\n                  ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1933, in compile_to_fn\r\n    code, linemap = self.codegen_with_cpp_wrapper()\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1725, in codegen_with_cpp_wrapper\r\n    compiled = self.compile_to_module().call\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1877, in compile_to_module\r\n    return self._compile_to_module()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1883, in _compile_to_module\r\n    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\r\n                                                             ^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1822, in codegen\r\n    self.scheduler.codegen()\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py\", line 3423, in codegen\r\n    return self._codegen()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/scheduler.py\", line 3501, in _codegen\r\n    self.get_backend(device).codegen_node(node)\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py\", line 80, in codegen_node\r\n    return self._triton_scheduling.codegen_node(node)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/codegen/simd.py\", line 1194, in codegen_node\r\n    return self.codegen_node_schedule(node_schedule, buf_accesses, numel, rnumel)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/codegen/simd.py\", line 1403, in codegen_node_schedule\r\n    src_code = kernel.codegen_kernel()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/codegen/triton.py\", line 2760, in codegen_kernel\r\n    **self.inductor_meta_common(),\r\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/_inductor/codegen/triton.py\", line 2626, in inductor_meta_common\r\n    \"backend_hash\": torch.utils._triton.triton_hash_with_backend(),\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/utils/_triton.py\", line 65, in triton_hash_with_backend\r\n    backend = triton_backend()\r\n              ^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/torch/utils/_triton.py\", line 57, in triton_backend\r\n    target = driver.active.get_current_target()\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/runtime/driver.py\", line 23, in __getattr__\r\n    self._initialize_obj()\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/runtime/driver.py\", line 20, in _initialize_obj\r\n    self._obj = self._init_fn()\r\n                ^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/runtime/driver.py\", line 9, in _create_driver\r\n    return actives[0]()\r\n           ^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/backends/nvidia/driver.py\", line 371, in __init__\r\n    self.utils = CudaUtils()  # TODO: make static\r\n                 ^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/backends/nvidia/driver.py\", line 80, in __init__\r\n    mod = compile_module_from_src(Path(os.path.join(dirname, \"driver.c\")).read_text(), \"cuda_utils\")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/backends/nvidia/driver.py\", line 57, in compile_module_from_src\r\n    so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/runtime/build.py\", line 48, in _build\r\n    ret = subprocess.check_call(cc_cmd)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/subprocess.py\", line 413, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpie2hawx7/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpie2hawx7/cuda_utils.cpython-311-x86_64-linux-gnu.so', '-lcuda', '-L/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/backends/nvidia/lib', '-L/usr/lib/wsl/lib', '-I/home/warden/source/torchchat/.venv/lib/python3.11/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpie2hawx7', '-I/usr/include/python3.11']' returned non-zero exit status 1.\r\n```\n\n### Versions\n\nOperating System Information\r\nLinux Furiosa 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nPRETTY_NAME=\"Ubuntu 24.04.1 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"24.04\"\r\nVERSION=\"24.04.1 LTS (Noble Numbat)\"\r\nVERSION_CODENAME=noble\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=noble\r\nLOGO=ubuntu-logo\r\n\r\nPython Version\r\nPython 3.11.10\r\n\r\nPIP Version\r\npip 24.0 from /home/warden/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n\r\nInstalled Packages\r\nabsl-py==2.1.0\r\naccelerate==1.0.0\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.9\r\naiosignal==1.3.1\r\naltair==5.4.1\r\nannotated-types==0.7.0\r\nantlr4-python3-runtime==4.9.3\r\nanyio==4.6.0\r\nattrs==24.2.0\r\nblinker==1.8.2\r\nblobfile==3.0.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\nchardet==5.2.0\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncmake==3.30.4\r\ncolorama==0.4.6\r\nDataProperty==1.0.1\r\ndatasets==3.0.1\r\ndill==0.3.8\r\ndistro==1.9.0\r\nevaluate==0.4.3\r\nfilelock==3.16.1\r\nFlask==3.0.3\r\nfrozenlist==1.4.1\r\nfsspec==2024.6.1\r\ngguf==0.10.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\nh11==0.14.0\r\nhttpcore==1.0.6\r\nhttpx==0.27.2\r\nhuggingface-hub==0.25.2\r\nidna==3.10\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.6.1\r\njoblib==1.4.2\r\njsonlines==4.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2024.10.1\r\nlm_eval==0.4.2\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==3.0.1\r\nmbstrdecoder==1.1.3\r\nmdurl==0.1.2\r\nmore-itertools==10.5.0\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nmultiprocess==0.70.16\r\nnarwhals==1.9.2\r\nnetworkx==3.4\r\nninja==1.11.1.1\r\nnltk==3.9.1\r\nnumexpr==2.10.1\r\nnumpy==1.26.4\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==9.1.0.70\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-nccl-cu12==2.21.5\r\nnvidia-nvjitlink-cu12==12.6.77\r\nnvidia-nvtx-cu12==12.1.105\r\nomegaconf==2.3.0\r\nopenai==1.51.2\r\npackaging==24.1\r\npandas==2.2.3\r\npathvalidate==3.2.1\r\npeft==0.13.1\r\npillow==10.4.0\r\nportalocker==2.10.1\r\npropcache==0.2.0\r\nprotobuf==5.28.2\r\npsutil==6.0.0\r\npyarrow==17.0.0\r\npybind11==2.13.6\r\npycryptodomex==3.21.0\r\npydantic==2.9.2\r\npydantic_core==2.23.4\r\npydeck==0.9.1\r\nPygments==2.18.0\r\npytablewriter==1.2.0\r\npython-dateutil==2.9.0.post0\r\npytorch-triton==3.1.0+cf34004b8a\r\npytz==2024.2\r\nPyYAML==6.0.2\r\nreferencing==0.35.1\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrich==13.9.2\r\nrouge-score==0.1.2\r\nrpds-py==0.20.0\r\nsacrebleu==2.4.3\r\nsafetensors==0.4.5\r\nscikit-learn==1.5.2\r\nscipy==1.14.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsnakeviz==2.2.0\r\nsniffio==1.3.1\r\nsqlitedict==2.1.0\r\nstreamlit==1.39.0\r\nsympy==1.13.1\r\ntabledata==1.3.3\r\ntabulate==0.9.0\r\ntcolorpy==0.1.6\r\ntenacity==9.0.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.8.0\r\ntokenizers==0.20.1\r\ntoml==0.10.2\r\ntorch==2.6.0.dev20241002+cu121\r\ntorchao==0.5.0\r\ntorchtune==0.3.0.dev20240928+cu121\r\ntorchvision==0.20.0.dev20241002+cu121\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntqdm-multiprocess==0.0.11\r\ntransformers==4.45.2\r\ntypepy==1.3.2\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nurllib3==2.2.3\r\nwatchdog==5.0.3\r\nWerkzeug==3.0.4\r\nword2number==1.1\r\nxxhash==3.5.0\r\nyarl==1.14.0\r\nzstandard==0.23.0\r\nzstd==1.5.5.1\r\n\r\nPyTorch Version\r\n2.6.0.dev20241002+cu121",
      "state": "open",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-10-10T22:38:34Z",
      "updated_at": "2024-10-17T13:14:45Z",
      "closed_at": null,
      "labels": [
        "Compile / AOTI"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1293/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1293",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1293",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:27.441069",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Hmm i don't seem to be able to repro this, do you only see it on a WSL machine or do you see it from a pure linux machine as well?\r\n\r\ni.e. I can export using the same command on a clean install",
          "created_at": "2024-10-11T00:15:59Z"
        },
        {
          "author": "gmagogsfm",
          "body": "@pianpwk ",
          "created_at": "2024-10-11T15:56:43Z"
        },
        {
          "author": "byjlw",
          "body": "I haven't tried pure linux since finding this issue on wsl. ",
          "created_at": "2024-10-11T16:44:31Z"
        },
        {
          "author": "byjlw",
          "body": "Compile also throws a similar error\r\n\r\n```\r\n\r\npython3 torchchat.py generate llama3.1 --prompt \"tell me a story about a bear\r\nUsing device=cuda NVIDIA GeForce RTX 4090\r\nTime to load model: 8.90 seconds\r\n/tmp/tmpcok5wf6p/main.c:5:10: fatal error: Python.h: No such file or directory\r\n    5 | #include <",
          "created_at": "2024-10-11T19:40:31Z"
        },
        {
          "author": "desertfire",
          "body": "This looks like a WSL lib path issue. The compilation cmd is looking for `Python.h` in `-I/usr/include/python3.11`, which is probably wrong. cc @xuhancn who has more experience on Windows build.",
          "created_at": "2024-10-15T16:23:50Z"
        }
      ]
    },
    {
      "issue_number": 992,
      "title": "Leverage the HF cache for models",
      "body": "### 🚀 The feature, motivation and pitch\n\ntorchchat currently uses the hf hub which has it's own model cache, torchchat copies it into it's own model directory so you end up two copies of the same model. \r\n\r\nWe should leverage the hf hub cache but not force users to use that location if they're using their own models. \r\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n\r\n[From r/localllama ](https://www.reddit.com/r/LocalLLaMA/comments/1eh6xmq/comment/lfzxuer/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\r\n\"One annoying thing is that it uses huggingface_hub for downloading but doesn't use the HF cache - it uses it's own .torchtune folder to store models so you just end up having double of full models (grr). Just use the defaul HF cache location.”\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-08-01T17:32:43Z",
      "updated_at": "2024-10-09T06:58:36Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/992/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/992",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/992",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:27.728550",
      "comments": [
        {
          "author": "orionr",
          "body": "Great job bringing these back as issues! Is this also a problem with torchtune given that we're using `.torchtune` for this? cc @kartikayk ?",
          "created_at": "2024-08-01T20:19:03Z"
        },
        {
          "author": "vmpuri",
          "body": "Going to add that we can use `hf_transfer` to \"potentially double the download speed\"\r\nhttps://huggingface.co/docs/hub/models-downloading\r\nhttps://huggingface.co/docs/huggingface_hub/v0.25.1/package_reference/environment_variables#hfhubenablehftransfer\r\n\r\nThis is an option to use a rust-based downlo",
          "created_at": "2024-10-09T06:58:34Z"
        }
      ]
    },
    {
      "issue_number": 1244,
      "title": "Browser is unable to do multiple turns for text only models",
      "body": "### 🐛 Describe the bug\n\nThis was a feature that was supported previously. I assume this was broken with the introduction of Multimodal support.\r\n```\r\npython3 torchchat.py server llama3.1\r\nstreamlit run torchchat/usages/browser.py\r\n```\r\n\r\nSend 2 messages and you will get `AssertionError: At most one text prompt is supported for each request`\n\n### Versions\n\nhttps://github.com/pytorch/torchchat/commit/8c7e688afe0c1d8a36b318fc5ca87a3c3eb403e3",
      "state": "closed",
      "author": "Jack-Khuu",
      "author_type": "User",
      "created_at": "2024-10-01T02:25:05Z",
      "updated_at": "2024-10-01T21:20:03Z",
      "closed_at": "2024-10-01T21:20:03Z",
      "labels": [
        "Browser"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1244/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "byjlw",
        "Jack-Khuu",
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1244",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1244",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:27.966069",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "I'm gonna take this one to, push the fix though",
          "created_at": "2024-10-01T16:36:45Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "https://github.com/pytorch/torchchat/pull/1247",
          "created_at": "2024-10-01T18:37:54Z"
        }
      ]
    },
    {
      "issue_number": 1229,
      "title": "llama-3.2-11b-vision : size mismatch for encoder.clip.token_pos_embedding.global_token_positional_embedding",
      "body": "### 🐛 Describe the bug\n\nFrom a clean install using the current main branch, llama-3.2-11b-vision seems to need some love.\r\n\r\nThe download of the model files from HugginFace succeded using :\r\n`python3 torchchat.py download llama-3.2-11b-vision`\r\n\r\nHowever, the usage attempt of the model returns the following error :\r\n\r\n```\r\npython3 torchchat.py generate  meta-llama/llama-3.2-11b-vision --image-prompts test.png --prompt \"analyse\"\r\nFailed to load torchao experimental a8wxdq quantizer with error: [Errno 2] No such file or directory: \r\n/home/guillaume/test/torchchat/torchao-build/src/ao/torchao/experimental/quant_api.py\r\nUsing device=cpu Intel(R) Core(TM) i7-14700K\r\nLoading model...\r\nTime to load model: 0.10 seconds\r\nTraceback (most recent call last):\r\n  File \"/home/guillaume/test/torchchat/torchchat.py\", line 83, in <module>\r\n    generate_main(args)\r\n  File \"/home/guillaume/test/torchchat/torchchat/generate.py\", line 1093, in main\r\n    gen = Generator(\r\n  File \"/home/guillaume/test/torchchat/torchchat/generate.py\", line 284, in __init__\r\n    self.model = _initialize_model(self.builder_args, self.quantize, self.tokenizer)\r\n  File \"/home/guillaume/test/torchchat/torchchat/cli/builder.py\", line 562, in _initialize_model\r\n    model = _load_model(builder_args)\r\n  File \"/home/guillaume/test/torchchat/torchchat/cli/builder.py\", line 482, in _load_model\r\n    model = _load_model_default(builder_args)\r\n  File \"/home/guillaume/test/torchchat/torchchat/cli/builder.py\", line 405, in _load_model_default\r\n    model.model.load_state_dict(state_dict, assign=True, strict=False)\r\n  File \"/home/guillaume/test/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2568, in load_state_dict\r\n    raise RuntimeError(\r\nRuntimeError: Error(s) in loading state_dict for DeepFusionModel:\r\n        size mismatch for encoder.clip.token_pos_embedding.local_token_positional_embedding: copying a param with shape torch.Size([1025, 1280]) from checkpoint, the shape in current model is torch.Size([1601, 1280]).\r\n        size mismatch for encoder.clip.token_pos_embedding.global_token_positional_embedding: copying a param with shape torch.Size([4, 4, 1025, 1280]) from checkpoint, the shape in current model is torch.Size([4, 4, 1601, 1280])\r\n```\n\n### Versions\n\npython collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240901+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-40-generic-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture :                          x86_64\r\nMode(s) opératoire(s) des processeurs : 32-bit, 64-bit\r\nAddress sizes:                          46 bits physical, 48 bits virtual\r\nBoutisme :                              Little Endian\r\nProcesseur(s) :                         28\r\nListe de processeur(s) en ligne :       0-27\r\nIdentifiant constructeur :              GenuineIntel\r\nNom de modèle :                         Intel(R) Core(TM) i7-14700K\r\nFamille de processeur :                 6\r\nModèle :                                183\r\nThread(s) par cœur :                    2\r\nCœur(s) par socket :                    20\r\nSocket(s) :                             1\r\nRévision :                              1\r\nVitesse maximale du processeur en MHz : 5600,0000\r\nVitesse minimale du processeur en MHz : 800,0000\r\nBogoMIPS :                              6835.20\r\nDrapaux :                               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\r\nVirtualisation :                        VT-x\r\nCache L1d :                             768 KiB (20 instances)\r\nCache L1i :                             1 MiB (20 instances)\r\nCache L2 :                              28 MiB (11 instances)\r\nCache L3 :                              33 MiB (1 instance)\r\nNœud(s) NUMA :                          1\r\nNœud NUMA 0 de processeur(s) :          0-27\r\nVulnerability Gather data sampling:     Not affected\r\nVulnerability Itlb multihit:            Not affected\r\nVulnerability L1tf:                     Not affected\r\nVulnerability Mds:                      Not affected\r\nVulnerability Meltdown:                 Not affected\r\nVulnerability Mmio stale data:          Not affected\r\nVulnerability Reg file data sampling:   Mitigation; Clear Register File\r\nVulnerability Retbleed:                 Not affected\r\nVulnerability Spec rstack overflow:     Not affected\r\nVulnerability Spec store bypass:        Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:               Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:               Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                    Not affected\r\nVulnerability Tsx async abort:          Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240901+cpu\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.0.0\r\n[pip3] torchvision==0.20.0.dev20240901+cpu\r\n[conda] Could not collect\r\n\r\n",
      "state": "open",
      "author": "openconcerto",
      "author_type": "User",
      "created_at": "2024-09-29T08:49:09Z",
      "updated_at": "2024-09-30T20:00:30Z",
      "closed_at": null,
      "labels": [
        "bug",
        "Llama 3.2- Multimodal"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1229/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Jack-Khuu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1229",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1229",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:28.220946",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for flagging, let me take a look",
          "created_at": "2024-09-29T21:20:04Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Looks like there is some config mismatch between the two versions of 11B that's not being accounted for\r\n* `meta-llama/Llama-3.2-11B-Vision`\r\n* `meta-llama/Llama-3.2-11B-Vision-Instruct`\r\n\r\nWe'll look into getting this fixed for `meta-llama/Llama-3.2-11B-Vision`. In the meantime, we'd love for you t",
          "created_at": "2024-09-29T22:19:41Z"
        },
        {
          "author": "openconcerto",
          "body": "I confirm that it works with Llama-3.2-11B-Vision.\r\n\r\nIt takes 53 minutes to answer the prompt on a PNG image of  2409x3435 pixels,\r\non my  Intel 14700K with 128GB of RAM (no gpu).",
          "created_at": "2024-09-30T19:41:05Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thank for checking and sharing the numbers\r\n\r\nWe're working to get torch.compile up and running for 11B, so that'll speed things up when it's ready",
          "created_at": "2024-09-30T20:00:29Z"
        }
      ]
    },
    {
      "issue_number": 1222,
      "title": "Clear model download documents",
      "body": "### 🐛 Describe the bug\r\n\r\nFrom the README, its not very clear how to download different flavor/sizes of the models from HF, unless someone go to the next section and find the inventory list https://github.com/pytorch/torchchat#download-weights\r\nmight be helpful to add the inventory list  command upper before the the download command.\r\n\r\nAlso as we have 3.2 it would be great to update the docs. \r\n\r\n\r\n```\r\n\r\n/torchchat$ python3 torchchat.py list\r\n\r\nModel                                        Aliases                                                    Downloaded \r\n-------------------------------------------- ---------------------------------------------------------- -----------\r\nmeta-llama/llama-2-7b-hf                     llama2-base, llama2-7b                                                \r\nmeta-llama/llama-2-7b-chat-hf                llama2, llama2-chat, llama2-7b-chat                                   \r\nmeta-llama/llama-2-13b-chat-hf               llama2-13b-chat                                                       \r\nmeta-llama/llama-2-70b-chat-hf               llama2-70b-chat                                                       \r\nmeta-llama/meta-llama-3-8b                   llama3-base                                                           \r\nmeta-llama/meta-llama-3-8b-instruct          llama3, llama3-chat, llama3-instruct                       Yes        \r\nmeta-llama/meta-llama-3-70b-instruct         llama3-70b                                                            \r\nmeta-llama/meta-llama-3.1-8b                 llama3.1-base                                                         \r\nmeta-llama/meta-llama-3.1-8b-instruct        llama3.1, llama3.1-chat, llama3.1-instruct                            \r\nmeta-llama/meta-llama-3.1-70b-instruct       llama3.1-70b                                                          \r\nmeta-llama/meta-llama-3.1-8b-instruct-tune   llama3.1-tune, llama3.1-chat-tune, llama3.1-instruct-tune             \r\nmeta-llama/meta-llama-3.1-70b-instruct-tune  llama3.1-70b-tune                                                     \r\nmeta-llama/meta-llama-3.2-1b                 llama3.2-1b-base                                                      \r\nmeta-llama/meta-llama-3.2-1b-instruct        llama3.2-1b, llama3.2-1b-chat, llama3.2-1b-instruct                   \r\nmeta-llama/llama-guard-3-1b                  llama3-1b-guard, llama3.2-1b-guard                                    \r\nmeta-llama/meta-llama-3.2-3b                 llama3.2-3b-base                                                      \r\nmeta-llama/meta-llama-3.2-3b-instruct        llama3.2-3b, llama3.2-3b-chat, llama3.2-3b-instruct                   \r\nmeta-llama/llama-3.2-11b-vision              llama3.2-11B-base, Llama-3.2-11B-Vision-base                          \r\nmeta-llama/llama-3.2-11b-vision-instruct     llama3.2-11B, Llama-3.2-11B-Vision, Llama-3.2-mm                      \r\nmeta-llama/codellama-7b-python-hf            codellama, codellama-7b                                               \r\nmeta-llama/codellama-34b-python-hf           codellama-34b                                                         \r\nmistralai/mistral-7b-v0.1                    mistral-7b-v01-base                                                   \r\nmistralai/mistral-7b-instruct-v0.1           mistral-7b-v01-instruct                                               \r\nmistralai/mistral-7b-instruct-v0.2           mistral, mistral-7b, mistral-7b-instruct                              \r\nopenlm-research/open_llama_7b                open-llama, open-llama-7b                                             \r\nstories15m                                                                                                         \r\nstories42m                                                                                                         \r\nstories110m                                                                                                        \r\n\r\n```\r\n### Versions\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240901+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1068-aws-x86_64-with-glibc2.31\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nCPU(s):                               16\r\nOn-line CPU(s) list:                  0-15\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   8\r\nSocket(s):                            1\r\nNUMA node(s):                         1\r\nVendor ID:                            GenuineIntel\r\nCPU family:                           6\r\nModel:                                143\r\nModel name:                           Intel(R) Xeon(R) Platinum 8488C\r\nStepping:                             8\r\nCPU MHz:                              2400.000\r\nBogoMIPS:                             4800.00\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            384 KiB\r\nL1i cache:                            256 KiB\r\nL2 cache:                             16 MiB\r\nL3 cache:                             105 MiB\r\nNUMA node0 CPU(s):                    0-15\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd ida arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear serialize amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton==3.0.0+dedb7bdf33\r\n[pip3] torch==2.5.0.dev20240901+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.0.0\r\n[pip3] torchvision==0.20.0.dev20240901+cu121\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+dedb7bdf33          pypi_0    pypi\r\n[conda] torch                     2.5.0.dev20240901+cu121          pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchtune                 0.0.0                    pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20240901+cu121          pypi_0    pypi\r\n```",
      "state": "closed",
      "author": "HamidShojanazeri",
      "author_type": "User",
      "created_at": "2024-09-27T22:16:38Z",
      "updated_at": "2024-09-30T16:02:55Z",
      "closed_at": "2024-09-30T16:02:54Z",
      "labels": [
        "documentation",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1222/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1222",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1222",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:28.454871",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "> different flavor/sized of the models from HF\r\n\r\nCan you provide some examples? \r\nThat said it would be good idea for us to link to the model list earlier: https://github.com/pytorch/torchchat?tab=readme-ov-file#models\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/7ccf7a7f-87bf-4b4f-adc2",
          "created_at": "2024-09-27T22:22:02Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "As for 3.2 that's a good idea, it'll be faster for demos than running it on the 8B",
          "created_at": "2024-09-27T22:23:23Z"
        },
        {
          "author": "HamidShojanazeri",
          "body": "> > different flavor/sized of the models from HF\r\n> \r\n> Can you provide some examples? That said it would be good idea for us to link to the model list earlier: [pytorch/torchchat#models](https://github.com/pytorch/torchchat?tab=readme-ov-file#models)\r\n> \r\n> ![image](https://private-user-images.gith",
          "created_at": "2024-09-27T22:26:34Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Updated the README to upsell model list\r\n\r\nThanks again @HamidShojanazeri !",
          "created_at": "2024-09-30T16:02:54Z"
        }
      ]
    },
    {
      "issue_number": 1207,
      "title": "Distributed inference runtime error",
      "body": "### 🐛 Describe the bug\n\nWhen trying to run distributed/run_dist_inference.sh . It has below error.\r\n[rank0]:[rank0]:     model = _load_model(builder_args)\r\n[rank0]:[rank0]:   File \"/scratch/grace/torchchat/torchchat/cli/builder.py\", line 473, in _load_model\r\n[rank0]:[rank0]:     model = _maybe_parellelize_model(model, builder_args, world_mesh, parallel_dims)\r\n[rank0]:[rank0]:   File \"/scratch/grace/torchchat/torchchat/cli/builder.py\", line 460, in _maybe_parellelize_model\r\n[rank0]:[rank0]:     parallelize_llama(model, world_mesh, parallel_dims)\r\n[rank0]:[rank0]:   File \"/scratch/grace/torchchat/distributed/parallelize_llama.py\", line 124, in parallelize_llama\r\n[rank0]:[rank0]:     model = apply_tp(model, world_mesh)\r\n[rank0]:[rank0]:   File \"/scratch/grace/torchchat/distributed/parallelize_llama.py\", line 69, in apply_tp\r\n[rank0]:[rank0]:     for transformer_block in model.layers:\r\n[rank0]:[rank0]:   File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1729, in __getattr__\r\n[rank0]:[rank0]:     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\r\n[rank0]:[rank0]: AttributeError: 'TextOnlyModel' object has no attribute 'layers'\n\n### Versions\n\nPyTorch version: 2.4.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.0\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1045-azure-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\nGPU 2: NVIDIA A100 80GB PCIe\r\nGPU 3: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 535.86.10\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nNUMA node(s):                       4\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              1\r\nModel name:                         AMD EPYC 7V13 64-Core Processor\r\nStepping:                           1\r\nCPU MHz:                            2445.443\r\nBogoMIPS:                           4890.88\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          3 MiB\r\nL1i cache:                          3 MiB\r\nL2 cache:                           48 MiB\r\nL3 cache:                           384 MiB\r\nNUMA node0 CPU(s):                  0-23\r\nNUMA node1 CPU(s):                  24-47\r\nNUMA node2 CPU(s):                  48-71\r\nNUMA node3 CPU(s):                  72-95\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.23.5\r\n[pip3] onnx==1.16.2\r\n[pip3] onnxruntime-training==1.18.0\r\n[pip3] pytorch-lightning==1.9.5\r\n[pip3] torch==2.4.1\r\n[pip3] torch-nebula==0.16.13\r\n[pip3] torch-ort==1.18.0\r\n[pip3] torch-tb-profiler==0.4.3\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.4.1\r\n[pip3] torchdata==0.7.1\r\n[pip3] torchmetrics==1.2.0\r\n[pip3] torchsnapshot==0.1.0\r\n[pip3] torchtune==0.3.0\r\n[pip3] torchvision==0.19.1\r\n[pip3] triton==3.0.0\r\n[conda] magma-cuda121             2.6.1                         1    pytorch\r\n[conda] mkl                       2022.2.1                 pypi_0    pypi\r\n[conda] mkl-include               2022.2.1                 pypi_0    pypi\r\n[conda] numpy                     1.23.5                   pypi_0    pypi\r\n[conda] pytorch-lightning         1.9.5                    pypi_0    pypi\r\n[conda] torch                     2.4.1                    pypi_0    pypi\r\n[conda] torch-nebula              0.16.13                  pypi_0    pypi\r\n[conda] torch-ort                 1.18.0                   pypi_0    pypi\r\n[conda] torch-tb-profiler         0.4.3                    pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchaudio                2.4.1                    pypi_0    pypi\r\n[conda] torchdata                 0.7.1                    pypi_0    pypi\r\n[conda] torchmetrics              1.2.0                    pypi_0    pypi\r\n[conda] torchsnapshot             0.1.0                    pypi_0    pypi\r\n[conda] torchtune                 0.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.1                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi",
      "state": "open",
      "author": "cascade812",
      "author_type": "User",
      "created_at": "2024-09-25T19:18:37Z",
      "updated_at": "2024-09-30T08:10:24Z",
      "closed_at": null,
      "labels": [
        "Distributed"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1207/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1207",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1207",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:28.658371",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for flagging @guijuzhang. Can you share command you ran as well?\r\n\r\ncc: @kwen2501 for distrubuted",
          "created_at": "2024-09-30T08:10:22Z"
        }
      ]
    },
    {
      "issue_number": 1220,
      "title": "lm_eval not working in the model.py",
      "body": "### 🐛 Describe the bug\r\n\r\nFollowing the installation README, and running dist_run.py, it fails to import utils from evaluate module. It also seems not being used atm, may as well remove it?\r\n\r\n```\r\n(torchchat)  hamidnazeri@submit-0:~/torchchat$ python dist_run.py --model_name meta-llama/Meta-Llama-3-8B-Instruct\r\nTraceback (most recent call last):\r\n  File \"/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/hamidnazeri/torchchat/dist_run.py\", line 37, in <module>\r\n    from torchchat.cli.builder import _initialize_tokenizer, TokenizerArgs\r\n  File \"/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/hamidnazeri/torchchat/torchchat/cli/builder.py\", line 29, in <module>\r\n    from torchchat.model import Model, ModelArgs, ModelType\r\n  File \"/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/hamidnazeri/torchchat/torchchat/model.py\", line 35, in <module>\r\n    import lm_eval  # noqa \r\n  File \"/data/home/hamidnazeri/miniconda3-2/envs/torchchat/lib/python3.10/site-packages/lm_eval/__init__.py\", line 1, in <module>\r\n    from .evaluator import evaluate, simple_evaluate\r\n  File \"/data/home/hamidnazeri/miniconda3-2/envs/torchchat/lib/python3.10/site-packages/lm_eval/evaluator.py\", line 11, in <module>\r\n    import lm_eval.api.metrics\r\n  File \"/data/home/hamidnazeri/miniconda3-2/envs/torchchat/lib/python3.10/site-packages/lm_eval/api/metrics.py\", line 168, in <module>\r\n    exact_match = hf_evaluate.load(\"exact_match\")\r\n  File \"/data/home/hamidnazeri/miniconda3-2/envs/torchchat/lib/python3.10/site-packages/evaluate/loading.py\", line 751, in load\r\n    evaluation_cls = import_main_class(evaluation_module.module_path)\r\n  File \"/data/home/hamidnazeri/miniconda3-2/envs/torchchat/lib/python3.10/site-packages/evaluate/loading.py\", line 76, in import_main_class\r\n    module = importlib.import_module(module_path)\r\n  File \"/data/home/hamidnazeri/miniconda3-2/envs/torchchat/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/data/home/hamidnazeri/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--exact_match/9d3b67e0c429cd7460b2b05aab53419b48eea369b73e1d9f185a56ca90c373d4/exact_match.py\", line 86, in <module>\r\n    @evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\r\nAttributeError: module 'evaluate' has no attribute 'utils'\r\n\r\n```\r\n\r\n### Versions\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240901+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1068-aws-x86_64-with-glibc2.31\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nCPU(s):                               16\r\nOn-line CPU(s) list:                  0-15\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   8\r\nSocket(s):                            1\r\nNUMA node(s):                         1\r\nVendor ID:                            GenuineIntel\r\nCPU family:                           6\r\nModel:                                143\r\nModel name:                           Intel(R) Xeon(R) Platinum 8488C\r\nStepping:                             8\r\nCPU MHz:                              2400.000\r\nBogoMIPS:                             4800.00\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            384 KiB\r\nL1i cache:                            256 KiB\r\nL2 cache:                             16 MiB\r\nL3 cache:                             105 MiB\r\nNUMA node0 CPU(s):                    0-15\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd ida arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear serialize amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton==3.0.0+dedb7bdf33\r\n[pip3] torch==2.5.0.dev20240901+cu121\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.0.0\r\n[pip3] torchvision==0.20.0.dev20240901+cu121\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+dedb7bdf33          pypi_0    pypi\r\n[conda] torch                     2.5.0.dev20240901+cu121          pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchtune                 0.0.0                    pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20240901+cu121          pypi_0    pypi\r\n```",
      "state": "closed",
      "author": "HamidShojanazeri",
      "author_type": "User",
      "created_at": "2024-09-27T17:54:17Z",
      "updated_at": "2024-09-30T08:08:08Z",
      "closed_at": "2024-09-30T08:08:08Z",
      "labels": [
        "bug",
        "Distributed"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1220/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1220",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1220",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:28.974313",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Looks like something got jumbled around with the recent pin bumps and refactor. Thanks for flagging we'll take a look \r\n\r\ncc @kwen2501 ",
          "created_at": "2024-09-27T18:20:38Z"
        },
        {
          "author": "kwen2501",
          "body": "I can repro the same error.",
          "created_at": "2024-09-27T23:30:44Z"
        },
        {
          "author": "kwen2501",
          "body": "It seems to be related to this TODO:\r\n\r\nhttps://github.com/pytorch/torchchat/blob/1980a69d6ded01cd0fa2df770b80bc34b09ade5a/torchchat/model.py#L33-L35\r\n\r\n@larryliu0820 Do you mind taking a look? \r\n\r\nI also notice that `lm_eval` is not used in model.py.",
          "created_at": "2024-09-27T23:36:11Z"
        },
        {
          "author": "kwen2501",
          "body": "Also @lessw2020, wondering why only Distributed got impacted. \r\nDoes it have to do with us using HF's Tokenizer?",
          "created_at": "2024-09-27T23:50:37Z"
        },
        {
          "author": "lessw2020",
          "body": "@kwen2501  - we don't use HF Tokenizer anymore.  I switched us to TorchChat's tokenizer about a month ago.  Our only HF reliance is on SafeTensor weights. ",
          "created_at": "2024-09-27T23:54:20Z"
        }
      ]
    },
    {
      "issue_number": 1223,
      "title": "CLI chat mode doesn't work on 11b model",
      "body": "### 🐛 Describe the bug\n\nchat mode on cli as well as on browser does not work on 11b model\n\n### Versions\n\nna",
      "state": "open",
      "author": "Gasoonjia",
      "author_type": "User",
      "created_at": "2024-09-27T23:37:46Z",
      "updated_at": "2024-09-28T17:15:43Z",
      "closed_at": null,
      "labels": [
        "Known Gaps",
        "Llama 3.2- Multimodal"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1223/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1223",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1223",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:29.238467",
      "comments": []
    },
    {
      "issue_number": 1167,
      "title": "Issue running on iOS",
      "body": "### 🐛 Describe the bug\n\nI followed all the instructions in the repo and got to the point of launching the Xcode project, when I hit the \"Play\" button, I run into build failed issues. I tried to remove the folders as pointed to by repo, but it still gave me the same duplicate symbols error. The detailed error is pasted below.\r\n\r\n```\r\nLd /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/LLaMA.app/LLaMA.debug.dylib normal (in target 'LLaMA' from project 'LLaMA')\r\n    cd /Users/raghukiran/Research/Chat/torchchat/et-build/src/executorch/examples/demo-apps/apple_ios/LLaMA\r\n    /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang -Xlinker -reproducible -target arm64-apple-ios17.0-simulator -dynamiclib -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator18.0.sdk -O0 -L/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/EagerLinkingTBDs/Debug-iphonesimulator -L/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/EagerLinkingTBDs/Debug-iphonesimulator -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -F/Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator -filelist /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/LLaMA.build/Debug-iphonesimulator/LLaMA.build/Objects-normal/arm64/LLaMA.LinkFileList -install_name @rpath/LLaMA.debug.dylib -Xlinker -rpath -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/PackageFrameworks -Xlinker -rpath -Xlinker @executable_path/Frameworks -Xlinker -rpath -Xlinker @loader_path/Frameworks -dead_strip -Xlinker -object_path_lto -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/LLaMA.build/Debug-iphonesimulator/LLaMA.build/Objects-normal/arm64/LLaMA_lto.o -Xlinker -export_dynamic -Xlinker -no_deduplicate -Xlinker -objc_abi_version -Xlinker 2 -fobjc-link-runtime -L/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/swift/iphonesimulator -L/usr/lib/swift -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/LLaMA.build/Debug-iphonesimulator/LLaMA.build/Objects-normal/arm64/LLaMA.swiftmodule -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -Wl,-no_warn_duplicate_libraries -force_load /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a -force_load /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a -force_load /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_quantized-simulator-debug.a -force_load /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libbackend_xnnpack-simulator-debug.a -force_load /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libbackend_coreml-simulator-debug.a -force_load /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libbackend_mps-simulator-debug.a -framework Accelerate -framework CoreML -lsqlite3 -framework Accelerate -framework CoreML -lsqlite3 -framework Metal -framework MetalPerformanceShaders -framework MetalPerformanceShadersGraph -framework Metal -framework MetalPerformanceShaders -framework MetalPerformanceShadersGraph -Xlinker -alias -Xlinker _main -Xlinker ___debug_main_executable_dylib_entry_point -lbackend_mps-simulator-debug -lkernels_quantized-simulator-release -lbackend_mps-simulator-release -lkernels_portable-simulator-release -lkernels_custom-simulator-release -lbackend_xnnpack-simulator-release -lbackend_xnnpack-simulator-debug -lbackend_coreml-simulator-debug -framework LLaMARunner -lkernels_optimized-simulator-debug -lkernels_portable-simulator-debug -lkernels_optimized-simulator-release -lbackend_coreml-simulator-release -lkernels_quantized-simulator-debug -lkernels_custom-simulator-debug -Xlinker -no_adhoc_codesign -Xlinker -dependency_info -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/LLaMA.build/Debug-iphonesimulator/LLaMA.build/Objects-normal/arm64/LLaMA_dependency_info.dat -o /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/LLaMA.app/LLaMA.debug.dylib -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/backend_mps_debug_dependencies.build/Objects-normal/arm64/backend_mps_debug_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_quantized_dependencies.build/Objects-normal/arm64/kernels_quantized_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/backend_mps_dependencies.build/Objects-normal/arm64/backend_mps_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_portable_dependencies.build/Objects-normal/arm64/kernels_portable_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_custom_dependencies.build/Objects-normal/arm64/kernels_custom_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/backend_xnnpack_dependencies.build/Objects-normal/arm64/backend_xnnpack_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/backend_xnnpack_debug_dependencies.build/Objects-normal/arm64/backend_xnnpack_debug_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/backend_coreml_debug_dependencies.build/Objects-normal/arm64/backend_coreml_debug_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_optimized_debug_dependencies.build/Objects-normal/arm64/kernels_optimized_debug_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_portable_debug_dependencies.build/Objects-normal/arm64/kernels_portable_debug_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_optimized_dependencies.build/Objects-normal/arm64/kernels_optimized_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/backend_coreml_dependencies.build/Objects-normal/arm64/backend_coreml_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_quantized_debug_dependencies.build/Objects-normal/arm64/kernels_quantized_debug_dependencies.swiftmodule -Xlinker -add_ast_path -Xlinker /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Intermediates.noindex/executorch.build/Debug-iphonesimulator/kernels_custom_debug_dependencies.build/Objects-normal/arm64/kernels_custom_debug_dependencies.swiftmodule\r\n\r\nduplicate symbol 'torch::executor::check_dim_in_dim_list(unsigned long, unsigned long, executorch::runtime::ArrayRef<long long> const&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_amin_amax_args(torch::executor::Tensor const&, executorch::runtime::ArrayRef<long long>, bool, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::get_init_index(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>> const&, unsigned long)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::resize_reduction_out(torch::executor::Tensor const&, torch::executor::optional<long long> const&, bool, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::get_out_numel(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>> const&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_mean_dim_args(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>>, bool, torch::executor::optional<torch::executor::ScalarType>, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::compute_reduced_out_size(torch::executor::Tensor const&, torch::executor::optional<long long> const&, bool, int*)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_reduction_args(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>> const&, bool, torch::executor::optional<torch::executor::ScalarType>, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::get_reduced_dim_product(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>> const&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::compute_reduced_out_size(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>> const&, bool, int*)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_prod_out_args(torch::executor::Tensor const&, torch::executor::optional<torch::executor::ScalarType>, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_argmin_argmax_args(torch::executor::Tensor const&, torch::executor::optional<long long>, bool, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_min_max_args(torch::executor::Tensor const&, long long, bool, torch::executor::Tensor&, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::get_out_numel(torch::executor::Tensor const&, torch::executor::optional<long long> const&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::get_reduced_dim_product(torch::executor::Tensor const&, torch::executor::optional<long long> const&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_reduction_args_single_dim(torch::executor::Tensor const&, torch::executor::optional<long long>, bool, torch::executor::optional<torch::executor::ScalarType>, torch::executor::Tensor&, bool)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::check_dim_list_is_valid(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>> const&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::resize_reduction_out(torch::executor::Tensor const&, torch::executor::optional<executorch::runtime::ArrayRef<long long>> const&, bool, torch::executor::Tensor&)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nduplicate symbol 'torch::executor::get_init_index(torch::executor::Tensor const&, torch::executor::optional<long long> const&, unsigned long)' in:\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_custom-simulator-debug.a[7](reduce_util.o)\r\n    /Users/raghukiran/Library/Developer/Xcode/DerivedData/LLaMA-gqvkgaeksxvvfcbtyazuovwtyoxn/Build/Products/Debug-iphonesimulator/libkernels_portable-simulator-debug.a[167](reduce_util.o)\r\nld: 19 duplicate symbols\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\n\n### Versions\n\nENVIRONMENT\r\n============\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240814\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.5 (arm64)\r\nGCC version: Could not collect\r\nClang version: 16.0.0 (clang-1600.0.26.3)\r\nCMake version: version 3.30.3\r\nLibc version: N/A\r\n\r\nPython version: 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 12:57:28) [Clang 14.0.6 ] (64-bit runtime)\r\nPython platform: macOS-14.5-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M3 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+9129892\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240814\r\n[pip3] torchao==0.5.0\r\n[pip3] torchtune==0.3.0.dev20240916+cpu\r\n[pip3] torchvision==0.20.0.dev20240814\r\n[conda] No relevant packages\r\n```",
      "state": "open",
      "author": "raghukiran1224",
      "author_type": "User",
      "created_at": "2024-09-20T17:25:43Z",
      "updated_at": "2024-09-20T18:59:13Z",
      "closed_at": null,
      "labels": [
        "bug",
        "Mobile - iOS"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1167/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1167",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1167",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:30.978623",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "@shoumikhin Does this look familiar to you?",
          "created_at": "2024-09-20T18:59:13Z"
        }
      ]
    },
    {
      "issue_number": 782,
      "title": "[UX] We are too quiet about errors - in particular missing HF authentication...",
      "body": "\r\nThe command `python3 torchchat.py where llama3` fails quietly presumably because I might not have the HF Token configured. \r\nI assumed the code was broken, though because I got a backtrace of the program and a message that some stuff could not be found.\r\n\r\nRunning with stories15M convinced me (after looking and source and not finding anything disagreeable....) that this must be a problem with downloading llama3, but nobody told me that as a user.\r\n\r\nAnd yes, I'm a typical user here... I sorta followed instructions (well, I logged in a while ago, should be enough?) and now I'm submitted a bug.  --- Point being this might attract a lot of \"(potential) operator error\" type messages that become error reports we have to triage and in parallel unsatisfied users, because they have convinced themselves it's torchchat that's broken, not that they've performed an imperfect jobs\r\n\r\n```\r\n(py311) mikekg@mikekg-mbp torchchat % python3 torchchat.py where llama3\r\n\r\nDownloading meta-llama/Meta-Llama-3-8B-Instruct from HuggingFace...\r\nConverting meta-llama/Meta-Llama-3-8B-Instruct to torchchat format...\r\nknown configs: ['13B', '70B', 'CodeLlama-7b-Python-hf', '34B', 'stories42M', '30B', 'stories110M', '7B', 'stories15M', 'Mistral-7B', 'Meta-Llama-3-8B']\r\nModel config {'block_size': 2048, 'vocab_size': 128256, 'n_layers': 32, 'n_heads': 32, 'dim': 4096, 'hidden_dim': 14336, 'n_local_heads': 8, 'head_dim': 128, 'rope_base': 500000.0, 'norm_eps': 1e-05, 'multiple_of': 1024, 'ffn_dim_multiplier': 1.3, 'use_tiktoken': True, 'max_seq_length': 8192}\r\nTraceback (most recent call last):\r\n  File \"/Users/mikekg/ci/torchchat/torchchat.py\", line 169, in <module>\r\n    check_args(args, \"where\")\r\n  File \"/Users/mikekg/ci/torchchat/cli.py\", line 39, in check_args\r\n    download_and_convert(args.model, args.model_directory, args.hf_token)\r\n  File \"/Users/mikekg/ci/torchchat/download.py\", line 97, in download_and_convert\r\n    _download_hf_snapshot(model_config, temp_dir, hf_token)\r\n  File \"/Users/mikekg/ci/torchchat/download.py\", line 61, in _download_hf_snapshot\r\n    convert_hf_checkpoint(\r\n  File \"/Users/mikekg/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/mikekg/ci/torchchat/build/convert_hf_checkpoint.py\", line 60, in convert_hf_checkpoint\r\n    raise RuntimeError(\r\nRuntimeError: Could not find /Users/mikekg/.torchchat/model-cache/downloads/meta-llama/Meta-Llama-3-8B-Instruct/pytorch_model.bin.index.json or /Users/mikekg/.torchchat/model-cache/downloads/meta-llama/Meta-Llama-3-8B-Instruct/original/consolidated.00.pth plus /Users/mikekg/.torchchat/model-cache/downloads/meta-llama/Meta-Llama-3-8B-Instruct/original/tokenizer.model\r\n(py311) mikekg@mikekg-mbp torchchat % python3 torchchat.py where stories15M\r\n\r\n/Users/mikekg/.torchchat/model-cache/stories15M\r\n(py311) mikekg@mikekg-mbp torchchat % \r\n```\r\n",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-05-13T16:39:41Z",
      "updated_at": "2024-09-19T16:35:03Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/782/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/782",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/782",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:31.226824",
      "comments": [
        {
          "author": "Roy6250",
          "body": "Hi @Jack-Khuu can I work on this issue?",
          "created_at": "2024-08-02T13:02:31Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Sorry I missed this @Roy6250, if you're still interested please do!!",
          "created_at": "2024-09-19T08:32:04Z"
        },
        {
          "author": "Roy6250",
          "body": "Sure, , would like to this issue. Thanks @Jack-Khuu ",
          "created_at": "2024-09-19T16:35:02Z"
        }
      ]
    },
    {
      "issue_number": 1117,
      "title": "install_et and build_native et clone and build executorch twice.",
      "body": "### 🐛 Describe the bug\r\n\r\nIn Set Up Executorch https://github.com/pytorch/torchchat#set-up-executorch, torchchat builds executorch:\r\n```\r\nexport TORCHCHAT_ROOT=${PWD}\r\n./torchchat/utils/scripts/install_et.sh\r\n[...]\r\n  Building wheel for executorch (pyproject.toml) ... done\r\n  Created wheel for executorch: filename=executorch-0.4.0a0+9129892-cp311-cp311-linux_aarch64.whl size=6566185 sha256=4bd75c14b16c0f415f27a8b3a6a884c18fee9f1da75ef85e30494126667a08ed\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-euj2dih9/wheels/9b/5a/9d/3c6c45b6398d70f3f494ffe58e7b288e3f04db656b40f7a4ac\r\nSuccessfully built executorch\r\nInstalling collected packages: sortedcontainers, flatbuffers, ruamel.yaml.clib, pluggy, parameterized, iniconfig, hypothesis, expecttest, execnet, ruamel.yaml, pytest, pytest-xdist, executorch\r\n  changing mode of /home/sunshine/torchchat/.venv/bin/hypothesis to 775\r\n  changing mode of /home/sunshine/torchchat/.venv/bin/py.test to 775\r\n  changing mode of /home/sunshine/torchchat/.venv/bin/pytest to 775\r\n  changing mode of /home/sunshine/torchchat/.venv/bin/flatc to 775\r\nSuccessfully installed execnet-2.1.1 executorch-0.4.0a0+9129892 expecttest-0.2.1 flatbuffers-20181003210633 hypothesis-6.112.0 iniconfig-2.0.0 parameterized-0.9.0 pluggy-1.5.0 pytest-8.3.2 pytest-xdist-3.6.1 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 sortedcontainers-2.4.0\r\n+ pip3 list\r\nPackage                   Version\r\n------------------------- ------------------\r\nabsl-py                   2.1.0\r\naccelerate                0.34.2\r\naiohappyeyeballs          2.4.0\r\naiohttp                   3.10.5\r\naiosignal                 1.3.1\r\naltair                    5.4.1\r\nannotated-types           0.7.0\r\nanyio                     4.4.0\r\nattrs                     24.2.0\r\nblinker                   1.8.2\r\nblobfile                  3.0.0\r\ncachetools                5.5.0\r\ncertifi                   2024.8.30\r\nchardet                   5.2.0\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncmake                     3.30.3\r\ncolorama                  0.4.6\r\nDataProperty              1.0.1\r\ndatasets                  2.21.0\r\ndill                      0.3.8\r\ndistro                    1.9.0\r\nevaluate                  0.4.2\r\nexecnet                   2.1.1\r\nexecutorch                0.4.0a0+9129892\r\nexpecttest                0.2.1\r\nfilelock                  3.16.0\r\nFlask                     3.0.3\r\nflatbuffers               20181003210633\r\nfrozenlist                1.4.1\r\nfsspec                    2024.6.1\r\ngguf                      0.10.0\r\ngitdb                     4.0.11\r\nGitPython                 3.1.43\r\nh11                       0.14.0\r\nhttpcore                  1.0.5\r\nhttpx                     0.27.2\r\nhuggingface_hub           0.24.6\r\nhypothesis                6.112.0\r\nidna                      3.8\r\niniconfig                 2.0.0\r\nitsdangerous              2.2.0\r\nJinja2                    3.1.4\r\njiter                     0.5.0\r\njoblib                    1.4.2\r\njsonlines                 4.0.0\r\njsonschema                4.23.0\r\njsonschema-specifications 2023.12.1\r\nlm_eval                   0.4.2\r\nlxml                      5.3.0\r\nmarkdown-it-py            3.0.0\r\nMarkupSafe                2.1.5\r\nmbstrdecoder              1.1.3\r\nmdurl                     0.1.2\r\nmore-itertools            10.5.0\r\nmpmath                    1.3.0\r\nmultidict                 6.0.5\r\nmultiprocess              0.70.16\r\nnarwhals                  1.6.3\r\nnetworkx                  3.3\r\nninja                     1.11.1.1\r\nnltk                      3.9.1\r\nnumexpr                   2.10.1\r\nnumpy                     1.26.4\r\nopenai                    1.44.0\r\npackaging                 24.1\r\npandas                    2.2.2\r\nparameterized             0.9.0\r\npathvalidate              3.2.1\r\npeft                      0.12.0\r\npillow                    10.4.0\r\npip                       24.1.1\r\npluggy                    1.5.0\r\nportalocker               2.10.1\r\nprotobuf                  5.28.0\r\npsutil                    6.0.0\r\npyarrow                   17.0.0\r\npybind11                  2.13.5\r\npycryptodomex             3.20.0\r\npydantic                  2.9.0\r\npydantic_core             2.23.2\r\npydeck                    0.9.1\r\nPygments                  2.18.0\r\npytablewriter             1.2.0\r\npytest                    8.3.2\r\npytest-xdist              3.6.1\r\npython-dateutil           2.9.0.post0\r\npytz                      2024.1\r\nPyYAML                    6.0.2\r\nreferencing               0.35.1\r\nregex                     2024.7.24\r\nrequests                  2.32.3\r\nrich                      13.8.0\r\nrouge-score               0.1.2\r\nrpds-py                   0.20.0\r\nruamel.yaml               0.18.6\r\nruamel.yaml.clib          0.2.8\r\nsacrebleu                 2.4.3\r\nsafetensors               0.4.5\r\nscikit-learn              1.5.1\r\nscipy                     1.14.1\r\nsentencepiece             0.2.0\r\nsetuptools                70.3.0\r\nsix                       1.16.0\r\nsmmap                     5.0.1\r\nsnakeviz                  2.2.0\r\nsniffio                   1.3.1\r\nsortedcontainers          2.4.0\r\nsqlitedict                2.1.0\r\nstreamlit                 1.38.0\r\nsympy                     1.13.1\r\ntabledata                 1.3.3\r\ntabulate                  0.9.0\r\ntcolorpy                  0.1.6\r\ntenacity                  8.5.0\r\nthreadpoolctl             3.5.0\r\ntiktoken                  0.7.0\r\ntimm                      1.0.7\r\ntokenizers                0.19.1\r\ntoml                      0.10.2\r\ntomli                     2.0.1\r\ntorch                     2.5.0.dev20240716\r\ntorchao                   0.4.0+gite11201a\r\ntorchaudio                2.4.0.dev20240716\r\ntorchsr                   1.0.4\r\ntorchvision               0.20.0.dev20240716\r\ntornado                   6.4.1\r\ntqdm                      4.66.5\r\ntqdm-multiprocess         0.0.11\r\ntransformers              4.42.4\r\ntypepy                    1.3.2\r\ntyping_extensions         4.12.2\r\ntzdata                    2024.1\r\nurllib3                   2.2.2\r\nwatchdog                  4.0.2\r\nWerkzeug                  3.0.4\r\nwheel                     0.44.0\r\nword2number               1.1\r\nxxhash                    3.5.0\r\nyarl                      1.10.0\r\nzstandard                 0.23.0\r\nzstd                      1.5.5.1\r\n+ popd\r\n~/torchchat ~/torchchat\r\n+ popd\r\n~/torchchat\r\n```\r\n\r\nThen, when I build to deploy and run as per https://github.com/pytorch/torchchat?tab=readme-ov-file#deploy-and-run-on-desktop, the build_native script erases executorch, clones it again, and builds it again!\r\n```\r\n(.venv) sunshine@raspberrypi:~/torchchat $ torchchat/utils/scripts/build_native.sh et\r\n+ '[' 1 -eq 0 ']'\r\n+ ((  1  ))\r\n+ case \"$1\" in\r\n+ echo 'Building et native runner...'\r\nBuilding et native runner...\r\n+ TARGET=et\r\n+ shift\r\n+ ((  0  ))\r\n+ '[' -z /home/sunshine/torchchat ']'\r\n+ source /home/sunshine/torchchat/torchchat/utils/scripts/install_utils.sh\r\n++ set -ex pipefail\r\n++ COMMON_CMAKE_ARGS='    -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON'\r\n+ '[' -z '' ']'\r\n+ ET_BUILD_DIR=et-build\r\n+ pushd /home/sunshine/torchchat\r\n~/torchchat ~/torchchat\r\n+ git submodule update --init\r\nSubmodule 'tokenizer/third-party/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'tokenizer/third-party/abseil-cpp'\r\nSubmodule 'tokenizer/third-party/re2' (https://github.com/google/re2.git) registered for path 'tokenizer/third-party/re2'\r\nSubmodule 'tokenizer/third-party/sentencepiece' (https://github.com/google/sentencepiece.git) registered for path 'tokenizer/third-party/sentencepiece'\r\nCloning into '/home/sunshine/torchchat/tokenizer/third-party/abseil-cpp'...\r\nCloning into '/home/sunshine/torchchat/tokenizer/third-party/re2'...\r\nCloning into '/home/sunshine/torchchat/tokenizer/third-party/sentencepiece'...\r\n[...]\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.0)\r\n+ popd\r\n~/torchchat ~/torchchat\r\n+ clone_executorch\r\n+ echo 'Cloning executorch to /home/sunshine/torchchat/et-build/src'\r\nCloning executorch to /home/sunshine/torchchat/et-build/src\r\n+ rm -rf /home/sunshine/torchchat/et-build\r\n[45351:45469:0908/135200.319023:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\n+ mkdir -p /home/sunshine/torchchat/et-build/src\r\n+ pushd /home/sunshine/torchchat/et-build/src\r\n~/torchchat/et-build/src ~/torchchat ~/torchchat\r\n+ git clone https://github.com/pytorch/executorch.git\r\nCloning into 'executorch'...\r\nremote: Enumerating objects: 117962, done.\r\nremote: Counting objects: 100% (5346/5346), done.\r\nremote: Compressing objects: 100% (2123/2123), done.\r\n[45351:45469:0908/135214.042078:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\nremote: Total 117962 (delta 3641), reused 4531 (delta 3072), pack-reused 112616 (from 1)\r\nReceiving objects: 100% (117962/117962), 86.71 MiB | 1.54 MiB/s, done.\r\nResolving deltas: 100% (91085/91085), done.\r\n+ cd executorch\r\n++ cat /home/sunshine/torchchat/install/.pins/et-pin.txt\r\n+ git checkout 91298923a0076c1b41059efb6dad2876426e4b03\r\nNote: switching to '91298923a0076c1b41059efb6dad2876426e4b03'.\r\n[...]\r\n```\r\n\r\n\r\n### Versions\r\n\r\n(.venv) sunshine@raspberrypi:~/torchchat $ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240716\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux trixie/sid (aarch64)\r\nGCC version: (Debian 13.3.0-3) 13.3.0\r\nClang version: 16.0.6 (27+b1)\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.11.2 (main, May  2 2024, 11:59:08) [GCC 12.2.0] (64-bit runtime)\r\nPython platform: Linux-6.6.31+rpt-rpi-v8-aarch64-with-glibc2.39\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         aarch64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nCPU(s):                               4\r\nOn-line CPU(s) list:                  0-3\r\nVendor ID:                            ARM\r\nModel name:                           Cortex-A76\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per cluster:                  4\r\nSocket(s):                            -\r\nCluster(s):                           1\r\nStepping:                             r4p1\r\nCPU(s) scaling MHz:                   100%\r\nCPU max MHz:                          2400.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             108.00\r\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nL1d cache:                            256 KiB (4 instances)\r\nL1i cache:                            256 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             2 MiB (1 instance)\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; CSV2, BHB\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+9129892\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240716\r\n[pip3] torchao==0.4.0+gite11201a\r\n[pip3] torchaudio==2.4.0.dev20240716\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchvision==0.20.0.dev20240716\r\n[conda] Could not collect\r\n(.venv) sunshine@raspberrypi:~/torchchat $ \r\n",
      "state": "closed",
      "author": "sunshinesfbay",
      "author_type": "User",
      "created_at": "2024-09-08T21:01:14Z",
      "updated_at": "2024-09-19T08:14:32Z",
      "closed_at": "2024-09-19T08:14:32Z",
      "labels": [
        "bug",
        "actionable",
        "ExecuTorch"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1117/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1117",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1117",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:31.516256",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Good catch, should be an easy fix and win (if downloaded don't download)",
          "created_at": "2024-09-09T22:30:48Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "This should be resolved as of https://github.com/pytorch/torchchat/commit/46beb08c5dc2831878c86dd1da73774c4a5bed91",
          "created_at": "2024-09-19T08:14:32Z"
        }
      ]
    },
    {
      "issue_number": 1118,
      "title": "build_native et fails",
      "body": "### 🐛 Describe the bug\n\nFollowing the instructions from README.md to build ET executor https://github.com/pytorch/torchchat?tab=readme-ov-file#deploy-and-run-on-desktop, build fails:\r\n\r\n```\r\n(.venv) sunshine@raspberrypi:~/torchchat $ torchchat/utils/scripts/build_native.sh et\r\n+ '[' 1 -eq 0 ']'\r\n+ ((  1  ))\r\n+ case \"$1\" in\r\n+ echo 'Building et native runner...'\r\nBuilding et native runner...\r\n+ TARGET=et\r\n+ shift\r\n+ ((  0  ))\r\n+ '[' -z /home/sunshine/torchchat ']'\r\n+ source /home/sunshine/torchchat/torchchat/utils/scripts/install_utils.sh\r\n++ set -ex pipefail\r\n++ COMMON_CMAKE_ARGS='    -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON'\r\n+ '[' -z '' ']'\r\n+ ET_BUILD_DIR=et-build\r\n+ pushd /home/sunshine/torchchat\r\n~/torchchat ~/torchchat\r\n+ git submodule update --init\r\nCloning into '/home/sunshine/torchchat/tokenizer/third-party/sentencepiece'...\r\n[45351:45469:0908/135145.358867:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\n[45351:45469:0908/135145.827117:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\n[45351:45469:0908/135147.603127:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\nSubmodule path 'tokenizer/third-party/abseil-cpp': checked out '854193071498f330b71083d7e06a7cd18e02a4cc'\r\nSubmodule path 'tokenizer/third-party/re2': checked out 'ac82d4f628a2045d89964ae11c48403d3b091af1'\r\nSubmodule path 'tokenizer/third-party/sentencepiece': checked out '7dcb541451b1862d73f473b3804ccf8f2a9e10f6'\r\n+ git submodule sync\r\nSynchronizing submodule url for 'tokenizer/third-party/abseil-cpp'\r\nSynchronizing submodule url for 'tokenizer/third-party/re2'\r\nSynchronizing submodule url for 'tokenizer/third-party/sentencepiece'\r\n+ [[ et == \\e\\t ]]\r\n+ find_cmake_prefix_path\r\n++ python3 -c 'from distutils.sysconfig import get_python_lib;print(get_python_lib())'\r\n+ path=/home/sunshine/torchchat/.venv/lib/python3.11/site-packages\r\n+ MY_CMAKE_PREFIX_PATH=/home/sunshine/torchchat/.venv/lib/python3.11/site-packages\r\n+ install_pip_dependencies\r\n+ echo 'Intalling common pip packages'\r\nIntalling common pip packages\r\n+ pip3 install wheel 'cmake>=3.19' ninja zstd\r\n[45351:45469:0908/135155.427334:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (0.44.0)\r\nRequirement already satisfied: cmake>=3.19 in ./.venv/lib/python3.11/site-packages (3.30.3)\r\nRequirement already satisfied: ninja in ./.venv/lib/python3.11/site-packages (1.11.1.1)\r\nRequirement already satisfied: zstd in ./.venv/lib/python3.11/site-packages (1.5.5.1)\r\n+ pushd /home/sunshine/torchchat\r\n~/torchchat ~/torchchat ~/torchchat\r\n+ pip3 install -r install/requirements.txt\r\n[45351:45469:0908/135157.305874:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nIgnoring tomli: markers 'python_version < \"3.11\"' don't match your environment\r\nRequirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 4)) (0.24.6)\r\nRequirement already satisfied: gguf in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 7)) (0.10.0)\r\nRequirement already satisfied: tiktoken in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 10)) (0.7.0)\r\nRequirement already satisfied: snakeviz in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 13)) (2.2.0)\r\nRequirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 14)) (0.2.0)\r\nRequirement already satisfied: numpy<2.0 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 15)) (1.26.4)\r\nRequirement already satisfied: lm-eval==0.4.2 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: blobfile in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 18)) (3.0.0)\r\nRequirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 20)) (1.44.0)\r\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 23)) (0.44.0)\r\nRequirement already satisfied: cmake>=3.24 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 24)) (3.30.3)\r\nRequirement already satisfied: ninja in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 25)) (1.11.1.1)\r\nRequirement already satisfied: zstd in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 26)) (1.5.5.1)\r\nRequirement already satisfied: streamlit in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 29)) (1.38.0)\r\nRequirement already satisfied: flask in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 32)) (3.0.3)\r\nRequirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.34.2)\r\nRequirement already satisfied: evaluate in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: datasets>=2.16.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.21.0)\r\nRequirement already satisfied: jsonlines in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.0.0)\r\nRequirement already satisfied: numexpr in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: peft>=0.2.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.12.0)\r\nRequirement already satisfied: pybind11>=2.6.2 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.13.5)\r\nRequirement already satisfied: pytablewriter in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.2.0)\r\nRequirement already satisfied: rouge-score>=0.0.4 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.2)\r\nRequirement already satisfied: sacrebleu>=1.5.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.3)\r\nRequirement already satisfied: scikit-learn>=0.24.1 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.5.1)\r\nRequirement already satisfied: sqlitedict in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: torch>=1.8 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.5.0.dev20240716)\r\nRequirement already satisfied: tqdm-multiprocess in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.0.11)\r\nRequirement already satisfied: transformers>=4.1 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.42.4)\r\nRequirement already satisfied: zstandard in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.23.0)\r\nRequirement already satisfied: dill in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.3.8)\r\nRequirement already satisfied: word2number in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1)\r\nRequirement already satisfied: more-itertools in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (10.5.0)\r\nRequirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (3.16.0)\r\nRequirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2024.6.1)\r\nRequirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (6.0.2)\r\nRequirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2.32.3)\r\nRequirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.66.5)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.12.2)\r\nRequirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken->-r install/requirements.txt (line 10)) (2024.7.24)\r\nRequirement already satisfied: tornado>=2.0 in ./.venv/lib/python3.11/site-packages (from snakeviz->-r install/requirements.txt (line 13)) (6.4.1)\r\nRequirement already satisfied: pycryptodomex>=3.8 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (3.20.0)\r\nRequirement already satisfied: urllib3<3,>=1.25.3 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (2.2.2)\r\nRequirement already satisfied: lxml>=4.9 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (5.3.0)\r\nRequirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (4.4.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.9.0)\r\nRequirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.27.2)\r\nRequirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.5.0)\r\nRequirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (2.9.0)\r\nRequirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.3.1)\r\nRequirement already satisfied: altair<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.4.1)\r\nRequirement already satisfied: blinker<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (1.8.2)\r\nRequirement already satisfied: cachetools<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.5.0)\r\nRequirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.1.7)\r\nRequirement already satisfied: pandas<3,>=1.3.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (2.2.2)\r\nRequirement already satisfied: pillow<11,>=7.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (10.4.0)\r\nRequirement already satisfied: protobuf<6,>=3.20 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.28.0)\r\nRequirement already satisfied: pyarrow>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (17.0.0)\r\nRequirement already satisfied: rich<14,>=10.14.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (13.8.0)\r\nRequirement already satisfied: tenacity<9,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.5.0)\r\nRequirement already satisfied: toml<2,>=0.10.1 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.10.2)\r\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (3.1.43)\r\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.9.1)\r\nRequirement already satisfied: watchdog<5,>=2.1.5 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (4.0.2)\r\nRequirement already satisfied: Werkzeug>=3.0.0 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.0.4)\r\nRequirement already satisfied: Jinja2>=3.1.2 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.1.4)\r\nRequirement already satisfied: itsdangerous>=2.1.2 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (2.2.0)\r\nRequirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.0)\r\nRequirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.5)\r\nRequirement already satisfied: jsonschema>=3.0 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (4.23.0)\r\nRequirement already satisfied: narwhals>=1.5.2 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (1.6.3)\r\nRequirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r install/requirements.txt (line 20)) (3.8)\r\nRequirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.70.16)\r\nRequirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.10.5)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (4.0.11)\r\nRequirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (2024.8.30)\r\nRequirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (1.0.5)\r\nRequirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (0.14.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from Jinja2>=3.1.2->flask->-r install/requirements.txt (line 32)) (2.1.5)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (0.7.0)\r\nRequirement already satisfied: pydantic-core==2.23.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (2.23.2)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub->-r install/requirements.txt (line 4)) (3.3.2)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (2.18.0)\r\nRequirement already satisfied: absl-py in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.9.1)\r\nRequirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.16.0)\r\nRequirement already satisfied: portalocker in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.9.0)\r\nRequirement already satisfied: colorama in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.6)\r\nRequirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.14.1)\r\nRequirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.13.1)\r\nRequirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.3)\r\nRequirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers>=4.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.19.1)\r\nRequirement already satisfied: attrs>=19.2.0 in ./.venv/lib/python3.11/site-packages (from jsonlines->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (24.2.0)\r\nRequirement already satisfied: setuptools>=38.3.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (70.3.0)\r\nRequirement already satisfied: DataProperty<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.0.1)\r\nRequirement already satisfied: mbstrdecoder<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1.3)\r\nRequirement already satisfied: pathvalidate<4,>=2.3.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.2.1)\r\nRequirement already satisfied: tabledata<2,>=1.3.1 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.3)\r\nRequirement already satisfied: tcolorpy<1,>=0.0.5 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.6)\r\nRequirement already satisfied: typepy<2,>=1.3.2 in ./.venv/lib/python3.11/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.2)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.0)\r\nRequirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.1)\r\nRequirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.5)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.10.0)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (5.0.1)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (2023.12.1)\r\nRequirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.35.1)\r\nRequirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.20.0)\r\nRequirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (0.1.2)\r\nRequirement already satisfied: chardet<6,>=3.0.4 in ./.venv/lib/python3.11/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (5.2.0)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.0)\r\n+ popd\r\n~/torchchat ~/torchchat\r\n+ clone_executorch\r\n+ echo 'Cloning executorch to /home/sunshine/torchchat/et-build/src'\r\nCloning executorch to /home/sunshine/torchchat/et-build/src\r\n+ rm -rf /home/sunshine/torchchat/et-build\r\n[45351:45469:0908/135200.319023:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\n+ mkdir -p /home/sunshine/torchchat/et-build/src\r\n+ pushd /home/sunshine/torchchat/et-build/src\r\n~/torchchat/et-build/src ~/torchchat ~/torchchat\r\n+ git clone https://github.com/pytorch/executorch.git\r\nCloning into 'executorch'...\r\nremote: Enumerating objects: 117962, done.\r\nremote: Counting objects: 100% (5346/5346), done.\r\nremote: Compressing objects: 100% (2123/2123), done.\r\n[45351:45469:0908/135214.042078:ERROR:v4l2_utils.cc(513)] Could not open /dev/video10: No such file or directory (2)\r\nremote: Total 117962 (delta 3641), reused 4531 (delta 3072), pack-reused 112616 (from 1)\r\nReceiving objects: 100% (117962/117962), 86.71 MiB | 1.54 MiB/s, done.\r\nResolving deltas: 100% (91085/91085), done.\r\n+ cd executorch\r\n++ cat /home/sunshine/torchchat/install/.pins/et-pin.txt\r\n+ git checkout 91298923a0076c1b41059efb6dad2876426e4b03\r\nNote: switching to '91298923a0076c1b41059efb6dad2876426e4b03'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by switching back to a branch.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -c with the switch command. Example:\r\n\r\n  git switch -c <new-branch-name>\r\n\r\nOr undo this operation with:\r\n\r\n  git switch -\r\n\r\nTurn off this advice by setting config variable advice.detachedHead to false\r\n\r\nHEAD is now at 91298923a immutable accessors in graph signature (#4433)\r\n+ echo 'Install executorch: submodule update'\r\nInstall executorch: submodule update\r\n+ git submodule sync\r\n+ git submodule update --init\r\nSubmodule 'backends/arm/third-party/ethos-u-core-driver' (https://review.mlplatform.org/ml/ethos-u/ethos-u-core-driver) registered for path 'backends/arm/third-party/ethos-u-core-driver'\r\nSubmodule 'backends/arm/third-party/serialization_lib' (https://review.mlplatform.org/tosa/serialization_lib) registered for path 'backends/arm/third-party/serialization_lib'\r\nSubmodule 'backends/vulkan/third-party/Vulkan-Headers' (https://github.com/KhronosGroup/Vulkan-Headers) registered for path 'backends/vulkan/third-party/Vulkan-Headers'\r\nSubmodule 'backends/vulkan/third-party/VulkanMemoryAllocator' (https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.git) registered for path 'backends/vulkan/third-party/VulkanMemoryAllocator'\r\nSubmodule 'backends/vulkan/third-party/volk' (https://github.com/zeux/volk) registered for path 'backends/vulkan/third-party/volk'\r\nSubmodule 'backends/xnnpack/third-party/FP16' (https://github.com/Maratyszcza/FP16.git) registered for path 'backends/xnnpack/third-party/FP16'\r\nSubmodule 'backends/xnnpack/third-party/FXdiv' (https://github.com/Maratyszcza/FXdiv.git) registered for path 'backends/xnnpack/third-party/FXdiv'\r\nSubmodule 'backends/xnnpack/third-party/XNNPACK' (https://github.com/digantdesai/XNNPACK.git) registered for path 'backends/xnnpack/third-party/XNNPACK'\r\nSubmodule 'backends/xnnpack/third-party/cpuinfo' (https://github.com/pytorch/cpuinfo.git) registered for path 'backends/xnnpack/third-party/cpuinfo'\r\nSubmodule 'backends/xnnpack/third-party/pthreadpool' (https://github.com/Maratyszcza/pthreadpool.git) registered for path 'backends/xnnpack/third-party/pthreadpool'\r\nSubmodule 'examples/third-party/LLaVA' (https://github.com/haotian-liu/LLaVA.git) registered for path 'examples/third-party/LLaVA'\r\nSubmodule 'examples/third-party/fbjni' (https://github.com/facebookincubator/fbjni.git) registered for path 'examples/third-party/fbjni'\r\nSubmodule 'extension/llm/third-party/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'extension/llm/third-party/abseil-cpp'\r\nSubmodule 'extension/llm/third-party/re2' (https://github.com/google/re2.git) registered for path 'extension/llm/third-party/re2'\r\nSubmodule 'extension/llm/third-party/sentencepiece' (https://github.com/google/sentencepiece.git) registered for path 'extension/llm/third-party/sentencepiece'\r\nSubmodule 'kernels/optimized/third-party/eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'kernels/optimized/third-party/eigen'\r\nSubmodule 'third-party/flatbuffers' (https://github.com/google/flatbuffers.git) registered for path 'third-party/flatbuffers'\r\nSubmodule 'third-party/flatcc' (https://github.com/dvidelabs/flatcc.git) registered for path 'third-party/flatcc'\r\nSubmodule 'third-party/gflags' (https://github.com/gflags/gflags.git) registered for path 'third-party/gflags'\r\nSubmodule 'third-party/googletest' (https://github.com/google/googletest.git) registered for path 'third-party/googletest'\r\nSubmodule 'third-party/ios-cmake' (https://github.com/leetal/ios-cmake) registered for path 'third-party/ios-cmake'\r\nSubmodule 'third-party/prelude' (https://github.com/facebook/buck2-prelude.git) registered for path 'third-party/prelude'\r\nSubmodule 'third-party/pybind11' (https://github.com/pybind/pybind11.git) registered for path 'third-party/pybind11'\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/arm/third-party/ethos-u-core-driver'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/arm/third-party/serialization_lib'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/vulkan/third-party/Vulkan-Headers'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/vulkan/third-party/VulkanMemoryAllocator'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/vulkan/third-party/volk'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/xnnpack/third-party/FP16'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/xnnpack/third-party/FXdiv'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/xnnpack/third-party/XNNPACK'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/xnnpack/third-party/cpuinfo'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/backends/xnnpack/third-party/pthreadpool'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/examples/third-party/LLaVA'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/examples/third-party/fbjni'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/extension/llm/third-party/abseil-cpp'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/extension/llm/third-party/re2'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/extension/llm/third-party/sentencepiece'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/kernels/optimized/third-party/eigen'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/third-party/flatbuffers'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/third-party/flatcc'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/third-party/gflags'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/third-party/googletest'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/third-party/ios-cmake'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/third-party/prelude'...\r\nCloning into '/home/sunshine/torchchat/et-build/src/executorch/third-party/pybind11'...\r\nSubmodule path 'backends/arm/third-party/ethos-u-core-driver': checked out '90f9df900acdc0718ecd2dfdc53780664758dec5'\r\nSubmodule path 'backends/arm/third-party/serialization_lib': checked out '187af0d41fe75d08d2a7ec84c1b4d24b9b641ed2'\r\nSubmodule path 'backends/vulkan/third-party/Vulkan-Headers': checked out '0c5928795a66e93f65e5e68a36d8daa79a209dc2'\r\nSubmodule path 'backends/vulkan/third-party/VulkanMemoryAllocator': checked out 'a6bfc237255a6bac1513f7c1ebde6d8aed6b5191'\r\nSubmodule path 'backends/vulkan/third-party/volk': checked out 'b3bc21e584f97400b6884cb2a541a56c6a5ddba3'\r\nSubmodule path 'backends/xnnpack/third-party/FP16': checked out '4dfe081cf6bcd15db339cf2680b9281b8451eeb3'\r\nSubmodule path 'backends/xnnpack/third-party/FXdiv': checked out 'b408327ac2a15ec3e43352421954f5b1967701d1'\r\nSubmodule path 'backends/xnnpack/third-party/XNNPACK': checked out '1d139a3b4b7155889c88c31f370a82c48e7ca89c'\r\nSubmodule path 'backends/xnnpack/third-party/cpuinfo': checked out 'd6860c477c99f1fce9e28eb206891af3c0e1a1d7'\r\nSubmodule path 'backends/xnnpack/third-party/pthreadpool': checked out '4fe0e1e183925bf8cfa6aae24237e724a96479b8'\r\nSubmodule path 'examples/third-party/LLaVA': checked out '7440ec9ee37b0374c6b5548818e89878e38f3353'\r\nSubmodule path 'examples/third-party/fbjni': checked out '52a14f0daa889a20d8984798b8d96eb03cebd334'\r\nSubmodule path 'extension/llm/third-party/abseil-cpp': checked out 'eb852207758a773965301d0ae717e4235fc5301a'\r\nSubmodule path 'extension/llm/third-party/re2': checked out '6dcd83d60f7944926bfd308cc13979fc53dd69ca'\r\nSubmodule path 'extension/llm/third-party/sentencepiece': checked out '6225e08edb2577757163b3f5dbba4c0b670ef445'\r\nSubmodule path 'kernels/optimized/third-party/eigen': checked out 'a39ade4ccf99df845ec85c580fbbb324f71952fa'\r\nSubmodule path 'third-party/flatbuffers': checked out '595bf0007ab1929570c7671f091313c8fc20644e'\r\nSubmodule path 'third-party/flatcc': checked out '896db54787e8b730a6be482c69324751f3f5f117'\r\nSubmodule path 'third-party/gflags': checked out 'a738fdf9338412f83ab3f26f31ac11ed3f3ec4bd'\r\nSubmodule path 'third-party/googletest': checked out 'e2239ee6043f73722e7aa812a459f54a28552929'\r\nSubmodule path 'third-party/ios-cmake': checked out '06465b27698424cf4a04a5ca4904d50a3c966c45'\r\nSubmodule path 'third-party/prelude': checked out '4e9e6d50b8b461564a7e351ff60b87fe59d7e53b'\r\nSubmodule path 'third-party/pybind11': checked out '8c7b8dd0ae74b36b7d42f77b0dd4096ebb7f4ab1'\r\n+ popd\r\n~/torchchat ~/torchchat\r\n+ install_executorch_libs false\r\n+ export 'CMAKE_ARGS=        -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON     -DCMAKE_PREFIX_PATH=/home/sunshine/torchchat/.venv/lib/python3.11/site-packages     -DCMAKE_INSTALL_PREFIX=/home/sunshine/torchchat/et-build/install'\r\n+ CMAKE_ARGS='        -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON     -DCMAKE_PREFIX_PATH=/home/sunshine/torchchat/.venv/lib/python3.11/site-packages     -DCMAKE_INSTALL_PREFIX=/home/sunshine/torchchat/et-build/install'\r\n+ export 'CMAKE_BUILD_ARGS=--target install'\r\n+ CMAKE_BUILD_ARGS='--target install'\r\n+ install_executorch_python_libs false\r\n+ '[' '!' -d /home/sunshine/torchchat/et-build ']'\r\n+ pushd /home/sunshine/torchchat/et-build/src\r\n~/torchchat/et-build/src ~/torchchat ~/torchchat\r\n+ cd executorch\r\n+ echo 'Building and installing python libraries'\r\nBuilding and installing python libraries\r\n+ '[' '' = false ']'\r\n+ echo 'Installing pybind'\r\nInstalling pybind\r\n+ bash ./install_requirements.sh --pybind xnnpack\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple, https://download.pytorch.org/whl/nightly/cpu\r\nRequirement already satisfied: torch==2.5.0.dev20240716 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (2.5.0.dev20240716)\r\nRequirement already satisfied: torchvision==0.20.0.dev20240716 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (0.20.0.dev20240716)\r\nRequirement already satisfied: cmake in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (3.30.3)\r\nRequirement already satisfied: pip>=23 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (24.1.1)\r\nRequirement already satisfied: pyyaml in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (6.0.2)\r\nRequirement already satisfied: setuptools>=63 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (70.3.0)\r\nRequirement already satisfied: tomli in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (2.0.1)\r\nRequirement already satisfied: wheel in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (0.44.0)\r\nRequirement already satisfied: zstd in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (1.5.5.1)\r\nRequirement already satisfied: timm==1.0.7 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (1.0.7)\r\nRequirement already satisfied: torchaudio==2.4.0.dev20240716 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (2.4.0.dev20240716)\r\nRequirement already satisfied: torchsr==1.0.4 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (1.0.4)\r\nRequirement already satisfied: transformers==4.42.4 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (4.42.4)\r\nRequirement already satisfied: filelock in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.16.0)\r\nRequirement already satisfied: typing-extensions>=4.8.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (4.12.2)\r\nRequirement already satisfied: sympy in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (1.13.1)\r\nRequirement already satisfied: networkx in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.3)\r\nRequirement already satisfied: jinja2 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.1.4)\r\nRequirement already satisfied: fsspec in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (2024.6.1)\r\nRequirement already satisfied: numpy in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torchvision==0.20.0.dev20240716) (1.26.4)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from torchvision==0.20.0.dev20240716) (10.4.0)\r\nRequirement already satisfied: huggingface_hub in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from timm==1.0.7) (0.24.6)\r\nRequirement already satisfied: safetensors in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from timm==1.0.7) (0.4.5)\r\nRequirement already satisfied: packaging>=20.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (24.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (2024.7.24)\r\nRequirement already satisfied: requests in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (2.32.3)\r\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (0.19.1)\r\nRequirement already satisfied: tqdm>=4.27 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (4.66.5)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from jinja2->torch==2.5.0.dev20240716) (2.1.5)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (3.8)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (2.2.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (2024.8.30)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from sympy->torch==2.5.0.dev20240716) (1.3.0)\r\nUsing pip 24.1.1 from /home/sunshine/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\nWARNING: The index url \"\" seems invalid, please provide a scheme.\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple, \r\nProcessing /home/sunshine/torchchat/et-build/src/executorch\r\n  Running command Preparing metadata (pyproject.toml)\r\n  running dist_info\r\n  creating /tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info\r\n  writing /tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/PKG-INFO\r\n  writing dependency_links to /tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/dependency_links.txt\r\n  writing entry points to /tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/entry_points.txt\r\n  writing requirements to /tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/requires.txt\r\n  writing top-level names to /tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/top_level.txt\r\n  writing manifest file '/tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/SOURCES.txt'\r\n  reading manifest file '/tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/SOURCES.txt'\r\n  adding license file 'LICENSE'\r\n  writing manifest file '/tmp/pip-modern-metadata-kkcl2rnz/executorch.egg-info/SOURCES.txt'\r\n  creating '/tmp/pip-modern-metadata-kkcl2rnz/executorch-0.4.0a0+9129892.dist-info'\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: expecttest in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.2.1)\r\nRequirement already satisfied: flatbuffers in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (20181003210633)\r\nRequirement already satisfied: hypothesis in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (6.112.0)\r\nRequirement already satisfied: mpmath==1.3.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.3.0)\r\nRequirement already satisfied: numpy>=1.25.2 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.26.4)\r\nRequirement already satisfied: packaging in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (24.1)\r\nRequirement already satisfied: pandas in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (2.2.2)\r\nRequirement already satisfied: parameterized in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.9.0)\r\nRequirement already satisfied: pytest in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (8.3.2)\r\nRequirement already satisfied: pytest-xdist in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (3.6.1)\r\nRequirement already satisfied: pyyaml in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (6.0.2)\r\nRequirement already satisfied: ruamel.yaml in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.18.6)\r\nRequirement already satisfied: sympy in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.13.1)\r\nRequirement already satisfied: tabulate in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.9.0)\r\nRequirement already satisfied: attrs>=22.2.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from hypothesis->executorch==0.4.0a0+9129892) (24.2.0)\r\nRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from hypothesis->executorch==0.4.0a0+9129892) (2.4.0)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.7 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2024.1)\r\nRequirement already satisfied: iniconfig in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from pytest->executorch==0.4.0a0+9129892) (2.0.0)\r\nRequirement already satisfied: pluggy<2,>=1.5 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from pytest->executorch==0.4.0a0+9129892) (1.5.0)\r\nRequirement already satisfied: execnet>=2.1 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from pytest-xdist->executorch==0.4.0a0+9129892) (2.1.1)\r\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from ruamel.yaml->executorch==0.4.0a0+9129892) (0.2.8)\r\nRequirement already satisfied: six>=1.5 in /home/sunshine/torchchat/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->executorch==0.4.0a0+9129892) (1.16.0)\r\nBuilding wheels for collected packages: executorch\r\n  Running command Building wheel for executorch (pyproject.toml)\r\n  running bdist_wheel\r\n  running build\r\n  command options for 'CustomBuild':\r\n    build_base = pip-out\r\n    build_purelib = pip-out/lib\r\n    build_platlib = pip-out/lib.linux-aarch64-cpython-311\r\n    build_lib = pip-out/lib.linux-aarch64-cpython-311\r\n    build_scripts = pip-out/scripts-3.11\r\n    build_temp = pip-out/temp.linux-aarch64-cpython-311\r\n    plat_name = linux-aarch64\r\n    compiler = None\r\n    parallel = 3\r\n    debug = None\r\n    force = None\r\n    executable = /home/sunshine/torchchat/.venv/bin/python3\r\n  creating /home/sunshine/torchchat/et-build/src/executorch/pip-out\r\n  creating /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311\r\n  creating /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out\r\n  deleting /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/CMakeCache.txt\r\n  cmake -S /home/sunshine/torchchat/et-build/src/executorch -B /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out -DBUCK2= -DPYTHON_EXECUTABLE=/home/sunshine/torchchat/.venv/bin/python3 -DCMAKE_PREFIX_PATH=/home/sunshine/torchchat/.venv/lib/python3.11/site-packages -DCMAKE_BUILD_TYPE=Release -DEXECUTORCH_ENABLE_LOGGING=ON -DEXECUTORCH_LOG_LEVEL=Info -DCMAKE_OSX_DEPLOYMENT_TARGET=10.15 -DEXECUTORCH_SEPARATE_FLATCC_HOST_PROJECT=OFF -DEXECUTORCH_BUILD_PYBIND=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED_AOT=ON -DEXECUTORCH_BUILD_KERNELS_CUSTOM=ON -DEXECUTORCH_BUILD_KERNELS_CUSTOM_AOT=ON -DCMAKE_BUILD_TYPE=Release -DEXECUTORCH_ENABLE_LOGGING=ON -DEXECUTORCH_LOG_LEVEL=Info -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON -DEXECUTORCH_BUILD_XNNPACK=ON -DCMAKE_PREFIX_PATH=/home/sunshine/torchchat/.venv/lib/python3.11/site-packages -DCMAKE_INSTALL_PREFIX=/home/sunshine/torchchat/et-build/install -DEXECUTORCH_BUILD_XNNPACK=ON\r\n  -- The C compiler identification is GNU 13.3.0\r\n  -- The CXX compiler identification is GNU 13.3.0\r\n  -- Detecting C compiler ABI info\r\n  -- Detecting C compiler ABI info - done\r\n  -- Check for working C compiler: /usr/bin/cc - skipped\r\n  -- Detecting C compile features\r\n  -- Detecting C compile features - done\r\n  -- Detecting CXX compiler ABI info\r\n  -- Detecting CXX compiler ABI info - done\r\n  -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n  -- Detecting CXX compile features\r\n  -- Detecting CXX compile features - done\r\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n  -- Found Threads: TRUE\r\n  -- Downloading FXdiv to /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/FXdiv-source (define FXDIV_SOURCE_DIR to avoid it)\r\n  -- Configuring done (0.0s)\r\n  -- Generating done (0.0s)\r\n  -- Build files have been written to: /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/FXdiv-download\r\n  [ 11%] Creating directories for 'fxdiv'\r\n  [ 22%] Performing download step (git clone) for 'fxdiv'\r\n  Cloning into 'FXdiv-source'...\r\n  Already on 'master'\r\n  Your branch is up to date with 'origin/master'.\r\n  [ 33%] Performing update step for 'fxdiv'\r\n  -- Fetching latest from the remote origin\r\n  [ 44%] No patch step for 'fxdiv'\r\n  [ 55%] No configure step for 'fxdiv'\r\n  [ 66%] No build step for 'fxdiv'\r\n  [ 77%] No install step for 'fxdiv'\r\n  [ 88%] No test step for 'fxdiv'\r\n  [100%] Completed 'fxdiv'\r\n  [100%] Built target fxdiv\r\n  -- Using python executable '/home/sunshine/torchchat/.venv/bin/python3'\r\n  -- Resolved buck2 as /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/buck2-bin/buck2-49670bee56a7d8a7696409ca6fbf7551d2469787.\r\n  -- Killing buck2 daemon\r\n  -- executorch: Generating source lists\r\n  -- executorch: Generating source file list /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/executorch_srcs.cmake\r\n  Error while generating /home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/executorch_srcs.cmake. Exit code: 1\r\n  Output:\r\n\r\n  Error:\r\n  Traceback (most recent call last):\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/buck_util.py\", line 26, in run\r\n      cp: subprocess.CompletedProcess = subprocess.run(\r\n                                        ^^^^^^^^^^^^^^^\r\n    File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\r\n      raise CalledProcessError(retcode, process.args,\r\n  subprocess.CalledProcessError: Command '['/home/sunshine/torchchat/et-build/src/executorch/pip-out/temp.linux-aarch64-cpython-311/cmake-out/buck2-bin/buck2-49670bee56a7d8a7696409ca6fbf7551d2469787', 'cquery', \"inputs(deps('//runtime/executor:program'))\"]' returned non-zero exit status 2.\r\n\r\n  The above exception was the direct cause of the following exception:\r\n\r\n  Traceback (most recent call last):\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/extract_sources.py\", line 218, in <module>\r\n      main()\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/extract_sources.py\", line 203, in main\r\n      target_to_srcs[name] = sorted(target.get_sources(graph, runner))\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/extract_sources.py\", line 116, in get_sources\r\n      sources: set[str] = set(runner.run([\"cquery\", query]))\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/home/sunshine/torchchat/et-build/src/executorch/build/buck_util.py\", line 31, in run\r\n      raise RuntimeError(ex.stderr.decode(\"utf-8\")) from ex\r\n  RuntimeError: Command failed:\r\n  Error validating working directory\r\n\r\n  Caused by:\r\n      0: Failed to stat `/home/sunshine/torchchat/et-build/src/executorch/buck-out/v2`\r\n      1: ENOENT: No such file or directory\r\n\r\n\r\n  CMake Error at build/Utils.cmake:191 (message):\r\n    executorch: source list generation failed\r\n  Call Stack (most recent call first):\r\n    CMakeLists.txt:327 (extract_sources)\r\n\r\n\r\n  -- Configuring incomplete, errors occurred!\r\n  error: command '/home/sunshine/torchchat/.venv/bin/cmake' failed with exit code 1\r\n  error: subprocess-exited-with-error\r\n  \r\n  Building wheel for executorch (pyproject.toml) did not run successfully.\r\n  exit code: 1\r\n  \r\n  See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /home/sunshine/torchchat/.venv/bin/python3 /home/sunshine/torchchat/.venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmp4jii_az6\r\n  cwd: /home/sunshine/torchchat/et-build/src/executorch\r\n  Building wheel for executorch (pyproject.toml) ... error\r\n  ERROR: Failed building wheel for executorch\r\nFailed to build executorch\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (executorch)\r\n\r\n```\n\n### Versions\n\n(.venv) sunshine@raspberrypi:~/torchchat $ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240716\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux trixie/sid (aarch64)\r\nGCC version: (Debian 13.3.0-3) 13.3.0\r\nClang version: 16.0.6 (27+b1)\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.11.2 (main, May  2 2024, 11:59:08) [GCC 12.2.0] (64-bit runtime)\r\nPython platform: Linux-6.6.31+rpt-rpi-v8-aarch64-with-glibc2.39\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         aarch64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nCPU(s):                               4\r\nOn-line CPU(s) list:                  0-3\r\nVendor ID:                            ARM\r\nModel name:                           Cortex-A76\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per cluster:                  4\r\nSocket(s):                            -\r\nCluster(s):                           1\r\nStepping:                             r4p1\r\nCPU(s) scaling MHz:                   100%\r\nCPU max MHz:                          2400.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             108.00\r\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nL1d cache:                            256 KiB (4 instances)\r\nL1i cache:                            256 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             2 MiB (1 instance)\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; CSV2, BHB\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+9129892\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240716\r\n[pip3] torchao==0.4.0+gite11201a\r\n[pip3] torchaudio==2.4.0.dev20240716\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchvision==0.20.0.dev20240716\r\n[conda] Could not collect\r\n(.venv) sunshine@raspberrypi:~/torchchat $ \r\n",
      "state": "closed",
      "author": "sunshinesfbay",
      "author_type": "User",
      "created_at": "2024-09-08T21:04:58Z",
      "updated_at": "2024-09-17T22:57:49Z",
      "closed_at": "2024-09-17T22:57:48Z",
      "labels": [
        "bug",
        "ExecuTorch"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1118/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1118",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1118",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:31.762761",
      "comments": [
        {
          "author": "sunshinesfbay",
          "body": "It's noteworthy that ET can be built, because install_et succeeds, while it fails for build_native.  \r\n\r\nIn Set Up Executorch https://github.com/pytorch/torchchat#set-up-executorch, torchchat builds executorch successfully as follows:\r\n```\r\nexport TORCHCHAT_ROOT=${PWD}\r\n./torchchat/utils/scripts/ins",
          "created_at": "2024-09-08T21:13:43Z"
        },
        {
          "author": "byjlw",
          "body": "This is a known issue in buck. To fix it please locate the buck executable and run the kill command. \r\nIt'll be within your torchchat source directory. My kill command looks like this. \r\n\r\n`/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpyth",
          "created_at": "2024-09-09T15:47:05Z"
        },
        {
          "author": "byjlw",
          "body": "@sunshinesfbay did this solve your problem? If i don't hear back i'll close the issue.",
          "created_at": "2024-09-13T03:19:16Z"
        }
      ]
    },
    {
      "issue_number": 1146,
      "title": "[Distributed] Did not find tokenizer at {tokenizer_path}",
      "body": "### 🐛 Describe the bug\n\n```\r\ntorchrun --nproc-per-node 8 dist_run.py\r\n```\r\n\r\n```\r\nknown configs: ['13B', '30B', '34B', '70B', '7B', 'CodeLlama-7b-Python-hf', 'Mistral-7B', 'stories110M', 'stories15M', 'stories42M', 'Meta-Llama-3-70B', 'Meta-Llama-3-8B', 'Meta-Llama-3.1-70B-Tune', 'Meta-Llama-3.1-70B', 'Meta-Llama-3.1-8B-Tune', 'Meta-Llama-3.1-8B']\r\n09-14 15:41:32.092 - dist_run:137 - Chat Model Config: TransformerArgs(block_size=2048, vocab_size=32000, n_layers=32, n_heads=32, dim=4096, hidden_dim=11008, n_local_heads=32, head_dim=128, rope_base=10000, norm_eps=1e-05, multiple_of=256, ffn_dim_multiplier=None, use_tiktoken=False, max_seq_length=8192, rope_scaling=None, n_stages=1, stage_idx=0)\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/kw2501/local/torchchat/dist_run.py\", line 277, in <module>\r\n[rank0]:     main()\r\n[rank0]:   File \"/home/kw2501/local/torchchat/dist_run.py\", line 139, in main\r\n[rank0]:     tokenizer = _build_chat_tokenizer()\r\n[rank0]:   File \"/home/kw2501/local/torchchat/dist_run.py\", line 94, in _build_chat_tokenizer\r\n[rank0]:     tokenizer_args = TokenizerArgs.from_args(args)\r\n[rank0]:   File \"/home/kw2501/local/torchchat/torchchat/cli/builder.py\", line 269, in from_args\r\n[rank0]:     raise RuntimeError(f\"did not find tokenizer at {tokenizer_path}\")\r\n[rank0]: RuntimeError: did not find tokenizer at /home/kw2501/.torchchat/model-cache/meta-llama/Meta-Llama-3-8B-Instruct/tokenizer.model\r\n```\r\n\r\nCc: @lessw2020 \n\n### Versions\n\nmain branch",
      "state": "open",
      "author": "kwen2501",
      "author_type": "User",
      "created_at": "2024-09-14T22:47:09Z",
      "updated_at": "2024-09-17T22:57:26Z",
      "closed_at": null,
      "labels": [
        "Distributed"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1146/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1146",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1146",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:31.983972",
      "comments": [
        {
          "author": "lessw2020",
          "body": "this highlights a current weakness - atm we are using the tokenizer downloaded by chat, but the safetensor weights by HF.\r\nThus, to get setup for a model we have to run:\r\n1 - `python3 torchchat.py download llama3 ` to get the tokenizer \r\n2 - `model = AutoModelForCausalLM.from_pretrained(\"meta-llama/",
          "created_at": "2024-09-14T23:11:57Z"
        },
        {
          "author": "lessw2020",
          "body": "we can hook into the torchchat download.py to download the tokenizer, and then pull the safetensor weights via an HF call as well. ",
          "created_at": "2024-09-14T23:19:23Z"
        }
      ]
    },
    {
      "issue_number": 1147,
      "title": "[distributed][perf] ensure that all decoding ops are happening on gpu with no cpu sync",
      "body": "### 🐛 Describe the bug\n\nper @kwen2501  - when we are doing decoding step:\r\n~~~\r\nnext_token = torch.tensor([decode_results[0][0]], device=device)\r\n~~~\r\n\"nit: I am not sure if the use of torch.tensor here would cause a sync from GPU to CPU (to get the scalar) then move to the GPU again (to create the tensor).\r\nIf there is no use of next_token in CPU domain, better to just use index op here.\r\n\r\nOr, is decode_results already on CPU? Hmm, then we'd need to think about how to arrange these CPU ops and GPU ops. Ideally, you would like to fire the send right after step().\"\r\n\r\n\n\n### Versions\n\nn/a",
      "state": "open",
      "author": "lessw2020",
      "author_type": "User",
      "created_at": "2024-09-15T00:09:56Z",
      "updated_at": "2024-09-17T22:57:11Z",
      "closed_at": null,
      "labels": [
        "performance",
        "Distributed"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1147/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1147",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1147",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:32.244851",
      "comments": []
    },
    {
      "issue_number": 1134,
      "title": "Failures when using PyTorch local build vs. binaries",
      "body": "### 🐛 Describe the bug\n\nI ran into an issue with loading the tokenizer, which was root caused to me using my local PyTorch build.\r\n\r\nAfter building the aoti runner, I ran the following command: `cmake-out/aoti_run exportedModels/stories15M.so -z /home/angelayi/.torchchat/model-cache/stories15M/tokenizer.model -i \"Once upon a time”`\r\n\r\nWith my local build, the above command ran into the error: `couldn't load /home/angelayi/.torchchat/model-cache/stories15M/tokenizer.model` which is from the sentencepiece tokenizer. Specifying `-l 2` doesn't change anything as this is the default setting. \r\n\r\nChanging to `-l 3` results in the following error:\r\n```\r\nterminate called after throwing an instance of 'std::invalid_argument'\r\n  what():  invalid encoder line: \r\nzsh: IOT instruction (core dumped)  cmake-out/aoti_run ../lucy_stories15M.so -z ../tokenizer.model -l 3 -i \r\n```\r\n\r\nAfter re-running `./install/install_requirements.sh`, this installs PyTorch version at 08142024, and runs successfully.\r\nSo I tried today's nightly (09112024) using `pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121`, and this also runs successfully.\r\nGoing back to my local PyTorch build, I checked out the commit `26e5572` which corresponds to the cutoff of today's nightly, and built PyTorch locally. This runs into the initial error with the tokenizers. \r\n\r\nI still didn't figure out how to run with my local PyTorch build, but quoting Nikita, this is motivation to create a docker/venv story :P \r\n\r\ncc @malfet @Jack-Khuu \n\n### Versions\n\nmain",
      "state": "open",
      "author": "angelayi",
      "author_type": "User",
      "created_at": "2024-09-11T23:57:18Z",
      "updated_at": "2024-09-12T01:01:24Z",
      "closed_at": null,
      "labels": [
        "bug",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1134/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1134",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1134",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:32.244875",
      "comments": []
    },
    {
      "issue_number": 1107,
      "title": "ET Runner build fails with buck2 \"Failed to stat `executorch/buck-out/v2`\" error",
      "body": "### 🐛 Describe the bug\n\nThe following command errors out\r\n`torchchat/utils/scripts/build_native.sh et`\r\n\r\n```\r\n+ '[' 1 -eq 0 ']'\r\n+ ((  1  ))\r\n+ case \"$1\" in\r\n+ echo 'Building et native runner...'\r\nBuilding et native runner...\r\n+ TARGET=et\r\n+ shift\r\n+ ((  0  ))\r\n+ '[' -z /Users/jessewhite/Documents/source/torchchat ']'\r\n+ source /Users/jessewhite/Documents/source/torchchat/torchchat/utils/scripts/install_utils.sh\r\n++ set -ex pipefail\r\n++ COMMON_CMAKE_ARGS='    -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON'\r\n+ '[' -z '' ']'\r\n+ ET_BUILD_DIR=et-build\r\n+ pushd /Users/jessewhite/Documents/source/torchchat\r\n~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ git submodule update --init\r\n+ git submodule sync\r\nSynchronizing submodule url for 'tokenizer/third-party/abseil-cpp'\r\nSynchronizing submodule url for 'tokenizer/third-party/re2'\r\nSynchronizing submodule url for 'tokenizer/third-party/sentencepiece'\r\n+ [[ et == \\e\\t ]]\r\n+ find_cmake_prefix_path\r\n++ python3 -c 'from distutils.sysconfig import get_python_lib;print(get_python_lib())'\r\n+ path=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages\r\n+ MY_CMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages\r\n+ install_pip_dependencies\r\n+ echo 'Intalling common pip packages'\r\nIntalling common pip packages\r\n+ pip3 install wheel 'cmake>=3.19' ninja zstd\r\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (0.44.0)\r\nRequirement already satisfied: cmake>=3.19 in ./.venv/lib/python3.11/site-packages (3.30.2)\r\nRequirement already satisfied: ninja in ./.venv/lib/python3.11/site-packages (1.11.1.1)\r\nRequirement already satisfied: zstd in ./.venv/lib/python3.11/site-packages (1.5.5.1)\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n+ pushd /Users/jessewhite/Documents/source/torchchat\r\n~/Documents/source/torchchat ~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ pip3 install -r install/requirements.txt\r\nIgnoring tomli: markers 'python_version < \"3.11\"' don't match your environment\r\nRequirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 4)) (0.24.6)\r\nRequirement already satisfied: gguf in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 7)) (0.10.0)\r\nRequirement already satisfied: tiktoken in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 10)) (0.7.0)\r\nRequirement already satisfied: snakeviz in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 13)) (2.2.0)\r\nRequirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 14)) (0.2.0)\r\nRequirement already satisfied: numpy<2.0 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 15)) (1.26.4)\r\nRequirement already satisfied: lm-eval==0.4.2 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: blobfile in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 18)) (3.0.0)\r\nRequirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 20)) (1.43.0)\r\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 23)) (0.44.0)\r\nRequirement already satisfied: cmake>=3.24 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 24)) (3.30.2)\r\nRequirement already satisfied: ninja in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 25)) (1.11.1.1)\r\nRequirement already satisfied: zstd in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 26)) (1.5.5.1)\r\nRequirement already satisfied: streamlit in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 29)) (1.38.0)\r\nRequirement already satisfied: flask in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 32)) (3.0.3)\r\nRequirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.34.0)\r\nRequirement already satisfied: evaluate in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: datasets>=2.16.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.21.0)\r\nRequirement already satisfied: jsonlines in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.0.0)\r\nRequirement already satisfied: numexpr in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: peft>=0.2.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.12.0)\r\nRequirement already satisfied: pybind11>=2.6.2 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.13.5)\r\nRequirement already satisfied: pytablewriter in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.2.0)\r\nRequirement already satisfied: rouge-score>=0.0.4 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.2)\r\nRequirement already satisfied: sacrebleu>=1.5.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.3)\r\nRequirement already satisfied: scikit-learn>=0.24.1 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.5.1)\r\nRequirement already satisfied: sqlitedict in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: torch>=1.8 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.5.0.dev20240716)\r\nRequirement already satisfied: tqdm-multiprocess in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.0.11)\r\nRequirement already satisfied: transformers>=4.1 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.42.4)\r\nRequirement already satisfied: zstandard in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.23.0)\r\nRequirement already satisfied: dill in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.3.8)\r\nRequirement already satisfied: word2number in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1)\r\nRequirement already satisfied: more-itertools in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (10.4.0)\r\nRequirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (3.15.4)\r\nRequirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2024.6.1)\r\nRequirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (6.0.2)\r\nRequirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2.32.3)\r\nRequirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.66.5)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.12.2)\r\nRequirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken->-r install/requirements.txt (line 10)) (2024.7.24)\r\nRequirement already satisfied: tornado>=2.0 in ./.venv/lib/python3.11/site-packages (from snakeviz->-r install/requirements.txt (line 13)) (6.4.1)\r\nRequirement already satisfied: pycryptodomex>=3.8 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (3.20.0)\r\nRequirement already satisfied: urllib3<3,>=1.25.3 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (2.2.2)\r\nRequirement already satisfied: lxml>=4.9 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (5.3.0)\r\nRequirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (4.4.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.9.0)\r\nRequirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.27.2)\r\nRequirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.5.0)\r\nRequirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (2.8.2)\r\nRequirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.3.1)\r\nRequirement already satisfied: altair<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.4.1)\r\nRequirement already satisfied: blinker<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (1.8.2)\r\nRequirement already satisfied: cachetools<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.5.0)\r\nRequirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.1.7)\r\nRequirement already satisfied: pandas<3,>=1.3.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (2.2.2)\r\nRequirement already satisfied: pillow<11,>=7.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (10.4.0)\r\nRequirement already satisfied: protobuf<6,>=3.20 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.28.0)\r\nRequirement already satisfied: pyarrow>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (17.0.0)\r\nRequirement already satisfied: rich<14,>=10.14.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (13.8.0)\r\nRequirement already satisfied: tenacity<9,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.5.0)\r\nRequirement already satisfied: toml<2,>=0.10.1 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.10.2)\r\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (3.1.43)\r\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.9.1)\r\nRequirement already satisfied: Werkzeug>=3.0.0 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.0.4)\r\nRequirement already satisfied: Jinja2>=3.1.2 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.1.4)\r\nRequirement already satisfied: itsdangerous>=2.1.2 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (2.2.0)\r\nRequirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.0)\r\nRequirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.4)\r\nRequirement already satisfied: jsonschema>=3.0 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (4.23.0)\r\nRequirement already satisfied: narwhals>=1.5.2 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (1.6.2)\r\nRequirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r install/requirements.txt (line 20)) (3.8)\r\nRequirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.70.16)\r\nRequirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.10.5)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (4.0.11)\r\nRequirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (2024.8.30)\r\nRequirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (1.0.5)\r\nRequirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (0.14.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from Jinja2>=3.1.2->flask->-r install/requirements.txt (line 32)) (2.1.5)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (0.7.0)\r\nRequirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (2.20.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub->-r install/requirements.txt (line 4)) (3.3.2)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (2.18.0)\r\nRequirement already satisfied: absl-py in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.9.1)\r\nRequirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.16.0)\r\nRequirement already satisfied: portalocker in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.9.0)\r\nRequirement already satisfied: colorama in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.6)\r\nRequirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.14.1)\r\nRequirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.13.1)\r\nRequirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.3)\r\nRequirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers>=4.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.19.1)\r\nRequirement already satisfied: attrs>=19.2.0 in ./.venv/lib/python3.11/site-packages (from jsonlines->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (24.2.0)\r\nRequirement already satisfied: setuptools>=38.3.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (65.5.0)\r\nRequirement already satisfied: DataProperty<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.0.1)\r\nRequirement already satisfied: mbstrdecoder<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1.3)\r\nRequirement already satisfied: pathvalidate<4,>=2.3.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.2.1)\r\nRequirement already satisfied: tabledata<2,>=1.3.1 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.3)\r\nRequirement already satisfied: tcolorpy<1,>=0.0.5 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.6)\r\nRequirement already satisfied: typepy<2,>=1.3.2 in ./.venv/lib/python3.11/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.2)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.0)\r\nRequirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.1)\r\nRequirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.5)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.9.8)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (5.0.1)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (2023.12.1)\r\nRequirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.35.1)\r\nRequirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.20.0)\r\nRequirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (0.1.2)\r\nRequirement already satisfied: chardet<6,>=3.0.4 in ./.venv/lib/python3.11/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (5.2.0)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.0)\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n+ popd\r\n~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ clone_executorch\r\n+ echo 'Cloning executorch to /Users/jessewhite/Documents/source/torchchat/et-build/src'\r\nCloning executorch to /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n+ rm -rf /Users/jessewhite/Documents/source/torchchat/et-build\r\n+ mkdir -p /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n+ pushd /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n~/Documents/source/torchchat/et-build/src ~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ git clone https://github.com/pytorch/executorch.git\r\nCloning into 'executorch'...\r\nremote: Enumerating objects: 115556, done.\r\nremote: Counting objects: 100% (2960/2960), done.\r\nremote: Compressing objects: 100% (1288/1288), done.\r\nremote: Total 115556 (delta 1935), reused 2510 (delta 1603), pack-reused 112596 (from 1)\r\nReceiving objects: 100% (115556/115556), 85.81 MiB | 7.51 MiB/s, done.\r\nResolving deltas: 100% (89385/89385), done.\r\n+ cd executorch\r\n++ cat /Users/jessewhite/Documents/source/torchchat/install/.pins/et-pin.txt\r\n+ git checkout 91298923a0076c1b41059efb6dad2876426e4b03\r\nNote: switching to '91298923a0076c1b41059efb6dad2876426e4b03'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by switching back to a branch.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -c with the switch command. Example:\r\n\r\n  git switch -c <new-branch-name>\r\n\r\nOr undo this operation with:\r\n\r\n  git switch -\r\n\r\nTurn off this advice by setting config variable advice.detachedHead to false\r\n\r\nHEAD is now at 91298923a immutable accessors in graph signature (#4433)\r\n+ echo 'Install executorch: submodule update'\r\nInstall executorch: submodule update\r\n+ git submodule sync\r\n+ git submodule update --init\r\nSubmodule 'backends/arm/third-party/ethos-u-core-driver' (https://review.mlplatform.org/ml/ethos-u/ethos-u-core-driver) registered for path 'backends/arm/third-party/ethos-u-core-driver'\r\nSubmodule 'backends/arm/third-party/serialization_lib' (https://review.mlplatform.org/tosa/serialization_lib) registered for path 'backends/arm/third-party/serialization_lib'\r\nSubmodule 'backends/vulkan/third-party/Vulkan-Headers' (https://github.com/KhronosGroup/Vulkan-Headers) registered for path 'backends/vulkan/third-party/Vulkan-Headers'\r\nSubmodule 'backends/vulkan/third-party/VulkanMemoryAllocator' (https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.git) registered for path 'backends/vulkan/third-party/VulkanMemoryAllocator'\r\nSubmodule 'backends/vulkan/third-party/volk' (https://github.com/zeux/volk) registered for path 'backends/vulkan/third-party/volk'\r\nSubmodule 'backends/xnnpack/third-party/FP16' (https://github.com/Maratyszcza/FP16.git) registered for path 'backends/xnnpack/third-party/FP16'\r\nSubmodule 'backends/xnnpack/third-party/FXdiv' (https://github.com/Maratyszcza/FXdiv.git) registered for path 'backends/xnnpack/third-party/FXdiv'\r\nSubmodule 'backends/xnnpack/third-party/XNNPACK' (https://github.com/digantdesai/XNNPACK.git) registered for path 'backends/xnnpack/third-party/XNNPACK'\r\nSubmodule 'backends/xnnpack/third-party/cpuinfo' (https://github.com/pytorch/cpuinfo.git) registered for path 'backends/xnnpack/third-party/cpuinfo'\r\nSubmodule 'backends/xnnpack/third-party/pthreadpool' (https://github.com/Maratyszcza/pthreadpool.git) registered for path 'backends/xnnpack/third-party/pthreadpool'\r\nSubmodule 'examples/third-party/LLaVA' (https://github.com/haotian-liu/LLaVA.git) registered for path 'examples/third-party/LLaVA'\r\nSubmodule 'examples/third-party/fbjni' (https://github.com/facebookincubator/fbjni.git) registered for path 'examples/third-party/fbjni'\r\nSubmodule 'extension/llm/third-party/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'extension/llm/third-party/abseil-cpp'\r\nSubmodule 'extension/llm/third-party/re2' (https://github.com/google/re2.git) registered for path 'extension/llm/third-party/re2'\r\nSubmodule 'extension/llm/third-party/sentencepiece' (https://github.com/google/sentencepiece.git) registered for path 'extension/llm/third-party/sentencepiece'\r\nSubmodule 'kernels/optimized/third-party/eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'kernels/optimized/third-party/eigen'\r\nSubmodule 'third-party/flatbuffers' (https://github.com/google/flatbuffers.git) registered for path 'third-party/flatbuffers'\r\nSubmodule 'third-party/flatcc' (https://github.com/dvidelabs/flatcc.git) registered for path 'third-party/flatcc'\r\nSubmodule 'third-party/gflags' (https://github.com/gflags/gflags.git) registered for path 'third-party/gflags'\r\nSubmodule 'third-party/googletest' (https://github.com/google/googletest.git) registered for path 'third-party/googletest'\r\nSubmodule 'third-party/ios-cmake' (https://github.com/leetal/ios-cmake) registered for path 'third-party/ios-cmake'\r\nSubmodule 'third-party/prelude' (https://github.com/facebook/buck2-prelude.git) registered for path 'third-party/prelude'\r\nSubmodule 'third-party/pybind11' (https://github.com/pybind/pybind11.git) registered for path 'third-party/pybind11'\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/arm/third-party/ethos-u-core-driver'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/arm/third-party/serialization_lib'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/vulkan/third-party/Vulkan-Headers'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/vulkan/third-party/VulkanMemoryAllocator'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/vulkan/third-party/volk'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/FP16'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/FXdiv'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/XNNPACK'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/cpuinfo'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/pthreadpool'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/examples/third-party/LLaVA'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/examples/third-party/fbjni'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/extension/llm/third-party/abseil-cpp'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/extension/llm/third-party/re2'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/extension/llm/third-party/sentencepiece'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/kernels/optimized/third-party/eigen'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/flatbuffers'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/flatcc'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/gflags'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/googletest'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/ios-cmake'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/prelude'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/pybind11'...\r\nSubmodule path 'backends/arm/third-party/ethos-u-core-driver': checked out '90f9df900acdc0718ecd2dfdc53780664758dec5'\r\nSubmodule path 'backends/arm/third-party/serialization_lib': checked out '187af0d41fe75d08d2a7ec84c1b4d24b9b641ed2'\r\nSubmodule path 'backends/vulkan/third-party/Vulkan-Headers': checked out '0c5928795a66e93f65e5e68a36d8daa79a209dc2'\r\nSubmodule path 'backends/vulkan/third-party/VulkanMemoryAllocator': checked out 'a6bfc237255a6bac1513f7c1ebde6d8aed6b5191'\r\nSubmodule path 'backends/vulkan/third-party/volk': checked out 'b3bc21e584f97400b6884cb2a541a56c6a5ddba3'\r\nSubmodule path 'backends/xnnpack/third-party/FP16': checked out '4dfe081cf6bcd15db339cf2680b9281b8451eeb3'\r\nSubmodule path 'backends/xnnpack/third-party/FXdiv': checked out 'b408327ac2a15ec3e43352421954f5b1967701d1'\r\nSubmodule path 'backends/xnnpack/third-party/XNNPACK': checked out '1d139a3b4b7155889c88c31f370a82c48e7ca89c'\r\nSubmodule path 'backends/xnnpack/third-party/cpuinfo': checked out 'd6860c477c99f1fce9e28eb206891af3c0e1a1d7'\r\nSubmodule path 'backends/xnnpack/third-party/pthreadpool': checked out '4fe0e1e183925bf8cfa6aae24237e724a96479b8'\r\nSubmodule path 'examples/third-party/LLaVA': checked out '7440ec9ee37b0374c6b5548818e89878e38f3353'\r\nSubmodule path 'examples/third-party/fbjni': checked out '52a14f0daa889a20d8984798b8d96eb03cebd334'\r\nSubmodule path 'extension/llm/third-party/abseil-cpp': checked out 'eb852207758a773965301d0ae717e4235fc5301a'\r\nSubmodule path 'extension/llm/third-party/re2': checked out '6dcd83d60f7944926bfd308cc13979fc53dd69ca'\r\nSubmodule path 'extension/llm/third-party/sentencepiece': checked out '6225e08edb2577757163b3f5dbba4c0b670ef445'\r\nSubmodule path 'kernels/optimized/third-party/eigen': checked out 'a39ade4ccf99df845ec85c580fbbb324f71952fa'\r\nSubmodule path 'third-party/flatbuffers': checked out '595bf0007ab1929570c7671f091313c8fc20644e'\r\nSubmodule path 'third-party/flatcc': checked out '896db54787e8b730a6be482c69324751f3f5f117'\r\nSubmodule path 'third-party/gflags': checked out 'a738fdf9338412f83ab3f26f31ac11ed3f3ec4bd'\r\nSubmodule path 'third-party/googletest': checked out 'e2239ee6043f73722e7aa812a459f54a28552929'\r\nSubmodule path 'third-party/ios-cmake': checked out '06465b27698424cf4a04a5ca4904d50a3c966c45'\r\nSubmodule path 'third-party/prelude': checked out '4e9e6d50b8b461564a7e351ff60b87fe59d7e53b'\r\nSubmodule path 'third-party/pybind11': checked out '8c7b8dd0ae74b36b7d42f77b0dd4096ebb7f4ab1'\r\n+ popd\r\n~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ install_executorch_libs false\r\n+ export 'CMAKE_ARGS=        -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON     -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages     -DCMAKE_INSTALL_PREFIX=/Users/jessewhite/Documents/source/torchchat/et-build/install'\r\n+ CMAKE_ARGS='        -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON     -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages     -DCMAKE_INSTALL_PREFIX=/Users/jessewhite/Documents/source/torchchat/et-build/install'\r\n+ export 'CMAKE_BUILD_ARGS=--target install'\r\n+ CMAKE_BUILD_ARGS='--target install'\r\n+ install_executorch_python_libs false\r\n+ '[' '!' -d /Users/jessewhite/Documents/source/torchchat/et-build ']'\r\n+ pushd /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n~/Documents/source/torchchat/et-build/src ~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ cd executorch\r\n+ echo 'Building and installing python libraries'\r\nBuilding and installing python libraries\r\n+ '[' '' = false ']'\r\n+ echo 'Installing pybind'\r\nInstalling pybind\r\n+ bash ./install_requirements.sh --pybind xnnpack\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\r\nRequirement already satisfied: torch==2.5.0.dev20240716 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (2.5.0.dev20240716)\r\nRequirement already satisfied: torchvision==0.20.0.dev20240716 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (0.20.0.dev20240716)\r\nRequirement already satisfied: cmake in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (3.30.2)\r\nRequirement already satisfied: pip>=23 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (24.0)\r\nRequirement already satisfied: pyyaml in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (6.0.2)\r\nRequirement already satisfied: setuptools>=63 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (65.5.0)\r\nRequirement already satisfied: tomli in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (2.0.1)\r\nRequirement already satisfied: wheel in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (0.44.0)\r\nRequirement already satisfied: zstd in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (1.5.5.1)\r\nRequirement already satisfied: timm==1.0.7 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (1.0.7)\r\nRequirement already satisfied: torchaudio==2.4.0.dev20240716 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (2.4.0.dev20240716)\r\nRequirement already satisfied: torchsr==1.0.4 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (1.0.4)\r\nRequirement already satisfied: transformers==4.42.4 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (4.42.4)\r\nRequirement already satisfied: filelock in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.15.4)\r\nRequirement already satisfied: typing-extensions>=4.8.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (4.12.2)\r\nRequirement already satisfied: sympy in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (1.13.1)\r\nRequirement already satisfied: networkx in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.3)\r\nRequirement already satisfied: jinja2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.1.4)\r\nRequirement already satisfied: fsspec in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (2024.6.1)\r\nRequirement already satisfied: numpy in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torchvision==0.20.0.dev20240716) (1.26.4)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torchvision==0.20.0.dev20240716) (10.4.0)\r\nRequirement already satisfied: huggingface_hub in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from timm==1.0.7) (0.24.6)\r\nRequirement already satisfied: safetensors in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from timm==1.0.7) (0.4.4)\r\nRequirement already satisfied: packaging>=20.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (24.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (2024.7.24)\r\nRequirement already satisfied: requests in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (2.32.3)\r\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (0.19.1)\r\nRequirement already satisfied: tqdm>=4.27 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (4.66.5)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from jinja2->torch==2.5.0.dev20240716) (2.1.5)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (3.8)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (2.2.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (2024.8.30)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from sympy->torch==2.5.0.dev20240716) (1.3.0)\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\nUsing pip 24.0 from /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\nWARNING: The index url \"\" seems invalid, please provide a scheme.\r\nLooking in indexes: https://pypi.org/simple, \r\nProcessing /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch\r\n  Running command Preparing metadata (pyproject.toml)\r\n  /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/setuptools/config/pyprojecttoml.py:108: _BetaConfiguration: Support for `[tool.setuptools]` in `pyproject.toml` is still *beta*.\r\n    warnings.warn(msg, _BetaConfiguration)\r\n  running dist_info\r\n  creating /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info\r\n  writing /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/PKG-INFO\r\n  writing dependency_links to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/dependency_links.txt\r\n  writing entry points to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/entry_points.txt\r\n  writing requirements to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/requires.txt\r\n  writing top-level names to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/top_level.txt\r\n  writing manifest file '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/SOURCES.txt'\r\n  reading manifest file '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/SOURCES.txt'\r\n  adding license file 'LICENSE'\r\n  writing manifest file '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch.egg-info/SOURCES.txt'\r\n  creating '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-kos3pdfu/executorch-0.4.0a0+9129892.dist-info'\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: expecttest in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.2.1)\r\nRequirement already satisfied: flatbuffers in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (24.3.25)\r\nRequirement already satisfied: hypothesis in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (6.111.2)\r\nRequirement already satisfied: mpmath==1.3.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.3.0)\r\nRequirement already satisfied: numpy>=1.25.2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.26.4)\r\nRequirement already satisfied: packaging in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (24.1)\r\nRequirement already satisfied: pandas in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (2.2.2)\r\nRequirement already satisfied: parameterized in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.9.0)\r\nRequirement already satisfied: pytest in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (8.3.2)\r\nRequirement already satisfied: pytest-xdist in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (3.6.1)\r\nRequirement already satisfied: pyyaml in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (6.0.2)\r\nRequirement already satisfied: ruamel.yaml in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.18.6)\r\nRequirement already satisfied: sympy in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.13.1)\r\nRequirement already satisfied: tabulate in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.9.0)\r\nRequirement already satisfied: attrs>=22.2.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from hypothesis->executorch==0.4.0a0+9129892) (24.2.0)\r\nRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from hypothesis->executorch==0.4.0a0+9129892) (2.4.0)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.7 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2024.1)\r\nRequirement already satisfied: iniconfig in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pytest->executorch==0.4.0a0+9129892) (2.0.0)\r\nRequirement already satisfied: pluggy<2,>=1.5 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pytest->executorch==0.4.0a0+9129892) (1.5.0)\r\nRequirement already satisfied: execnet>=2.1 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pytest-xdist->executorch==0.4.0a0+9129892) (2.1.1)\r\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from ruamel.yaml->executorch==0.4.0a0+9129892) (0.2.8)\r\nRequirement already satisfied: six>=1.5 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->executorch==0.4.0a0+9129892) (1.16.0)\r\nBuilding wheels for collected packages: executorch\r\n  Running command Building wheel for executorch (pyproject.toml)\r\n  /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/setuptools/config/pyprojecttoml.py:108: _BetaConfiguration: Support for `[tool.setuptools]` in `pyproject.toml` is still *beta*.\r\n    warnings.warn(msg, _BetaConfiguration)\r\n  running bdist_wheel\r\n  running build\r\n  command options for 'CustomBuild':\r\n    build_base = pip-out\r\n    build_purelib = pip-out/lib\r\n    build_platlib = pip-out/lib.macosx-10.9-universal2-cpython-311\r\n    build_lib = pip-out/lib.macosx-10.9-universal2-cpython-311\r\n    build_scripts = pip-out/scripts-3.11\r\n    build_temp = pip-out/temp.macosx-10.9-universal2-cpython-311\r\n    plat_name = macosx-10.9-universal2\r\n    compiler = None\r\n    parallel = 15\r\n    debug = None\r\n    force = None\r\n    executable = /Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11\r\n  creating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out\r\n  creating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311\r\n  creating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out\r\n  deleting /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/CMakeCache.txt\r\n  cmake -S /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch -B /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out -DBUCK2= -DPYTHON_EXECUTABLE=/Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11 -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages -DCMAKE_BUILD_TYPE=Release -DEXECUTORCH_ENABLE_LOGGING=ON -DEXECUTORCH_LOG_LEVEL=Info -DCMAKE_OSX_DEPLOYMENT_TARGET=10.15 -DEXECUTORCH_SEPARATE_FLATCC_HOST_PROJECT=OFF -DEXECUTORCH_BUILD_PYBIND=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED_AOT=ON -DEXECUTORCH_BUILD_KERNELS_CUSTOM=ON -DEXECUTORCH_BUILD_KERNELS_CUSTOM_AOT=ON -DCMAKE_BUILD_TYPE=Release -DEXECUTORCH_ENABLE_LOGGING=ON -DEXECUTORCH_LOG_LEVEL=Info -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON -DEXECUTORCH_BUILD_XNNPACK=ON -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages -DCMAKE_INSTALL_PREFIX=/Users/jessewhite/Documents/source/torchchat/et-build/install -DEXECUTORCH_BUILD_XNNPACK=ON\r\n  -- The C compiler identification is AppleClang 15.0.0.15000100\r\n  -- The CXX compiler identification is AppleClang 15.0.0.15000100\r\n  -- Detecting C compiler ABI info\r\n  -- Detecting C compiler ABI info - done\r\n  -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\r\n  -- Detecting C compile features\r\n  -- Detecting C compile features - done\r\n  -- Detecting CXX compiler ABI info\r\n  -- Detecting CXX compiler ABI info - done\r\n  -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\r\n  -- Detecting CXX compile features\r\n  -- Detecting CXX compile features - done\r\n  -- Downloading FXdiv to /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/FXdiv-source (define FXDIV_SOURCE_DIR to avoid it)\r\n  -- Configuring done (0.0s)\r\n  -- Generating done (0.0s)\r\n  -- Build files have been written to: /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/FXdiv-download\r\n  [ 11%] Creating directories for 'fxdiv'\r\n  [ 22%] Performing download step (git clone) for 'fxdiv'\r\n  Cloning into 'FXdiv-source'...\r\n  Already on 'master'\r\n  Your branch is up to date with 'origin/master'.\r\n  [ 33%] Performing update step for 'fxdiv'\r\n  -- Fetching latest from the remote origin\r\n  [ 44%] No patch step for 'fxdiv'\r\n  [ 55%] No configure step for 'fxdiv'\r\n  [ 66%] No build step for 'fxdiv'\r\n  [ 77%] No install step for 'fxdiv'\r\n  [ 88%] No test step for 'fxdiv'\r\n  [100%] Completed 'fxdiv'\r\n  [100%] Built target fxdiv\r\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n  -- Found Threads: TRUE\r\n  -- Using python executable '/Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11'\r\n  -- Resolved buck2 as /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/buck2-bin/buck2-99773fe6f7963a72ae5f7b737c02836e.\r\n  -- Killing buck2 daemon\r\n  -- executorch: Generating source lists\r\n  -- executorch: Generating source file list /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/executorch_srcs.cmake\r\n  Error while generating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/executorch_srcs.cmake. Exit code: 1\r\n  Output:\r\n\r\n  Error:\r\n  Traceback (most recent call last):\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/buck_util.py\", line 26, in run\r\n      cp: subprocess.CompletedProcess = subprocess.run(\r\n                                        ^^^^^^^^^^^^^^^\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 571, in run\r\n      raise CalledProcessError(retcode, process.args,\r\n  subprocess.CalledProcessError: Command '['/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/buck2-bin/buck2-99773fe6f7963a72ae5f7b737c02836e', 'cquery', \"inputs(deps('//runtime/executor:program'))\"]' returned non-zero exit status 2.\r\n\r\n  The above exception was the direct cause of the following exception:\r\n\r\n  Traceback (most recent call last):\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/extract_sources.py\", line 218, in <module>\r\n      main()\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/extract_sources.py\", line 203, in main\r\n      target_to_srcs[name] = sorted(target.get_sources(graph, runner))\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/extract_sources.py\", line 116, in get_sources\r\n      sources: set[str] = set(runner.run([\"cquery\", query]))\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/buck_util.py\", line 31, in run\r\n      raise RuntimeError(ex.stderr.decode(\"utf-8\")) from ex\r\n  RuntimeError: Command failed:\r\n  Error validating working directory\r\n\r\n  Caused by:\r\n      0: Failed to stat `/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/buck-out/v2`\r\n      1: ENOENT: No such file or directory\r\n\r\n\r\n  CMake Error at build/Utils.cmake:191 (message):\r\n    executorch: source list generation failed\r\n  Call Stack (most recent call first):\r\n    CMakeLists.txt:327 (extract_sources)\r\n\r\n\r\n  -- Configuring incomplete, errors occurred!\r\n  error: command '/Users/jessewhite/Documents/source/torchchat/.venv/bin/cmake' failed with exit code 1\r\n  error: subprocess-exited-with-error\r\n  \r\n  × Building wheel for executorch (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11 /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/tmpx76eqaoy\r\n  cwd: /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch\r\n  Building wheel for executorch (pyproject.toml) ... error\r\n  ERROR: Failed building wheel for executorch\r\nFailed to build executorch\r\nERROR: Could not build wheels for executorch, which is required to install pyproject.toml-based projects\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n```\n\n### Versions\n\nOperating System Information\r\nDarwin MadMax.local 23.1.0 Darwin Kernel Version 23.1.0: Mon Oct 9 21:33:00 PDT 2023; root:xnu-10002.41.9~7/RELEASE_ARM64_T6031 arm64\r\n\r\nPython Version\r\nPython 3.11.8\r\n\r\nPIP Version\r\npip 24.0 from /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n\r\nInstalled Packages\r\nabsl-py==2.1.0\r\naccelerate==0.34.0\r\naiohappyeyeballs==2.4.0\r\naiohttp==3.10.5\r\naiosignal==1.3.1\r\naltair==5.4.1\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nattrs==24.2.0\r\nblinker==1.8.2\r\nblobfile==3.0.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\nchardet==5.2.0\r\ncharset-normalizer==3.3.2\r\nclick==8.1.7\r\ncmake==3.30.2\r\ncolorama==0.4.6\r\nDataProperty==1.0.1\r\ndatasets==2.21.0\r\ndill==0.3.8\r\ndistro==1.9.0\r\nevaluate==0.4.2\r\nfilelock==3.15.4\r\nFlask==3.0.3\r\nfrozenlist==1.4.1\r\nfsspec==2024.6.1\r\ngguf==0.10.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttpx==0.27.2\r\nhuggingface-hub==0.24.6\r\nidna==3.8\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.5.0\r\njoblib==1.4.2\r\njsonlines==4.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\nlm_eval==0.4.2\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmbstrdecoder==1.1.3\r\nmdurl==0.1.2\r\nmore-itertools==10.4.0\r\nmpmath==1.3.0\r\nmultidict==6.0.5\r\nmultiprocess==0.70.16\r\nnarwhals==1.6.0\r\nnetworkx==3.3\r\nninja==1.11.1.1\r\nnltk==3.9.1\r\nnumexpr==2.10.1\r\nnumpy==1.26.4\r\nopenai==1.43.0\r\npackaging==24.1\r\npandas==2.2.2\r\npathvalidate==3.2.1\r\npeft==0.12.0\r\npillow==10.4.0\r\nportalocker==2.10.1\r\nprotobuf==5.28.0\r\npsutil==6.0.0\r\npyarrow==17.0.0\r\npybind11==2.13.5\r\npycryptodomex==3.20.0\r\npydantic==2.8.2\r\npydantic_core==2.20.1\r\npydeck==0.9.1\r\nPygments==2.18.0\r\npytablewriter==1.2.0\r\npython-dateutil==2.9.0.post0\r\npytz==2024.1\r\nPyYAML==6.0.2\r\nreferencing==0.35.1\r\nregex==2024.7.24\r\nrequests==2.32.3\r\nrich==13.8.0\r\nrouge_score==0.1.2\r\nrpds-py==0.20.0\r\nsacrebleu==2.4.3\r\nsafetensors==0.4.4\r\nscikit-learn==1.5.1\r\nscipy==1.14.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsnakeviz==2.2.0\r\nsniffio==1.3.1\r\nsqlitedict==2.1.0\r\nstreamlit==1.38.0\r\nsympy==1.13.1\r\ntabledata==1.3.3\r\ntabulate==0.9.0\r\ntcolorpy==0.1.6\r\ntenacity==8.5.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.7.0\r\ntimm==1.0.7\r\ntokenizers==0.19.1\r\ntoml==0.10.2\r\ntomli==2.0.1\r\ntorch==2.5.0.dev20240716\r\ntorchao @ git+https://github.com/pytorch/ao.git@e11201a62669f582d81cdb33e031a07fb8dfc4f3\r\ntorchaudio==2.4.0.dev20240716\r\ntorchsr==1.0.4\r\ntorchvision==0.20.0.dev20240716\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntqdm-multiprocess==0.0.11\r\ntransformers==4.42.4\r\ntypepy==1.3.2\r\ntyping_extensions==4.12.2\r\ntzdata==2024.1\r\nurllib3==2.2.2\r\nWerkzeug==3.0.4\r\nword2number==1.1\r\nxxhash==3.5.0\r\nyarl==1.9.7\r\nzstandard==0.23.0\r\nzstd==1.5.5.1\r\n\r\nPyTorch Version\r\n2.5.0.dev20240716",
      "state": "closed",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-09-04T17:18:38Z",
      "updated_at": "2024-09-09T15:43:29Z",
      "closed_at": "2024-09-09T15:43:29Z",
      "labels": [
        "bug",
        "ExecuTorch"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1107/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1107",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1107",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:32.244883",
      "comments": [
        {
          "author": "dbort",
          "body": "Does the `/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/buck-out/v2` directory exist?\r\n\r\nDoes it help if you run\r\n```\r\n/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/buck2-bin/buck2-99773fe6f7963a7",
          "created_at": "2024-09-06T15:20:15Z"
        },
        {
          "author": "dbort",
          "body": "@byjlw said that the directory is present, and that the `buck2 kill` fixed it.\r\n\r\nThing is, we already have this line that's supposed to kill it when starting cmake: https://github.com/pytorch/executorch/blob/97396091080d152a09c20d4fa5a7ef3981b2ed48/build/Utils.cmake#L267\r\n\r\nAnd the logs show that i",
          "created_at": "2024-09-06T15:42:48Z"
        },
        {
          "author": "byjlw",
          "body": "Running this command fixed the problem. \r\n\r\n`/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/buck2-bin/buck2-99773fe6f7963a72ae5f7b737c02836e kill`",
          "created_at": "2024-09-09T15:43:29Z"
        }
      ]
    },
    {
      "issue_number": 1024,
      "title": "Support for quantized llm for smaller memory devices",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\nI believe, this is what ollama's one huge advantage. This can also encourage devs to go test llm which they can run on their machine's capabilities. Like me, I have my m1 16gb, I cannot really enjoy testing the meta llms, especially the llama3.1\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\r\n\r\n### RFC (Optional)\r\n\r\n_No response_",
      "state": "closed",
      "author": "jhetuts",
      "author_type": "User",
      "created_at": "2024-08-09T14:09:03Z",
      "updated_at": "2024-09-05T05:49:11Z",
      "closed_at": "2024-09-05T05:49:11Z",
      "labels": [
        "need-user-input",
        "Quantization"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1024/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1024",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1024",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:32.444422",
      "comments": [
        {
          "author": "larryliu0820",
          "body": "Hi @jhetuts thanks for providing feedback! Can you give a specific example on what quantized llm you are referring to? For llama3.1 we have a few quantization options and you can refer to this [readme](https://github.com/pytorch/torchchat/blob/main/docs/quantization.md#eager-mode). m1 16gb should be",
          "created_at": "2024-08-09T17:05:15Z"
        },
        {
          "author": "jhetuts",
          "body": "Got it @larryliu0820 , I've been testing with this. Thanks!",
          "created_at": "2024-09-05T05:49:11Z"
        }
      ]
    },
    {
      "issue_number": 1106,
      "title": "Can't run ET Install",
      "body": "### 🐛 Describe the bug\n\nCommand throws the following error\r\n`./torchchat/utils/scripts/install_et.sh`\r\n\r\nfollowing directory is empty \r\n`/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/buck-out/v2``\r\n```\r\n++ dirname ./torchchat/utils/scripts/install_et.sh\r\n+ source ./torchchat/utils/scripts/install_utils.sh\r\n++ set -ex pipefail\r\n++ COMMON_CMAKE_ARGS='    -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON'\r\n+ '[' '' == '' ']'\r\n+ ET_BUILD_DIR=et-build\r\n+ ENABLE_ET_PYBIND=pipefail\r\n+ pushd /Users/jessewhite/Documents/source/torchchat\r\n~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ find_cmake_prefix_path\r\n++ python3 -c 'from distutils.sysconfig import get_python_lib;print(get_python_lib())'\r\n+ path=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages\r\n+ MY_CMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages\r\n+ install_pip_dependencies\r\n+ echo 'Intalling common pip packages'\r\nIntalling common pip packages\r\n+ pip3 install wheel 'cmake>=3.19' ninja zstd\r\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (0.44.0)\r\nRequirement already satisfied: cmake>=3.19 in ./.venv/lib/python3.11/site-packages (3.30.2)\r\nRequirement already satisfied: ninja in ./.venv/lib/python3.11/site-packages (1.11.1.1)\r\nRequirement already satisfied: zstd in ./.venv/lib/python3.11/site-packages (1.5.5.1)\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n+ pushd /Users/jessewhite/Documents/source/torchchat\r\n~/Documents/source/torchchat ~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ pip3 install -r install/requirements.txt\r\nIgnoring tomli: markers 'python_version < \"3.11\"' don't match your environment\r\nRequirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 4)) (0.24.6)\r\nRequirement already satisfied: gguf in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 7)) (0.10.0)\r\nRequirement already satisfied: tiktoken in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 10)) (0.7.0)\r\nRequirement already satisfied: snakeviz in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 13)) (2.2.0)\r\nRequirement already satisfied: sentencepiece in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 14)) (0.2.0)\r\nRequirement already satisfied: numpy<2.0 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 15)) (1.26.4)\r\nRequirement already satisfied: lm-eval==0.4.2 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: blobfile in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 18)) (3.0.0)\r\nRequirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 20)) (1.43.0)\r\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 23)) (0.44.0)\r\nRequirement already satisfied: cmake>=3.24 in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 24)) (3.30.2)\r\nRequirement already satisfied: ninja in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 25)) (1.11.1.1)\r\nRequirement already satisfied: zstd in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 26)) (1.5.5.1)\r\nRequirement already satisfied: streamlit in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 29)) (1.38.0)\r\nRequirement already satisfied: flask in ./.venv/lib/python3.11/site-packages (from -r install/requirements.txt (line 32)) (3.0.3)\r\nRequirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.34.0)\r\nRequirement already satisfied: evaluate in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.2)\r\nRequirement already satisfied: datasets>=2.16.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.21.0)\r\nRequirement already satisfied: jsonlines in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.0.0)\r\nRequirement already satisfied: numexpr in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: peft>=0.2.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.12.0)\r\nRequirement already satisfied: pybind11>=2.6.2 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.13.5)\r\nRequirement already satisfied: pytablewriter in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.2.0)\r\nRequirement already satisfied: rouge-score>=0.0.4 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.2)\r\nRequirement already satisfied: sacrebleu>=1.5.0 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.3)\r\nRequirement already satisfied: scikit-learn>=0.24.1 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.5.1)\r\nRequirement already satisfied: sqlitedict in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: torch>=1.8 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.5.0.dev20240716)\r\nRequirement already satisfied: tqdm-multiprocess in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.0.11)\r\nRequirement already satisfied: transformers>=4.1 in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (4.42.4)\r\nRequirement already satisfied: zstandard in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.23.0)\r\nRequirement already satisfied: dill in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.3.8)\r\nRequirement already satisfied: word2number in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1)\r\nRequirement already satisfied: more-itertools in ./.venv/lib/python3.11/site-packages (from lm-eval==0.4.2->-r install/requirements.txt (line 17)) (10.4.0)\r\nRequirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (3.15.4)\r\nRequirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2024.6.1)\r\nRequirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (24.1)\r\nRequirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (6.0.2)\r\nRequirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (2.32.3)\r\nRequirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.66.5)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub->-r install/requirements.txt (line 4)) (4.12.2)\r\nRequirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken->-r install/requirements.txt (line 10)) (2024.7.24)\r\nRequirement already satisfied: tornado>=2.0 in ./.venv/lib/python3.11/site-packages (from snakeviz->-r install/requirements.txt (line 13)) (6.4.1)\r\nRequirement already satisfied: pycryptodomex>=3.8 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (3.20.0)\r\nRequirement already satisfied: urllib3<3,>=1.25.3 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (2.2.2)\r\nRequirement already satisfied: lxml>=4.9 in ./.venv/lib/python3.11/site-packages (from blobfile->-r install/requirements.txt (line 18)) (5.3.0)\r\nRequirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (4.4.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.9.0)\r\nRequirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.27.2)\r\nRequirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (0.5.0)\r\nRequirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (2.8.2)\r\nRequirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai->-r install/requirements.txt (line 20)) (1.3.1)\r\nRequirement already satisfied: altair<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.4.1)\r\nRequirement already satisfied: blinker<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (1.8.2)\r\nRequirement already satisfied: cachetools<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.5.0)\r\nRequirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.1.7)\r\nRequirement already satisfied: pandas<3,>=1.3.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (2.2.2)\r\nRequirement already satisfied: pillow<11,>=7.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (10.4.0)\r\nRequirement already satisfied: protobuf<6,>=3.20 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (5.28.0)\r\nRequirement already satisfied: pyarrow>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (17.0.0)\r\nRequirement already satisfied: rich<14,>=10.14.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (13.8.0)\r\nRequirement already satisfied: tenacity<9,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (8.5.0)\r\nRequirement already satisfied: toml<2,>=0.10.1 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.10.2)\r\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (3.1.43)\r\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in ./.venv/lib/python3.11/site-packages (from streamlit->-r install/requirements.txt (line 29)) (0.9.1)\r\nRequirement already satisfied: Werkzeug>=3.0.0 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.0.4)\r\nRequirement already satisfied: Jinja2>=3.1.2 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (3.1.4)\r\nRequirement already satisfied: itsdangerous>=2.1.2 in ./.venv/lib/python3.11/site-packages (from flask->-r install/requirements.txt (line 32)) (2.2.0)\r\nRequirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.0)\r\nRequirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from accelerate>=0.21.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.4)\r\nRequirement already satisfied: jsonschema>=3.0 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (4.23.0)\r\nRequirement already satisfied: narwhals>=1.5.2 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (1.6.2)\r\nRequirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r install/requirements.txt (line 20)) (3.8)\r\nRequirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.70.16)\r\nRequirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.10.5)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (4.0.11)\r\nRequirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (2024.8.30)\r\nRequirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (1.0.5)\r\nRequirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r install/requirements.txt (line 20)) (0.14.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from Jinja2>=3.1.2->flask->-r install/requirements.txt (line 32)) (2.1.5)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit->-r install/requirements.txt (line 29)) (2024.1)\r\nRequirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (0.7.0)\r\nRequirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r install/requirements.txt (line 20)) (2.20.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub->-r install/requirements.txt (line 4)) (3.3.2)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (2.18.0)\r\nRequirement already satisfied: absl-py in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.1.0)\r\nRequirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.9.1)\r\nRequirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.11/site-packages (from rouge-score>=0.0.4->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.16.0)\r\nRequirement already satisfied: portalocker in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.10.1)\r\nRequirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.9.0)\r\nRequirement already satisfied: colorama in ./.venv/lib/python3.11/site-packages (from sacrebleu>=1.5.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.4.6)\r\nRequirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.14.1)\r\nRequirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.5.0)\r\nRequirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.13.1)\r\nRequirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.3)\r\nRequirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers>=4.1->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.19.1)\r\nRequirement already satisfied: attrs>=19.2.0 in ./.venv/lib/python3.11/site-packages (from jsonlines->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (24.2.0)\r\nRequirement already satisfied: setuptools>=38.3.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (65.5.0)\r\nRequirement already satisfied: DataProperty<2,>=1.0.1 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.0.1)\r\nRequirement already satisfied: mbstrdecoder<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.1.3)\r\nRequirement already satisfied: pathvalidate<4,>=2.3.0 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (3.2.1)\r\nRequirement already satisfied: tabledata<2,>=1.3.1 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.3)\r\nRequirement already satisfied: tcolorpy<1,>=0.0.5 in ./.venv/lib/python3.11/site-packages (from pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (0.1.6)\r\nRequirement already satisfied: typepy<2,>=1.3.2 in ./.venv/lib/python3.11/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.2)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (2.4.0)\r\nRequirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.1)\r\nRequirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.4.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (6.0.5)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.9.8)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r install/requirements.txt (line 29)) (5.0.1)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (2023.12.1)\r\nRequirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.35.1)\r\nRequirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r install/requirements.txt (line 29)) (0.20.0)\r\nRequirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r install/requirements.txt (line 29)) (0.1.2)\r\nRequirement already satisfied: chardet<6,>=3.0.4 in ./.venv/lib/python3.11/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (5.2.0)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.8->lm-eval==0.4.2->-r install/requirements.txt (line 17)) (1.3.0)\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n+ popd\r\n~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ clone_executorch\r\n+ echo 'Cloning executorch to /Users/jessewhite/Documents/source/torchchat/et-build/src'\r\nCloning executorch to /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n+ rm -rf /Users/jessewhite/Documents/source/torchchat/et-build\r\n+ mkdir -p /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n+ pushd /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n~/Documents/source/torchchat/et-build/src ~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ git clone https://github.com/pytorch/executorch.git\r\nCloning into 'executorch'...\r\nremote: Enumerating objects: 115546, done.\r\nremote: Counting objects: 100% (2950/2950), done.\r\nremote: Compressing objects: 100% (1286/1286), done.\r\nremote: Total 115546 (delta 1933), reused 2484 (delta 1595), pack-reused 112596 (from 1)\r\nReceiving objects: 100% (115546/115546), 85.98 MiB | 7.08 MiB/s, done.\r\nResolving deltas: 100% (89349/89349), done.\r\n+ cd executorch\r\n++ cat /Users/jessewhite/Documents/source/torchchat/install/.pins/et-pin.txt\r\n+ git checkout 91298923a0076c1b41059efb6dad2876426e4b03\r\nNote: switching to '91298923a0076c1b41059efb6dad2876426e4b03'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by switching back to a branch.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -c with the switch command. Example:\r\n\r\n  git switch -c <new-branch-name>\r\n\r\nOr undo this operation with:\r\n\r\n  git switch -\r\n\r\nTurn off this advice by setting config variable advice.detachedHead to false\r\n\r\nHEAD is now at 91298923a immutable accessors in graph signature (#4433)\r\n+ echo 'Install executorch: submodule update'\r\nInstall executorch: submodule update\r\n+ git submodule sync\r\n+ git submodule update --init\r\nSubmodule 'backends/arm/third-party/ethos-u-core-driver' (https://review.mlplatform.org/ml/ethos-u/ethos-u-core-driver) registered for path 'backends/arm/third-party/ethos-u-core-driver'\r\nSubmodule 'backends/arm/third-party/serialization_lib' (https://review.mlplatform.org/tosa/serialization_lib) registered for path 'backends/arm/third-party/serialization_lib'\r\nSubmodule 'backends/vulkan/third-party/Vulkan-Headers' (https://github.com/KhronosGroup/Vulkan-Headers) registered for path 'backends/vulkan/third-party/Vulkan-Headers'\r\nSubmodule 'backends/vulkan/third-party/VulkanMemoryAllocator' (https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.git) registered for path 'backends/vulkan/third-party/VulkanMemoryAllocator'\r\nSubmodule 'backends/vulkan/third-party/volk' (https://github.com/zeux/volk) registered for path 'backends/vulkan/third-party/volk'\r\nSubmodule 'backends/xnnpack/third-party/FP16' (https://github.com/Maratyszcza/FP16.git) registered for path 'backends/xnnpack/third-party/FP16'\r\nSubmodule 'backends/xnnpack/third-party/FXdiv' (https://github.com/Maratyszcza/FXdiv.git) registered for path 'backends/xnnpack/third-party/FXdiv'\r\nSubmodule 'backends/xnnpack/third-party/XNNPACK' (https://github.com/digantdesai/XNNPACK.git) registered for path 'backends/xnnpack/third-party/XNNPACK'\r\nSubmodule 'backends/xnnpack/third-party/cpuinfo' (https://github.com/pytorch/cpuinfo.git) registered for path 'backends/xnnpack/third-party/cpuinfo'\r\nSubmodule 'backends/xnnpack/third-party/pthreadpool' (https://github.com/Maratyszcza/pthreadpool.git) registered for path 'backends/xnnpack/third-party/pthreadpool'\r\nSubmodule 'examples/third-party/LLaVA' (https://github.com/haotian-liu/LLaVA.git) registered for path 'examples/third-party/LLaVA'\r\nSubmodule 'examples/third-party/fbjni' (https://github.com/facebookincubator/fbjni.git) registered for path 'examples/third-party/fbjni'\r\nSubmodule 'extension/llm/third-party/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'extension/llm/third-party/abseil-cpp'\r\nSubmodule 'extension/llm/third-party/re2' (https://github.com/google/re2.git) registered for path 'extension/llm/third-party/re2'\r\nSubmodule 'extension/llm/third-party/sentencepiece' (https://github.com/google/sentencepiece.git) registered for path 'extension/llm/third-party/sentencepiece'\r\nSubmodule 'kernels/optimized/third-party/eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'kernels/optimized/third-party/eigen'\r\nSubmodule 'third-party/flatbuffers' (https://github.com/google/flatbuffers.git) registered for path 'third-party/flatbuffers'\r\nSubmodule 'third-party/flatcc' (https://github.com/dvidelabs/flatcc.git) registered for path 'third-party/flatcc'\r\nSubmodule 'third-party/gflags' (https://github.com/gflags/gflags.git) registered for path 'third-party/gflags'\r\nSubmodule 'third-party/googletest' (https://github.com/google/googletest.git) registered for path 'third-party/googletest'\r\nSubmodule 'third-party/ios-cmake' (https://github.com/leetal/ios-cmake) registered for path 'third-party/ios-cmake'\r\nSubmodule 'third-party/prelude' (https://github.com/facebook/buck2-prelude.git) registered for path 'third-party/prelude'\r\nSubmodule 'third-party/pybind11' (https://github.com/pybind/pybind11.git) registered for path 'third-party/pybind11'\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/arm/third-party/ethos-u-core-driver'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/arm/third-party/serialization_lib'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/vulkan/third-party/Vulkan-Headers'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/vulkan/third-party/VulkanMemoryAllocator'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/vulkan/third-party/volk'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/FP16'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/FXdiv'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/XNNPACK'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/cpuinfo'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/backends/xnnpack/third-party/pthreadpool'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/examples/third-party/LLaVA'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/examples/third-party/fbjni'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/extension/llm/third-party/abseil-cpp'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/extension/llm/third-party/re2'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/extension/llm/third-party/sentencepiece'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/kernels/optimized/third-party/eigen'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/flatbuffers'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/flatcc'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/gflags'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/googletest'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/ios-cmake'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/prelude'...\r\nCloning into '/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/third-party/pybind11'...\r\nSubmodule path 'backends/arm/third-party/ethos-u-core-driver': checked out '90f9df900acdc0718ecd2dfdc53780664758dec5'\r\nSubmodule path 'backends/arm/third-party/serialization_lib': checked out '187af0d41fe75d08d2a7ec84c1b4d24b9b641ed2'\r\nSubmodule path 'backends/vulkan/third-party/Vulkan-Headers': checked out '0c5928795a66e93f65e5e68a36d8daa79a209dc2'\r\nSubmodule path 'backends/vulkan/third-party/VulkanMemoryAllocator': checked out 'a6bfc237255a6bac1513f7c1ebde6d8aed6b5191'\r\nSubmodule path 'backends/vulkan/third-party/volk': checked out 'b3bc21e584f97400b6884cb2a541a56c6a5ddba3'\r\nSubmodule path 'backends/xnnpack/third-party/FP16': checked out '4dfe081cf6bcd15db339cf2680b9281b8451eeb3'\r\nSubmodule path 'backends/xnnpack/third-party/FXdiv': checked out 'b408327ac2a15ec3e43352421954f5b1967701d1'\r\nSubmodule path 'backends/xnnpack/third-party/XNNPACK': checked out '1d139a3b4b7155889c88c31f370a82c48e7ca89c'\r\nSubmodule path 'backends/xnnpack/third-party/cpuinfo': checked out 'd6860c477c99f1fce9e28eb206891af3c0e1a1d7'\r\nSubmodule path 'backends/xnnpack/third-party/pthreadpool': checked out '4fe0e1e183925bf8cfa6aae24237e724a96479b8'\r\nSubmodule path 'examples/third-party/LLaVA': checked out '7440ec9ee37b0374c6b5548818e89878e38f3353'\r\nSubmodule path 'examples/third-party/fbjni': checked out '52a14f0daa889a20d8984798b8d96eb03cebd334'\r\nSubmodule path 'extension/llm/third-party/abseil-cpp': checked out 'eb852207758a773965301d0ae717e4235fc5301a'\r\nSubmodule path 'extension/llm/third-party/re2': checked out '6dcd83d60f7944926bfd308cc13979fc53dd69ca'\r\nSubmodule path 'extension/llm/third-party/sentencepiece': checked out '6225e08edb2577757163b3f5dbba4c0b670ef445'\r\nSubmodule path 'kernels/optimized/third-party/eigen': checked out 'a39ade4ccf99df845ec85c580fbbb324f71952fa'\r\nSubmodule path 'third-party/flatbuffers': checked out '595bf0007ab1929570c7671f091313c8fc20644e'\r\nSubmodule path 'third-party/flatcc': checked out '896db54787e8b730a6be482c69324751f3f5f117'\r\nSubmodule path 'third-party/gflags': checked out 'a738fdf9338412f83ab3f26f31ac11ed3f3ec4bd'\r\nSubmodule path 'third-party/googletest': checked out 'e2239ee6043f73722e7aa812a459f54a28552929'\r\nSubmodule path 'third-party/ios-cmake': checked out '06465b27698424cf4a04a5ca4904d50a3c966c45'\r\nSubmodule path 'third-party/prelude': checked out '4e9e6d50b8b461564a7e351ff60b87fe59d7e53b'\r\nSubmodule path 'third-party/pybind11': checked out '8c7b8dd0ae74b36b7d42f77b0dd4096ebb7f4ab1'\r\n+ popd\r\n~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ install_executorch_libs pipefail\r\n+ export 'CMAKE_ARGS=        -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON     -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages     -DCMAKE_INSTALL_PREFIX=/Users/jessewhite/Documents/source/torchchat/et-build/install'\r\n+ CMAKE_ARGS='        -DCMAKE_BUILD_TYPE=Release     -DEXECUTORCH_ENABLE_LOGGING=ON     -DEXECUTORCH_LOG_LEVEL=Info     -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON     -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON     -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON     -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON     -DEXECUTORCH_BUILD_XNNPACK=ON     -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages     -DCMAKE_INSTALL_PREFIX=/Users/jessewhite/Documents/source/torchchat/et-build/install'\r\n+ export 'CMAKE_BUILD_ARGS=--target install'\r\n+ CMAKE_BUILD_ARGS='--target install'\r\n+ install_executorch_python_libs pipefail\r\n+ '[' '!' -d /Users/jessewhite/Documents/source/torchchat/et-build ']'\r\n+ pushd /Users/jessewhite/Documents/source/torchchat/et-build/src\r\n~/Documents/source/torchchat/et-build/src ~/Documents/source/torchchat ~/Documents/source/torchchat\r\n+ cd executorch\r\n+ echo 'Building and installing python libraries'\r\nBuilding and installing python libraries\r\n+ '[' pipefail = false ']'\r\n+ echo 'Installing pybind'\r\nInstalling pybind\r\n+ bash ./install_requirements.sh --pybind xnnpack\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\r\nRequirement already satisfied: torch==2.5.0.dev20240716 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (2.5.0.dev20240716)\r\nRequirement already satisfied: torchvision==0.20.0.dev20240716 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (0.20.0.dev20240716)\r\nRequirement already satisfied: cmake in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (3.30.2)\r\nRequirement already satisfied: pip>=23 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (24.0)\r\nRequirement already satisfied: pyyaml in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (6.0.2)\r\nRequirement already satisfied: setuptools>=63 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (65.5.0)\r\nRequirement already satisfied: tomli in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (2.0.1)\r\nRequirement already satisfied: wheel in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (0.44.0)\r\nRequirement already satisfied: zstd in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (1.5.5.1)\r\nRequirement already satisfied: timm==1.0.7 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (1.0.7)\r\nRequirement already satisfied: torchaudio==2.4.0.dev20240716 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (2.4.0.dev20240716)\r\nRequirement already satisfied: torchsr==1.0.4 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (1.0.4)\r\nRequirement already satisfied: transformers==4.42.4 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (4.42.4)\r\nRequirement already satisfied: filelock in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.15.4)\r\nRequirement already satisfied: typing-extensions>=4.8.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (4.12.2)\r\nRequirement already satisfied: sympy in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (1.13.1)\r\nRequirement already satisfied: networkx in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.3)\r\nRequirement already satisfied: jinja2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (3.1.4)\r\nRequirement already satisfied: fsspec in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torch==2.5.0.dev20240716) (2024.6.1)\r\nRequirement already satisfied: numpy in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torchvision==0.20.0.dev20240716) (1.26.4)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from torchvision==0.20.0.dev20240716) (10.4.0)\r\nRequirement already satisfied: huggingface_hub in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from timm==1.0.7) (0.24.6)\r\nRequirement already satisfied: safetensors in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from timm==1.0.7) (0.4.4)\r\nRequirement already satisfied: packaging>=20.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (24.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (2024.7.24)\r\nRequirement already satisfied: requests in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (2.32.3)\r\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (0.19.1)\r\nRequirement already satisfied: tqdm>=4.27 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from transformers==4.42.4) (4.66.5)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from jinja2->torch==2.5.0.dev20240716) (2.1.5)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (3.8)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (2.2.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from requests->transformers==4.42.4) (2024.8.30)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from sympy->torch==2.5.0.dev20240716) (1.3.0)\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\nUsing pip 24.0 from /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\nWARNING: The index url \"\" seems invalid, please provide a scheme.\r\nLooking in indexes: https://pypi.org/simple, \r\nProcessing /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch\r\n  Running command Preparing metadata (pyproject.toml)\r\n  /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/setuptools/config/pyprojecttoml.py:108: _BetaConfiguration: Support for `[tool.setuptools]` in `pyproject.toml` is still *beta*.\r\n    warnings.warn(msg, _BetaConfiguration)\r\n  running dist_info\r\n  creating /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info\r\n  writing /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/PKG-INFO\r\n  writing dependency_links to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/dependency_links.txt\r\n  writing entry points to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/entry_points.txt\r\n  writing requirements to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/requires.txt\r\n  writing top-level names to /private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/top_level.txt\r\n  writing manifest file '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/SOURCES.txt'\r\n  reading manifest file '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/SOURCES.txt'\r\n  adding license file 'LICENSE'\r\n  writing manifest file '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch.egg-info/SOURCES.txt'\r\n  creating '/private/var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/pip-modern-metadata-d5qo6ub9/executorch-0.4.0a0+9129892.dist-info'\r\n  Preparing metadata (pyproject.toml) ... done\r\nWARNING: Location 'expecttest/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting expecttest (from executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for expecttest from https://files.pythonhosted.org/packages/73/a8/9427813686fe043eaf817b99444d4d321ce14f9434e3ffb6bbd1a182a8df/expecttest-0.2.1-py3-none-any.whl.metadata\r\n  Using cached expecttest-0.2.1-py3-none-any.whl.metadata (2.5 kB)\r\nWARNING: Location 'flatbuffers/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting flatbuffers (from executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for flatbuffers from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\r\n  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\r\nWARNING: Location 'hypothesis/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting hypothesis (from executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for hypothesis from https://files.pythonhosted.org/packages/e1/8c/8c58a2773d00bdfe6895180006127128c5dce51fafba984c530200e6fff6/hypothesis-6.111.2-py3-none-any.whl.metadata\r\n  Using cached hypothesis-6.111.2-py3-none-any.whl.metadata (6.2 kB)\r\nRequirement already satisfied: mpmath==1.3.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.3.0)\r\nRequirement already satisfied: numpy>=1.25.2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.26.4)\r\nRequirement already satisfied: packaging in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (24.1)\r\nRequirement already satisfied: pandas in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (2.2.2)\r\nWARNING: Location 'parameterized/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting parameterized (from executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for parameterized from https://files.pythonhosted.org/packages/00/2f/804f58f0b856ab3bf21617cccf5b39206e6c4c94c2cd227bde125ea6105f/parameterized-0.9.0-py2.py3-none-any.whl.metadata\r\n  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\r\nWARNING: Location 'pytest/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting pytest (from executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for pytest from https://files.pythonhosted.org/packages/0f/f9/cf155cf32ca7d6fa3601bc4c5dd19086af4b320b706919d48a4c79081cf9/pytest-8.3.2-py3-none-any.whl.metadata\r\n  Using cached pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\r\nWARNING: Location 'pytest-xdist/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting pytest-xdist (from executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for pytest-xdist from https://files.pythonhosted.org/packages/6d/82/1d96bf03ee4c0fdc3c0cbe61470070e659ca78dc0086fb88b66c185e2449/pytest_xdist-3.6.1-py3-none-any.whl.metadata\r\n  Using cached pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\r\nRequirement already satisfied: pyyaml in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (6.0.2)\r\nWARNING: Location 'ruamel-yaml/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting ruamel.yaml (from executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for ruamel.yaml from https://files.pythonhosted.org/packages/73/67/8ece580cc363331d9a53055130f86b096bf16e38156e33b1d3014fffda6b/ruamel.yaml-0.18.6-py3-none-any.whl.metadata\r\n  Using cached ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\r\nRequirement already satisfied: sympy in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (1.13.1)\r\nRequirement already satisfied: tabulate in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from executorch==0.4.0a0+9129892) (0.9.0)\r\nRequirement already satisfied: attrs>=22.2.0 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from hypothesis->executorch==0.4.0a0+9129892) (24.2.0)\r\nWARNING: Location 'sortedcontainers/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis->executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for sortedcontainers<3.0.0,>=2.1.0 from https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata\r\n  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\r\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2.9.0.post0)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2024.1)\r\nRequirement already satisfied: tzdata>=2022.7 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from pandas->executorch==0.4.0a0+9129892) (2024.1)\r\nWARNING: Location 'iniconfig/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting iniconfig (from pytest->executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for iniconfig from https://files.pythonhosted.org/packages/ef/a6/62565a6e1cf69e10f5727360368e451d4b7f58beeac6173dc9db836a5b46/iniconfig-2.0.0-py3-none-any.whl.metadata\r\n  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\r\nWARNING: Location 'pluggy/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting pluggy<2,>=1.5 (from pytest->executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for pluggy<2,>=1.5 from https://files.pythonhosted.org/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl.metadata\r\n  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\r\nWARNING: Location 'execnet/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting execnet>=2.1 (from pytest-xdist->executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for execnet>=2.1 from https://files.pythonhosted.org/packages/43/09/2aea36ff60d16dd8879bdb2f5b3ee0ba8d08cbbdcdfe870e695ce3784385/execnet-2.1.1-py3-none-any.whl.metadata\r\n  Using cached execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\r\nWARNING: Location 'ruamel-yaml-clib/' is ignored: it is either a non-existing path or lacks a specific scheme.\r\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->executorch==0.4.0a0+9129892)\r\n  Obtaining dependency information for ruamel.yaml.clib>=0.2.7 from https://files.pythonhosted.org/packages/01/b0/4ddef56e9f703d7909febc3a421d709a3482cda25826816ec595b73e3847/ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl.metadata\r\n  Using cached ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl.metadata (2.2 kB)\r\nRequirement already satisfied: six>=1.5 in /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->executorch==0.4.0a0+9129892) (1.16.0)\r\nUsing cached expecttest-0.2.1-py3-none-any.whl (7.4 kB)\r\nUsing cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\r\nUsing cached hypothesis-6.111.2-py3-none-any.whl (467 kB)\r\nUsing cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\r\nUsing cached pytest-8.3.2-py3-none-any.whl (341 kB)\r\nUsing cached pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\r\nUsing cached ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\r\nUsing cached execnet-2.1.1-py3-none-any.whl (40 kB)\r\nUsing cached pluggy-1.5.0-py3-none-any.whl (20 kB)\r\nUsing cached ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl (134 kB)\r\nUsing cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\nUsing cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nBuilding wheels for collected packages: executorch\r\n  Running command Building wheel for executorch (pyproject.toml)\r\n  /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/setuptools/config/pyprojecttoml.py:108: _BetaConfiguration: Support for `[tool.setuptools]` in `pyproject.toml` is still *beta*.\r\n    warnings.warn(msg, _BetaConfiguration)\r\n  running bdist_wheel\r\n  running build\r\n  command options for 'CustomBuild':\r\n    build_base = pip-out\r\n    build_purelib = pip-out/lib\r\n    build_platlib = pip-out/lib.macosx-10.9-universal2-cpython-311\r\n    build_lib = pip-out/lib.macosx-10.9-universal2-cpython-311\r\n    build_scripts = pip-out/scripts-3.11\r\n    build_temp = pip-out/temp.macosx-10.9-universal2-cpython-311\r\n    plat_name = macosx-10.9-universal2\r\n    compiler = None\r\n    parallel = 15\r\n    debug = None\r\n    force = None\r\n    executable = /Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11\r\n  creating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out\r\n  creating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311\r\n  creating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out\r\n  deleting /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/CMakeCache.txt\r\n  cmake -S /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch -B /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out -DBUCK2= -DPYTHON_EXECUTABLE=/Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11 -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages -DCMAKE_BUILD_TYPE=Release -DEXECUTORCH_ENABLE_LOGGING=ON -DEXECUTORCH_LOG_LEVEL=Info -DCMAKE_OSX_DEPLOYMENT_TARGET=10.15 -DEXECUTORCH_SEPARATE_FLATCC_HOST_PROJECT=OFF -DEXECUTORCH_BUILD_PYBIND=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED_AOT=ON -DEXECUTORCH_BUILD_KERNELS_CUSTOM=ON -DEXECUTORCH_BUILD_KERNELS_CUSTOM_AOT=ON -DCMAKE_BUILD_TYPE=Release -DEXECUTORCH_ENABLE_LOGGING=ON -DEXECUTORCH_LOG_LEVEL=Info -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON -DEXECUTORCH_BUILD_EXTENSION_DATA_LOADER=ON -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON -DEXECUTORCH_BUILD_KERNELS_QUANTIZED=ON -DEXECUTORCH_BUILD_XNNPACK=ON -DCMAKE_PREFIX_PATH=/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages -DCMAKE_INSTALL_PREFIX=/Users/jessewhite/Documents/source/torchchat/et-build/install -DEXECUTORCH_BUILD_XNNPACK=ON\r\n  -- The C compiler identification is AppleClang 15.0.0.15000100\r\n  -- The CXX compiler identification is AppleClang 15.0.0.15000100\r\n  -- Detecting C compiler ABI info\r\n  -- Detecting C compiler ABI info - done\r\n  -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\r\n  -- Detecting C compile features\r\n  -- Detecting C compile features - done\r\n  -- Detecting CXX compiler ABI info\r\n  -- Detecting CXX compiler ABI info - done\r\n  -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\r\n  -- Detecting CXX compile features\r\n  -- Detecting CXX compile features - done\r\n  -- Downloading FXdiv to /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/FXdiv-source (define FXDIV_SOURCE_DIR to avoid it)\r\n  -- Configuring done (0.0s)\r\n  -- Generating done (0.0s)\r\n  -- Build files have been written to: /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/FXdiv-download\r\n  [ 11%] Creating directories for 'fxdiv'\r\n  [ 22%] Performing download step (git clone) for 'fxdiv'\r\n  Cloning into 'FXdiv-source'...\r\n  Already on 'master'\r\n  Your branch is up to date with 'origin/master'.\r\n  [ 33%] Performing update step for 'fxdiv'\r\n  -- Fetching latest from the remote origin\r\n  [ 44%] No patch step for 'fxdiv'\r\n  [ 55%] No configure step for 'fxdiv'\r\n  [ 66%] No build step for 'fxdiv'\r\n  [ 77%] No install step for 'fxdiv'\r\n  [ 88%] No test step for 'fxdiv'\r\n  [100%] Completed 'fxdiv'\r\n  [100%] Built target fxdiv\r\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n  -- Found Threads: TRUE\r\n  -- Using python executable '/Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11'\r\n  -- Resolved buck2 as /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/buck2-bin/buck2-99773fe6f7963a72ae5f7b737c02836e.\r\n  -- Killing buck2 daemon\r\n  -- executorch: Generating source lists\r\n  -- executorch: Generating source file list /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/executorch_srcs.cmake\r\n  Error while generating /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/executorch_srcs.cmake. Exit code: 1\r\n  Output:\r\n\r\n  Error:\r\n  Traceback (most recent call last):\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/buck_util.py\", line 26, in run\r\n      cp: subprocess.CompletedProcess = subprocess.run(\r\n                                        ^^^^^^^^^^^^^^^\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 571, in run\r\n      raise CalledProcessError(retcode, process.args,\r\n  subprocess.CalledProcessError: Command '['/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/pip-out/temp.macosx-10.9-universal2-cpython-311/cmake-out/buck2-bin/buck2-99773fe6f7963a72ae5f7b737c02836e', 'cquery', \"inputs(deps('//runtime/executor:program'))\"]' returned non-zero exit status 2.\r\n\r\n  The above exception was the direct cause of the following exception:\r\n\r\n  Traceback (most recent call last):\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/extract_sources.py\", line 218, in <module>\r\n      main()\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/extract_sources.py\", line 203, in main\r\n      target_to_srcs[name] = sorted(target.get_sources(graph, runner))\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/extract_sources.py\", line 116, in get_sources\r\n      sources: set[str] = set(runner.run([\"cquery\", query]))\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    File \"/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/build/buck_util.py\", line 31, in run\r\n      raise RuntimeError(ex.stderr.decode(\"utf-8\")) from ex\r\n  RuntimeError: Command failed:\r\n  Error validating working directory\r\n\r\n  Caused by:\r\n      0: Failed to stat `/Users/jessewhite/Documents/source/torchchat/et-build/src/executorch/buck-out/v2`\r\n      1: ENOENT: No such file or directory\r\n\r\n\r\n  CMake Error at build/Utils.cmake:191 (message):\r\n    executorch: source list generation failed\r\n  Call Stack (most recent call first):\r\n    CMakeLists.txt:327 (extract_sources)\r\n\r\n\r\n  -- Configuring incomplete, errors occurred!\r\n  error: command '/Users/jessewhite/Documents/source/torchchat/.venv/bin/cmake' failed with exit code 1\r\n  error: subprocess-exited-with-error\r\n  \r\n  × Building wheel for executorch (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /Users/jessewhite/Documents/source/torchchat/.venv/bin/python3.11 /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /var/folders/7k/2n2lk60d5n57_33yh299tt0c0000gn/T/tmpihrnis4e\r\n  cwd: /Users/jessewhite/Documents/source/torchchat/et-build/src/executorch\r\n  Building wheel for executorch (pyproject.toml) ... error\r\n  ERROR: Failed building wheel for executorch\r\nFailed to build executorch\r\nERROR: Could not build wheels for executorch, which is required to install pyproject.toml-based projects\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n(.venv) jessewhite@MadMax torchchat %                    \r\n```\n\n### Versions\n\nOperating System Information\r\nDarwin MadMax.local 23.1.0 Darwin Kernel Version 23.1.0: Mon Oct 9 21:33:00 PDT 2023; root:xnu-10002.41.9~7/RELEASE_ARM64_T6031 arm64\r\n\r\nPython Version\r\nPython 3.11.8\r\n\r\nPIP Version\r\npip 24.0 from /Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n\r\nInstalled Packages\r\nabsl-py==2.1.0\r\naccelerate==0.34.0\r\naiohappyeyeballs==2.4.0\r\naiohttp==3.10.5\r\naiosignal==1.3.1\r\naltair==5.4.1\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nattrs==24.2.0\r\nblinker==1.8.2\r\nblobfile==3.0.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\nchardet==5.2.0\r\ncharset-normalizer==3.3.2\r\nclick==8.1.7\r\ncmake==3.30.2\r\ncolorama==0.4.6\r\nDataProperty==1.0.1\r\ndatasets==2.21.0\r\ndill==0.3.8\r\ndistro==1.9.0\r\nevaluate==0.4.2\r\nfilelock==3.15.4\r\nFlask==3.0.3\r\nfrozenlist==1.4.1\r\nfsspec==2024.6.1\r\ngguf==0.10.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttpx==0.27.2\r\nhuggingface-hub==0.24.6\r\nidna==3.8\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.5.0\r\njoblib==1.4.2\r\njsonlines==4.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\nlm_eval==0.4.2\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmbstrdecoder==1.1.3\r\nmdurl==0.1.2\r\nmore-itertools==10.4.0\r\nmpmath==1.3.0\r\nmultidict==6.0.5\r\nmultiprocess==0.70.16\r\nnarwhals==1.6.0\r\nnetworkx==3.3\r\nninja==1.11.1.1\r\nnltk==3.9.1\r\nnumexpr==2.10.1\r\nnumpy==1.26.4\r\nopenai==1.43.0\r\npackaging==24.1\r\npandas==2.2.2\r\npathvalidate==3.2.1\r\npeft==0.12.0\r\npillow==10.4.0\r\nportalocker==2.10.1\r\nprotobuf==5.28.0\r\npsutil==6.0.0\r\npyarrow==17.0.0\r\npybind11==2.13.5\r\npycryptodomex==3.20.0\r\npydantic==2.8.2\r\npydantic_core==2.20.1\r\npydeck==0.9.1\r\nPygments==2.18.0\r\npytablewriter==1.2.0\r\npython-dateutil==2.9.0.post0\r\npytz==2024.1\r\nPyYAML==6.0.2\r\nreferencing==0.35.1\r\nregex==2024.7.24\r\nrequests==2.32.3\r\nrich==13.8.0\r\nrouge_score==0.1.2\r\nrpds-py==0.20.0\r\nsacrebleu==2.4.3\r\nsafetensors==0.4.4\r\nscikit-learn==1.5.1\r\nscipy==1.14.1\r\nsentencepiece==0.2.0\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsnakeviz==2.2.0\r\nsniffio==1.3.1\r\nsqlitedict==2.1.0\r\nstreamlit==1.38.0\r\nsympy==1.13.1\r\ntabledata==1.3.3\r\ntabulate==0.9.0\r\ntcolorpy==0.1.6\r\ntenacity==8.5.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.7.0\r\ntimm==1.0.7\r\ntokenizers==0.19.1\r\ntoml==0.10.2\r\ntomli==2.0.1\r\ntorch==2.5.0.dev20240716\r\ntorchao @ git+https://github.com/pytorch/ao.git@e11201a62669f582d81cdb33e031a07fb8dfc4f3\r\ntorchaudio==2.4.0.dev20240716\r\ntorchsr==1.0.4\r\ntorchvision==0.20.0.dev20240716\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntqdm-multiprocess==0.0.11\r\ntransformers==4.42.4\r\ntypepy==1.3.2\r\ntyping_extensions==4.12.2\r\ntzdata==2024.1\r\nurllib3==2.2.2\r\nWerkzeug==3.0.4\r\nword2number==1.1\r\nxxhash==3.5.0\r\nyarl==1.9.7\r\nzstandard==0.23.0\r\nzstd==1.5.5.1\r\n\r\nPyTorch Version\r\n2.5.0.dev20240716",
      "state": "closed",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-09-04T16:21:51Z",
      "updated_at": "2024-09-04T17:57:47Z",
      "closed_at": "2024-09-04T17:57:47Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1106/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1106",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1106",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:32.694918",
      "comments": [
        {
          "author": "byjlw",
          "body": "installed by finding the buck2 executable in the executorch/cmake-out directory and running buck2 kill. \r\nDealing with a new issue now though. #1107",
          "created_at": "2024-09-04T17:57:47Z"
        }
      ]
    },
    {
      "issue_number": 1101,
      "title": "should have the ability to output debug artifacts when exporting to a .pte file",
      "body": "### 🚀 The feature, motivation and pitch\n\nshould be able to use --generate_etrecord to get the artifacts for debugging like this. \r\n`python3 torchchat.py export llama3.1 --quantize torchchat/quant_config/mobile.json --output-pte-path llama3.1.pte --generate_etrecord`\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-09-03T21:23:04Z",
      "updated_at": "2024-09-04T17:04:47Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "ExecuTorch"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1101/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1101",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1101",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:32.929280",
      "comments": []
    },
    {
      "issue_number": 187,
      "title": "[Feature request] Stop Executorch dump of graph, unless there some debug flag",
      "body": "\r\nThe graph is basically spam that can't be analyzed by the users and typically so long that even if you want to analyze it, you can't scoll back to a place of interest.  Create an option (say ET_LOG, or reuse TORCHLOGS which is the new way of controlling torch logging for all parts of pytorch, including torch.export, giving our users a single interface.)\r\n\r\nhttps://github.com/pytorch/torchat/actions/runs/8681411543/job/23803982796?pr=173",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-14T23:32:38Z",
      "updated_at": "2024-08-30T15:13:38Z",
      "closed_at": "2024-08-30T15:13:38Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/187/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "byjlw",
        "iseeyuan"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/187",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/187",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:34.731727",
      "comments": [
        {
          "author": "orionr",
          "body": "@iseeyuan @byjlw @larryliu0820 any thoughts on this one? Thanks",
          "created_at": "2024-04-15T13:54:16Z"
        },
        {
          "author": "mikekgfb",
          "body": "may already be resolved by setting logger level, currently dumped using logger.info()",
          "created_at": "2024-04-25T08:06:27Z"
        },
        {
          "author": "byjlw",
          "body": "This is resolved. ",
          "created_at": "2024-08-30T15:13:38Z"
        }
      ]
    },
    {
      "issue_number": 1087,
      "title": "[Distributed Inference] running under .inference_mode() = assert Dtensor NotImplementedError: Operator aten.matmul.default does not have a sharding strategy registered.",
      "body": "### 🐛 Describe the bug\n\nrun dist inference under .inference_mode():\r\nresults in:\r\n~~~\r\n[rank1]:[rank1]: Traceback (most recent call last):\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/pipelining/stage.py\", line 530, in forward_one_chunk\r\n[rank1]:[rank1]:     output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/pipelining/stage.py\", line 464, in forward_maybe_with_nosync\r\n[rank1]:[rank1]:     out_val = self.submod(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n[rank1]:[rank1]:     return self._call_impl(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n[rank1]:[rank1]:     return forward_call(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/data/users/less/local/torchchat/build/model_dist.py\", line 109, in forward\r\n[rank1]:[rank1]:     x = layer(x, input_pos, freqs_cis, mask)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n[rank1]:[rank1]:     return self._call_impl(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n[rank1]:[rank1]:     return forward_call(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/data/users/less/local/torchchat/build/model_dist.py\", line 152, in forward\r\n[rank1]:[rank1]:     h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n[rank1]:[rank1]:     return self._call_impl(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n[rank1]:[rank1]:     return forward_call(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/data/users/less/local/torchchat/build/model_dist.py\", line 234, in forward\r\n[rank1]:[rank1]:     q: DTensor = self.wq(x)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n[rank1]:[rank1]:     return self._call_impl(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1788, in _call_impl\r\n[rank1]:[rank1]:     result = forward_call(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\r\n[rank1]:[rank1]:     return F.linear(input, self.weight, self.bias)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\r\n[rank1]:[rank1]:     return disable_fn(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 636, in _fn\r\n[rank1]:[rank1]:     return fn(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/api.py\", line 343, in __torch_dispatch__\r\n[rank1]:[rank1]:     return DTensor._op_dispatcher.dispatch(\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py\", line 121, in dispatch\r\n[rank1]:[rank1]:     return self._custom_op_handlers[op_call](op_call, args, kwargs)  # type: ignore[operator]\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py\", line 52, in decompose_handler\r\n[rank1]:[rank1]:     r = op_call.decompose(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/_ops.py\", line 756, in decompose\r\n[rank1]:[rank1]:     return self._op_dk(dk, *args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\r\n[rank1]:[rank1]:     return disable_fn(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 636, in _fn\r\n[rank1]:[rank1]:     return fn(*args, **kwargs)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/api.py\", line 343, in __torch_dispatch__\r\n[rank1]:[rank1]:     return DTensor._op_dispatcher.dispatch(\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py\", line 127, in dispatch\r\n[rank1]:[rank1]:     self.sharding_propagator.propagate(op_info)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/_sharding_prop.py\", line 186, in propagate\r\n[rank1]:[rank1]:     output_sharding = self.propagate_op_sharding(op_info.schema)\r\n[rank1]:[rank1]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/_sharding_prop.py\", line 434, in propagate_op_sharding_non_cached\r\n[rank1]:[rank1]:     raise NotImplementedError(\r\n[rank1]:[rank1]: NotImplementedError: Operator aten.matmul.default does not have a sharding strategy registered.\r\n~~~\r\n\r\n\n\n### Versions\n\nN/A",
      "state": "closed",
      "author": "lessw2020",
      "author_type": "User",
      "created_at": "2024-08-29T03:23:17Z",
      "updated_at": "2024-08-30T05:45:24Z",
      "closed_at": "2024-08-30T05:45:24Z",
      "labels": [
        "Distributed"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1087/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1087",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1087",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:34.986475",
      "comments": [
        {
          "author": "lessw2020",
          "body": "Note - running under torch.no_grad() works fine. ",
          "created_at": "2024-08-29T03:23:37Z"
        }
      ]
    },
    {
      "issue_number": 80,
      "title": "[Feature request] Delete executorch_portable_utils.py - this should be installed by executorch setup",
      "body": "x-ref: ET iusability issue\r\nhttps://github.com/pytorch/executorch/issues/2909\r\n\r\n@byjlw @metascroy ",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-07T07:08:25Z",
      "updated_at": "2024-08-29T21:49:21Z",
      "closed_at": "2024-08-29T21:49:20Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/80/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dbort",
        "metascroy"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/80",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/80",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:35.200389",
      "comments": []
    },
    {
      "issue_number": 1086,
      "title": "[Distributed Inference] moving stage.submod to non-fp32 (bf16, fp16) results in dtensor assert \"self.mask_buffer.data is not None\"",
      "body": "### 🐛 Describe the bug\n\nUsing our prototype parallel blocks for built in distributed, we can run tp + pp in fp32 successfully. \r\nHowever, moving the model to bfloat16 or fp32 results in an embedding assert:\r\n\r\n~~~\r\n[rank0]:[rank0]: Traceback (most recent call last):\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/pipelining/stage.py\", line 530, in forward_one_chunk\r\n[rank0]:[rank0]:     output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/pipelining/stage.py\", line 464, in forward_maybe_with_nosync\r\n[rank0]:[rank0]:     out_val = self.submod(*args, **kwargs)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n[rank0]:[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:[rank0]:   File \"/data/users/less/local/torchchat/build/model_dist.py\", line 105, in forward\r\n[rank0]:[rank0]:     x: DTensor = self.tok_embeddings(x)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1801, in _call_impl\r\n[rank0]:[rank0]:     hook_result = hook(self, args, result)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/api.py\", line 895, in <lambda>\r\n[rank0]:[rank0]:     lambda mod, inputs, outputs: output_fn(mod, outputs, device_mesh)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/tensor/parallel/style.py\", line 251, in _prepare_output_fn\r\n[rank0]:[rank0]:     outputs = outputs.redistribute(placements=output_layouts, async_op=True)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/api.py\", line 541, in redistribute\r\n[rank0]:[rank0]:     return Redistribute.apply(self, device_mesh, placements, async_op)\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\r\n[rank0]:[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/_redistribute.py\", line 295, in forward\r\n[rank0]:[rank0]:     output = redistribute_local_tensor(\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/_redistribute.py\", line 196, in redistribute_local_tensor\r\n[rank0]:[rank0]:     new_local_tensor = partial_spec._reduce_value(\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/ops/_embedding_ops.py\", line 119, in _reduce_value\r\n[rank0]:[rank0]:     assert self.mask_buffer.data is not None\r\n[rank0]:[rank0]: AssertionError\r\n~~~\r\nThis issue is to track the debugging and resolution. \n\n### Versions\n\nN/A",
      "state": "closed",
      "author": "lessw2020",
      "author_type": "User",
      "created_at": "2024-08-29T03:13:55Z",
      "updated_at": "2024-08-29T18:58:15Z",
      "closed_at": "2024-08-29T18:58:15Z",
      "labels": [
        "Distributed"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1086/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lessw2020"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1086",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1086",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:35.200413",
      "comments": [
        {
          "author": "kwen2501",
          "body": "Looking at the error stack:\r\n```\r\n[rank0]:[rank0]:   File \"/home/less/local/miniconda3/envs/newserver/lib/python3.10/site-packages/torch/distributed/_tensor/ops/_embedding_ops.py\", line 119, in _reduce_value\r\n[rank0]:[rank0]:     assert self.mask_buffer.data is not None\r\n[rank0]:[rank0]: AssertionEr",
          "created_at": "2024-08-29T16:19:28Z"
        },
        {
          "author": "kwen2501",
          "body": "```\r\nassert self.mask_buffer.data is not None\r\n```\r\nIs this assert complaining that the weight (of embedding) is not loaded?",
          "created_at": "2024-08-29T16:20:40Z"
        },
        {
          "author": "lessw2020",
          "body": "I've found that if we set the activations we pass in for creating the pipelineStage are the same as the 'future' dtype we will run with the model, then I am able to run the model with real weights in the proper dtype. (in this case fp16, so we have to pass in fp16 sample activations and then things ",
          "created_at": "2024-08-29T17:29:39Z"
        },
        {
          "author": "lessw2020",
          "body": "I've further updated things to now store the checkpoint dtype in the internal lookup table and then we adjust accordingly. \r\nThus things now work as expected and at the same time the user does not have to provide/care about this...it just works. \r\nThis is ultimately a limitation of the built in para",
          "created_at": "2024-08-29T18:58:15Z"
        }
      ]
    },
    {
      "issue_number": 1069,
      "title": "download for 3.1 is broken",
      "body": "### 🐛 Describe the bug\n\nThe following commands will throw an exception. \r\n`python3 torchchat.py generate llama3.1 --prompt \"write me a story about a boy and his bear\"`\r\n`python3 torchchat.py download llama3.1`\r\n\r\n```\r\nonverting meta-llama/Meta-Llama-3.1-8B-Instruct to torchchat format...\r\nTraceback (most recent call last):\r\n  File \"/Users/jessewhite/Documents/source/torchchat/torchchat.py\", line 97, in <module>\r\n    download_main(args)\r\n  File \"/Users/jessewhite/Documents/source/torchchat/download.py\", line 198, in download_main\r\n    download_and_convert(args.model, args.model_directory, args.hf_token)\r\n  File \"/Users/jessewhite/Documents/source/torchchat/download.py\", line 91, in download_and_convert\r\n    _download_hf_snapshot(model_config, temp_dir, hf_token)\r\n  File \"/Users/jessewhite/Documents/source/torchchat/download.py\", line 55, in _download_hf_snapshot\r\n    convert_hf_checkpoint(\r\n  File \"/Users/jessewhite/Documents/source/torchchat/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jessewhite/Documents/source/torchchat/build/convert_hf_checkpoint.py\", line 35, in convert_hf_checkpoint\r\n    config = TransformerArgs.from_name(model_name)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: type object 'TransformerArgs' has no attribute 'from_name'\r\n\r\n```\n\n### Versions\n\n\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240814\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.1 (arm64)\r\nGCC version: Could not collect\r\nClang version: 15.0.0 (clang-1500.1.0.2.5)\r\nCMake version: version 3.30.2\r\nLibc version: N/A\r\n\r\nPython version: 3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang-1300.0.29.30)] (64-bit runtime)\r\nPython platform: macOS-14.1-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M3 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240814\r\n[pip3] torchao==0.4.0+gite11201a\r\n[conda] Could not collect",
      "state": "closed",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-08-28T05:03:36Z",
      "updated_at": "2024-08-28T06:43:57Z",
      "closed_at": "2024-08-28T06:43:57Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1069/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Gasoonjia"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1069",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1069",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:35.513420",
      "comments": [
        {
          "author": "Gasoonjia",
          "body": "I recently merged a PR that related to TransformerArgs. Let me check",
          "created_at": "2024-08-28T05:11:28Z"
        },
        {
          "author": "Gasoonjia",
          "body": "https://github.com/pytorch/torchchat/pull/1071 this PR solves the issue. @byjlw please take a look if have time!",
          "created_at": "2024-08-28T05:57:48Z"
        }
      ]
    },
    {
      "issue_number": 1066,
      "title": "Slow eval performance for .pte models",
      "body": "### 🐛 Describe the bug\n\nEval is very slow for PTE models vs. non-exported models - the opposite should be true and can be observed in generate. I suspect this has to do with some improper setup of the KV cache or prefill in the eval script. \r\n\r\nI landed https://github.com/pytorch/torchchat/pull/1053 to implement sequential prefill so that .pte files would complete the eval script successfully. We might be able to resolve this issue by porting the parallel prefill implementation from ExecuTorch.\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240716\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.6.1 (arm64)\r\nGCC version: Could not collect\r\nClang version: 15.0.0 (clang-1500.3.9.4)\r\nCMake version: version 3.30.2\r\nLibc version: N/A\r\n\r\nPython version: 3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)] (64-bit runtime)\r\nPython platform: macOS-14.6.1-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M3 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+9129892\r\n[pip3] flake8==6.0.0\r\n[pip3] flake8-breakpoint==1.1.0\r\n[pip3] flake8-bugbear==23.6.5\r\n[pip3] flake8-comprehensions==3.12.0\r\n[pip3] flake8-plugin-utils==1.3.3\r\n[pip3] flake8-pyi==23.5.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240716\r\n[pip3] torchao==0.4.0+gite11201a\r\n[pip3] torchaudio==2.4.0.dev20240716\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchvision==0.20.0.",
      "state": "open",
      "author": "vmpuri",
      "author_type": "User",
      "created_at": "2024-08-27T17:34:54Z",
      "updated_at": "2024-08-27T17:34:55Z",
      "closed_at": null,
      "labels": [
        "performance",
        "actionable",
        "ExecuTorch"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1066/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1066",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1066",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:35.759418",
      "comments": []
    },
    {
      "issue_number": 1040,
      "title": "support load model locally",
      "body": "### 🚀 The feature, motivation and pitch\n\nI have downloaded the llama3-8B model from huggingface into local dir as below\r\n![image](https://github.com/user-attachments/assets/e9fb5e17-beef-4e78-a075-cfd530e9f24b)\r\n\r\nBut I can't run `python3 torchchat.py generate --checkpoint_dir /home/Meta-Llama-3-8B-Instruct --prompt \"It was a dark and stormy night, and\"`  since it's not a single pt file.\r\nHow should we run the pre-downloaded model in torchchat?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "closed",
      "author": "irasin",
      "author_type": "User",
      "created_at": "2024-08-19T07:36:53Z",
      "updated_at": "2024-08-20T09:38:42Z",
      "closed_at": "2024-08-20T06:13:24Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1040/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1040",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1040",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:35.759439",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Can you try to generate a .pth with this: https://github.com/pytorch/torchchat/blob/1566512baa889ef044231595b593749231ab0170/.ci/scripts/convert_checkpoint.sh",
          "created_at": "2024-08-19T16:57:36Z"
        },
        {
          "author": "irasin",
          "body": "Thanks a lot.\r\nFollow the code, I found that only \"*.bin\" files are supported in torchchat, so I have to modify the code to support \"*.safetensors\", and I can run the local model now.",
          "created_at": "2024-08-20T06:13:24Z"
        },
        {
          "author": "sunshinesfbay",
          "body": "@irasin would you be able to submit a PR for docs/ADVANCED-USERS.md how to recognize and navigate this issue? Also any other issues/errors/unclear instructions you encountered?",
          "created_at": "2024-08-20T09:38:41Z"
        }
      ]
    },
    {
      "issue_number": 1022,
      "title": "Eval script fails on CPU on model generated by ExecuTorch",
      "body": "### 🐛 Describe the bug\n\nI am using ET and generating the quantized version of the model as shown in the README.\r\n\r\n```\r\npython torchchat.py export llama3.1 --quantize config/data/mobile.json --output-pte-path llama3.1.pte\r\n```\r\n\r\nThen we when I tried to evaluate the model using the python runtime on Desktop , it fails\r\n\r\n```\r\npython torchchat.py eval llama3.1 --pte-path llama3.1.pte --limit 5\r\nNumExpr defaulting to 16 threads.\r\nPyTorch version 2.5.0.dev20240716+cpu available.\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nUsing device=cpu\r\nLoading model...\r\nTime to load model: 0.05 seconds\r\nLoading custom ops library: /home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/executorch/examples/models/llama2/custom_ops/libcustom_ops_aot_lib.so\r\nI 00:00:00.004209 executorch:program.cpp:133] InternalConsistency verification requested but not available\r\n-----------------------------------------------------------\r\nUsing device 'cpu'\r\n/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\r\n[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\r\n[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\r\n[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\r\n[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\r\n[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\r\nDownloading builder script: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10.7k/10.7k [00:00<00:00, 46.8MB/s]\r\nDownloading readme: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.78k/7.78k [00:00<00:00, 39.3MB/s]\r\nRepo card metadata block was not found. Setting CardData to empty.\r\nRepo card metadata block was not found. Setting CardData to empty.\r\nDownloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.72M/4.72M [00:00<00:00, 64.4MB/s]\r\nGenerating test split: 62 examples [00:00, 1903.53 examples/s]\r\nGenerating train split: 629 examples [00:00, 5131.04 examples/s]\r\nGenerating validation split: 60 examples [00:00, 7172.82 examples/s]\r\nBuilding contexts for wikitext on rank 0...\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 796.73it/s]\r\nRunning loglikelihood_rolling requests\r\n  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]E 00:00:31.679303 executorch:tensor_impl.cpp:86] Attempted to resize a static tensor to a new shape at dimension 1 old_size: 1 new_size: 1263\r\nE 00:00:31.679320 executorch:method.cpp:824] Error setting input 0: 0x10\r\n  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\r\nTime to run eval: 6.75s.\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/torchchat/torchchat.py\", line 92, in <module>\r\n    eval_main(args)\r\n  File \"/home/ubuntu/torchchat/eval.py\", line 252, in main\r\n    result = eval(\r\n  File \"/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/torchchat/eval.py\", line 198, in eval\r\n    eval_results = evaluate(\r\n  File \"/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/lm_eval/utils.py\", line 288, in _wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/lm_eval/evaluator.py\", line 373, in evaluate\r\n    resps = getattr(lm, reqtype)(cloned_reqs)\r\n  File \"/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/lm_eval/models/huggingface.py\", line 840, in loglikelihood_rolling\r\n    string_nll = self._loglikelihood_tokens(\r\n  File \"/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/lm_eval/models/huggingface.py\", line 1033, in _loglikelihood_tokens\r\n    self._model_call(batched_inps, **call_kwargs), dim=-1\r\n  File \"/home/ubuntu/torchchat/eval.py\", line 146, in _model_call\r\n    logits = self._model_forward(x, input_pos)\r\n  File \"/home/ubuntu/torchchat/eval.py\", line 240, in <lambda>\r\n    model_forward = lambda x, input_pos: model(x, input_pos)  # noqa\r\n  File \"/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1716, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/torchchat/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1727, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/torchchat/build/model_et.py\", line 23, in forward\r\n    logits = self.model_.forward(forward_inputs)\r\nRuntimeError: method->set_inputs() for method 'forward' failed with error 0x12\r\n```\n\n### Versions\n\n```\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240716+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1014-aws-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             16\r\nOn-line CPU(s) list:                0-15\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\r\nCPU family:                         6\r\nModel:                              106\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 8\r\nSocket(s):                          1\r\nStepping:                           6\r\nBogoMIPS:                           5799.93\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd ida arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear flush_l1d arch_capabilities\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          384 KiB (8 instances)\r\nL1i cache:                          256 KiB (8 instances)\r\nL2 cache:                           10 MiB (8 instances)\r\nL3 cache:                           54 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-15\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+c757499\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240716+cpu\r\n[pip3] torchao==0.3.1\r\n[pip3] torchaudio==2.4.0.dev20240716+cpu\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchvision==0.20.0.dev20240716+cpu\r\n[conda] executorch                0.4.0a0+c757499          pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.5.0.dev20240716+cpu          pypi_0    pypi\r\n[conda] torchao                   0.3.1                    pypi_0    pypi\r\n[conda] torchaudio                2.4.0.dev20240716+cpu          pypi_0    pypi\r\n[conda] torchsr                   1.0.4                    pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20240716+cpu          pypi_0    pypi\r\n(torchchat) ubuntu@ip-172-31-7-68:~/torchchat$ \r\n\r\n```",
      "state": "closed",
      "author": "agunapal",
      "author_type": "User",
      "created_at": "2024-08-08T02:32:20Z",
      "updated_at": "2024-08-19T18:34:26Z",
      "closed_at": "2024-08-19T18:34:26Z",
      "labels": [
        "duplicate",
        "Known Gaps",
        "ExecuTorch"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1022/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1022",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1022",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:36.013010",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Thanks for flagging @agunapal \r\n\r\nThis is a known issue that we'll be looking at https://github.com/pytorch/torchchat/issues/938\r\nSince eval is using the same concept as generate for evaluation, most likely there's a wiring bug",
          "created_at": "2024-08-08T15:56:05Z"
        },
        {
          "author": "agunapal",
          "body": "Interestingly, generate works for me\r\n```\r\n python torchchat.py generate llama3.1 --device cpu --pte-path llama3.1.pte --prompt \"Hello my name is\"\r\nNumExpr defaulting to 16 threads.\r\nPyTorch version 2.5.0.dev20240716+cpu available.\r\nWarning: checkpoint path ignored because an exported DSO or PTE pat",
          "created_at": "2024-08-08T17:41:24Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Thanks again for flagging. We have @vmpuri looking at this\r\n\r\nTracking in umbrella issue",
          "created_at": "2024-08-19T18:34:26Z"
        }
      ]
    },
    {
      "issue_number": 1017,
      "title": "ImportError: cannot import name 'int4_weight_only' from 'torchao.quantization.quant_api' ",
      "body": "### 🐛 Describe the bug\r\n\r\nSample code to reproduce the problem:\r\n\r\n```python\r\npython torchchat.py chat llama3\r\n```\r\n\r\nFull traceback:\r\n\r\n```\r\nNumExpr defaulting to 6 threads.\r\nPyTorch version 2.4.0+cu121 available.\r\nTraceback (most recent call last):\r\n  File \"E:\\gitrepos\\torchchat\\torchchat.py\", line 67, in <module>\r\n    from generate import main as generate_main\r\n  File \"E:\\gitrepos\\torchchat\\generate.py\", line 20, in <module>\r\n    from build.builder import (\r\n  File \"E:\\gitrepos\\torchchat\\build\\builder.py\", line 22, in <module>\r\n    from quantization.quantize import quantize_model\r\n  File \"E:\\gitrepos\\torchchat\\quantization\\quantize.py\", line 40, in <module>\r\n    from torchao.quantization.quant_api import (\r\nImportError: cannot import name 'int4_weight_only' from 'torchao.quantization.quant_api'  (E:\\gitrepos\\torchchat\\.venv\\lib\\site-packages\\torchao\\quantization\\quant_api.py)\r\n```\r\n\r\n### Versions\r\n\r\nEnvironment:\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: N/A\r\n\r\nPython version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)\r\nPython platform: Windows-10-10.0.19045-SP0\r\nIs CUDA available: True\r\nCUDA runtime version: 11.1.74\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti\r\nNvidia driver version: 555.85\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture=9\r\nCurrentClockSpeed=3696\r\nDeviceID=CPU0\r\nFamily=205\r\nL2CacheSize=1536\r\nL2CacheSpeed=\r\nManufacturer=GenuineIntel\r\nMaxClockSpeed=3696\r\nName=Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz\r\nProcessorType=3\r\nRevision=\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.4.0+cu121\r\n[pip3] torchao==0.1\r\n[pip3] torchaudio==2.4.0+cu121\r\n[pip3] torchvision==0.19.0+cu121\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               11.3.1               h59b6b97_2\r\n[conda] mkl                       2021.4.0                 pypi_0    pypi\r\n[conda] mkl-service               2.3.0            py38hb782905_0\r\n[conda] mkl_fft                   1.2.0            py38h45dec08_0\r\n[conda] mkl_random                1.1.1            py38h47e9c7a_0\r\n[conda] numpy                     1.24.4                   pypi_0    pypi\r\n[conda] numpydoc                  1.1.0              pyhd3eb1b0_1\r\n[conda] pytorch-lightning         2.2.4                    pypi_0    pypi\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torch                     2.3.0+cu121              pypi_0    pypi\r\n[conda] torch-fidelity            0.3.0                    pypi_0    pypi\r\n[conda] torchaudio                2.3.0+cu121              pypi_0    pypi\r\n[conda] torchinfo                 1.8.0                    pypi_0    pypi\r\n[conda] torchmetrics              0.11.4                   pypi_0    pypi\r\n[conda] torchnet                  0.0.4                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.18.0+cu121             pypi_0    pypi\r\n[conda] torchviz                  0.0.2                    pypi_0    pypi\r\n```\r\n\r\nTried installing `torchao` using `python -m pip install torchao`. But the error still persists.",
      "state": "closed",
      "author": "sadimanna",
      "author_type": "User",
      "created_at": "2024-08-07T11:08:47Z",
      "updated_at": "2024-08-19T18:29:40Z",
      "closed_at": "2024-08-19T18:29:39Z",
      "labels": [
        "bug",
        "Quantization"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1017/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1017",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1017",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:36.266322",
      "comments": [
        {
          "author": "byjlw",
          "body": "To verify. Are you running in a venv? Can you run pull and run `./install_requirements.sh` again. \r\n`",
          "created_at": "2024-08-07T17:09:03Z"
        },
        {
          "author": "sadimanna",
          "body": "@byjlw Yes, I am running in a venv as instructed in the readme file. I will try what you suggested.",
          "created_at": "2024-08-07T17:51:37Z"
        },
        {
          "author": "byjlw",
          "body": "Great let me know how it goes. \r\nWe are actively landing changes in core and AO (using pins on nightly) to improve the performance and capability of torchchat so I recommend you run `./install_requirements.sh` everytime you pull to ensure you're bringing in the latest changes from those libraries. ",
          "created_at": "2024-08-07T18:05:39Z"
        },
        {
          "author": "msaroufim",
          "body": "@sadimanna you have a very old version of torchao installed, we published 0.4 so make sure to upgrade to that \r\n\r\n`pip install torchao --force-reinstall`",
          "created_at": "2024-08-07T23:28:32Z"
        },
        {
          "author": "sadimanna",
          "body": "@msaroufim using your command, it starts installing torch-2.4.0 and numpy-2.0.1 but is not upgrading torchao to 0.4",
          "created_at": "2024-08-08T05:54:03Z"
        }
      ]
    },
    {
      "issue_number": 1009,
      "title": "[Feature request] Langchain Support - Chat Model",
      "body": "### 🚀 The feature, motivation and pitch\r\n\r\nRequesting to support Langchain by creating a [ChatModel](https://python.langchain.com/v0.2/docs/integrations/chat/) for Langchain. Since Torchchat already supports  OpenAI compatible chat apis they should be very similar to chat models like ChatOllama, ChatLlamaCpp. \r\n\r\nI know this has to be done in Langchain's [repository](https://github.com/langchain-ai/langchain) and I created a discussion [there](https://github.com/langchain-ai/langchain/discussions/25034) but it would be great if someone knowledgeable in both Langchain and TorchChat is willing to take initiative on it. \r\n",
      "state": "open",
      "author": "raymon-io",
      "author_type": "User",
      "created_at": "2024-08-04T13:22:25Z",
      "updated_at": "2024-08-19T18:26:06Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1009/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1009",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1009",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:36.588292",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Good call out, with the recent OpenAI API polished in TC, this can be an easy win \r\n\r\ncc: @vmpuri ",
          "created_at": "2024-08-19T18:26:05Z"
        }
      ]
    },
    {
      "issue_number": 1038,
      "title": "How to deploy a new model by torchchat?",
      "body": "I want to use torchchat to load the trained model directly from the local. How to change the torchchat/config/data/models.json? Need to change download _ and _ convert in download.py?And, what other documents may need to be changed?",
      "state": "open",
      "author": "liu8060",
      "author_type": "User",
      "created_at": "2024-08-16T09:33:29Z",
      "updated_at": "2024-08-19T18:24:37Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1038/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1038",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1038",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:36.807189",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Glad to have you try things out\r\n\r\nWhat file format is the local model you're working with?",
          "created_at": "2024-08-16T20:02:35Z"
        },
        {
          "author": "liu8060",
          "body": "two formats\r\nOne is a folder format, which includes the model’s weight files, tokenizer files, and other necessary components. \r\nThe other format is a PyTorch .pth file, which contains the entire model.\r\n\r\nThe model's base model is llama3-8b",
          "created_at": "2024-08-19T01:42:12Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "If the model is accessible from huggingface: Here's an example PR of how you can add it\r\n\r\nhttps://github.com/pytorch/torchchat/pull/947 Specifically the known_model_configs and model.json\r\n\r\n\r\n> The other format is a PyTorch .pth file, which contains the entire model.\r\n\r\nFor this you can can add a ",
          "created_at": "2024-08-19T16:52:16Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "> One is a folder format, which includes the model’s weight files, tokenizer files, and other necessary components.\r\n\r\nSimilar to this case? https://github.com/pytorch/torchchat/issues/1040\r\n\r\n ",
          "created_at": "2024-08-19T17:00:16Z"
        }
      ]
    },
    {
      "issue_number": 963,
      "title": "Can't install requirements when using Python-3.12",
      "body": "### 🐛 Describe the bug\r\n\r\n`pip install` fails because it could not find `torch` even though, it's present in the environment:\r\n```\r\n% pip install git+https://github.com/pytorch/ao.git@d36de1b144b73bf753bd082109c2b5d0141abd5b \r\nCollecting git+https://github.com/pytorch/ao.git@d36de1b144b73bf753bd082109c2b5d0141abd5b\r\n  Cloning https://github.com/pytorch/ao.git (to revision d36de1b144b73bf753bd082109c2b5d0141abd5b) to /private/var/folders/rk/fxg20zvx6vvb5bk7cplq4xrc0000gn/T/pip-req-build-r05jkt6n\r\n  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/ao.git /private/var/folders/rk/fxg20zvx6vvb5bk7cplq4xrc0000gn/T/pip-req-build-r05jkt6n\r\n  Running command git rev-parse -q --verify 'sha^d36de1b144b73bf753bd082109c2b5d0141abd5b'\r\n  Running command git fetch -q https://github.com/pytorch/ao.git d36de1b144b73bf753bd082109c2b5d0141abd5b\r\n  Running command git checkout -q d36de1b144b73bf753bd082109c2b5d0141abd5b\r\n  Resolved https://github.com/pytorch/ao.git to commit d36de1b144b73bf753bd082109c2b5d0141abd5b\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × Getting requirements to build wheel did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [20 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"/Users/nshulga/py3.12-torch2.4/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"/Users/nshulga/py3.12-torch2.4/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/Users/nshulga/py3.12-torch2.4/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\r\n          return hook(config_settings)\r\n                 ^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/private/var/folders/rk/fxg20zvx6vvb5bk7cplq4xrc0000gn/T/pip-build-env-x_bm22ut/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 327, in get_requires_for_build_wheel\r\n          return self._get_build_requires(config_settings, requirements=[])\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/private/var/folders/rk/fxg20zvx6vvb5bk7cplq4xrc0000gn/T/pip-build-env-x_bm22ut/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 297, in _get_build_requires\r\n          self.run_setup()\r\n        File \"/private/var/folders/rk/fxg20zvx6vvb5bk7cplq4xrc0000gn/T/pip-build-env-x_bm22ut/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 497, in run_setup\r\n          super().run_setup(setup_script=setup_script)\r\n        File \"/private/var/folders/rk/fxg20zvx6vvb5bk7cplq4xrc0000gn/T/pip-build-env-x_bm22ut/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 313, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 31, in <module>\r\n      ModuleNotFoundError: No module named 'torch'\r\n      [end of output]\r\n```\r\n### Versions\r\n\r\nCI",
      "state": "open",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-07-29T20:48:28Z",
      "updated_at": "2024-08-07T23:27:16Z",
      "closed_at": null,
      "labels": [
        "Known Gaps",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/963/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/963",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/963",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:37.029573",
      "comments": [
        {
          "author": "malfet",
          "body": "But I found a workaround\r\n```\r\npip install setuptools wheel\r\npip install --no-use-pep517 git+https://github.com/pytorch/ao.git@d36de1b144b73bf753bd082109c2b5d0141abd5b\r\n```",
          "created_at": "2024-07-29T20:57:16Z"
        },
        {
          "author": "larryliu0820",
          "body": "@msaroufim I wanted to raise this issue earlier but it slipped - what is the best way to install ao nightly on mac? It doesn't look like there's a nightly wheel under this URL: https://download.pytorch.org/whl/nightly/torchao-nightly/, that's why we have to pull from git and build from source. This ",
          "created_at": "2024-07-29T21:31:15Z"
        },
        {
          "author": "msaroufim",
          "body": "We don't have mac binaries right now but it's something we could use help with, we leverage some workflows from dev infra and those are a bit opaque to me https://github.com/pytorch/ao/blob/main/.github/workflows/build_wheels_linux.yml",
          "created_at": "2024-08-02T02:14:26Z"
        },
        {
          "author": "msaroufim",
          "body": "Update So we now have m1 support for our release binaries, as of today this 0.4 which I pushed manually. Not planning on adding nightly support until we figure out automation",
          "created_at": "2024-08-07T23:27:16Z"
        }
      ]
    },
    {
      "issue_number": 990,
      "title": "Failed to build wheel for executorch. Failed to stat buck-out/v2",
      "body": "### 🐛 Describe the bug\n\nWhile running the command:\r\n\r\n`./scripts/install_et.sh`\r\n\r\n\r\nI get:\r\n\r\n```\r\n  -- Using python executable '/usr/bin/python3'\r\n  -- Resolved buck2 as /mnt/d/TorchChat/et-build/src/executorch/pip-out/temp.linux-x86_64-cpython-310/cmake-out/buck2-bin/buck2-3bbde7daa94987db468d021ad625bc93dc62ba7fcb16945cb09b64aab077f284.\r\n  -- Killing buck2 daemon\r\n  -- executorch: Generating source lists\r\n  -- executorch: Generating source file list /mnt/d/TorchChat/et-build/src/executorch/pip-out/temp.linux-x86_64-cpython-310/cmake-out/executorch_srcs.cmake\r\n  Error while generating /mnt/d/TorchChat/et-build/src/executorch/pip-out/temp.linux-x86_64-cpython-310/cmake-out/executorch_srcs.cmake. Exit code: 1\r\n  Output:\r\n\r\n  Error:\r\n  Traceback (most recent call last):\r\n    File \"/mnt/d/TorchChat/et-build/src/executorch/build/buck_util.py\", line 26, in run\r\n      cp: subprocess.CompletedProcess = subprocess.run(\r\n    File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\r\n      raise CalledProcessError(retcode, process.args,\r\n  subprocess.CalledProcessError: Command '['/mnt/d/TorchChat/et-build/src/executorch/pip-out/temp.linux-x86_64-cpython-310/cmake-out/buck2-bin/buck2-3bbde7daa94987db468d021ad625bc93dc62ba7fcb16945cb09b64aab077f284', 'cquery', \"inputs(deps('//runtime/executor:program'))\"]' returned non-zero exit status 2.\r\n\r\n  The above exception was the direct cause of the following exception:\r\n\r\n  Traceback (most recent call last):\r\n    File \"/mnt/d/TorchChat/et-build/src/executorch/build/extract_sources.py\", line 218, in <module>\r\n      main()\r\n    File \"/mnt/d/TorchChat/et-build/src/executorch/build/extract_sources.py\", line 203, in main\r\n      target_to_srcs[name] = sorted(target.get_sources(graph, runner))\r\n    File \"/mnt/d/TorchChat/et-build/src/executorch/build/extract_sources.py\", line 116, in get_sources\r\n      sources: set[str] = set(runner.run([\"cquery\", query]))\r\n    File \"/mnt/d/TorchChat/et-build/src/executorch/build/buck_util.py\", line 31, in run\r\n      raise RuntimeError(ex.stderr.decode(\"utf-8\")) from ex\r\n  RuntimeError: Command failed:\r\n  Error validating working directory\r\n\r\n  Caused by:\r\n      0: Failed to stat `/mnt/d/TorchChat/et-build/src/executorch/buck-out/v2`\r\n      1: ENOENT: No such file or directory\r\n\r\n\r\n  CMake Error at build/Utils.cmake:191 (message):\r\n    executorch: source list generation failed\r\n  Call Stack (most recent call first):\r\n    CMakeLists.txt:327 (extract_sources)\r\n\r\n\r\n  -- Configuring incomplete, errors occurred!\r\n  error: command '/home/wael/.local/bin/cmake' failed with exit code 1\r\n  error: subprocess-exited-with-error\r\n\r\n  × Building wheel for executorch (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> See above for output.\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /usr/bin/python3 /home/wael/.local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmpex1c4j5p\r\n  cwd: /mnt/d/TorchChat/et-build/src/executorch\r\n  Building wheel for executorch (pyproject.toml) ... error\r\n  ERROR: Failed building wheel for executorch\r\nFailed to build executorch\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (executorch)\r\n```\n\n### Versions\n\nMy env is so bad even this script erros out. I'm on WSL btw.\r\n\r\n```\r\nCollecting environment information...\r\nTraceback (most recent call last):\r\n  File \"/mnt/d/TorchChat/collect_env.py\", line 651, in <module>\r\n    main()\r\n  File \"/mnt/d/TorchChat/collect_env.py\", line 634, in main\r\n    output = get_pretty_env_info()\r\n  File \"/mnt/d/TorchChat/collect_env.py\", line 629, in get_pretty_env_info\r\n    return pretty_str(get_env_info())\r\n  File \"/mnt/d/TorchChat/collect_env.py\", line 454, in get_env_info\r\n    pip_version, pip_list_output = get_pip_packages(run_lambda)\r\n  File \"/mnt/d/TorchChat/collect_env.py\", line 411, in get_pip_packages\r\n    out = run_with_pip([sys.executable, '-mpip'])\r\n  File \"/mnt/d/TorchChat/collect_env.py\", line 406, in run_with_pip\r\n    for line in out.splitlines()\r\nAttributeError: 'NoneType' object has no attribute 'splitlines'\r\n```",
      "state": "closed",
      "author": "WaelShaikh",
      "author_type": "User",
      "created_at": "2024-08-01T10:01:58Z",
      "updated_at": "2024-08-06T11:11:37Z",
      "closed_at": "2024-08-02T08:03:06Z",
      "labels": [
        "duplicate",
        "Known Gaps"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/990/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/990",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/990",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:37.246264",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Hi @WaelShaikh, thanks for testing out the repo!\r\n\r\nWe've been focusing on development support mainly for Macs and Linux, but Windows/WSL support is definitely something we plan on looking into in the future.",
          "created_at": "2024-08-01T19:59:34Z"
        },
        {
          "author": "sunshinesfbay",
          "body": "Looks very similar to #985",
          "created_at": "2024-08-01T21:53:56Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "@WaelShaikh  To help align the investigation, let's merge this with https://github.com/pytorch/torchchat/issues/985",
          "created_at": "2024-08-02T08:03:06Z"
        },
        {
          "author": "lherrera-db",
          "body": "I am getting the same error on a Mac Apple M1 Pro, Sonoma 14.4.1. Here´s  the error message:\r\n\r\n\r\n -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n  -- Found Threads: TRUE\r\n  -- Using python executable '/Users/luis.herrera/projects/torchchat/.venv/",
          "created_at": "2024-08-06T11:08:47Z"
        }
      ]
    },
    {
      "issue_number": 1001,
      "title": "[Raspbian] streamlit GUI interface does not work / no documentation how to install",
      "body": "### 🐛 Describe the bug\n\n\r\nfrom #985:\r\n> 2. If you're interested in debugging the browser, feel free to spin up another issue with the error message from this\r\n>    > streamlit run torchchat.py -- browser llama3\r\n\r\nThanks, I will.  I suspect it's pretty straightforward - there's no streamlit installed on my system.  I assumed that your install script would install it, or tell me to install it if I needed that?!\r\n\r\n```\r\n$ streamlit\r\nbash: streamlit: command not found\r\n```\r\n\r\nI have no idea what to install for / how to install streamlit, and even less so whether it's available for this platform.  It wasn't high on my list, and so I moved on when it didn't work.  (Was curious to try the GUI just for kicks, in case this was installed by default with the OS.)\r\n\r\nHere's the Raspbian version  I used:\r\n\r\n```\r\n$ uname -a\r\nLinux raspberrypi 6.6.31+rpt-rpi-v8 #1 SMP PREEMPT Debian 1:6.6.31-1+rpt1 (2024-05-29) aarch64 GNU/Linux\r\n```\r\n\r\n\r\nopening a separate issue as suggested in #985 \r\n\r\n\n\n### Versions\n\nwget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n--2024-08-02 20:22:47--  https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 23357 (23K) [text/plain]\r\nSaving to: 'collect_env.py.1'\r\n\r\ncollect_env.py.1    100%[===================>]  22.81K  --.-KB/s    in 0.005s  \r\n\r\n2024-08-02 20:22:47 (4.57 MB/s) - 'collect_env.py.1' saved [23357/23357]\r\n\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux trixie/sid (aarch64)\r\nGCC version: (Debian 13.3.0-3) 13.3.0\r\nClang version: 16.0.6 (27+b1)\r\nCMake version: Could not collect\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.11.2 (main, May  2 2024, 11:59:08) [GCC 12.2.0] (64-bit runtime)\r\nPython platform: Linux-6.6.31+rpt-rpi-v8-aarch64-with-glibc2.39\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                         aarch64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nCPU(s):                               4\r\nOn-line CPU(s) list:                  0-3\r\nVendor ID:                            ARM\r\nModel name:                           Cortex-A76\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per cluster:                  4\r\nSocket(s):                            -\r\nCluster(s):                           1\r\nStepping:                             r4p1\r\nCPU(s) scaling MHz:                   100%\r\nCPU max MHz:                          2400.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             108.00\r\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nL1d cache:                            256 KiB (4 instances)\r\nL1i cache:                            256 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             2 MiB (1 instance)\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; CSV2, BHB\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.10.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] types-flake8-2020==1.8\r\n[pip3] types-flake8-bugbear==23.9.16\r\n[pip3] types-flake8-builtins==2.2\r\n[pip3] types-flake8-docstrings==1.7\r\n[pip3] types-flake8-plugin-utils==1.3\r\n[pip3] types-flake8-rst-docstrings==0.3\r\n[pip3] types-flake8-simplify==0.21\r\n[pip3] types-flake8-typing-imports==1.15\r\n[pip3] types-mypy-extensions==1.0\r\n[conda] Could not collect\r\n",
      "state": "closed",
      "author": "sunshinesfbay",
      "author_type": "User",
      "created_at": "2024-08-03T03:24:10Z",
      "updated_at": "2024-08-06T00:32:41Z",
      "closed_at": "2024-08-06T00:32:40Z",
      "labels": [
        "bug",
        "Browser"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1001/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1001",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1001",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:37.596333",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "> I assumed that your install script would install it, or tell me to install it if I needed that\r\n\r\nYup, your assumption is right, the install script should've installed it for you\r\nhttps://github.com/pytorch/torchchat/blob/dc684d870b58371be4e46f502719d187d120a62b/requirements.txt#L28 \r\n\r\nCan you ve",
          "created_at": "2024-08-03T19:00:29Z"
        },
        {
          "author": "sunshinesfbay",
          "body": "pip install streamlit installed it.  (Strange, because it should have just been installed from requirements.txt!)  \r\n\r\nAfter installing. I'm getting the following:\r\n\r\n(pt2) sunshine@raspberrypi:~/torchchat $ streamlit run torchchat.py -- browser stories110m\r\n2024-08-05 07:10:57.811 'latin-1' codec c",
          "created_at": "2024-08-05T14:14:00Z"
        },
        {
          "author": "byjlw",
          "body": "> pip install streamlit installed it. (Strange, because it should have just been installed from requirements.txt!)\r\n> \r\n> After installing. I'm getting the following:\r\n> \r\n> (pt2) sunshine@raspberrypi:~/torchchat $ streamlit run torchchat.py -- browser stories110m 2024-08-05 07:10:57.811 'latin-1' c",
          "created_at": "2024-08-05T17:31:14Z"
        },
        {
          "author": "sunshinesfbay",
          "body": "\r\nFresh install fixed this, and it works now:\r\n\r\n```\r\n(.venv) tc@raspberrypi:~/torchchat $ history \r\n    1  vi ~/.profile \r\n    2  source ~/.profile \r\n    3  vi ~/.bashrc \r\n    4  # get the code\r\n    5  git clone https://github.com/pytorch/torchchat.git\r\n    6  cd torchchat\r\n    7  # set up a virtua",
          "created_at": "2024-08-06T00:32:40Z"
        }
      ]
    },
    {
      "issue_number": 976,
      "title": "Crashes with internal assert while parsing options",
      "body": "### 🐛 Describe the bug\r\n\r\nSee example below:\r\n```\r\n$ python3 torchchat.py generate  --help\r\nTraceback (most recent call last):\r\n  File \"/home/nshulga/git/pytorch/torchchat/torchchat.py\", line 53, in <module>\r\n    args = parser.parse_args()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 1869, in parse_args\r\n    args, argv = self.parse_known_args(args, namespace)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 1902, in parse_known_args\r\n    namespace, args = self._parse_known_args(args, namespace)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 2096, in _parse_known_args\r\n    positionals_end_index = consume_positionals(start_index)\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 2073, in consume_positionals\r\n    take_action(action, args)\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 1978, in take_action\r\n    action(self, namespace, argument_values, option_string)\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 1241, in __call__\r\n    subnamespace, arg_strings = parser.parse_known_args(arg_strings, None)\r\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 1902, in parse_known_args\r\n    namespace, args = self._parse_known_args(args, namespace)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 2114, in _parse_known_args\r\n    start_index = consume_optional(start_index)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 2054, in consume_optional\r\n    take_action(action, args, option_string)\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 1978, in take_action\r\n    action(self, namespace, argument_values, option_string)\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 1119, in __call__\r\n    parser.print_help()\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 2601, in print_help\r\n    self._print_message(self.format_help(), file)\r\n                        ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 2585, in format_help\r\n    return formatter.format_help()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 286, in format_help\r\n    help = self._root_section.format_help()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 217, in format_help\r\n    item_help = join([func(*args) for func, args in self.items])\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 217, in <listcomp>\r\n    item_help = join([func(*args) for func, args in self.items])\r\n                      ^^^^^^^^^^^\r\n  File \"/home/nshulga/miniconda3/envs/py311/lib/python3.11/argparse.py\", line 341, in _format_usage\r\n    assert ' '.join(opt_parts) == opt_usage\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n```\r\n\r\n### Versions\r\n\r\nNot sure",
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-07-30T19:04:42Z",
      "updated_at": "2024-08-03T20:34:05Z",
      "closed_at": "2024-08-03T20:34:05Z",
      "labels": [
        "bug",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/976/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/976",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/976",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:37.818443",
      "comments": [
        {
          "author": "byjlw",
          "body": "I'm unable to re-pro with up to date main",
          "created_at": "2024-07-30T20:04:27Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Ditto",
          "created_at": "2024-07-30T20:05:29Z"
        },
        {
          "author": "malfet",
          "body": "@Jack-Khuu , @byjlw are you on Linux or Mac? And what Python version are you using?\r\nI.e. same commit passes for me with py3.12 on Mac, but fails with py3.11 on Linux:\r\n```\r\n% git rev-parse HEAD; uname -a; python3 --version; python3 torchchat.py generate --help\r\nb2c3f2697e2bbcda1b51f0c85adfaba27b900",
          "created_at": "2024-07-30T20:55:57Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Was able to repro crash on Linux devGPU\r\n\r\nCLI works on mac\r\n```\r\nb2c3f2697e2bbcda1b51f0c85adfaba27b900857\r\nDarwin jackkhuu-mbp 23.5.0 Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000 arm64\r\nPython 3.12.3\r\n```",
          "created_at": "2024-07-30T21:13:29Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "https://github.com/pytorch/torchchat/commit/4f937ac55746aa7f866dc0e827492c9a4a686d10\r\n\r\nWas the first PR where it starts failing",
          "created_at": "2024-07-30T21:22:24Z"
        }
      ]
    },
    {
      "issue_number": 1003,
      "title": "No attribute 'prompt' and 'num_samples'",
      "body": "When running the server by `python3 torchchat.py server llama3.1`, I was facing with the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/ywsung/OpenSources/torchchat/torchchat.py\", line 84, in <module>\r\n    server_main(args)\r\n  File \"/Users/ywsung/OpenSources/torchchat/server.py\", line 85, in main\r\n    gen = initialize_generator(args)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ywsung/OpenSources/torchchat/server.py\", line 69, in initialize_generator\r\n    generator_args = GeneratorArgs.from_args(args)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ywsung/OpenSources/torchchat/generate.py\", line 113, in from_args\r\n    prompt=args.prompt,\r\n           ^^^^^^^^^^^\r\nAttributeError: 'Namespace' object has no attribute 'prompt'\r\n```\r\n\r\nThe reason of the error seems like that the \"generate config\" requires prompt.\r\nAnd as you could see below, the server script internally initializes the GeneratorArgs instance by calling `GeneratorArgs.from_args(args)`:\r\n```\r\n# Add CLI Args specific to user prompted generation\r\n# Include prompt and num_sample args when the subcommand is generate\r\ndef _add_generation_args(parser, verb: str) -> None:\r\n    generator_parser = parser.add_argument_group(\r\n        \"Generation\", \"Configs for generating output based on provided prompt\"\r\n    )\r\n\r\n    if verb == \"generate\":\r\n        generator_parser.add_argument(\r\n            \"--prompt\",\r\n            type=str,\r\n            default=\"Hello, my name is\",\r\n            help=\"Input prompt for manual output generation\",\r\n        )\r\n        generator_parser.add_argument(\r\n            \"--num-samples\",\r\n            type=int,\r\n            default=1,\r\n            help=\"Number of samples\",\r\n        )\r\n```\r\n\r\nAlso, it not only 'prompt' but also 'num_samples' is empty..",
      "state": "closed",
      "author": "YeonwooSung",
      "author_type": "User",
      "created_at": "2024-08-03T13:32:22Z",
      "updated_at": "2024-08-03T20:32:58Z",
      "closed_at": "2024-08-03T20:28:57Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1003/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1003",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1003",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:38.040280",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Ah looks like I missed this in https://github.com/pytorch/torchchat/pull/987\r\n\r\nThanks for spinning up a fix, I’ll comment there",
          "created_at": "2024-08-03T15:24:38Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Landed fix, thanks again for flaggin",
          "created_at": "2024-08-03T20:32:57Z"
        }
      ]
    },
    {
      "issue_number": 1002,
      "title": "No attribute Prompt",
      "body": "### 🐛 Describe the bug\n\nAttributeError: 'Namespace' object has no attribute 'prompt'\n\n### Versions\n\npython collect_env.py\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240710\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 15.0 (arm64)\r\nGCC version: Could not collect\r\nClang version: 14.0.3 (clang-1403.0.22.14.1)\r\nCMake version: version 3.30.1\r\nLibc version: N/A\r\n\r\nPython version: 3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)] (64-bit runtime)\r\nPython platform: macOS-15.0-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M2 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240710\r\n[pip3] torchao==0.3.1\r\n[conda] Could not collect",
      "state": "closed",
      "author": "bookandlover",
      "author_type": "User",
      "created_at": "2024-08-03T12:22:22Z",
      "updated_at": "2024-08-03T15:40:19Z",
      "closed_at": "2024-08-03T15:40:01Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/1002/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/1002",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/1002",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:38.268011",
      "comments": [
        {
          "author": "YeonwooSung",
          "body": "According to the `cli.py` file, the argument 'prompt' is only added for \"generate\", but the initializer of the server requires the prompt\r\n\r\n```python\r\n# Add CLI Args specific to user prompted generation\r\n# Include prompt and num_sample args when the subcommand is generate\r\ndef _add_generation_args(",
          "created_at": "2024-08-03T13:24:02Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Ah looks like i missed this in https://github.com/pytorch/torchchat/pull/987\r\n\r\nThanks for flagging ",
          "created_at": "2024-08-03T15:24:04Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Merging with https://github.com/pytorch/torchchat/issues/1003\r\n\r\nthanks again for flagging ",
          "created_at": "2024-08-03T15:40:01Z"
        }
      ]
    },
    {
      "issue_number": 984,
      "title": "Memory usage is wrong (reporting 0) for non-CUDA commands",
      "body": "### 🐛 Describe the bug\n\nFor example \r\n`Memory used: 0.00 GB`\r\n\r\n```\r\n> python3 torchchat.py generate llama3.1 --dso-path exportedModels/llama3.1.so --prompt \"Hello my name is\"\r\n\r\nNumExpr defaulting to 10 threads.\r\nPyTorch version 2.5.0.dev20240710 available.\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nWarning: checkpoint path ignored because an exported DSO or PTE path specified\r\nUsing device=mps \r\nLoading model...\r\nCannot load specified DSO to mps. Attempting to load model to CPU instead\r\nTime to load model: 0.20 seconds\r\n-----------------------------------------------------------\r\nHello my name is Julia and I am a Junior at the University of Washington studying Communications with a focus in Public Relations. I am also a part of the University’s Public Relations Student Society of America (PRSSA), where I currently hold the position of Secretary.\r\nIn my free time, I love to stay active whether it’s hiking, running, or trying out new workout classes. I am also passionate about photography and capturing life’s precious moments. Some of my favorite places to visit are the beaches of Half Moon Bay in California and the mountains of Whistler, BC.\r\nThis is my blog where I will be sharing my thoughts on PR, advertising, and other marketing related topics. I hope you enjoy reading and will also consider sharing your thoughts with me! Feel free to follow me for more updates on my adventures and musings.\r\nI look forward to connecting with you and learning more about the PR world! – Julia\r\nHi Julia! I think your blog is a great idea! As a fellow UW student\r\nTime for inference 1: 92.83 sec total, time to first token 4.04 sec with sequential prefill, 199 tokens, 2.14 tokens/sec, 466.49 ms/token\r\nBandwidth achieved: 34.43 GB/s\r\n*** This first iteration will include cold start effects for dynamic import, hardware caches. ***\r\n\r\n========================================\r\n\r\nAverage tokens/sec: 2.14\r\nMemory used: 0.00 GB\r\n```\r\n\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240710\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.5 (arm64)\r\nGCC version: Could not collect\r\nClang version: 15.0.0 (clang-1500.1.0.2.5)\r\nCMake version: version 3.30.1\r\nLibc version: N/A\r\n\r\nPython version: 3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)] (64-bit runtime)\r\nPython platform: macOS-14.5-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M1 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240710\r\n[pip3] torchao==0.3.1\r\n[conda] Could not collect",
      "state": "open",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2024-07-31T18:32:06Z",
      "updated_at": "2024-08-02T08:16:12Z",
      "closed_at": null,
      "labels": [
        "bug",
        "actionable"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/984/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/984",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/984",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:38.492259",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Seems like this field is populated from torch.cuda.max_memory_reserved(), so it's only populated when using cuda\r\n\r\nhttps://github.com/pytorch/torchchat/blob/a3bf37d0dbac56c8c747e0610c1e2403cd386dc6/generate.py#L830\r\n\r\n",
          "created_at": "2024-07-31T19:36:54Z"
        },
        {
          "author": "g2david",
          "body": "I am using this pytorch 2.5 rocm  and python 3.10.\r\n pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.1\r\n\r\n(py_3.10) root@ae03f6964c19:~/torchchat# python3 torchchat.py generate llama3.1 --prompt \"write me a story about a boy and his bear\"\r\nN",
          "created_at": "2024-08-02T03:50:03Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "@g2david Seems like either the GPU access is not enabled on your machine or there a over utilization of memory\r\n\r\nOne other thing to check is whether the base README instructions gets you the same error (to eliminate a dependency bug)\r\nCan you spin up a new GitHub issue? ",
          "created_at": "2024-08-02T08:12:42Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Marking this as actionable with any of the options that follow:\r\na) Show the field only when Cuda is available\r\nb) Populate the field with the non-cuda equivalents",
          "created_at": "2024-08-02T08:16:07Z"
        }
      ]
    },
    {
      "issue_number": 989,
      "title": "Weird model behaviour on Server/Browser: Looks like it's not using the template",
      "body": "Hi,\r\n\r\nI'm trying out the torchchat right now, started the streamlit application with llama3 model\r\n![image](https://github.com/user-attachments/assets/3ee31c11-29ed-423a-ac29-c155bf38ebcf)\r\n\r\nI just texted Hi !!\r\n- Why is this text generation behaviour unusal , Is it the problem with model being converted to torchchat format ?\r\n\r\n![image](https://github.com/user-attachments/assets/4d38eb53-a4f4-4a58-bae0-09a7169219e9)\r\n",
      "state": "open",
      "author": "akhilreddy0703",
      "author_type": "User",
      "created_at": "2024-08-01T05:52:19Z",
      "updated_at": "2024-08-02T08:05:45Z",
      "closed_at": null,
      "labels": [
        "bug",
        "actionable",
        "Browser"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/989/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vmpuri"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/989",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/989",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:38.719380",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Hi @akhilreddy0703, thanks for testing out the repo\r\nSeems like a potential formatting/templating issue that gets surfaced in the UI\r\n\r\n@vmpuri Can you take a look at this? I don't recall this happening via the CLI chat",
          "created_at": "2024-08-01T06:20:02Z"
        },
        {
          "author": "akhilreddy0703",
          "body": "I saw a similar response with `torchchat sever` as well, please check it once",
          "created_at": "2024-08-01T09:13:52Z"
        }
      ]
    },
    {
      "issue_number": 977,
      "title": "Android app crash with stories110m model",
      "body": "### 🐛 Describe the bug\n\nHey!\r\n\r\nI tried to run torchchat on my low-end Android device and got this app crash in logcat with the `stories110m` model:\r\n\r\n```\r\nFatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 17142 (Thread-3), pid 17085 (torch.torchchat)\r\nCmdline: org.pytorch.torchchat\r\npid: 17085, tid: 17142, name: Thread-3  >>> org.pytorch.torchchat <<<\r\n      #01 pc 0000000001913aec  /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!libexecutorch_llama_jni.so (offset 0x1ab000) (et_pal_abort+8) (BuildId: 825150fdc280f1dd9cdaefab2ca01549b499eac3)\r\n      #02 pc 0000000001913a40  /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!libexecutorch_llama_jni.so (offset 0x1ab000) (torch::executor::runtime_abort()+8) (BuildId: 825150fdc280f1dd9cdaefab2ca01549b499eac3)\r\n      #03 pc 00000000018cef60  /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!libexecutorch_llama_jni.so (offset 0x1ab000) (torch::executor::Tiktoken::load(std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char> > const&)+1772) (BuildId: 825150fdc280f1dd9cdaefab2ca01549b499eac3)\r\n      #04 pc 00000000018c263c  /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!libexecutorch_llama_jni.so (offset 0x1ab000) (torch::executor::Runner::load()+892) (BuildId: 825150fdc280f1dd9cdaefab2ca01549b499eac3)\r\n      #05 pc 00000000001753dc  /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!libexecutorch_llama_jni.so (offset 0x1ab000) (facebook::jni::detail::MethodWrapper<int (executorch_jni::ExecuTorchLlamaJni::*)(), &(executorch_jni::ExecuTorchLlamaJni::load()), executorch_jni::ExecuTorchLlamaJni, int>::dispatch(facebook::jni::alias_ref<facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*>)+60) (BuildId: 825150fdc280f1dd9cdaefab2ca01549b499eac3)\r\n      #06 pc 0000000000175304  /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!libexecutorch_llama_jni.so (offset 0x1ab000) (facebook::jni::detail::FunctionWrapper<int (*)(facebook::jni::alias_ref<facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*>), facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*, int>::call(_JNIEnv*, _jobject*, int (*)(facebook::jni::alias_ref<facebook::jni::detail::JTypeFor<facebook::jni::HybridClass<executorch_jni::ExecuTorchLlamaJni, facebook::jni::detail::BaseHybridClass>::JavaPart, facebook::jni::JObject, void>::_javaobject*>))+60) (BuildId: 825150fdc280f1dd9cdaefab2ca01549b499eac3)\r\n      #12 pc 0000000000001bec  [anon:dalvik-classes4.dex extracted in memory from /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!classes4.dex] (org.pytorch.torchchat.MainActivity.setLocalModel+0)\r\n      #17 pc 000000000000172c  [anon:dalvik-classes4.dex extracted in memory from /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!classes4.dex] (org.pytorch.torchchat.MainActivity.access$700+0)\r\n      #22 pc 000000000000138c  [anon:dalvik-classes4.dex extracted in memory from /data/app/~~5lVO-wgDBy1pCoR1QhuHsg==/org.pytorch.torchchat-aRLplpZtZODSuxkK1AvhSw==/base.apk!classes4.dex] (org.pytorch.torchchat.MainActivity$1.run+0)\r\n```\r\n\r\nRough repro:\r\n\r\n```bash\r\n# setup python environment\r\npython3 -m venv venv\r\nsource venv/bin/activate\r\nexport TORCHCHAT_ROOT=${PWD}\r\n./scripts/install_et.sh\r\n\r\n# android lib setup\r\nwget https://ossci-android.s3.amazonaws.com/executorch/main/executorch-llama-tiktoken-rc3-0719.aar\r\nmkdir android/torchchat/app/libs/\r\nmv executorch-llama-tiktoken-rc3-0719.aar android/torchchat/app/libs/executorch.aar\r\n\r\n# model export\r\npython3 torchchat.py download stories110m\r\npython3 torchchat.py export stories110m --output-pte-path stories110m.pte\r\n\r\n# copy to device\r\nadb shell mkdir -p /data/local/tmp/llama\r\nadb push $HOME/.torchchat/model-cache/stories110M/tokenizer.model /data/local/tmp/llama\r\nadb push stories110m.pte /data/local/tmp/llama\r\n\r\n# run app\r\ncd android/torchchat && ./gradlew installDebug\r\n```\n\n### Versions\n\nExecutorch commit: `b2c3f2697e2bbcda1b51f0c85adfaba27b900857`\r\n\r\n```\r\nPyTorch version: 2.5.0.dev20240716\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.5 (arm64)\r\nGCC version: Could not collect\r\nClang version: 15.0.0 (clang-1500.3.9.4)\r\nCMake version: version 3.30.1\r\nLibc version: N/A\r\n\r\nPython version: 3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)] (64-bit runtime)\r\nPython platform: macOS-14.5-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M1 Max\r\n\r\nVersions of relevant libraries:\r\n[pip3] executorch==0.4.0a0+c757499\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.0.dev20240716\r\n[pip3] torchao==0.3.1\r\n[pip3] torchao==0.3.1\r\n[pip3] torchaudio==2.4.0.dev20240716\r\n[pip3] torchsr==1.0.4\r\n[pip3] torchvision==0.20.0.dev20240716\r\n[conda] Could not collect\r\n```",
      "state": "closed",
      "author": "iceychris",
      "author_type": "User",
      "created_at": "2024-07-30T21:57:41Z",
      "updated_at": "2024-08-01T23:12:26Z",
      "closed_at": "2024-08-01T23:12:11Z",
      "labels": [
        "bug",
        "Mobile - Android"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/977/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/977",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/977",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:38.982638",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Hi Christian, thanks for testing out the repo!!\r\n\r\nStories uses a sentencepiece tokenizer, while the .aar provided in README is for a tiktoken tokenizer (e.g. Llama 3)\r\nIn general, other tokenizers can be build via the ExecuTorch script\r\n\r\n> Note: The AAR file listed in Step 1 has the tiktoken token",
          "created_at": "2024-07-30T22:44:24Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Hi @iceychris, here's the aar with BPE sentencepiece. Should work with stories\r\n\r\nhttps://ossci-android.s3.amazonaws.com/executorch/main/executorch-llama-bpe-rc3-0719.aar\r\n\r\nWe'll be adding it to the README in a bit",
          "created_at": "2024-07-31T18:34:05Z"
        }
      ]
    },
    {
      "issue_number": 988,
      "title": "Could we request support for a smallish (~4-5B param) modern vision LLM? LLava-1.6 or Nanollava?",
      "body": "### 🚀 The feature, motivation and pitch\n\nHaving good basic pytorch support for inferencing LLMs is key to continued success of pytorch. Vision LLM models tend to have uneven support on mainstream inferencing engines like Llama.cpp due to the need to reimplement CLIP/SIGLIP etc. Pytorch could natively support performant vision LLMs with quantization on ARM devices, which would make a big difference in usability. \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### RFC (Optional)\n\n_No response_",
      "state": "open",
      "author": "kinchahoy",
      "author_type": "User",
      "created_at": "2024-08-01T03:59:17Z",
      "updated_at": "2024-08-01T05:50:16Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/988/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/988",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/988",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:39.288726",
      "comments": [
        {
          "author": "byjlw",
          "body": "We are currently working on llava support. Getting close! @Gasoonjia @larryliu0820 ",
          "created_at": "2024-08-01T04:45:42Z"
        }
      ]
    },
    {
      "issue_number": 617,
      "title": "cache executorch builds on runners",
      "body": "As suggested by @malfet ",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-05-01T19:35:29Z",
      "updated_at": "2024-07-30T17:36:50Z",
      "closed_at": null,
      "labels": [
        "performance"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/617/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "malfet",
        "larryliu0820",
        "mikekgfb",
        "metascroy"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/617",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/617",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:41.337921",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "This is the gt explanation for caches, https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows#cache-hits-and-misses \r\n\r\nbut we don't have an example of how the cache is populated.",
          "created_at": "2024-05-01T19:44:13Z"
        },
        {
          "author": "metascroy",
          "body": "Summarizing offline convo:\r\n\r\n* cache all of et-build\r\n* Do something like \"basically python setup.py bdist_wheel -d /path/to/wheel-out-cache-dir/\"\r\n\r\ncc @dbort  @larryliu0820 ",
          "created_at": "2024-05-02T00:11:34Z"
        },
        {
          "author": "larryliu0820",
          "body": "Short term:\r\n\r\n- Cache et-build/ since it contains build artifact (static libraries) as well as source code\r\n- Enable executorch nightly build so we can download pybind.so directly. For a lot of CI jobs that's the only thing we need.\r\n- Make sure we pin on ExecuTorch nightly and when we checkout Exe",
          "created_at": "2024-05-02T00:21:42Z"
        },
        {
          "author": "mikekgfb",
          "body": "it seems it’s straightforward — indicate what the cache should cache, how to get a cache key (I guess so we can figure out changes?  git version and hardware system, probably), and it will just make things appear (if it has it), and you can check whether cache checkout was successful, or else build ",
          "created_at": "2024-05-02T00:38:55Z"
        }
      ]
    },
    {
      "issue_number": 681,
      "title": "Executorch excessive non-actionable output https://github.com/pytorch/executorch/issues/3515",
      "body": "see https://github.com/pytorch/executorch/issues/3515 documents excessive non-actionable output",
      "state": "open",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-05-05T05:10:21Z",
      "updated_at": "2024-07-30T17:35:29Z",
      "closed_at": null,
      "labels": [
        "actionable"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/681/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "orionr",
        "byjlw"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/681",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/681",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:41.604155",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "Assigning to @byjlw as per guidance from @orionr that Jesse's team will take lead on torchchat ",
          "created_at": "2024-05-05T23:35:18Z"
        }
      ]
    },
    {
      "issue_number": 903,
      "title": "Github code search doesnt work with folders called `build`",
      "body": "### 🐛 Describe the bug\n\nI was trying to look for the  `model.py` definition\r\nhttps://github.com/pytorch/torchchat/tree/main/build but it wasn't showing up\r\n<img width=\"816\" alt=\"Screenshot 2024-07-15 at 6 54 39 PM\" src=\"https://github.com/user-attachments/assets/11021312-9e40-4ec6-adad-0a52a24f06e0\">\r\n\r\ngenerate.py which is not in builder works fine\r\n<img width=\"805\" alt=\"Screenshot 2024-07-15 at 6 54 54 PM\" src=\"https://github.com/user-attachments/assets/8f6eaf5e-7e76-4a3d-b1cc-37254c6e3515\">\r\n\r\nCan we rename the folder to anything else, build to me signifies either release infra scripts or artifacts that are created after installing a package not model building utilities\r\n\n\n### Versions\n\nNightlies",
      "state": "open",
      "author": "msaroufim",
      "author_type": "User",
      "created_at": "2024-07-16T01:55:45Z",
      "updated_at": "2024-07-30T15:11:19Z",
      "closed_at": null,
      "labels": [
        "actionable"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/903/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/903",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/903",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:41.870951",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "I was able to repro the search results with the in repo search. \r\nDefinitely a weird search interaction (the GH search from upper right does find it properly)\r\n\r\nRegardless, build is an overloaded name and we'll look into renaming",
          "created_at": "2024-07-17T07:40:25Z"
        }
      ]
    },
    {
      "issue_number": 493,
      "title": "Test FP16/BF16 in runner_et",
      "body": "In https://github.com/pytorch/torchchat/pull/492/, we fix an issue with FP16/BF16 in runner-aoti.\r\n\r\nThere is likely a similar issue in runner-et, but runner-et does not build atm.  Test this once runner-et builds.",
      "state": "closed",
      "author": "metascroy",
      "author_type": "User",
      "created_at": "2024-04-26T06:27:50Z",
      "updated_at": "2024-07-21T22:14:29Z",
      "closed_at": "2024-07-21T22:14:29Z",
      "labels": [
        "launch blocker"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/493/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "metascroy"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/493",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/493",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:42.092344",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "@metascroy also make sure we run unit tests `for dtypes in [ float32, float16, bfloat16]` (both runner-et and runner-aoti)\r\nThank you!",
          "created_at": "2024-04-27T03:00:14Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "\r\nClosing issues that are finished or not-actioned upon in over 2 months\r\n\r\n",
          "created_at": "2024-07-21T22:14:29Z"
        }
      ]
    },
    {
      "issue_number": 512,
      "title": "Need to unpack from AVX and repack for GPU if we perform quantization on CPU",
      "body": "@jerryzh168 \r\n> one device may not be compatible with another (gives wrong results) as recently discovered by @HDCharles, and it's a silent error right now. have we done any evaluation on accuracy for this change (on cuda)?\r\n\r\n@mikekgfb \r\n> We have not done this, but @malfet and I have discussed this, and Intel had previously promised us an unpack routine - so we would be bale to unpack() the different formats. BTW, we need an unpack for the GPU packing format as well\r\n\r\nref: #508 (https://github.com/pytorch/torchchat/pull/508#discussion_r1581708543)",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-27T02:08:57Z",
      "updated_at": "2024-07-21T22:05:37Z",
      "closed_at": "2024-07-21T22:05:37Z",
      "labels": [
        "launch blocker"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/512/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "malfet",
        "supriyar",
        "jerryzh168",
        "HDCharles"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/512",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/512",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:42.343695",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "If we don't have any tests failing for this, that's a concern in and of itself",
          "created_at": "2024-04-27T02:53:48Z"
        },
        {
          "author": "mikekgfb",
          "body": "We don't need to get dedicated unpack routines from vendors (they have an incentive to not support unload, because it puts  the packed weights in a vendor jail linked to their hardware, so getting that function doesn't line up with their commercial incentives...)\r\n\r\nAll we have to do is multiply wit",
          "created_at": "2024-04-27T20:01:09Z"
        },
        {
          "author": "mikekgfb",
          "body": "How do we hook packing conversion to [tensor.to](http://tensor.to/)(device=…) to reflect move between devices and reformat packing.  With the recent change where we always convert on CPU to avoid overflowing device memory, we become much more vulnerable because it's not just a problem when the user ",
          "created_at": "2024-04-27T20:10:21Z"
        },
        {
          "author": "jerryzh168",
          "body": "so if we implemnet quantization with tensor subclass, we are planning to put this stuff there, for example: https://github.com/pytorch/ao/blob/main/torchao/quantization/subclass.py#L239\r\n\r\nwithout this I feel we probably have to call an explicit method to do it",
          "created_at": "2024-04-29T16:42:45Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Closing issues that are finished or not-actioned upon in over 2 months",
          "created_at": "2024-07-21T22:05:37Z"
        }
      ]
    },
    {
      "issue_number": 700,
      "title": "README.md and other publicly facing docs should contain accurate instructions",
      "body": "Users that start with the repo should be able to follow thru the documentation on supported HW/SW platforms, but right now there are at least 15 open issues that mention documentation improvements one way or another:\r\n- https://github.com/pytorch/torchchat/issues/337\r\n- https://github.com/pytorch/torchchat/issues/528\r\n- https://github.com/pytorch/torchchat/issues/542\r\n- https://github.com/pytorch/torchchat/issues/544\r\n- https://github.com/pytorch/torchchat/issues/555\r\n- https://github.com/pytorch/torchchat/issues/557\r\n- https://github.com/pytorch/torchchat/issues/558\r\n- https://github.com/pytorch/torchchat/issues/561\r\n- https://github.com/pytorch/torchchat/issues/564\r\n- https://github.com/pytorch/torchchat/issues/699\r\n\r\nCreating this umbrella issue to track overall docs launch readiness",
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-05-06T19:29:05Z",
      "updated_at": "2024-07-21T22:04:16Z",
      "closed_at": "2024-07-21T22:04:14Z",
      "labels": [
        "launch blocker"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/700/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/700",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/700",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:42.568493",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "I merged MISC.md and ADVANCED-USERS.md in https://github.com/pytorch/torchchat/pull/765",
          "created_at": "2024-05-12T20:07:56Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Closing untracked umbrella task",
          "created_at": "2024-07-21T22:04:15Z"
        }
      ]
    },
    {
      "issue_number": 692,
      "title": "[LAUNCH BLOCKER] TorchChat results seems less connected than they could have been",
      "body": "For example generating text from the same prompt using llama.cpp and TorchChat produces following results:\r\n```\r\nHello, my name is **Marcus**, and I am a 33-year-old software developer from California. I have been using the internet for the past 20 years, and I have seen it evolve into a powerful tool for communication, entertainment, and information. However, I have also seen the darker side of the internet, including cyberbullying, harassment, and the spread of misinformation.\r\n\r\nAs a software developer, I have a unique perspective on the internet and its potential impact on society. I believe that the internet can be a force for good, but it must be used responsibly and ethically. This is why I am passionate about promoting digital citizenship and raising awareness about the importance of online safety and security.\r\n\r\nIn my free time, I enjoy writing, hiking, and playing music. I am also a volunteer firefighter, and I have seen firsthand the impact of the\r\n\r\n```\r\nvs \r\n```\r\nHello, my name is _______________ and I'm here to talk about my experience with ______________ (addiction, trauma, mental health issue, etc.).\r\nI understand that you are here to help me and I appreciate your willingness to listen. It takes a lot of courage to share my story, but I hope that by doing so, it will help me heal and move forward.\r\nCan you tell me more about the support groups you offer? How do they work? What kind of people attend them? Are they confidential?\r\nI'm still not sure if this is the right place for me, but I'm willing to give it a try. Can you tell me more about your program and how it can help me?\r\nI've tried other programs before, but they didn't work for me. What makes your program different?\r\nI'm worried that if I share my story, people will judge me or think less of me. Can you guarantee confidentiality?\r\nThank you for being here for me and supporting me on this journey. I really appreciate it. [end of text]\r\n```\r\n\r\nIt's very subjective, but 2nd text (about person who wants to find more information about metal health/addiction programs, feels more believable/coherent then story about 33 SWE who is also a volunteer firefighter. What it looks like is that by 3rd paragraph TorchChat lost context about two previous ones, which sounds like a context size of stories15M, but not of Llama-2",
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-05-06T16:31:38Z",
      "updated_at": "2024-07-21T22:00:21Z",
      "closed_at": "2024-07-21T22:00:09Z",
      "labels": [
        "launch blocker"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/692/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/692",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/692",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:42.764983",
      "comments": [
        {
          "author": "orionr",
          "body": "@malfet can you please add the exact prompt string and model you're using for each? Thank you.\r\n\r\nI think this is a launch blocker given that it is at the heart of correctness issues - are we agreed? cc @ali-khosh @byjlw @mikekgfb ",
          "created_at": "2024-05-06T19:23:04Z"
        },
        {
          "author": "ali-khosh",
          "body": "I agree. @malfet can we try a few other prompts? also are we now convinced its a context window problem (i.e., I assume both models, prompts, target device, params, are exactly the same)",
          "created_at": "2024-05-06T20:02:12Z"
        },
        {
          "author": "malfet",
          "body": "@ali-khosh Models are not exactly the same, but TorchChat is incompatible with weights published on https://llama.meta.com/llama-downloads/ and GGML is incompatible with the conversion format we are using for TorchChat, see https://github.com/pytorch/torchchat/issues/704 :/",
          "created_at": "2024-05-06T20:35:15Z"
        },
        {
          "author": "ali-khosh",
          "body": "Got it I see. That makes it trickier to unblock us for this task since it's harder to know the root cause. Are you thinking of addressing 704 before tackling this problem?",
          "created_at": "2024-05-06T20:42:43Z"
        },
        {
          "author": "mikekgfb",
          "body": "See my comment on 704. \r\nFor \"GGML is incompatible with the conversion format we are using for TorchChat,\" I don't understand what this means?\r\nYou can't load what we use into GGML?  Or there is no GGUF that runs in torchchat and GGML?\r\n\r\n@metascroy wrote GGUF importer and reading a suitable GGUF sh",
          "created_at": "2024-05-07T06:49:15Z"
        }
      ]
    },
    {
      "issue_number": 645,
      "title": "[PERFORMANCE] Investigate performance decrease on 4b embedding quantization",
      "body": "              FWIW, here was my train of thoughts and actions:\r\n\r\n- i ran llama3 on my macbook pro with eager. it was able to generate texts great. but it felt slow because it is doing floating point.\r\n- i wanted to try quantized version on CPU hoping that it will give me perf boost.\r\n- i wasn't able to find copy-pastable quantization instructions from top-level readme. But it linked to this page (https://github.com/pytorch/torchchat/blob/main/docs/quantization.md)\r\n- on that page, i saw `'{\"embedding\" : {\"bitwidth\": 8, \"groupsize\": 0}}'` config, which i tried. it was still a bit slow (3.58 tokens/s). then i tried ` '{\"embedding\" : {\"bitwidth\": 4, \"groupsize\": 32}}'`, and again it was still slow (2.38 tokens/s)\r\n- next i tried copy pasting `{\"embedding\": {\"bitwidth\": 4, \"groupsize\":32}, \"linear:a8w4dq\": {\"groupsize\" : 64}}'`, because i thought it will hopefully accelerate the linear layer. and hit the original error.\r\n\r\nsounds like i picked a wrong config that doesn't work for desktop.\r\n\r\ni think an easy solution right now is to have a table of what quantization options work for each platforms (eager, compile, aoti, mobile)\r\n\r\n_Originally posted by @mergennachin in https://github.com/pytorch/torchchat/issues/621#issuecomment-2090786452_\r\n            ",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-05-03T14:45:12Z",
      "updated_at": "2024-07-21T21:57:20Z",
      "closed_at": "2024-07-21T21:57:20Z",
      "labels": [
        "performance"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/645/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "kimishpatel"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/645",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/645",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:43.008143",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "@kimishpatel indicated that this warranted followup investigation because he had prior perf results indicating performance benefit.",
          "created_at": "2024-05-03T14:46:15Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Closing stale umbrella issues",
          "created_at": "2024-07-21T21:57:20Z"
        }
      ]
    },
    {
      "issue_number": 2,
      "title": "[Feature request] Improved PyTorch CPU floating-point performance",
      "body": "PyTorch should deliver best-in-class inference performance on LLM models supported by this repo",
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-03-28T22:41:47Z",
      "updated_at": "2024-07-21T21:52:32Z",
      "closed_at": "2024-07-21T21:52:32Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/2/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "malfet"
      ],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/2",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/2",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:43.280675",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "Several kernels landed by @malfet . Thank you!",
          "created_at": "2024-04-25T08:11:10Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Closing stale umbrella issues",
          "created_at": "2024-07-21T21:52:32Z"
        }
      ]
    },
    {
      "issue_number": 3,
      "title": "[Feature request] Improved PyTorch CPU quantized performance",
      "body": null,
      "state": "closed",
      "author": "malfet",
      "author_type": "User",
      "created_at": "2024-03-28T22:42:56Z",
      "updated_at": "2024-07-21T21:52:09Z",
      "closed_at": "2024-07-21T21:52:08Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/3/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/3",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/3",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:43.501516",
      "comments": [
        {
          "author": "mikekgfb",
          "body": "Several workstreams have made great progress:\r\n* int4pack_mm - Thank you @malfet !\r\n* int8_mm - Thank you @malfet \r\n* a8w4dq - @swolchok and @digantdesai discussing a solution.  Thank you both!",
          "created_at": "2024-04-25T08:10:42Z"
        },
        {
          "author": "Jack-Khuu",
          "body": "Closing stale umbrella issues",
          "created_at": "2024-07-21T21:52:08Z"
        }
      ]
    },
    {
      "issue_number": 448,
      "title": "[Feature Request] switch controller and controlled party for browser chat",
      "body": "In the long term, we might explore having torchchat/generate spawn the flask app and locally invoke generate_main, that way we can use the agurmnts parser, and we only need to send a handful of parameters from generator to the browser app, rather than going full circle from torchchat to flask-webserver and then have fllask send the args back to torchchat in a new invocation.\r\n\r\nThe one advantage of having the flask app manage things is that we could actually swap out torchchat for the binary runner and use it to chat with the binary",
      "state": "closed",
      "author": "mikekgfb",
      "author_type": "User",
      "created_at": "2024-04-24T05:41:46Z",
      "updated_at": "2024-07-21T21:47:30Z",
      "closed_at": "2024-07-21T21:47:30Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pytorch/torchchat/issues/448/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pytorch/torchchat/issues/448",
      "api_url": "https://api.github.com/repos/pytorch/torchchat/issues/448",
      "repository": "pytorch/torchchat",
      "extraction_date": "2025-06-22T00:43:43.733034",
      "comments": [
        {
          "author": "Jack-Khuu",
          "body": "Closing stale issues",
          "created_at": "2024-07-21T21:47:30Z"
        }
      ]
    }
  ]
}