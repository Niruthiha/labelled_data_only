{
  "repository": "containers/ramalama",
  "repository_info": {
    "repo": "containers/ramalama",
    "stars": 1814,
    "language": "Python",
    "description": "RamaLama is an open-source developer tool that simplifies the local serving of AI models from any source and facilitates their use for inference in production, all through the familiar language of con",
    "url": "https://github.com/containers/ramalama",
    "topics": [
      "ai",
      "containers",
      "cuda",
      "hip",
      "inference-server",
      "intel",
      "llamacpp",
      "llm",
      "podman",
      "vllm"
    ],
    "created_at": "2024-07-24T19:09:58Z",
    "updated_at": "2025-06-21T19:36:45Z",
    "search_query": "local llm language:python stars:>2",
    "total_issues_estimate": 50,
    "labeled_issues_estimate": 33,
    "labeling_rate": 66.7,
    "sample_labeled": 10,
    "sample_total": 15,
    "has_issues": true,
    "repo_id": 833306239,
    "default_branch": "main",
    "size": 3265
  },
  "extraction_date": "2025-06-22T00:51:04.436310",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 114,
  "issues": [
    {
      "issue_number": 1577,
      "title": "MacOs M4 junk output with latest image on GPU",
      "body": "### Issue Description\n\nI have a podman machine with libkrun support started and running using Podman Desktop. Its on the latest podman 5.5.1 as well.\n\n```\n2025-06-21 19:33:25 - DEBUG - exec_cmd: podman run --rm --label ai.ramalama.model=llama3.2 --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.port=8080 --label ai.ramalama.command=run --device /dev/dri -p 8080:8080 --security-opt=label=disable --cap-drop=all --security-opt=no-new-privileges --pull newer -d -i --label ai.ramalama --name ramalama_irmopnLcUa --env=HOME=/tmp --init --mount=type=bind,src=/Users/bmahabir/.local/share/ramalama/store/ollama/llama3.2/llama3.2/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff,destination=/mnt/models/model.file,ro --mount=type=bind,src=/Users/bmahabir/.local/share/ramalama/store/ollama/llama3.2/llama3.2/snapshots/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff/chat_template,destination=/mnt/models/chat_template.file,ro quay.io/ramalama/ramalama:0.9 /usr/libexec/ramalama/ramalama-serve-core llama-server --port 8080 --model /mnt/models/model.file --no-warmup --jinja --log-colors --alias llama3.2 --ctx-size 2048 --temp 0.8 --cache-reuse 256 -v -ngl 999 --threads 7 --host 0.0.0.0\n7e392ef271db9a5dedfab8bd70804a4a204988e188a27409fad7b46b189d0e3a\nðŸ¦­ > hi\nHowHowHowHow how how\n```\n\nUsing the cpu only container it works great, wondering what could've changed here?\n\n### Steps to reproduce the issue\n\nuse podman desktop to create a vm with libkrun support \nthen use ramalama\n\n### Describe the results you received\n\n```\nðŸ¦­ > hi\nHowHowHowHow how how\n```\n\n### Describe the results you expected\n\n```\nðŸ¦­ > hi\nhi how are you\n```\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"Client\": {\n                \"APIVersion\": \"5.5.1\",\n                \"BuildOrigin\": \"pkginstaller\",\n                \"Built\": 1749159952,\n                \"BuiltTime\": \"Thu Jun  5 17:45:52 2025\",\n                \"GitCommit\": \"850db76dd78a0641eddb9ee19ee6f60d2c59bcfa\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"darwin\",\n                \"OsArch\": \"darwin/arm64\",\n                \"Version\": \"5.5.1\"\n            },\n            \"host\": {\n                \"arch\": \"arm64\",\n                \"buildahVersion\": \"1.39.2\",\n                \"cgroupControllers\": [\n                    \"cpuset\",\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\",\n                    \"rdma\",\n                    \"misc\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.12-3.fc41.aarch64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.12, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.64,\n                    \"systemPercent\": 0.19,\n                    \"userPercent\": 0.18\n                },\n                \"cpus\": 8,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"coreos\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2047,\n                \"hostname\": \"localhost.localdomain\",\n                \"idMappings\": {\n                    \"gidmap\": null,\n                    \"uidmap\": null\n                },\n                \"kernel\": \"6.12.13-200.fc41.aarch64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 26567778304,\n                \"memTotal\": 29288497152,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc41.aarch64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.0-1.fc41.aarch64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.20-2.fc41.aarch64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.20\\ncommit: 9c9a76ac11994701dd666c4f0b869ceffb599a66\\nrundir: /run/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250121.g4f2c8e7-2.fc41.aarch64\",\n                    \"version\": \"pasta 0^20250121.g4f2c8e7-2.fc41.aarch64-pasta\\nCopyright Red Hat\\nGNU General Public License, version 2 or later\\n  <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"unix:///run/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": false,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": true,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-1.fc41.aarch64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 0,\n                \"swapTotal\": 0,\n                \"uptime\": \"0h 4m 14.00s\",\n                \"variant\": \"v8\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/usr/share/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 0,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {\n                    \"overlay.additionalImageStores\": [\n                        \"/usr/lib/containers/storage\"\n                    ],\n                    \"overlay.imagestore\": \"/usr/lib/containers/storage\",\n                    \"overlay.mountopt\": \"nodev,metacopy=on\"\n                },\n                \"graphRoot\": \"/var/lib/containers/storage\",\n                \"graphRootAllocated\": 98899800064,\n                \"graphRootUsed\": 14402322432,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"false\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"true\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"true\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 1\n                },\n                \"runRoot\": \"/run/containers/storage\",\n                \"transientStore\": false,\n                \"volumePath\": \"/var/lib/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.1\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1741651200,\n                \"BuiltTime\": \"Mon Mar 10 20:00:00 2025\",\n                \"GitCommit\": \"b79bc8afe796cba51dd906270a7e1056ccdfcf9e\",\n                \"GoVersion\": \"go1.23.7\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/arm64\",\n                \"Version\": \"5.4.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.9\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/Users/bmahabir/ramalama/shortnames/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF/gemma-3-1b-it-Q4_K_M.gguf\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/Users/bmahabir/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.9.2\"\n}\nbmahabir@bmahabir-mac ramalama %\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "bmahabirbu",
      "author_type": "User",
      "created_at": "2025-06-21T23:37:11Z",
      "updated_at": "2025-06-22T03:12:22Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1577/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1577",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1577",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:29.594731",
      "comments": [
        {
          "author": "bmahabirbu",
          "body": "I also tried with mistral which gave a bunch of ####. This reminds me of a issue I tackled a long time ago with cuda. It turned out to be a container build issue flag that was incorrect maybe its a similar issue",
          "created_at": "2025-06-21T23:38:38Z"
        },
        {
          "author": "bmahabirbu",
          "body": "think I caught the issue \n\n```\n  vulkan | asahi)\n    common_flags+=(\"-DGGML_VULKAN=1\")\n    ;;\n```\nIt should be ON.\n",
          "created_at": "2025-06-21T23:43:06Z"
        },
        {
          "author": "bmahabirbu",
          "body": "actually lower down in build llama.cpp whisper script we see that it is indeed turned ON\n\n```\nramalama)\n    if [ \"$uname_m\" = \"x86_64\" ] || [ \"$uname_m\" = \"aarch64\" ]; then\n      common_flags+=(\"-DGGML_VULKAN=ON\")\n```\nSo this couldnt be it",
          "created_at": "2025-06-22T00:02:17Z"
        },
        {
          "author": "bmahabirbu",
          "body": "somehow it magically fixed itself!\n\nMaybe updating to podman 5.5 didnt fully take effect?\n\nMight still keep this issue open if others were having problems",
          "created_at": "2025-06-22T00:09:11Z"
        }
      ]
    },
    {
      "issue_number": 1572,
      "title": "llama server is not available at localhost when running \"ramalama serve --host 127.0.0.1\"",
      "body": "### Issue Description\n\nTo try to serve a model inside localhost only, I attempted to run `RAMALAMA_CONTAINER_ENGINE=docker ramalama serve --host 127.0.0.1 qwen2.5-coder:3b`. Trying to curl to `http://127.0.0.1:8080` failed, because ramalama instead of telling docker to do port forward from 127.0.0.1, it tells llama.cpp to serve at 127.0.0.1, which in turn makes docker port forwarding no longer work because it needs llama.cpp to always serve at 0.0.0.0 inside the container.\n\n### Steps to reproduce the issue\n\nStep 1: Run `RAMALAMA_CONTAINER_ENGINE=docker ramalama serve --host 127.0.0.1 qwen2.5-coder:3b`\nStep 2: \n```\ncurl -X POST http://localhost:8080/infill \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"input_prefix\": \"import math\\n\\ndef calculate_factorial(n):\\n    \\\"\\\"\\\"\\n    Calculates the factorial of a non-negative integer.\\n    \\\"\\\"\\\"\\n    if not isinstance(n, int) or n < 0:\\n        raise ValueError(\\\"Input must be a non-negative integer\\\")\",\n  \"input_suffix\": \"\\n    return result\\n\\n# Example Usage:\\nprint(f\\\"The factorial of 5 is: {calculate_factorial(5)}\\\")\\nprint(f\\\"The factorial of 0 is: {calculate_factorial(0)}\\\")\"\n}'\n```\n\n### Describe the results you received\n\nConnection refused\n\n### Describe the results you expected\n\ncan connect to http://localhost:8080 succesfully\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"intel\",\n    \"Engine\": {\n        \"Info\": {\n            \"Architecture\": \"x86_64\",\n            \"CDISpecDirs\": [\n                \"/etc/cdi\",\n                \"/var/run/cdi\"\n            ],\n            \"CPUSet\": true,\n            \"CPUShares\": true,\n            \"CgroupDriver\": \"systemd\",\n            \"CgroupVersion\": \"2\",\n            \"ClientInfo\": {\n                \"Arch\": \"amd64\",\n                \"BuildTime\": \"Fri May 30 00:00:00 2025\",\n                \"Context\": \"default\",\n                \"Debug\": false,\n                \"DefaultAPIVersion\": \"1.50\",\n                \"GitCommit\": \"1.fc42\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"Plugins\": [\n                    {\n                        \"Name\": \"buildx\",\n                        \"Path\": \"/usr/libexec/docker/cli-plugins/docker-buildx\",\n                        \"SchemaVersion\": \"0.1.0\",\n                        \"ShortDescription\": \"Docker Buildx\",\n                        \"Vendor\": \"Docker Inc.\",\n                        \"Version\": \"0.24.0\"\n                    },\n                    {\n                        \"Name\": \"compose\",\n                        \"Path\": \"/var/home/lephuong/.docker/cli-plugins/docker-compose\",\n                        \"SchemaVersion\": \"0.1.0\",\n                        \"ShortDescription\": \"Docker Compose\",\n                        \"Vendor\": \"Docker Inc.\",\n                        \"Version\": \"v2.33.1\"\n                    }\n                ],\n                \"Version\": \"28.2.2\",\n                \"Warnings\": null\n            },\n            \"Containerd\": {\n                \"Address\": \"/run/containerd/containerd.sock\",\n                \"Namespaces\": {\n                    \"Containers\": \"moby\",\n                    \"Plugins\": \"plugins.moby\"\n                }\n            },\n            \"ContainerdCommit\": {\n                \"ID\": \"2.fc42\"\n            },\n            \"Containers\": 4,\n            \"ContainersPaused\": 0,\n            \"ContainersRunning\": 1,\n            \"ContainersStopped\": 3,\n            \"CpuCfsPeriod\": true,\n            \"CpuCfsQuota\": true,\n            \"Debug\": false,\n            \"DefaultRuntime\": \"runc\",\n            \"DockerRootDir\": \"/var/lib/docker\",\n            \"Driver\": \"btrfs\",\n            \"DriverStatus\": [\n                [\n                    \"Btrfs\",\n                    \"\"\n                ]\n            ],\n            \"ExperimentalBuild\": false,\n            \"FirewallBackend\": {\n                \"Driver\": \"iptables+firewalld\"\n            },\n            \"GenericResources\": null,\n            \"HttpProxy\": \"\",\n            \"HttpsProxy\": \"\",\n            \"ID\": \"d7a3e046-5eaf-435a-9c53-5423c8179aa7\",\n            \"IPv4Forwarding\": true,\n            \"Images\": 49,\n            \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n            \"InitBinary\": \"/usr/bin/tini-static\",\n            \"InitCommit\": {\n                \"ID\": \"\"\n            },\n            \"Isolation\": \"\",\n            \"KernelVersion\": \"6.14.11-300.fc42.x86_64\",\n            \"Labels\": [],\n            \"LiveRestoreEnabled\": false,\n            \"LoggingDriver\": \"json-file\",\n            \"MemTotal\": 33063309312,\n            \"MemoryLimit\": true,\n            \"NCPU\": 22,\n            \"NEventsListener\": 0,\n            \"NFd\": 37,\n            \"NGoroutines\": 65,\n            \"Name\": \"phuonglh2\",\n            \"NoProxy\": \"\",\n            \"OSType\": \"linux\",\n            \"OSVersion\": \"42\",\n            \"OomKillDisable\": false,\n            \"OperatingSystem\": \"Fedora Linux 42.20250619.0 (Kinoite)\",\n            \"PidsLimit\": true,\n            \"Plugins\": {\n                \"Authorization\": null,\n                \"Log\": [\n                    \"awslogs\",\n                    \"fluentd\",\n                    \"gcplogs\",\n                    \"gelf\",\n                    \"journald\",\n                    \"json-file\",\n                    \"local\",\n                    \"splunk\",\n                    \"syslog\"\n                ],\n                \"Network\": [\n                    \"bridge\",\n                    \"host\",\n                    \"ipvlan\",\n                    \"macvlan\",\n                    \"null\",\n                    \"overlay\"\n                ],\n                \"Volume\": [\n                    \"local\"\n                ]\n            },\n            \"RegistryConfig\": {\n                \"IndexConfigs\": {\n                    \"docker.io\": {\n                        \"Mirrors\": [],\n                        \"Name\": \"docker.io\",\n                        \"Official\": true,\n                        \"Secure\": true\n                    }\n                },\n                \"InsecureRegistryCIDRs\": [\n                    \"::1/128\",\n                    \"127.0.0.0/8\"\n                ],\n                \"Mirrors\": null\n            },\n            \"RuncCommit\": {\n                \"ID\": \"\"\n            },\n            \"Runtimes\": {\n                \"io.containerd.runc.v2\": {\n                    \"path\": \"runc\",\n                    \"status\": {\n                        \"org.opencontainers.runtime-spec.features\": \"{\\\"ociVersionMin\\\":\\\"1.0.0\\\",\\\"ociVersionMax\\\":\\\"1.2.1\\\",\\\"hooks\\\":[\\\"prestart\\\",\\\"createRuntime\\\",\\\"createContainer\\\",\\\"startContainer\\\",\\\"poststart\\\",\\\"poststop\\\"],\\\"mountOptions\\\":[\\\"async\\\",\\\"atime\\\",\\\"bind\\\",\\\"defaults\\\",\\\"dev\\\",\\\"diratime\\\",\\\"dirsync\\\",\\\"exec\\\",\\\"iversion\\\",\\\"lazytime\\\",\\\"loud\\\",\\\"mand\\\",\\\"noatime\\\",\\\"nodev\\\",\\\"nodiratime\\\",\\\"noexec\\\",\\\"noiversion\\\",\\\"nolazytime\\\",\\\"nomand\\\",\\\"norelatime\\\",\\\"nostrictatime\\\",\\\"nosuid\\\",\\\"nosymfollow\\\",\\\"private\\\",\\\"ratime\\\",\\\"rbind\\\",\\\"rdev\\\",\\\"rdiratime\\\",\\\"relatime\\\",\\\"remount\\\",\\\"rexec\\\",\\\"rnoatime\\\",\\\"rnodev\\\",\\\"rnodiratime\\\",\\\"rnoexec\\\",\\\"rnorelatime\\\",\\\"rnostrictatime\\\",\\\"rnosuid\\\",\\\"rnosymfollow\\\",\\\"ro\\\",\\\"rprivate\\\",\\\"rrelatime\\\",\\\"rro\\\",\\\"rrw\\\",\\\"rshared\\\",\\\"rslave\\\",\\\"rstrictatime\\\",\\\"rsuid\\\",\\\"rsymfollow\\\",\\\"runbindable\\\",\\\"rw\\\",\\\"shared\\\",\\\"silent\\\",\\\"slave\\\",\\\"strictatime\\\",\\\"suid\\\",\\\"symfollow\\\",\\\"sync\\\",\\\"tmpcopyup\\\",\\\"unbindable\\\"],\\\"linux\\\":{\\\"namespaces\\\":[\\\"cgroup\\\",\\\"ipc\\\",\\\"mount\\\",\\\"network\\\",\\\"pid\\\",\\\"time\\\",\\\"user\\\",\\\"uts\\\"],\\\"capabilities\\\":[\\\"CAP_CHOWN\\\",\\\"CAP_DAC_OVERRIDE\\\",\\\"CAP_DAC_READ_SEARCH\\\",\\\"CAP_FOWNER\\\",\\\"CAP_FSETID\\\",\\\"CAP_KILL\\\",\\\"CAP_SETGID\\\",\\\"CAP_SETUID\\\",\\\"CAP_SETPCAP\\\",\\\"CAP_LINUX_IMMUTABLE\\\",\\\"CAP_NET_BIND_SERVICE\\\",\\\"CAP_NET_BROADCAST\\\",\\\"CAP_NET_ADMIN\\\",\\\"CAP_NET_RAW\\\",\\\"CAP_IPC_LOCK\\\",\\\"CAP_IPC_OWNER\\\",\\\"CAP_SYS_MODULE\\\",\\\"CAP_SYS_RAWIO\\\",\\\"CAP_SYS_CHROOT\\\",\\\"CAP_SYS_PTRACE\\\",\\\"CAP_SYS_PACCT\\\",\\\"CAP_SYS_ADMIN\\\",\\\"CAP_SYS_BOOT\\\",\\\"CAP_SYS_NICE\\\",\\\"CAP_SYS_RESOURCE\\\",\\\"CAP_SYS_TIME\\\",\\\"CAP_SYS_TTY_CONFIG\\\",\\\"CAP_MKNOD\\\",\\\"CAP_LEASE\\\",\\\"CAP_AUDIT_WRITE\\\",\\\"CAP_AUDIT_CONTROL\\\",\\\"CAP_SETFCAP\\\",\\\"CAP_MAC_OVERRIDE\\\",\\\"CAP_MAC_ADMIN\\\",\\\"CAP_SYSLOG\\\",\\\"CAP_WAKE_ALARM\\\",\\\"CAP_BLOCK_SUSPEND\\\",\\\"CAP_AUDIT_READ\\\",\\\"CAP_PERFMON\\\",\\\"CAP_BPF\\\",\\\"CAP_CHECKPOINT_RESTORE\\\"],\\\"cgroup\\\":{\\\"v1\\\":true,\\\"v2\\\":true,\\\"systemd\\\":true,\\\"systemdUser\\\":true,\\\"rdma\\\":true},\\\"seccomp\\\":{\\\"enabled\\\":true,\\\"actions\\\":[\\\"SCMP_ACT_ALLOW\\\",\\\"SCMP_ACT_ERRNO\\\",\\\"SCMP_ACT_KILL\\\",\\\"SCMP_ACT_KILL_PROCESS\\\",\\\"SCMP_ACT_KILL_THREAD\\\",\\\"SCMP_ACT_LOG\\\",\\\"SCMP_ACT_NOTIFY\\\",\\\"SCMP_ACT_TRACE\\\",\\\"SCMP_ACT_TRAP\\\"],\\\"operators\\\":[\\\"SCMP_CMP_EQ\\\",\\\"SCMP_CMP_GE\\\",\\\"SCMP_CMP_GT\\\",\\\"SCMP_CMP_LE\\\",\\\"SCMP_CMP_LT\\\",\\\"SCMP_CMP_MASKED_EQ\\\",\\\"SCMP_CMP_NE\\\"],\\\"archs\\\":[\\\"SCMP_ARCH_AARCH64\\\",\\\"SCMP_ARCH_ARM\\\",\\\"SCMP_ARCH_MIPS\\\",\\\"SCMP_ARCH_MIPS64\\\",\\\"SCMP_ARCH_MIPS64N32\\\",\\\"SCMP_ARCH_MIPSEL\\\",\\\"SCMP_ARCH_MIPSEL64\\\",\\\"SCMP_ARCH_MIPSEL64N32\\\",\\\"SCMP_ARCH_PPC\\\",\\\"SCMP_ARCH_PPC64\\\",\\\"SCMP_ARCH_PPC64LE\\\",\\\"SCMP_ARCH_RISCV64\\\",\\\"SCMP_ARCH_S390\\\",\\\"SCMP_ARCH_S390X\\\",\\\"SCMP_ARCH_X32\\\",\\\"SCMP_ARCH_X86\\\",\\\"SCMP_ARCH_X86_64\\\"],\\\"knownFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"],\\\"supportedFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"]},\\\"apparmor\\\":{\\\"enabled\\\":true},\\\"selinux\\\":{\\\"enabled\\\":true},\\\"intelRdt\\\":{\\\"enabled\\\":true},\\\"mountExtensions\\\":{\\\"idmap\\\":{\\\"enabled\\\":true}}},\\\"annotations\\\":{\\\"io.github.seccomp.libseccomp.version\\\":\\\"2.5.5\\\",\\\"org.opencontainers.runc.checkpoint.enabled\\\":\\\"true\\\",\\\"org.opencontainers.runc.commit\\\":\\\"\\\",\\\"org.opencontainers.runc.version\\\":\\\"1.3.0\\\\n\\\"},\\\"potentiallyUnsafeConfigAnnotations\\\":[\\\"bundle\\\",\\\"org.systemd.property.\\\",\\\"org.criu.config\\\"]}\"\n                    }\n                },\n                \"runc\": {\n                    \"path\": \"runc\",\n                    \"status\": {\n                        \"org.opencontainers.runtime-spec.features\": \"{\\\"ociVersionMin\\\":\\\"1.0.0\\\",\\\"ociVersionMax\\\":\\\"1.2.1\\\",\\\"hooks\\\":[\\\"prestart\\\",\\\"createRuntime\\\",\\\"createContainer\\\",\\\"startContainer\\\",\\\"poststart\\\",\\\"poststop\\\"],\\\"mountOptions\\\":[\\\"async\\\",\\\"atime\\\",\\\"bind\\\",\\\"defaults\\\",\\\"dev\\\",\\\"diratime\\\",\\\"dirsync\\\",\\\"exec\\\",\\\"iversion\\\",\\\"lazytime\\\",\\\"loud\\\",\\\"mand\\\",\\\"noatime\\\",\\\"nodev\\\",\\\"nodiratime\\\",\\\"noexec\\\",\\\"noiversion\\\",\\\"nolazytime\\\",\\\"nomand\\\",\\\"norelatime\\\",\\\"nostrictatime\\\",\\\"nosuid\\\",\\\"nosymfollow\\\",\\\"private\\\",\\\"ratime\\\",\\\"rbind\\\",\\\"rdev\\\",\\\"rdiratime\\\",\\\"relatime\\\",\\\"remount\\\",\\\"rexec\\\",\\\"rnoatime\\\",\\\"rnodev\\\",\\\"rnodiratime\\\",\\\"rnoexec\\\",\\\"rnorelatime\\\",\\\"rnostrictatime\\\",\\\"rnosuid\\\",\\\"rnosymfollow\\\",\\\"ro\\\",\\\"rprivate\\\",\\\"rrelatime\\\",\\\"rro\\\",\\\"rrw\\\",\\\"rshared\\\",\\\"rslave\\\",\\\"rstrictatime\\\",\\\"rsuid\\\",\\\"rsymfollow\\\",\\\"runbindable\\\",\\\"rw\\\",\\\"shared\\\",\\\"silent\\\",\\\"slave\\\",\\\"strictatime\\\",\\\"suid\\\",\\\"symfollow\\\",\\\"sync\\\",\\\"tmpcopyup\\\",\\\"unbindable\\\"],\\\"linux\\\":{\\\"namespaces\\\":[\\\"cgroup\\\",\\\"ipc\\\",\\\"mount\\\",\\\"network\\\",\\\"pid\\\",\\\"time\\\",\\\"user\\\",\\\"uts\\\"],\\\"capabilities\\\":[\\\"CAP_CHOWN\\\",\\\"CAP_DAC_OVERRIDE\\\",\\\"CAP_DAC_READ_SEARCH\\\",\\\"CAP_FOWNER\\\",\\\"CAP_FSETID\\\",\\\"CAP_KILL\\\",\\\"CAP_SETGID\\\",\\\"CAP_SETUID\\\",\\\"CAP_SETPCAP\\\",\\\"CAP_LINUX_IMMUTABLE\\\",\\\"CAP_NET_BIND_SERVICE\\\",\\\"CAP_NET_BROADCAST\\\",\\\"CAP_NET_ADMIN\\\",\\\"CAP_NET_RAW\\\",\\\"CAP_IPC_LOCK\\\",\\\"CAP_IPC_OWNER\\\",\\\"CAP_SYS_MODULE\\\",\\\"CAP_SYS_RAWIO\\\",\\\"CAP_SYS_CHROOT\\\",\\\"CAP_SYS_PTRACE\\\",\\\"CAP_SYS_PACCT\\\",\\\"CAP_SYS_ADMIN\\\",\\\"CAP_SYS_BOOT\\\",\\\"CAP_SYS_NICE\\\",\\\"CAP_SYS_RESOURCE\\\",\\\"CAP_SYS_TIME\\\",\\\"CAP_SYS_TTY_CONFIG\\\",\\\"CAP_MKNOD\\\",\\\"CAP_LEASE\\\",\\\"CAP_AUDIT_WRITE\\\",\\\"CAP_AUDIT_CONTROL\\\",\\\"CAP_SETFCAP\\\",\\\"CAP_MAC_OVERRIDE\\\",\\\"CAP_MAC_ADMIN\\\",\\\"CAP_SYSLOG\\\",\\\"CAP_WAKE_ALARM\\\",\\\"CAP_BLOCK_SUSPEND\\\",\\\"CAP_AUDIT_READ\\\",\\\"CAP_PERFMON\\\",\\\"CAP_BPF\\\",\\\"CAP_CHECKPOINT_RESTORE\\\"],\\\"cgroup\\\":{\\\"v1\\\":true,\\\"v2\\\":true,\\\"systemd\\\":true,\\\"systemdUser\\\":true,\\\"rdma\\\":true},\\\"seccomp\\\":{\\\"enabled\\\":true,\\\"actions\\\":[\\\"SCMP_ACT_ALLOW\\\",\\\"SCMP_ACT_ERRNO\\\",\\\"SCMP_ACT_KILL\\\",\\\"SCMP_ACT_KILL_PROCESS\\\",\\\"SCMP_ACT_KILL_THREAD\\\",\\\"SCMP_ACT_LOG\\\",\\\"SCMP_ACT_NOTIFY\\\",\\\"SCMP_ACT_TRACE\\\",\\\"SCMP_ACT_TRAP\\\"],\\\"operators\\\":[\\\"SCMP_CMP_EQ\\\",\\\"SCMP_CMP_GE\\\",\\\"SCMP_CMP_GT\\\",\\\"SCMP_CMP_LE\\\",\\\"SCMP_CMP_LT\\\",\\\"SCMP_CMP_MASKED_EQ\\\",\\\"SCMP_CMP_NE\\\"],\\\"archs\\\":[\\\"SCMP_ARCH_AARCH64\\\",\\\"SCMP_ARCH_ARM\\\",\\\"SCMP_ARCH_MIPS\\\",\\\"SCMP_ARCH_MIPS64\\\",\\\"SCMP_ARCH_MIPS64N32\\\",\\\"SCMP_ARCH_MIPSEL\\\",\\\"SCMP_ARCH_MIPSEL64\\\",\\\"SCMP_ARCH_MIPSEL64N32\\\",\\\"SCMP_ARCH_PPC\\\",\\\"SCMP_ARCH_PPC64\\\",\\\"SCMP_ARCH_PPC64LE\\\",\\\"SCMP_ARCH_RISCV64\\\",\\\"SCMP_ARCH_S390\\\",\\\"SCMP_ARCH_S390X\\\",\\\"SCMP_ARCH_X32\\\",\\\"SCMP_ARCH_X86\\\",\\\"SCMP_ARCH_X86_64\\\"],\\\"knownFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"],\\\"supportedFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"]},\\\"apparmor\\\":{\\\"enabled\\\":true},\\\"selinux\\\":{\\\"enabled\\\":true},\\\"intelRdt\\\":{\\\"enabled\\\":true},\\\"mountExtensions\\\":{\\\"idmap\\\":{\\\"enabled\\\":true}}},\\\"annotations\\\":{\\\"io.github.seccomp.libseccomp.version\\\":\\\"2.5.5\\\",\\\"org.opencontainers.runc.checkpoint.enabled\\\":\\\"true\\\",\\\"org.opencontainers.runc.commit\\\":\\\"\\\",\\\"org.opencontainers.runc.version\\\":\\\"1.3.0\\\\n\\\"},\\\"potentiallyUnsafeConfigAnnotations\\\":[\\\"bundle\\\",\\\"org.systemd.property.\\\",\\\"org.criu.config\\\"]}\"\n                    }\n                }\n            },\n            \"SecurityOptions\": [\n                \"name=seccomp,profile=builtin\",\n                \"name=cgroupns\"\n            ],\n            \"ServerVersion\": \"28.2.2\",\n            \"SwapLimit\": true,\n            \"Swarm\": {\n                \"ControlAvailable\": false,\n                \"Error\": \"\",\n                \"LocalNodeState\": \"inactive\",\n                \"NodeAddr\": \"\",\n                \"NodeID\": \"\",\n                \"RemoteManagers\": null\n            },\n            \"SystemTime\": \"2025-06-20T14:47:38.990529559+04:00\",\n            \"Warnings\": null\n        },\n        \"Name\": \"docker\"\n    },\n    \"Image\": \"quay.io/ramalama/intel-gpu:0.9\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/var/home/lephuong/.local/share/uv/tools/ramalama/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF/gemma-3-1b-it-Q4_K_M.gguf\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/var/home/lephuong/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.9.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "lephuongbg",
      "author_type": "User",
      "created_at": "2025-06-20T09:54:40Z",
      "updated_at": "2025-06-20T13:24:06Z",
      "closed_at": null,
      "labels": [
        "bug",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1572/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1572",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1572",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:29.813011",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Could you do a dry-run and look at the docker command we are executing, and tell us what you would have expected it to run?\n\nThe info looks like Podman info as well.\n\n",
          "created_at": "2025-06-20T10:44:29Z"
        },
        {
          "author": "lephuongbg",
          "body": "This is dry run command output:\n\n```\nRAMALAMA_CONTAINER_ENGINE=docker ramalama --dry-run serve --host 172.0.0.1 qwen2.5-coder:3b\ndocker run --rm --label ai.ramalama.model=qwen2.5-coder:3b --label ai.ramalama.engine=docker --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.port=8080 --label ai",
          "created_at": "2025-06-20T10:51:29Z"
        },
        {
          "author": "rhatdan",
          "body": "Interested in opening a PR? I think the thing to think about is there would be a differene on the llama-server in the --container versus --nocontainer use case.  Without a container the llama-server should be --host 127.0.0.1\n",
          "created_at": "2025-06-20T12:21:49Z"
        },
        {
          "author": "ericcurtin",
          "body": "Are you expecting docker does this or something else:\n\n```\ndocker run --rm -it -p 127.0.0.1:8080:80 ...\n```\n\n?\n\nI think the solution may be to introduce a --oci-port option, like we have a --oci-runtime option. That sets the -p option in docker. We have some options that kinda have a double meaning ",
          "created_at": "2025-06-20T13:23:54Z"
        }
      ]
    },
    {
      "issue_number": 1561,
      "title": "quay.io/ramalama/asahi:0.9 does not work",
      "body": "### Issue Description\n\nlatest asahi image does not work\n\n### Steps to reproduce the issue\n\nask ramalama to serve a model\n\n### Describe the results you received\n\nramalama exists with an error message\n\n### Describe the results you expected\n\nramalama starts\n\n### ramalama info output\n\n```yaml\nkraxel@dobby ~# ramalama --debug --image quay.io/ramalama/asahi:0.9 serve tinyllama\n2025-06-18 15:45:40 - DEBUG - Checking if 8080 is available\nOpenAI RESTAPI: http://localhost:8080\n2025-06-18 15:45:40 - DEBUG - run_cmd: nvidia-smi\n2025-06-18 15:45:40 - DEBUG - Working directory: None\n2025-06-18 15:45:40 - DEBUG - Ignore stderr: False\n2025-06-18 15:45:40 - DEBUG - Ignore all: False\n2025-06-18 15:45:40 - DEBUG - exec_cmd: podman run --rm --label ai.ramalama.model=tinyllama --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.port=8080 --label ai.ramalama.command=serve --device /dev/dri -e ASAHI_VISIBLE_DEVICES=1 -p 8080:8080 --security-opt=label=disable --cap-drop=all --security-opt=no-new-privileges --pull newer -t -i --label ai.ramalama --name ramalama_3ZV50m5MSZ --env=HOME=/tmp --init --mount=type=bind,src=/home/kraxel/.local/share/ramalama/store/ollama/tinyllama/tinyllama/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816,destination=/mnt/models/model.file,ro --mount=type=bind,src=/home/kraxel/.local/share/ramalama/store/ollama/tinyllama/tinyllama/snapshots/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816/chat_template_converted,destination=/mnt/models/chat_template.file,ro quay.io/ramalama/asahi:0.9 /usr/libexec/ramalama/ramalama-serve-core llama-server --port 8080 --model /mnt/models/model.file --jinja --alias tinyllama --ctx-size 2048 --temp 0.8 --cache-reuse 256 -v -ngl 999 --threads 4 --host 0.0.0.0\nWARNING: image platform (linux/amd64) does not match the expected platform (linux/arm64)\nDRM_IOCTL_ASAHI_GET_PARAMS failed: Inappropriate ioctl for device\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nmacbook air m2\n\n### Additional information\n\nGoing back to the 0.8 image works.",
      "state": "closed",
      "author": "kraxel",
      "author_type": "User",
      "created_at": "2025-06-18T13:46:59Z",
      "updated_at": "2025-06-20T12:26:28Z",
      "closed_at": "2025-06-20T12:01:57Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1561/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1561",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1561",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:30.065249",
      "comments": [
        {
          "author": "taronaeo",
          "body": "This appears to be an ARM64 issue as denoted by the warning\n```\nWARNING: image platform (linux/amd64) does not match the expected platform (linux/arm64)\n```\n\nTested on AMD64 and it is working fine. Let me test this on ARM64 and triage as well.",
          "created_at": "2025-06-18T13:53:11Z"
        },
        {
          "author": "taronaeo",
          "body": "Okay sorry, I can't test this because I'm not on Asahi. Will leave this for other maintainers who run Asahi to test",
          "created_at": "2025-06-18T14:03:56Z"
        },
        {
          "author": "ericcurtin",
          "body": "Version 0.9 and 0.9.2 of these images only have amd64 variants which is wrong. We will be moving away from this asahi image soon and just use the ramalama one, when this gets rebased onto the mesa version in Fedrora rawhide:\n\nslp/mesa-krunkit\n",
          "created_at": "2025-06-18T20:29:19Z"
        },
        {
          "author": "rhatdan",
          "body": "Should be fixed now.",
          "created_at": "2025-06-20T12:01:57Z"
        },
        {
          "author": "rhatdan",
          "body": "I must have screwed up on the upload.  Trying to repush images.\n",
          "created_at": "2025-06-20T12:26:28Z"
        }
      ]
    },
    {
      "issue_number": 1553,
      "title": "Suspect code in Model.garbage_collection",
      "body": "### Issue Description\n\n- `os.path.join(root, model_root, model_file)` is very unlikely to be correct, although I might be missing something here. If this leads to false misses, that can lead to unwanted file removals.\n- `file_has_a_symlink` is clearly initialized in an incorrect place, leading to missed file deletions.\n- (The O(N*M) file walk code does not scale ideally.)\n\n\n### Steps to reproduce the issue\n\nFound by code inspection.\n\n### Describe the results you received\n\nNotebookLM, when fed `is_symlink_to`+`garbage_collection`, finds these (+more) and does an OK job of expressing the details.\n\n### Describe the results you expected\n\nNo thoughts, except Aha! moments, when reading the code\n\n### ramalama info output\n\n```yaml\nN/A\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAs of commit 3f012ba00ee31fdd76c5e3f07d1434ab8e386598 .\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "mtrmac",
      "author_type": "User",
      "created_at": "2025-06-17T19:35:41Z",
      "updated_at": "2025-06-19T14:39:21Z",
      "closed_at": "2025-06-18T06:36:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1553/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "engelmi"
      ],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1553",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1553",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:30.263377",
      "comments": [
        {
          "author": "rhatdan",
          "body": "@engelmi PTAL",
          "created_at": "2025-06-17T21:05:05Z"
        },
        {
          "author": "engelmi",
          "body": "@mtrmac I assume these are the analyzed functions, right?\n`is_symlink_to`: https://github.com/containers/ramalama/blob/main/ramalama/model.py#L154-L161\n`garbage_collection`: https://github.com/containers/ramalama/blob/main/ramalama/model.py#L163-L181\nBoth are part of the old model store and aren't u",
          "created_at": "2025-06-18T06:36:12Z"
        },
        {
          "author": "ericcurtin",
          "body": "I'd love to see a demo of notebooklm, have never used :)\n\nWe can delete the old model store at this change, that ship has sailed",
          "created_at": "2025-06-19T14:39:21Z"
        }
      ]
    },
    {
      "issue_number": 1357,
      "title": "Cannot access local variable 'response' where it is not associated with a value",
      "body": "### Issue Description\n\n```\n$ ramalama run granite3-moe:3b\nðŸ¦­ > What's the best beer glass shape to minimize heat transfer?\nâ Traceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 180, in <module>\n    main(sys.argv[1:])\n    ~~~~^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 175, in main\n    run_shell_loop(ramalama_shell)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 159, in run_shell_loop\n    ramalama_shell.cmdloop()\n    ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib64/python3.13/cmd.py\", line 146, in cmdloop\n    stop = self.onecmd(line)\n  File \"/usr/lib64/python3.13/cmd.py\", line 223, in onecmd\n    return self.default(line)\n           ~~~~~~~~~~~~^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 108, in default\n    {\"role\": \"assistant\", \"content\": req(self.conversation_history, self.url, self.parsed_args.color)}\n                                     ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 81, in req\n    return res(response, color)\n               ^^^^^^^^\nUnboundLocalError: cannot access local variable 'response' where it is not associated with a value\n\n$ ramalama run granite3-moe:3b\nðŸ¦­ > What's the best beer glass shape to minimize heat transfer?\nâ Traceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 180, in <module>\n    main(sys.argv[1:])\n    ~~~~^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 175, in main\n    run_shell_loop(ramalama_shell)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 159, in run_shell_loop\n    ramalama_shell.cmdloop()\n    ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib64/python3.13/cmd.py\", line 146, in cmdloop\n    stop = self.onecmd(line)\n  File \"/usr/lib64/python3.13/cmd.py\", line 223, in onecmd\n    return self.default(line)\n           ~~~~~~~~~~~~^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 108, in default\n    {\"role\": \"assistant\", \"content\": req(self.conversation_history, self.url, self.parsed_args.color)}\n                                     ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 81, in req\n    return res(response, color)\n               ^^^^^^^^\nUnboundLocalError: cannot access local variable 'response' where it is not associated with a value\n\n$ ramalama run granite3-moe:3b\nðŸ¦­ > What's the best beer glass shape to minimize heat transfer?\nThe best beer glass shape to minimize heat transfer is the pint glass, also known as the English pint glass. This glass shape has a narrower top and wider bottom, which allows for a more efficient heat transfer between the beer and the environment. The wider bottom of the glass helps to keep the beer cooler for a longer period of time.\n```\n\n### Steps to reproduce the issue\n\nSee above.\n\n### Describe the results you received\n\nSee above.\n\n### Describe the results you expected\n\nNo crash or at least crashing every time.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"hip\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 97.61,\n                    \"systemPercent\": 0.54,\n                    \"userPercent\": 1.84\n                },\n                \"cpus\": 32,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2038,\n                \"hostname\": \"heidr\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.4-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 3613769728,\n                \"memTotal\": 67308138496,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250503.g587980c-1.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 466944,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"87h 18m 39.00s (Approximately 3.62 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/grayshade/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 8,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 8\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/grayshade/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1999860924416,\n                \"graphRootUsed\": 1109652234240,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 104\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/grayshade/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.0-rc2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1746057600,\n                \"BuiltTime\": \"Thu May  1 03:00:00 2025\",\n                \"GitCommit\": \"3c4cf521428ce11906976ddd08a5a6e8626cd50e\",\n                \"GoVersion\": \"go1.24.2\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.0-rc2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/rocm:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/grayshade/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nFedora 42\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "lnicola",
      "author_type": "User",
      "created_at": "2025-05-07T08:33:44Z",
      "updated_at": "2025-06-18T20:24:05Z",
      "closed_at": "2025-06-18T20:24:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1357/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1357",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1357",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:30.593945",
      "comments": [
        {
          "author": "jwboyer",
          "body": "@ericcurtin is this fixed by #1345 ?",
          "created_at": "2025-05-07T12:38:16Z"
        },
        {
          "author": "ieaves",
          "body": "@lnicola are you still experiencing this issue? So far as I can tell, things appear to be working on the most recent releases. If so, either @ericcurtin or @rhatdan it's probably okay to close this one.",
          "created_at": "2025-06-18T19:59:46Z"
        },
        {
          "author": "ericcurtin",
          "body": "Sorry @lnicola @jwboyer @ieaves , yes this was fixed already by a PR",
          "created_at": "2025-06-18T20:24:05Z"
        }
      ]
    },
    {
      "issue_number": 1557,
      "title": "Only one model supported, got 4",
      "body": "### Issue Description\n\nI'm trying to run the new granite 3.3 models from HF and trying to convert them to GUFF. I run `ramalama convert --gguf Q4_K_M hf://ibm-granite/granite-3.3-8b-instruct oci://quay.example.com/llms/granite-3.3-8b-instruct:20250618` but I end up with an error reading:\n\n```\nDownload complete. Moving file to /tmp/tmpce67gg4m/model-00001-of-00004.safetensorsâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4.97G/4.97G [02:21<00:00, 117MB/s]\nFetching 15 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:21<00:00,  9.46s/it]\nFailed to create new snapshot: Only one model supported, got 4: [<ramalama.huggingface.HuggingfaceCLIFile object at 0x7f8306386cf0>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x7f8306341590>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x7f8306341450>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x7f83063308a0>]\nRemoving snapshot...\nError: Only one model supported, got 4: [<ramalama.huggingface.HuggingfaceCLIFile object at 0x7f8306386cf0>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x7f8306341590>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x7f8306341450>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x7f83063308a0>]\n```\n\n### Steps to reproduce the issue\n\nrun something like\n\n```\nramalama convert --gguf Q4_K_M hf://ibm-granite/granite-3.3-8b-instruct oci://quay.example.com/llms/granite-3.3-8b-instruct:20250618\n```\n\n### Describe the results you received\n\nError and no converted model files\n\n### Describe the results you expected\n\nA model consisting of multiple files converted in a GGUF format\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"intel\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.1\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.32,\n                    \"systemPercent\": 0.26,\n                    \"userPercent\": 0.42\n                },\n                \"cpus\": 18,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"server\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2017,\n                \"hostname\": \"kuroi.thuisnet.com\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000008,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.11-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 16461479936,\n                \"memTotal\": 66688864256,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.15.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.15.0\"\n                    },\n                    \"package\": \"netavark-1.15.2-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.15.2\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250611.g0293c6f-1.fc42.x86_64\",\n                    \"version\": \"pasta 0^20250611.g0293c6f-1.fc42.x86_64\\nCopyright Red Hat\\nGNU General Public License, version 2 or later\\n  <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"5h 46m 28.00s (Approximately 0.21 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/maxim/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 18,\n                    \"paused\": 0,\n                    \"running\": 17,\n                    \"stopped\": 1\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/maxim/.local/share/containers/storage\",\n                \"graphRootAllocated\": 201745268736,\n                \"graphRootUsed\": 53595451392,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 16\n                },\n                \"runRoot\": \"/run/user/1000000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/maxim/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.1\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1749081600,\n                \"BuiltTime\": \"Thu Jun  5 02:00:00 2025\",\n                \"GitCommit\": \"850db76dd78a0641eddb9ee19ee6f60d2c59bcfa\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/intel-gpu:0.9\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/maxim/.virtualenvs/rama/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF/gemma-3-1b-it-Q4_K_M.gguf\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/maxim/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.9.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "wzzrd",
      "author_type": "User",
      "created_at": "2025-06-18T10:19:54Z",
      "updated_at": "2025-06-18T14:10:44Z",
      "closed_at": "2025-06-18T14:10:44Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1557/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "taronaeo"
      ],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1557",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1557",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:30.797566",
      "comments": [
        {
          "author": "taronaeo",
          "body": "Hi @wzzrd, thanks for filing this. Can confirm that it is a bug and I am triaging it now.",
          "created_at": "2025-06-18T13:13:56Z"
        },
        {
          "author": "engelmi",
          "body": "@taronaeo https://github.com/containers/ramalama/pull/1559 provides a quick fix for that specific error. No guarantee the convert works, though.",
          "created_at": "2025-06-18T13:16:10Z"
        },
        {
          "author": "wzzrd",
          "body": "> [@taronaeo](https://github.com/taronaeo) [#1559](https://github.com/containers/ramalama/pull/1559) provides a quick fix for that specific error. No guarantee the convert works, though.\n\ntesting that now\n",
          "created_at": "2025-06-18T13:16:54Z"
        },
        {
          "author": "wzzrd",
          "body": "tested and converted a model in safetensors format to gguf successfully",
          "created_at": "2025-06-18T13:23:27Z"
        },
        {
          "author": "taronaeo",
          "body": "@engelmi Triaged using `git bisect` to this commit. \n\n```diff\n0e2940becd68e53c5fc053e3168d6aacaa7556e5 is the first bad commit\ncommit 0e2940becd68e53c5fc053e3168d6aacaa7556e5\nAuthor: Michael Engel <mengel@redhat.com>\nDate:   Tue Apr 15 15:14:27 2025 +0200\n\n    Use new model store by default\n    \n   ",
          "created_at": "2025-06-18T13:27:51Z"
        }
      ]
    },
    {
      "issue_number": 1552,
      "title": "Exception starting RamaLama after a fresh install on macOS",
      "body": "### Issue Description\n\nI am trying to install and use RamaLama on my M1 Pro laptop with macOS 15.5 and using Python 3.13. I am using HomeBrew to manage the Python installations and to install RamaLama, but I have also seen this error when using `pip` to install. The installation works fine, but I get an exception when I try to run the RamaLama command.\n\n\n\n### Steps to reproduce the issue\n\n1. Install ramalama using either `pip install ramalama` or `brew install ramalama`\n2. Run the `ramalama` command\n\n### Describe the results you received\n\n`âžœ  /Users ramalama\nTraceback (most recent call last):\n  File \"/opt/homebrew/bin/ramalama\", line 8, in <module>\n    sys.exit(main())\n             ~~~~^^\n  File \"/opt/homebrew/Cellar/ramalama/0.9.2/libexec/lib/python3.13/site-packages/ramalama/cli.py\", line 1182, in main\n    parser, args = init_cli()\n                   ~~~~~~~~^^\n  File \"/opt/homebrew/Cellar/ramalama/0.9.2/libexec/lib/python3.13/site-packages/ramalama/cli.py\", line 123, in init_cli\n    configure_subcommands(parser)\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n  File \"/opt/homebrew/Cellar/ramalama/0.9.2/libexec/lib/python3.13/site-packages/ramalama/cli.py\", line 241, in configure_subcommands\n    chat_parser(subparsers)\n    ~~~~~~~~~~~^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/ramalama/0.9.2/libexec/lib/python3.13/site-packages/ramalama/cli.py\", line 919, in chat_parser\n    parser.add_argument(\"--prefix\", type=str, help=\"prefix for the user prompt\", default=default_prefix())\n                                                                                         ~~~~~~~~~~~~~~^^\n  File \"/opt/homebrew/Cellar/ramalama/0.9.2/libexec/lib/python3.13/site-packages/ramalama/chat.py\", line 54, in default_prefix\n    if os.path.basename(engine) == \"podman\":\n       ~~~~~~~~~~~~~~~~^^^^^^^^\n  File \"<frozen posixpath>\", line 168, in basename\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n`\n\n### Describe the results you expected\n\nI expected no errors to occur\n\n### ramalama info output\n\n```yaml\nâžœ  /Users ramalama info\nTraceback (most recent call last):\n  File \"/opt/homebrew/bin/ramalama\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/ramalama/cli.py\", line 1182, in main\n    parser, args = init_cli()\n                   ^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/ramalama/cli.py\", line 123, in init_cli\n    configure_subcommands(parser)\n  File \"/opt/homebrew/lib/python3.11/site-packages/ramalama/cli.py\", line 241, in configure_subcommands\n    chat_parser(subparsers)\n  File \"/opt/homebrew/lib/python3.11/site-packages/ramalama/cli.py\", line 919, in chat_parser\n    parser.add_argument(\"--prefix\", type=str, help=\"prefix for the user prompt\", default=default_prefix())\n                                                                                         ^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/ramalama/chat.py\", line 54, in default_prefix\n    if os.path.basename(engine) == \"podman\":\n       ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen posixpath>\", line 142, in basename\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "alanfx",
      "author_type": "User",
      "created_at": "2025-06-17T19:24:51Z",
      "updated_at": "2025-06-18T13:08:10Z",
      "closed_at": "2025-06-18T09:19:25Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1552/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1552",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1552",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:30.989683",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Definitely looks like a screwed up install",
          "created_at": "2025-06-17T21:06:35Z"
        },
        {
          "author": "holehan",
          "body": "Same issue here (macOS 15.5, homebrew, M1 Pro), my workaround:\n\n```sh\nRAMALAMA_CONTAINER_ENGINE=docker ramalama info\n```",
          "created_at": "2025-06-18T08:08:45Z"
        },
        {
          "author": "rhatdan",
          "body": "Ok I see the problem and will fix it.\n\n```\n\ndef default_prefix():\n    if \"LLAMA_PROMPT_PREFIX\" in os.environ:\n        return os.environ[\"LLAMA_PROMPT_PREFIX\"]\n\n    if not EMOJI:\n        return \"> \"\n\n    engine = CONFIG.engine\n\n    if engine:\n        if os.path.basename(engine) == \"podman\":\n         ",
          "created_at": "2025-06-18T08:57:12Z"
        },
        {
          "author": "alanfx",
          "body": "Thanks, for the quick fix!",
          "created_at": "2025-06-18T13:08:10Z"
        }
      ]
    },
    {
      "issue_number": 1546,
      "title": "keeping shortnames up to date",
      "body": "### Feature request description\n\nI am wondering what/how to keep the list of shortnames up-to-date.\nIt can be challenging due to the fast pace of AI model development.\nWhat is Ramalama general policy here?\n\nSome examples:\n\n- latest Granite is 3.3 (vs 3.1)  [4.0 preview even exists now]\n- Mistral 3.1 (`mistral-small-2503` exists)\n- Qwen 3 exists (vs 2.5)\n- latest Phi v4 (vs 2)\n\nI guess there is a bit of a trade-off too - stability vs latest-greatest.\n\n\n### Suggest potential solution\n\nI wonder if it makes sense to focus more on `hf://ggml-org/` in general say?\n\nIs there any way to provide more unversioned or `latest` aliases in general?\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "open",
      "author": "juhp",
      "author_type": "User",
      "created_at": "2025-06-17T05:56:27Z",
      "updated_at": "2025-06-17T15:59:06Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1546/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1546",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1546",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:31.195609",
      "comments": [
        {
          "author": "rhatdan",
          "body": "We have no great way yet other then community suggesting changes,  I don't know if renovate would help here.\n\nI would love to get a way to do better then just plain shortnames, and actually pick out the model which would work best on a particular hardware. But I would need data scientist help for th",
          "created_at": "2025-06-17T09:06:18Z"
        },
        {
          "author": "ericcurtin",
          "body": "> ### Feature request description\n> I am wondering what/how to keep the list of shortnames up-to-date. It can be challenging due to the fast pace of AI model development. What is Ramalama general policy here?\n> \n> Some examples:\n> \n> * latest Granite is 3.3 (vs 3.1)  [4.0 preview even exists now]\n> ",
          "created_at": "2025-06-17T10:30:54Z"
        },
        {
          "author": "ericcurtin",
          "body": "I do think we should be avoiding aliases like this:\n\n  \"granite:2b\" = \"ollama://granite3.1-dense:2b\"\n  \"granite:8b\" = \"ollama://granite3.1-dense:8b\"\n\nI mean they had a name already as designated by the creator/author of the model. And now people complain the version is out of date because we brought",
          "created_at": "2025-06-17T10:35:51Z"
        },
        {
          "author": "rhatdan",
          "body": "The question I have is could we do better, and have some smarts around when to use dense versus moe (I have no idea what the difference is) But users would like a tool like RamaLama to pick the `best` one for their particular hardware.\n",
          "created_at": "2025-06-17T11:07:51Z"
        },
        {
          "author": "ericcurtin",
          "body": "dense vs moe is not a hardware thing, it's just the type of model you choose. Like:\n\ndense\nmoe\nreasoning\netc.\n",
          "created_at": "2025-06-17T13:50:02Z"
        }
      ]
    },
    {
      "issue_number": 1259,
      "title": "ramalama-cli image for 0.7.5 reports 0.7.4 version",
      "body": "### Issue Description\n\n```bash\npodman run quay.io/ramalama/ramalama-cli:0.7.5 version\nramalama version 0.7.4\n```\n\nI don't know if it's a output issue or if we really have 0.7.4 inside ?\n\n### Steps to reproduce the issue\n\nrun `podman run quay.io/ramalama/ramalama-cli:0.7.5 version`\n\n### Describe the results you received\n\n0.7.4\n\n### Describe the results you expected\n\n0.7.5\n\n### ramalama info output\n\n```yaml\nN/A\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-23T22:01:46Z",
      "updated_at": "2025-06-17T15:02:45Z",
      "closed_at": "2025-06-17T15:02:45Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1259/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1259",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1259",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:31.389686",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Somewhat a chicken and an egg scenario, but we could theoretically release first, push containers after (default to using :latest when new container isn't available yet)",
          "created_at": "2025-04-23T22:05:17Z"
        },
        {
          "author": "rhatdan",
          "body": "This is actually  a mistake.  I did update the version in the X86_64, I believe.  Can not check right now, since I am on plane. MAC Version might not have been updated.",
          "created_at": "2025-04-24T10:55:11Z"
        },
        {
          "author": "rhatdan",
          "body": "The way I did it on X86 was to update version, build images and then release.\n But this is definitely a chicken and egg.",
          "created_at": "2025-04-24T10:56:14Z"
        },
        {
          "author": "taronaeo",
          "body": "Closing this since we are on `v0.9.2` now and tested to be working fine as of this release.\n\n```sh\n$ podman run quay.io/ramalama/ramalama-cli:0.9.2 version\n\nTrying to pull quay.io/ramalama/ramalama-cli:0.9.2...\nGetting image source signatures\nCopying blob dfadf50b5776 done   | \nCopying blob 05b552b1",
          "created_at": "2025-06-17T15:02:45Z"
        }
      ]
    },
    {
      "issue_number": 1482,
      "title": "Fallback to Vulkan on unsupported AMD GPUs",
      "body": "### Feature request description\n\nWhen using GPUs that are not supported by ROCm (i.e. RX 580/gfx803), Ramalama should use the Vulkan image automatically instead.\n\n### Suggest potential solution\n\n`amdkfd` driver exports `gfx_target_version` in `/sys/class/kfd/kfd/topology/nodes/*/properties` that can be used to detect the GPU architecture. Use it to disable the ROCm image for pre-`gfx900` GPUs.\n\n### Have you considered any alternatives?\n\nUser can manually override the image in their configuration file, but I would prefer this working out of the box.\n\nAlternatively, a custom ROCm build could be done to enable support for gfx8 GPUs, however the performance is ~20x slower than the Vulkan image in my own testing.\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "alaviss",
      "author_type": "User",
      "created_at": "2025-06-08T14:29:13Z",
      "updated_at": "2025-06-17T09:20:03Z",
      "closed_at": "2025-06-17T09:20:03Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1482/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1482",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1482",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:31.595134",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "This makes perfect sense to me, wanna open a PR @alaviss ?\n\nAnother secondary benefit of the is we can remove all the pre-gfx9 files from the container image. Haven't measured but it could save GBs in image size.",
          "created_at": "2025-06-08T14:44:39Z"
        },
        {
          "author": "Split7fire",
          "body": "Also interested in working inference on not-so-old RX5700XT (gfx1010). Where can I find *Vulkan* image?",
          "created_at": "2025-06-09T16:58:22Z"
        },
        {
          "author": "alaviss",
          "body": "> wanna open a PR?\n\nI can't promise that I will have time to work on this, unfortunately.\n\n> Where can I find _Vulkan_ image?\n\nThe CPU image is actually the Vulkan image. Just add this into your `ramalama.conf`:\n\n```\n[ramalama.images]\nHIP_VISIBLE_DEVICES = \"quay.io/ramalama/ramalama\"\n```",
          "created_at": "2025-06-09T17:03:55Z"
        },
        {
          "author": "Split7fire",
          "body": "@alaviss Strange, I tried to create config in `/etc` and in `~/.config` but seem nothing happened. I have ramalama 0.8.5. Can it be the issue?",
          "created_at": "2025-06-11T14:27:14Z"
        },
        {
          "author": "alaviss",
          "body": "It shouldn't be, configuration support was added long ago. It's likely that you just placed the file at the wrong path. See [`ramalama.conf(5)`](https://github.com/containers/ramalama/blob/003612abf7ef6f6b9a0de0ebbccb5ae13c3f1f6d/docs/ramalama.conf.5.md) for where ramalama looks for configuration fi",
          "created_at": "2025-06-11T17:51:09Z"
        }
      ]
    },
    {
      "issue_number": 1533,
      "title": "AI models in microVMs",
      "body": "### Feature request description\n\nWe either already have some of this functionality:\n\nhttps://github.com/microsandbox/microsandbox\n\nIt should be just:\n\n```\nramalama run --oci-runtime krun smollm:135m\n```\n\nNeeds testing to ensure GPU access works. We should do a follow on blog post to this:\n\nhttps://developers.redhat.com/articles/2025/02/20/how-ramalama-runs-ai-models-isolation-default\n\nbut taking it one step further, encapsulating in microVMs. Tagging @slp for awareness.\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-06-16T09:53:11Z",
      "updated_at": "2025-06-16T22:54:51Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1533/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1533",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1533",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:31.821856",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "First attempt, GPU passthrough isn't working (I tried with AMD GPU) and there's some weird issue where \">\" displays as \"e\", could be because of the emoji or TERM or whatever\n\n```\nramalama run --oci-runtime krun smollm:135m\nðŸ¦­ e\n```\n",
          "created_at": "2025-06-16T10:14:53Z"
        },
        {
          "author": "slp",
          "body": "I'm assuming the goal is to make it work on Linux, isn't it?",
          "created_at": "2025-06-16T12:46:36Z"
        },
        {
          "author": "rhatdan",
          "body": "Well the goal would be to make this work on a MAC to launch a Linux container with GPU Accelleration.",
          "created_at": "2025-06-16T13:13:46Z"
        },
        {
          "author": "ericcurtin",
          "body": "@slp I intended Linux... I think that would be a good start... We could potentially do macOS as a follow on I guess if we thought it was worth it... Like the above command I tried was Linux with AMD GPU (ROCm)",
          "created_at": "2025-06-16T16:00:38Z"
        },
        {
          "author": "rhatdan",
          "body": "Sure, I thought you wanted to compete against container.\n\nWhat is the value of Linux, where I can just use traditional containers?  Currently can krun leak random GPUs like nvidia into the VM?",
          "created_at": "2025-06-16T19:44:04Z"
        }
      ]
    },
    {
      "issue_number": 1524,
      "title": "No user-directed error if model fails to load",
      "body": "### Issue Description\n\nWhen using `ramalama run`, if the model fails to load (for example, because it exceeds the GPU memory size), no user-directed error is printed. Instead the prompt is displayed as usual. When actually trying to use it, a cryptic `Error: could not connect to: http://127.0.0.1:8080/v1/chat/completions` error appears.\n\n### Steps to reproduce the issue\n\n```\nramalama run granite-code:34b\n```\nOn a system that does not have enough GPU memory for this model.\n\n### Describe the results you received\n\n```\nðŸ¦­ > test\nError: could not connect to: http://127.0.0.1:8080/v1/chat/completions\n```\n\n### Describe the results you expected\n\nRunning the command with `--debug` prints the following information (before still showing the prompt):\n```\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 20199.32 MiB on device 0: cudaMalloc failed: out of memory\nalloc_tensor_range: failed to allocate CUDA0 buffer of size 21180522496\nllama_model_load: error loading model: unable to allocate CUDA0 buffer\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model '/mnt/models/model.file'\nsrv    load_model: failed to load model, '/mnt/models/model.file'\nsrv    operator(): operator(): cleaning up before exit...\nmain: exiting due to model loading error\n```\n\nI would expect to see *some* kind of indication that the model failed to load even when not using the `--debug` flag.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 95.98,\n                    \"systemPercent\": 0.47,\n                    \"userPercent\": 3.55\n                },\n                \"cpus\": 32,\n                \"databaseBackend\": \"boltdb\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 1901,\n                \"hostname\": \"fedora.fritz.box\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.9-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 16767385600,\n                \"memTotal\": 98703872000,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.15.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.15.0\"\n                    },\n                    \"package\": \"netavark-1.15.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.15.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250512.g8ec1341-1.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8218898432,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"8h 31m 6.00s (Approximately 0.33 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/npopov/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 147,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 147\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/npopov/.local/share/containers/storage\",\n                \"graphRootAllocated\": 998483427328,\n                \"graphRootUsed\": 764710838272,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 53\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/npopov/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.0\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1747180800,\n                \"BuiltTime\": \"Wed May 14 02:00:00 2025\",\n                \"GitCommit\": \"0dbcb51477ee7ab8d3b47d30facf71fc38bb0c98\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.9\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/npopov/.local/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/npopov/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.9.1\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "nikic",
      "author_type": "User",
      "created_at": "2025-06-13T16:05:17Z",
      "updated_at": "2025-06-16T16:58:22Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1524/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1524",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1524",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.039674",
      "comments": [
        {
          "author": "nikic",
          "body": "It's probably worth noting that it's also possible to get the `Error: could not connect to: http://127.0.0.1:8080/v1/chat/completions` error also in the case where you enter input too quickly, especially for a model running on the CPU. For example:\n\n```\nnpopov@fedora:~/repos$ ramalama --image quay.i",
          "created_at": "2025-06-13T16:15:39Z"
        },
        {
          "author": "taronaeo",
          "body": "Actually I think Ollama throws a 500 status code in these scenarios right? But yes, this would definitely be a better user-experience improvement that we should make. Would you like to open a PR? ðŸ˜ƒ ",
          "created_at": "2025-06-16T16:57:46Z"
        }
      ]
    },
    {
      "issue_number": 1525,
      "title": "Invalid argument to --image silently ignored",
      "body": "### Issue Description\n\nPassing an invalid argument to `--image` silently ignores the argument and falls back to the default, in my case `quay.io/ramalama/cuda:0.9`.\n\nThe case I actually ran into is of course not `--image foobar`, but rather `--image quay.io/ramalama/ramalama` trying to force execution on the CPU. This worked on ramalama 0.8, but on ramalama 0.9 it only works if the version is specified as `--image quay.io/ramalama/ramalama:0.9`, otherwise it just fails silently.\n\n### Steps to reproduce the issue\n\n```\nramalama --image foobar run granite:8b\n```\n\n### Describe the results you received\n\nNo error, fallback to `quay.io/ramalama/cuda:0.9`.\n\n### Describe the results you expected\n\nError that the image is invalid.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 96.01,\n                    \"systemPercent\": 0.47,\n                    \"userPercent\": 3.51\n                },\n                \"cpus\": 32,\n                \"databaseBackend\": \"boltdb\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 1901,\n                \"hostname\": \"fedora.fritz.box\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.9-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 30134116352,\n                \"memTotal\": 98703872000,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.15.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.15.0\"\n                    },\n                    \"package\": \"netavark-1.15.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.15.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250512.g8ec1341-1.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8242814976,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"8h 42m 11.00s (Approximately 0.33 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/npopov/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 147,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 147\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/npopov/.local/share/containers/storage\",\n                \"graphRootAllocated\": 998483427328,\n                \"graphRootUsed\": 764713713664,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 53\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/npopov/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.0\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1747180800,\n                \"BuiltTime\": \"Wed May 14 02:00:00 2025\",\n                \"GitCommit\": \"0dbcb51477ee7ab8d3b47d30facf71fc38bb0c98\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.9\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/npopov/.local/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/npopov/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.9.1\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "nikic",
      "author_type": "User",
      "created_at": "2025-06-13T16:13:01Z",
      "updated_at": "2025-06-16T13:55:46Z",
      "closed_at": "2025-06-16T13:55:46Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1525/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1525",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1525",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.294187",
      "comments": []
    },
    {
      "issue_number": 1509,
      "title": "Reasoning flag",
      "body": "### Feature request description\n\nMore great work from @ochafik , we should look into the flag, maybe add it as an explicit option to RamaLama document it:\n\nhttps://github.com/ggml-org/llama.cpp/pull/13771\n\nIt can be used to define the amount of reasoning you want, to the point one can completely turn off reasoning if they'd like.\n\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-06-12T07:15:45Z",
      "updated_at": "2025-06-15T20:27:47Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1509/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1509",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1509",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.294209",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "@rhatdan the alias we have:\n\ndeepseek -> deepseek-r1\n\nis sometimes questionable.\n\ndeepseek-r1 is the reasoning one\ndeepseek-v3 is the non-reasoning one (it's MoE technically)\n\nThere's also more specific ones like:\n\ndeepseek-coder\n\nI guess deepseek-v3 is the most generic one, but reasoning via deepse",
          "created_at": "2025-06-12T07:17:23Z"
        },
        {
          "author": "rhatdan",
          "body": "Well shouldwe change deepseek -> deepseek-v3?\n",
          "created_at": "2025-06-15T20:24:46Z"
        },
        {
          "author": "ericcurtin",
          "body": "> Well shouldwe change deepseek -> deepseek-v3?\n\nI actually think that's worse, because r1 is the \"famous\" one.\n\ndeepseek-r2 should be out soon.\n\n",
          "created_at": "2025-06-15T20:27:47Z"
        }
      ]
    },
    {
      "issue_number": 1368,
      "title": "README example failing with `ModuleNotFoundError: No module named 'ramalama'`",
      "body": "### Issue Description\n\nI tried running `ramalama run granite3-moe`, which is the example from the README, and I ran into this error:\n\n```\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/ramalama/0.8.2/libexec/libexec/ramalama/ramalama-run-core\", line 101, in <module>\n    main(sys.argv)\n    ~~~~^^^^^^^^^^\n  File \"/usr/local/Cellar/ramalama/0.8.2/libexec/libexec/ramalama/ramalama-run-core\", line 60, in main\n    from ramalama.cli import serve_cli\nModuleNotFoundError: No module named 'ramalama'\n```\n\n\n\n### Steps to reproduce the issue\n\nTo install ramalama, I ran `curl -fsSL https://raw.githubusercontent.com/containers/ramalama/s/install.sh | bash`\n\nThen, I ran `ramalama run granite3-moe`\n\n### Describe the results you received\n\nI ran into this error:\n\n```\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/ramalama/0.8.2/libexec/libexec/ramalama/ramalama-run-core\", line 101, in <module>\n    main(sys.argv)\n    ~~~~^^^^^^^^^^\n  File \"/usr/local/Cellar/ramalama/0.8.2/libexec/libexec/ramalama/ramalama-run-core\", line 60, in main\n    from ramalama.cli import serve_cli\nModuleNotFoundError: No module named 'ramalama'\n```\n\n\n### Describe the results you expected\n\nI expected the command to execute successfully pulling and running the model.\n\n### ramalama info output\n\n```yaml\n$ ramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Name\": null\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/local/Cellar/ramalama/0.8.2/libexec/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/Users/stevenkrawczyk/.local/share/ramalama\",\n    \"UseContainer\": false,\n    \"Version\": \"0.8.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "steventkrawczyk",
      "author_type": "User",
      "created_at": "2025-05-09T00:22:12Z",
      "updated_at": "2025-06-15T19:49:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 25,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1368/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ericcurtin"
      ],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1368",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1368",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.495534",
      "comments": []
    },
    {
      "issue_number": 1115,
      "title": "Distributed inferencing",
      "body": "A decent reference implementation:\n\nhttps://github.com/exo-explore/exo",
      "state": "closed",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-04-03T16:12:39Z",
      "updated_at": "2025-06-13T14:36:51Z",
      "closed_at": "2025-06-13T14:36:51Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1115/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1115",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1115",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.495554",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "https://github.com/containers/ramalama/discussions/1092",
          "created_at": "2025-04-03T16:18:11Z"
        },
        {
          "author": "ericcurtin",
          "body": "Closing for now... But we would probably steer towards vllm-d if we were to implement",
          "created_at": "2025-06-13T14:36:51Z"
        }
      ]
    },
    {
      "issue_number": 1306,
      "title": "Using ramalama server or run with --rag induces a core dump",
      "body": "### Issue Description\n\nI'm attempting to experiment with ramalama and RAG (per https://developers.redhat.com/articles/2025/04/03/simplify-ai-data-integration-ramalama-and-rag) and am running into an issue. When I try to serve or run ramalama with rag (i.e. `$ ramalama run --rag quay.io/myrepository/ragdata MODEL`) it drops into a prompt ('>'), without the prompt prefix in both cases, and if you enter anything in the prompt you get a stack trace ending in an APIConnectionError followed by a core dump. If you leave the prompt up and try `curl localhost:8080` you'll get a connection reset, which is the same message in the aforementioned APIConnectionError. Seems like llama-server (or something else?) on the image is hanging, then crashing when it gets any kind of interaction. I even ran the command and when the prompt was available, attempted podman attach and that caused a core dump too.\n\n### Steps to reproduce the issue\n\n1. Run the following (using real files, etc):\n\n```\n$ ramalama rag test.pdf quay.io/myrepository/ragdata\n....\n$ ramalama run --rag quay.io/myrepository/ragdata mistral:latest\n```\n\n2. Observe the process borking\n\n### Describe the results you received\n\n```\n$ ramalama --debug run --rag quay.io/csutherl/ragdata mistral:latest\nrun_cmd:  podman image inspect quay.io/csutherl/ragdata\nWorking directory: None\nIgnore stderr: False\nIgnore all: False\nCommand finished with return code: 0\nChecking if 8080 is available\nrun_cmd:  podman inspect quay.io/ramalama/intel-gpu-rag:0.7\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nrun_cmd:  podman inspect quay.io/ramalama/intel-gpu-rag:0.7\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nexec_cmd:  podman run --rm -i --label ai.ramalama --name ramalama_oIzcQcNjjf --env=HOME=/tmp --init --security-opt=label=disable --cap-drop=all --security-opt=no-new-privileges --label ai.ramalama.model=mistral:latest --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.port=8080 --label ai.ramalama.command=run --env LLAMA_PROMPT_PREFIX=ðŸ¦­ >  --pull=newer -t -p 8080:8080 --device /dev/dri --device /dev/accel -e INTEL_VISIBLE_DEVICES=1 --network bridge --mount=type=image,source=quay.io/csutherl/ragdata,destination=/rag,rw=true --mount=type=bind,src=/home/csutherl/.local/share/ramalama/models/ollama/mistral:latest,destination=/mnt/models/model.file,ro quay.io/ramalama/intel-gpu-rag:0.7 bash -c nohup llama-server --port 8080 --model /mnt/models/model.file --alias mistral:latest --ctx-size 2048 --temp 0.8 --jinja --cache-reuse 256 -v -ngl 999 --threads 11 --host 0.0.0.0 &> /tmp/llama-server.log & rag_framework run /rag/vector.db\n> test\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n        pool_request.request\n    )\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/lib64/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/openai/_base_client.py\", line 969, in request\n    response = self._client.send(\n        request,\n        stream=stream or self._should_stream_response_body(request=request),\n        **kwargs,\n    )\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n        request,\n    ...<2 lines>...\n        history=[],\n    )\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n        request,\n        follow_redirects=follow_redirects,\n        history=history,\n    )\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib64/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/bin/rag_framework\", line 217, in <module>\n    args.func(args.vector_path)  # pass vector_path argument to the respective function\n    ~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/usr/bin/rag_framework\", line 182, in run_rag\n    rag.cmdloop()\n    ~~~~~~~~~~~^^\n  File \"/usr/lib64/python3.13/cmd.py\", line 146, in cmdloop\n    stop = self.onecmd(line)\n  File \"/usr/lib64/python3.13/cmd.py\", line 223, in onecmd\n    return self.default(line)\n           ~~~~~~~~~~~~^^^^^^\n  File \"/usr/bin/rag_framework\", line 143, in default\n    self.query(user_content)\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/usr/bin/rag_framework\", line 105, in query\n    response = self.llm.chat.completions.create(\n        model=\"your-model-name\",\n        messages=[{\"role\": \"user\", \"content\": metaprompt}],\n        stream=True\n    )\n  File \"/usr/local/lib/python3.13/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n    return self._post(\n           ~~~~~~~~~~^\n        \"/chat/completions\",\n        ^^^^^^^^^^^^^^^^^^^^\n    ...<43 lines>...\n        stream_cls=Stream[ChatCompletionChunk],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/openai/_base_client.py\", line 1239, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/openai/_base_client.py\", line 1001, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\nbash: line 1:     3 Aborted                 (core dumped) nohup llama-server --port 8080 --model /mnt/models/model.file --alias mistral:latest --ctx-size 2048 --temp 0.8 --jinja --cache-reuse 256 -v -ngl 999 --threads 11 --host 0.0.0.0 &> /tmp/llama-server.log\n```\n\n### Describe the results you expected\n\nA valid response and not a crashed process.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"intel\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 98.36,\n                    \"systemPercent\": 0.41,\n                    \"userPercent\": 1.23\n                },\n                \"cpus\": 22,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2039,\n                \"hostname\": \"myfedora\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 17833,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 165536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 17833,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 165536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.2-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 9243951104,\n                \"memTotal\": 66819031040,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/17833/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/17833/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8588644352,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"290h 0m 4.00s (Approximately 12.08 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/csutherl/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 9,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 9\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/csutherl/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1022488809472,\n                \"graphRootUsed\": 52024283136,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 5\n                },\n                \"runRoot\": \"/run/user/17833/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/csutherl/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Tue Apr  1 20:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/intel-gpu:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/csutherl/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "csutherl",
      "author_type": "User",
      "created_at": "2025-04-29T15:21:09Z",
      "updated_at": "2025-06-13T09:55:59Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1306/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1306",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1306",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.713382",
      "comments": [
        {
          "author": "rhatdan",
          "body": "@ericcurtin @bmahabirbu PTAL",
          "created_at": "2025-04-29T16:32:44Z"
        },
        {
          "author": "bmahabirbu",
          "body": "Tested on M4 and cannot reproduce the error with 0.8 images. I have a feeling it could be the intel-gpu image potentially\n\n@csutherl can you try doing this with specifying the cpu only image `ramalama --image quay.io/ramalama/ramalama ...` and see if that works? ",
          "created_at": "2025-04-29T17:56:40Z"
        },
        {
          "author": "csutherl",
          "body": "@bmahabirbu yeah, that immediately fails with a ModuleNotFoundError:\n\n```\n$ ramalama --debug --image quay.io/ramalama/ramalama run --rag quay.io/csutherl/ragdata mistral:latest\nrun_cmd:  podman image inspect quay.io/csutherl/ragdata\nWorking directory: None\nIgnore stderr: False\nIgnore all: False\nComm",
          "created_at": "2025-04-29T18:12:07Z"
        },
        {
          "author": "rhatdan",
          "body": "ramalama --debug --image **quay.io/ramalama/ramalama-rag** run --rag quay.io/csutherl/ragdata mistral:latest\n\nNeed to use ramalama-rag image.",
          "created_at": "2025-04-29T20:02:36Z"
        },
        {
          "author": "rhatdan",
          "body": "@afazekas PTAL",
          "created_at": "2025-04-29T20:03:53Z"
        }
      ]
    },
    {
      "issue_number": 1521,
      "title": "quay.io/ramalama/ramalama-rag:0.8.5 is broken",
      "body": "### Issue Description\n\nOn Fedora 42, I have python3-ramalama.noarch 0.8.5-1.fc42.\n\nTry:\n```\n$ echo \"Something something about something.\" > out.md\n$ ramalama rag out.md localhost/myrag:0.1\n\nBuilding localhost/myrag:0.1 ...\nadding vectordb ...\n77cb9e6bb889865a049f6e595a5e6105a70febba556fad87ad45ca54f13a8a22\n$ ramalama run --rag localhost/myrag:0.1 ollama://gemma3/gemma3:4b\n> hello\nTraceback (most recent call last):\n  File \"/usr/lib/python3.9/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/lib/python3.9/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/usr/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/usr/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n  File \"/usr/lib/python3.9/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/usr/lib/python3.9/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n  File \"/usr/lib/python3.9/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/usr/lib/python3.9/site-packages/httpcore/_backends/sync.py\", line 215, in connect_tcp\n    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n  File \"/usr/lib64/python3.9/contextlib.py\", line 137, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/usr/lib/python3.9/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.9/site-packages/openai/_base_client.py\", line 969, in request\n    response = self._client.send(\n  File \"/usr/lib/python3.9/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/usr/lib/python3.9/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/usr/lib/python3.9/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/usr/lib/python3.9/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/usr/lib/python3.9/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/usr/lib64/python3.9/contextlib.py\", line 137, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/usr/lib/python3.9/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/bin/rag_framework\", line 217, in <module>\n    args.func(args.vector_path)  # pass vector_path argument to the respective function\n  File \"/usr/bin/rag_framework\", line 182, in run_rag\n    rag.cmdloop()\n  File \"/usr/lib64/python3.9/cmd.py\", line 138, in cmdloop\n    stop = self.onecmd(line)\n  File \"/usr/lib64/python3.9/cmd.py\", line 216, in onecmd\n    return self.default(line)\n  File \"/usr/bin/rag_framework\", line 143, in default\n    self.query(user_content)\n  File \"/usr/bin/rag_framework\", line 105, in query\n    response = self.llm.chat.completions.create(\n  File \"/usr/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.9/site-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n    return self._post(\n  File \"/usr/lib/python3.9/site-packages/openai/_base_client.py\", line 1239, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/usr/lib/python3.9/site-packages/openai/_base_client.py\", line 1001, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n```\n\nAlso, the running container is quay.io/ramalama/ramalama-rag:0.8 and we see\n```\nbash-5.1# cat /tmp/llama-server.log\nTraceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 17, in <module>\n    main(sys.argv[1:])\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 9, in main\n    from ramalama.common import exec_cmd\nModuleNotFoundError: No module named 'ramalama'\n```\n\nThis is because the ramalama modules are installed for python 3.11 (`/usr/lib/python3.11/site-packages/ramalama`), while `python3 --version` gives Python 3.9.21.\n\n```\n$ for v in 0.8.1 0.8.3 0.8.5 0.9.0 ; do \\\n    echo \">>> VERSION $v\"; \\\n    podman run -ti \"quay.io/ramalama/ramalama-rag:$v\" \\\n      bash -c 'find / 2>/dev/null | grep ramalama/common.py; python3 --version; ls -lad /usr/bin/python3' ; \\\n    podman images | grep \"/ramalama-rag.*$v\" ; \\\ndone\n>>> VERSION 0.8.1\n/usr/lib/python3.9/site-packages/ramalama/common.py\nPython 3.11.9\nlrwxrwxrwx. 1 root root 19 Apr 28 11:40 /usr/bin/python3 -> /usr/bin/python3.11\nquay.io/ramalama/ramalama-rag                                 0.8.1              d6d8d4f0ed25  6 weeks ago        3.97 GB\n>>> VERSION 0.8.3\n/usr/lib/python3.11/site-packages/ramalama/common.py\nPython 3.11.9\nlrwxrwxrwx. 1 root root 19 May 12 10:39 /usr/bin/python3 -> /usr/bin/python3.11\nquay.io/ramalama/ramalama-rag                                 0.8.3              2cebb6ffbf75  4 weeks ago        4.13 GB\n>>> VERSION 0.8.5\n/usr/lib/python3.11/site-packages/ramalama/common.py\nPython 3.9.21\nlrwxrwxrwx. 1 root root 9 Feb 11 17:32 /usr/bin/python3 -> python3.9\nquay.io/ramalama/ramalama-rag                                 0.8.5              4f18350f7dcc  3 weeks ago        3.9 GB\n>>> VERSION 0.9.0\n/usr/lib/python3.11/site-packages/ramalama/common.py\nPython 3.11.11\nlrwxrwxrwx. 1 root root 19 Jun  2 09:43 /usr/bin/python3 -> /usr/bin/python3.11\nquay.io/ramalama/ramalama-rag                                 0.9.0              6ae625e673de  10 days ago        3.98 GB\n```\n\n\nNot sure what went wrong there. The code to build the rag container [here](https://github.com/containers/ramalama/blob/691c235b80ba7326436f8be426464e8504e6ca65/container-images/scripts/build_rag.sh#L32) seems confusing with respect to setting the python version, but not obviously wrong. From the build log [here](https://github.com/containers/ramalama/actions/runs/15275842608), I don't understand what went wrong.\n\nThere are no stable branches here for 0.8 version and the main branch is already at 0.9. But since any installed ramalama 0.8 version will try to use quay.io/ramalama/ramalama-rag:0.8, I think a fixed image should be pushed. It's might be simpler than to require every 0.8 user to upgrade.\n\n\n---\n\n`ramalama --image quay.io/ramalama/ramalama-rag:0.9.0 run --rag localhost/myrag:0.1 ollama://gemma3/gemma3:4b` does not work to select another image. Which is probably a bug:\n```\n        # force accel_image to use -rag version. Drop TAG if it exists\n        # so that accel_image will add -rag to the image specification.\n        if hasattr(args, \"rag\") and args.rag:\n            args.image = args.image.split(\":\")[0]\n        args.image = accel_image(CONFIG, args)\n```\n\n### Steps to reproduce the issue\n\nSee above\n\n### Describe the results you received\n\nSee above\n\n### Describe the results you expected\n\nSee above\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.1\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 96.25,\n                    \"systemPercent\": 0.86,\n                    \"userPercent\": 2.89\n                },\n                \"cpus\": 8,\n                \"databaseBackend\": \"boltdb\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 1977,\n                \"hostname\": \"rh1.local\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.5-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 2253086720,\n                \"memTotal\": 33334882304,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.15.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.15.0\"\n                    },\n                    \"package\": \"netavark-1.15.2-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.15.2\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250606.g754c6d7-1.fc42.x86_64\",\n                    \"version\": \"pasta 0^20250606.g754c6d7-1.fc42.x86_64\\nCopyright Red Hat\\nGNU General Public License, version 2 or later\\n  <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 23994228736,\n                \"swapTotal\": 46152015872,\n                \"uptime\": \"790h 22m 55.00s (Approximately 32.92 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/thom/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 69,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 69\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/thom/.local/share/containers/storage\",\n                \"graphRootAllocated\": 473764462592,\n                \"graphRootUsed\": 459607855104,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 42\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/thom/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.1\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1749081600,\n                \"BuiltTime\": \"Thu Jun  5 02:00:00 2025\",\n                \"GitCommit\": \"850db76dd78a0641eddb9ee19ee6f60d2c59bcfa\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/thom/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.5\"\n}\n```\n\n### Upstream Latest Release\n\nNo\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "thom311",
      "author_type": "User",
      "created_at": "2025-06-13T09:54:21Z",
      "updated_at": "2025-06-13T09:54:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1521/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 1,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1521",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1521",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.919586",
      "comments": []
    },
    {
      "issue_number": 1515,
      "title": "Detects Intel Arc Graphics A770, A750",
      "body": "### Feature request description\n\nhttps://github.com/containers/ramalama/blob/e5635d1d1483bb20d4931bd666880b766999b96f/ramalama/common.py#L482-L488\n\nShould also add `0x56a0` (A770) and `0x56a1` (A750)\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "kwaa",
      "author_type": "User",
      "created_at": "2025-06-12T12:36:57Z",
      "updated_at": "2025-06-13T08:26:44Z",
      "closed_at": "2025-06-13T08:26:44Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1515/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1515",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1515",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.919608",
      "comments": []
    },
    {
      "issue_number": 1493,
      "title": "Skip huggingface-cli fallback for llama-server style huggingface uris",
      "body": "### Issue Description\n\nThe huggingface-cli fallback logic will attempt to download the entire model repo for llama-server style huggingface references. There does not appear to be any logic in huggingface-cli to replicate the llama-server behaviour so would be best to just skip it\n\n### Steps to reproduce the issue\n\nhttps://github.com/containers/ramalama/issues/1489#issuecomment-2958049710\n\n### Describe the results you received\n\nhttps://github.com/containers/ramalama/issues/1489#issuecomment-2958049710\n\n### Describe the results you expected\n\nShould fail fast when pulling a llama-server style huggingface uri fails\n\n### ramalama info output\n\n```yaml\nN/A\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\nFollow-up issue from https://github.com/containers/ramalama/issues/1489",
      "state": "open",
      "author": "olliewalsh",
      "author_type": "User",
      "created_at": "2025-06-10T09:45:17Z",
      "updated_at": "2025-06-12T21:42:22Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1493/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1493",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1493",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:32.919617",
      "comments": [
        {
          "author": "olliewalsh",
          "body": "@ericcurtin This one is still open",
          "created_at": "2025-06-12T20:29:25Z"
        }
      ]
    },
    {
      "issue_number": 1519,
      "title": "s3 pulling support",
      "body": "### Feature request description\n\nBut it would be nice to have at least basic s3:// pulling support, here is it in C++:\n\nhttps://github.com/ggml-org/llama.cpp/blob/ed52f3668e633423054a4eab61bb7efee47025ab/tools/run/run.cpp#L788\n\n\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-06-12T14:41:07Z",
      "updated_at": "2025-06-12T21:33:42Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1519/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1519",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1519",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:33.115713",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Tagging @davdunc\n\nIt would be another pulling source, like ollama, huggingface, oci registries, modelscope, etc.\n",
          "created_at": "2025-06-12T14:42:15Z"
        }
      ]
    },
    {
      "issue_number": 1508,
      "title": "`ramalama serve smolvlm` bailing out",
      "body": "### Issue Description\n\nThis works on my machine, but at least two different users reported this bailing out @rhatdan and @cedricclyburn:\n\n```\nFailed to create new snapshot: Only one model supported, got 4: [<ramalama.huggingface.HuggingfaceCLIFile object at 0x10206e900>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x102056850>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x102056990>, <ramalama.huggingface.HuggingfaceCLIFile object at 0x10202a650>]\nRemoving snapshot...\nError: [Errno 2] No such file or directory: '/Users/cclyburn/.local/share/ramalama/store/huggingface/ggml-org/SmolVLM-500M-Instruct-GGUF/snapshots/\n```\n\n\n### Steps to reproduce the issue\n\nNA\n\n### Describe the results you received\n\nNA\n\n### Describe the results you expected\n\nNA\n\n### ramalama info output\n\n```yaml\nNA\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nNA\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-06-12T04:35:48Z",
      "updated_at": "2025-06-12T20:16:47Z",
      "closed_at": "2025-06-12T20:16:47Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1508/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1508",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1508",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:33.312589",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "@olliewalsh this feature has been working great on my machine, since it was introduced... For some other users, not so much",
          "created_at": "2025-06-12T05:44:35Z"
        },
        {
          "author": "ericcurtin",
          "body": "I reproduced this on @cedricclyburn 's machine by blowing out the whole model store.\n\nHe was on:\n\n```\nramalama 0.9.1\n```\n\nI dunno if he had huggingface cli installed too, we have some code around that which I wouldn't mind deleting. Our huggingface client is fine.\n",
          "created_at": "2025-06-12T07:21:55Z"
        },
        {
          "author": "olliewalsh",
          "body": "Yes, looks like a dup of https://github.com/containers/ramalama/issues/1493.\n\nI assume it is download using huggingface-cli in this case as it has all 4 gguf files from https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/tree/main\n",
          "created_at": "2025-06-12T08:28:58Z"
        },
        {
          "author": "olliewalsh",
          "body": "Ultimately the issue here is that it is failing to remove the snapshot. @engelmi Possibly a regression in https://github.com/containers/ramalama/commit/4f53c653864ed2b46a08bf76bcad17b7289f21ff#diff-452df1d3a56e954c748b85f61013db53d15918e86ce6326d9ebc000351a4cd46 ",
          "created_at": "2025-06-12T08:54:53Z"
        },
        {
          "author": "olliewalsh",
          "body": "Can reproduce this by forcing it to fallback to huggingface-cli.\n\n@engelmi Only get the \"No such file or directory\" in the 2nd attempt, probably just need to handle the case where the snapshot doesn't exist when calling remove_snapshot()\n\n```\n(venv) owalsh@pluto:~/hf/ramalama$ ramalama pull smolvlm\n",
          "created_at": "2025-06-12T09:04:05Z"
        }
      ]
    },
    {
      "issue_number": 1432,
      "title": "\"ramalama lightspeed\" command",
      "body": "### Feature request description\n\nConnect our client to rhel lighspeed endpoint over TLS:\n\nhttps://github.com/search?q=repo%3Arhel-lightspeed%2Fcommand-line-assistant+endpoint&type=code\n\njust to achieve a basic chatbot, no complex functionality, just a basic chatbot that connects to the endpoint. Leave the enhanced features to:\n\nhttps://github.com/rhel-lightspeed/command-line-assistant\n\nBasically `ramalama lightspeed` should behave like `ramalama run` but it just connects to the lightspeed endpoint as a client.\n\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-05-21T18:27:27Z",
      "updated_at": "2025-06-11T11:08:15Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1432/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "r0x0d"
      ],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1432",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1432",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:35.560841",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "We should loop in @mairin @r0x0d etc. here",
          "created_at": "2025-05-21T18:31:39Z"
        },
        {
          "author": "r0x0d",
          "body": "@ericcurtin, hi! Let me know how we can help with this. \n\nIf you need someone to implement that, we can sync in slack and I could try, just let me know.",
          "created_at": "2025-05-27T10:29:40Z"
        },
        {
          "author": "ericcurtin",
          "body": "@r0x0d \n\nJust enhance the client code in RamaLama to talk to the lightspeed endpoint, it's just another openai endpoint at the end of the day, a reference implementation is here:\n\nhttps://github.com/rhel-lightspeed/command-line-assistant\n\nplease feel free to take this on. We don't want to reimplemen",
          "created_at": "2025-06-10T00:15:54Z"
        },
        {
          "author": "r0x0d",
          "body": "@ericcurtin can you assign the issue to me? I will take this task and implement it when I have some spare time. ",
          "created_at": "2025-06-11T11:03:14Z"
        }
      ]
    },
    {
      "issue_number": 1112,
      "title": "Explore replacing python3 ollama puller with \"podman artifact pull\"",
      "body": "We wrote an ollama puller from scratch in python3, it's like 2 http requests. But the final request is like a \"podman artifact pull\", explore if we can replace this pull with \"podman artifact pull\"\n\nPotential benefits:\n\n- pushing Ollama artifact to OCI registry without conversion steps\n- we can take advantage of authentication code in podman and in general the great compatibility podman has with many types of OCI registries",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-04-03T11:34:08Z",
      "updated_at": "2025-06-11T10:22:49Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1112/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1112",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1112",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:35.761078",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Python3 implementation:\n\nhttps://github.com/ericcurtin/lm-pull/blob/f688fb83fce2c96efeadb096b6cdaea11e133d4a/lm-pull.py#L262\n\nthe current ramalama implementation is based on this but more complex",
          "created_at": "2025-04-04T11:45:45Z"
        },
        {
          "author": "runcom",
          "body": "Alright, I've been working on this recently and while `podman artifact pull` can totally be used for the ollama registry, the problem is the registry itself.\nThe registry at registry.ollama.ai doesn't fully adhere to the protocol for pulling images. Just querying the registry status URL, which is th",
          "created_at": "2025-06-06T10:22:41Z"
        },
        {
          "author": "rhatdan",
          "body": "This would be cool, but we need to start handling Artifacts in RamaLama.  Currently RamaLama only supports OCI Images",
          "created_at": "2025-06-07T16:20:57Z"
        },
        {
          "author": "ericcurtin",
          "body": "> Alright, I've been working on this recently and while `podman artifact pull` can totally be used for the ollama registry, the problem is the registry itself. The registry at registry.ollama.ai doesn't fully adhere to the protocol for pulling images. Just querying the registry status URL, which is ",
          "created_at": "2025-06-09T13:43:57Z"
        },
        {
          "author": "runcom",
          "body": "> My vote would be to open a PR in podman with what you've done so far :)\n\nI'm torn on this... The problem with ollama and podman right now is the ollama registry misbehaving. If I open a PR in podman (read: containers/image) to patch it the way I did in my testing, it'll be a huge stopgap.\n\n> What ",
          "created_at": "2025-06-10T09:45:55Z"
        }
      ]
    },
    {
      "issue_number": 1479,
      "title": "Performance regression between 0.7.4 and 0.9.0",
      "body": "### Issue Description\n\nWhile working on an Podman AI Lab workflow to automate the update of ramalama version used in Podman AI Lab, I noticed, when switching from 0.7.4 to 0.9.0 a big performance drop in my validation tests:\n\nHere are the logs from the server in the 2 configurations:\n\n0.7.4:\n\nprompt eval time =    3058.24 ms /    64 tokens (   47.78 ms per token,    20.93 tokens per second)\n       eval time =   83033.60 ms /   455 tokens (  182.49 ms per token,     5.48 tokens per second)\n      total time =   86091.84 ms /   519 tokens\n\t  \n0.9.0:\nprompt eval time =  121635.23 ms /    64 tokens ( 1900.55 ms per token,     0.53 tokens per second)\n       eval time =   15152.22 ms /    93 tokens (  162.93 ms per token,     6.14 tokens per second)\n      total time =  136787.44 ms /   157 tokens\nsrv  update_slots: all slots are idle\n\n\n### Steps to reproduce the issue\n\nNeed Podman AI Lab dev and modifying the image reference in the code\n\n### Describe the results you received\n\nSee above\n\n### Describe the results you expected\n\nSimilar performance numbers\n\n### ramalama info output\n\n```yaml\nN/A\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "jeffmaury",
      "author_type": "User",
      "created_at": "2025-06-06T14:02:46Z",
      "updated_at": "2025-06-10T16:15:48Z",
      "closed_at": "2025-06-10T12:01:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 30,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1479/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1479",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1479",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:36.001419",
      "comments": []
    },
    {
      "issue_number": 1489,
      "title": "regression: llama-server style huggingface reference no longer works",
      "body": "### Issue Description\n\nAutomatic resolution of URLs in the form of `hf.co/unsloth/Llama-3.1-8B-Instruct-GGUF:Q4_K_M` no longer works since 0.9.0.\n\n### Steps to reproduce the issue\n\n1. `ramalama pull hf.co/unsloth/Llama-3.1-8B-Instruct-GGUF:Q4_K_M`\n\n### Describe the results you received\n\n```\nDownloading huggingface://Llama-3.1-8B-Instruct-GGUF:Q4_K_M ...\nTrying to pull huggingface://Llama-3.1-8B-Instruct-GGUF:Q4_K_M ...\nURL pull failed and huggingface-cli not available\nError: Failed to pull model: 'failed to pull https://huggingface.co/unsloth/raw/main/Llama-3.1-8B-Instruct-GGUF: HTTP Error 401: Unauthorized\n```\n\n### Describe the results you expected\n\nThis should work\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"hip\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.0\",\n                \"cgroupControllers\": [\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.2.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: unknown\"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 92.65,\n                    \"systemPercent\": 2.11,\n                    \"userPercent\": 5.23\n                },\n                \"cpus\": 12,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"opensuse-microos\",\n                    \"version\": \"20250601\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2034,\n                \"hostname\": \"leorize-workstation\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 60252,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 60252,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.6-2-default\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 1959829504,\n                \"memTotal\": 16689545216,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.15.0-1.1.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.15.0\"\n                    },\n                    \"package\": \"netavark-1.15.0-1.1.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.15.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.1.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/60252/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-20250512.8ec1341-1.1.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/60252/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 3033354240,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"41h 32m 30.00s (Approximately 1.71 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.opensuse.org\",\n                    \"registry.suse.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/leorize/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 4,\n                    \"paused\": 0,\n                    \"running\": 2,\n                    \"stopped\": 2\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/leorize/.local/share/containers/storage\",\n                \"graphRootAllocated\": 658138083328,\n                \"graphRootUsed\": 369539608576,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"false\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"true\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 109\n                },\n                \"runRoot\": \"/run/user/60252/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/leorize/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.0\",\n                \"Built\": 1747285822,\n                \"BuiltTime\": \"Thu May 15 00:10:22 2025\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.9\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/leorize/.config/ramalama/shortnames.conf\",\n            \"/home/leorize/.local/share/uv/tools/ramalama/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi-4\": \"hf://bartowski/phi-4-GGUF/phi-4-IQ3_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/leorize/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.9.0\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\nThis was working in ramalama 0.8.5",
      "state": "closed",
      "author": "alaviss",
      "author_type": "User",
      "created_at": "2025-06-10T07:41:15Z",
      "updated_at": "2025-06-10T09:25:41Z",
      "closed_at": "2025-06-10T09:25:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1489/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1489",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1489",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:36.001441",
      "comments": [
        {
          "author": "rhatdan",
          "body": "This worked on older versions?",
          "created_at": "2025-06-10T07:53:28Z"
        },
        {
          "author": "rhatdan",
          "body": "I see it pulling tons of models, as opposed to a single model in 0.8.5\n\n![Image](https://github.com/user-attachments/assets/62c7c5c7-b72d-42c9-9823-196da007e0d5)",
          "created_at": "2025-06-10T07:56:23Z"
        },
        {
          "author": "taronaeo",
          "body": "Hi @alaviss, I can reproduce your issue but I have yet to test it on versions prior to `0.9.0`. However, I have had much success pulling the image using this command instead\n\n```sh\n$ podman exec -it ramalama ramalama pull hf.co/unsloth/Llama-3.1-8B-Instruct-GGUF/Llama-3.1-8B-Instruct-Q4_K_M.gguf\n\nDo",
          "created_at": "2025-06-10T07:56:36Z"
        },
        {
          "author": "taronaeo",
          "body": "> I see it pulling tons of models, as opposed to a single model in 0.8.5\n> \n> ![Image](https://github.com/user-attachments/assets/62c7c5c7-b72d-42c9-9823-196da007e0d5)\n\nLooks like `0.9.0` fails outright instead of downloading a single specified model. I'm getting the same results as the author:\n\n```",
          "created_at": "2025-06-10T07:59:33Z"
        },
        {
          "author": "rhatdan",
          "body": "@yeahdongcn is this your change that caused this?",
          "created_at": "2025-06-10T08:02:05Z"
        }
      ]
    },
    {
      "issue_number": 1463,
      "title": "bug: ramalama convert TypeError: argument of type 'NoneType' is not iterable",
      "body": "### Issue Description\n\nRamalama fails to convert a HuggingFace model to OCI via the `ramalama --container convert hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf oci://quay.io/taronaeo/Qwen2.5-1.5B-Instruct-GGUF` command\n\n### Steps to reproduce the issue\n\nFollowing the example: https://github.com/containers/ramalama/blob/6d7cfa88a49f3b7e92a61ccba128c6ac10c7e7ab/docs/ramalama-convert.1.md?plain=1#L38-L58\n\n3. Run the `ramalama convert` command: `ramalama --container convert hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf oci://quay.io/taronaeo/Qwen2.5-1.5B-Instruct-GGUF`\n\n### Describe the results you received\n\n```sh\n$ ramalama --container convert hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf oci://quay.io/taronaeo/Qwen2.5-1.5B-Instruct-GGUF\nConverting /var/lib/ramalama/store to /var/lib/ramalama/store ...\nTraceback (most recent call last):\n  File \"/usr/bin/ramalama\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/lib/python3.11/site-packages/ramalama/cli.py\", line 1155, in main\n    args.func(args)\n  File \"/usr/lib/python3.11/site-packages/ramalama/cli.py\", line 662, in convert_cli\n    model.convert(source_model, args)\n  File \"/usr/lib/python3.11/site-packages/ramalama/oci.py\", line 327, in convert\n    self._convert(source_model, args)\n  File \"/usr/lib/python3.11/site-packages/ramalama/oci.py\", line 311, in _convert\n    run_cmd([self.conman, \"manifest\", \"rm\", self.model], ignore_stderr=True, stdout=None)\n  File \"/usr/lib/python3.11/site-packages/ramalama/common.py\", line 164, in run_cmd\n    logger.debug(f\"run_cmd: {quoted(args)}\")\n                             ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/site-packages/ramalama/common.py\", line 132, in quoted\n    return \" \".join(['\"' + element + '\"' if ' ' in element else element for element in arr])\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/site-packages/ramalama/common.py\", line 132, in <listcomp>\n    return \" \".join(['\"' + element + '\"' if ' ' in element else element for element in arr])\n                                            ^^^^^^^^^^^^^^\nTypeError: argument of type 'NoneType' is not iterable\n```\n\n### Describe the results you expected\n\nIf this is an expected failure, it should be caught and thrown the appropriate information. Otherwise, it should convert the HuggingFace GGUF model to OCI\n\n### ramalama info output\n\n```yaml\n$ podman exec -it ramalama ramalama info                                                                                                                      \n{                                                                                                                                                                 \n    \"Accelerator\": \"none\",                                                                                                                                        \n    \"Engine\": {                                                                  \n        \"Name\": null                                                             \n    },                                                                           \n    \"Image\": \"quay.io/ramalama/ramalama:0.9\",                                    \n    \"Runtime\": \"llama.cpp\",                                                      \n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\" \n        }\n    },\n    \"Store\": \"/var/lib/ramalama\",\n    \"UseContainer\": false,\n    \"Version\": \"0.9.0\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAMD64 Fedora Server / 16 vCPU / 32 GB RAM / SMT\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "taronaeo",
      "author_type": "User",
      "created_at": "2025-06-03T11:30:00Z",
      "updated_at": "2025-06-10T07:36:21Z",
      "closed_at": "2025-06-10T07:36:21Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1463/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1463",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1463",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:36.240846",
      "comments": [
        {
          "author": "rhatdan",
          "body": "I tried your command above, and got.\n```\n$ ./bin/ramalama --container convert hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf oci://quay.io/taronaeo/Qwen2.5-1.5B-Instruct-GGUF\nDownloading huggingface://Qwen2.5-1.5B-Instruct-Q4_K_M.gguf:latest ...\nTrying to pull h",
          "created_at": "2025-06-03T12:26:06Z"
        },
        {
          "author": "rhatdan",
          "body": "No idea how you got an None element in your list of args?",
          "created_at": "2025-06-03T12:27:43Z"
        },
        {
          "author": "taronaeo",
          "body": "Interesting... Let me try again and if it still fails, I'll copy and paste my commands from start to finish. Btw, are you running bare-metal instead of in a container env?",
          "created_at": "2025-06-03T12:29:09Z"
        },
        {
          "author": "taronaeo",
          "body": "Steps:\n1. Ensure podman has no existing images related to ramalama\n```sh\n$ podman images\nREPOSITORY  TAG         IMAGE ID    CREATED     SIZE\n```\n\n2. Build ramalama image\n```sh\n$ container_build.sh build ramalama\n...truncated...\n25 files removed\n+ rm -rf /var/cache/dnf '/opt/rocm-*/lib/*/library/*gf",
          "created_at": "2025-06-03T18:21:49Z"
        },
        {
          "author": "rhatdan",
          "body": "Well this looks like you have a container and then you are running ramalama within the contaiener.  I will try this when I get a chance.  I guess the issue is something to do with args, if you could edit the code to print(args) that might help diagnose what is going on.",
          "created_at": "2025-06-03T23:51:29Z"
        }
      ]
    },
    {
      "issue_number": 1480,
      "title": "Flake fails to build on Asahi Linux",
      "body": "### Issue Description\n\n`nix run github:containers/ramalama` pulls in dependency `llama-cpp` which fails to build from source.\n\n### Steps to reproduce the issue\n\nUsing nixos, run `nix run github:containers/ramalama`\n\n### Describe the results you received\n\n[command output](https://pastebin.com/Bfrfh2L9)\n\n[build failure log](https://pastebin.com/15SYUKk8).\n\n### Describe the results you expected\n\nThe flake should work as documented [in the PR](https://github.com/containers/ramalama/pull/581).\n\n### ramalama info output\n\n```yaml\nN/A (does not build)\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "waltmck",
      "author_type": "User",
      "created_at": "2025-06-06T19:09:41Z",
      "updated_at": "2025-06-09T11:55:51Z",
      "closed_at": "2025-06-09T11:55:50Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1480/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1480",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1480",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:36.472061",
      "comments": [
        {
          "author": "rhatdan",
          "body": "I have no idea how this is supposed to work.  Does nix attempt to build everything from scratch.  RamaLama is more about running AI MOdels in containers, so building llama.cpp for every platform is not one of our goals.",
          "created_at": "2025-06-07T13:02:23Z"
        },
        {
          "author": "smooge",
          "body": "I believe the problem could be seen as being able to replicate building the containers that ramalama uses. That said, the bug would be better looked at llama-cpp versus ramalama to see why it is not able to build.",
          "created_at": "2025-06-07T13:09:41Z"
        },
        {
          "author": "ericcurtin",
          "body": "Yeah, llama.cpp should be it's own package... llama.cpp could be a dependancy of RamaLama, but it's not a strictly required dependancy as one can just use containers...",
          "created_at": "2025-06-07T14:48:06Z"
        },
        {
          "author": "rhatdan",
          "body": "The issue is we build llama.cpp differently for each GPU.  So I don't believe it can be easily packaged.",
          "created_at": "2025-06-07T16:23:25Z"
        },
        {
          "author": "waltmck",
          "body": "It is possible that this issue was caused by a `nixpkgs` update. Generally good practice is to maintain a `flake.lock` file in-tree that tracks the latest tested version of `nixpkgs`, which would completely prevent breakage from dependencies changing under you (this dependency pinning is one of the ",
          "created_at": "2025-06-07T19:02:00Z"
        }
      ]
    },
    {
      "issue_number": 1470,
      "title": "Unable to use --device to specify GPU device on a multi-GPU system (e.g.: /dev/dri/renderD129)",
      "body": "### Issue Description\n\nI have a system with two GPUs, and I was previously able to run one model on each GPU, by having one of the terminals add a parameter that I was able to find via \"ramalama --debug run\":\n\n```\n$ ramalama run llama3.1 --device /dev/dri/renderD129\n```\n\nI forget what version of ramalama was on this system when this worked; amid some confusion of what caused models to stop running, I re-installed the system.  (As it turns out, with akmod-nvidia installed, I need to remember to re-run \"sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\".)\n\nThe version that has the bug is python3-ramalama-0.8.2-1.fc42, which seems to be the latest available for Fedora 42 right now.  I have an older system that is running python3-ramalama-0.5.5-1.fc41, which I belive was updated near the time when I had the \"run one LLM on each of the two cards\" test running.\n\n\n### Steps to reproduce the issue\n\n1. In one shell, run `$ ramalama run llama3.1`.\n2. Wait for the model to load.  Confirm (via nvtop) that \"Device 0\" has loaded the model into memory, and is using GPU time on \"Device 0\".  (If the model doesn't load on Device 0, try to find a way; perhaps using `--device /dev/dri/renderD128`.)\n3. In another shell, run `$ ramalama run llama3.1 --device /dev/dri/renderD129`.\n4. Wait for the model to load.  Confirm (via nvtop) that \"Device 1\" has loaded the model into memory, and is using GPU time on \"Device 1\".\n\n(The GPUs on this system have 8 GB of memory, and the model seems to use 5.13 GB, so trying to load two models on one device would fail due to not having enough memory.)\n\n### Describe the results you received\n\nOn the first execution of \"ramalama run llama3.1 --device /dev/dri/renderD129\", to try to have the model loaded on the render device for \"card1\", the model ends up loading and running on \"renderD128\", which corresponds to \"card0\".\n\nWhile holding that first prompt open, trying to execute \"ramalama run llama3.1 --device /dev/dri/renderD129\" again results in a timeout after the first query:\n\n```\nError: could not connect to: http://127.0.0.1:8080/v1/chat/completions\n```\n\nAt this point, nvtop shows the model loaded on \"Device 0\", using 5.133 GiB of 8 GiB (both cards are NVidia GeForce GTX 1070 Mobile GPUs), while \"Device 1\" shows 0.085 GiB / 8.000GiB\" loaded.\n\n\n### Describe the results you expected\n\nWhat I expected, and was seeing before, was that I had two Ramalama sessions, one on each GPU, and it could be confirmed via watching nvtop for GPU usage and memory usage, while entering queries into each prompt.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 97.09,\n                    \"systemPercent\": 1.14,\n                    \"userPercent\": 1.77\n                },\n                \"cpus\": 8,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"xfce\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2047,\n                \"hostname\": \"force-majeure\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.9-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 9001594880,\n                \"memTotal\": 16709844992,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.15.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.15.0\"\n                    },\n                    \"package\": \"netavark-1.15.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.15.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250512.g8ec1341-1.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"0h 12m 19.00s\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/bgurney/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 1,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/bgurney/.local/share/containers/storage\",\n                \"graphRootAllocated\": 873422376960,\n                \"graphRootUsed\": 15914131456,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"extfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 2\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/bgurney/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.0\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1747180800,\n                \"BuiltTime\": \"Tue May 13 20:00:00 2025\",\n                \"GitCommit\": \"0dbcb51477ee7ab8d3b47d30facf71fc38bb0c98\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/bgurney/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.2\"\n}\n```\n\n### Upstream Latest Release\n\nNo\n\n### Additional environment details\n\nThe system is running Fedora 42 with the following packages:\nkernel-6.14.9-300.fc42-x86_64\nakmod-nvidia-570.153.02-1.fc42.x86_64\nnvidia-container-toolkit-1.17.7-1.fc42.x86_64\n\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "bgurney-rh",
      "author_type": "User",
      "created_at": "2025-06-04T21:33:05Z",
      "updated_at": "2025-06-05T17:05:38Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1470/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1470",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1470",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:36.768580",
      "comments": [
        {
          "author": "taronaeo",
          "body": "I have a similar setup with you for my Fedora homelab server. But unfortunately I do not have a 2nd GPU to reproduce what you have here. Can you run `git bisect` in this project to narrow down to the commit that caused this issue?",
          "created_at": "2025-06-05T14:07:35Z"
        },
        {
          "author": "bgurney-rh",
          "body": "I think I can try to bisect it, once I figure out how to build and run it locally.\n\nThough, looking at the git log, I see a commit that seems to be in release v0.7.1, f1c2a2f \"Default devices should be added even if user specified devices\", and I wonder if that might have been the point where this w",
          "created_at": "2025-06-05T15:31:26Z"
        },
        {
          "author": "taronaeo",
          "body": "Try these steps. I've just re-did the installation process and it should be the same for you since I'm on Fedora also.\n\n1. Clone RamaLama Project\n```sh\ncd /opt\ngit clone https://github.com/containers/ramalama\n```\n\n2. Install RamaLama Python\n```sh\ncd /opt/ramalama\npip install argcomplete\nsource ~/.ba",
          "created_at": "2025-06-05T16:41:39Z"
        }
      ]
    },
    {
      "issue_number": 1453,
      "title": "\"Invalid reference format\" error with ramalama-rag",
      "body": "### Issue Description\n\nI have been able to get ramalama up and running by installing it with pip. I am trying to use ramalama-rag with the \"--ocr\" flag to \"teach\" models about a large pdf so that I can ask them questions about it. Something was definitely happening, because my CPU load increased for 30-45 minutes, but the program eventually returned with an error.\n\nI thought it might have something to do with the way I specify the storage location of the generated OCI image. All the examples I can find in the ramalama documentation seem to involve storing the image on quay.io. Is there a way to store the generated image locally? Am I doing something else wrong?\n\n### Steps to reproduce the issue\n\n I was able to reproduce it with a minimal example using a one-line markdown file.\n\nThe command:\n\nramalama rag ./minimal.md ./myrag_oci_image\n\nThe output:\n\nBuilding ./myrag_oci_image ...\nadding vectordb ...\nError: tag ./myrag_oci_image: invalid reference format\nError: Command '['podman', 'build', '--no-cache', '--network=none', '-q', '-t', './myrag_oci_image', '-f', '/home/USER/DIRECTORY/RamaLama_rag_st_103oa/vectordb/tmpsglj4o7o', '/home/USER/DIRECTORY/RamaLama_rag_st_103oa']' returned non-zero exit status 125.\n\n### Describe the results you received\n\nSee description\n\n### Describe the results you expected\n\nI hoped the command would generate an OCI image containing rag data compatible with ramalama run/serve and store it in ./myrag_oci_image. \n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"hip\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.3\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon_2.1.12-4_amd64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.12, commit: unknown\"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 95.72,\n                    \"systemPercent\": 0.31,\n                    \"userPercent\": 3.97\n                },\n                \"cpus\": 32,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"codename\": \"trixie\",\n                    \"distribution\": \"debian\",\n                    \"version\": \"13\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2045,\n                \"hostname\": \"HOSTNAME\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.12.27-amd64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 34653073408,\n                \"memTotal\": 98721914880,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns_1.14.0-3_amd64\",\n                        \"path\": \"/usr/lib/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark_1.14.0-2_amd64\",\n                    \"path\": \"/usr/lib/podman/netavark\",\n                    \"version\": \"netavark 1.14.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun_1.21-1_amd64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt_0.0~git20250503.587980c-2_amd64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": false\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns_1.2.1-1.1_amd64\",\n                    \"version\": \"slirp4netns version 1.2.1\\ncommit: 09e31e92fa3d2a1d3ca261adaeb012c8d75a8194\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.6.0\"\n                },\n                \"swapFree\": 100516491264,\n                \"swapTotal\": 100516491264,\n                \"uptime\": \"7h 46m 31.00s (Approximately 0.29 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {},\n            \"store\": {\n                \"configFile\": \"/home/USER/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 3,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 3\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/USER/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1897813508096,\n                \"graphRootUsed\": 98584215552,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 4\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/USER/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Debian\",\n                \"Built\": 1748111104,\n                \"BuiltTime\": \"Sat May 24 13:25:04 2025\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.24.2\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/rocm:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/USER/DIRECTORY/.venv/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/USER/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nRegarding running the latest upstream release: I installed using pip. Would that be the latest release?\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "jnwatkins",
      "author_type": "User",
      "created_at": "2025-05-30T01:22:03Z",
      "updated_at": "2025-06-04T23:43:41Z",
      "closed_at": "2025-06-04T23:43:40Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1453/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1453",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1453",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:36.969116",
      "comments": [
        {
          "author": "rhatdan",
          "body": "@bmahabirbu PTAL",
          "created_at": "2025-05-30T12:34:14Z"
        },
        {
          "author": "bmahabirbu",
          "body": "@jnwatkins you'll want to specify it like this\n\n`ramalama rag /path/to/minimal.md myrag_oci_image:latest`\n\nThat will tag the oci image as localhost/myrag_oci_image:latest \n\nYou can check the output of `podman images`to confirm!",
          "created_at": "2025-06-03T10:42:53Z"
        },
        {
          "author": "jnwatkins",
          "body": "@bmahabirbu \n\nThis worked, thank you! I probably should have learned more about podman before diving in.\n\nThat said, this issue does give me an idea for possible further development. The ramalama-rag command generated the embeddings and vector.db (a computationally expensive process) before throwing",
          "created_at": "2025-06-04T23:43:40Z"
        }
      ]
    },
    {
      "issue_number": 1460,
      "title": "Improve `container_build.sh` Help Information",
      "body": "### Feature request description\n\nWhile running the `container_build.sh` or `container_build.sh --help` command, we see that only the `-d` and `-r` commands are listed in the help information. But looking through `parse_arguments()` in `container_build.sh`, there are more flags available than listed.\n\nWe should update the help information to include the additional flags, especially the `-C` flag that would have saved me a lot of time from rebuilding the container images.\n\nFYI:\n```\nparse_arguments() {\n  while [[ $# -gt 0 ]]; do\n    case \"$1\" in\n      -h|--help)\n        print_usage\n        exit 0\n        ;;\n      -c)\n        ci=\"true\"\n        shift\n        ;;\n      -C)\n        nocache=\"\"\n        shift\n        ;;\n      -d)\n        dryrun=\"$1\"\n        shift\n        ;;\n      -r)\n        rm_after_build=\"true\"\n        nocache=\"\"\n        shift\n        ;;\n      -v)\n        version=\"$2\"\n        shift\n        shift\n        ;;\n      build|push|multi-arch)\n        command=\"$1\"\n        shift\n        ;;\n      *)\n        target=\"$1\"\n        shift\n        ;;\n    esac\n  done\n}\n```\n\n### Suggest potential solution\n\nInclude the missing flags into the help information\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "taronaeo",
      "author_type": "User",
      "created_at": "2025-06-03T09:29:02Z",
      "updated_at": "2025-06-03T16:15:12Z",
      "closed_at": "2025-06-03T16:15:11Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1460/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "taronaeo"
      ],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1460",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1460",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:37.177632",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Interested in opening a PR.\n\nI tend to just use the Makefile.\n\nmake build IMAGE=cuda ...\n",
          "created_at": "2025-06-03T10:27:08Z"
        }
      ]
    },
    {
      "issue_number": 1443,
      "title": "Ramalama sits idle for several minutes",
      "body": "### Issue Description\n\nRunning `ramalama --debug run deepseek-coder-v2:236b` will sit idle (0% cpu being used by the process) without any output for up to 40 minutes. Then it starts the model download, which can get stuck at 99% for another several minutes.\n\nIt might be something machine specific.\n\n### Steps to reproduce the issue\n\n`ramalama --debug run deepseek-coder-v2:236b`\n\n### Describe the results you received\n\nVery long waiting time for the actual download to happen.\n\n### Describe the results you expected\n\nThe download to start right away.\n\n### ramalama info output\n\n```yaml\n# ramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.2\",\n                \"cgroupControllers\": [\n                    \"cpuset\",\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"hugetlb\",\n                    \"pids\",\n                    \"rdma\",\n                    \"misc\",\n                    \"dmem\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.99,\n                    \"systemPercent\": 0,\n                    \"userPercent\": 0.01\n                },\n                \"cpus\": 64,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"server\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2047,\n                \"hostname\": \"*REDACTED*\",\n                \"idMappings\": {\n                    \"gidmap\": null,\n                    \"uidmap\": null\n                },\n                \"kernel\": \"6.14.0-63.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 84061331456,\n                \"memTotal\": 201339006976,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.20-2.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.20\\ncommit: 9c9a76ac11994701dd666c4f0b869ceffb599a66\\nrundir: /run/user/0/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": false,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8589668352,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"339h 39m 39.00s (Approximately 14.12 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/usr/share/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 1,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {\n                    \"overlay.additionalImageStores\": [\n                        \"/usr/lib/containers/storage\"\n                    ],\n                    \"overlay.imagestore\": \"/usr/lib/containers/storage\",\n                    \"overlay.mountopt\": \"nodev,metacopy=on\"\n                },\n                \"graphRoot\": \"/var/lib/containers/storage\",\n                \"graphRootAllocated\": 238299381760,\n                \"graphRootUsed\": 121273004032,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"false\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"true\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"true\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 16\n                },\n                \"runRoot\": \"/run/containers/storage\",\n                \"transientStore\": false,\n                \"volumePath\": \"/var/lib/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.1\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1741651200,\n                \"BuiltTime\": \"Mon Mar 10 20:00:00 2025\",\n                \"GitCommit\": \"b79bc8afe796cba51dd906270a7e1056ccdfcf9e\",\n                \"GoVersion\": \"go1.24.0\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/root/.local/share/uv/tools/ramalama/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/var/lib/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "almusil",
      "author_type": "User",
      "created_at": "2025-05-28T11:11:05Z",
      "updated_at": "2025-05-29T15:43:59Z",
      "closed_at": "2025-05-29T15:43:58Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1443/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1443",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1443",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:37.379809",
      "comments": [
        {
          "author": "almusil",
          "body": "It happens when the `registry.ollama.ai` resolves to IPv6 on hosts that don't have IPv6 access, but have IPv6 default route. It would be nice to have a way of specifying the preferred address family e.g. `ramalama -4 run`/`ramalam -6 run`.",
          "created_at": "2025-05-28T12:37:16Z"
        },
        {
          "author": "rhatdan",
          "body": "Interested in opening a PR?",
          "created_at": "2025-05-28T14:15:26Z"
        },
        {
          "author": "almusil",
          "body": "Sure I can try to put something together.",
          "created_at": "2025-05-29T05:21:25Z"
        },
        {
          "author": "rhatdan",
          "body": "I am now thinking this is not something we can or need to fix in RamaLama, since your system is badly configured.  We can not work around all of the issues that this bad network would cause, and users expectations of ramalama to only use IPV4 if configured will not work, since container engines and ",
          "created_at": "2025-05-29T11:00:51Z"
        },
        {
          "author": "almusil",
          "body": "That's fair point, there are ways how to avoid this issue on the host.",
          "created_at": "2025-05-29T15:43:58Z"
        }
      ]
    },
    {
      "issue_number": 1414,
      "title": "Always failed to run a specific model using `ramalama run $model_name`",
      "body": "### Issue Description\n\nI wanted to run a chatbot using the run command, but it never succeeded. Can you provide me with some tips on what happened here?\n\n\n\n### Steps to reproduce the issue\n\n```\n$ cat /etc/redhat-release \nFedora release 40 (Forty)\n\n$ pip show ramalama\nName: ramalama\nVersion: 0.8.3\nSummary: RamaLama is a command line tool for working with AI LLM models.\nHome-page: https://github.com/containers/ramalama\n\n$ ramalama pull granite3-moe\n\n$ ramalama pull tiny\n\n$ ramalama list\nNAME                                      MODIFIED       SIZE     \nollama://granite3-moe/granite3-moe:latest 14 minutes ago 783.77 MB\nollama://tinyllama/tinyllama:latest       2 seconds ago  608.16 MB\n\n$ ramalama run granite3-moe\nðŸ¦­ > how are you?\nError: could not connect to: http://127.0.0.1:8080/v1/chat/completions\n\n$ ramalama run tinyllama\nðŸ¦­ > what is the longest river in the world?\nError: could not connect to: http://127.0.0.1:8080/v1/chat/completions\n\n$ ramalama serve granite3-moe\nserving on port 8080\nbuild: 5335 (d8919424) with cc (GCC) 11.5.0 20240719 (Red Hat 11.5.0-5) for x86_64-redhat-linux\nsystem info: n_threads = 4, n_threads_batch = 4, total_threads = 8\n\n```\n\n![Image](https://github.com/user-attachments/assets/a0a6a322-71c1-4199-bf83-465537b00e24)\n\n\n\n### Describe the results you received\n\nthe `ramalama run` always failed, but it's successful to serve the model using `ramalama serve`.\n\n### Describe the results you expected\n\nI expected that I could run the chatbot successfully.\n\n### ramalama info output\n\n```yaml\n$ ramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.12-2.fc40.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.12, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 96.22,\n                    \"systemPercent\": 0.75,\n                    \"userPercent\": 3.03\n                },\n                \"cpus\": 8,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"matecompiz\",\n                    \"version\": \"40\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2048,\n                \"hostname\": \"fedora\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.13.8-100.fc40.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 17538068480,\n                \"memTotal\": 33331781632,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc40.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.0-1.fc40.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.20-2.fc40.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.20\\ncommit: 9c9a76ac11994701dd666c4f0b869ceffb599a66\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250217.ga1e48a0-2.fc40.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-1.fc40.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.7.0\\nSLIRP_CONFIG_VERSION_MAX: 4\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"5h 39m 32.00s (Approximately 0.21 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\",\n                    \"registry-proxy.engineering.redhat.com\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/songliu/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 0,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/songliu/.local/share/containers/storage\",\n                \"graphRootAllocated\": 510405902336,\n                \"graphRootUsed\": 152152965120,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 152\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/songliu/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.0\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1739232000,\n                \"BuiltTime\": \"Tue Feb 11 08:00:00 2025\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.22.11\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/songliu/.local/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/songliu/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.3\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "melodyliu1986",
      "author_type": "User",
      "created_at": "2025-05-16T07:38:07Z",
      "updated_at": "2025-05-29T12:52:49Z",
      "closed_at": "2025-05-29T12:52:49Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1414/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1414",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1414",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:37.568040",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "I added a debug option here that will help you debug\n\nhttps://github.com/containers/ramalama/pull/1415/files\n\nBut it would be faster if you changed those lines of code in the container yourself to see why it's failing, it looks like the server is not starting or crashing for some reason.\n\nOne thing ",
          "created_at": "2025-05-16T08:49:55Z"
        },
        {
          "author": "melodyliu1986",
          "body": "> I added a debug option here that will help you debug\n> \n> https://github.com/containers/ramalama/pull/1415/files\n> \n> But it would be faster if you changed those lines of code in the container yourself to see why it's failing, it looks like the server is not starting or crashing for some reason.\n>",
          "created_at": "2025-05-19T08:47:51Z"
        },
        {
          "author": "ericcurtin",
          "body": "> > I added a debug option here that will help you debug\n> > https://github.com/containers/ramalama/pull/1415/files\n> > But it would be faster if you changed those lines of code in the container yourself to see why it's failing, it looks like the server is not starting or crashing for some reason.\n>",
          "created_at": "2025-05-19T17:13:15Z"
        }
      ]
    },
    {
      "issue_number": 1445,
      "title": "bug(ramalama-run): AttributeError: 'RamaLamaShell' object has no attribute 'do_hello'. Did you mean: 'do_help'?",
      "body": "### Issue Description\n\nWhile running the command `podman exec -it ramalama ramalama run hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf`, chatting with the model with a message of \"hello world\" causes it to return `None` and crash entirely.\n\n### Steps to reproduce the issue\n\n1. `podman run -td --name ramalama ramalama`\n2. `podman exec -it ramalama ramalama run hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf`\n3. Type in `hello world` and notice that it crashes with `None` and throws `AttributeError: 'RamaLamaShell' object has no attribute 'do_hello'. Did you mean: 'do_help'?`\n\n### Describe the results you received\n\n```\npodman exec -it ramalama ramalama run hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf\nDownloading hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf:latest ...\nTrying to pull hf://lmstudio-community/Qwen2.5-1.5B-Instruct-GGUF/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf:latest ...\n 99% |â–ˆâ–ˆâ–ˆâ–ˆ  |  933.18 MB/ 940.37 MB 110.93 MB/s        0s\nðŸ¦™ > hello world\nNoneTraceback (most recent call last):\n  File \"/usr/lib64/python3.11/cmd.py\", line 214, in onecmd\n    func = getattr(self, 'do_' + cmd)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'RamaLamaShell' object has no attribute 'do_hello'. Did you mean: 'do_help'?\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 193, in <module>\n    main(sys.argv[1:])\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 188, in main\n    run_shell_loop(ramalama_shell)\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 170, in run_shell_loop\n    ramalama_shell.cmdloop()\n  File \"/usr/lib64/python3.11/cmd.py\", line 138, in cmdloop\n    stop = self.onecmd(line)\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.11/cmd.py\", line 216, in onecmd\n    return self.default(line)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 114, in default\n    response = req(self.conversation_history, self.url, self.parsed_args)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 83, in req\n    return res(response, parsed_args.color)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/libexec/ramalama/ramalama-client-core\", line 49, in res\n    assistant_response += choice\nTypeError: can only concatenate str (not \"NoneType\") to str\n```\n\n### Describe the results you expected\n\nThe model should respond appropriately without crashing.\n\n### ramalama info output\n\n```yaml\n{                                \n    \"Accelerator\": \"none\",\n    \"Engine\": {       \n        \"Name\": null\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.8\",                                                                                                                     \n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\" \n        }\n    },\n    \"Store\": \"/var/lib/ramalama\",\n    \"UseContainer\": false,\n    \"Version\": \"0.8.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\nTested with latest commit `859609e59e9644897195902e959146146a469074` and `b9171dcf4f524f8652bda4cc04f87a94acc3f815`. Both have the same issue.\n\nTested on AMD64 and s390x systems as well and both are experiencing the same issue.",
      "state": "closed",
      "author": "taronaeo",
      "author_type": "User",
      "created_at": "2025-05-29T07:59:48Z",
      "updated_at": "2025-05-29T12:42:22Z",
      "closed_at": "2025-05-29T12:42:22Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1445/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1445",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1445",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:37.848046",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Is this repeatable, it is working for me when I test locally.\n\nTheir is no do_help or do_hello in all of our code base (Or hello), so this looks like the tool is thinking that \"hello world\" is being interpreted as a command, to do_hello.",
          "created_at": "2025-05-29T10:51:48Z"
        },
        {
          "author": "rhatdan",
          "body": "Ok I found that choice could be set to None which would cause this error.\n",
          "created_at": "2025-05-29T10:58:09Z"
        },
        {
          "author": "taronaeo",
          "body": "Nice! I did a `git bisect` command to find out which commit introduced the problem and it narrowed down to this. I'm testing your PR now.\n\n```\n$ git bisect bad\nd23ed7d7ec5e0ca0d609b5257350177b63a848c6 is the first bad commit\ncommit d23ed7d7ec5e0ca0d609b5257350177b63a848c6 (HEAD)\nAuthor: Robert Sturl",
          "created_at": "2025-05-29T12:14:10Z"
        },
        {
          "author": "taronaeo",
          "body": "Update: The PR fixes this bug ðŸ‘ðŸ» ",
          "created_at": "2025-05-29T12:29:19Z"
        }
      ]
    },
    {
      "issue_number": 1440,
      "title": "Quadlet generator output misses mmproj mount",
      "body": "### Issue Description\n\nThe quadlet generator (ran for granite3.2-vision)  output seems to miss the mount for `mmproj.file`. \n\n\n\n### Steps to reproduce the issue\n\nWhen running ramalama serve with `hf://ibm-research/granite-vision-3.2-2b-GGUF`, two mounts are created in the quadlet, one with the familiar `/mnt/models/model.file` and one for `/mnt/models/mmproj.file`. That second one (that mounts `mmproj-model-f16.guff` to `/mnt/models/mmproj.file` in the container) is missing.\n\n1. generate a quadlet like `ramalama serve -p 8888 -c 0 --generate quadlet hf://ibm-research/granite-vision-3.2-2b-GGUF`\n2. run the quadlet\n3. watch output \n4. see it break :) \n\n### Describe the results you received\n\nfile not found error at the end of startup\n\n### Describe the results you expected\n\nmodel running\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"intel\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.40.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.45,\n                    \"systemPercent\": 0.16,\n                    \"userPercent\": 0.39\n                },\n                \"cpus\": 18,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"server\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2020,\n                \"hostname\": \"kuroi.thuisnet.com\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000008,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.6-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 4538257408,\n                \"memTotal\": 66688888832,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.15.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.15.0\"\n                    },\n                    \"package\": \"netavark-1.15.0-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.15.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250512.g8ec1341-1.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8575393792,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"274h 40m 49.00s (Approximately 11.42 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/maxim/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 15,\n                    \"paused\": 0,\n                    \"running\": 15,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/maxim/.local/share/containers/storage\",\n                \"graphRootAllocated\": 201745268736,\n                \"graphRootUsed\": 62523592704,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 61\n                },\n                \"runRoot\": \"/run/user/1000000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/maxim/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.5.0\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1747180800,\n                \"BuiltTime\": \"Wed May 14 02:00:00 2025\",\n                \"GitCommit\": \"0dbcb51477ee7ab8d3b47d30facf71fc38bb0c98\",\n                \"GoVersion\": \"go1.24.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.5.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/intel-gpu:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/maxim/.virtualenvs/rama/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"gemma3:12b\": \"hf://ggml-org/gemma-3-12b-it-GGUF\",\n            \"gemma3:1b\": \"hf://ggml-org/gemma-3-1b-it-GGUF\",\n            \"gemma3:27b\": \"hf://ggml-org/gemma-3-27b-it-GGUF\",\n            \"gemma3:4b\": \"hf://ggml-org/gemma-3-4b-it-GGUF\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"qwen2.5vl\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:2b\": \"hf://ggml-org/Qwen2.5-VL-2B-Instruct-GGUF\",\n            \"qwen2.5vl:32b\": \"hf://ggml-org/Qwen2.5-VL-32B-Instruct-GGUF\",\n            \"qwen2.5vl:3b\": \"hf://ggml-org/Qwen2.5-VL-3B-Instruct-GGUF\",\n            \"qwen2.5vl:7b\": \"hf://ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"smolvlm\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"smolvlm:256m\": \"hf://ggml-org/SmolVLM-256M-Instruct-GGUF\",\n            \"smolvlm:2b\": \"hf://ggml-org/SmolVLM-Instruct-GGUF\",\n            \"smolvlm:500m\": \"hf://ggml-org/SmolVLM-500M-Instruct-GGUF\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/maxim/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "wzzrd",
      "author_type": "User",
      "created_at": "2025-05-27T15:07:35Z",
      "updated_at": "2025-05-27T23:41:23Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1440/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "olliewalsh"
      ],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1440",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1440",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:38.106809",
      "comments": [
        {
          "author": "olliewalsh",
          "body": "Will take a look. Also missing for kube and quadlet/kube",
          "created_at": "2025-05-27T15:39:22Z"
        },
        {
          "author": "rhatdan",
          "body": "Than @olliewalsh I assigned this to you.",
          "created_at": "2025-05-27T15:46:23Z"
        },
        {
          "author": "olliewalsh",
          "body": "Quadlet generation doesn't support multiple `Mount` entries right now (chat template file is also not working). I'll need to fix this first.",
          "created_at": "2025-05-27T23:41:22Z"
        }
      ]
    },
    {
      "issue_number": 1431,
      "title": "[RFE] Support for tools in streaming mode",
      "body": "### Feature request description\n\nI can't use tools with streaming, it always fails with the following error:\n\n```\n{\"code\":500,\"message\":\"Cannot use tools with stream\",\"type\":\"server_error\"}\n```\n\nSteps to reproduce:\n\n* Pull some model that supports tool (e.g. Qwen3:8b)\n* Serve the model\n* Use a client with tools in streaming mode (I used specifically the Java Langchain4j OpenAPI client)\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "jesuino",
      "author_type": "User",
      "created_at": "2025-05-21T14:30:08Z",
      "updated_at": "2025-05-27T11:34:04Z",
      "closed_at": "2025-05-27T11:34:04Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1431/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1431",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1431",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:38.325869",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Could you show precisely the command you are executing?",
          "created_at": "2025-05-22T12:25:29Z"
        },
        {
          "author": "ericcurtin",
          "body": "We need this merged in llama.cpp first:\n\nhttps://github.com/ggml-org/llama.cpp/pull/12379",
          "created_at": "2025-05-22T14:09:11Z"
        },
        {
          "author": "jesuino",
          "body": "Thanks, I was suspecting that it was at `llama.cpp` level\n\n@rhatdan I am actually using the serve mode. Here's my HTTP request:\n\n```\n2025-05-22 11:33:44,113 DEBUG [dev.lan.htt.cli.log.LoggingHttpClient] (JavaFX Application Thread) HTTP request:\n- method: POST\n- url: http://localhost:8080/chat/comple",
          "created_at": "2025-05-22T14:34:51Z"
        },
        {
          "author": "cgruver",
          "body": "@jesuino I am keeping a llama.cpp branch here - https://github.com/cgruver/llama.cpp/tree/tools\n\nIt is the code in https://github.com/ggml-org/llama.cpp/pull/12379 that I am trying to keep merged with upstream https://github.com/ggml-org/llama.cpp\n\nIf you want, you can build your own Ramalama with m",
          "created_at": "2025-05-22T18:30:28Z"
        }
      ]
    },
    {
      "issue_number": 1390,
      "title": "Add Moore Threads GPU support",
      "body": "### Feature request description\n\n[llama.cpp](https://github.com/ggml-org/llama.cpp) now supports Moore Threads GPUs through the `musa` backend.\nWould you be open to integrating this into `ramalama` as well?\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "yeahdongcn",
      "author_type": "User",
      "created_at": "2025-05-12T11:10:14Z",
      "updated_at": "2025-05-23T14:59:35Z",
      "closed_at": "2025-05-23T14:59:34Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1390/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1390",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1390",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:38.552090",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Yes open a PR with a new Containerfile in here for the new protocol, build it, test it:\n\nhttps://github.com/containers/ramalama/tree/main/container-images",
          "created_at": "2025-05-12T11:55:43Z"
        },
        {
          "author": "ericcurtin",
          "body": "You also want to figure out a way of auto-detecting a device is of type Moore Threads, so this image is automatically pulled",
          "created_at": "2025-05-12T11:57:23Z"
        },
        {
          "author": "ericcurtin",
          "body": "Also you'll notice we poke some holes as required for GPU's in this function:\n\nhttps://github.com/containers/ramalama/blob/4f240699da40e0e2da4fca94dd40501349106ef9/ramalama/engine.py#L117\n\nI dunno if there's any extra ones required for Moore Threads... But if there is, that's the place to put it\n",
          "created_at": "2025-05-12T15:19:30Z"
        },
        {
          "author": "yeahdongcn",
          "body": "> Also you'll notice we poke some holes as required for GPU's in this function:\n> \n> [ramalama/ramalama/engine.py](https://github.com/containers/ramalama/blob/4f240699da40e0e2da4fca94dd40501349106ef9/ramalama/engine.py#L117)\n> \n> Line 117 in [4f24069](/containers/ramalama/commit/4f240699da40e0e2da4f",
          "created_at": "2025-05-13T01:12:11Z"
        },
        {
          "author": "ericcurtin",
          "body": "Closing hopefully this makes next release, thanks for the contributions @yeahdongcn ",
          "created_at": "2025-05-23T14:59:34Z"
        }
      ]
    },
    {
      "issue_number": 1421,
      "title": "Hugging Face repo quantization tag shoud ignore case",
      "body": "### Issue Description\n\nThe Hugging Face repo quantization tag should ignore case\n\n### Steps to reproduce the issue\n\n```\n$ ramalama pull  hf://ggml-org/gemma-3-4b-it-GGUF:f16\n$ ramalama pull  hf://ggml-org/gemma-3-4b-it-GGUF:F16\n```\n\n### Describe the results you received\n\n```\n$ ramalama pull  hf://ggml-org/gemma-3-4b-it-GGUF:f16\nDownloading hf://ggml-org/gemma-3-4b-it-GGUF:f16 ...\nTrying to pull hf://ggml-org/gemma-3-4b-it-GGUF:f16 ...\n 99% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  |  811.43 MB/ 811.82 MB  58.66 MB/s        0s\n\n$ ramalama pull  hf://ggml-org/gemma-3-4b-it-GGUF:F16\nDownloading hf://ggml-org/gemma-3-4b-it-GGUF:F16 ...\nTrying to pull hf://ggml-org/gemma-3-4b-it-GGUF:F16 ...\nFetching 6 files:   0%|                                                                                                                                                                        | 0/6 [00:00<?, ?it/s]\n...\n\nTraceback (most recent call last):\n  File \"/home/owalsh/ramalamamm/ramalama/ramalama/huggingface.py\", line 442, in _pull_with_model_store\n    self.store.new_snapshot(tag, snapshot_hash, files)\n  File \"/home/owalsh/ramalamamm/ramalama/ramalama/model_store.py\", line 522, in new_snapshot\n    self._download_snapshot_files(model_tag, snapshot_hash, snapshot_files)\n  File \"/home/owalsh/ramalamamm/ramalama/ramalama/model_store.py\", line 455, in _download_snapshot_files\n    os.symlink(blob_relative_path, self.get_snapshot_file_path(snapshot_hash, file.name))\nFileExistsError: [Errno 17] File exists: '../../blobs/sha256-29f4b518b636635613894282bda2a01bad964c8d474c4147343057b7ac442d50' -> '/home/owalsh/.local/share/ramalama/store/huggingface/ggml-org/gemma-3-4b-it-GGUF/snapshots/sha256-29f4b518b636635613894282bda2a01bad964c8d474c4147343057b7ac442d50/gemma-3-4b-it-f16.gguf'\n```\n\n### Describe the results you expected\n\nThe 2nd pull should be considered identical to the first.\n\n### ramalama info output\n\n```yaml\nN/A\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "olliewalsh",
      "author_type": "User",
      "created_at": "2025-05-16T14:28:51Z",
      "updated_at": "2025-05-16T15:29:31Z",
      "closed_at": "2025-05-16T15:29:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1421/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1421",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1421",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:38.863963",
      "comments": []
    },
    {
      "issue_number": 1405,
      "title": "Multimodal/vision for \"ramlama serve\"",
      "body": "### Feature request description\n\nWhen running like so, we should detect there is a mmproj-* .gguf file, download two files and serve in a multimodal/vision way:\n\n`ramlama serve hf://ggml-org/SmolVLM-500M-Instruct-GGUF`\n\nHere is how to do it with just llama.cpp and two files:\n\n```\n#!/bin/bash\n\nmodel_file=\"/Users/ecurtin/.local/share/ramalama/store/huggingface/ggml-org/SmolVLM-500M-Instruct-GGUF/SmolVLM-500M-Instruct-Q8_0.gguf/blobs/sha256-9d4612de6a42214499e301494a3ecc2be0abdd9de44e663bda63f1152fad1bf4\"\nmmproj_file=\"/Users/ecurtin/.local/share/ramalama/store/huggingface/ggml-org/SmolVLM-500M-Instruct-GGUF/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf/blobs/sha256-d1eb8b6b23979205fdf63703ed10f788131a3f812c7b1f72e0119d5d81295150\"\n\nllama-server --model \"$model_file\" --mmproj \"$mmproj_file\"\n```\n\nEasy way to test is use index.html from here:\n\nhttps://github.com/ngxson/smolvlm-realtime-webcam\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-05-13T13:39:33Z",
      "updated_at": "2025-05-16T13:34:59Z",
      "closed_at": "2025-05-16T13:34:59Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1405/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1405",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1405",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:38.863992",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "We should probably do something like this if this makes sense:\n\n`--mount=type=bind,src=/Users/ecurtin/.local/share/ramalama/store/huggingface/ggml-org/SmolVLM-500M-Instruct-GGUF/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf/blobs/sha256-d1eb8b6b23979205fdf63703ed10f788131a3f812c7b1f72e0119d5d81295150,desti",
          "created_at": "2025-05-13T13:43:49Z"
        },
        {
          "author": "olliewalsh",
          "body": "Can't pull the top level of that repo:\n\n```ramalama pull hf://ggml-org/SmolVLM-500M-Instruct-GGUF\n   ...\n  Download complete. Moving file to /tmp/tmp6pablm2j/README.md\n  Download complete. Moving file to /tmp/tmp6pablm2j/.gitattributes\n  Download complete. Moving file to /tmp/tmp6pablm2j/mmproj-Smol",
          "created_at": "2025-05-13T15:35:04Z"
        },
        {
          "author": "ericcurtin",
          "body": "`ramlama serve hf://ggml-org/SmolVLM-500M-Instruct-GGUF`\n\nshould download what:\n\n`llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF`\n\ndoes by default.\n",
          "created_at": "2025-05-13T15:47:45Z"
        },
        {
          "author": "ericcurtin",
          "body": "@engelmi just tagging you so you are aware of this effort",
          "created_at": "2025-05-13T15:48:57Z"
        },
        {
          "author": "rhatdan",
          "body": "This makes sense to me, and looks like a good way to implement it.\n\n",
          "created_at": "2025-05-13T18:32:13Z"
        }
      ]
    },
    {
      "issue_number": 1382,
      "title": "make build fails with Docker due to --volume flag",
      "body": "### Issue Description\n\nIt looks like the following commit introduced a --volume flag that breaks Docker-based builds:\nhttps://github.com/containers/ramalama/commit/6c59fd7fd50ffe0048f1558f2062f0bbe76f706e\n\nAs a result, `make build` no longer works correctly in Docker environments.\n\n### Steps to reproduce the issue\n\n`make build` with Docker installed\n\n### Describe the results you received\n\ndocker build --no-cache --platform linux/x86_64 --volume=/home/xiaodongye/ws/ggml/ramalama:/run/ramalama --security-opt=label=disable -t quay.io/ramalama/ramalama -f ramalama/Containerfile .\nunknown flag: --volume\nSee 'docker buildx build --help'.\nmake: *** [Makefile:104: build] Error 125\n\n### Describe the results you expected\n\n`make build` pass\n\n### ramalama info output\n\n```yaml\nN/A\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "yeahdongcn",
      "author_type": "User",
      "created_at": "2025-05-10T12:35:48Z",
      "updated_at": "2025-05-13T09:33:41Z",
      "closed_at": "2025-05-12T09:44:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1382/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1382",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1382",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:40.942754",
      "comments": [
        {
          "author": "rhatdan",
          "body": "I guess the question is whether or not you want RamaLama installed within the container, if not, we could just build on docker and ignore an empty /run/ramalama\n\nAnother option would be to checkout the ramalama project and install the main branch. The current tool installs whatever is checkout in yo",
          "created_at": "2025-05-11T15:54:20Z"
        },
        {
          "author": "yeahdongcn",
          "body": "> I guess the question is whether or not you want RamaLama installed within the container, if not, we could just build on docker and ignore an empty /run/ramalama\n> \n> Another option would be to checkout the ramalama project and install the main branch. The current tool installs whatever is checkout",
          "created_at": "2025-05-13T09:33:39Z"
        }
      ]
    },
    {
      "issue_number": 1394,
      "title": "ramalama serve with quay.io/ramalama/cuda:0.8.3 image fails with ModuleNotFoundError: No module named 'ramalama'",
      "body": "### Issue Description\n\n```\nramalama --image quay.io/ramalama/cuda:0.8.3  serve deepseek-r1:7b\n\nserving on port 8080\nTraceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 17, in <module>\n    main(sys.argv[1:])\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 9, in main\n    from ramalama.common import exec_cmd\nModuleNotFoundError: No module named 'ramalama'\n```\n\nwhile `cuda:0.8.2` works so this looks like a regression in the cuda image build\n\n```\nramalama --image quay.io/ramalama/cuda:0.8.2  serve deepseek-r1:7b\n\nserving on port 8080\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n```\n\n### Steps to reproduce the issue\n\nRun `ramalama serve --image quay.io/ramalama/cuda:0.8.3 `\n\n### Describe the results you received\n\n```\nTraceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 17, in <module>\n    main(sys.argv[1:])\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 9, in main\n    from ramalama.common import exec_cmd\n```\n\n### Describe the results you expected\n\nModel is loaded and no errors reported\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpuset\",\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"hugetlb\",\n                    \"pids\",\n                    \"rdma\",\n                    \"misc\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.el9.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: 52f60d65890dbb8430d05c22545c0d4f96d0f4db\"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.84,\n                    \"systemPercent\": 0.08,\n                    \"userPercent\": 0.08\n                },\n                \"cpus\": 64,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"centos\",\n                    \"version\": \"9\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2045,\n                \"hostname\": \"compute02.remote-lab.net\",\n                \"idMappings\": {\n                    \"gidmap\": null,\n                    \"uidmap\": null\n                },\n                \"kernel\": \"5.14.0-582.el9.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 236097675264,\n                \"memTotal\": 269768204288,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-3.el9.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.0-1.el9.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.el9.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/0/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-1.el9.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": false,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.2-1.el9.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.2\\ncommit: 0f13345bcef588d2bb70d662d41e92ee8a816d85\\nlibslirp: 4.4.0\\nSLIRP_CONFIG_VERSION_MAX: 3\\nlibseccomp: 2.5.2\"\n                },\n                \"swapFree\": 34359734272,\n                \"swapTotal\": 34359734272,\n                \"uptime\": \"0h 44m 3.00s\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.access.redhat.com\",\n                    \"registry.redhat.io\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/etc/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 3,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 2\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {\n                    \"overlay.mountopt\": \"nodev,metacopy=on\"\n                },\n                \"graphRoot\": \"/var/lib/containers/storage\",\n                \"graphRootAllocated\": 1881881710592,\n                \"graphRootUsed\": 205119422464,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"false\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"true\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"true\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 13\n                },\n                \"runRoot\": \"/run/containers/storage\",\n                \"transientStore\": false,\n                \"volumePath\": \"/var/lib/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.0\",\n                \"Built\": 1746017502,\n                \"BuiltTime\": \"Wed Apr 30 15:51:42 2025\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.23.4 (Red Hat 1.23.4-1.el9)\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/local/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/var/lib/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.3\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "mcornea",
      "author_type": "User",
      "created_at": "2025-05-12T18:14:04Z",
      "updated_at": "2025-05-13T08:18:01Z",
      "closed_at": "2025-05-12T20:57:35Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1394/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1394",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1394",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:41.144431",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Working on it, should have new images up today.",
          "created_at": "2025-05-12T19:10:42Z"
        },
        {
          "author": "rhatdan",
          "body": "Could you try this again, I believe it is fixed, reopen if I am mistaken.",
          "created_at": "2025-05-12T20:57:35Z"
        },
        {
          "author": "mcornea",
          "body": "Thanks @rhatdan I can confirm it's fixed with the latest image.",
          "created_at": "2025-05-13T08:18:00Z"
        }
      ]
    },
    {
      "issue_number": 150,
      "title": "Vision models",
      "body": "## Value Statement\r\n\r\nAs someone who wants a boring way to use AI\r\nI would like to expose an image/PDF/document to the LLM\r\nSo that I can make requests and extract information, all within Ramalama\r\n\r\n## Notes\r\n\r\nVarious models now contain vision functionality, where they can ingest data from images, and answer questions about those images.  Recently, the accuracy of these LLM-based OCR text extractions can exceed that of dedicated OCR tooling (even paid products like AWS Textract).  The same vision models can also be used to extract information from PDF documents fairly easily after converting them to images.\r\n\r\nWe can use a similar interface to the planned Whisper.cpp implementation, since both are just contexts or data we provide to the LLMs.  This has not been detailed anywhere, so below is a proposal/example of how it could look.\r\n\r\n```\r\n$ ramalama run --context-file ./document.pdf phi3.5-vision\r\n>> When is this letter dated?\r\nThe date in the letter is `1st January 1999`\r\n\r\n>> What is this document about?\r\nThis document is an instruction manual detailing how to use Ramalama, a cool new way to run LLMs (Large Language Models) across Linux and MacOS.  It supports text and vision-based models.\r\n\r\n$ ramalama run --context-file ./painting.png phi3.5-vision\r\n>> What is in the painting?\r\nThis is an abstract oil painting about something and something else.  It seems to be inspired by some artist.\r\n```\r\n\r\nThe primary issue is neither ollama or llama.cpp support vision models at this moment, so would either need a custom implementation, or would require adding something like vllm.",
      "state": "open",
      "author": "p5",
      "author_type": "User",
      "created_at": "2024-09-16T16:56:17Z",
      "updated_at": "2025-05-12T22:16:08Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/150/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 2,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/150",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/150",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:41.407183",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "We had intended on merging vllm support soon, we started it here:\r\n\r\nhttps://github.com/containers/ramalama/pull/97\r\n\r\nthis is what we think an outline of what it should look like, basically we want to introduce a --runtime flag, kinda like like  the podman one that switches between crun, runc, krun",
          "created_at": "2024-09-17T14:53:12Z"
        },
        {
          "author": "ericcurtin",
          "body": "@rhatdan merged the first vllm-related PR, I dunno if you want to take a stab at implementing the other things you had in mind @p5 ",
          "created_at": "2024-09-24T00:53:35Z"
        },
        {
          "author": "rhatdan",
          "body": "@p5 still interested in this?",
          "created_at": "2024-10-14T12:11:32Z"
        },
        {
          "author": "p5",
          "body": "Hey Dan, Eric\n\nMy free time is very limited at the minute.  Starting a new job in 2 weeks and there's a lot to get in order.\n\nI still feel vision models would be a great addition to ramalama, but I'm going to be in a Windows-only environment :sigh: so unsure how much I'll be able to help out.",
          "created_at": "2024-10-14T12:20:08Z"
        },
        {
          "author": "rhatdan",
          "body": "Thanks @p5, good luck with the new job.",
          "created_at": "2024-10-14T12:22:12Z"
        }
      ]
    },
    {
      "issue_number": 1376,
      "title": "openvino image broken on MacOS ARM",
      "body": "### Issue Description\n\nWhen starting the container, rosetta emits an error : rosetta error: failed to open elf at /lib64/ld-linux-x86-64.so.2 although the image is for aarch64\n\n### Steps to reproduce the issue\n\nSee Podman AI Lab PR or run a model using quay.io/ramalama/openvino image\n\n### Describe the results you received\n\nError reported in container log\n\n### Describe the results you expected\n\nContainer should start\n\n### ramalama info output\n\n```yaml\nN/A\n```\n\n### Upstream Latest Release\n\nNo\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "jeffmaury",
      "author_type": "User",
      "created_at": "2025-05-09T17:37:09Z",
      "updated_at": "2025-05-12T19:31:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1376/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1376",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1376",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:41.606255",
      "comments": [
        {
          "author": "rhatdan",
          "body": "@bmahabirbu PTAL",
          "created_at": "2025-05-10T10:40:43Z"
        },
        {
          "author": "rhatdan",
          "body": "This looks like there is only X86 support, did the former AI Lab Recipes image work on Arm?\nOpenVino does not seem to have Arm binaries.",
          "created_at": "2025-05-10T10:50:43Z"
        },
        {
          "author": "bmahabirbu",
          "body": "As of now I believe that openvino model server doest support arm which is odd as the regular toolkit does ",
          "created_at": "2025-05-12T19:26:06Z"
        },
        {
          "author": "bmahabirbu",
          "body": "https://github.com/openvinotoolkit/model_server/issues/2648",
          "created_at": "2025-05-12T19:31:19Z"
        }
      ]
    },
    {
      "issue_number": 598,
      "title": "Provide model info in chat ui & allow multiple models",
      "body": "Rather than serving all models with generic `model.file` filename, ramalama should provide more information about the currently served or loaded models.\n\nAlso, could allow passing a model-config file to allow to switch easily between models with a single instance of a server. \nhttps://llama-cpp-python.readthedocs.io/en/latest/server/#configuration-and-multi-model-support \n\nHere, I've renamed the file ramalama downloaded to `granite-code` so it shows at llamacpp:port/models.\n![Image](https://github.com/user-attachments/assets/25ac6fed-f7e7-4fb7-9890-bca1210b25b1)\n\nCurrently, any model served by ramalama is listed as `model.file` \n![Image](https://github.com/user-attachments/assets/71c7316b-34d7-4233-9510-56ddd0175a9a)\n",
      "state": "open",
      "author": "sallyom",
      "author_type": "User",
      "created_at": "2025-01-16T22:38:33Z",
      "updated_at": "2025-05-12T12:10:33Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 25,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/598/reactions",
        "total_count": 6,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 6,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/598",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/598",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:41.799273",
      "comments": []
    },
    {
      "issue_number": 1206,
      "title": "ramalama unecessarily expects /usr/bin/nvidia-container-runtime on Fedora 42",
      "body": "### Issue Description\n\nI setup ramalama on Fedora 42 according to the instructions. `podman run --rm --device=nvidia.com/gpu=all fedora nvidia-smi` shows the nvidia device.\n\n`ramalama run granite` fails with: `Error: no valid executable found for OCI runtime /usr/bin/nvidia-container-runtime: invalid argument`\n\nI am using golang-github-nvidia-container-toolkit-1.17.3-1.fc42 and python3-ramalama-0.7.2-1.fc42. The file`/usr/bin/nvidia-container-runtime` does not exist on my system with these packages installed.\n\nI noticed that removing https://github.com/containers/ramalama/blob/main/ramalama/model.py#L322 makes ramalama work as expected.\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. dnf install  nvidia-container-toolkit python3-ramalama\n2. nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n3. nvidia-ctk config --in-place --set nvidia-container-runtime.mode=cdi # not sure if this is needed\n4. ramalama run granite\n\n\n\n### Describe the results you received\n\nError: no valid executable found for OCI runtime /usr/bin/nvidia-container-runtime: invalid argument\n\n### Describe the results you expected\n\nramalama prompt\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 97.5,\n                    \"systemPercent\": 0.45,\n                    \"userPercent\": 2.05\n                },\n                \"cpus\": 24,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2041,\n                \"hostname\": \"genius\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.2-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 106652884992,\n                \"memTotal\": 134821318656,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"1h 6m 27.00s (Approximately 0.04 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/till/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 6,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 5\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/till/.local/share/containers/storage\",\n                \"graphRootAllocated\": 536608768000,\n                \"graphRootUsed\": 109001195520,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 24\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/till/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Wed Apr  2 02:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/till/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.2\"\n}\n```\n\n### Upstream Latest Release\n\nNo\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "tyll",
      "author_type": "User",
      "created_at": "2025-04-16T21:05:41Z",
      "updated_at": "2025-05-10T12:03:39Z",
      "closed_at": "2025-04-17T12:18:45Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1206/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1206",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1206",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:41.799295",
      "comments": [
        {
          "author": "rhatdan",
          "body": "On my machine without it I get:\n\n./bin/ramalama run granite\nError: crun: cannot stat `/usr/lib64/libEGL_nvidia.so.570.124.06`: No such file or directory: OCI runtime attempted to invoke a command that was not found\n\nI can do an access check on it, and not include it if it is not installed.",
          "created_at": "2025-04-16T21:45:53Z"
        },
        {
          "author": "tyll",
          "body": "How did you get the file? The RPM does not contain it for me. `/usr/lib64/libEGL_nvidia.so.570.124.0` does not exist for me, either. I have /usr/lib64/libEGL_nvidia.so.570.133.07. I use the nvidia drivers from rpmfusion: /etc/yum.repos.d/rpmfusion-nonfree-nvidia-driver.repo - maybe it is related to ",
          "created_at": "2025-04-16T22:01:33Z"
        },
        {
          "author": "rhatdan",
          "body": "I am on Fedora 42 and get the drivers from the same place.",
          "created_at": "2025-04-17T09:07:57Z"
        },
        {
          "author": "ericcurtin",
          "body": "There's Fedora install steps here:\n\nhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\n\nit is better if we always install nvidia-container-runtime , the Container can change at any point. nvidia-container-runtime ensures guest container and host OS stay on co",
          "created_at": "2025-04-17T09:52:29Z"
        },
        {
          "author": "tyll",
          "body": "I installed the nvidia-container-toolkit as it was described in https://github.com/containers/ramalama/blob/main/docs/ramalama-cuda.7.md with a plain\n\n`dnf install -y nvidia-container-toolkit` - this installs golang-github-nvidia-container-toolkit-1.17.3-1.fc42.x86_64 on Fedora 42 from the official ",
          "created_at": "2025-04-17T10:20:10Z"
        }
      ]
    },
    {
      "issue_number": 1377,
      "title": "Add support for vision model",
      "body": "### Feature request description\n\nllama.cpp now supports vision model, see: https://simonwillison.net/2025/May/10/llama-cpp-vision/ and https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal.md .\n\nHere is an example usage with the rpc-server built with the latest upstream:\n\n```ShellSession\n$ ./build/bin/rpc-server -c -p 8080 &\n$ ./build/bin/llama-mtmd-cli -hf unsloth/gemma-3-4b-it-GGUF:Q4_K_XL --rpc 127.0.0.1:8080\n...\n> /image /usr/share/icons/Adwaita/16x16/devices/audio-headphones.png\nImage /usr/share/icons/Adwaita/16x16/devices/audio-headphones.png loaded\n\n> Describe the image\n...\n``` \n\n### Suggest potential solution\n\n- [x] Update the llama.cpp version in `container-images/scripts/build_llama_and_whisper.sh`\n- [ ] Adjust the client to use the mtmd for handling multimodal inputs.\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "TristanCacqueray",
      "author_type": "User",
      "created_at": "2025-05-10T09:07:03Z",
      "updated_at": "2025-05-10T10:32:41Z",
      "closed_at": "2025-05-10T10:32:41Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1377/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1377",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1377",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:42.061529",
      "comments": []
    },
    {
      "issue_number": 1374,
      "title": "Ramalama run fails with OLMoE model",
      "body": "### Issue Description\n\nOLMoE (https://github.com/allenai/OLMoE) is an open model with all data, code and logs. When trying to run it with ramalama, there is only repeating metadata visible in the chat.\n\n### Steps to reproduce the issue\n\n```\nramalama  run  hf://allenai/OLMoE-1B-7B-0924-GGUF/olmoe-1b-7b-0924-q4_k_m.gguf\n\nðŸ¦­ > what is olmoe?\n```\n\n### Describe the results you received\n\nendless output of:\n```\nwhat is assisant?<|im_end|>\n<|im_start|>assitant\nwhat is aidtant?<|im_end|>\n<|im_start|>assistant\nwhat is assistat?<|im_end|>\n<|im_start|>assistat\nwhat is asissta?<|im_end|>\n<|im_start|>assistance\nwhat is aidtance?<|im_end|>\n<|im_start|>assistance\nwhat is aidsta?<|im_end|>\n<|im_start|>assistat\nwhat is asista?<|im^C\nðŸ¦­ > \n\n```\n\n### Describe the results you expected\n\nA proper response from the model.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 97.11,\n                    \"systemPercent\": 0.43,\n                    \"userPercent\": 2.46\n                },\n                \"cpus\": 24,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2042,\n                \"hostname\": \"genius\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.5-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 106559668224,\n                \"memTotal\": 134821134336,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250503.g587980c-1.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"0h 17m 44.00s\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/till/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 5,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 5\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/till/.local/share/containers/storage\",\n                \"graphRootAllocated\": 536608768000,\n                \"graphRootUsed\": 163425890304,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 34\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/till/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Wed Apr  2 02:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/till/.local/share/ramalama/shortnames.conf\",\n            \"/home/till/.config/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-i\": \"hf://ibm-granite/granite-8b-code-instruct-4k-GGUF/granite-8b-code-instruct.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/till/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "tyll",
      "author_type": "User",
      "created_at": "2025-05-09T16:18:22Z",
      "updated_at": "2025-05-09T16:18:22Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1374/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1374",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1374",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:42.061551",
      "comments": []
    },
    {
      "issue_number": 1362,
      "title": "llama-server doesn't start with ramalama serve --rag: ModuleNotFoundError: No module named 'ramalama'",
      "body": "### Issue Description\n\nllama-server doesn't start with ramalama serve --rag\n\n`/tmp/llama-server.log` shows \n\n```\nTraceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 17, in <module>\n    main(sys.argv[1:])\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 9, in main\n    from ramalama.common import exec_cmd\nModuleNotFoundError: No module named 'ramalama'\n```\n\nChecking the container it looks like python3 points to python3.11 while the ramalama module was installed for python3.9:\n\n```\nbash-5.1# ls -l /usr/bin/python3\nlrwxrwxrwx. 1 root root 19 May  5 10:52 /usr/bin/python3 -> /usr/bin/python3.11\n\nbash-5.1# ls -l /usr/lib/python3.11/site-packages/\ntotal 8\ndrwxr-xr-x. 2 root root    6 Apr  3 09:36 __pycache__\ndrwxr-xr-x. 3 root root   63 May  5 10:52 _distutils_hack\n-rw-r--r--. 1 root root  151 Jul 25  2024 distutils-precedence.pth\ndrwxr-xr-x. 5 root root  136 May  5 10:52 pip\ndrwxr-xr-x. 2 root root  133 May  5 10:52 pip-22.3.1.dist-info\ndrwxr-xr-x. 5 root root   73 May  5 10:52 pkg_resources\ndrwxr-xr-x. 8 root root 4096 May  5 10:52 setuptools\ndrwxr-xr-x. 2 root root  143 May  5 10:52 setuptools-65.5.1.dist-info\n\n\nbash-5.1# ls -l /usr/lib/python3.9/site-packages/\ntotal 220\ndrwxr-xr-x. 2 root root    90 Mar 13 07:22 PySocks-1.7.1-py3.9.egg-info\ndrwxr-xr-x. 2 root root  4096 Mar 13 07:22 __pycache__\ndrwxr-xr-x. 3 root root    63 Mar 13 07:22 _distutils_hack\ndrwxr-xr-x. 5 root root  4096 Mar 13 07:22 chardet\ndrwxr-xr-x. 2 root root   112 Mar 13 07:22 chardet-4.0.0.dist-info\ndrwxr-xr-x. 6 root root  4096 Mar 13 07:22 dateutil\ndrwxr-xr-x. 2 root root   126 Mar 13 07:22 decorator-4.4.2-py3.9.egg-info\n-rw-r--r--. 1 root root 17222 Feb 29  2020 decorator.py\n-rw-r--r--. 1 root root   152 Jul 25  2024 distutils-precedence.pth\ndrwxr-xr-x. 9 root root  4096 Mar 13 07:22 dnf\ndrwxr-xr-x. 3 root root  4096 Mar 13 07:22 dnf-plugins\ndrwxr-xr-x. 3 root root    44 Mar 13 07:22 dnfpluginscore\ndrwxr-xr-x. 4 root root    41 Mar 13 07:22 gi\ndrwxr-xr-x. 3 root root   174 Mar 13 07:22 idna\ndrwxr-xr-x. 2 root root    90 Mar 13 07:22 idna-2.10-py3.9.egg-info\ndrwxr-xr-x. 3 root root   131 Mar 13 07:22 iniparse\ndrwxr-xr-x. 2 root root   110 Mar 13 07:22 iniparse-0.4-py3.9.egg-info\ndrwxr-xr-x. 5 root root   111 May  5 10:51 pip\ndrwxr-xr-x. 2 root root   133 May  5 10:51 pip-21.3.1.dist-info\ndrwxr-xr-x. 5 root root    73 Mar 13 07:22 pkg_resources\ndrwxr-xr-x. 3 root root    93 Mar 13 07:22 pygtkcompat\ndrwxr-xr-x. 2 root root   114 Mar 13 07:22 pyinotify-0.9.6-py3.9.egg-info\n-rw-r--r--. 1 root root 89008 Aug 10  2021 pyinotify.py\ndrwxr-xr-x. 2 root root   126 Mar 13 07:22 python_dateutil-2.8.1-py3.9.egg-info\ndrwxr-xr-x. 3 root root  4096 May  5 10:18 ramalama\ndrwxr-xr-x. 3 root root   143 May  5 10:18 ramalama-0.8.2.dist-info\ndrwxr-xr-x. 3 root root  4096 Mar 13 07:22 requests\ndrwxr-xr-x. 2 root root    88 Mar 13 07:22 requests-2.25.1.dist-info\ndrwxr-xr-x. 7 root root  4096 Mar 13 07:22 setuptools\ndrwxr-xr-x. 2 root root   171 Mar 13 07:22 setuptools-53.0.0.dist-info\ndrwxr-xr-x. 2 root root   119 Mar 13 07:22 six-1.15.0.dist-info\n-rw-r--r--. 1 root root 34287 Feb 16  2022 six.py\n-rw-r--r--. 1 root root 31086 Sep 20  2019 socks.py\n-rw-r--r--. 1 root root  3966 Sep 20  2019 sockshandler.py\ndrwxr-xr-x. 6 root root  4096 Mar 13 07:22 urllib3\ndrwxr-xr-x. 2 root root   110 Mar 13 07:22 urllib3-1.26.5-py3.9.egg-info\n\n\nbash-5.1# python3.11 /usr/libexec/ramalama/ramalama-serve-core llama-server --port 8080 --model /mnt/models/model.file --alias qwen2.5-coder:14b --ctx-size 2048 --temp 0.8 --jinja --cache-reuse 256 -v --flash-attn -ngl 999 --threads 32 --host 0.0.0.0\nTraceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 17, in <module>\n    main(sys.argv[1:])\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 9, in main\n    from ramalama.common import exec_cmd\nModuleNotFoundError: No module named 'ramalama'\n\nbash-5.1# python3.9 /usr/libexec/ramalama/ramalama-serve-core llama-server --port 8080 --model /mnt/models/model.file --alias qwen2.5-coder:14b --ctx-size 2048 --temp 0.8 --jinja --cache-reuse 256 -v --flash-attn -ngl 999 --threads 32 --host 0.0.0.0\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nbuild: 5177 (80982e81) with cc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7) for x86_64-redhat-linux\nsystem info: n_threads = 32, n_threads_batch = 32, total_threads = 64\n```\n\n### Steps to reproduce the issue\n\n`ramalama --debug serve --rag quay.io/mariuscornea/ragtest qwen2.5-coder:14b` \n\n\n\n### Describe the results you received\n\n```\nrun_cmd:  podman image inspect quay.io/mariuscornea/ragtest\nWorking directory: None\nIgnore stderr: False\nIgnore all: False\nCommand finished with return code: 0\nChecking if 8080 is available\nserving on port 8080\nrun_cmd:  podman inspect quay.io/ramalama/cuda-rag:0.8\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nrun_cmd:  podman inspect quay.io/ramalama/cuda-rag:0.8\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nexec_cmd:  podman run --rm --label ai.ramalama.model=qwen2.5-coder:14b --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.port=8080 --label ai.ramalama.command=serve --device /dev/dri --device nvidia.com/gpu=all -e CUDA_VISIBLE_DEVICES=0 --runtime /usr/bin/nvidia-container-runtime -p 8080:8080 --security-opt=label=disable --cap-drop=all --security-opt=no-new-privileges --pull newer --mount=type=image,source=quay.io/mariuscornea/ragtest,destination=/rag,rw=true -t -i --label ai.ramalama --name ramalama_YpviVgxKwR --env=HOME=/tmp --init --label ai.ramalama.model=qwen2.5-coder:14b --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.port=8080 --label ai.ramalama.command=serve --mount=type=bind,src=/var/lib/ramalama/store/ollama/qwen2.5-coder/qwen2.5-coder/blobs/sha256-ac9bc7a69dab38da1c790838955f1293420b55ab555ef6b4615efa1c1507b1ed,destination=/mnt/models/model.file,ro --mount=type=bind,src=/var/lib/ramalama/store/ollama/qwen2.5-coder/qwen2.5-coder/snapshots/sha256-ac9bc7a69dab38da1c790838955f1293420b55ab555ef6b4615efa1c1507b1ed/chat_template_converted,destination=/mnt/models/chat_template.file,ro quay.io/ramalama/cuda-rag:0.8 bash -c \"nohup /usr/libexec/ramalama/ramalama-serve-core llama-server --port 8080 --model /mnt/models/model.file --alias qwen2.5-coder:14b --ctx-size 2048 --temp 0.8 --jinja --cache-reuse 256 -v --flash-attn -ngl 999 --threads 32 --host 0.0.0.0 &> /tmp/llama-server.log & rag_framework run /rag/vector.db\"\n> w\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/usr/lib64/python3.11/contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 969, in request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n  File \"/usr/lib64/python3.11/contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/bin/rag_framework\", line 217, in <module>\n    args.func(args.vector_path)  # pass vector_path argument to the respective function\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/bin/rag_framework\", line 182, in run_rag\n    rag.cmdloop()\n  File \"/usr/lib64/python3.11/cmd.py\", line 138, in cmdloop\n    stop = self.onecmd(line)\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.11/cmd.py\", line 216, in onecmd\n    return self.default(line)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/bin/rag_framework\", line 143, in default\n    self.query(user_content)\n  File \"/usr/bin/rag_framework\", line 105, in query\n    response = self.llm.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1239, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1001, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\n````\n\n### Describe the results you expected\n\n`llama-server` starts without errors\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpuset\",\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"hugetlb\",\n                    \"pids\",\n                    \"rdma\",\n                    \"misc\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.el9.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: 52f60d65890dbb8430d05c22545c0d4f96d0f4db\"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 98.94,\n                    \"systemPercent\": 0.15,\n                    \"userPercent\": 0.91\n                },\n                \"cpus\": 64,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"centos\",\n                    \"version\": \"9\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2045,\n                \"hostname\": \"compute02.remote-lab.net\",\n                \"idMappings\": {\n                    \"gidmap\": null,\n                    \"uidmap\": null\n                },\n                \"kernel\": \"5.14.0-582.el9.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 188717056000,\n                \"memTotal\": 269768208384,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-3.el9.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.0-1.el9.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.el9.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/0/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-1.el9.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": false,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.2-1.el9.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.2\\ncommit: 0f13345bcef588d2bb70d662d41e92ee8a816d85\\nlibslirp: 4.4.0\\nSLIRP_CONFIG_VERSION_MAX: 3\\nlibseccomp: 2.5.2\"\n                },\n                \"swapFree\": 34359734272,\n                \"swapTotal\": 34359734272,\n                \"uptime\": \"2h 22m 20.00s (Approximately 0.08 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.access.redhat.com\",\n                    \"registry.redhat.io\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/etc/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 3,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 3\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {\n                    \"overlay.mountopt\": \"nodev,metacopy=on\"\n                },\n                \"graphRoot\": \"/var/lib/containers/storage\",\n                \"graphRootAllocated\": 1881881710592,\n                \"graphRootUsed\": 200829624320,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"false\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"true\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"true\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 12\n                },\n                \"runRoot\": \"/run/containers/storage\",\n                \"transientStore\": false,\n                \"volumePath\": \"/var/lib/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.0\",\n                \"Built\": 1746017502,\n                \"BuiltTime\": \"Wed Apr 30 15:51:42 2025\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.23.4 (Red Hat 1.23.4-1.el9)\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/local/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/var/lib/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n`/tmp/llama-server.log` in the created container shows the following error:\n\n```\npodman exec -it ramalama_JSLf2kbtcG cat /tmp/llama-server.log\nTraceback (most recent call last):\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 17, in <module>\n    main(sys.argv[1:])\n  File \"/usr/libexec/ramalama/ramalama-serve-core\", line 9, in main\n    from ramalama.common import exec_cmd\nModuleNotFoundError: No module named 'ramalama'\n```",
      "state": "closed",
      "author": "mcornea",
      "author_type": "User",
      "created_at": "2025-05-07T20:07:38Z",
      "updated_at": "2025-05-09T12:16:59Z",
      "closed_at": "2025-05-09T12:16:59Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1362/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1362",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1362",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:42.061561",
      "comments": [
        {
          "author": "rhatdan",
          "body": "This is happening because of a mixup between python versions.  We need to move away from ubi9 and get to ubi10 or move to fedora:42.\n\nSadly llama.cpp can not be built with fedora 42 gcc compiler at this time.",
          "created_at": "2025-05-08T13:52:27Z"
        }
      ]
    },
    {
      "issue_number": 1354,
      "title": "On a system with dual 3090 Nvidia GPUs a single GPU is used when running ramalama serve and fails with cudaMalloc failed: out of memory as the model is too large for a single GPU",
      "body": "### Issue Description\n\nOn a system with dual 3090 Nvidia GPUs a single GPU is used when running ramalama serve and fails with cudaMalloc failed: out of memory as the model is too large for a single GPU.\n\n```\nnvidia-smi\nTue May  6 22:09:48 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off |   00000000:06:00.0 Off |                  N/A |\n| 37%   27C    P8              9W /  275W |       4MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 3090        Off |   00000000:21:00.0 Off |                  N/A |\n| 38%   26C    P8              7W /  275W |       4MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n```\n\n### Steps to reproduce the issue\n\n`ramalama serve deepseek-r1:70b`\n\n### Describe the results you received\n\n```\n[root@compute02 ~]# ramalama serve deepseek-r1:70b\nserving on port 8080\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nbuild: 5177 (80982e81) with cc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7) for x86_64-redhat-linux\nsystem info: n_threads = 32, n_threads_batch = 32, total_threads = 64\n\nsystem_info: n_threads = 32 (n_threads_batch = 32) / 64 | CUDA : ARCHS = 500,610,700,750,800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 63\nmain: loading model\nsrv    load_model: loading model '/mnt/models/model.file'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23872 MiB free\nllama_model_loader: loaded meta data with 30 key-value pairs and 724 tensors from /mnt/models/model.file (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 70B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Llama\nllama_model_loader: - kv   4:                         general.size_label str              = 70B\nllama_model_loader: - kv   5:                          llama.block_count u32              = 80\nllama_model_loader: - kv   6:                       llama.context_length u32              = 131072\nllama_model_loader: - kv   7:                     llama.embedding_length u32              = 8192\nllama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 28672\nllama_model_loader: - kv   9:                 llama.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                          general.file_type u32              = 15\nllama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128001\nllama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128001\nllama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  162 tensors\nllama_model_loader: - type q4_K:  441 tensors\nllama_model_loader: - type q5_K:   40 tensors\nllama_model_loader: - type q6_K:   81 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 39.59 GiB (4.82 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 8192\nprint_info: n_layer          = 80\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 28672\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 70B\nprint_info: model params     = 70.55 B\nprint_info: general.name     = DeepSeek R1 Distill Llama 70B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<ï½œbeginâ–ofâ–sentenceï½œ>'\nprint_info: EOS token        = 128001 '<ï½œendâ–ofâ–sentenceï½œ>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: PAD token        = 128001 '<ï½œendâ–ofâ–sentenceï½œ>'\nprint_info: LF token         = 198 'ÄŠ'\nprint_info: EOG token        = 128001 '<ï½œendâ–ofâ–sentenceï½œ>'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 39979.48 MiB on device 0: cudaMalloc failed: out of memory\nalloc_tensor_range: failed to allocate CUDA0 buffer of size 41921528064\nllama_model_load: error loading model: unable to allocate CUDA0 buffer\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model '/mnt/models/model.file'\nsrv    load_model: failed to load model, '/mnt/models/model.file'\nsrv    operator(): operator(): cleaning up before exit...\nmain: exiting due to model loading error\n```\n\n### Describe the results you expected\n\nI'd expect the model gets loaded on both GPUs and the `cudaMalloc failed: out of memory` error would not show up.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.3\",\n                \"cgroupControllers\": [\n                    \"cpuset\",\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"hugetlb\",\n                    \"pids\",\n                    \"rdma\",\n                    \"misc\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.el9.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: 52f60d65890dbb8430d05c22545c0d4f96d0f4db\"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.35,\n                    \"systemPercent\": 0.41,\n                    \"userPercent\": 0.24\n                },\n                \"cpus\": 64,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"centos\",\n                    \"version\": \"9\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2045,\n                \"hostname\": \"compute02.remote-lab.net\",\n                \"idMappings\": {\n                    \"gidmap\": null,\n                    \"uidmap\": null\n                },\n                \"kernel\": \"5.14.0-580.el9.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 187548078080,\n                \"memTotal\": 269768187904,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-3.el9.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.0-1.el9.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.el9.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/0/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-1.el9.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": false,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.2-1.el9.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.2\\ncommit: 0f13345bcef588d2bb70d662d41e92ee8a816d85\\nlibslirp: 4.4.0\\nSLIRP_CONFIG_VERSION_MAX: 3\\nlibseccomp: 2.5.2\"\n                },\n                \"swapFree\": 34359734272,\n                \"swapTotal\": 34359734272,\n                \"uptime\": \"0h 47m 33.00s\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.access.redhat.com\",\n                    \"registry.redhat.io\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/etc/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 3,\n                    \"paused\": 0,\n                    \"running\": 2,\n                    \"stopped\": 1\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {\n                    \"overlay.mountopt\": \"nodev,metacopy=on\"\n                },\n                \"graphRoot\": \"/var/lib/containers/storage\",\n                \"graphRootAllocated\": 1881881710592,\n                \"graphRootUsed\": 165828968448,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"false\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"true\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"true\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 6\n                },\n                \"runRoot\": \"/run/containers/storage\",\n                \"transientStore\": false,\n                \"volumePath\": \"/var/lib/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.0\",\n                \"Built\": 1742308875,\n                \"BuiltTime\": \"Tue Mar 18 16:41:15 2025\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.23.4 (Red Hat 1.23.4-1.el9)\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.0\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/local/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral-small3.1\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral-small3.1:24b\": \"hf://bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ2_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/var/lib/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nThis is a bare metal machine with CentOS Stream 9.\n\n### Additional information\n\nLooking at the container created by ramalama I can see the podman run command includes `CUDA_VISIBLE_DEVICES=0` env var which restricts the visible devices to GPU 0. Adding `podman inspect` output below:\n\n```\n[\n     {\n          \"Id\": \"5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b\",\n          \"Created\": \"2025-05-06T22:00:41.32663793+03:00\",\n          \"Path\": \"/run/podman-init\",\n          \"Args\": [\n               \"--\",\n               \"/usr/libexec/ramalama/ramalama-serve-core\",\n               \"llama-server\",\n               \"--port\",\n               \"8080\",\n               \"--model\",\n               \"/mnt/models/model.file\",\n               \"--alias\",\n               \"deepseek-r1:70b\",\n               \"--ctx-size\",\n               \"2048\",\n               \"--temp\",\n               \"0.8\",\n               \"--jinja\",\n               \"--cache-reuse\",\n               \"256\",\n               \"--flash-attn\",\n               \"-ngl\",\n               \"999\",\n               \"--threads\",\n               \"32\",\n               \"--host\",\n               \"0.0.0.0\"\n          ],\n          \"State\": {\n               \"OciVersion\": \"1.2.0\",\n               \"Status\": \"running\",\n               \"Running\": true,\n               \"Paused\": false,\n               \"Restarting\": false,\n               \"OOMKilled\": false,\n               \"Dead\": false,\n               \"Pid\": 5005,\n               \"ConmonPid\": 4997,\n               \"ExitCode\": 0,\n               \"Error\": \"\",\n               \"StartedAt\": \"2025-05-06T22:00:41.726015185+03:00\",\n               \"FinishedAt\": \"0001-01-01T00:00:00Z\",\n               \"CgroupPath\": \"/machine.slice/libpod-5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b.scope\",\n               \"CheckpointedAt\": \"0001-01-01T00:00:00Z\",\n               \"RestoredAt\": \"0001-01-01T00:00:00Z\"\n          },\n          \"Image\": \"91434132d805028800c46c87aa0f74f61efebc07409fe416dfb72d171705c654\",\n          \"ImageDigest\": \"sha256:671fffebb5b741e705c6a8e862324e16db0b27bd99809089dbe87a76d845822f\",\n          \"ImageName\": \"quay.io/ramalama/cuda:0.8\",\n          \"Rootfs\": \"\",\n          \"Pod\": \"\",\n          \"ResolvConfPath\": \"/run/containers/storage/overlay-containers/5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b/userdata/resolv.conf\",\n          \"HostnamePath\": \"/run/containers/storage/overlay-containers/5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b/userdata/hostname\",\n          \"HostsPath\": \"/run/containers/storage/overlay-containers/5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b/userdata/hosts\",\n          \"StaticDir\": \"/var/lib/containers/storage/overlay-containers/5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b/userdata\",\n          \"OCIConfigPath\": \"/var/lib/containers/storage/overlay-containers/5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b/userdata/config.json\",\n          \"OCIRuntime\": \"/usr/bin/nvidia-container-runtime\",\n          \"ConmonPidFile\": \"/run/containers/storage/overlay-containers/5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b/userdata/conmon.pid\",\n          \"PidFile\": \"/run/containers/storage/overlay-containers/5604715c8b96f289cddb28cd6b51dd9359e08c823d6c216d9fe291a54f4cf93b/userdata/pidfile\",\n          \"Name\": \"ramalama_M9OGd1Z2OB\",\n          \"RestartCount\": 0,\n          \"Driver\": \"overlay\",\n          \"MountLabel\": \"system_u:object_r:container_file_t:s0:c1022,c1023\",\n          \"ProcessLabel\": \"\",\n          \"AppArmorProfile\": \"\",\n          \"EffectiveCaps\": null,\n          \"BoundingCaps\": null,\n          \"ExecIDs\": [],\n          \"GraphDriver\": {\n               \"Name\": \"overlay\",\n               \"Data\": {\n                    \"LowerDir\": \"/var/lib/containers/storage/overlay/ff0f696ce878527613a1fcdbf22386ba73ee7f1bf04a4d660a07ecb1c5132786/diff:/var/lib/containers/storage/overlay/fde386c0b246bd627dc025c35ac0c81e2bae148f05c753a43e06de41b22ba232/diff:/var/lib/containers/storage/overlay/3de4400a63f8c076f38d29d7942f7a56739da66c579ff573fb53858f09957865/diff:/var/lib/containers/storage/overlay/676f4db7a9731a1d10ad91c3acb6c563e29c89616d6283b793efaa5c7d7021ae/diff:/var/lib/containers/storage/overlay/b767adff1476e5b5346ddd0176d7b46c30aa3046111bb23a4b1738c08e964258/diff:/var/lib/containers/storage/overlay/193363f352156ccdf21fe12403c05b91e6b2e7ce153bbc76bc1bc0781bc5ebd2/diff:/var/lib/containers/storage/overlay/148e61e05f2f21b8cbf5c7fc265e049d9d6872529e1eb705be3544902b3b6d4e/diff:/var/lib/containers/storage/overlay/389c76fba61ec2dafe5e7902df3769831a92b0e852f09575b925325ce00a3644/diff:/var/lib/containers/storage/overlay/6dac60fce80b45e5534582b82a64f30fb0f8f63dcb3cf235540ffd27b8cb22d9/diff:/var/lib/containers/storage/overlay/441fdf66dda1e4051e4b478d61e56c5ba4cdbe418ed6b40d1322ec6e3521a5e4/diff:/var/lib/containers/storage/overlay/eca092a1244e3114d68f1a344d6f77ffde83ec3b4589ebebe16899012ef0d501/diff:/var/lib/containers/storage/overlay/106755864f545e21ddaee821db360e776660fb63474d339aa74392c3764676bd/diff:/var/lib/containers/storage/overlay/911664f515d7bef00e9c355cc10e898974f31329f87261d5b82935f5cddb97ed/diff:/var/lib/containers/storage/overlay/5ab036872dd7cc27d1d24fc1cccc763a8827061155e08ae1ccaf067ec5a755a2/diff\",\n                    \"MergedDir\": \"/var/lib/containers/storage/overlay/335bbb6f4ea27863dfdc276422b16ce18ab986fdd0685cc2aeaaccc18dcd4338/merged\",\n                    \"UpperDir\": \"/var/lib/containers/storage/overlay/335bbb6f4ea27863dfdc276422b16ce18ab986fdd0685cc2aeaaccc18dcd4338/diff\",\n                    \"WorkDir\": \"/var/lib/containers/storage/overlay/335bbb6f4ea27863dfdc276422b16ce18ab986fdd0685cc2aeaaccc18dcd4338/work\"\n               }\n          },\n          \"Mounts\": [\n               {\n                    \"Type\": \"bind\",\n                    \"Source\": \"/var/lib/ramalama/store/ollama/deepseek-r1/deepseek-r1/blobs/sha256-4cd576d9aa16961244012223abf01445567b061f1814b57dfef699e4cf8df339\",\n                    \"Destination\": \"/mnt/models/model.file\",\n                    \"Driver\": \"\",\n                    \"Mode\": \"\",\n                    \"Options\": [\n                         \"rbind\"\n                    ],\n                    \"RW\": false,\n                    \"Propagation\": \"rprivate\"\n               },\n               {\n                    \"Type\": \"bind\",\n                    \"Source\": \"/var/lib/ramalama/store/ollama/deepseek-r1/deepseek-r1/snapshots/sha256-4cd576d9aa16961244012223abf01445567b061f1814b57dfef699e4cf8df339/chat_template_converted\",\n                    \"Destination\": \"/mnt/models/chat_template.file\",\n                    \"Driver\": \"\",\n                    \"Mode\": \"\",\n                    \"Options\": [\n                         \"rbind\"\n                    ],\n                    \"RW\": false,\n                    \"Propagation\": \"rprivate\"\n               }\n          ],\n          \"Dependencies\": [],\n          \"NetworkSettings\": {\n               \"EndpointID\": \"\",\n               \"Gateway\": \"10.88.0.1\",\n               \"IPAddress\": \"10.88.0.4\",\n               \"IPPrefixLen\": 16,\n               \"IPv6Gateway\": \"\",\n               \"GlobalIPv6Address\": \"\",\n               \"GlobalIPv6PrefixLen\": 0,\n               \"MacAddress\": \"fa:1b:10:3c:ad:1e\",\n               \"Bridge\": \"\",\n               \"SandboxID\": \"\",\n               \"HairpinMode\": false,\n               \"LinkLocalIPv6Address\": \"\",\n               \"LinkLocalIPv6PrefixLen\": 0,\n               \"Ports\": {\n                    \"8080/tcp\": [\n                         {\n                              \"HostIp\": \"0.0.0.0\",\n                              \"HostPort\": \"8080\"\n                         }\n                    ]\n               },\n               \"SandboxKey\": \"/run/netns/netns-51cdac05-a3ea-4710-09c0-4a3a8a72f919\",\n               \"Networks\": {\n                    \"podman\": {\n                         \"EndpointID\": \"\",\n                         \"Gateway\": \"10.88.0.1\",\n                         \"IPAddress\": \"10.88.0.4\",\n                         \"IPPrefixLen\": 16,\n                         \"IPv6Gateway\": \"\",\n                         \"GlobalIPv6Address\": \"\",\n                         \"GlobalIPv6PrefixLen\": 0,\n                         \"MacAddress\": \"fa:1b:10:3c:ad:1e\",\n                         \"NetworkID\": \"2f259bab93aaaaa2542ba43ef33eb990d0999ee1b9924b557b7be53c0b7a1bb9\",\n                         \"DriverOpts\": null,\n                         \"IPAMConfig\": null,\n                         \"Links\": null,\n                         \"Aliases\": [\n                              \"5604715c8b96\"\n                         ]\n                    }\n               }\n          },\n          \"Namespace\": \"\",\n          \"IsInfra\": false,\n          \"IsService\": false,\n          \"KubeExitCodePropagation\": \"invalid\",\n          \"lockNumber\": 3,\n          \"Config\": {\n               \"Hostname\": \"5604715c8b96\",\n               \"Domainname\": \"\",\n               \"User\": \"\",\n               \"AttachStdin\": false,\n               \"AttachStdout\": false,\n               \"AttachStderr\": false,\n               \"Tty\": true,\n               \"OpenStdin\": true,\n               \"StdinOnce\": false,\n               \"Env\": [\n                    \"LD_LIBRARY_PATH=/usr/local/cuda/lib64\",\n                    \"NV_LIBNCCL_VERSION=2.25.1\",\n                    \"TERM=xterm\",\n                    \"PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                    \"NV_CUDA_LIB_VERSION=12.8.1-1\",\n                    \"NV_LIBNPP_VERSION=12.3.3.100-1\",\n                    \"NVIDIA_PRODUCT_NAME=CUDA\",\n                    \"NVIDIA_REQUIRE_CUDA=cuda\\u003e=12.8 brand=unknown,driver\\u003e=470,driver\\u003c471 brand=grid,driver\\u003e=470,driver\\u003c471 brand=tesla,driver\\u003e=470,driver\\u003c471 brand=nvidia,driver\\u003e=470,driver\\u003c471 brand=quadro,driver\\u003e=470,driver\\u003c471 brand=quadrortx,driver\\u003e=470,driver\\u003c471 brand=nvidiartx,driver\\u003e=470,driver\\u003c471 brand=vapps,driver\\u003e=470,driver\\u003c471 brand=vpc,driver\\u003e=470,driver\\u003c471 brand=vcs,driver\\u003e=470,driver\\u003c471 brand=vws,driver\\u003e=470,driver\\u003c471 brand=cloudgaming,driver\\u003e=470,driver\\u003c471 brand=unknown,driver\\u003e=535,driver\\u003c536 brand=grid,driver\\u003e=535,driver\\u003c536 brand=tesla,driver\\u003e=535,driver\\u003c536 brand=nvidia,driver\\u003e=535,driver\\u003c536 brand=quadro,driver\\u003e=535,driver\\u003c536 brand=quadrortx,driver\\u003e=535,driver\\u003c536 brand=nvidiartx,driver\\u003e=535,driver\\u003c536 brand=vapps,driver\\u003e=535,driver\\u003c536 brand=vpc,driver\\u003e=535,driver\\u003c536 brand=vcs,driver\\u003e=535,driver\\u003c536 brand=vws,driver\\u003e=535,driver\\u003c536 brand=cloudgaming,driver\\u003e=535,driver\\u003c536 brand=unknown,driver\\u003e=550,driver\\u003c551 brand=grid,driver\\u003e=550,driver\\u003c551 brand=tesla,driver\\u003e=550,driver\\u003c551 brand=nvidia,driver\\u003e=550,driver\\u003c551 brand=quadro,driver\\u003e=550,driver\\u003c551 brand=quadrortx,driver\\u003e=550,driver\\u003c551 brand=nvidiartx,driver\\u003e=550,driver\\u003c551 brand=vapps,driver\\u003e=550,driver\\u003c551 brand=vpc,driver\\u003e=550,driver\\u003c551 brand=vcs,driver\\u003e=550,driver\\u003c551 brand=vws,driver\\u003e=550,driver\\u003c551 brand=cloudgaming,driver\\u003e=550,driver\\u003c551 brand=unknown,driver\\u003e=560,driver\\u003c561 brand=grid,driver\\u003e=560,driver\\u003c561 brand=tesla,driver\\u003e=560,driver\\u003c561 brand=nvidia,driver\\u003e=560,driver\\u003c561 brand=quadro,driver\\u003e=560,driver\\u003c561 brand=quadrortx,driver\\u003e=560,driver\\u003c561 brand=nvidiartx,driver\\u003e=560,driver\\u003c561 brand=vapps,driver\\u003e=560,driver\\u003c561 brand=vpc,driver\\u003e=560,driver\\u003c561 brand=vcs,driver\\u003e=560,driver\\u003c561 brand=vws,driver\\u003e=560,driver\\u003c561 brand=cloudgaming,driver\\u003e=560,driver\\u003c561 brand=unknown,driver\\u003e=565,driver\\u003c566 brand=grid,driver\\u003e=565,driver\\u003c566 brand=tesla,driver\\u003e=565,driver\\u003c566 brand=nvidia,driver\\u003e=565,driver\\u003c566 brand=quadro,driver\\u003e=565,driver\\u003c566 brand=quadrortx,driver\\u003e=565,driver\\u003c566 brand=nvidiartx,driver\\u003e=565,driver\\u003c566 brand=vapps,driver\\u003e=565,driver\\u003c566 brand=vpc,driver\\u003e=565,driver\\u003c566 brand=vcs,driver\\u003e=565,driver\\u003c566 brand=vws,driver\\u003e=565,driver\\u003c566 brand=cloudgaming,driver\\u003e=565,driver\\u003c566\",\n                    \"NCCL_VERSION=2.25.1\",\n                    \"CUDA_VISIBLE_DEVICES=0\",\n                    \"HOME=/tmp\",\n                    \"container=oci\",\n                    \"NV_LIBNPP_PACKAGE=libnpp-12-8-12.3.3.100-1\",\n                    \"NV_LIBNCCL_PACKAGE=libnccl-2.25.1-1+cuda12.8\",\n                    \"NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\",\n                    \"NVIDIA_VISIBLE_DEVICES=all\",\n                    \"NV_LIBNCCL_PACKAGE_NAME=libnccl\",\n                    \"CUDA_VERSION=12.8.1\",\n                    \"NV_NVTX_VERSION=12.8.90-1\",\n                    \"NVARCH=x86_64\",\n                    \"NV_LIBCUBLAS_VERSION=12.8.4.1-1\",\n                    \"NVIDIA_DRIVER_CAPABILITIES=compute,utility\",\n                    \"PKG_CMD=yum\",\n                    \"NV_CUDA_CUDART_VERSION=12.8.90-1\",\n                    \"HOSTNAME=5604715c8b96\",\n                    \"NVIDIA_VISIBLE_DEVICES=void\"\n               ],\n               \"Cmd\": [\n                    \"/usr/libexec/ramalama/ramalama-serve-core\",\n                    \"llama-server\",\n                    \"--port\",\n                    \"8080\",\n                    \"--model\",\n                    \"/mnt/models/model.file\",\n                    \"--alias\",\n                    \"deepseek-r1:70b\",\n                    \"--ctx-size\",\n                    \"2048\",\n                    \"--temp\",\n                    \"0.8\",\n                    \"--jinja\",\n                    \"--cache-reuse\",\n                    \"256\",\n                    \"--flash-attn\",\n                    \"-ngl\",\n                    \"999\",\n                    \"--threads\",\n                    \"32\",\n                    \"--host\",\n                    \"0.0.0.0\"\n               ],\n               \"Image\": \"quay.io/ramalama/cuda:0.8\",\n               \"Volumes\": null,\n               \"WorkingDir\": \"/\",\n               \"Entrypoint\": null,\n               \"OnBuild\": null,\n               \"Labels\": {\n                    \"ai.ramalama\": \"\",\n                    \"ai.ramalama.command\": \"serve\",\n                    \"ai.ramalama.engine\": \"podman\",\n                    \"ai.ramalama.model\": \"deepseek-r1:70b\",\n                    \"ai.ramalama.port\": \"8080\",\n                    \"ai.ramalama.runtime\": \"llama.cpp\",\n                    \"architecture\": \"x86_64\",\n                    \"build-date\": \"2025-03-13T07:14:50Z\",\n                    \"com.redhat.component\": \"ubi9-container\",\n                    \"com.redhat.license_terms\": \"https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI\",\n                    \"description\": \"The Universal Base Image is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.\",\n                    \"distribution-scope\": \"public\",\n                    \"io.buildah.version\": \"1.39.4\",\n                    \"io.k8s.description\": \"The Universal Base Image is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.\",\n                    \"io.k8s.display-name\": \"Red Hat Universal Base Image 9\",\n                    \"io.openshift.expose-services\": \"\",\n                    \"io.openshift.tags\": \"base rhel9\",\n                    \"maintainer\": \"NVIDIA CORPORATION \\u003csw-cuda-installer@nvidia.com\\u003e\",\n                    \"name\": \"ubi9\",\n                    \"release\": \"1741850090\",\n                    \"summary\": \"Provides the latest release of Red Hat Universal Base Image 9.\",\n                    \"url\": \"https://www.redhat.com\",\n                    \"vcs-ref\": \"ecbd6d61fc4ddfbae467860a593ee69f34ec1120\",\n                    \"vcs-type\": \"git\",\n                    \"vendor\": \"Red Hat, Inc.\",\n                    \"version\": \"9.5\"\n               },\n               \"Annotations\": {\n                    \"io.container.manager\": \"libpod\",\n                    \"io.podman.annotations.autoremove\": \"TRUE\",\n                    \"io.podman.annotations.init\": \"TRUE\",\n                    \"io.podman.annotations.label\": \"disable\",\n                    \"org.opencontainers.image.stopSignal\": \"15\",\n                    \"org.systemd.property.KillSignal\": \"15\",\n                    \"org.systemd.property.TimeoutStopUSec\": \"uint64 10000000\"\n               },\n               \"StopSignal\": \"SIGTERM\",\n               \"HealthcheckOnFailureAction\": \"none\",\n               \"HealthLogDestination\": \"local\",\n               \"HealthcheckMaxLogCount\": 5,\n               \"HealthcheckMaxLogSize\": 500,\n               \"CreateCommand\": [\n                    \"podman\",\n                    \"run\",\n                    \"--rm\",\n                    \"--label\",\n                    \"ai.ramalama.model=deepseek-r1:70b\",\n                    \"--label\",\n                    \"ai.ramalama.engine=podman\",\n                    \"--label\",\n                    \"ai.ramalama.runtime=llama.cpp\",\n                    \"--label\",\n                    \"ai.ramalama.port=8080\",\n                    \"--label\",\n                    \"ai.ramalama.command=serve\",\n                    \"--device\",\n                    \"/dev/dri\",\n                    \"--device\",\n                    \"nvidia.com/gpu=all\",\n                    \"-e\",\n                    \"CUDA_VISIBLE_DEVICES=0\",\n                    \"--runtime\",\n                    \"/usr/bin/nvidia-container-runtime\",\n                    \"-p\",\n                    \"8080:8080\",\n                    \"--security-opt=label=disable\",\n                    \"--cap-drop=all\",\n                    \"--security-opt=no-new-privileges\",\n                    \"--pull\",\n                    \"newer\",\n                    \"-t\",\n                    \"-i\",\n                    \"--label\",\n                    \"ai.ramalama\",\n                    \"--name\",\n                    \"ramalama_M9OGd1Z2OB\",\n                    \"--env=HOME=/tmp\",\n                    \"--init\",\n                    \"--label\",\n                    \"ai.ramalama.model=deepseek-r1:70b\",\n                    \"--label\",\n                    \"ai.ramalama.engine=podman\",\n                    \"--label\",\n                    \"ai.ramalama.runtime=llama.cpp\",\n                    \"--label\",\n                    \"ai.ramalama.port=8080\",\n                    \"--label\",\n                    \"ai.ramalama.command=serve\",\n                    \"--mount=type=bind,src=/var/lib/ramalama/store/ollama/deepseek-r1/deepseek-r1/blobs/sha256-4cd576d9aa16961244012223abf01445567b061f1814b57dfef699e4cf8df339,destination=/mnt/models/model.file,ro\",\n                    \"--mount=type=bind,src=/var/lib/ramalama/store/ollama/deepseek-r1/deepseek-r1/snapshots/sha256-4cd576d9aa16961244012223abf01445567b061f1814b57dfef699e4cf8df339/chat_template_converted,destination=/mnt/models/chat_template.file,ro\",\n                    \"quay.io/ramalama/cuda:0.8\",\n                    \"/usr/libexec/ramalama/ramalama-serve-core\",\n                    \"llama-server\",\n                    \"--port\",\n                    \"8080\",\n                    \"--model\",\n                    \"/mnt/models/model.file\",\n                    \"--alias\",\n                    \"deepseek-r1:70b\",\n                    \"--ctx-size\",\n                    \"2048\",\n                    \"--temp\",\n                    \"0.8\",\n                    \"--jinja\",\n                    \"--cache-reuse\",\n                    \"256\",\n                    \"--flash-attn\",\n                    \"-ngl\",\n                    \"999\",\n                    \"--threads\",\n                    \"32\",\n                    \"--host\",\n                    \"0.0.0.0\"\n               ],\n               \"Umask\": \"0022\",\n               \"Timeout\": 0,\n               \"StopTimeout\": 10,\n               \"Passwd\": true,\n               \"sdNotifyMode\": \"container\",\n               \"ExposedPorts\": {\n                    \"8080/tcp\": {}\n               }\n          },\n          \"HostConfig\": {\n               \"Binds\": [\n                    \"/var/lib/ramalama/store/ollama/deepseek-r1/deepseek-r1/blobs/sha256-4cd576d9aa16961244012223abf01445567b061f1814b57dfef699e4cf8df339:/mnt/models/model.file:ro,rprivate,rbind\",\n                    \"/var/lib/ramalama/store/ollama/deepseek-r1/deepseek-r1/snapshots/sha256-4cd576d9aa16961244012223abf01445567b061f1814b57dfef699e4cf8df339/chat_template_converted:/mnt/models/chat_template.file:ro,rprivate,rbind\"\n               ],\n               \"CgroupManager\": \"systemd\",\n               \"CgroupMode\": \"private\",\n               \"ContainerIDFile\": \"\",\n               \"LogConfig\": {\n                    \"Type\": \"journald\",\n                    \"Config\": null,\n                    \"Path\": \"\",\n                    \"Tag\": \"\",\n                    \"Size\": \"0B\"\n               },\n               \"NetworkMode\": \"bridge\",\n               \"PortBindings\": {\n                    \"8080/tcp\": [\n                         {\n                              \"HostIp\": \"0.0.0.0\",\n                              \"HostPort\": \"8080\"\n                         }\n                    ]\n               },\n               \"RestartPolicy\": {\n                    \"Name\": \"no\",\n                    \"MaximumRetryCount\": 0\n               },\n               \"AutoRemove\": true,\n               \"AutoRemoveImage\": false,\n               \"Annotations\": {\n                    \"io.container.manager\": \"libpod\",\n                    \"io.podman.annotations.autoremove\": \"TRUE\",\n                    \"io.podman.annotations.init\": \"TRUE\",\n                    \"io.podman.annotations.label\": \"disable\",\n                    \"org.opencontainers.image.stopSignal\": \"15\",\n                    \"org.systemd.property.KillSignal\": \"15\",\n                    \"org.systemd.property.TimeoutStopUSec\": \"uint64 10000000\"\n               },\n               \"VolumeDriver\": \"\",\n               \"VolumesFrom\": null,\n               \"CapAdd\": [],\n               \"CapDrop\": [\n                    \"CAP_CHOWN\",\n                    \"CAP_DAC_OVERRIDE\",\n                    \"CAP_FOWNER\",\n                    \"CAP_FSETID\",\n                    \"CAP_KILL\",\n                    \"CAP_NET_BIND_SERVICE\",\n                    \"CAP_SETFCAP\",\n                    \"CAP_SETGID\",\n                    \"CAP_SETPCAP\",\n                    \"CAP_SETUID\",\n                    \"CAP_SYS_CHROOT\"\n               ],\n               \"Dns\": [],\n               \"DnsOptions\": [],\n               \"DnsSearch\": [],\n               \"ExtraHosts\": [],\n               \"HostsFile\": \"\",\n               \"GroupAdd\": [],\n               \"IpcMode\": \"shareable\",\n               \"Cgroup\": \"\",\n               \"Cgroups\": \"default\",\n               \"Links\": null,\n               \"OomScoreAdj\": 0,\n               \"PidMode\": \"private\",\n               \"Privileged\": false,\n               \"PublishAllPorts\": false,\n               \"ReadonlyRootfs\": false,\n               \"SecurityOpt\": [\n                    \"no-new-privileges\",\n                    \"label=disable\"\n               ],\n               \"Tmpfs\": {},\n               \"UTSMode\": \"private\",\n               \"UsernsMode\": \"\",\n               \"ShmSize\": 65536000,\n               \"Runtime\": \"oci\",\n               \"ConsoleSize\": [\n                    0,\n                    0\n               ],\n               \"Isolation\": \"\",\n               \"CpuShares\": 0,\n               \"Memory\": 0,\n               \"NanoCpus\": 0,\n               \"CgroupParent\": \"\",\n               \"BlkioWeight\": 0,\n               \"BlkioWeightDevice\": null,\n               \"BlkioDeviceReadBps\": null,\n               \"BlkioDeviceWriteBps\": null,\n               \"BlkioDeviceReadIOps\": null,\n               \"BlkioDeviceWriteIOps\": null,\n               \"CpuPeriod\": 0,\n               \"CpuQuota\": 0,\n               \"CpuRealtimePeriod\": 0,\n               \"CpuRealtimeRuntime\": 0,\n               \"CpusetCpus\": \"\",\n               \"CpusetMems\": \"\",\n               \"Devices\": [\n                    {\n                         \"PathOnHost\": \"/dev/dri/card0\",\n                         \"PathInContainer\": \"/dev/dri/card0\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/nvidia-uvm\",\n                         \"PathInContainer\": \"/dev/nvidia-uvm\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/nvidia-uvm-tools\",\n                         \"PathInContainer\": \"/dev/nvidia-uvm-tools\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/nvidiactl\",\n                         \"PathInContainer\": \"/dev/nvidiactl\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/nvidia0\",\n                         \"PathInContainer\": \"/dev/nvidia0\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/nvidia1\",\n                         \"PathInContainer\": \"/dev/nvidia1\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/dri/card1\",\n                         \"PathInContainer\": \"/dev/dri/card1\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/dri/card2\",\n                         \"PathInContainer\": \"/dev/dri/card2\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/dri/renderD128\",\n                         \"PathInContainer\": \"/dev/dri/renderD128\",\n                         \"CgroupPermissions\": \"\"\n                    },\n                    {\n                         \"PathOnHost\": \"/dev/dri/renderD129\",\n                         \"PathInContainer\": \"/dev/dri/renderD129\",\n                         \"CgroupPermissions\": \"\"\n                    }\n               ],\n               \"DiskQuota\": 0,\n               \"KernelMemory\": 0,\n               \"MemoryReservation\": 0,\n               \"MemorySwap\": 0,\n               \"MemorySwappiness\": 0,\n               \"OomKillDisable\": false,\n               \"Init\": true,\n               \"PidsLimit\": 2048,\n               \"Ulimits\": [\n                    {\n                         \"Name\": \"RLIMIT_NOFILE\",\n                         \"Soft\": 1048576,\n                         \"Hard\": 1048576\n                    },\n                    {\n                         \"Name\": \"RLIMIT_NPROC\",\n                         \"Soft\": 1048576,\n                         \"Hard\": 1048576\n                    }\n               ],\n               \"CpuCount\": 0,\n               \"CpuPercent\": 0,\n               \"IOMaximumIOps\": 0,\n               \"IOMaximumBandwidth\": 0,\n               \"CgroupConf\": null\n          },\n          \"UseImageHosts\": false,\n          \"UseImageHostname\": false\n     }\n]\n```",
      "state": "closed",
      "author": "mcornea",
      "author_type": "User",
      "created_at": "2025-05-06T19:19:08Z",
      "updated_at": "2025-05-09T06:54:17Z",
      "closed_at": "2025-05-07T18:19:01Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1354/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1354",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1354",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:42.236337",
      "comments": [
        {
          "author": "rhatdan",
          "body": "llama.cpp does not support multiple GPUs (I believe).  You might need to use vllm for something like this, which is not well supported in RamaLama.\n@ggerganov Am I correct?",
          "created_at": "2025-05-06T19:54:13Z"
        },
        {
          "author": "mcornea",
          "body": "It does seem to run fine when updating `CUDA_VISIBLE_DEVICES=0,1` in https://github.com/containers/ramalama/blob/main/ramalama/common.py#L394\n\n```\nramalama --debug  serve deepseek-r1:70b \nChecking if 8080 is available\nserving on port 8080\nexec_cmd:  podman run --rm --label ai.ramalama.model=deepseek",
          "created_at": "2025-05-06T20:33:23Z"
        },
        {
          "author": "rhatdan",
          "body": "Do you think we should change to that as default, when using llama.cpp?  Would that work on systems with only one GPU.",
          "created_at": "2025-05-06T20:38:36Z"
        },
        {
          "author": "mcornea",
          "body": "I started https://github.com/containers/ramalama/pull/1355 which should address the issue by passing all available GPUs in the `CUDA_VISIBLE_DEVICES` env var which should work for both single and multi GPUs.",
          "created_at": "2025-05-06T21:26:25Z"
        },
        {
          "author": "ericcurtin",
          "body": "Can llama.cpp utilize multiple GPUs? If no, we should use the most appropriate one",
          "created_at": "2025-05-06T22:04:58Z"
        }
      ]
    },
    {
      "issue_number": 1325,
      "title": "model store (partial breaking functionality)",
      "body": "### Issue Description\n\n âœ— [] ramalama list - rm -a removes all models [249]\n   (from function `bail-now' in file test/system/helpers.podman.bash, line 122,\n    from function `die' in file test/system/helpers.podman.bash, line 848,\n    from function `run_ramalama' in file test/system/helpers.bash, line 186,\n    in test file test/system/010-list.bats, line 47)\n     `run_ramalama rm -a' failed\n\n   [13:19:31.013447000] $ /Users/ecurtin/git/ramalama/bin/ramalama rm -a\n   [13:19:31.138270000] Error: removing gemma3/gemma3:latest (partial): [Errno 2] No such file or directory: '/Users/ecurtin/.local/share/ramalama/store/ollama/gemma3/gemma3/refs/latest (partial)\n   [13:19:31.143426000] [ rc=1 (** EXPECTED 0 **) ]\n   #/vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n   #| FAIL: exit code is 1; expected 0\n   #\\^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   # [teardown]\n\n### Steps to reproduce the issue\n\n âœ— [] ramalama list - rm -a removes all models [249]\n   (from function `bail-now' in file test/system/helpers.podman.bash, line 122,\n    from function `die' in file test/system/helpers.podman.bash, line 848,\n    from function `run_ramalama' in file test/system/helpers.bash, line 186,\n    in test file test/system/010-list.bats, line 47)\n     `run_ramalama rm -a' failed\n\n   [13:19:31.013447000] $ /Users/ecurtin/git/ramalama/bin/ramalama rm -a\n   [13:19:31.138270000] Error: removing gemma3/gemma3:latest (partial): [Errno 2] No such file or directory: '/Users/ecurtin/.local/share/ramalama/store/ollama/gemma3/gemma3/refs/latest (partial)\n   [13:19:31.143426000] [ rc=1 (** EXPECTED 0 **) ]\n   #/vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n   #| FAIL: exit code is 1; expected 0\n   #\\^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   # [teardown]\n\n### Describe the results you received\n\n âœ— [] ramalama list - rm -a removes all models [249]\n   (from function `bail-now' in file test/system/helpers.podman.bash, line 122,\n    from function `die' in file test/system/helpers.podman.bash, line 848,\n    from function `run_ramalama' in file test/system/helpers.bash, line 186,\n    in test file test/system/010-list.bats, line 47)\n     `run_ramalama rm -a' failed\n\n   [13:19:31.013447000] $ /Users/ecurtin/git/ramalama/bin/ramalama rm -a\n   [13:19:31.138270000] Error: removing gemma3/gemma3:latest (partial): [Errno 2] No such file or directory: '/Users/ecurtin/.local/share/ramalama/store/ollama/gemma3/gemma3/refs/latest (partial)\n   [13:19:31.143426000] [ rc=1 (** EXPECTED 0 **) ]\n   #/vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n   #| FAIL: exit code is 1; expected 0\n   #\\^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   # [teardown]\n\n### Describe the results you expected\n\n âœ— [] ramalama list - rm -a removes all models [249]\n   (from function `bail-now' in file test/system/helpers.podman.bash, line 122,\n    from function `die' in file test/system/helpers.podman.bash, line 848,\n    from function `run_ramalama' in file test/system/helpers.bash, line 186,\n    in test file test/system/010-list.bats, line 47)\n     `run_ramalama rm -a' failed\n\n   [13:19:31.013447000] $ /Users/ecurtin/git/ramalama/bin/ramalama rm -a\n   [13:19:31.138270000] Error: removing gemma3/gemma3:latest (partial): [Errno 2] No such file or directory: '/Users/ecurtin/.local/share/ramalama/store/ollama/gemma3/gemma3/refs/latest (partial)\n   [13:19:31.143426000] [ rc=1 (** EXPECTED 0 **) ]\n   #/vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n   #| FAIL: exit code is 1; expected 0\n   #\\^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   # [teardown]\n\n### ramalama info output\n\n```yaml\nâœ— [] ramalama list - rm -a removes all models [249]\n   (from function `bail-now' in file test/system/helpers.podman.bash, line 122,\n    from function `die' in file test/system/helpers.podman.bash, line 848,\n    from function `run_ramalama' in file test/system/helpers.bash, line 186,\n    in test file test/system/010-list.bats, line 47)\n     `run_ramalama rm -a' failed\n\n   [13:19:31.013447000] $ /Users/ecurtin/git/ramalama/bin/ramalama rm -a\n   [13:19:31.138270000] Error: removing gemma3/gemma3:latest (partial): [Errno 2] No such file or directory: '/Users/ecurtin/.local/share/ramalama/store/ollama/gemma3/gemma3/refs/latest (partial)\n   [13:19:31.143426000] [ rc=1 (** EXPECTED 0 **) ]\n   #/vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n   #| FAIL: exit code is 1; expected 0\n   #\\^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   # [teardown]\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-05-01T12:20:30Z",
      "updated_at": "2025-05-07T06:07:06Z",
      "closed_at": "2025-05-07T06:07:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1325/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1325",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1325",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:42.455812",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "@engelmi another one",
          "created_at": "2025-05-01T12:20:40Z"
        },
        {
          "author": "ericcurtin",
          "body": "If time is a constraint @engelmi please open a revert PR for this:\n\nhttps://github.com/containers/ramalama/pull/1117/files\n\nand we can re-introduce again at a later date with a fully working solution.",
          "created_at": "2025-05-01T12:22:34Z"
        },
        {
          "author": "ericcurtin",
          "body": "Trying to fix this manually and getting this:\n\n```\n âœ— [] ramalama list - rm -a removes all models [225]\n   (from function `bail-now' in file test/system/helpers.podman.bash, line 122,\n    from function `die' in file test/system/helpers.podman.bash, line 848,\n    from function `run_ramalama' in file ",
          "created_at": "2025-05-01T12:49:15Z"
        },
        {
          "author": "ericcurtin",
          "body": "```\n~/git/ramalama$ bin/ramalama ls\nNAME                                          MODIFIED       SIZE\nollama://gemma3/gemma3:latest                 55 years ago   0 B\nollama://tinyllama/tinyllama:latest (partial) 26 seconds ago 571.43 MB\nollama://smollm/smollm:135m                   6 days ago     8",
          "created_at": "2025-05-01T12:50:27Z"
        },
        {
          "author": "ericcurtin",
          "body": "I think I worked around this by doing a:\n\n```\nrm -rf /Users/ecurtin/.local/share/ramalama/store\n```\n\nthere are a couple of not so great states existing",
          "created_at": "2025-05-01T12:52:54Z"
        }
      ]
    },
    {
      "issue_number": 750,
      "title": "Ai will crash if reached context size",
      "body": "After playing with the AI for a while it will crash after exceeding the context size. Should we increase the context size so this doesn't happen to the end user?  \n\n```\ncontext size exceeded\nllama_decode: failed to decode, ret = 1\nfailed to decode\nfailed to generate response\nbrian@DESKTOP-SB69448:~/ramalama$\n```",
      "state": "closed",
      "author": "bmahabirbu",
      "author_type": "User",
      "created_at": "2025-02-06T18:36:05Z",
      "updated_at": "2025-05-07T01:08:38Z",
      "closed_at": "2025-05-07T00:28:30Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/750/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/750",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/750",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:42.677327",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "This is something that needs to be resolved in llama.cpp rather than RamaLama",
          "created_at": "2025-02-06T18:43:04Z"
        },
        {
          "author": "dylanmtaylor",
          "body": "I'm also hitting this issue a bunch",
          "created_at": "2025-02-09T18:25:51Z"
        },
        {
          "author": "ericcurtin",
          "body": "You can manually change it via \"-c\", but any kind of automatic change of context size changing would have to go to llama.cpp",
          "created_at": "2025-02-09T19:15:02Z"
        },
        {
          "author": "rhatdan",
          "body": "Is this getting any better with the latest release?\n\nOne key issue is when AI go into infinite loops. I would prefer them to crash rather then burn energy.",
          "created_at": "2025-04-02T10:54:32Z"
        },
        {
          "author": "ericcurtin",
          "body": "This might go away when we move to the client server approach:\n\nhttps://github.com/containers/ramalama/pull/1068",
          "created_at": "2025-04-02T14:12:10Z"
        }
      ]
    },
    {
      "issue_number": 1348,
      "title": "`ramalama` model store has issues reconciling between system and virtualenv installations",
      "body": "### Issue Description\n\nI have a Fedora system with the `python3-ramalama` package installed via `dnf`\n\nWhen I also try to install `ramalama` via `pip` in a virtualenv, I get some strange behavior with the model store\n\n### Steps to reproduce the issue\n\n1. Install `python3-ramalama` via `dnf` on a Fedora system\n2. Download a model\n3. Create a virtualenv and install `ramalama` via `pip`\n\n### Describe the results you received\n\n```bash\ninstructlab@beanlab1:~$ ramalama ls\nNAME                     MODIFIED       SIZE   \nollama://llama3.2:latest 20 seconds ago 1.88 GB\ninstructlab@beanlab1:~$ python -m venv venv\ninstructlab@beanlab1:~$ source venv/bin/activate\n(venv) instructlab@beanlab1:~$ pip install --quiet ramalama\n(venv) instructlab@beanlab1:~$ ramalama ls\nStarting importing AI models to new store...\nFailed to import /home/instructlab/.local/share/ramalama/models/ollama: No such file: '/home/instructlab/.local/share/ramalama/models/ollama/llama3.2'\nNAME                              MODIFIED     SIZE\nollama://llama3.2/llama3.2:latest 55 years ago 0 B \n(venv) instructlab@beanlab1:~$ deactivate \ninstructlab@beanlab1:~$ ramalama ls\nNAME MODIFIED SIZE\ninstructlab@beanlab1:~$ \n```\n\n### Describe the results you expected\n\nModel store should be the same, regardless of where RamaLama is being run\n\n### ramalama info output\n\n```yaml\n**system install**\n\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.97,\n                    \"systemPercent\": 0.02,\n                    \"userPercent\": 0.01\n                },\n                \"cpus\": 24,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2044,\n                \"hostname\": \"beanlab1\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.4-200.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 112128143360,\n                \"memTotal\": 134255890432,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250415.g2340bbf-1.fc41.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-1.fc41.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"143h 59m 56.00s (Approximately 5.96 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/instructlab/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 1,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 1\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/instructlab/.local/share/containers/storage\",\n                \"graphRootAllocated\": 4595208159232,\n                \"graphRootUsed\": 132971622400,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 56\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/instructlab/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Tue Apr  1 20:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.23.7\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/instructlab/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n\n**virtualenv install**\n\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.97,\n                    \"systemPercent\": 0.02,\n                    \"userPercent\": 0.01\n                },\n                \"cpus\": 24,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2044,\n                \"hostname\": \"beanlab1\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.4-200.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 112116604928,\n                \"memTotal\": 134255890432,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250415.g2340bbf-1.fc41.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-1.fc41.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"144h 0m 53.00s (Approximately 6.00 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/instructlab/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 1,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 1\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/instructlab/.local/share/containers/storage\",\n                \"graphRootAllocated\": 4595208159232,\n                \"graphRootUsed\": 132971667456,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 56\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/instructlab/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Tue Apr  1 20:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.23.7\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/home/instructlab/venv/share/ramalama/shortnames.conf\",\n            \"/usr/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/instructlab/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.1\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nFedora 41, kernel 6.14.4-200.fc41.x86_64\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "nathan-weinberg",
      "author_type": "User",
      "created_at": "2025-05-05T15:54:56Z",
      "updated_at": "2025-05-05T16:08:15Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1348/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1348",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1348",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:43.108921",
      "comments": [
        {
          "author": "rhatdan",
          "body": "We got a PR on this subject today;\n\nhttps://github.com/containers/ramalama/pull/1340\n",
          "created_at": "2025-05-05T16:02:50Z"
        },
        {
          "author": "ericcurtin",
          "body": "The solution here is update the Fedora package, this is expected behaviour because the Fedora version is old (doesn't have the concept of model store enabled, it does have a model store, but it's the old one)",
          "created_at": "2025-05-05T16:08:14Z"
        }
      ]
    },
    {
      "issue_number": 1248,
      "title": "Fix CI tests so they run with the currently checked in version of the container",
      "body": "### Feature request description\n\nCurrently when our test suite runs, it uses the last released version of the container, this can slow down development and cause maintenance issues. We build the CPU inferencing version of the container image in each CI build, make sure we use that one.\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-04-23T10:27:12Z",
      "updated_at": "2025-05-05T14:53:52Z",
      "closed_at": "2025-05-05T14:53:52Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1248/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1248",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1248",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:43.284039",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "I think this is working now, thanks @rhatdan ",
          "created_at": "2025-05-03T09:21:11Z"
        },
        {
          "author": "nathan-weinberg",
          "body": "Should we close this?",
          "created_at": "2025-05-05T14:29:32Z"
        }
      ]
    },
    {
      "issue_number": 1303,
      "title": "Error: statfs /home/douglas/.local/share/ramalama/store/ollama/tinyllama/tinyllama/snapshots/sha256-82cad9d8c383d729a0e9eeedcabc51316fdb40c9961d75673007439a4d3fc81e/chat_template_converted: no such file or directory",
      "body": "### Issue Description\n\nTrying to run a local rag with ramalama v0.8.0 using tinyllama model, it says:\n\nError: statfs /home/douglas/.local/share/ramalama/store/ollama/tinyllama/tinyllama/snapshots/sha256-82cad9d8c383d729a0e9eeedcabc51316fdb40c9961d75673007439a4d3fc81e/chat_template_converted: no such file or directory\n\nif counts, before installing v0.8.0 via pip, I had ramalama v0.7.4 installed by rpm\n\ncat /etc/redhat-release\nFedora release 41 (Forty One)\n\n### Steps to reproduce the issue\n\nramalama rag ./README.md localhost/dougsland_readme\nAttempting to pull quay.io/ramalama/ramalama-rag:0.8...\n\nBuilding localhost/dougsland_readme...\nadding vectordb...\nc9f5b456815e86f1e8c2f13497234a8d020499a3df9491de0b877d8b56f9c2a3\n\nramalama run --rag localhost/dougsland_readme tinyllama\n 99% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  |  605.54 MB/ 608.16 MB  21.48 MB/s        0s\n\n\nError: statfs /home/douglas/.local/share/ramalama/store/ollama/tinyllama/tinyllama/snapshots/sha256-82cad9d8c383d729a0e9eeedcabc51316fdb40c9961d75673007439a4d3fc81e/chat_template_converted: no such file or directory\n\n\n$ ramalama version\nramalama version 0.8.0\n\n$ lspci | grep -i vga\n00:02.0 VGA compatible controller: Intel Corporation TigerLake-H GT1 [UHD Graphics] (rev 01)\n01:00.0 VGA compatible controller: NVIDIA Corporation GA107M [GeForce RTX 3050 Ti Mobile] (rev a1)\n\n### Describe the results you received\n\nError: statfs /home/douglas/.local/share/ramalama/store/ollama/tinyllama/tinyllama/snapshots/sha256-82cad9d8c383d729a0e9eeedcabc51316fdb40c9961d75673007439a4d3fc81e/chat_template_converted: no such file or directory\n\n### Describe the results you expected\n\nNo error at all.\n\n### ramalama info output\n\n```yaml\nramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.37.5\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.12-3.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.12, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.94,\n                    \"systemPercent\": 0.03,\n                    \"userPercent\": 0.04\n                },\n                \"cpus\": 16,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2047,\n                \"hostname\": \"fedora\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.11.4-301.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 35252920320,\n                \"memTotal\": 67110440960,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.12.2-2.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.12.2\"\n                    },\n                    \"package\": \"netavark-1.12.2-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.12.2\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.17-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.17\\ncommit: 000fa0d4eeed8938301f3bcf8206405315bc1017\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20240906.g6b38f07-1.fc41.x86_64\",\n                    \"version\": \"pasta 0^20240906.g6b38f07-1.fc41.x86_64\\nCopyright Red Hat\\nGNU General Public License, version 2 or later\\n  <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": false,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"415h 49m 58.00s (Approximately 17.29 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/douglas/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 1,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/douglas/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1998694907904,\n                \"graphRootUsed\": 20443574272,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 5\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/douglas/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.2.5\",\n                \"Built\": 1729209600,\n                \"BuiltTime\": \"Thu Oct 17 20:00:00 2024\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.23.2\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.2.5\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/local/share/ramalama/shortnames.conf\",\n            \"/usr/local/share/ramalama/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/douglas/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.0\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "dougsland",
      "author_type": "User",
      "created_at": "2025-04-28T21:29:31Z",
      "updated_at": "2025-05-05T14:43:41Z",
      "closed_at": "2025-05-05T14:43:40Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1303/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1303",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1303",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:43.491451",
      "comments": [
        {
          "author": "rhatdan",
          "body": "@engelmi @ericcurtin is this caused by the new model store?\n\n@dougsland could you try to just run a model?\n\nramalama run granite\n",
          "created_at": "2025-04-29T10:35:42Z"
        },
        {
          "author": "engelmi",
          "body": "@rhatdan @dougsland Yes, thats from the new model store. Essentially, `tinyllama` will be pulled from ollama which uses go templates and which the model store automatically converts to jinja templates. It stores and mounts that generated template file into the container. \n@dougsland Could you provid",
          "created_at": "2025-04-29T11:36:00Z"
        },
        {
          "author": "dougsland",
          "body": "> [@rhatdan](https://github.com/rhatdan) [@dougsland](https://github.com/dougsland) Yes, thats from the new model store. Essentially, `tinyllama` will be pulled from ollama which uses go templates and which the model store automatically converts to jinja templates. It stores and mounts that generate",
          "created_at": "2025-04-29T13:37:48Z"
        },
        {
          "author": "dougsland",
          "body": "> [@dougsland](https://github.com/dougsland) could you try to just run a model?\n> \n> ramalama run granite\n\nYes, it worked @rhatdan ",
          "created_at": "2025-04-29T13:38:28Z"
        },
        {
          "author": "engelmi",
          "body": "Thanks @dougsland \nHmm... `granite` is also pulled from ollama and has a go template which should be converted. Did you have `tinyllama` pulled previously? @dougsland If yes, then it might be some error in the migration script.\n\nEdit: \nThinking about it again, when models from ollama (usually) store",
          "created_at": "2025-04-29T14:14:07Z"
        }
      ]
    },
    {
      "issue_number": 1087,
      "title": "Support goose integration",
      "body": "goose has support for ollama. \nhttps://block.github.io/goose/docs/getting-started/providers#local-llms-ollama\n\nIt will be nice to have it work with ramalama as well.\n\nI tried configuring ramalama through the ollama integration and see this:\n\nStarted model serving\n```\nramalama serve --name mygranite granite3-moe:latest\nserving on port 8080\n\n```\n\nTried to configure goose\n```\n goose configure\n\nThis will update your existing config file\n  if you prefer, you can edit it directly at /home/mrunalp/.config/goose/config.yaml\n\nâ”Œ   goose-configure \nâ”‚\nâ—‡  What would you like to configure?\nâ”‚  Configure Providers \nâ”‚\nâ—‡  Which model provider should we use?\nâ”‚  Ollama \nâ”‚\nâ—  OLLAMA_HOST is already configured\nâ”‚  \nâ—‡  Would you like to update this value?\nâ”‚  Yes \nâ”‚\nâ—‡  Enter new value for OLLAMA_HOST\nâ”‚  http://0.0.0.0:8080\nâ”‚\nâ—‡  Enter a model from that provider:\nâ”‚  granite3-moe:latest\nâ”‚\nâ—‡  Server error: Object {\"error\": Object {\"code\": Number(500), \"message\": String(\"tools param requires --jinja flag\"), \"type\": String(\"server_error\")}}\nâ”‚\nâ””  Failed to configure provider: init chat completion request with tool did not succeed.\n\n```",
      "state": "closed",
      "author": "mrunalp",
      "author_type": "User",
      "created_at": "2025-04-01T02:03:15Z",
      "updated_at": "2025-05-03T09:56:20Z",
      "closed_at": "2025-05-03T09:56:20Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1087/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1087",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1087",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:43.768571",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "OpenAI should work. Please open a PR in goose GitHub to document this when complete",
          "created_at": "2025-04-01T02:54:23Z"
        },
        {
          "author": "mrunalp",
          "body": "Yes, I got it working with:\n```\nramalama serve --name mygranite --runtime-args=\"--jinja\" granite3-moe:latest \n```\n\n```\ngoose-configure \nâ”‚\nâ—‡  What would you like to configure?\nâ”‚  Configure Providers \nâ”‚\nâ—‡  Which model provider should we use?\nâ”‚  Ollama \nâ”‚\nâ—  OLLAMA_HOST is already configured\nâ”‚  \nâ—‡  Wou",
          "created_at": "2025-04-01T04:25:13Z"
        }
      ]
    },
    {
      "issue_number": 1267,
      "title": "interactive mode CLI",
      "body": "### Feature request description\n\neach time I run commands, I enter `ramalama <something>` like `ramalama run <something>`\n\nI would like to know if an interactive mode would be possible where I could enter in an interactive `chat` and then only enter the RamaLama arguments where I would have completion, etc.\n\nlike\n\n```\nramalama --interactive\n\ninfo\n ....\n\nrun foo\n  ....\n^C\nversion\n\nconvert tinyllama foo\n```\n\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "open",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-24T17:47:24Z",
      "updated_at": "2025-05-03T09:20:25Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1267/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1267",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1267",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:43.992486",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Why do you want this?",
          "created_at": "2025-04-25T21:14:08Z"
        },
        {
          "author": "benoitf",
          "body": "I launch once the cli container and then all commands are ramalama commands. I don't need to prefix all my commands or to run again a new container \n\nLike I run the python or node.js container",
          "created_at": "2025-04-25T21:23:20Z"
        },
        {
          "author": "rhatdan",
          "body": "That feels like something you could build simply for yourself.  Not sure there would be huge demand for this.",
          "created_at": "2025-04-26T12:45:48Z"
        },
        {
          "author": "ericcurtin",
          "body": "alias rl=\"ramalama\"\n\nsprings to mind.\n\nThis isn't incredibly hard to implement as a separate project. This is basically implement my own custom shell.\n\nThe thing is when you execute run, then we have to spin up another shell of sorts so we would have to maintain multiple shells.\n",
          "created_at": "2025-05-03T09:20:01Z"
        }
      ]
    },
    {
      "issue_number": 1268,
      "title": "partial alias/mapping between a container and the model being used",
      "body": "### Issue Description\n\nif I use RamaLama on the host and it starts a container with `ramalama run ollama://tinyllama:latest` I have\n\n`ai.ramalama.model: ollama://tinyllama:latest`\n\nso I know which model is used if I compare with the value of `ramalama model list` command\n\nbut if I use `ramalama serve tinyllama` the value is `ai.ramalama.model: tinyllama`\n\nso it contains the value I provided so it's difficult/impossible to map the container to the original model\n\nI would expect to see `ollama://tinyllama:latest` as well for `ai.ramalama.model`\n\n\nin container mode, there is an alias being added but the alias is not the fully qualified model name as well\nhttps://github.com/containers/ramalama/pull/1009\n\n### Steps to reproduce the issue\n\nstart models using shortnames or a subpart\nor use the CLI container\n\n### Describe the results you received\n\ndifferent format of `ai.ramalama.model`\n\n### Describe the results you expected\n\nI expect to always see one of the name returned by `ramalama list`\n\n### ramalama info output\n\n```yaml\n0.7.5 / macOS\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-24T19:57:36Z",
      "updated_at": "2025-05-02T13:43:53Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1268/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1268",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1268",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:44.227227",
      "comments": [
        {
          "author": "rhatdan",
          "body": "We do translate if we have the name expanded by shortnames, but if ollama expands it we don't.\n\n@ericcurtin Do you know if there is an easy way to figure out tinylama=ollama://tinyllama:latest?\n\nI guess we could just check if the name has",
          "created_at": "2025-05-01T10:25:20Z"
        },
        {
          "author": "rhatdan",
          "body": "We could decorate the shortname our self with TRANSPORT://model:TAG, if we don't see TRANSPORT or TAG specified, but in some cases doesn't ollama translate tiny->tinyllama.",
          "created_at": "2025-05-01T10:26:41Z"
        },
        {
          "author": "ericcurtin",
          "body": "I guess:\n\n`ramalama model list` == `ramalama list`\n\nplaying with docker I guess ðŸ˜„ \n\nI think `ramalama list/ls` is fine, we don't really do reverse-shortname resolution though (to display the shortname version). Should we? What does podman do here?\n",
          "created_at": "2025-05-01T10:40:48Z"
        },
        {
          "author": "ericcurtin",
          "body": "> [@ericcurtin](https://github.com/ericcurtin) Do you know if there is an easy way to figure out tinylama=ollama://tinyllama:latest?\n\nIt wouldn't be that hard to do a reverse lookup if we wanted to.",
          "created_at": "2025-05-01T10:42:09Z"
        },
        {
          "author": "benoitf",
          "body": "The idea behind this is: How to get the model id from 'model list' command when I have a container running \n\nSo no change expected on the list command, but on the annotation/label applied on the container to have as model name the FQN model, the one we can get from 'list' command\n\nIt's to be able to",
          "created_at": "2025-05-01T11:07:55Z"
        }
      ]
    },
    {
      "issue_number": 1262,
      "title": "Ctrl-C should interrupt completion, not exit",
      "body": "### Feature request description\n\nWhen Ctrl-C is typed, I think it'd be more intuitive to stop completion if there's one currently ongoing rather than exit.\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "jlebon",
      "author_type": "User",
      "created_at": "2025-04-23T23:27:03Z",
      "updated_at": "2025-05-02T09:09:29Z",
      "closed_at": "2025-05-02T09:09:29Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1262/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1262",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1262",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:44.415469",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Makes sense, it's around these lines of code:\n\nhttps://github.com/containers/ramalama/blob/e1f8fb8b6bc808158c952146fe739ef802953c02/libexec/ramalama/ramalama-client-core#L82\n\nLike this issue:\n\nhttps://github.com/containers/ramalama/discussions/1261",
          "created_at": "2025-04-23T23:30:27Z"
        },
        {
          "author": "rhatdan",
          "body": "I agree, although I would like to see two levels.\n\nOne if I asked that AI to do something and it is outputing, stop AI Output.\nIf I am at prompt and I say ^c then exit.\n\n",
          "created_at": "2025-04-24T10:50:36Z"
        },
        {
          "author": "kpouget",
          "body": "interactive REPL tools like Bash/Zsh/GDB/python don't exit on `^c`, they just give a new clean prompt, while `^d` exits\nat first glance, this behavior seems more intuitive to me",
          "created_at": "2025-04-24T11:36:52Z"
        },
        {
          "author": "rhatdan",
          "body": "That works for me also, although I would like `exit` to work, or `bye`.\n",
          "created_at": "2025-04-24T11:42:11Z"
        },
        {
          "author": "ericcurtin",
          "body": "> I agree, although I would like to see two levels.\n> \n> One if I asked that AI to do something and it is outputing, stop AI Output. If I am at prompt and I say ^c then exit.\n\nI agree with @rhatdan here FWIW ... I don't want RamaLama to be like vim... Ahhhhh how do I exit... (although I love vim)\n\n\"",
          "created_at": "2025-04-24T16:18:20Z"
        }
      ]
    },
    {
      "issue_number": 1315,
      "title": "do not seem to result in usable man pages on a Mac.",
      "body": "### Issue Description\n\nDirections on  [README.md](https://github.com/containers/ramalama/tree/main/docs#readme) do not seem to result in usable man pages on a Mac.\n\n<img width=\"1404\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/aff69fef-09cf-49e5-abc3-dda6a8281378\" />\n\nThe files are created as expected, but they cannot be used with the man command.\n\nAccording to @jhjaggars  this is a Mac-only issue. Do we need to add additional information to the readme? If so, what information is needed?\n\n### Steps to reproduce the issue\n\nTo build standard man pages, run make install-tools \nfollowed by make docs. \nResults will be in docs.\n\nrun man ramalama (or any other file) \n\n\n### Describe the results you received\n\nno manual entry for ramalama\n\n### Describe the results you expected\n\nthe man page to open\n\n### ramalama info output\n\n```yaml\nNo manual entry for ramalama\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "arburka",
      "author_type": "User",
      "created_at": "2025-04-30T16:59:13Z",
      "updated_at": "2025-05-01T17:36:57Z",
      "closed_at": "2025-05-01T17:26:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1315/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1315",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1315",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:44.628044",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "On macOS we install via brew, so that would be what to look at, brew packaging if it can't be fixed in upstream llama.cpp",
          "created_at": "2025-04-30T18:03:38Z"
        },
        {
          "author": "arburka",
          "body": "I do not have the technical ability to make the change - would it make sense to add a comment in the file stating a workaround?\n\nIf so what is that workaround, move files to manpath?\n",
          "created_at": "2025-04-30T21:33:54Z"
        },
        {
          "author": "rhatdan",
          "body": "Man pages are working on my mac?",
          "created_at": "2025-05-01T00:12:57Z"
        },
        {
          "author": "arburka",
          "body": "Yes, man pages work, just not the ramalama ones?\n\nFor example, I type man podman and it pulls this up.\n<img width=\"879\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/73b546d6-2844-4135-97a0-0d77d9d2909a\" />\n\nWhen I do the same for `man ramalama` or others like `man ramalama-list'\nit sa",
          "created_at": "2025-05-01T01:53:23Z"
        },
        {
          "author": "rhatdan",
          "body": "How did you install RamaLama?  My laptop might be the outlier, but I installed via pipx?",
          "created_at": "2025-05-01T09:48:51Z"
        }
      ]
    },
    {
      "issue_number": 1278,
      "title": "New model store doubling model name",
      "body": "### Issue Description\n\n\n```\n$ bin/ramalama ls\nStarting importing AI models to new store...\nImported /Users/ecurtin/.local/share/ramalama/models/ollama/smollm:135m -> /Users/ecurtin/.local/share/ramalama/store/ollama/smollm/smollm/snapshots/sha256-e0a34469bc46d5348026979b54d997a0be594c1edfd36928bb472d34f5a1f117/smollm:135m\nImported /Users/ecurtin/.local/share/ramalama/models/ollama/smollm:1.7b -> /Users/ecurtin/.local/share/ramalama/store/ollama/smollm/smollm/snapshots/sha256-fa9c77c540510d4ac24593497c6a56df798617c9eb94bd0e9434aea9d51c9b76/smollm:1.7b\nNAME                        MODIFIED               SIZE\nollama://smollm/smollm:135m Less than a second ago 87.48 MB\nollama://smollm/smollm:1.7b Less than a second ago 944.83 MB\necurtin@ecur arm64 12:58:36 0 cli-advancements\n$ bin/ramalama ls\nNAME                        MODIFIED      SIZE\nollama://smollm/smollm:135m 4 seconds ago 87.48 MB\nollama://smollm/smollm:1.7b 3 seconds ago 944.83 MB\n```\n\n\n### Steps to reproduce the issue\n\nJust run an \"ls\" on an existing installation\n\n### Describe the results you received\n\nThe model name is duplicated\n\n### Describe the results you expected\n\nThe model name is not duplicated\n\n### ramalama info output\n\n```yaml\nNA\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-04-25T12:01:16Z",
      "updated_at": "2025-05-01T10:52:14Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1278/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1278",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1278",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:44.822971",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "@engelmi noticed this\n\nWould be nice if we postponed next release until we fix this. The double model name\n\nI did like how seamless this was... It just worked",
          "created_at": "2025-04-25T12:02:18Z"
        },
        {
          "author": "ericcurtin",
          "body": "Also I wonder should we take the opportunity to remove the \":\" character from all files names with the migration (illegal character on Windows file systems)",
          "created_at": "2025-04-25T12:03:47Z"
        },
        {
          "author": "engelmi",
          "body": "@ericcurtin What do you mean by doubling model name? The `smollm/smollm` part is due to the structure of `<source>/<organization>/<name>` and if the organization is not given, then it falls back to using the model name as organization. We can easily remove this duplicate.\n\n> Also I wonder should we ",
          "created_at": "2025-04-25T12:20:02Z"
        },
        {
          "author": "ericcurtin",
          "body": "Sounds like it's just an aesthetic change to \"ramalama ls\" output.\n\nThanks for the explanations @engelmi . Feel free to ignore my comment about Windows, since this only happens for migrated systems, doesn't matter I guess.\n",
          "created_at": "2025-04-25T12:27:17Z"
        },
        {
          "author": "ericcurtin",
          "body": "FWIW in the actual ollama pulling protocol, when a model has no organization they use \"library/\". We could have just copied that, it's implementation details. In theory you could have \"smollm:135m\" and \"smollm/smollm:135m\" as two different models in the ollama registry, so \"library/\" is probably mor",
          "created_at": "2025-04-25T12:30:02Z"
        }
      ]
    },
    {
      "issue_number": 1275,
      "title": "version from the main branch should have a marker to report 'in development' mode",
      "body": "### Feature request description\n\nwhen I'm getting the version of RamaLama from the current main branch, today I have `0.7.5`\n\nbut this version is not really the one that I'm using, as 0.7.5 is corresponding to a previous tag.\n\nUsually, the version of the development has a suffix to indicate that it's not really attached to a given tag but in the process of development.\n\nIf I open/report a bug, instead of reporting `0.7.5` I would report for example `0.8.0-dev` or `0.7.6-next` meaning I'm using a development version\n\n### Suggest potential solution\n\nuse a suffix to indicate it's not a fixed version\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-25T07:55:40Z",
      "updated_at": "2025-05-01T10:18:13Z",
      "closed_at": "2025-05-01T09:49:34Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1275/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1275",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1275",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:45.074907",
      "comments": [
        {
          "author": "rhatdan",
          "body": "We decided against this.",
          "created_at": "2025-05-01T09:49:34Z"
        },
        {
          "author": "benoitf",
          "body": "I think there was a misunderstanding \n\nIt's not to have a separate devel branch and a main branch\n\njust to update the version in the main branch to not be a fixed version but a  -dev suffix\n\nPeople would still contribute to the main branch.",
          "created_at": "2025-05-01T10:18:12Z"
        }
      ]
    },
    {
      "issue_number": 1321,
      "title": "make env LLAMA_PROMPT_PREFIX show when set",
      "body": "### Issue Description\n\nIf users would like to set LLAMA_PROMPT_PREFIX to have their own prompt is doesnt work.\n\n### Steps to reproduce the issue\n\nexport LLAMA_PROMPT_PREFIX=\"my company EMOJI icon>\"\nramalama run mistral\nðŸ¦­ >\n\n\n### Describe the results you received\n\nEven when set, I see:\nðŸ¦­ >\n\n### Describe the results you expected\n\nexport LLAMA_PROMPT_PREFIX=\"my company EMOJI icon>\"\nramalama run mistral\nmy company EMOJI icon>\n\n### ramalama info output\n\n```yaml\n$ ramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpuset\",\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"hugetlb\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.71,\n                    \"systemPercent\": 0.03,\n                    \"userPercent\": 0.26\n                },\n                \"cpus\": 16,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2045,\n                \"hostname\": \"fedora\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.11.4-301.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 31015186432,\n                \"memTotal\": 67110449152,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250415.g2340bbf-1.fc41.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"27h 18m 6.00s (Approximately 1.12 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/douglas/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 3,\n                    \"paused\": 0,\n                    \"running\": 2,\n                    \"stopped\": 1\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/douglas/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1998694907904,\n                \"graphRootUsed\": 35231080448,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 6\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/douglas/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Tue Apr  1 20:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.23.7\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.8\",\n    \"Runtime\": \"llama.cpp\",\n    \"Shortnames\": {\n        \"Files\": [\n            \"/usr/local/share/ramalama/shortnames.conf\",\n            \"/usr/local/share/ramalama/shortnames.conf\",\n            \"/home/douglas/bostondevconf/ramalama/douglas/ramalama/shortnames/shortnames.conf\"\n        ],\n        \"Names\": {\n            \"cerebrum\": \"huggingface://froggeric/Cerebrum-1.0-7b-GGUF/Cerebrum-1.0-7b-Q4_KS.gguf\",\n            \"deepseek\": \"ollama://deepseek-r1\",\n            \"dragon\": \"huggingface://llmware/dragon-mistral-7b-v0/dragon-mistral-7b-q4_k_m.gguf\",\n            \"gemma3\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"gemma3:12b\": \"hf://bartowski/google_gemma-3-12b-it-GGUF/google_gemma-3-12b-it-IQ2_M.gguf\",\n            \"gemma3:1b\": \"hf://bartowski/google_gemma-3-1b-it-GGUF/google_gemma-3-1b-it-IQ2_M.gguf\",\n            \"gemma3:27b\": \"hf://bartowski/google_gemma-3-27b-it-GGUF/google_gemma-3-27b-it-IQ2_M.gguf\",\n            \"gemma3:4b\": \"hf://bartowski/google_gemma-3-4b-it-GGUF/google_gemma-3-4b-it-IQ2_M.gguf\",\n            \"granite\": \"ollama://granite3.1-dense\",\n            \"granite-code\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:20b\": \"hf://ibm-granite/granite-20b-code-base-8k-GGUF/granite-20b-code-base.Q4_K_M.gguf\",\n            \"granite-code:34b\": \"hf://ibm-granite/granite-34b-code-base-8k-GGUF/granite-34b-code-base.Q4_K_M.gguf\",\n            \"granite-code:3b\": \"hf://ibm-granite/granite-3b-code-base-2k-GGUF/granite-3b-code-base.Q4_K_M.gguf\",\n            \"granite-code:8b\": \"hf://ibm-granite/granite-8b-code-base-4k-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab-7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite-lab-8b\": \"huggingface://ibm-granite/granite-8b-code-base-GGUF/granite-8b-code-base.Q4_K_M.gguf\",\n            \"granite-lab:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"hermes\": \"huggingface://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf\",\n            \"ibm/granite\": \"ollama://granite3.1-dense:8b\",\n            \"ibm/granite:2b\": \"ollama://granite3.1-dense:2b\",\n            \"ibm/granite:7b\": \"huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf\",\n            \"ibm/granite:8b\": \"ollama://granite3.1-dense:8b\",\n            \"merlinite\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab-7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite-lab:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"merlinite:7b\": \"huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf\",\n            \"mistral\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v1\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n            \"mistral:7b-v2\": \"huggingface://TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n            \"mistral:7b-v3\": \"huggingface://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n            \"mistral_code_16k\": \"huggingface://TheBloke/Mistral-7B-Code-16K-qlora-GGUF/mistral-7b-code-16k-qlora.Q4_K_M.gguf\",\n            \"mistral_codealpaca\": \"huggingface://TheBloke/Mistral-7B-codealpaca-lora-GGUF/mistral-7b-codealpaca-lora.Q4_K_M.gguf\",\n            \"mixtao\": \"huggingface://MaziyarPanahi/MixTAO-7Bx2-MoE-Instruct-v7.0-GGUF/MixTAO-7Bx2-MoE-Instruct-v7.0.Q4_K_M.gguf\",\n            \"openchat\": \"huggingface://TheBloke/openchat-3.5-0106-GGUF/openchat-3.5-0106.Q4_K_M.gguf\",\n            \"openorca\": \"huggingface://TheBloke/Mistral-7B-OpenOrca-GGUF/mistral-7b-openorca.Q4_K_M.gguf\",\n            \"phi2\": \"huggingface://MaziyarPanahi/phi-2-GGUF/phi-2.Q4_K_M.gguf\",\n            \"smollm:135m\": \"ollama://smollm:135m\",\n            \"tiny\": \"ollama://tinyllama\"\n        }\n    },\n    \"Store\": \"/home/douglas/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.8.1\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "dougsland",
      "author_type": "User",
      "created_at": "2025-04-30T22:50:45Z",
      "updated_at": "2025-05-01T05:12:54Z",
      "closed_at": "2025-04-30T23:49:55Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1321/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1321",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1321",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:47.029076",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Closing... Not a bug, working as designed",
          "created_at": "2025-04-30T23:49:55Z"
        }
      ]
    },
    {
      "issue_number": 1304,
      "title": "Error: pop from empty list",
      "body": "### Issue Description\n\nShow 'Error: pop from empty list' when run a mode the first time.\n\nâœ— ramalama run llama3.2 \n 99% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |    1.88 GB/   1.88 GB   7.68 MB/s        0sError: pop from empty list\n\n### Steps to reproduce the issue\n\nRun a model that you have not pulled before.\n\nâœ— ramalama run llama3.2 \n 99% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |    1.88 GB/   1.88 GB   7.68 MB/s        0sError: pop from empty list\n\n### Describe the results you received\n\nError: pop from empty list\n\n### Describe the results you expected\n\nNo error 'Error: pop from empty list'\n\n### ramalama info output\n\n```yaml\nâœ— ramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.31.2\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.7-2.fc37.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.7, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 98.28,\n                    \"systemPercent\": 0.43,\n                    \"userPercent\": 1.29\n                },\n                \"cpus\": 8,\n                \"databaseBackend\": \"boltdb\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"37\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2048,\n                \"hostname\": \"fedora\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.4.12-100.fc37.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 2187198464,\n                \"memTotal\": 33331286016,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.7.0-1.fc37.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.7.0\"\n                    },\n                    \"package\": \"netavark-1.7.0-1.fc37.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.7.0\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.8.6-1.fc37.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.8.6\\ncommit: 73f759f4a39769f60990e7d225f561b4f4f06bcf\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20230625.g32660ce-1.fc37.x86_64\",\n                    \"version\": \"pasta 0^20230625.g32660ce-1.fc37.x86_64\\nCopyright Red Hat\\nGNU Affero GPL version 3 or later <https://www.gnu.org/licenses/agpl-3.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.2.0-8.fc37.x86_64\",\n                    \"version\": \"slirp4netns version 1.2.0\\ncommit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383\\nlibslirp: 4.7.0\\nSLIRP_CONFIG_VERSION_MAX: 4\\nlibseccomp: 2.5.3\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"23h 21m 54.00s (Approximately 0.96 days)\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\",\n                    \"quay.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/zguo/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 0,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/zguo/.local/share/containers/storage\",\n                \"graphRootAllocated\": 501657608192,\n                \"graphRootUsed\": 118163988480,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"extfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 19\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/zguo/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"4.6.1\",\n                \"Built\": 1691705275,\n                \"BuiltTime\": \"Fri Aug 11 06:07:55 2023\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.19.10\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"4.6.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/zguo/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "guoguojenna",
      "author_type": "User",
      "created_at": "2025-04-29T01:05:55Z",
      "updated_at": "2025-04-30T16:11:29Z",
      "closed_at": "2025-04-30T12:16:03Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1304/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1304",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1304",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:47.230318",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Please run with --debug flag.",
          "created_at": "2025-04-29T10:37:22Z"
        },
        {
          "author": "rhatdan",
          "body": "@ggerganov Any ideas on this, I think it is coming from llama.cpp?",
          "created_at": "2025-04-29T13:30:52Z"
        },
        {
          "author": "ggerganov",
          "body": "This error is thrown by the `minja` module:\n\nhttps://github.com/ggml-org/llama.cpp/blob/7d3af70b089bb349b5d17eb01839224c99ec1952/common/minja/minja.hpp#L218\n\nNot sure why it happens only on the first run - next runs are ok.",
          "created_at": "2025-04-29T13:37:03Z"
        },
        {
          "author": "rhatdan",
          "body": "We are thinking this could be fixed by `https://github.com/containers/ramalama/pull/1307` if it is ramalama related.\n",
          "created_at": "2025-04-29T16:31:25Z"
        },
        {
          "author": "rhatdan",
          "body": "@guoguojenna could you try that fix?\n",
          "created_at": "2025-04-29T16:31:46Z"
        }
      ]
    },
    {
      "issue_number": 1308,
      "title": "RuntimeError: CUDA version (0, 0) is not supported. Minimum required version is 12.4.",
      "body": "### Issue Description\n\nDepending nvidia-smi version it doesn't include version flag.\n\n\n\n### Steps to reproduce the issue\n\nTrigger ramalama\n\n```\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n```\n\n```\n$ ramalama\nTraceback (most recent call last):\n  File \"/home/orion/ramalama/venv/bin/ramalama\", line 108, in <module>\n    main(sys.argv[1:])\n  File \"/home/orion/ramalama/venv/bin/ramalama\", line 58, in main\n    parser, args = ramalama.init_cli()\n  File \"/home/orion/ramalama/venv/lib/python3.10/site-packages/ramalama/cli.py\", line 76, in init_cli\n    parser = create_argument_parser(description)\n  File \"/home/orion/ramalama/venv/lib/python3.10/site-packages/ramalama/cli.py\", line 112, in create_argument_parser\n    configure_arguments(parser)\n  File \"/home/orion/ramalama/venv/lib/python3.10/site-packages/ramalama/cli.py\", line 150, in configure_arguments\n    default=accel_image(CONFIG, None),\n  File \"/home/orion/ramalama/venv/lib/python3.10/site-packages/ramalama/common.py\", line 607, in accel_image\n    image = select_cuda_image(config)\n  File \"/home/orion/ramalama/venv/lib/python3.10/site-packages/ramalama/common.py\", line 574, in select_cuda_image\n    raise RuntimeError(f\"CUDA version {cuda_version} is not supported. Minimum required version is 12.4.\")\n```\n\nAs we can see below CUDA Version is 12.6, higher than 12.4.\nnvidia-smi: 540.4.0\n\n```\n$ nvidia-smi\nTue Apr 29 11:53:17 2025\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 540.4.0                Driver Version: 540.4.0      CUDA Version: 12.6     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Orin (nvgpu)                  N/A  | N/A              N/A |                  N/A |\n| N/A   N/A  N/A               N/A /  N/A | Not Supported        |     N/A          N/A |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n```\n\n### Describe the results you received\n\nRuntimeError(f\"CUDA version {cuda_version} is not supported. Minimum required version is 12.4.\")\n\n### Describe the results you expected\n\nNo error, doesn't matter which version nvidia-smi is installed (with --version or not so the code is compatible).\n\n### ramalama info output\n\n```yaml\n$ ramalama info\nTraceback (most recent call last):\n  File \"/usr/local/bin/ramalama\", line 108, in <module>\n    main(sys.argv[1:])\n  File \"/usr/local/bin/ramalama\", line 58, in main\n    parser, args = ramalama.init_cli()\n  File \"/usr/local/lib/python3.10/dist-packages/ramalama/cli.py\", line 76, in init_cli\n    parser = create_argument_parser(description)\n  File \"/usr/local/lib/python3.10/dist-packages/ramalama/cli.py\", line 112, in create_argument_parser\n    configure_arguments(parser)\n  File \"/usr/local/lib/python3.10/dist-packages/ramalama/cli.py\", line 150, in configure_arguments\n    default=accel_image(CONFIG, None),\n  File \"/usr/local/lib/python3.10/dist-packages/ramalama/common.py\", line 607, in accel_image\n    image = select_cuda_image(config)\n  File \"/usr/local/lib/python3.10/dist-packages/ramalama/common.py\", line 574, in select_cuda_image\n    raise RuntimeError(f\"CUDA version {cuda_version} is not supported. Minimum required version is 12.4.\")\nRuntimeError: CUDA version (0, 0) is not supported. Minimum required version is 12.4.\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "dougsland",
      "author_type": "User",
      "created_at": "2025-04-29T18:00:22Z",
      "updated_at": "2025-04-30T13:47:14Z",
      "closed_at": "2025-04-30T13:47:14Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1308/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1308",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1308",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:47.438108",
      "comments": [
        {
          "author": "dougsland",
          "body": "Proposal patch to fix (for both cases):\n\n```\n$ git diff\ndiff --git a/ramalama/common.py b/ramalama/common.py\nindex ab95b35..3e7e06f 100644\n--- a/ramalama/common.py\n+++ b/ramalama/common.py\n@@ -531,7 +531,7 @@ def check_cuda_version():\n     \"\"\"\n     try:\n         # Run nvidia-smi --version to get ver",
          "created_at": "2025-04-29T18:06:00Z"
        },
        {
          "author": "rhatdan",
          "body": "Seems to work , please open a PR.",
          "created_at": "2025-04-29T20:00:59Z"
        }
      ]
    },
    {
      "issue_number": 1264,
      "title": "get feedback on the pull command when the model already exists",
      "body": "### Feature request description\n\nToday, if I launch for the first time\n\n`ramalama pull tinyllama`\n\nI get a progress bar and I see that something is happening and it's fetching the model.\n\n\nNow, if I start again the same command, it exits almost immediately and nothing is reported. So while I don't see an error I might thing it worked, but I suppose having a trace like `Model already exists, skipping` or `Model already up-to-date` might give a better outcome of what has being done.\n\n### Suggest potential solution\n\nadd a message saying it's already there\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-24T09:51:44Z",
      "updated_at": "2025-04-30T07:23:21Z",
      "closed_at": "2025-04-30T07:23:20Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1264/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1264",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1264",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:47.664774",
      "comments": [
        {
          "author": "rhatdan",
          "body": "On plane what does podman do in this case?",
          "created_at": "2025-04-24T10:47:18Z"
        },
        {
          "author": "benoitf",
          "body": "podman pull again:\n\n```\npodman pull httpd                                                                                                                                                                                              Trying to pull docker.io/library/httpd:latest...\nGetting image source",
          "created_at": "2025-04-24T11:00:23Z"
        },
        {
          "author": "rhatdan",
          "body": "Ok so we have precidence.  We could just output the object.\n",
          "created_at": "2025-04-24T11:04:17Z"
        },
        {
          "author": "ericcurtin",
          "body": "Yeah, we took a `git fetch` like approach here, if there's nothing to pull, show nada and return an exit code of zero",
          "created_at": "2025-04-25T23:38:20Z"
        },
        {
          "author": "rhatdan",
          "body": "@ericcurtin which would you prefer?\n\n```\n$ podman pull alpine\nTrying to pull docker.io/library/alpine:latest...\nGetting image source signatures\nCopying blob f18232174bc9 skipped: already exists  \nCopying config aded1e1a5b done   | \nWriting manifest to image destination\naded1e1a5b3705116fa0a92ba074a5",
          "created_at": "2025-04-26T12:55:21Z"
        }
      ]
    },
    {
      "issue_number": 1289,
      "title": "Bad output from model served by ramalama",
      "body": "### Issue Description\n\nI'm getting bad/weird output from the ramalama:latest image + mistral:latest model (and the granite-code:latest model too).  I'm using the following `ramalama --debug --image quay.io/ramalama/ramalama:latest serve --port 11434 mistral:latest` which starts up fine and hosts the web UI, but when prompting I get what appears to be nonsense repeating for quite a long while before stopping with mistral, and granite-code borks the output stream on the web UI and never prints anything (at least not while I had a very brief wait for it).\n\nThanks to the suggestion of @benoitf to use `--ngl 0` to force CPU inferencing I was able to get normal output from the models.\n\nNOTE: I'm specifying the ramalama:latest image because I ran into #1251 with the intel-gpu image.\n\n### Steps to reproduce the issue\n\nTo reproduce:\n\n1. Serve  a model with ramalama: `ramalama --debug --image quay.io/ramalama/ramalama:latest serve --port 11434 mistral:latest`\n2. Visit the web UI at `http://localhost:11434`\n3. Prompt it with something like \"What color is the sky?\"\n4. Observe random output when using mistral model, or weird UI issues if using granite-code\n\n### Describe the results you received\n\nFollowing the reproduction steps above, here is some example output:\n\n```\nQ: Hello! How are you?\nA: mobilejaxDOM constitu whilst whilst AB mobileonical delightha whilst Trad whilstDroidridgeute constitusubscription AB organis sustoiDroid organis...\n```\n\nand\n\n```\nQ: What color is the sky?\nA: Question Question Q Q Q Question Question User Question Question Q Q Q Q Question Question Q Q Q Question Q Q Question User User Q Q Question Question Q Q Q Q Q Q Question Question User Q Q Q Q Q Q Q User Question # Question Question Question Q Q Question Q Q Q Question Question # User Question Question // Q Q Q Question Question Q Question Question Q Question Q # Q User Q Q Q Q Q Q // # Q Question Question Question Q Q Question User Q Q User Q Q Q # User Q Question Q Question Question Q Q Question Q Q Q Question Q Question Question Q Question Question Q // Q Q User Q Q Question Q Question Question Question Question Question Question Q User Q Question Q Question Q Question Question Q /******/ Q Q # Q Question\n```\n\nwith the output strings repeating until I killed it for several minutes.\n\n### Describe the results you expected\n\nActual responses rather than seemingly random words/letters.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"intel\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 98.16,\n                    \"systemPercent\": 0.4,\n                    \"userPercent\": 1.45\n                },\n                \"cpus\": 22,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2044,\n                \"hostname\": \"my-fedora42\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 17833,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 165536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 17833,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 165536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.2-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 15207501824,\n                \"memTotal\": 66819031040,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/17833/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/17833/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8588636160,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"196h 9m 45.00s (Approximately 8.17 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/me/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 4,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 4\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/me/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1022488809472,\n                \"graphRootUsed\": 46517395456,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 2\n                },\n                \"runRoot\": \"/run/user/17833/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/me/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Tue Apr  1 20:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/me/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "csutherl",
      "author_type": "User",
      "created_at": "2025-04-25T17:31:37Z",
      "updated_at": "2025-04-29T15:37:00Z",
      "closed_at": "2025-04-28T18:31:06Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1289/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1289",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1289",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:47.863224",
      "comments": [
        {
          "author": "rhatdan",
          "body": "First you do not need to specify `--image quay.io/ramalama/ramalama:latest` RamaLama should pick the best image for you.\n\nIf there are no GPUs for you to use, then it should fall back to CPU only inferencing? Are you seeing --ngl=  in your debug line?",
          "created_at": "2025-04-25T21:12:21Z"
        },
        {
          "author": "csutherl",
          "body": "If I don't specify the image, then it tries to use what is best (which is the intel-gpu) but that image is broken at the moment, as described in #1251.\n\nAs far as `--ngl=` being in the output, it's not in the debug output when I specify the image (though it note that it tried using the gpu but switc",
          "created_at": "2025-04-26T01:16:40Z"
        },
        {
          "author": "rhatdan",
          "body": "Ok images will be replaced on Monday.  Not sure what the default for --ngl if  you override the image.",
          "created_at": "2025-04-26T12:32:08Z"
        },
        {
          "author": "rhatdan",
          "body": "RamaLama 0.8.0 is now released, reopen if this still does not work.",
          "created_at": "2025-04-28T18:31:06Z"
        },
        {
          "author": "csutherl",
          "body": "I just retried and still have the same issue:\n\n```\nwhat color is the sky?\n\nsubscription constitu // sustÐºÑ‚Ñƒ AB sust mobile AB Mobile constitu bowl animatedha concer animatedhaoi whilst //odes mobileitan sust mobileonical whilstoiDOM susthaoi ABoiSubscriptionRLjax AB whilst constituDroid towards ABst",
          "created_at": "2025-04-29T15:36:46Z"
        }
      ]
    },
    {
      "issue_number": 1296,
      "title": "Error: ramalama login for 'Ollama' not implemented",
      "body": "### Issue Description\n\n```\n$ ramalama login --token=XYZ huggingface\nError: ramalama login for 'Ollama' not implemented\n\nThe error message is confusing. The command login to 'huggingface', but the output reports 'Ollama'.\n```\n\n### Steps to reproduce the issue\n\nhttps://github.com/containers/ramalama/blob/main/docs/ramalama-login.1.md\n\n### Describe the results you received\n\nError: ramalama login for 'Ollama' not implemented\n\n### Describe the results you expected\n\nThe output of login should be related to huggingface.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.38.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.12-3.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.12, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 96.93,\n                    \"systemPercent\": 1.4,\n                    \"userPercent\": 1.66\n                },\n                \"cpus\": 4,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"cloud\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2048,\n                \"hostname\": \"yizhan-image-mode\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1002,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 655360,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1002,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 655360,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.11.4-301.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 914583552,\n                \"memTotal\": 8316452864,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.12.2-2.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.12.2\"\n                    },\n                    \"package\": \"netavark-1.12.2-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.12.2\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.17-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.17\\ncommit: 000fa0d4eeed8938301f3bcf8206405315bc1017\\nrundir: /run/user/1002/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20240906.g6b38f07-1.fc41.x86_64\",\n                    \"version\": \"pasta 0^20240906.g6b38f07-1.fc41.x86_64\\nCopyright Red Hat\\nGNU General Public License, version 2 or later\\n  <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1002/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8273522688,\n                \"swapTotal\": 8316252160,\n                \"uptime\": \"3303h 9m 8.00s (Approximately 137.62 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/zguo/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 0,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/zguo/.local/share/containers/storage\",\n                \"graphRootAllocated\": 63267909632,\n                \"graphRootUsed\": 19560669184,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 30\n                },\n                \"runRoot\": \"/run/user/1002/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/zguo/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.3.1\",\n                \"Built\": 1732147200,\n                \"BuiltTime\": \"Thu Nov 21 00:00:00 2024\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.23.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.3.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/zguo/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nfc41\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "guoguojenna",
      "author_type": "User",
      "created_at": "2025-04-28T01:15:45Z",
      "updated_at": "2025-04-29T13:04:18Z",
      "closed_at": "2025-04-29T13:04:17Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1296/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1296",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1296",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:48.082910",
      "comments": [
        {
          "author": "guoguojenna",
          "body": "I am thinking could we set the transport automatically based on detected model prefixes, ensuring consistency between the model and transport? Fallback to explicit transport only when no model prefix is detected.",
          "created_at": "2025-04-28T02:03:59Z"
        },
        {
          "author": "rhatdan",
          "body": "Fixed by https://github.com/containers/ramalama/pull/1298\n",
          "created_at": "2025-04-29T13:04:17Z"
        }
      ]
    },
    {
      "issue_number": 1297,
      "title": "Look like it should be the version of tool ramalama itself, not AI Model",
      "body": "### Issue Description\n\nActual output:\n```\n$ ramalama -h | grep 'version             display version of AI Model'\n    version             display version of AI Model\n```\nExpect:\n```\n$ ramalama -h | grep 'version            '\n    version             display version of ramalama\n```\n\n\n### Steps to reproduce the issue\n\n```\n$ ramalama -h | grep 'version             display version of AI Model'\n    version             display version of AI Model\n```\n\n### Describe the results you received\n\n```\n$ ramalama -h | grep 'version             display version of AI Model'\n    version             display version of AI Model\n```\n\n### Describe the results you expected\n\n```\n$ ramalama -h | grep 'version             '\n    version             display version of ramalama\n```\n\n### ramalama info output\n\n```yaml\n$ ramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.38.0\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.12-3.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.12, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 96.93,\n                    \"systemPercent\": 1.4,\n                    \"userPercent\": 1.67\n                },\n                \"cpus\": 4,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"cloud\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2047,\n                \"hostname\": \"yizhan-image-mode\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1002,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 655360,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1002,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 655360,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.11.4-301.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 990515200,\n                \"memTotal\": 8316452864,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.12.2-2.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.12.2\"\n                    },\n                    \"package\": \"netavark-1.12.2-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.12.2\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.17-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.17\\ncommit: 000fa0d4eeed8938301f3bcf8206405315bc1017\\nrundir: /run/user/1002/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20240906.g6b38f07-1.fc41.x86_64\",\n                    \"version\": \"pasta 0^20240906.g6b38f07-1.fc41.x86_64\\nCopyright Red Hat\\nGNU General Public License, version 2 or later\\n  <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1002/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 7484993536,\n                \"swapTotal\": 8316252160,\n                \"uptime\": \"3305h 28m 11.00s (Approximately 137.71 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/zguo/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 1,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/zguo/.local/share/containers/storage\",\n                \"graphRootAllocated\": 63267909632,\n                \"graphRootUsed\": 25416507392,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 30\n                },\n                \"runRoot\": \"/run/user/1002/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/zguo/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.3.1\",\n                \"Built\": 1732147200,\n                \"BuiltTime\": \"Thu Nov 21 00:00:00 2024\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.23.3\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.3.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/zguo/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "guoguojenna",
      "author_type": "User",
      "created_at": "2025-04-28T03:40:24Z",
      "updated_at": "2025-04-28T18:20:09Z",
      "closed_at": "2025-04-28T18:20:09Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1297/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1297",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1297",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:48.304467",
      "comments": []
    },
    {
      "issue_number": 1263,
      "title": "expose shortnames with the CLI",
      "body": "### Feature request description\n\nramalama has shortnames\n\nfrom README:\n\n> to make it easier for users, RamaLama uses shortname files, which container alias names for fully specified AI Models allowing users to specify the shorter names when referring to models. \n\nbut the provenance of shortnames, based on the documentation can be for example:\n\n- Distribution /usr/share/ramalama/shortnames.conf\n- Administrators /etc/ramamala/shortnames.conf\n- Users $HOME/.config/ramalama/shortnames.conf\n\nso it's not easy to know what are the available shortnames or to list them\n\nI would like to have a CLI command to list them (having also a JSON formatter output)\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-24T09:41:54Z",
      "updated_at": "2025-04-28T11:23:56Z",
      "closed_at": "2025-04-28T11:23:56Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1263/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 1,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1263",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1263",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:48.304493",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Ok with just using ramalama info, or do you need a different command?",
          "created_at": "2025-04-24T10:48:12Z"
        },
        {
          "author": "benoitf",
          "body": "I would be fine with any command, it might be a good pattern to have that part of the `info` as it would display the current configuration used by RamaLama\n\nfrom a user POV, maybe people would expect it to see that as well inside `list` (`list --shortnames` but well for now list is tied to the 'down",
          "created_at": "2025-04-24T10:51:14Z"
        },
        {
          "author": "rhatdan",
          "body": "`ramalama info` might make sense to show changes in ramalama.conf as well.",
          "created_at": "2025-04-24T11:10:44Z"
        },
        {
          "author": "benoitf",
          "body": "for another command, It would depend, if later, you want to allow people to manage the shortnames from the cli\n\n`ramalama shortname list`\n`ramalama shortname add tiny ollama://tinyllama`\netc.\n\n",
          "created_at": "2025-04-24T11:16:43Z"
        },
        {
          "author": "benoitf",
          "body": "or\n\n`ramalama model list --downloaded`\n`ramalama model list --shortnames`\n`ramalama model alias tiny ollama://tinyllama`\n\n\nbut yes for now, info is perfect",
          "created_at": "2025-04-24T11:18:02Z"
        }
      ]
    },
    {
      "issue_number": 758,
      "title": "Using vllm runtime generates \"unrecognized arguments\" error",
      "body": "An attempt to use `vllm` runtime generates the following error:\n```\nerror: unrecognized arguments: llama-run -c 2048 --temp 0.8 -v --ngl 999 /mnt/models/model.file\n```\n\nFull log:\n```\n$ ramalama --debug --runtime vllm run llama3.2\nexec_cmd:  podman run --rm -i --label RAMALAMA --security-opt=label=disable --name ramalama_PNB6UFIqIM --pull=newer -t --device /dev/dri --device nvidia.com/gpu=all -e CUDA_VISIBLE_DEVICES=0 --mount=type=bind,src=/home/dw/.local/share/ramalama/models/ollama/llama3.2:latest,destination=/mnt/models/model.file,ro quay.io/modh/vllm:rhoai-2.17-cuda llama-run -c 2048 --temp 0.8 -v --ngl 999 /mnt/models/model.file\nusage: __main__.py [-h] [--host HOST] [--port PORT] [--uvicorn-log-level {debug,info,warning,error,critical,trace}] [--allow-credentials] [--allowed-origins ALLOWED_ORIGINS] [--allowed-methods ALLOWED_METHODS]\n                   [--allowed-headers ALLOWED_HEADERS] [--api-key API_KEY] [--lora-modules LORA_MODULES [LORA_MODULES ...]] [--prompt-adapters PROMPT_ADAPTERS [PROMPT_ADAPTERS ...]] [--chat-template CHAT_TEMPLATE]\n                   [--chat-template-content-format {auto,string,openai}] [--response-role RESPONSE_ROLE] [--ssl-keyfile SSL_KEYFILE] [--ssl-certfile SSL_CERTFILE] [--ssl-ca-certs SSL_CA_CERTS] [--ssl-cert-reqs SSL_CERT_REQS]\n                   [--root-path ROOT_PATH] [--middleware MIDDLEWARE] [--return-tokens-as-token-ids] [--disable-frontend-multiprocessing] [--enable-request-id-headers] [--enable-auto-tool-choice]\n                   [--tool-call-parser {granite-20b-fc,granite,hermes,internlm,jamba,llama3_json,mistral,pythonic} or name registered in --tool-parser-plugin] [--tool-parser-plugin TOOL_PARSER_PLUGIN] [--model MODEL]\n                   [--task {auto,generate,embedding,embed,classify,score,reward}] [--tokenizer TOKENIZER] [--skip-tokenizer-init] [--revision REVISION] [--code-revision CODE_REVISION] [--tokenizer-revision TOKENIZER_REVISION]\n                   [--tokenizer-mode {auto,slow,mistral}] [--trust-remote-code] [--allowed-local-media-path ALLOWED_LOCAL_MEDIA_PATH] [--download-dir DOWNLOAD_DIR]\n                   [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral,runai_streamer}] [--config-format {auto,hf,mistral}] [--dtype {auto,half,float16,bfloat16,float,float32}]\n                   [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}] [--quantization-param-path QUANTIZATION_PARAM_PATH] [--max-model-len MAX_MODEL_LEN] [--guided-decoding-backend {outlines,lm-format-enforcer,xgrammar}]\n                   [--logits-processor-pattern LOGITS_PROCESSOR_PATTERN] [--distributed-executor-backend {ray,mp}] [--worker-use-ray] [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE] [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n                   [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS] [--ray-workers-use-nsight] [--block-size {8,16,32,64,128}] [--enable-prefix-caching | --no-enable-prefix-caching] [--disable-sliding-window]\n                   [--use-v2-block-manager] [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS] [--seed SEED] [--swap-space SWAP_SPACE] [--cpu-offload-gb CPU_OFFLOAD_GB] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n                   [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE] [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS] [--max-num-seqs MAX_NUM_SEQS] [--max-logprobs MAX_LOGPROBS] [--disable-log-stats]\n                   [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,None}]\n                   [--rope-scaling ROPE_SCALING] [--rope-theta ROPE_THETA] [--hf-overrides HF_OVERRIDES] [--enforce-eager] [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE] [--disable-custom-all-reduce]\n                   [--tokenizer-pool-size TOKENIZER_POOL_SIZE] [--tokenizer-pool-type TOKENIZER_POOL_TYPE] [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG] [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]\n                   [--mm-processor-kwargs MM_PROCESSOR_KWARGS] [--disable-mm-preprocessor-cache] [--enable-lora] [--enable-lora-bias] [--max-loras MAX_LORAS] [--max-lora-rank MAX_LORA_RANK]\n                   [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE] [--lora-dtype {auto,float16,bfloat16}] [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS] [--max-cpu-loras MAX_CPU_LORAS] [--fully-sharded-loras]\n                   [--enable-prompt-adapter] [--max-prompt-adapters MAX_PROMPT_ADAPTERS] [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN] [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu,hpu}]\n                   [--num-scheduler-steps NUM_SCHEDULER_STEPS] [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]] [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR] [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n                   [--speculative-model SPECULATIVE_MODEL]\n                   [--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,None}]\n                   [--num-speculative-tokens NUM_SPECULATIVE_TOKENS] [--speculative-disable-mqa-scorer] [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]\n                   [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN] [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE] [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]\n                   [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN] [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]\n                   [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD] [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]\n                   [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]] [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG] [--ignore-patterns IGNORE_PATTERNS] [--preemption-mode PREEMPTION_MODE]\n                   [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]] [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH] [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n                   [--collect-detailed-traces COLLECT_DETAILED_TRACES] [--disable-async-output-proc] [--scheduling-policy {fcfs,priority}] [--override-neuron-config OVERRIDE_NEURON_CONFIG]\n                   [--override-pooler-config OVERRIDE_POOLER_CONFIG] [--compilation-config COMPILATION_CONFIG] [--kv-transfer-config KV_TRANSFER_CONFIG] [--worker-cls WORKER_CLS] [--generation-config GENERATION_CONFIG]\n                   [--disable-log-requests] [--max-log-len MAX_LOG_LEN] [--disable-fastapi-docs] [--enable-prompt-tokens-details] [--model-name MODEL_NAME] [--max-sequence-length MAX_SEQUENCE_LENGTH] [--max-new-tokens MAX_NEW_TOKENS]\n                   [--max-batch-size MAX_BATCH_SIZE] [--max-concurrent-requests MAX_CONCURRENT_REQUESTS] [--dtype-str DTYPE_STR] [--quantize {awq,gptq,squeezellm,None}] [--num-gpus NUM_GPUS] [--num-shard NUM_SHARD]\n                   [--output-special-tokens OUTPUT_SPECIAL_TOKENS] [--default-include-stop-seqs DEFAULT_INCLUDE_STOP_SEQS] [--grpc-port GRPC_PORT] [--tls-cert-path TLS_CERT_PATH] [--tls-key-path TLS_KEY_PATH]\n                   [--tls-client-ca-cert-path TLS_CLIENT_CA_CERT_PATH] [--adapter-cache ADAPTER_CACHE] [--prefix-store-path PREFIX_STORE_PATH] [--speculator-name SPECULATOR_NAME] [--speculator-n-candidates SPECULATOR_N_CANDIDATES]\n                   [--speculator-max-batch-size SPECULATOR_MAX_BATCH_SIZE] [--enable-vllm-log-requests ENABLE_VLLM_LOG_REQUESTS] [--disable-prompt-logprobs DISABLE_PROMPT_LOGPROBS]\n__main__.py: error: unrecognized arguments: llama-run -c 2048 --temp 0.8 -v --ngl 999 /mnt/models/model.file\n```\n```\n$ rpm -qv podman\npodman-5.3.1-1.fc41.x86_64\n```\n```\n$ rpm -qv python3-ramalama\npython3-ramalama-0.5.5-1.fc41.noarch\n```\n```\n$ rpm -qv golang-github-nvidia-container-toolkit\ngolang-github-nvidia-container-toolkit-1.16.2-1.fc41.x86_64\n```\n```\n$ nvidia-ctk cdi list\nINFO[0000] Found 3 CDI devices                          \nnvidia.com/gpu=0\nnvidia.com/gpu=GPU-9282fe1f-02bd-d793-11a8-5341a0858e3b\nnvidia.com/gpu=all\n```",
      "state": "open",
      "author": "dwrobel",
      "author_type": "User",
      "created_at": "2025-02-07T12:15:31Z",
      "updated_at": "2025-04-28T02:42:53Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/758/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/758",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/758",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:48.560031",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Yes vllm can only do serve at this point.",
          "created_at": "2025-02-07T20:12:59Z"
        },
        {
          "author": "rhatdan",
          "body": "Not even sure how well that works either.",
          "created_at": "2025-02-07T20:13:12Z"
        },
        {
          "author": "guoguojenna",
          "body": "It works for me. I installed ramalama from the latest source code.\n\n$ rpm -q podman\npodman-5.3.1-1.fc41.x86_64\n\n$ ramalama version\nramalama version 0.7.5\n\n```\n$  ramalama --debug --runtime vllm run llama3.2\nexec_cmd:  podman run --rm --label ai.ramalama.model=llama3.2 --label ai.ramalama.engine=podm",
          "created_at": "2025-04-28T02:42:52Z"
        }
      ]
    },
    {
      "issue_number": 1274,
      "title": "build script to build the CLI image should allow to use the content of the current ramalama directory",
      "body": "### Feature request description\n\nWhile investigating https://github.com/containers/ramalama/issues/1269 I made some changes in my RamaLama cloned directory.\n\nthen ran `make build IMAGE=ramalama-cli`\n\nbut seeing my changes were not applied I saw that in fact the script is cloning the main branch\n\nhttps://github.com/containers/ramalama/blob/c327936811a507d85e503467e4aa0fecd0dd4d9b/container-images/scripts/build-cli.sh#L27\n\nI would say that, from development POV, I would expect it uses the local copy of RamaLama.\n\nif I clone a given tag/branch of RamaLama it will also use the main branch\n\n\n### Suggest potential solution\n\ndo not clone a remote repository but instead, as we're already inside RamaLama, use the current files\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\nTo workaround, I need to modify the build script by cloning my remote fork, etc. This is not really straightforward as it's increasing the inner loop development mode.\n\n\n",
      "state": "closed",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-25T07:45:55Z",
      "updated_at": "2025-04-26T12:23:06Z",
      "closed_at": "2025-04-26T12:23:06Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1274/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1274",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1274",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:48.737158",
      "comments": []
    },
    {
      "issue_number": 1258,
      "title": "get only the short version for `ramalama version`",
      "body": "### Feature request description\n\ntoday there is `ramalama version` displaying `ramalama version 0.7.5` but it's not convenient to parse\n\nI would like to have a `--json` option or `--format json` option or a `-q` option (as commented by Dan)\n\n### Suggest potential solution\n\n_No response_\n\n### Have you considered any alternatives?\n\n_No response_\n\n### Additional context\n\n_No response_",
      "state": "closed",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-23T21:59:57Z",
      "updated_at": "2025-04-25T10:06:50Z",
      "closed_at": "2025-04-25T10:06:50Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1258/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1258",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1258",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:48.737181",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Json or just -q for example\n\n```\nramalama version -q\n0.7.5\n```\n",
          "created_at": "2025-04-24T10:25:48Z"
        },
        {
          "author": "benoitf",
          "body": "it would work with this option as well ðŸ‘ ",
          "created_at": "2025-04-24T10:34:18Z"
        },
        {
          "author": "rhatdan",
          "body": "Ok, on an airplane which is blocking git, will push later today. Real simple fix.",
          "created_at": "2025-04-24T10:39:39Z"
        },
        {
          "author": "rhatdan",
          "body": "BTW you get json from ramalama info now, which includes version.",
          "created_at": "2025-04-24T10:40:40Z"
        },
        {
          "author": "benoitf",
          "body": "ok thanks so maybe we can close this issue\n\n",
          "created_at": "2025-04-24T10:57:57Z"
        }
      ]
    },
    {
      "issue_number": 1269,
      "title": "RamaLama CLI running inside a container fails to see if a port is already taken on macOS",
      "body": "### Issue Description\n\nRamaLama detects if the port is already taken or not before starting a 'serve' command\n\ndefault port is 8080\n\nbut when using the container's CLI, it fails to detect if ports are open or not and will start all the containers using the 8080 port, so the first one might succeed but the others will conflict\n\n### Steps to reproduce the issue\n\nrun the CLI container's `quay.io/ramalama/ramalama-cli` (by mounting the socket on macOS for example so the CLI has access to the host podman and can start containers)\n\nthen, try to serve 2 models\nfirst one will use port 8080, the second one will also use port 8080 and will fail\n\n### Describe the results you received\n\nfailure to see if port is taken or not\n\n### Describe the results you expected\n\nalways be able to detect if port is taken or not\n\n### ramalama info output\n\n```yaml\n0.7.5/or main branch\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n`host.containers.internal` (or `host.docker.internal` for docker) (or `host.docker.internal` for both) should be used when RamaLama CLI is being launched inside a container instead of checking \n\n\nhere localhost is used all the time\nhttps://github.com/containers/ramalama/blob/c43013d62d621012ea3e4a4ba25358545afa94da/ramalama/model.py#L655\n",
      "state": "closed",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-04-24T20:25:12Z",
      "updated_at": "2025-04-25T08:39:35Z",
      "closed_at": "2025-04-25T08:39:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1269/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1269",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1269",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:49.019388",
      "comments": [
        {
          "author": "benoitf",
          "body": "maybe easier to use `--network host`\n",
          "created_at": "2025-04-25T08:39:34Z"
        }
      ]
    },
    {
      "issue_number": 1234,
      "title": "Fails with cuda 12.4",
      "body": "### Issue Description\n\nI just upgraded and ramalama seems to have lost its ability to run models.  It seems like the container only supports cuda>=12.8 now.  Is this intentional?\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. `pipx install ramalam`\n2. `ramalama run ollama://gemma3:1b`\n\n\n### Describe the results you received\n\n```\nramalama run ollama://gemma3:1b\nChecking for newer image quay.io/ramalama/cuda:0.7\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running prestart hook #0: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'\nnvidia-container-cli: requirement error: unsatisfied condition: cuda>=12.8, please update your driver to a newer version, or use an earlier cuda container: unknown\n```\n\n### Describe the results you expected\n\nI expected the model to be run with cuda 12.4.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"Architecture\": \"x86_64\",\n            \"BridgeNfIp6tables\": false,\n            \"BridgeNfIptables\": false,\n            \"CDISpecDirs\": [],\n            \"CPUSet\": true,\n            \"CPUShares\": true,\n            \"CgroupDriver\": \"systemd\",\n            \"CgroupVersion\": \"2\",\n            \"ClientInfo\": {\n                \"Arch\": \"amd64\",\n                \"BuildTime\": \"Thu Apr 17 09:54:52 2025\",\n                \"Context\": \"default\",\n                \"Debug\": false,\n                \"DefaultAPIVersion\": \"1.49\",\n                \"GitCommit\": \"4d8c241\",\n                \"GoVersion\": \"go1.23.8\",\n                \"Os\": \"linux\",\n                \"Platform\": {\n                    \"Name\": \"Docker Engine - Community\"\n                },\n                \"Plugins\": [\n                    {\n                        \"Name\": \"buildx\",\n                        \"Path\": \"/usr/libexec/docker/cli-plugins/docker-buildx\",\n                        \"SchemaVersion\": \"0.1.0\",\n                        \"ShortDescription\": \"Docker Buildx\",\n                        \"Vendor\": \"Docker Inc.\",\n                        \"Version\": \"v0.22.0\"\n                    },\n                    {\n                        \"Name\": \"compose\",\n                        \"Path\": \"/usr/libexec/docker/cli-plugins/docker-compose\",\n                        \"SchemaVersion\": \"0.1.0\",\n                        \"ShortDescription\": \"Docker Compose\",\n                        \"Vendor\": \"Docker Inc.\",\n                        \"Version\": \"v2.35.0\"\n                    }\n                ],\n                \"Version\": \"28.1.0\",\n                \"Warnings\": null\n            },\n            \"Containerd\": {\n                \"Address\": \"/run/containerd/containerd.sock\",\n                \"Namespaces\": {\n                    \"Containers\": \"moby\",\n                    \"Plugins\": \"plugins.moby\"\n                }\n            },\n            \"ContainerdCommit\": {\n                \"ID\": \"05044ec0a9a75232cad458027ca83437aae3f4da\"\n            },\n            \"Containers\": 1,\n            \"ContainersPaused\": 0,\n            \"ContainersRunning\": 1,\n            \"ContainersStopped\": 0,\n            \"CpuCfsPeriod\": true,\n            \"CpuCfsQuota\": true,\n            \"Debug\": false,\n            \"DefaultRuntime\": \"runc\",\n            \"DockerRootDir\": \"/var/lib/docker\",\n            \"Driver\": \"overlay2\",\n            \"DriverStatus\": [\n                [\n                    \"Backing Filesystem\",\n                    \"extfs\"\n                ],\n                [\n                    \"Supports d_type\",\n                    \"true\"\n                ],\n                [\n                    \"Using metacopy\",\n                    \"false\"\n                ],\n                [\n                    \"Native Overlay Diff\",\n                    \"true\"\n                ],\n                [\n                    \"userxattr\",\n                    \"false\"\n                ]\n            ],\n            \"ExperimentalBuild\": false,\n            \"FirewallBackend\": {\n                \"Driver\": \"iptables\"\n            },\n            \"GenericResources\": null,\n            \"HttpProxy\": \"\",\n            \"HttpsProxy\": \"\",\n            \"ID\": \"612ff3e0-416b-4750-ab55-5ee99532fe83\",\n            \"IPv4Forwarding\": true,\n            \"Images\": 43,\n            \"IndexServerAddress\": \"https://index.docker.io/v1/\",\n            \"InitBinary\": \"docker-init\",\n            \"InitCommit\": {\n                \"ID\": \"de40ad0\"\n            },\n            \"Isolation\": \"\",\n            \"KernelVersion\": \"6.8.0-52-generic\",\n            \"Labels\": [],\n            \"LiveRestoreEnabled\": false,\n            \"LoggingDriver\": \"json-file\",\n            \"MemTotal\": 101080018944,\n            \"MemoryLimit\": true,\n            \"NCPU\": 32,\n            \"NEventsListener\": 0,\n            \"NFd\": 31,\n            \"NGoroutines\": 47,\n            \"Name\": \"banana\",\n            \"NoProxy\": \"\",\n            \"OSType\": \"linux\",\n            \"OSVersion\": \"22.04\",\n            \"OomKillDisable\": false,\n            \"OperatingSystem\": \"Ubuntu 22.04.5 LTS\",\n            \"PidsLimit\": true,\n            \"Plugins\": {\n                \"Authorization\": null,\n                \"Log\": [\n                    \"awslogs\",\n                    \"fluentd\",\n                    \"gcplogs\",\n                    \"gelf\",\n                    \"journald\",\n                    \"json-file\",\n                    \"local\",\n                    \"splunk\",\n                    \"syslog\"\n                ],\n                \"Network\": [\n                    \"bridge\",\n                    \"host\",\n                    \"ipvlan\",\n                    \"macvlan\",\n                    \"null\",\n                    \"overlay\"\n                ],\n                \"Volume\": [\n                    \"local\"\n                ]\n            },\n            \"RegistryConfig\": {\n                \"IndexConfigs\": {\n                    \"docker.io\": {\n                        \"Mirrors\": [],\n                        \"Name\": \"docker.io\",\n                        \"Official\": true,\n                        \"Secure\": true\n                    }\n                },\n                \"InsecureRegistryCIDRs\": [\n                    \"::1/128\",\n                    \"127.0.0.0/8\"\n                ],\n                \"Mirrors\": null\n            },\n            \"RuncCommit\": {\n                \"ID\": \"v1.2.5-0-g59923ef\"\n            },\n            \"Runtimes\": {\n                \"io.containerd.runc.v2\": {\n                    \"path\": \"runc\",\n                    \"status\": {\n                        \"org.opencontainers.runtime-spec.features\": \"{\\\"ociVersionMin\\\":\\\"1.0.0\\\",\\\"ociVersionMax\\\":\\\"1.2.0\\\",\\\"hooks\\\":[\\\"prestart\\\",\\\"createRuntime\\\",\\\"createContainer\\\",\\\"startContainer\\\",\\\"poststart\\\",\\\"poststop\\\"],\\\"mountOptions\\\":[\\\"async\\\",\\\"atime\\\",\\\"bind\\\",\\\"defaults\\\",\\\"dev\\\",\\\"diratime\\\",\\\"dirsync\\\",\\\"exec\\\",\\\"iversion\\\",\\\"lazytime\\\",\\\"loud\\\",\\\"mand\\\",\\\"noatime\\\",\\\"nodev\\\",\\\"nodiratime\\\",\\\"noexec\\\",\\\"noiversion\\\",\\\"nolazytime\\\",\\\"nomand\\\",\\\"norelatime\\\",\\\"nostrictatime\\\",\\\"nosuid\\\",\\\"nosymfollow\\\",\\\"private\\\",\\\"ratime\\\",\\\"rbind\\\",\\\"rdev\\\",\\\"rdiratime\\\",\\\"relatime\\\",\\\"remount\\\",\\\"rexec\\\",\\\"rnoatime\\\",\\\"rnodev\\\",\\\"rnodiratime\\\",\\\"rnoexec\\\",\\\"rnorelatime\\\",\\\"rnostrictatime\\\",\\\"rnosuid\\\",\\\"rnosymfollow\\\",\\\"ro\\\",\\\"rprivate\\\",\\\"rrelatime\\\",\\\"rro\\\",\\\"rrw\\\",\\\"rshared\\\",\\\"rslave\\\",\\\"rstrictatime\\\",\\\"rsuid\\\",\\\"rsymfollow\\\",\\\"runbindable\\\",\\\"rw\\\",\\\"shared\\\",\\\"silent\\\",\\\"slave\\\",\\\"strictatime\\\",\\\"suid\\\",\\\"symfollow\\\",\\\"sync\\\",\\\"tmpcopyup\\\",\\\"unbindable\\\"],\\\"linux\\\":{\\\"namespaces\\\":[\\\"cgroup\\\",\\\"ipc\\\",\\\"mount\\\",\\\"network\\\",\\\"pid\\\",\\\"time\\\",\\\"user\\\",\\\"uts\\\"],\\\"capabilities\\\":[\\\"CAP_CHOWN\\\",\\\"CAP_DAC_OVERRIDE\\\",\\\"CAP_DAC_READ_SEARCH\\\",\\\"CAP_FOWNER\\\",\\\"CAP_FSETID\\\",\\\"CAP_KILL\\\",\\\"CAP_SETGID\\\",\\\"CAP_SETUID\\\",\\\"CAP_SETPCAP\\\",\\\"CAP_LINUX_IMMUTABLE\\\",\\\"CAP_NET_BIND_SERVICE\\\",\\\"CAP_NET_BROADCAST\\\",\\\"CAP_NET_ADMIN\\\",\\\"CAP_NET_RAW\\\",\\\"CAP_IPC_LOCK\\\",\\\"CAP_IPC_OWNER\\\",\\\"CAP_SYS_MODULE\\\",\\\"CAP_SYS_RAWIO\\\",\\\"CAP_SYS_CHROOT\\\",\\\"CAP_SYS_PTRACE\\\",\\\"CAP_SYS_PACCT\\\",\\\"CAP_SYS_ADMIN\\\",\\\"CAP_SYS_BOOT\\\",\\\"CAP_SYS_NICE\\\",\\\"CAP_SYS_RESOURCE\\\",\\\"CAP_SYS_TIME\\\",\\\"CAP_SYS_TTY_CONFIG\\\",\\\"CAP_MKNOD\\\",\\\"CAP_LEASE\\\",\\\"CAP_AUDIT_WRITE\\\",\\\"CAP_AUDIT_CONTROL\\\",\\\"CAP_SETFCAP\\\",\\\"CAP_MAC_OVERRIDE\\\",\\\"CAP_MAC_ADMIN\\\",\\\"CAP_SYSLOG\\\",\\\"CAP_WAKE_ALARM\\\",\\\"CAP_BLOCK_SUSPEND\\\",\\\"CAP_AUDIT_READ\\\",\\\"CAP_PERFMON\\\",\\\"CAP_BPF\\\",\\\"CAP_CHECKPOINT_RESTORE\\\"],\\\"cgroup\\\":{\\\"v1\\\":true,\\\"v2\\\":true,\\\"systemd\\\":true,\\\"systemdUser\\\":true,\\\"rdma\\\":true},\\\"seccomp\\\":{\\\"enabled\\\":true,\\\"actions\\\":[\\\"SCMP_ACT_ALLOW\\\",\\\"SCMP_ACT_ERRNO\\\",\\\"SCMP_ACT_KILL\\\",\\\"SCMP_ACT_KILL_PROCESS\\\",\\\"SCMP_ACT_KILL_THREAD\\\",\\\"SCMP_ACT_LOG\\\",\\\"SCMP_ACT_NOTIFY\\\",\\\"SCMP_ACT_TRACE\\\",\\\"SCMP_ACT_TRAP\\\"],\\\"operators\\\":[\\\"SCMP_CMP_EQ\\\",\\\"SCMP_CMP_GE\\\",\\\"SCMP_CMP_GT\\\",\\\"SCMP_CMP_LE\\\",\\\"SCMP_CMP_LT\\\",\\\"SCMP_CMP_MASKED_EQ\\\",\\\"SCMP_CMP_NE\\\"],\\\"archs\\\":[\\\"SCMP_ARCH_AARCH64\\\",\\\"SCMP_ARCH_ARM\\\",\\\"SCMP_ARCH_MIPS\\\",\\\"SCMP_ARCH_MIPS64\\\",\\\"SCMP_ARCH_MIPS64N32\\\",\\\"SCMP_ARCH_MIPSEL\\\",\\\"SCMP_ARCH_MIPSEL64\\\",\\\"SCMP_ARCH_MIPSEL64N32\\\",\\\"SCMP_ARCH_PPC\\\",\\\"SCMP_ARCH_PPC64\\\",\\\"SCMP_ARCH_PPC64LE\\\",\\\"SCMP_ARCH_RISCV64\\\",\\\"SCMP_ARCH_S390\\\",\\\"SCMP_ARCH_S390X\\\",\\\"SCMP_ARCH_X32\\\",\\\"SCMP_ARCH_X86\\\",\\\"SCMP_ARCH_X86_64\\\"],\\\"knownFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"],\\\"supportedFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"]},\\\"apparmor\\\":{\\\"enabled\\\":true},\\\"selinux\\\":{\\\"enabled\\\":true},\\\"intelRdt\\\":{\\\"enabled\\\":true},\\\"mountExtensions\\\":{\\\"idmap\\\":{\\\"enabled\\\":true}}},\\\"annotations\\\":{\\\"io.github.seccomp.libseccomp.version\\\":\\\"2.5.3\\\",\\\"org.opencontainers.runc.checkpoint.enabled\\\":\\\"true\\\",\\\"org.opencontainers.runc.commit\\\":\\\"v1.2.5-0-g59923ef\\\",\\\"org.opencontainers.runc.version\\\":\\\"1.2.5\\\"},\\\"potentiallyUnsafeConfigAnnotations\\\":[\\\"bundle\\\",\\\"org.systemd.property.\\\",\\\"org.criu.config\\\"]}\"\n                    }\n                },\n                \"nvidia\": {\n                    \"path\": \"nvidia-container-runtime\",\n                    \"status\": {\n                        \"org.opencontainers.runtime-spec.features\": \"{\\\"ociVersionMin\\\":\\\"1.0.0\\\",\\\"ociVersionMax\\\":\\\"1.2.0\\\",\\\"hooks\\\":[\\\"prestart\\\",\\\"createRuntime\\\",\\\"createContainer\\\",\\\"startContainer\\\",\\\"poststart\\\",\\\"poststop\\\"],\\\"mountOptions\\\":[\\\"async\\\",\\\"atime\\\",\\\"bind\\\",\\\"defaults\\\",\\\"dev\\\",\\\"diratime\\\",\\\"dirsync\\\",\\\"exec\\\",\\\"iversion\\\",\\\"lazytime\\\",\\\"loud\\\",\\\"mand\\\",\\\"noatime\\\",\\\"nodev\\\",\\\"nodiratime\\\",\\\"noexec\\\",\\\"noiversion\\\",\\\"nolazytime\\\",\\\"nomand\\\",\\\"norelatime\\\",\\\"nostrictatime\\\",\\\"nosuid\\\",\\\"nosymfollow\\\",\\\"private\\\",\\\"ratime\\\",\\\"rbind\\\",\\\"rdev\\\",\\\"rdiratime\\\",\\\"relatime\\\",\\\"remount\\\",\\\"rexec\\\",\\\"rnoatime\\\",\\\"rnodev\\\",\\\"rnodiratime\\\",\\\"rnoexec\\\",\\\"rnorelatime\\\",\\\"rnostrictatime\\\",\\\"rnosuid\\\",\\\"rnosymfollow\\\",\\\"ro\\\",\\\"rprivate\\\",\\\"rrelatime\\\",\\\"rro\\\",\\\"rrw\\\",\\\"rshared\\\",\\\"rslave\\\",\\\"rstrictatime\\\",\\\"rsuid\\\",\\\"rsymfollow\\\",\\\"runbindable\\\",\\\"rw\\\",\\\"shared\\\",\\\"silent\\\",\\\"slave\\\",\\\"strictatime\\\",\\\"suid\\\",\\\"symfollow\\\",\\\"sync\\\",\\\"tmpcopyup\\\",\\\"unbindable\\\"],\\\"linux\\\":{\\\"namespaces\\\":[\\\"cgroup\\\",\\\"ipc\\\",\\\"mount\\\",\\\"network\\\",\\\"pid\\\",\\\"time\\\",\\\"user\\\",\\\"uts\\\"],\\\"capabilities\\\":[\\\"CAP_CHOWN\\\",\\\"CAP_DAC_OVERRIDE\\\",\\\"CAP_DAC_READ_SEARCH\\\",\\\"CAP_FOWNER\\\",\\\"CAP_FSETID\\\",\\\"CAP_KILL\\\",\\\"CAP_SETGID\\\",\\\"CAP_SETUID\\\",\\\"CAP_SETPCAP\\\",\\\"CAP_LINUX_IMMUTABLE\\\",\\\"CAP_NET_BIND_SERVICE\\\",\\\"CAP_NET_BROADCAST\\\",\\\"CAP_NET_ADMIN\\\",\\\"CAP_NET_RAW\\\",\\\"CAP_IPC_LOCK\\\",\\\"CAP_IPC_OWNER\\\",\\\"CAP_SYS_MODULE\\\",\\\"CAP_SYS_RAWIO\\\",\\\"CAP_SYS_CHROOT\\\",\\\"CAP_SYS_PTRACE\\\",\\\"CAP_SYS_PACCT\\\",\\\"CAP_SYS_ADMIN\\\",\\\"CAP_SYS_BOOT\\\",\\\"CAP_SYS_NICE\\\",\\\"CAP_SYS_RESOURCE\\\",\\\"CAP_SYS_TIME\\\",\\\"CAP_SYS_TTY_CONFIG\\\",\\\"CAP_MKNOD\\\",\\\"CAP_LEASE\\\",\\\"CAP_AUDIT_WRITE\\\",\\\"CAP_AUDIT_CONTROL\\\",\\\"CAP_SETFCAP\\\",\\\"CAP_MAC_OVERRIDE\\\",\\\"CAP_MAC_ADMIN\\\",\\\"CAP_SYSLOG\\\",\\\"CAP_WAKE_ALARM\\\",\\\"CAP_BLOCK_SUSPEND\\\",\\\"CAP_AUDIT_READ\\\",\\\"CAP_PERFMON\\\",\\\"CAP_BPF\\\",\\\"CAP_CHECKPOINT_RESTORE\\\"],\\\"cgroup\\\":{\\\"v1\\\":true,\\\"v2\\\":true,\\\"systemd\\\":true,\\\"systemdUser\\\":true,\\\"rdma\\\":true},\\\"seccomp\\\":{\\\"enabled\\\":true,\\\"actions\\\":[\\\"SCMP_ACT_ALLOW\\\",\\\"SCMP_ACT_ERRNO\\\",\\\"SCMP_ACT_KILL\\\",\\\"SCMP_ACT_KILL_PROCESS\\\",\\\"SCMP_ACT_KILL_THREAD\\\",\\\"SCMP_ACT_LOG\\\",\\\"SCMP_ACT_NOTIFY\\\",\\\"SCMP_ACT_TRACE\\\",\\\"SCMP_ACT_TRAP\\\"],\\\"operators\\\":[\\\"SCMP_CMP_EQ\\\",\\\"SCMP_CMP_GE\\\",\\\"SCMP_CMP_GT\\\",\\\"SCMP_CMP_LE\\\",\\\"SCMP_CMP_LT\\\",\\\"SCMP_CMP_MASKED_EQ\\\",\\\"SCMP_CMP_NE\\\"],\\\"archs\\\":[\\\"SCMP_ARCH_AARCH64\\\",\\\"SCMP_ARCH_ARM\\\",\\\"SCMP_ARCH_MIPS\\\",\\\"SCMP_ARCH_MIPS64\\\",\\\"SCMP_ARCH_MIPS64N32\\\",\\\"SCMP_ARCH_MIPSEL\\\",\\\"SCMP_ARCH_MIPSEL64\\\",\\\"SCMP_ARCH_MIPSEL64N32\\\",\\\"SCMP_ARCH_PPC\\\",\\\"SCMP_ARCH_PPC64\\\",\\\"SCMP_ARCH_PPC64LE\\\",\\\"SCMP_ARCH_RISCV64\\\",\\\"SCMP_ARCH_S390\\\",\\\"SCMP_ARCH_S390X\\\",\\\"SCMP_ARCH_X32\\\",\\\"SCMP_ARCH_X86\\\",\\\"SCMP_ARCH_X86_64\\\"],\\\"knownFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"],\\\"supportedFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"]},\\\"apparmor\\\":{\\\"enabled\\\":true},\\\"selinux\\\":{\\\"enabled\\\":true},\\\"intelRdt\\\":{\\\"enabled\\\":true},\\\"mountExtensions\\\":{\\\"idmap\\\":{\\\"enabled\\\":true}}},\\\"annotations\\\":{\\\"io.github.seccomp.libseccomp.version\\\":\\\"2.5.3\\\",\\\"org.opencontainers.runc.checkpoint.enabled\\\":\\\"true\\\",\\\"org.opencontainers.runc.commit\\\":\\\"v1.2.5-0-g59923ef\\\",\\\"org.opencontainers.runc.version\\\":\\\"1.2.5\\\"},\\\"potentiallyUnsafeConfigAnnotations\\\":[\\\"bundle\\\",\\\"org.systemd.property.\\\",\\\"org.criu.config\\\"]}\"\n                    }\n                },\n                \"runc\": {\n                    \"path\": \"runc\",\n                    \"status\": {\n                        \"org.opencontainers.runtime-spec.features\": \"{\\\"ociVersionMin\\\":\\\"1.0.0\\\",\\\"ociVersionMax\\\":\\\"1.2.0\\\",\\\"hooks\\\":[\\\"prestart\\\",\\\"createRuntime\\\",\\\"createContainer\\\",\\\"startContainer\\\",\\\"poststart\\\",\\\"poststop\\\"],\\\"mountOptions\\\":[\\\"async\\\",\\\"atime\\\",\\\"bind\\\",\\\"defaults\\\",\\\"dev\\\",\\\"diratime\\\",\\\"dirsync\\\",\\\"exec\\\",\\\"iversion\\\",\\\"lazytime\\\",\\\"loud\\\",\\\"mand\\\",\\\"noatime\\\",\\\"nodev\\\",\\\"nodiratime\\\",\\\"noexec\\\",\\\"noiversion\\\",\\\"nolazytime\\\",\\\"nomand\\\",\\\"norelatime\\\",\\\"nostrictatime\\\",\\\"nosuid\\\",\\\"nosymfollow\\\",\\\"private\\\",\\\"ratime\\\",\\\"rbind\\\",\\\"rdev\\\",\\\"rdiratime\\\",\\\"relatime\\\",\\\"remount\\\",\\\"rexec\\\",\\\"rnoatime\\\",\\\"rnodev\\\",\\\"rnodiratime\\\",\\\"rnoexec\\\",\\\"rnorelatime\\\",\\\"rnostrictatime\\\",\\\"rnosuid\\\",\\\"rnosymfollow\\\",\\\"ro\\\",\\\"rprivate\\\",\\\"rrelatime\\\",\\\"rro\\\",\\\"rrw\\\",\\\"rshared\\\",\\\"rslave\\\",\\\"rstrictatime\\\",\\\"rsuid\\\",\\\"rsymfollow\\\",\\\"runbindable\\\",\\\"rw\\\",\\\"shared\\\",\\\"silent\\\",\\\"slave\\\",\\\"strictatime\\\",\\\"suid\\\",\\\"symfollow\\\",\\\"sync\\\",\\\"tmpcopyup\\\",\\\"unbindable\\\"],\\\"linux\\\":{\\\"namespaces\\\":[\\\"cgroup\\\",\\\"ipc\\\",\\\"mount\\\",\\\"network\\\",\\\"pid\\\",\\\"time\\\",\\\"user\\\",\\\"uts\\\"],\\\"capabilities\\\":[\\\"CAP_CHOWN\\\",\\\"CAP_DAC_OVERRIDE\\\",\\\"CAP_DAC_READ_SEARCH\\\",\\\"CAP_FOWNER\\\",\\\"CAP_FSETID\\\",\\\"CAP_KILL\\\",\\\"CAP_SETGID\\\",\\\"CAP_SETUID\\\",\\\"CAP_SETPCAP\\\",\\\"CAP_LINUX_IMMUTABLE\\\",\\\"CAP_NET_BIND_SERVICE\\\",\\\"CAP_NET_BROADCAST\\\",\\\"CAP_NET_ADMIN\\\",\\\"CAP_NET_RAW\\\",\\\"CAP_IPC_LOCK\\\",\\\"CAP_IPC_OWNER\\\",\\\"CAP_SYS_MODULE\\\",\\\"CAP_SYS_RAWIO\\\",\\\"CAP_SYS_CHROOT\\\",\\\"CAP_SYS_PTRACE\\\",\\\"CAP_SYS_PACCT\\\",\\\"CAP_SYS_ADMIN\\\",\\\"CAP_SYS_BOOT\\\",\\\"CAP_SYS_NICE\\\",\\\"CAP_SYS_RESOURCE\\\",\\\"CAP_SYS_TIME\\\",\\\"CAP_SYS_TTY_CONFIG\\\",\\\"CAP_MKNOD\\\",\\\"CAP_LEASE\\\",\\\"CAP_AUDIT_WRITE\\\",\\\"CAP_AUDIT_CONTROL\\\",\\\"CAP_SETFCAP\\\",\\\"CAP_MAC_OVERRIDE\\\",\\\"CAP_MAC_ADMIN\\\",\\\"CAP_SYSLOG\\\",\\\"CAP_WAKE_ALARM\\\",\\\"CAP_BLOCK_SUSPEND\\\",\\\"CAP_AUDIT_READ\\\",\\\"CAP_PERFMON\\\",\\\"CAP_BPF\\\",\\\"CAP_CHECKPOINT_RESTORE\\\"],\\\"cgroup\\\":{\\\"v1\\\":true,\\\"v2\\\":true,\\\"systemd\\\":true,\\\"systemdUser\\\":true,\\\"rdma\\\":true},\\\"seccomp\\\":{\\\"enabled\\\":true,\\\"actions\\\":[\\\"SCMP_ACT_ALLOW\\\",\\\"SCMP_ACT_ERRNO\\\",\\\"SCMP_ACT_KILL\\\",\\\"SCMP_ACT_KILL_PROCESS\\\",\\\"SCMP_ACT_KILL_THREAD\\\",\\\"SCMP_ACT_LOG\\\",\\\"SCMP_ACT_NOTIFY\\\",\\\"SCMP_ACT_TRACE\\\",\\\"SCMP_ACT_TRAP\\\"],\\\"operators\\\":[\\\"SCMP_CMP_EQ\\\",\\\"SCMP_CMP_GE\\\",\\\"SCMP_CMP_GT\\\",\\\"SCMP_CMP_LE\\\",\\\"SCMP_CMP_LT\\\",\\\"SCMP_CMP_MASKED_EQ\\\",\\\"SCMP_CMP_NE\\\"],\\\"archs\\\":[\\\"SCMP_ARCH_AARCH64\\\",\\\"SCMP_ARCH_ARM\\\",\\\"SCMP_ARCH_MIPS\\\",\\\"SCMP_ARCH_MIPS64\\\",\\\"SCMP_ARCH_MIPS64N32\\\",\\\"SCMP_ARCH_MIPSEL\\\",\\\"SCMP_ARCH_MIPSEL64\\\",\\\"SCMP_ARCH_MIPSEL64N32\\\",\\\"SCMP_ARCH_PPC\\\",\\\"SCMP_ARCH_PPC64\\\",\\\"SCMP_ARCH_PPC64LE\\\",\\\"SCMP_ARCH_RISCV64\\\",\\\"SCMP_ARCH_S390\\\",\\\"SCMP_ARCH_S390X\\\",\\\"SCMP_ARCH_X32\\\",\\\"SCMP_ARCH_X86\\\",\\\"SCMP_ARCH_X86_64\\\"],\\\"knownFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"],\\\"supportedFlags\\\":[\\\"SECCOMP_FILTER_FLAG_TSYNC\\\",\\\"SECCOMP_FILTER_FLAG_SPEC_ALLOW\\\",\\\"SECCOMP_FILTER_FLAG_LOG\\\"]},\\\"apparmor\\\":{\\\"enabled\\\":true},\\\"selinux\\\":{\\\"enabled\\\":true},\\\"intelRdt\\\":{\\\"enabled\\\":true},\\\"mountExtensions\\\":{\\\"idmap\\\":{\\\"enabled\\\":true}}},\\\"annotations\\\":{\\\"io.github.seccomp.libseccomp.version\\\":\\\"2.5.3\\\",\\\"org.opencontainers.runc.checkpoint.enabled\\\":\\\"true\\\",\\\"org.opencontainers.runc.commit\\\":\\\"v1.2.5-0-g59923ef\\\",\\\"org.opencontainers.runc.version\\\":\\\"1.2.5\\\"},\\\"potentiallyUnsafeConfigAnnotations\\\":[\\\"bundle\\\",\\\"org.systemd.property.\\\",\\\"org.criu.config\\\"]}\"\n                    }\n                }\n            },\n            \"SecurityOptions\": [\n                \"name=apparmor\",\n                \"name=seccomp,profile=builtin\",\n                \"name=cgroupns\"\n            ],\n            \"ServerVersion\": \"28.1.0\",\n            \"SwapLimit\": true,\n            \"Swarm\": {\n                \"ControlAvailable\": false,\n                \"Error\": \"\",\n                \"LocalNodeState\": \"inactive\",\n                \"NodeAddr\": \"\",\n                \"NodeID\": \"\",\n                \"RemoteManagers\": null\n            },\n            \"SystemTime\": \"2025-04-21T09:16:46.197445869-04:00\",\n            \"Warnings\": null\n        },\n        \"Name\": \"docker\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/ed/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nAdditional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",
      "state": "closed",
      "author": "edmcman",
      "author_type": "User",
      "created_at": "2025-04-21T13:18:02Z",
      "updated_at": "2025-04-25T02:58:23Z",
      "closed_at": "2025-04-25T02:58:22Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 18,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1234/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1234",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1234",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:49.249110",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Could you try with an older cuda image?  Are you running on an older OS?",
          "created_at": "2025-04-21T14:24:38Z"
        },
        {
          "author": "edmcman",
          "body": "Sure, I'm trying now.  I see old tag names at https://quay.io/repository/ramalama/ramalama?tab=history and can run them with `ramalama --image path:tag ...`",
          "created_at": "2025-04-21T14:31:12Z"
        },
        {
          "author": "edmcman",
          "body": "Also it's Ubuntu 22.04.5 LTS.",
          "created_at": "2025-04-21T14:50:04Z"
        },
        {
          "author": "rhatdan",
          "body": "I guess we could start to push two images,  And make RamaLama smart enough about which one is on the local system to pull the older one.\n",
          "created_at": "2025-04-21T14:56:30Z"
        },
        {
          "author": "edmcman",
          "body": "I wasn't able to get the older tags to work (though a lot of them seem to be deleted).\n\nI thought nvidia runtime / #953 was supposed to fix this problem?",
          "created_at": "2025-04-21T15:18:04Z"
        }
      ]
    },
    {
      "issue_number": 1260,
      "title": "Using `ramalama run --rag` makes the model unable to answer things outside the RAG",
      "body": "### Issue Description\n\nWhen running with a RAG,\n1. The prompt changes (instead of the seal emoji, it's just `>`)\n2. The model becomes very slow\n3. But more importantly, the model seems to know nothing about things outside the RAG\n\n### Steps to reproduce the issue\n\nWithout RAG:\n\n```\n$ ramalama run qwen2.5-coder:14b\nðŸ¦­ > how do I reverse a list in Python?\nIn Python, you can reverse a list in several ways. Here are three common methods:\n\n...\n```\n\nWith RAG:\n\n```\n$ echo \"Jonathan's favourite food is pizza.\" > out.md\n$ ramalama --image quay.io/ramalama/ramalama-rag rag out.md localhost/myrag:0.1\n\nBuilding localhost/myrag:0.1...\nadding vectordb...\n138bd1bdce699c0e73dd52603b409762a680da4d9372a733ddfa84060e78fabb\n$ ramalama run --rag localhost/myrag:0.1 qwen2.5-coder:14b\n> What's Jonathan's favourite food?\nPizza\n> how do I reverse a list in Python?\n!['I don't know']\n```\n\n### Describe the results you received\n\nIt doesn't know how to answer questions it previously did when run without the RAG.\n\n### Describe the results you expected\n\nIt knows how to answer questions it previously did when run without the RAG.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.2\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 98.76,\n                    \"systemPercent\": 0.27,\n                    \"userPercent\": 0.98\n                },\n                \"cpus\": 16,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"silverblue\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2043,\n                \"hostname\": \"flux\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 100000,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.13.9-200.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 13267943424,\n                \"memTotal\": 67100880896,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.20-2.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.20\\ncommit: 9c9a76ac11994701dd666c4f0b869ceffb599a66\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc41.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-1.fc41.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 5491326976,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"201h 32m 52.00s (Approximately 8.38 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/var/home/jlebon/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 3,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 2\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/var/home/jlebon/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1022488477696,\n                \"graphRootUsed\": 908609159168,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 263\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/var/home/jlebon/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.1\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1741651200,\n                \"BuiltTime\": \"Mon Mar 10 20:00:00 2025\",\n                \"GitCommit\": \"b79bc8afe796cba51dd906270a7e1056ccdfcf9e\",\n                \"GoVersion\": \"go1.23.7\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/var/home/jlebon/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n```\n\n### Upstream Latest Release\n\nNo\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\nUnrelated: you need to update https://github.com/containers/ramalama/blob/e1f8fb8b6bc808158c952146fe739ef802953c02/.github/ISSUE_TEMPLATE/bug_report.yaml?plain=1#L50.",
      "state": "open",
      "author": "jlebon",
      "author_type": "User",
      "created_at": "2025-04-23T22:15:33Z",
      "updated_at": "2025-04-24T04:28:59Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1260/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1260",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1260",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:49.497335",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Well noticed \"rag\" and \"run\" prompts are different @bmahabirbu has got this on his radar... @bmahabirbu is well equipped to comment on a lot of these queries ",
          "created_at": "2025-04-23T22:50:52Z"
        },
        {
          "author": "bmahabirbu",
          "body": "Hi @jlebon thanks for the questions hopefully my explanation's help!\n\n1. The difference here is that under the hood RAG has to query the model serve, the rag database, and put an interactive shell on top of this. We're working on a feature that will address these differences by intercepting the llam",
          "created_at": "2025-04-24T04:28:58Z"
        }
      ]
    },
    {
      "issue_number": 1244,
      "title": "ramalama rag fails with non-lowercase model names very late without clear recovery option",
      "body": "### Issue Description\n\n`ramalama  rag Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf localhost/RHEL9_v2networking-rag` fails after a lot of processing with\n\n`Error: tag localhost/RHEL9_v2networking-rag: invalid reference format: repository name must be lowercase\nError: Command '['podman', 'build', '--no-cache', '--network=none', '-q', '-t', 'localhost/RHEL9_v2networking-rag', '-f', '/home/till/RamaLama_rag_exj7fczq/vectordb/tmpv6phj0f2', '/home/till/RamaLama_rag_exj7fczq']' returned non-zero exit status 125.`\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1.  wget https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/pdf/configuring_and_managing_networking/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf\n2. ramalama  rag Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf localhost/RHEL9_v2networking-rag\n3.\n\n\n### Describe the results you received\n\n`Error: tag localhost/RHEL9_v2networking-rag: invalid reference format: repository name must be lowercase\nError: Command '['podman', 'build', '--no-cache', '--network=none', '-q', '-t', 'localhost/RHEL9_v2networking-rag', '-f', '/home/till/RamaLama_rag_exj7fczq/vectordb/tmpv6phj0f2', '/home/till/RamaLama_rag_exj7fczq']' returned non-zero exit status 125.\n`\n\n### Describe the results you expected\n\n* Successful tagging\n* or ramalama converting the tag name to lowercase and telling me\n* or ramalama asking for an alternative tag name to recover from this and keep the result in a different tag \n* or failing early with the error that the tag needs to be lowercase.\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.61,\n                    \"systemPercent\": 0.23,\n                    \"userPercent\": 0.16\n                },\n                \"cpus\": 24,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2042,\n                \"hostname\": \"genius\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.2-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 89296375808,\n                \"memTotal\": 134821318656,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250415.g2340bbf-1.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 6955978752,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"144h 26m 35.00s (Approximately 6.00 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/till/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 5,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 5\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/till/.local/share/containers/storage\",\n                \"graphRootAllocated\": 536608768000,\n                \"graphRootUsed\": 162747043840,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 31\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/till/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Wed Apr  2 02:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/till/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.5\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "tyll",
      "author_type": "User",
      "created_at": "2025-04-22T20:25:40Z",
      "updated_at": "2025-04-23T12:58:08Z",
      "closed_at": "2025-04-23T12:58:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1244/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1244",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1244",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:49.719227",
      "comments": [
        {
          "author": "rhatdan",
          "body": "I think the best thing would be process the name first and throw an error.  What is happening is the docling->vector.db is finished, and we are doing a podman build with the tag, and that is blowing up.",
          "created_at": "2025-04-23T11:15:20Z"
        }
      ]
    },
    {
      "issue_number": 1251,
      "title": "intel-gpu:0.7.5 seems to be missing awk",
      "body": "### Issue Description\n\nOutput from starting ramalama with 0.7.5 intel-gpu image:\n\n```\napr 23 13:01:45 kuroi.thuisnet.com systemd[1397]: Started ramalama.service.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507679]: ad33c5eeee204e67b592e630a3d2ae478d8ed8d4adf68fc4f3e210281ce96983\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: /opt/intel/oneapi/setvars.sh: line 514: awk: command not found\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: /opt/intel/oneapi/setvars.sh: line 222: awk: command not found\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: /opt/intel/oneapi/setvars.sh: line 222: awk: command not found\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: /opt/intel/oneapi/setvars.sh: line 652: awk: command not found\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: :: initializing oneAPI environment ...\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:    entrypoint.sh: BASH_VERSION = 5.2.37(1)-release\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:    args: Using \"$@\" for setvars.sh arguments: llama-server --port 9999 --model /mnt/models/model.file --alias granite3->\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: /opt/intel/oneapi/setvars.sh: line 222: awk: command not found\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: :: WARNING: No env scripts found: No \"env/vars.sh\" scripts to process.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:    This can be caused by a bad or incomplete \"--config\" file.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:    Can also be caused by an incomplete or missing oneAPI installation.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:    Can also be caused by redefining 'cd' via an alias or function.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: usage: source setvars.sh [--force] [--config=file] [--help] [...]\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   --force        Force setvars.sh to re-run, doing so may overload environment.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   --config=file  Customize env vars using a setvars.sh configuration file.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   --help         Display this help message and exit.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   ...            Additional args are passed to individual env/vars.sh scripts\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:                  and should follow this script's arguments.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   Some POSIX shells do not accept command-line options. In that case, you can pass\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   command-line options via the SETVARS_ARGS environment variable. For example:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   $ SETVARS_ARGS=\"--config=config.txt\" ; export SETVARS_ARGS\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   $ . path/to/setvars.sh\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   The SETVARS_ARGS environment variable is cleared on exiting setvars.sh.\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: The oneAPI toolkits no longer support 32-bit libraries, starting with the 2025.0 toolkit release. See the oneAPI releas>\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]: terminate called after throwing an instance of 'sycl::_V1::exception'\napr 23 13:01:45 kuroi.thuisnet.com ramalama[1507699]:   what():  No device of requested type available. Please check https://software.intel.com/content/www/us/en/develop/art>\napr 23 13:01:46 kuroi.thuisnet.com podman[1507815]: 2025-04-23 13:01:46.333971152 +0200 CEST m=+0.041094300 container died ad33c5eeee204e67b592e630a3d2ae478d8ed8d4adf68fc4f3>\napr 23 13:01:46 kuroi.thuisnet.com podman[1507815]: 2025-04-23 13:01:46.375641522 +0200 CEST m=+0.082764609 container remove ad33c5eeee204e67b592e630a3d2ae478d8ed8d4adf68fc4>\napr 23 13:01:46 kuroi.thuisnet.com systemd[1397]: ramalama.service: Main process exited, code=exited, status=139/n/a\napr 23 13:01:46 kuroi.thuisnet.com systemd[1397]: ramalama.service: Failed with result 'exit-code'.\n```\n\nMy quadlet is the same as for 0.7.4, which starts fine. The awk command does seem to miss from the image.\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. Run 0.7.4, change quadlet to use 0.7.5 image\n2. Daemon-reload\n3. Restart ramalama\n\n\n### Describe the results you received\n\nError as above\n\n### Describe the results you expected\n\nNicely starting model in ramalama \n\n### ramalama info output\n\n```yaml\nIf you are unable to run ramalama info for any reason, please provide the ramalama version, operating system and its version and the architecture you are running.\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nAdditional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",
      "state": "closed",
      "author": "wzzrd",
      "author_type": "User",
      "created_at": "2025-04-23T11:04:11Z",
      "updated_at": "2025-04-23T12:06:48Z",
      "closed_at": "2025-04-23T12:06:48Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1251/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1251",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1251",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:49.937091",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "https://github.com/containers/ramalama/pull/1252",
          "created_at": "2025-04-23T11:34:22Z"
        }
      ]
    },
    {
      "issue_number": 1221,
      "title": "ramalama rag fails with RHEL 9 networking guide PDF",
      "body": "### Issue Description\n\nramalama rag Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf localhost/RHEL9networking-rag fails without actionable information\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. wget https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/pdf/configuring_and_managing_network\ning/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf\n2. ramalama rag Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf localhost/RHEL9networki\nng-rag \n3.\n\n\n### Describe the results you received\n\nDownloading detection model, please wait. This may take several minutes depending upon your network connection.\nDownloading recognition model, please wait. This may take several minutes depending upon your network connection.\n/usr/bin/doc2rag:46: DeprecationWarning: Use contextualize() instead.\n  doc_text = chunker.serialize(chunk=chunk)\nError: Command '['podman', 'run', '--rm', '-v', '/home/till/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf:/docs//home/till/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf:ro,z', '-v', '/home/till/RamaLama_rag_j3ndvtft/vectordb:/output:z', '--device', 'nvidia.com/gpu=all', '-e', 'CUDA_VISIBLE_DEVICES=0', 'quay.io/ramalama/cuda-rag:0.7', 'doc2rag', '/output', '/docs/']' returned non-zero exit status 137.\n\nThe final error is also only shown after hitting the enter key. So it first seems that something happens for a long time while it actually just seems to be waiting for input.\n\n### Describe the results you expected\n\nSuccessfully built rag. It worked with this file: https://github.com/nmstate/nmstate.github.io/blob/main/examples.md\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.43,\n                    \"systemPercent\": 0.23,\n                    \"userPercent\": 0.34\n                },\n                \"cpus\": 24,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2042,\n                \"hostname\": \"genius\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.2-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 131237969920,\n                \"memTotal\": 134821318656,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 6406365184,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"25h 57m 0.00s (Approximately 1.04 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/till/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 5,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 5\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/till/.local/share/containers/storage\",\n                \"graphRootAllocated\": 536608768000,\n                \"graphRootUsed\": 149010317312,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 28\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/till/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Wed Apr  2 02:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/till/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n```\n\n### Upstream Latest Release\n\nNo\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nRun with debug output:\n\n```\n$ ramalama --debug rag Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf localhost/RHEL9networking-rag\nrun_cmd:  podman inspect quay.io/ramalama/cuda-rag:0.7\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nrun_cmd:  podman run --rm -v /home/till/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf:/docs//home/till/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf:ro,z -v /home/till/RamaLama_rag_rcq_q9st/vectordb:/output:z --device nvidia.com/gpu=all -e CUDA_VISIBLE_DEVICES=0 quay.io/ramalama/cuda-rag:0.7 doc2rag /output /docs/\nWorking directory: None\nIgnore stderr: False\nIgnore all: False\nDownloading detection model, please wait. This may take several minutes depending upon your network connection.\nDownloading recognition model, please wait. This may take several minutes depending upon your network connection.\n/usr/bin/doc2rag:46: DeprecationWarning: Use contextualize() instead.\n  doc_text = chunker.serialize(chunk=chunk)\nError: Command '['podman', 'run', '--rm', '-v', '/home/till/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf:/docs//home/till/Red_Hat_Enterprise_Linux-9-Configuring_and_managing_networking-en-US.pdf:ro,z', '-v', '/home/till/RamaLama_rag_rcq_q9st/vectordb:/output:z', '--device', 'nvidia.com/gpu=all', '-e', 'CUDA_VISIBLE_DEVICES=0', 'quay.io/ramalama/cuda-rag:0.7', 'doc2rag', '/output', '/docs/']' returned non-zero exit status 137.\n```",
      "state": "closed",
      "author": "tyll",
      "author_type": "User",
      "created_at": "2025-04-17T21:55:42Z",
      "updated_at": "2025-04-22T20:18:53Z",
      "closed_at": "2025-04-19T11:11:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1221/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1221",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1221",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:50.195440",
      "comments": [
        {
          "author": "iimsand",
          "body": "I had the same error with the pdf from here https://arxiv.org/abs/2412.05299 (https://arxiv.org/pdf/2412.05299).\n\nAfter some investigation and this issue https://github.com/qdrant/qdrant/issues/4532, I found out that the Memory was filled during the execution of this line: https://github.com/contain",
          "created_at": "2025-04-18T12:52:54Z"
        },
        {
          "author": "rhatdan",
          "body": "@bmahabirbu PTAL",
          "created_at": "2025-04-18T14:42:16Z"
        },
        {
          "author": "rhatdan",
          "body": "Google exit code 137\n```\nWhat exit code 137 means for Kubernetesâ€‹ Exit code 137 is a signal that occurs when a container's memory exceeds the memory limit provided in the pod specification. When a container consumes too much memory, Kubernetes kills it to protect it from consuming too many resources",
          "created_at": "2025-04-18T15:27:13Z"
        },
        {
          "author": "bmahabirbu",
          "body": "Thank you @iimsand for the detailed investigation. Qdrant uses RAM to load the vector database for queries for faster retrieval. There is an option for qdrant to use the on-disk storage rather than RAM. Ill investigate those optimizations you linked and see how to add them!\n\n",
          "created_at": "2025-04-18T15:36:09Z"
        },
        {
          "author": "bmahabirbu",
          "body": "https://python-client.qdrant.tech/qdrant_client.qdrant_fastembed\n\naccording to this under get_fastembed_sparse_vector_params I can specifiy on_disk! Ill try this out and see if it works.\n\nI'm using the qdrant fastembed mixin so there is some differences between the regular qdrant commands",
          "created_at": "2025-04-18T15:58:12Z"
        }
      ]
    },
    {
      "issue_number": 1212,
      "title": "ramalama does not work with granite instruct model",
      "body": "### Issue Description\n\nTrying to run ramalama with granite instruct fails.\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. ramalama run hf://ibm-granite/granite-8b-code-instruct-4k-GGUF/granite-8b-code-instruct.Q4_K_M.gguf\n2. write `hello` into the prompt\n\n\n### Describe the results you received\n\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  this custom template is not supported\n\n\n### Describe the results you expected\n\nI should be able to interact with the model or get an actionable error message. \n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"cuda\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.5,\n                    \"systemPercent\": 0.18,\n                    \"userPercent\": 0.31\n                },\n                \"cpus\": 24,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2041,\n                \"hostname\": \"genius\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.2-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 87334432768,\n                \"memTotal\": 134821318656,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-2.fc42.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"14h 50m 32.00s (Approximately 0.58 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/till/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 6,\n                    \"paused\": 0,\n                    \"running\": 1,\n                    \"stopped\": 5\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/till/.local/share/containers/storage\",\n                \"graphRootAllocated\": 536608768000,\n                \"graphRootUsed\": 125268328448,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 26\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/till/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Wed Apr  2 02:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/cuda:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/till/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n```\n\n### Upstream Latest Release\n\nNo\n\n### Additional environment details\n\n_No response_\n\n### Additional information\n\n_No response_",
      "state": "closed",
      "author": "tyll",
      "author_type": "User",
      "created_at": "2025-04-17T10:49:21Z",
      "updated_at": "2025-04-22T15:18:54Z",
      "closed_at": "2025-04-22T15:18:54Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1212/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1212",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1212",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:50.392471",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Do other models work on your system?\n\n",
          "created_at": "2025-04-17T12:25:13Z"
        },
        {
          "author": "tyll",
          "body": "yes, I tried granite granite-code:8b deepseek so far.",
          "created_at": "2025-04-17T19:07:58Z"
        },
        {
          "author": "afazekas",
          "body": "I seen similar error message with llama-cpp with another model, the --jinja flag solved it.",
          "created_at": "2025-04-18T05:40:53Z"
        },
        {
          "author": "tyll",
          "body": "Thanks, `ramalama  run --runtime-args=\"--jinja\" hf://ibm-granite/granite-8b-code-instruct-4k-GGUF/granite-8b-code-instruct.Q4_K_M.gguf` works. Would be great if ramalama detected it by itself.",
          "created_at": "2025-04-22T12:39:16Z"
        },
        {
          "author": "afazekas",
          "body": "ramalama  serve  hf://ibm-granite/granite-8b-code-instruct-4k-GGUF/granite-8b-code-instruct.Q4_K_M.gguf  # always using it .\nperhaps run should do it too.",
          "created_at": "2025-04-22T13:41:03Z"
        }
      ]
    },
    {
      "issue_number": 643,
      "title": "Login to huggingface with a token asks for username and password",
      "body": "I would like to use a model from huggingface. Since the model is hidden behind authentication, I follow [examples section of Documentation for ramalama login](https://github.com/containers/ramalama/blob/main/docs/ramalama-login.1.md#examples) and run:\n\n```\n$ ramalama login --token=\"${TOKEN}\" huggingface\n```\n\nbut instead of logging me in, I'm being asked for `Username: `.\n\n---\n\nI have found a [trivial typo](https://github.com/containers/ramalama/blob/3e90238d9cafb8e4d29eaed45a4e3b8ae390fb01/ramalama/cli.py#L282) on this line:\n\n```\nif registry in [\"ollama\", \"hf\" \"huggingface\"]:\n```\n\nwhich python interprets as:\n\n```\n>>> repr([\"ollama\", \"hf\" \"huggingface\"])\n\"['ollama', 'hfhuggingface']\"\n```\n\nso I assume it should be:\n\n```\nif registry in [\"ollama\", \"hf\", \"huggingface\"]:\n```\n\n---\n\nThis is not enough though, because the registry from the command line (`huggingface`) is not [one of the expected prefixes in factory function New()](https://github.com/containers/ramalama/blob/3e90238d9cafb8e4d29eaed45a4e3b8ae390fb01/ramalama/cli.py#L859). I'm not sure how to fix this properly, but temporary workaround with:\n\n```\nif model.startswith(\"huggingface\") or model.startswith(\"huggingface://\") or model.startswith(\"hf://\") or model.startswith(\"hf.co/\"):\n```\n\nled me to an error:\n\n```\nError: huggingface-cli not available, skipping login.\n```\n\nI haven't seen a requirement of this CLI to be installed on the system anywhere in the documentation. I _guess_ best place would be in the examples section of `ramalama-login`, either using the command (`pip install -U \"huggingface_hub[cli]\"`) or linking to https://huggingface.co/docs/huggingface_hub/en/guides/cli where I found it.\n\n---\n\nAfter all of this I was able to login.",
      "state": "open",
      "author": "pbabinca",
      "author_type": "User",
      "created_at": "2025-01-28T13:13:06Z",
      "updated_at": "2025-04-21T07:27:04Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/643/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/643",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/643",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:50.600757",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "I think the best fix is to implement ramalama login in python3 from scratch it shouldn't be too bad, I've seen at least 3 implementations of it.",
          "created_at": "2025-01-28T14:00:48Z"
        },
        {
          "author": "ericcurtin",
          "body": "But yes a documentation addition to install huggingface would be enough to close this issue for now. It's deliberately not a strong dependancy because it can be hard to install it on some platforms.\n\nWould you like to open a PR @pbabinca with a documentation update?\n\nIf you are really interested in ",
          "created_at": "2025-01-28T14:03:20Z"
        },
        {
          "author": "pbabinca",
          "body": "> Would you like to open a PR [@pbabinca](https://github.com/pbabinca) with a documentation update?\n\nOpened #645 which should solve the last issue.\n\n---\n\nThe other two issues, though, are not resolved yet. I'm not sure if test of the prefix `huggingface` on the top/instead of the \"scheme\" of URL is ",
          "created_at": "2025-01-28T16:05:59Z"
        },
        {
          "author": "melodyliu1986",
          "body": "I received the mail \"Opportunities to Contribute to RamaLama AI Project\", and saw these \"good first issues\", let me fix one minor bug before going deeper\n\n[Fix bug in login_cli and update huggingface or hf registry behavior #1232](https://github.com/containers/ramalama/pull/1232)",
          "created_at": "2025-04-21T07:27:02Z"
        }
      ]
    },
    {
      "issue_number": 1222,
      "title": "rocm-ubi container build  fails around dnf remove gcc-c++",
      "body": "### Issue Description\n\n```\ndnf remove -y python3-devel libcurl-devel gcc gcc-c++ make cmake findutils\nUpdating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nError: \n Problem: The operation would result in removing the following protected packages: sudo\n(try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages)\n```\nsudo was not on the base image installed in the meantime,\nSomehow the above command leads to trying to uninstall it\n\nKnown workaround, skipping the dnf remove step.\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. container_build.sh build\n\n\n\n### Describe the results you received\n\nDescribe the results you received\n\n### Describe the results you expected\n\nAll image builds on\n\n>  ./container_build.sh build\n\n### ramalama info output\n\n```yaml\n-- Installing: /usr/include/llama.h\n-- Installing: /usr/include/llama-cpp.h\n-- Installing: /usr/lib64/cmake/llama/llama-config.cmake\n-- Installing: /usr/lib64/cmake/llama/llama-version.cmake\n-- Installing: /usr/bin/convert_hf_to_gguf.py\n-- Installing: /usr/lib64/pkgconfig/llama.pc\n+ cd ..\n+ rm -rf llama.cpp\n+ available dnf\n+ command -v dnf\n+ dnf_remove\n+ dnf remove -y python3-devel libcurl-devel gcc gcc-c++ make cmake findutils\nUpdating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nError: \n Problem: The operation would result in removing the following protected packages: sudo\n(try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages)\nError: building at STEP \"RUN build_llama_and_whisper.sh \"rocm\"\": while running runtime: exit status 1\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nf41 host\n\n### Additional information\n\nThe other llama containers buils",
      "state": "closed",
      "author": "afazekas",
      "author_type": "User",
      "created_at": "2025-04-18T05:14:34Z",
      "updated_at": "2025-04-20T10:26:31Z",
      "closed_at": "2025-04-20T10:26:30Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1222/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1222",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1222",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:50.827313",
      "comments": [
        {
          "author": "afazekas",
          "body": "asahi-rag also can fail to build without any cc,\ngcc can be removed implicitly on \n\n```\n+ dnf remove -y python3-devel libcurl-devel make cmake findutils\nAfter this operation, 301 MiB will be freed (install 0 B, remove 301 MiB).\nRunning transaction\nPackage               Arch   Version          Reposi",
          "created_at": "2025-04-18T11:22:48Z"
        },
        {
          "author": "ericcurtin",
          "body": "@afazekas feel free to open a PR reducing the things removed in the `dnf remove` line. Our images are pretty huge, removing these things only make a small difference anyway.",
          "created_at": "2025-04-18T11:42:28Z"
        },
        {
          "author": "afazekas",
          "body": "I'll will C what else is needed, some recent pip dep changes might also lead to we need to install few packages from:\n\n> dnf install -y python3-sentencepiece sentencepiece-libs sentencepiece-devel",
          "created_at": "2025-04-18T12:21:03Z"
        },
        {
          "author": "ericcurtin",
          "body": "I C what you did there",
          "created_at": "2025-04-18T18:07:57Z"
        }
      ]
    },
    {
      "issue_number": 1205,
      "title": "Ramalama build issue",
      "body": "### Issue Description\n\n``` Successfully built ramalama\nInstalling collected packages: ramalama\nSuccessfully installed ramalama-0.7.4\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n+ cd ..\n+ rm -rf ramalama\nrm: cannot remove 'ramalama/.git/config': Stale file handle\nError: building at STEP \"RUN build_llama_and_whisper.sh \"ramalama\"\": while running runtime: exit status 1\nbrian@DESKTOP-SB69448:~/ramalama$ ./container_build.sh build ramalama\n```\n\nRunning into a build issue with Ramalama building the main container. Seems it has something to do with deleting ramalama after cloning inside the container \n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. ./container_build build ramalama\n\n\n### Describe the results you received\n\nDescribe the results you received\n\n### Describe the results you expected\n\nDescribe the results you expected\n\n### ramalama info output\n\n```yaml\nIf you are unable to run ramalama info for any reason, please provide the ramalama version, operating system and its version and the architecture you are running.\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nAdditional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",
      "state": "closed",
      "author": "bmahabirbu",
      "author_type": "User",
      "created_at": "2025-04-16T20:20:40Z",
      "updated_at": "2025-04-18T01:50:20Z",
      "closed_at": "2025-04-17T16:09:16Z",
      "labels": [
        "help wanted"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1205/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1205",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1205",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:51.042668",
      "comments": [
        {
          "author": "bmahabirbu",
          "body": "Wondering if its just on my end or @rhatdan if you experienced this? ",
          "created_at": "2025-04-16T20:22:04Z"
        },
        {
          "author": "rhatdan",
          "body": "./container_build.sh build ramalama\n\nSeems to be working for me.\n\nI usually do `make build IMAGE=ramalama` for the builds, but this calls into the build_container.sh command.",
          "created_at": "2025-04-17T12:21:40Z"
        },
        {
          "author": "bmahabirbu",
          "body": "Thank you ill give it a try! Glad it works for you then it's definitely my machine. Could be either wsl or I'm in an older version of podman. ",
          "created_at": "2025-04-17T13:14:22Z"
        },
        {
          "author": "rhatdan",
          "body": "Continue the conversation here if you are still having issues.",
          "created_at": "2025-04-17T16:09:31Z"
        },
        {
          "author": "bmahabirbu",
          "body": "Works on my fedora 42 machine using make seems its just something up with my cuda machine thanks!",
          "created_at": "2025-04-18T01:50:07Z"
        }
      ]
    },
    {
      "issue_number": 1198,
      "title": "ramalama rag outputs nonsense for all models and any RAG input",
      "body": "### Issue Description\n\nEven a simple or empty rag content makes the `ramalama run` not working, output is non-sense for any prompt.\n\nSeen with python3-ramalama-0.7.2-1.fc41.noarch RPM from Fedora (despite issues described in #1170) and still with ramalama 0.7.4 installed from `pip install ramalama`:\n\n```\n[hhorak]$ touch test.md\n\n[hhorak]$ ramalama rag test.md localhost:mytestrag\n\nBuilding localhost:mytestrag...\nadding vectordb...\n9ea1acb955c2be109ac5d8f68be11b23282da38db887a59accd602c8b29f6a90\n\n[hhorak]$ ramalama run --rag localhost:mytestrag granite3-moe \n> hello\nonininininininininininininininininininininininininiin^C\n<snipped...>\n```\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1.\n2.\n3.\n\n\n### Describe the results you received\n\nDescribe the results you received\n\n### Describe the results you expected\n\nDescribe the results you expected\n\n### ramalama info output\n\n```yaml\nIf you are unable to run ramalama info for any reason, please provide the ramalama version, operating system and its version and the architecture you are running.\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nAdditional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",
      "state": "closed",
      "author": "hhorak",
      "author_type": "User",
      "created_at": "2025-04-16T08:27:16Z",
      "updated_at": "2025-04-18T01:07:30Z",
      "closed_at": "2025-04-17T12:27:43Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1198/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1198",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1198",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:51.243432",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "@hhorak did you try other models? What does your hardware look like?",
          "created_at": "2025-04-16T11:37:05Z"
        },
        {
          "author": "hhorak",
          "body": "> [@hhorak](https://github.com/hhorak) did you try other models? What does your hardware look like?\n\nI tried only `granite3-moe` and `llama3` so far. I might try others. The models work fine when using without the rag, but behave similarly when using the rag even with empty extra file.\n\nI'm trying t",
          "created_at": "2025-04-16T13:39:14Z"
        },
        {
          "author": "rhatdan",
          "body": "I see the same issue on Cuda.\n\n@bmahabirbu PTAL",
          "created_at": "2025-04-16T13:47:17Z"
        },
        {
          "author": "bmahabirbu",
          "body": "This is odd I tested on m4 mac and it worked. Im using llama3.2 with this image which should be the latest. Maybe it has to be a gpu image problem.\n`quay.io/ramalama/ramalama-rag  0.7         bc76916cdd0c  2 days ago     3.24 GB`\n\nI tested using both a sample pdf and a .md file",
          "created_at": "2025-04-16T15:23:10Z"
        },
        {
          "author": "bmahabirbu",
          "body": "Ill try to run this later today on my cuda machine. @hhorak are you using the base ramalama-rag image or is it a gpu one? You can check using podman images. Also you can try using --debug on your commands to see what image is being downloaded and run",
          "created_at": "2025-04-16T15:48:49Z"
        }
      ]
    },
    {
      "issue_number": 1204,
      "title": "Invalid option for --runtime=vllm",
      "body": "### Issue Description\n\nRunning `ramalama --runtime=vllm run granite3-dense` works like a charm, but the same command with `serve` instead of `run` throws:\n\n```\n ramalama --runtime vllm serve granite3-dense\nserving on port 8080\n\n:: initializing oneAPI environment ...\n   entrypoint.sh: BASH_VERSION = 5.2.32(1)-release\n   args: Using \"$@\" for setvars.sh arguments: --port 8080 --model /mnt/models/model.file --max_model_len 2048\n:: compiler -- latest\n:: mkl -- latest\n:: tbb -- latest\n:: umf -- latest\n:: oneAPI environment initialized ::\n\n/usr/bin/entrypoint.sh: line 6: exec: --: invalid option\nexec: usage: exec [-cl] [-a name] [command [argument ...]] [redirection ...]\n```\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. install ramalama through pip on F42\n2. run the above command\n3.\n\n\n### Describe the results you received\nbreakage \n\n### Describe the results you expected\nsuccessful inference\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"intel\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 98.66,\n                    \"systemPercent\": 0.83,\n                    \"userPercent\": 0.51\n                },\n                \"cpus\": 18,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"server\",\n                    \"version\": \"42\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2048,\n                \"hostname\": \"chorny.thuisnet.com\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000008,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.14.2-300.fc42.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 751022080,\n                \"memTotal\": 66689716224,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc42.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc42.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc42.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/1000000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc42.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8587653120,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"5h 27m 30.00s (Approximately 0.21 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/maxim/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 0,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/maxim/.local/share/containers/storage\",\n                \"graphRootAllocated\": 32094052352,\n                \"graphRootUsed\": 15218200576,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 1\n                },\n                \"runRoot\": \"/run/user/1000000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/maxim/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Wed Apr  2 02:00:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/intel-gpu:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/maxim/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n\n```\n\n### Upstream Latest Release\n\nYes\n",
      "state": "open",
      "author": "wzzrd",
      "author_type": "User",
      "created_at": "2025-04-16T19:19:36Z",
      "updated_at": "2025-04-17T20:12:11Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1204/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1204",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1204",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:51.511191",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Please run with debug to show us the podman line.  I believe the problem is we are not grabbing the vllm image when attempting to run vllm.  I tried this locally and I see.\n\n podman run --rm -i --label ai.ramalama --name ramalama_TidnUuIOaD --env=HOME=/tmp --init --runtime /usr/bin/nvidia-container-",
          "created_at": "2025-04-17T12:35:28Z"
        },
        {
          "author": "rhatdan",
          "body": "    docker.io/vllm/vllm-openai:latest",
          "created_at": "2025-04-17T12:36:38Z"
        },
        {
          "author": "wzzrd",
          "body": "Fairly sure you are correct ;)\n\n```\nramalama --debug --runtime vllm serve -c 16384 --temp 0.8 --ngl 999 --threads 9 --host 0.0.0.0 --port 9999 --device /dev/accel --device /dev/dri granite3-dense:8b\nexec_cmd:  podman run --rm -i --label ai.ramalama --name ramalama_2rudBfe3Eb --env=HOME=/tmp --init -",
          "created_at": "2025-04-17T13:21:22Z"
        },
        {
          "author": "rhatdan",
          "body": "This got me further:\n\n$ ramalama --image **docker.io/vllm/vllm-openai:latest** --debug --runtime vllm serve -c 16384 --temp 0.8 --ngl 999 --threads 9 --host 0.0.0.0 --port 9999 --device /dev/accel --device /dev/dri granite3-dense:8b\nexec_cmd:  podman run --rm -i --label ai.ramalama --name ramalama_f",
          "created_at": "2025-04-17T16:43:13Z"
        },
        {
          "author": "rhatdan",
          "body": "@robertgshaw2-redhat  any idea what is going on here, or someone who could check?",
          "created_at": "2025-04-17T16:44:26Z"
        }
      ]
    },
    {
      "issue_number": 1139,
      "title": "RFE: `ramalama gguf` command to translate models to GGUF format",
      "body": "As an ignorant newbie to the AI ecosystem, I was tripped up during my initial attempt to run a model from Hugging Face because `ramalama` was expecting a quantized GGUF model (see https://github.com/containers/ramalama/issues/691).\n\nIf we want to continue on the path to making AI boring, it would be great if there was a subcommand to `ramalama` that would generate the GGUF files for consumption.\n\nBonus points if it could also happen transparently when doing `ramalama run` and the user isn't aware of the GGUF requirement.\n\nYes, `llama.cpp` does present some nice helper links when you try to run an non-GGUF model:\n\n```\n$ ramalama pull huggingface://deepseek-ai/DeepSeek-V3-0324                                                                                                                                                                                                                                                                                             \n                                                                                                                                                                                                                                                                                                                                                                                             \nllama.cpp does not support running safetensor models, please use a/convert to the GGUF format using:                                                                                                                                                                                                                                                                                         \n- https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-V3-0324                                                                                                                                                                                                                                                                                                      \n- https://huggingface.co/spaces/ggml-org/gguf-my-repo                                                                                                                                                                                                                                                                                                                                        \n\n...\n```\n\n...but when you want the thing to Just Work, it would be a huge improvement if the conversion to GGUF could happen as needed.  (Probably some hardware constraints here, but I suspect `ramalama` could detect this?)",
      "state": "closed",
      "author": "miabbott",
      "author_type": "User",
      "created_at": "2025-04-07T20:28:55Z",
      "updated_at": "2025-04-17T13:23:39Z",
      "closed_at": "2025-04-17T13:23:39Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1139/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1139",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1139",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:53.607497",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "This is worth exploring, we could just tightly integrate:\n\nhttps://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf.py\n\nI have never done this before, don't know how computationally expensive this is. There is this also:\n\nhttps://github.com/ngxson/ggml-easy\n\nfrom @ngxson which to be hone",
          "created_at": "2025-04-07T21:24:01Z"
        },
        {
          "author": "ngxson",
          "body": "> from [@ngxson](https://github.com/ngxson) which to be honest I'm hoping just becomes a feature of llama-server so it can run a .safetensors file without conversion. But I am far from an expert on these file formats and what makes sense.\n\nI think that future is still quite far. While technically we",
          "created_at": "2025-04-07T21:55:17Z"
        },
        {
          "author": "rhatdan",
          "body": "@ggerganov thoughts?",
          "created_at": "2025-04-11T11:40:55Z"
        },
        {
          "author": "ngxson",
          "body": "FYI, we also recently support `convert_hf_to_gguf.py --remote user/repo` that allows converting safetensors --> gguf remotely without having to download HF model to disk (it relies on [HTTP Range header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Range_requests))",
          "created_at": "2025-04-11T12:04:32Z"
        },
        {
          "author": "rhatdan",
          "body": "Might be interesting to change ramalama pull with a --convert-to-gguf option.",
          "created_at": "2025-04-11T19:19:05Z"
        }
      ]
    },
    {
      "issue_number": 1202,
      "title": "chat template file from model causes failure",
      "body": "### Issue Description\n\nThe new model store implicitly detects the chat template from the model (e.g. embedded in gguf or from ollama) and passes it to `llama-run` and `llama-serve`. However, recently this has led to cause issues and even failures (see steps to reproduce).\nThis seems to be independent of the model store, but really about the `--chat-template-file` passed to llama.cpp\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue:\nSimply running `ramalama --use-model-store` for models with custom chat templates such as smollm:135m or granite-code. This leads to the chat-template-file being passed to llama.cpp.\n\n### Describe the results you received\n\nUsing the chat template from the model (e.g. embedded in gguf) causes failure:\n```bash\n$ ramalama run smollm:135m\nFailed to infer a tool call example (possible template bug)\nðŸ¦­ > hi\nA classic problem in physics that has puzzled scientists for centuries!\n\nThe problem of time dilation is one of the most mind-bending and intriguing mysteries in all of physics. It's a fundamental concept in modern physics, and I'll try to break it down for you in simple terms.\n\n**What is time dilation?**\n...\n$ ðŸ¦­ > next\ndecode: n_tokens == 0\nllama_decode: failed to decode, ret = -1\nfailed to decode\nfailed to generate response\n```\n\n\n### Describe the results you expected\n\nNot passing the chat template to `llama-run` yields far better results:\n```\n$ ramalama run smollm:135m\nðŸ¦­ > hi\nHello! How can I help you today?\nðŸ¦­ > \n```\n\n\n### ramalama info output\n\n```yaml\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.2\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 95.67,\n                    \"systemPercent\": 0.82,\n                    \"userPercent\": 3.51\n                },\n                \"cpus\": 16,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2032,\n                \"hostname\": \"fedora\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 1000,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 524288,\n                            \"size\": 65536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.13.9-200.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 11474231296,\n                \"memTotal\": 65965858816,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.20-2.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.20\\ncommit: 9c9a76ac11994701dd666c4f0b869ceffb599a66\\nrundir: /run/user/1000/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc41.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/1000/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"\",\n                    \"package\": \"\",\n                    \"version\": \"\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"31h 45m 4.00s (Approximately 1.29 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/mengel/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 0,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 0\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/mengel/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1022488477696,\n                \"graphRootUsed\": 82620403712,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 48\n                },\n                \"runRoot\": \"/run/user/1000/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/mengel/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.1\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1741651200,\n                \"BuiltTime\": \"Tue Mar 11 01:00:00 2025\",\n                \"GitCommit\": \"b79bc8afe796cba51dd906270a7e1056ccdfcf9e\",\n                \"GoVersion\": \"go1.23.7\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.1\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama:0.7\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/mengel/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nAdditional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",
      "state": "open",
      "author": "engelmi",
      "author_type": "User",
      "created_at": "2025-04-16T13:55:31Z",
      "updated_at": "2025-04-16T13:55:31Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1202/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1202",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1202",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:53.812077",
      "comments": []
    },
    {
      "issue_number": 1192,
      "title": "ramalama 0.7.4 still using 0.7.0 ramalama-rag image",
      "body": "### Issue Description\n\nI tried out ramalama 0.7.4 installed via pip3 on mac and fedora 41 - in both cases ramalama rag is pulling older image 0.7.0 that still has the init_empty_ weights bug documented in https://github.com/containers/ramalama/issues/1157 and fixed in ramalama-rag:0.7.4\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1.ramalama rag doc.pdf quay.io/ehaynes/ragstuff\n2.\n3.\n\n\n### Describe the results you received\n\nDescribe the results you received\nFile \"/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3736, in get_init_context\n    init_contexts = [no_init_weights(), init_empty_weights()]\n                                        ^^^^^^^^^^^^^^^^^^\nNameError: name 'init_empty_weights' is not defined\n-> Cannot close object, library is destroyed. This may cause a memory leak!\nError: Command '['podman', 'run', '--rm', '-v', '/Users/ehaynes/Downloads/SCTE_35_2022b.pdf:/docs//Users/ehaynes/Downloads/SCTE_35_2022b.pdf:ro,z', '-v', '/Users/ehaynes/Downloads/RamaLama_rag_s3t6ag2h/vectordb:/output:z', '**quay.io/ramalama/ramalama-rag:0.7**', 'doc2rag', '/output', '/docs/']' returned non-zero exit status 1.\n\n\n### Describe the results you expected\n\nDescribe the results you expected\n\nexpected ramalama 7.4 to use image quay.io/ramalama/ramalama-rag:0.7.4 not 0.7.0\n\nif  run command with --image=quay.io/ramalama/ramalama-rag:0.7.4 it avoids the init_empty_weights bug in the 0.7.0 image\n\n### ramalama info output\n\n```yaml\nIf you are unable to run ramalama info for any reason, please provide the ramalama version, operating system and its version and the architecture you are running.\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"Client\": {\n                \"APIVersion\": \"5.4.1\",\n                \"BuildOrigin\": \"pkginstaller\",\n                \"Built\": 1741718260,\n                \"BuiltTime\": \"Tue Mar 11 14:37:40 2025\",\n                \"GitCommit\": \"b79bc8afe796cba51dd906270a7e1056ccdfcf9e\",\n                \"GoVersion\": \"go1.24.1\",\n                \"Os\": \"darwin\",\n                \"OsArch\": \"darwin/arm64\",\n                \"Version\": \"5.4.1\"\n            },\n            \"host\": {\n                \"arch\": \"arm64\",\n                \"buildahVersion\": \"1.38.1\",\n                \"cgroupControllers\": [\n                    \"cpuset\",\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\",\n                    \"rdma\",\n                    \"misc\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.12-3.fc41.aarch64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.12, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 99.49,\n                    \"systemPercent\": 0.12,\n                    \"userPercent\": 0.39\n                },\n                \"cpus\": 6,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"coreos\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2032,\n                \"hostname\": \"localhost.localdomain\",\n                \"idMappings\": {\n                    \"gidmap\": null,\n                    \"uidmap\": null\n                },\n                \"kernel\": \"6.12.7-200.fc41.aarch64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 8591785984,\n                \"memTotal\": 19463438336,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.13.1-1.fc41.aarch64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.13.1\"\n                    },\n                    \"package\": \"netavark-1.13.1-1.fc41.aarch64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.13.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.19.1-1.fc41.aarch64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.19.1\\ncommit: 3e32a70c93f5aa5fea69b50256cca7fd4aa23c80\\nrundir: /run/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20241211.g09478d5-1.fc41.aarch64\",\n                    \"version\": \"pasta 0^20241211.g09478d5-1.fc41.aarch64-pasta\\nCopyright Red Hat\\nGNU General Public License, version 2 or later\\n  <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.\\n\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"unix:///run/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": false,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": true,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-1.fc41.aarch64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 0,\n                \"swapTotal\": 0,\n                \"uptime\": \"74h 54m 13.00s (Approximately 3.08 days)\",\n                \"variant\": \"v8\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/usr/share/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 12,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 12\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {\n                    \"overlay.imagestore\": \"/usr/lib/containers/storage\",\n                    \"overlay.mountopt\": \"nodev,metacopy=on\"\n                },\n                \"graphRoot\": \"/var/lib/containers/storage\",\n                \"graphRootAllocated\": 98899800064,\n                \"graphRootUsed\": 39025475584,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"xfs\",\n                    \"Native Overlay Diff\": \"false\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"true\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"true\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 72\n                },\n                \"runRoot\": \"/run/containers/storage\",\n                \"transientStore\": false,\n                \"volumePath\": \"/var/lib/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.3.2\",\n                \"Built\": 1737504000,\n                \"BuiltTime\": \"Tue Jan 21 19:00:00 2025\",\n                \"GitCommit\": \"\",\n                \"GoVersion\": \"go1.23.4\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/arm64\",\n                \"Version\": \"5.3.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/Users/ehaynes/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.4\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nAdditional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",
      "state": "closed",
      "author": "edhaynes",
      "author_type": "User",
      "created_at": "2025-04-14T18:58:00Z",
      "updated_at": "2025-04-16T13:43:43Z",
      "closed_at": "2025-04-16T13:43:42Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1192/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1192",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1192",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:53.812097",
      "comments": [
        {
          "author": "rhatdan",
          "body": "\nWhat does --debug show?\n$ ramalama --debug rag README.md quay.io/rhatdan/myrag\nrun_cmd:  podman inspect quay.io/ramalama/cuda-rag:0.7\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nrun_cmd:  podman run --rm -v /home/dwalsh/ramalama/README.md:/docs",
          "created_at": "2025-04-15T12:02:51Z"
        },
        {
          "author": "rhatdan",
          "body": "I pulled all three versions and see the same image:\n\n$ podman images | grep ramalama-rag\nquay.io/ramalama/ramalama-rag                latest                            **29bd874256d5**  26 hours ago  3.65 GB\nquay.io/ramalama/ramalama-rag                0.7                               **29bd874256d",
          "created_at": "2025-04-15T12:05:14Z"
        },
        {
          "author": "edhaynes",
          "body": "I guess I just assumed ramalama 0.7.4 should use ramalama-rag image 0.7.4 0 - it's weird you're seeing the same image above because when force it to use 0.7.4 with the --image= I get different behavior and avoid the \"NameError: name 'init_empty_weights' is not defined\"\n\nehaynes@ehaynes-mac Downloads",
          "created_at": "2025-04-15T12:20:28Z"
        },
        {
          "author": "edhaynes",
          "body": "podman images | grep ramalama-rag \nquay.io/ramalama/ramalama-rag                                                                       0.7.4             bc76916cdd0c  26 hours ago  3.24 GB\nquay.io/ramalama/ramalama-rag                                                                       0.7.3      ",
          "created_at": "2025-04-15T12:25:01Z"
        },
        {
          "author": "rhatdan",
          "body": "Could you try to remove quay.io/ramalama/ramalama-rag:0.7 and see if it pulls the correct version.\n\npodman rmi quay.io/ramalama/ramalama-rag:0.7 --force\n",
          "created_at": "2025-04-15T13:34:35Z"
        }
      ]
    },
    {
      "issue_number": 709,
      "title": "logging into Hugging Face with a token, prompts for username/password?",
      "body": "After getting `ramalama` and `huggingface_hub` installed, I tried to login to Hugging Face with a token. I was surprised to see that I was prompted for a username and password.\n\n```\n$ export RAMALAMA_TRANSPORT=huggingface\n$ ramalama login --token hf_...\nUsername: miabbott@redhat.com\nPassword: \nLogin Succeeded!\n```\n\nI was able to leave the password field blank, but why am I getting prompted at all? Shouldn't the token be enough to access HF?\n\n\n_Edit:  yup, I just posted a full token in this comment...it was read-only, but I've already deleted it_",
      "state": "closed",
      "author": "miabbott",
      "author_type": "User",
      "created_at": "2025-02-02T18:24:20Z",
      "updated_at": "2025-04-16T12:43:35Z",
      "closed_at": "2025-04-16T12:43:35Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/709/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dougsland"
      ],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/709",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/709",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:54.033388",
      "comments": [
        {
          "author": "dougsland",
          "body": "As I am working in the #698, let me try this one too. Probably I will need your help for both @miabbott ",
          "created_at": "2025-02-02T23:19:42Z"
        },
        {
          "author": "tyll",
          "body": "This seems to be a duplicate of #643",
          "created_at": "2025-04-16T07:59:31Z"
        }
      ]
    },
    {
      "issue_number": 1200,
      "title": "`llama3.2:3b` model response ends with an escape code",
      "body": "### Issue Description\n\nI am trying out RamaLama with llama3.2:3b model on my laptop for my ML project.\nI am getting the model response with an ANSI escape code attached at the end.\n\n`$ echo \"Hello, World!\" | ramalama run llama3.2:3b > output.txt`\n\nText under output.txt --> `Hello! It's nice to meet you! How can I assist you today?\\x1b[0m`\n\n### Steps to reproduce the issue\n\nSteps to reproduce the issue\n1. Make sure that you have the dependencies installed. \n     `$ ramalama version` \n     `$ podman --version`\n2. Pull `llama3.2:3b` model using \n     `$ ramalama pull llama3.2:3b`\n3. Run the below command on the terminal: \n     `$ echo \"Hello, World!\" | ramalama run llama3.2:3b > output.txt`\n4. Open the output.txt file in an editor, and you will see the escape code printed at the end of the model response.\n\n### Describe the results you received\n\nOn running `echo \"Hello, World!\" | ramalama run llama3.2:3b > output.txt`, the content of the output.txt file contains an ANSI escape code at the end of the response captured.\n\n[output.txt](https://github.com/user-attachments/files/19771511/output.txt)\n\n### Describe the results you expected\n\nThe response from the model should not contain any escape code at the end.\n\n### ramalama info output\n\n```yaml\n$ ramalama info\n{\n    \"Accelerator\": \"none\",\n    \"Engine\": {\n        \"Info\": {\n            \"host\": {\n                \"arch\": \"amd64\",\n                \"buildahVersion\": \"1.39.4\",\n                \"cgroupControllers\": [\n                    \"cpu\",\n                    \"io\",\n                    \"memory\",\n                    \"pids\"\n                ],\n                \"cgroupManager\": \"systemd\",\n                \"cgroupVersion\": \"v2\",\n                \"conmon\": {\n                    \"package\": \"conmon-2.1.13-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/conmon\",\n                    \"version\": \"conmon version 2.1.13, commit: \"\n                },\n                \"cpuUtilization\": {\n                    \"idlePercent\": 95.36,\n                    \"systemPercent\": 1.02,\n                    \"userPercent\": 3.61\n                },\n                \"cpus\": 16,\n                \"databaseBackend\": \"sqlite\",\n                \"distribution\": {\n                    \"distribution\": \"fedora\",\n                    \"variant\": \"workstation\",\n                    \"version\": \"41\"\n                },\n                \"eventLogger\": \"journald\",\n                \"freeLocks\": 2046,\n                \"hostname\": \"rochandr-thinkpadp16vgen1.rmtin.csb\",\n                \"idMappings\": {\n                    \"gidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 4203193,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 165536\n                        }\n                    ],\n                    \"uidmap\": [\n                        {\n                            \"container_id\": 0,\n                            \"host_id\": 4203193,\n                            \"size\": 1\n                        },\n                        {\n                            \"container_id\": 1,\n                            \"host_id\": 165536,\n                            \"size\": 165536\n                        }\n                    ]\n                },\n                \"kernel\": \"6.13.10-200.fc41.x86_64\",\n                \"linkmode\": \"dynamic\",\n                \"logDriver\": \"journald\",\n                \"memFree\": 38331723776,\n                \"memTotal\": 65965850624,\n                \"networkBackend\": \"netavark\",\n                \"networkBackendInfo\": {\n                    \"backend\": \"netavark\",\n                    \"dns\": {\n                        \"package\": \"aardvark-dns-1.14.0-1.fc41.x86_64\",\n                        \"path\": \"/usr/libexec/podman/aardvark-dns\",\n                        \"version\": \"aardvark-dns 1.14.0\"\n                    },\n                    \"package\": \"netavark-1.14.1-1.fc41.x86_64\",\n                    \"path\": \"/usr/libexec/podman/netavark\",\n                    \"version\": \"netavark 1.14.1\"\n                },\n                \"ociRuntime\": {\n                    \"name\": \"crun\",\n                    \"package\": \"crun-1.21-1.fc41.x86_64\",\n                    \"path\": \"/usr/bin/crun\",\n                    \"version\": \"crun version 1.21\\ncommit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88\\nrundir: /run/user/4203193/crun\\nspec: 1.0.0\\n+SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\"\n                },\n                \"os\": \"linux\",\n                \"pasta\": {\n                    \"executable\": \"/usr/bin/pasta\",\n                    \"package\": \"passt-0^20250320.g32f6212-2.fc41.x86_64\",\n                    \"version\": \"\"\n                },\n                \"remoteSocket\": {\n                    \"exists\": true,\n                    \"path\": \"/run/user/4203193/podman/podman.sock\"\n                },\n                \"rootlessNetworkCmd\": \"pasta\",\n                \"security\": {\n                    \"apparmorEnabled\": false,\n                    \"capabilities\": \"CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\",\n                    \"rootless\": true,\n                    \"seccompEnabled\": true,\n                    \"seccompProfilePath\": \"/usr/share/containers/seccomp.json\",\n                    \"selinuxEnabled\": true\n                },\n                \"serviceIsRemote\": false,\n                \"slirp4netns\": {\n                    \"executable\": \"/usr/bin/slirp4netns\",\n                    \"package\": \"slirp4netns-1.3.1-1.fc41.x86_64\",\n                    \"version\": \"slirp4netns version 1.3.1\\ncommit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236\\nlibslirp: 4.8.0\\nSLIRP_CONFIG_VERSION_MAX: 5\\nlibseccomp: 2.5.5\"\n                },\n                \"swapFree\": 8589930496,\n                \"swapTotal\": 8589930496,\n                \"uptime\": \"7h 27m 11.00s (Approximately 0.29 days)\",\n                \"variant\": \"\"\n            },\n            \"plugins\": {\n                \"authorization\": null,\n                \"log\": [\n                    \"k8s-file\",\n                    \"none\",\n                    \"passthrough\",\n                    \"journald\"\n                ],\n                \"network\": [\n                    \"bridge\",\n                    \"macvlan\",\n                    \"ipvlan\"\n                ],\n                \"volume\": [\n                    \"local\"\n                ]\n            },\n            \"registries\": {\n                \"search\": [\n                    \"registry.fedoraproject.org\",\n                    \"registry.access.redhat.com\",\n                    \"docker.io\"\n                ]\n            },\n            \"store\": {\n                \"configFile\": \"/home/rochandr/.config/containers/storage.conf\",\n                \"containerStore\": {\n                    \"number\": 2,\n                    \"paused\": 0,\n                    \"running\": 0,\n                    \"stopped\": 2\n                },\n                \"graphDriverName\": \"overlay\",\n                \"graphOptions\": {},\n                \"graphRoot\": \"/home/rochandr/.local/share/containers/storage\",\n                \"graphRootAllocated\": 1022488809472,\n                \"graphRootUsed\": 50422181888,\n                \"graphStatus\": {\n                    \"Backing Filesystem\": \"btrfs\",\n                    \"Native Overlay Diff\": \"true\",\n                    \"Supports d_type\": \"true\",\n                    \"Supports shifting\": \"false\",\n                    \"Supports volatile\": \"true\",\n                    \"Using metacopy\": \"false\"\n                },\n                \"imageCopyTmpDir\": \"/var/tmp\",\n                \"imageStore\": {\n                    \"number\": 8\n                },\n                \"runRoot\": \"/run/user/4203193/containers\",\n                \"transientStore\": false,\n                \"volumePath\": \"/home/rochandr/.local/share/containers/storage/volumes\"\n            },\n            \"version\": {\n                \"APIVersion\": \"5.4.2\",\n                \"BuildOrigin\": \"Fedora Project\",\n                \"Built\": 1743552000,\n                \"BuiltTime\": \"Wed Apr  2 05:30:00 2025\",\n                \"GitCommit\": \"be85287fcf4590961614ee37be65eeb315e5d9ff\",\n                \"GoVersion\": \"go1.23.7\",\n                \"Os\": \"linux\",\n                \"OsArch\": \"linux/amd64\",\n                \"Version\": \"5.4.2\"\n            }\n        },\n        \"Name\": \"podman\"\n    },\n    \"Image\": \"quay.io/ramalama/ramalama\",\n    \"Runtime\": \"llama.cpp\",\n    \"Store\": \"/home/rochandr/.local/share/ramalama\",\n    \"UseContainer\": true,\n    \"Version\": \"0.7.2\"\n}\n```\n\n### Upstream Latest Release\n\nYes\n\n### Additional environment details\n\nAdditional environment details\n\n### Additional information\n\nAdditional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",
      "state": "open",
      "author": "r14chandra",
      "author_type": "User",
      "created_at": "2025-04-16T12:36:09Z",
      "updated_at": "2025-04-16T12:36:09Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1200/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1200",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1200",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:54.216455",
      "comments": []
    },
    {
      "issue_number": 828,
      "title": "ramalama pull/run hangs when host uses IPv6",
      "body": "This could be a \"me\" issue in the sense networking is broader than AI, but I'll report in the interest of not needing to configure the host, and for others to find.\n\n`ramalama --debug pull deepseek-r1:1.5b` wouldn't complete on my system, and didn't print any messages.\n\nSetting `sudo sysctl net.ipv6.conf.all.disable_ipv6=1` allowed ramalama to pull successfully.\n\nLooks like Ollama registry doesn't support ip6: https://github.com/ollama/ollama/issues/2216. Huggingface didn't work for me either tho I've read they are dual ip4/ip6.\n\nIs this something ramalama can gracefully handle, or is there some general config I should set on my machine (Fedora 41 Workstation) that's outside of ramalama's responsibility (eg disable ip6)?",
      "state": "closed",
      "author": "puttup",
      "author_type": "User",
      "created_at": "2025-02-16T00:53:12Z",
      "updated_at": "2025-04-16T11:31:36Z",
      "closed_at": "2025-04-16T11:31:36Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/828/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/828",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/828",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:54.216472",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "I see this as more of an Ollama registry issue :) But if someone opens a PR to prefer IPv4, no complaints. I think a global to force IPv4 could be too much, because some internal networks may want to use IPv6, etc.\n\nI think we could force IPv4 for https://registry.ollama.ai (and huggingface if we pr",
          "created_at": "2025-02-16T12:46:00Z"
        },
        {
          "author": "puttup",
          "body": "Oh totally! I was more thinking fallback to ip4 if 6 falls. Providing a timeout in urlopen worked in my little test script but I didn't try this in ramalama",
          "created_at": "2025-02-16T18:28:02Z"
        },
        {
          "author": "ondrejbudai",
          "body": "FTR, I tried reproducing this on both Fedora 42, and MacOS, and pulling `deepseek-r1:1.5b` works fine for me on IPv6-enabled systems.",
          "created_at": "2025-04-16T09:06:06Z"
        },
        {
          "author": "ericcurtin",
          "body": "Closing for now, can re-open if we see it again",
          "created_at": "2025-04-16T11:31:36Z"
        }
      ]
    },
    {
      "issue_number": 1073,
      "title": "rag daemon with inotify/fanotify support",
      "body": "Would behave like:\n\n```\nramalama rag --watchdir /somedir\n```\n\non changes to files in the directory would automatically add the new data to the vector db.",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-03-29T20:15:44Z",
      "updated_at": "2025-04-14T21:58:05Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1073/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1073",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1073",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:54.399276",
      "comments": [
        {
          "author": "rhatdan",
          "body": "So this would run as a continuously running service?\n\nShould this be?\n\nramalama rag --watchdir /somedir quay.io/REPO/myrag\n\nAnd then update and push the image, or does it just update a local rag database and then it is up to the user to convert it to an image and push it to a registry, when they are",
          "created_at": "2025-03-30T10:31:04Z"
        },
        {
          "author": "ericcurtin",
          "body": "> So this would run as a continuously running service?\n\nYes, it can be thought of as like a Google Drive daemon, dropbox daemon, etc.\n\n> \n> Should this be?\n> \n> ramalama rag --watchdir /somedir quay.io/REPO/myrag\n> \n> And then update and push the image, or does it just update a local rag database an",
          "created_at": "2025-03-31T10:10:58Z"
        }
      ]
    },
    {
      "issue_number": 51,
      "title": "Implement whisper.cpp",
      "body": "If there is a way to auto-detect between language model files and asr model files. We should do that, or if that's not possible we should just use a runtime flag, so some options for the runtime flag would be:\r\n\r\n```\r\nramalama --runtime vllm\r\nramalama --runtime llama.cpp\r\nramalama --runtime whisper.cpp\r\n```\r\n",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2024-08-21T11:09:53Z",
      "updated_at": "2025-04-14T13:21:58Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 26,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/51/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/51",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/51",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:54.626862",
      "comments": []
    },
    {
      "issue_number": 1162,
      "title": "rag: ramalama run --rag tries to connect to port 8080 even if a different port is chosen",
      "body": "When port 8080 is already used by a different program, ramalama chooses a different port but the `rag_framework` script hard-codes the port number, leading to a \"connection refused\" error when interacting on the prompt:\n\n```console\n$ nc -l -p 8080 &\n$ ramalama --debug --image quay.io/ramalama/ramalama-rag run -y.io/localhost/myrag:0.1 deepseek\nWorking directory: None\nIgnore stderr: False\nIgnore all: False\nCommand finished with return code: 0\nChecking if 8080 is available\nChecking if 8084 is available\nexec_cmd:  podman run --rm -i --label ai.ramalama --name ramalama_lij2JQobRo --env=HOME=/tmp --init --security-opt=label=disable --cap-drop=all --security-opt=no-new-privileges --label ai.ramalama.model=ollama://deepseek-r1 --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.port=8084 --label ai.ramalama.command=run --env LLAMA_PROMPT_PREFIX=ðŸ¦­ >  --pull=newer -t -p 8084:8084 --device /dev/dri --device /dev/kfd -e HIP_VISIBLE_DEVICES=0 --network bridge --mount=type=image,source=localhost/myrag:0.1,destination=/rag,rw=true --mount=type=bind,src=/home/ueno/.local/share/ramalama/models/ollama/deepseek-r1:latest,destination=/mnt/models/model.file,ro quay.io/ramalama/ramalama-rag:0.7 bash -c nohup llama-server --port 8084 --model /mnt/models/model.file --alias deepseek-r1 --ctx-size 2048 --temp 0.8 --jinja -v -ngl 0 --threads 8 --host 0.0.0.0 &> /tmp/llama-server.log & rag_framework run /rag/vector.db\n[...]\n> What's the goal of RamaLama project?\n[...]\nopenai.APIConnectionError: Connection error.\n```\n\nI can think of two ways to fix it:\n- add an option to `rag_framework` to specify the connecting port and pass the actual port number\n- assume `--network private` and always use port 8080",
      "state": "open",
      "author": "ueno",
      "author_type": "User",
      "created_at": "2025-04-10T13:07:03Z",
      "updated_at": "2025-04-14T11:39:05Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1162/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1162",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1162",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:54.626882",
      "comments": [
        {
          "author": "rhatdan",
          "body": "We should leak the PORT into the container so that scripts will no which to connect to.  Interested in opening a PR?",
          "created_at": "2025-04-10T15:45:10Z"
        },
        {
          "author": "ueno",
          "body": "A slightly better approach might be to communicate through a Unix domain socket, given the latest llama.cpp [got](https://github.com/ggml-org/llama.cpp/pull/12613) support for it in llama-server. Then rag_framework can be modified as below, so no external ports are needed:\n```diff\ndiff --git a/conta",
          "created_at": "2025-04-14T04:56:46Z"
        },
        {
          "author": "rhatdan",
          "body": "I like this alot, although I think in the long run we might move to llama-stack for connecting these services together.\n",
          "created_at": "2025-04-14T10:16:20Z"
        },
        {
          "author": "rhatdan",
          "body": "@ueno do you know if we have the latest llama.cpp in our containers yet, or do we need to update the release?",
          "created_at": "2025-04-14T10:16:56Z"
        },
        {
          "author": "ueno",
          "body": "@rhatdan yes, the latest container [image](https://quay.io/repository/ramalama/ramalama/manifest/sha256:51892a55dbbf6b9c117e26ef8e234b5db331edcc99e10095f00084112f1bdf95) seems to include the feature.",
          "created_at": "2025-04-14T11:39:04Z"
        }
      ]
    },
    {
      "issue_number": 947,
      "title": "Feature Request: Tool calling",
      "body": "It would be nice to support tool calls when serving a model.  For llama.cpp, this means passing `--jinja` and possibly `--chat-template-file`.  These should probably be options in `ramalama serve`.",
      "state": "open",
      "author": "edmcman",
      "author_type": "User",
      "created_at": "2025-03-11T19:50:51Z",
      "updated_at": "2025-04-13T11:36:19Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/947/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/947",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/947",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:54.844484",
      "comments": [
        {
          "author": "edmcman",
          "body": "This may have been added in https://github.com/containers/ramalama/pull/952?",
          "created_at": "2025-04-02T14:40:06Z"
        },
        {
          "author": "edmcman",
          "body": "How does one pass a custom chat template file to `ramalama serve`?  This is often needed, unfortunately.  [See here](https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md#usage---need-tool-aware-jinja-template)",
          "created_at": "2025-04-02T14:53:20Z"
        },
        {
          "author": "edmcman",
          "body": "It doesn't seem to be working even when the chat template in the GGUF is correct.\n\nI ran: `ramalama serve file:///home/ed/.cache/llama.cpp/bartowski_Qwen2.5-7B-Instruct-GGUF_Qwen2.5-7B-Instruct-Q4_K_M.gguf`\n\nAnd then the following command from the llama.cpp docs:\n```\ncurl http://localhost:8080/v1/ch",
          "created_at": "2025-04-02T15:40:11Z"
        },
        {
          "author": "ericcurtin",
          "body": "Try \"--runtime-args\" \"--jinja\"",
          "created_at": "2025-04-02T16:06:31Z"
        },
        {
          "author": "edmcman",
          "body": "Sure, that will work.  That is why I added `--runtime-args` in the first place.\n\nBeyond that, though, I would argue this should be an abstraction in ramalama.  I think @engelmi added some support for this, but only with `--use-model-store`?  And I can't see how to change the chat template manually.",
          "created_at": "2025-04-02T16:18:15Z"
        }
      ]
    },
    {
      "issue_number": 1057,
      "title": "Feature request: Support huggingface://user/repo:tag format for huggingface models",
      "body": "For example, to run a HF quant on llama.cpp, I run `llama-cli -hf mradermacher/ToolACE-2-Llama-3.1-8B-GGUF:Q4_K_M`.  It would be nice if the huggingface transport supported that syntax.  In many cases, this is probably equivalent to appending \".gguf\", but not for models split into multiple files.",
      "state": "closed",
      "author": "edmcman",
      "author_type": "User",
      "created_at": "2025-03-27T13:48:45Z",
      "updated_at": "2025-04-13T09:57:49Z",
      "closed_at": "2025-04-13T09:57:49Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1057/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1057",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1057",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:55.048779",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "@edmcman I agree, I ported it to llama-run in llama.cpp here:\n\nhttps://github.com/ggml-org/llama.cpp/pull/11449/files\n\nNeeds to be ported to python3 for RamaLama. Interested in taking this on @edmcman ?\n",
          "created_at": "2025-03-27T15:22:30Z"
        },
        {
          "author": "edmcman",
          "body": "I'm not sure when I'll be able to get around to this, so if someone else can knock it out quickly, that would probably be better. If not, I'll get to it... eventually.",
          "created_at": "2025-03-27T17:41:50Z"
        },
        {
          "author": "edmcman",
          "body": "I'm taking a look at this now.  I don't think this will be much technical work because IIUC HF uses the same API as Ollama for tags.  And ramalama already supports tags in Ollama.\n\nBut I have some behavior questions.  `ramalama pull huggingface://user/repo` currently will pull down an entire repo.  ",
          "created_at": "2025-04-05T14:00:11Z"
        },
        {
          "author": "edmcman",
          "body": "Some work on this here: https://github.com/containers/ramalama/compare/main...edmcman:ramalama:hf-tag?expand=1\n\nI'm currently downloading a file from `ramalama pull hf://mradermacher/ToolACE-2-Llama-3.1-8B-GGUF` so that's progress.",
          "created_at": "2025-04-05T20:11:47Z"
        },
        {
          "author": "edmcman",
          "body": "[This funciton](https://github.com/containers/ramalama/blob/e0fb7f0bd5953e6994f8dbaca2d0fd2ef4e99ad7/ramalama/huggingface.py#L139) is kind of weird.\n\n* llama.cpp *only* supports gguf models, right?  So looking for safetensors is not a sufficient check.  It should just look to see if gguf is missing.",
          "created_at": "2025-04-05T20:22:40Z"
        }
      ]
    },
    {
      "issue_number": 1135,
      "title": "Idea/Feature Request: MCP Client for the UI",
      "body": "MCP Getting popular an offers simple extention of the LLM capabilities.\n\nPlease consider adding MCP Client implementation to the ramalama\n\nThanks,\nD",
      "state": "open",
      "author": "qdrddr",
      "author_type": "User",
      "created_at": "2025-04-07T14:03:33Z",
      "updated_at": "2025-04-10T19:55:50Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1135/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1135",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1135",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:55.247291",
      "comments": [
        {
          "author": "rhatdan",
          "body": "That is our next step.  For now I am thinking going simple and just adding SQL database support.\n\nCurrently we can do MODEL and MODEL+RAG, next step is MODEL+DB. Adding fully MCP seems like a step too large from a CLI point of view.  \n\nWDYT?\n",
          "created_at": "2025-04-07T14:40:30Z"
        },
        {
          "author": "qdrddr",
          "body": "I think MCP Client is actually very simple and can bring a lot of functionality and integration options.",
          "created_at": "2025-04-07T15:24:42Z"
        },
        {
          "author": "rhatdan",
          "body": "Sure but what would the ramalama UI be?\n\nWe want to keep the UI simple.  Potentially as simple as give a HOST/PORT for a `remote` SQL Database\n\n--database ip=HOST:PORT\n\nAnd then RamaLama would wire this together.\n\nOr --database container=docker.io/mariadb,source=/path/to/db \n\n",
          "created_at": "2025-04-07T16:10:05Z"
        },
        {
          "author": "qdrddr",
          "body": "You would need MCP Client JSON config (or create an empty `{}` config file if not present) with a list of MCP Servers to connect to ( I would suggest focusing on sse only). From the CLI perspective, you might just give a high-level MCP-Client Ebable/Disable option; nothing else would be needed.\n\nMCP",
          "created_at": "2025-04-08T04:47:36Z"
        },
        {
          "author": "rhatdan",
          "body": "Interested in opening PRs?",
          "created_at": "2025-04-08T15:25:41Z"
        }
      ]
    },
    {
      "issue_number": 354,
      "title": "Gen AI models (stable diffusion)",
      "body": "I would like a way to generate images locally using Stable Diffusion-esqe models.\n\n`ramalama serve --name stable-diffusion stable-diffusion`\n\nhave it download a specified model, the allow me to ask the model for something and generate an image.\n\nWould love to discuss the interface/tui/flow of this more with the team!",
      "state": "open",
      "author": "jtligon",
      "author_type": "User",
      "created_at": "2024-10-22T15:36:30Z",
      "updated_at": "2025-04-09T13:56:07Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/354/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/354",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/354",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:55.481097",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "https://diffusionbee.com/ an example of a similar tool",
          "created_at": "2024-10-22T15:37:24Z"
        },
        {
          "author": "ericcurtin",
          "body": "This might be the best tool to look into to start as it uses the same library as llama.cpp:\n\nhttps://github.com/leejet/stable-diffusion.cpp\n\nGGML",
          "created_at": "2024-11-11T22:59:01Z"
        },
        {
          "author": "ericcurtin",
          "body": "Here's a UI for it:\n\nhttps://jellybox.com/",
          "created_at": "2024-11-11T23:30:12Z"
        },
        {
          "author": "rhatdan",
          "body": "Is this something we care about or is llama.cpp need to get this functionality so we can take advantage?",
          "created_at": "2025-04-07T19:02:41Z"
        },
        {
          "author": "ericcurtin",
          "body": "I don't expect llama.cpp to ever get this functionality, it's like whisper.cpp just another different form of AI inferencing\n\nAlthough all three of these projects use [ggml](https://github.com/ggml-org/ggml) llama.cpp, whisper.cpp, stable-diffusion.cpp",
          "created_at": "2025-04-07T21:46:56Z"
        }
      ]
    },
    {
      "issue_number": 912,
      "title": "EPIC: Support for vLLM",
      "body": "### Epic domain\n\nCurrently, our inferencing stack primarily supports LlamaCPP for Mac and Windows environments. \nTo enhance our model execution capabilities, ensure consistency between local and production environments, and drive adoption among application developers, we aim to extend our support to vLLM. \n\nObjective:\n- Enable vLLM as an alternative inferencing stack alongside LlamaCPP.\n- Ensure consistency between local and production environments.\n- Promote adoption and usage of vLLM by application developers.\n\n## Scope\n\n**1. CPU only**\n\nIn a first approach, we should be providing all the images for supporting vLLM running with CPU for Mac and Windows environments.\ncf: https://github.com/containers/ramalama/issues/801\n\n**2. Research & Feasibility Analysis for GPU support**\n\nIn order to provide inferecing speed and efficiency, we should be supporting GPU-backed execution for VLLM on Mac/Windows. We'll need to dig into the ways vLLM is executed and analyse the effort required to support backends that can offload operations to the GPU\n\n**3. Benchmark**\n\nWe should be tooling ourselves in order to conduct benchmarking and performance tests as part of our CI.\nWe would in particular, be interested by comparing:\n- native, vs container\n- llamacpp, vs vLLM\n\n## Sub-tasks\n\n- [ ] [Provide CPU only images for Mac](https://github.com/containers/ramalama/issues/801)\n- [ ] Provide CPU only images for Windows\n- [ ] Research analysis for GPU support Mac\n- [ ] Benchmarks suite for vLLM\n\n\n\n### Additional context\n\n_No response_\n\n",
      "state": "open",
      "author": "slemeur",
      "author_type": "User",
      "created_at": "2025-03-04T09:46:35Z",
      "updated_at": "2025-04-06T21:27:08Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/912/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/912",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/912",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:55.674700",
      "comments": [
        {
          "author": "martinhoyer",
          "body": "fwiw, I've been trying to do build a `fedora:42` image with\nhttps://repo.radeon.com/rocm/el9/6.3.4/ rpm packages and https://download.pytorch.org/whl/nightly/torch/ wheels, loosely following [vllm docs](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html?device=rocm#build-wheel-from",
          "created_at": "2025-03-16T21:50:36Z"
        },
        {
          "author": "martinhoyer",
          "body": "Tried again as I see gfx1100 listed on vllm build docs. It wasn't smooth but got it running at least locally on Fedora 41 and https://repo.radeon.com/rocm/el9/6.3.4/ packages. No time to work on it further.\n\nI've been using `uv pip` with `--index https://repo.radeon.com/rocm/manylinux/rocm-rel-6.3.4",
          "created_at": "2025-03-31T19:25:33Z"
        },
        {
          "author": "ericcurtin",
          "body": "> Tried again as I see gtx1100 listed on vllm build docs. It wasn't smooth but got it running at least locally on Fedora 41 and https://repo.radeon.com/rocm/el9/6.3.4/ packages. No time to work on it further.\n> \n> I've been using `uv pip` with `--index https://repo.radeon.com/rocm/manylinux/rocm-rel",
          "created_at": "2025-03-31T19:29:29Z"
        },
        {
          "author": "martinhoyer",
          "body": "> Did this work with a random popular .gguf lets just say for sake of argument deepseek-r1:1.5b ?\n\nI've only tried a random Tinyllama(safetensors), but can check tomorrow.\n\n> Would be nice if even as a start we added just a cpu inferencing version of vLLM built for all images here:\n> \n> https://gith",
          "created_at": "2025-03-31T19:43:30Z"
        },
        {
          "author": "rhatdan",
          "body": "openVINO is now part of the default ramalama image, although I have not tested anything.  We were asked to add it by podman-desktop team.",
          "created_at": "2025-04-01T17:35:42Z"
        }
      ]
    },
    {
      "issue_number": 852,
      "title": "Move slow tests to end of CI runs",
      "body": "Fail faster to get quicker response where possible.\nE.g. Validation checks like the manpage checks etc. should run before any slow jobs.",
      "state": "open",
      "author": "mkesper",
      "author_type": "User",
      "created_at": "2025-02-18T17:06:52Z",
      "updated_at": "2025-04-04T22:23:01Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/852/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/852",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/852",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:55.931746",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "One can also parallelize the tests and run in different builds to make the whole thing faster. I thought some builds ran the man pages check first",
          "created_at": "2025-02-18T19:24:47Z"
        },
        {
          "author": "rhatdan",
          "body": "@mkesper Interested in opening a PR?",
          "created_at": "2025-02-27T14:48:20Z"
        },
        {
          "author": "abhibongale",
          "body": "Hi @mkesper,\n\nI hope this issue is open. \n\nI am looking at github action jobs. Want to make sure that I am looking at correct place, right?. If yes, then I will explore CI yml files and Makefile.\n\ncc: @rhatdan, @ericcurtin ",
          "created_at": "2025-04-04T22:23:00Z"
        }
      ]
    },
    {
      "issue_number": 825,
      "title": "Ensure linting always applied",
      "body": "It would be beneficial to enforce linting in CI so no commits are accepted where linting fails.\nEvery time you run `make validate` the linter will be run and automatically fix existing code, leading to bigger diffs of code than wanted.",
      "state": "closed",
      "author": "mkesper",
      "author_type": "User",
      "created_at": "2025-02-15T15:43:53Z",
      "updated_at": "2025-04-03T12:52:32Z",
      "closed_at": "2025-04-03T12:52:32Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/825/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/825",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/825",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:57.798841",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "It was this way before, it has been removed from CI somehow, would welcome a PR to bring it back in.",
          "created_at": "2025-02-16T12:47:27Z"
        },
        {
          "author": "machadovilaca",
          "body": "this was fixed in https://github.com/containers/ramalama/commit/15c857d15f0b97448cdd39b9e9ea576d92220e40\nI think this issue and #845 can be closed @ericcurtin ",
          "created_at": "2025-04-03T12:48:40Z"
        }
      ]
    },
    {
      "issue_number": 694,
      "title": "JetBrains AI Assistant integration?",
      "body": "Is RamaLama a drop-in replacement to Ollama? I'm trying to use RamaLama with JetBrains AI Assistant but `failed to connect`\n\n![Image](https://github.com/user-attachments/assets/b72b3ab6-4186-4921-a06b-23dbf82c13e1)\n\nLogs:\n```\nrequest: GET / 192.168.***.*** 200\n```\n\nIt seems ramalama returned `gzip is not supported by this browser` to JetBrains AI Assistant:\n![Image](https://github.com/user-attachments/assets/0ddbd432-3027-4b31-af98-637d1b8fad90)\n\n## Steps to reproduce\n\n1. `ramalama --image localhost/ramalama/rocm-gfx9:latest serve qwen2.5-coder:7b`\n2. Launch JetBrains IDE (such as Pycharm Professional). Enable Ollama as shown in the screenshot.",
      "state": "closed",
      "author": "nikAizuddin",
      "author_type": "User",
      "created_at": "2025-02-01T03:02:07Z",
      "updated_at": "2025-04-03T10:12:52Z",
      "closed_at": "2025-04-03T10:12:52Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/694/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/694",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/694",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:58.013252",
      "comments": [
        {
          "author": "dougsland",
          "body": "@nikAizuddin https://www.jetbrains.com/pycharm/ is a valid IDE? Never used JetBrains, I would like to give it a try.",
          "created_at": "2025-02-01T03:31:12Z"
        },
        {
          "author": "nikAizuddin",
          "body": "> [@nikAizuddin](https://github.com/nikAizuddin) https://www.jetbrains.com/pycharm/ is a valid IDE? Never used JetBrains, I would like to give it a try.\n\nYup, PyCharm Community Edition should be okay.\n1. Under `Settings` > `Plugins`, install JetBrains AI Assistant plugin\n2. After plugin installation",
          "created_at": "2025-02-01T04:03:02Z"
        },
        {
          "author": "eye942",
          "body": "looks like ramalama directly uses llama.cpp as opposed to ollama which wraps the responses in its own api\nhttps://github.com/containers/ramalama/blob/032efaeed9c8e3c9bba06f985bc2d902a1b0c023/ramalama/model.py#L364\n\nError message seems to be coming from:\nhttps://github.com/ggerganov/llama.cpp/blob/aa",
          "created_at": "2025-02-06T02:33:54Z"
        },
        {
          "author": "rhatdan",
          "body": "I would prefer 3, llama.cpp is a much more open project and the basis of most of the work being done by Ollama.  JetBrains should support llama.cpp\n",
          "created_at": "2025-02-06T13:39:43Z"
        },
        {
          "author": "ericcurtin",
          "body": "You raised my interest though @eye942 we have the llama.cpp webui on by default in RamaLama. Does it cause issue having it on? I think it's quite useful, but have always assumed it had no impact on the REST API (maybe a false assumption)\n",
          "created_at": "2025-02-06T13:42:25Z"
        }
      ]
    },
    {
      "issue_number": 812,
      "title": "provides binary/installer to ease the installation/onboarding of ramalama",
      "body": "## Proposal: Provide Self-Contained Installers for Windows and macOS  \n\n### Problem  \nCurrently, Python is not installed by default on Windows and macOS. Since ramalama package requires a Python runtime, installation becomes more complex compared to a self-contained binary.  \n\n- The package is available on PyPI, but users need a proper Python installation.  \n- Homebrew can be used on macOS, but not all users have it installed.  \n- Windows users need to install Python separately before using the package.  \n\n### Suggested Solution  \nTo improve accessibility, I am thinking of:  \n\n1. A `.pkg` installer for macOS  and .exe installer for Windows\n2. A self-contained binary for Windows and macOS (with a potential startup delay due to unpacking) or a directory to unpack\n\n### Potential Approach  \nIt seems that **PyInstaller** can generate these self-contained packages out of the box. Using it to create platform-specific installers might simplify installation and adoption.  \n\n### Benefits  \n- Easier installation process without requiring users to set up Python manually  \n- Broader accessibility for non-developer users  \n- Reduces friction in adoption  \n",
      "state": "open",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-02-13T20:28:53Z",
      "updated_at": "2025-04-02T14:22:24Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/812/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/812",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/812",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:58.344192",
      "comments": [
        {
          "author": "rhatdan",
          "body": "@lsm5 I wonder if this is something we could execute via github actions, when we generate a release?",
          "created_at": "2025-02-13T21:24:21Z"
        },
        {
          "author": "ericcurtin",
          "body": "macOS is a packaging effort.\n\nCould we consider running RamaLama inside podman-machine or WSL2 for Windows? Porting it to Windows will be a significant effort.\n\nNote if we run RamaLama directly on Windows and/or macOS you lose all the container features of RamaLama, which is kind of a key goal of Ra",
          "created_at": "2025-02-13T22:17:16Z"
        },
        {
          "author": "benoitf",
          "body": "> Note if we run RamaLama directly on Windows and/or macOS you lose all the container features of RamaLama, which is kind of a key goal of RamaLama and Podman Desktop.\n\nHello, I'm not sure to follow there ? it's only a packaging thing. So python runtime is included.\nI don't see why we wouldn't be ab",
          "created_at": "2025-02-14T07:02:30Z"
        },
        {
          "author": "ericcurtin",
          "body": "> > Note if we run RamaLama directly on Windows and/or macOS you lose all the container features of RamaLama, which is kind of a key goal of RamaLama and Podman Desktop.\n> \n> Hello, I'm not sure to follow there ? it's only a packaging thing. So python runtime is included. I don't see why we wouldn't",
          "created_at": "2025-02-14T09:21:31Z"
        },
        {
          "author": "benoitf",
          "body": ">Because containers don't exist in Windows or macOS, but if you run RamaLama inside a Linux VM like podman-machine or WSL2 (WSL2 already should have the GPU passthrough necessary on WIndows) you are in a Linux environment where you can run containers.\n\nif you have a podman machine on macOS or Window",
          "created_at": "2025-02-14T09:40:57Z"
        }
      ]
    },
    {
      "issue_number": 1076,
      "title": "New OCI artifact type \"docker model runner\"",
      "body": "There's two main model as OCI artefact types outside of RamaLama, Ollama and Docker Model Runner, For Ollama we did the OCI pulling in python3 with one of the container image layers being the .gguf file. Part of this issue is to figure out what dockers format.\n\nFor Ollama we implemented the pulling of that format in python.\n\nOne of the questions is do we also pull the docker format in python3? Or do we explore pulling these formats via skopeo/podman?",
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-03-31T10:05:39Z",
      "updated_at": "2025-04-02T12:19:48Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1076/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1076",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1076",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:58.581856",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "There's a podman and a ramalama side to this:\n\nhttps://github.com/containers/podman/issues/25758",
          "created_at": "2025-04-02T12:19:46Z"
        }
      ]
    },
    {
      "issue_number": 679,
      "title": "Project Charter Outline",
      "body": " Project Charter Outline to use in creating charter for Ramalama.\n\n- Introduction/Overview\n- Responsibilities & Deliverables\n- Relevant Background (what need is being met)\n- In-Scope\n- Out of scope\n\nI am happy to take a first pass at this if the project leaders would like.\n\n\n",
      "state": "open",
      "author": "caradelia",
      "author_type": "User",
      "created_at": "2025-01-30T19:30:48Z",
      "updated_at": "2025-04-02T11:00:11Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/679/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/679",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/679",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:58.785530",
      "comments": [
        {
          "author": "rhatdan",
          "body": "@caradelia could you move forward on this?",
          "created_at": "2025-04-02T10:59:35Z"
        }
      ]
    },
    {
      "issue_number": 906,
      "title": "Cannot run converted models (imported using convert command)",
      "body": "It seems to be not possible to run model which were imported using `convert` command:\n\n```\n$ ramalama run --ngl 0 oci://localhost/pllum8b-instr-q4km:latest\nLoading modelgguf_init_from_file_impl: failed to read magic\nllama_model_load: error loading model: llama_model_loader: failed to load model from /mnt/models/model.file\n\nllama_model_load_from_file_impl: failed to load model\ninitialize_model: error: unable to load model from file: /mnt/models/model.file\n```\n\nThe same error is reported independly whether `raw` or `car` option was used:\n\n```\n$ ramalama convert --type raw file:///home/dw/.ollama/models/blobs/sha256-19314ef0159c739868860d0ee15851e7b19a0433a94c1e0afa727a8a013bd0fd pllum8b-instr-q4km\nConverting /home/dw/.local/share/ramalama/models/file/home/dw/.ollama/models/blobs/sha256-19314ef0159c739868860d0ee15851e7b19a0433a94c1e0afa727a8a013bd0fd to pllum8b-instr-q4km...\nBuilding pllum8b-instr-q4km...\n$ ramalama ls | grep pllum8b-instr-q4km\noci://localhost/pllum8b-instr-q4km:latest 19 seconds ago 790 B\n$ $ ramalama --debug run --ngl 0 oci://localhost/pllum8b-instr-q4km:latest\nrun_cmd:  podman image inspect localhost/pllum8b-instr-q4km:latest\nWorking directory: None\nIgnore stderr: False\nIgnore all: False\nCommand finished with return code: 0\nrun_cmd:  podman inspect quay.io/ramalama/rocm:0.6\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nexec_cmd:  podman run --rm -i --label ai.ramalama --name ramalama_kOB7kc5lx4 --env=HOME=/tmp --init --security-opt=label=disable --cap-drop=all --security-opt=no-new-privileges --label ai.ramalama.model=oci://localhost/pllum8b-instr-q4km:latest --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.command=run --env LLAMA_PROMPT_PREFIX=ðŸ¦­ >  --pull=newer -t --device /dev/dri --device /dev/kfd -e HIP_VISIBLE_DEVICES=0 --network none --mount=type=image,src=localhost/pllum8b-instr-q4km:latest,destination=/mnt/models,subpath=/models quay.io/ramalama/rocm:0.6 llama-run -c 2048 --temp 0.8 -v --ngl 0 /mnt/models/model.file\nLoading modelggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Ryzen Embedded R1505G with Radeon Vega Gfx, gfx902:xnack+ (0x902), VMM: no, Wave Size: 64\nllama_model_load_from_file_impl: using device ROCm0 (AMD Ryzen Embedded R1505G with Radeon Vega Gfx) - 31055 MiB free\ngguf_init_from_file_impl: failed to read magic\nllama_model_load: error loading model: llama_model_loader: failed to load model from /mnt/models/model.file\n\nllama_model_load_from_file_impl: failed to load model\ninitialize_model: error: unable to load model from file: /mnt/models/model.file\n```\n\n```\n$ ramalama convert --type car file:///home/dw/.ollama/models/blobs/sha256-19314ef0159c739868860d0ee15851e7b19a0433a94c1e0afa727a8a013bd0fd pllum8b-instr-q4km\nConverting /home/dw/.local/share/ramalama/models/file/home/dw/.ollama/models/blobs/sha256-19314ef0159c739868860d0ee15851e7b19a0433a94c1e0afa727a8a013bd0fd to pllum8b-instr-q4km...\nBuilding pllum8b-instr-q4km...\n$ ramalama ls | grep pllum8b-instr-q4km\noci://localhost/pllum8b-instr-q4km:latest 4 seconds ago  791 B   \n$ ramalama --debug run --ngl 0 oci://localhost/pllum8b-instr-q4km:latest\nrun_cmd:  podman image inspect localhost/pllum8b-instr-q4km:latest\nWorking directory: None\nIgnore stderr: False\nIgnore all: False\nCommand finished with return code: 0\nrun_cmd:  podman inspect quay.io/ramalama/rocm:0.6\nWorking directory: None\nIgnore stderr: False\nIgnore all: True\nCommand finished with return code: 0\nexec_cmd:  podman run --rm -i --label ai.ramalama --name ramalama_AoHxUZatGd --env=HOME=/tmp --init --security-opt=label=disable --cap-drop=all --security-opt=no-new-privileges --label ai.ramalama.model=oci://localhost/pllum8b-instr-q4km:latest --label ai.ramalama.engine=podman --label ai.ramalama.runtime=llama.cpp --label ai.ramalama.command=run --env LLAMA_PROMPT_PREFIX=ðŸ¦­ >  --pull=newer -t --device /dev/dri --device /dev/kfd -e HIP_VISIBLE_DEVICES=0 --network none --mount=type=image,src=localhost/pllum8b-instr-q4km:latest,destination=/mnt/models,subpath=/models quay.io/ramalama/rocm:0.6 llama-run -c 2048 --temp 0.8 -v --ngl 0 /mnt/models/model.file\nLoading modelggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Ryzen Embedded R1505G with Radeon Vega Gfx, gfx902:xnack+ (0x902), VMM: no, Wave Size: 64\nllama_model_load_from_file_impl: using device ROCm0 (AMD Ryzen Embedded R1505G with Radeon Vega Gfx) - 31055 MiB free\ngguf_init_from_file_impl: failed to read magic\nllama_model_load: error loading model: llama_model_loader: failed to load model from /mnt/models/model.file\n\nllama_model_load_from_file_impl: failed to load model\ninitialize_model: error: unable to load model from file: /mnt/models/model.file\n```\n\n```\n$ rpm -qv python3-ramalama\npython3-ramalama-0.6.2-1.fc40.noarch\n```\n\nInput file to `convert` command was obtained as described in https://github.com/containers/ramalama/issues/904.",
      "state": "open",
      "author": "dwrobel",
      "author_type": "User",
      "created_at": "2025-03-03T11:18:38Z",
      "updated_at": "2025-04-02T10:13:09Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/906/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/906",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/906",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:58.957249",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Can you run the ollama model successfully?\n\n",
          "created_at": "2025-03-10T17:41:51Z"
        },
        {
          "author": "dwrobel",
          "body": "Yep. The model is working fine in `ollama`:\n```\n$ ollama run pllum8b-instr-q4km\n>>> /show modelfile\n# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this, replace FROM with:\n# FROM pllum8b-instr-q4km:latest\n\nFROM /home/dw/.ollama/models/blobs/sha256-19314ef0159c739868860d0e",
          "created_at": "2025-03-10T17:58:17Z"
        },
        {
          "author": "rhatdan",
          "body": "```\nollama run pllum8b-instr-q4km\npulling manifest \nError: pull model manifest: file does not exist\n```",
          "created_at": "2025-03-10T18:27:11Z"
        },
        {
          "author": "dwrobel",
          "body": "I converted this model from https://huggingface.co/CYFRAGOVPL/Llama-PLLuM-8B-instruct.\nHere are the steps _(based on my notes)_:\n```\n$ git clone https://huggingface.co/CYFRAGOVPL/Llama-PLLuM-8B-instruct\n$ git clone https://github.com/ggerganov/llama.cpp.git\n# make sure you've dependencies listed in:",
          "created_at": "2025-03-10T19:31:44Z"
        },
        {
          "author": "rhatdan",
          "body": "Are you still seeing this issue?",
          "created_at": "2025-03-31T20:00:25Z"
        }
      ]
    },
    {
      "issue_number": 886,
      "title": "CERTIFICATE_VERIFY_FAILED on clean install, Mac Sonoma 14.7.4 with Podman 5.3.2",
      "body": "Installed using the curl command, when running:\n\nramalama run granite3-moe\n\ncauses:\n\nError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)>",
      "state": "open",
      "author": "utherp0",
      "author_type": "User",
      "created_at": "2025-02-26T10:53:01Z",
      "updated_at": "2025-04-02T10:12:30Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/886/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/886",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/886",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:59.195365",
      "comments": [
        {
          "author": "gadig17",
          "body": "I ran into the same issue (different model). I found a solution to it.\n\nHere are the steps for macOS:\n\n1. Open the folder /Applications/Python 3.x (x is the version you are running).\n2. Double click the Install Certificates.command. It will open a terminal and install the certificate.\n\nIt would be g",
          "created_at": "2025-02-26T19:40:24Z"
        },
        {
          "author": "utherp0",
          "body": "Perfect, fixed my issue; worth mentioning as a caveat on the README?",
          "created_at": "2025-02-27T11:13:54Z"
        },
        {
          "author": "ericcurtin",
          "body": "@utherp0 if you open the PR to mention this somewhere I'll merge it.\n\nI'm curious why I never encountered this, I'm on macOS most of the time.\n\nI'm on 15.3.1 though, my machine forces me to stay upto date.\n\n",
          "created_at": "2025-02-27T11:32:36Z"
        },
        {
          "author": "gadig17",
          "body": "I am on 15.3.1 so it looks like it is version-independent.\r\n\r\n\r\nOn Thu, Feb 27, 2025 at 6:32â€¯AM Eric Curtin ***@***.***>\r\nwrote:\r\n\r\n> @utherp0 <https://github.com/utherp0> if you open the PR to mention this\r\n> somewhere I'll merge it.\r\n>\r\n> I'm curious why I never encountered this, I'm on macOS most",
          "created_at": "2025-02-27T12:58:36Z"
        },
        {
          "author": "utherp0",
          "body": "https://github.com/containers/ramalama/pull/893",
          "created_at": "2025-02-27T13:51:12Z"
        }
      ]
    },
    {
      "issue_number": 801,
      "title": "unable to run ramalama using --runtime vllm on macOS",
      "body": "trying ramalama on macOS 15 using --runtime vllm\n\nI got \n```\nTrying to pull quay.io/modh/vllm:rhoai-2.18-cuda...\nError: choosing an image from manifest list docker://quay.io/modh/vllm:rhoai-2.18-cuda: no image found in image index for architecture \"arm64\", variant \"v8\", OS \"linux\"\n```\n\nit seems to fetch a cuda image while I am on Apple silicon.",
      "state": "open",
      "author": "benoitf",
      "author_type": "User",
      "created_at": "2025-02-13T11:05:17Z",
      "updated_at": "2025-04-02T10:07:42Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/801/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/801",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/801",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:59.399502",
      "comments": [
        {
          "author": "benoitf",
          "body": "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/index.html\n\nit's not optimal but it can run on macOS\n\n```\n$ podman run --rm -it -p 8090:8000 localhost/vllm-cpu-env --model TinyLlama/TinyLlama-1.1B-Chat-v1.0                                                             INFO 02-13 10:52:",
          "created_at": "2025-02-13T11:09:15Z"
        },
        {
          "author": "ericcurtin",
          "body": "I think you found your answer in the above doc:\n\n\"vLLM has experimental support for macOS with Apple silicon. For now, users shall build from the source vLLM to natively run on macOS.\"\n\nThis is one for the vLLM folks",
          "created_at": "2025-02-13T11:38:00Z"
        },
        {
          "author": "ericcurtin",
          "body": "A general issue around vLLM should be opened if not already, could do with some Containerfile work.\n\nllama.cpp is more suitable for macOS runtime today",
          "created_at": "2025-02-13T11:40:02Z"
        },
        {
          "author": "benoitf",
          "body": "I don't see why it's being closed as fixed\n\nThe user experience is bad. It tries to fetch an image that does not exists\n\nIt should report a good error message",
          "created_at": "2025-02-13T11:40:19Z"
        },
        {
          "author": "benoitf",
          "body": "> \"vLLM has experimental support for macOS with Apple silicon. For now, users shall build from the source vLLM to natively run on macOS.\"\n\nsome ramalama images are building llama.cpp from sources so it could also build vLLM for a later arm/macOS usage\nI'm not talking about running vLLM locally on my",
          "created_at": "2025-02-13T11:58:25Z"
        }
      ]
    },
    {
      "issue_number": 642,
      "title": "Add support for safetensors (non-GGUF) model format",
      "body": "Instead of a single model file, ramalama will need to mount and pull a directory.  It would be really nice to have `ramalama pull the/model --format  safetensors` or something like that",
      "state": "open",
      "author": "sallyom",
      "author_type": "User",
      "created_at": "2025-01-28T12:55:40Z",
      "updated_at": "2025-04-02T07:48:56Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/642/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/642",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/642",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:59.604052",
      "comments": [
        {
          "author": "bmahabirbu",
          "body": "Great idea, if we're using vllm (or others that support safe tensor) as the runtime we can mount the folder as is. If we are using llama.cpp we can convert the safetensors to a gguf then mount the file as usual! ",
          "created_at": "2025-02-06T19:08:39Z"
        },
        {
          "author": "ericcurtin",
          "body": "I agree we should implement this. I would say even drop the \"--format safetensors\", I think it would not be too hard to automatically detect a certain model is safetensors.",
          "created_at": "2025-02-06T22:39:46Z"
        },
        {
          "author": "rhatdan",
          "body": "SGTM",
          "created_at": "2025-02-07T10:34:03Z"
        }
      ]
    },
    {
      "issue_number": 607,
      "title": "Implement OpenVINO",
      "body": null,
      "state": "open",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-01-21T16:16:52Z",
      "updated_at": "2025-04-02T07:46:59Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/607/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/607",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/607",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:59.796955",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "@slemeur feel free to fill this with details\n",
          "created_at": "2025-01-21T16:17:39Z"
        },
        {
          "author": "ghchris2021",
          "body": "+1, IMO yes, this would be great to provide support for this existing framework / ecosystem of ML model running / serving.\n\nThe existing openvino tools are well maintained and capable and are Docker friendly but IMO it could be nice to see them integrate into the overarching capabilities that ramala",
          "created_at": "2025-02-23T00:46:01Z"
        },
        {
          "author": "rhatdan",
          "body": "OpenVINO is now packaged in our images.  Not sure if we want to turn this on directly in RamaLama\n\nramalama --runtime openvino?\n\nWe would need to add a script to execute this correctly.",
          "created_at": "2025-04-01T18:56:24Z"
        }
      ]
    },
    {
      "issue_number": 184,
      "title": "Add ramalama serve --generate compose MODEL which would generate a docker-compose file for running AI Model Service.",
      "body": "`ramalama serve --generate quadlet` was just added, and generates a quadlet which outputs a quadlet file to run the model as a service.",
      "state": "open",
      "author": "rhatdan",
      "author_type": "User",
      "created_at": "2024-09-24T21:21:33Z",
      "updated_at": "2025-04-01T22:01:45Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/184/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/184",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/184",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:50:59.975541",
      "comments": [
        {
          "author": "rhatdan",
          "body": "We now have pretty good support for Quadlets and Kubernetes.\n",
          "created_at": "2024-11-06T20:37:32Z"
        },
        {
          "author": "abhibongale",
          "body": "Hi @rhatdan, this issue is still open? If yes, can you please suggest me from where should I start looking? \n\nand brief summary about the ticket and what is expected from the changes will be great. It's pretty clear from title but still don't want to diverge from expectation.",
          "created_at": "2025-04-01T09:59:40Z"
        },
        {
          "author": "rhatdan",
          "body": "The issue is still open, but I am not sure it is possible yet.  Quadlet and K8s supports the concept of mounting images into containers.\n\n--mount type=image,source=quay.io/ramalama/granite,destination=/mnt/models;ro \n\nDocker is just getting this functionality now, so not sure if Compose has the abil",
          "created_at": "2025-04-01T13:14:38Z"
        }
      ]
    },
    {
      "issue_number": 1086,
      "title": "Any plans to incorporate ipex-llm",
      "body": "I see the support for openvino, are there any plans to support https://github.com/intel/ipex-llm\nIt has been reported to me that ollama+ipex-llm is very fast on intel ultra iGpu",
      "state": "open",
      "author": "mecattaf",
      "author_type": "User",
      "created_at": "2025-03-31T20:30:04Z",
      "updated_at": "2025-04-01T21:59:33Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/1086/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/1086",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/1086",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:51:00.179986",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "PRs welcome @mecattaf ",
          "created_at": "2025-03-31T20:45:09Z"
        }
      ]
    },
    {
      "issue_number": 868,
      "title": "Idea: Implement configurations encompassing huggingface transformers, diffusers frameworks and underlying back-end options.",
      "body": "\"The RamaLama project's goal is to make working with AI boring through the use of OCI containers....\"\n\nI'm very impressed by the capability / flexibility offered by ramalama and the compelling opportunity which it has to fill a presently large gap in facilitating ML model acquisition / use / sysadmin / security.\n\nI have enjoyed using several of the technologies involved ('containers' ecosystem, podman, etc. etc.) and also ML models / frameworks such as llama.cpp, openvino, huggingface transformers, huggingface diffusers, pytorch, OCI, CDI, et. al. I think there's great need and opportunity to have such synergistic unification of tools (containers, linux tool suite + model resources + inference framework resources to facilitate / enable \"making it boring\" to enable ML models as 'applications' use cases.  But of course there's enough complexity of implementation details that it makes the end user's use case (something everybody needs) hard enough to do sysadmin / devops / security for that only few developers reinvent the wheel to set this all up bespoke.  ramalama is on track for solving a lot of the \"implementation details\" and encapsulating them.\n\nIf huggingface transformers was enabled as a basis inference framework it would provide essentially instant (on release day) support for something like 95%+ of open weights LLM models with excellent support.  This broadens the reach of e.g. llama.cpp, ollama, onnx, openvino since only a small subset of released models are supported by such inference engines (significant independent community / project developer engineering is needed for some / many models).  It also provides a faster path for users to use the models ASAP as opposed to possibly having no ramalama / llama.cpp / ollama etc. support to run them for perhaps many months for the subset of models that are ultimately supported by such non-HF-transformers / pytorch / ... based engines.\n\nUse case examples / documentation etc. for how to use almost all newly released models is almost ubiquitously provided first, foremost, and often exclusively in the context of \"how to use this model with huggingface transformers\".\n\nThe python based HF framework pipeline etc. for a given model / class of models usually exposes all major or possible / supported use cases for inferencing the models.\n\nThat's unlike e.g. llama.cpp etc. runtimes where it's more common to have some bumpy roads in terms of prompt templates, tokenization, metadata, quantization vs. quality etc. since non HF transformers engines are not usually directly supported by the model makers and the conversions / mappings / derivative support can be complex.\n\n\"pip\" or similar installations are able to be used from trustworthy upstream repos / vendors to install the dependencies to run models with HF transformers, acquire back end configurations needed to accelerate inference for nvidia / amd / intel et. al. GPUs.  The required packages / modules are commonly available in major linux distribution repos or from official and major upstream vendor sources (vendor's container registry images, vendor's sites / hubs...).\n\nThe same things said above about the benefits for running ML models based on HF transformers apply but moreso for huggingface diffusers and the diffusion (et.al.) models it supports.  One does not see e.g. llama.cpp AFAIK supporting diffusion model inference / serving, and HF diffusers is probably the primary (besides ONNX or apple / qualcomm / samsung specific options) way open diffusion models are able to be inferenced.\n\nSo it could be a low-effort integration to package / containerize HF transformers + HF diffusers and enable relevant backend components (e.g. pytorch, openvino) to enable CPU / GPU based inference on multiple platforms but garner the benefit of \nramalama's unified command line tools / OCI / container support value added etc.\n\n\n\n> As mentioned the openvino model format / support and inference engine is also able to be\n> leveraged by the very prominent and broadly relevant / contemporary huggingface transformers / diffusers model inference framework projects as one possible back end via huggingface's 'optimum-intel' project and via pytorch support for an alternative backend (which in turn supports xpu (intel GPUs) directly, cuda / nvidia GPUs, and many others.\n> \n> So as an intersecting tangent (distinct but overlapping feature) I think it's very highly commendable to consider looking at providing some ramalama containerized inference configurations for HF diffusers, HF transformers and the ability of those to \n> work with the various backend inference chains / platforms they indirectly support e.g. optimum-intel, openvino, pytorch, onnx, etc.\n> \n> https://huggingface.co/docs/optimum/main/en/intel/index\n> \n> https://huggingface.co/docs/transformers/perf_infer_gpu_one\n> \n> IME most models (LLMs, diffusion models) released in the past year+ time frames have very prompt if not same release day support for inference using HF transformers / diffusers and\n> very commonly underlying pytorch based inference execution support.  This inference opption is usually among the best documented, best supported, and most flexible wrt. configuration of all others.  \n> \n> Other inference engine / framework based support / configurations for released models may in a moderately small subset of model type / release cases may eventually (often months later) be independently developed / enabled e.g. llama.cpp, but in many cases many model categories and specific releases simply are historically unlikely to be supported even after 1-2 year+ time frames by some inference runtimes such as llama.cpp, onnx, openvino, whereas usually they'll have excellent \"at launch\" pytorch / huggingface inference support & documentation.\n> \n> So for those reasons I can envision it to be a great boon in capability and addressing broad / contemporary user use cases for ramalama (\"The RamaLama project's goal is to make working with AI boring through the use of OCI containers.\") to support inference / serving configurations \n> based on huggingface / pytorch / openvino framework / engine options all of which can\n> very well coexist with the OCI / container / podman / command set etc. key enabling aspects of ramalama.\n> \n> https://huggingface.co/docs/transformers/index\n> \n> https://huggingface.co/docs/diffusers/index\n> \n>  \n\n _Originally posted by @ghchris2021 in [#607](https://github.com/containers/ramalama/issues/607#issuecomment-2676473660)_",
      "state": "open",
      "author": "ghchris2021",
      "author_type": "User",
      "created_at": "2025-02-23T01:31:50Z",
      "updated_at": "2025-04-01T21:58:12Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/868/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/868",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/868",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:51:00.360851",
      "comments": [
        {
          "author": "aguadoenzo",
          "body": "Seconding this. There are many platforms that will run popular models but none that support transformers & sentence-transformers as a fallback (Ollama claims to support it, but AFAIK, only if it's in a GUFF format which llama.cpp understands).\nAs it's a fallback, it does not even need to be super op",
          "created_at": "2025-02-27T08:08:07Z"
        },
        {
          "author": "ericcurtin",
          "body": "It's certainly something we are interested in @engelmi was looking into something similar. PRs welcome for this.",
          "created_at": "2025-02-27T09:56:26Z"
        }
      ]
    },
    {
      "issue_number": 867,
      "title": "File upload ",
      "body": "Would love the ability to upload files for evaluation in the webserver.\n\nI would love to help with this when I have time.",
      "state": "open",
      "author": "GeoDerp",
      "author_type": "User",
      "created_at": "2025-02-23T01:16:17Z",
      "updated_at": "2025-04-01T21:58:11Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/867/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/867",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/867",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:51:00.568164",
      "comments": [
        {
          "author": "rhatdan",
          "body": "Not sure what you mean?  We can currently push models to OCI, do you want to push models to Huggingface and Ollama?",
          "created_at": "2025-02-23T10:45:02Z"
        },
        {
          "author": "GeoDerp",
          "body": "Sorry about the confusion. ðŸ˜…\nOn the `olama serve` webserver. I would love the ability to input .txt and .pdf files into the prompt with the text. ",
          "created_at": "2025-02-23T11:23:37Z"
        },
        {
          "author": "rhatdan",
          "body": "You mean `ramalama serve`, but would love to see a PR to see what you would like.\n",
          "created_at": "2025-02-23T11:56:23Z"
        },
        {
          "author": "ericcurtin",
          "body": "We can do things like:\n\n```\ncat some-textfile.txt | ramalama run \"Some prompt:\"\n```\n\nalready.\n\nNot sure if that's the kinda thing you are looking for.\n\n.pdf not quite there yet...\n",
          "created_at": "2025-02-23T13:19:49Z"
        },
        {
          "author": "GeoDerp",
          "body": "> We can do things like:\n> \n> ```\n> cat some-textfile.txt | ramalama run \"Some prompt:\"\n> ```\n> \n> already.\n> \n> Not sure if that's the kinda thing you are looking for.\n> \n> .pdf not quite there yet...\n> \n\nYeah totally! \nBut yes, the ability to parse multiple files / pdfs was the idea. â¤ï¸",
          "created_at": "2025-02-23T21:06:21Z"
        }
      ]
    },
    {
      "issue_number": 539,
      "title": "ramalama client",
      "body": "As a test tool for \"ramalama serve\" (would also be useful as a follow on feature to connect to remote endpoints like openai, perplexity, etc.).\n\nShould behave identically to \"ramalama run\", except it does not inference within the client process, it should send a request to the server process and stream the results back.\n\nCan exist here as python or, it could also be written in C++ and eventually contributed back to llama.cpp upstream.\n",
      "state": "closed",
      "author": "ericcurtin",
      "author_type": "User",
      "created_at": "2025-01-07T10:47:25Z",
      "updated_at": "2025-03-22T03:28:53Z",
      "closed_at": "2025-03-22T03:28:53Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/containers/ramalama/issues/539/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/containers/ramalama/issues/539",
      "api_url": "https://api.github.com/repos/containers/ramalama/issues/539",
      "repository": "containers/ramalama",
      "extraction_date": "2025-06-22T00:51:02.537156",
      "comments": [
        {
          "author": "ericcurtin",
          "body": "Probably makes more sense as a ramalama-only tool in python as the official openai library is in python:\n\nhttps://github.com/openai/openai-python",
          "created_at": "2025-01-07T10:52:05Z"
        },
        {
          "author": "ericcurtin",
          "body": "Since this will rely on python library outside the standard python library, by default we should execute this inside the container",
          "created_at": "2025-01-07T11:10:28Z"
        },
        {
          "author": "ericcurtin",
          "body": "This will also help us test vllm, since vllm only operates as a server",
          "created_at": "2025-01-07T11:15:47Z"
        },
        {
          "author": "rhatdan",
          "body": "Is there an Open Source Python chat tool available for this already that we could vendor into RamaLama?",
          "created_at": "2025-02-19T14:13:38Z"
        },
        {
          "author": "ericcurtin",
          "body": "@rhatdan this library makes the most sense to build from:\n\nhttps://github.com/openai/openai-python\n\nwhat better place to start an OpenAI client than the OpenAI library itself.",
          "created_at": "2025-02-19T14:15:22Z"
        }
      ]
    }
  ]
}