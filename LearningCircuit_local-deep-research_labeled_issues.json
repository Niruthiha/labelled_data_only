{
  "repository": "LearningCircuit/local-deep-research",
  "repository_info": {
    "repo": "LearningCircuit/local-deep-research",
    "stars": 2887,
    "language": "Python",
    "description": "Local Deep Research is an AI-powered assistant that transforms complex questions into comprehensive, cited reports by conducting iterative analysis using any LLM across diverse knowledge sources inclu",
    "url": "https://github.com/LearningCircuit/local-deep-research",
    "topics": [
      "academia",
      "ai",
      "arxiv",
      "brave",
      "deep-research",
      "gemma",
      "llama",
      "local",
      "local-deep-research",
      "local-llm",
      "mistral",
      "pubmed",
      "research",
      "research-tool",
      "retrieval-augmented-generation",
      "search",
      "search-engine",
      "searxng",
      "self-hosted",
      "wikipedia"
    ],
    "created_at": "2025-02-09T15:41:32Z",
    "updated_at": "2025-06-21T20:39:18Z",
    "search_query": "local llm language:python stars:>2",
    "total_issues_estimate": 50,
    "labeled_issues_estimate": 46,
    "labeling_rate": 93.3,
    "sample_labeled": 14,
    "sample_total": 15,
    "has_issues": true,
    "repo_id": 929868765,
    "default_branch": "main",
    "size": 4879
  },
  "extraction_date": "2025-06-22T00:44:42.815175",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 93,
  "issues": [
    {
      "issue_number": 509,
      "title": "Sources not included in results, but they are referenced.",
      "body": "**Describe the bug**\nWhen doing Detailed research with \"Focused Iteration\" strategy, sources are referenced but not included in the results.\n\n**To Reproduce**\nSteps to reproduce the behavior:\nQuery: \"Put together a list of genes that express in old neurons, but do not express in healthy adult neurons.\"\nProvider: Ollama (local)\nLanguage Model: Qwen3:30b-a3b\nSearch Engine: SearXNG\nSearch Strategy: Focused Iteration\n\n\n**Expected behavior**\nExpected to be able to look at cited sources.\n\n**System Information:**\n - OS: Windows, WSL2, Ubuntu 22.\n - Python Version: Docker Image\n - Model Used: Qwen3:30b-a3b\n - Hardware Specs: nVidia RTX 5090\n\n**Additional context**\nI ran the exact same query with both simple and detailed, using \"Source Based\" and both of them included sources in the output.  All other settings were the same except this.\n\n**Output/Logs**\n[research.md](https://github.com/user-attachments/files/20847045/research.md)\n[research_logs.json](https://github.com/user-attachments/files/20847038/research_logs_3.json)\n",
      "state": "closed",
      "author": "MicahZoltu",
      "author_type": "User",
      "created_at": "2025-06-21T14:41:35Z",
      "updated_at": "2025-06-21T20:17:32Z",
      "closed_at": "2025-06-21T20:17:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/509/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/509",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/509",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:17.640427",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hey @MicahZoltu,\n\nThank you for including logs!\n\nThis appears to be an issue with the detailed report functionality and its handling of citations. By any chance, have you noticed whether this issue happens consistently, or just with certain queries? Does it happen with the default \"source-based\" sea",
          "created_at": "2025-06-21T18:32:48Z"
        },
        {
          "author": "djpetti",
          "body": "Okay, based on my own tests, this only seems to happen with the focused strategy, but it happens consistently. I will look into it further.",
          "created_at": "2025-06-21T18:40:14Z"
        },
        {
          "author": "MicahZoltu",
          "body": "It doesn't occur with the default source-based strategy, even with the exact same query and other settings.",
          "created_at": "2025-06-21T19:03:56Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Maybe with num_ctx option for local models that is currently in dev branch it will get better.",
          "created_at": "2025-06-21T20:04:05Z"
        }
      ]
    },
    {
      "issue_number": 494,
      "title": "Simpler installation options",
      "body": "We need simpler installation options especially for windows.",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-06-12T18:15:48Z",
      "updated_at": "2025-06-21T20:13:55Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "help wanted",
        "question"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/494/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/494",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/494",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:17.881284",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Does anyone have any idea what would be a good approach for installing the software except docker?",
          "created_at": "2025-06-12T18:16:53Z"
        }
      ]
    },
    {
      "issue_number": 499,
      "title": "ÊòØÂê¶ÂèØ‰ª•Âú®ÂÆåÂÖ®ÂÜÖÁΩëÁéØÂ¢É‰∏ãÂ∑•‰Ωú (Can you work in a full intranet environment?)",
      "body": "ÂÜÖÁΩëÁéØÂ¢É Êó†Ê≥ïËøûÊé•Â§ñÁΩë Â∏åÊúõÂèØ‰ª•ÂØπÊï∞ÊçÆÂ∫ìÊï∞ÊçÆ‰ª•ÂèäÊñáÊ°£Êï∞ÊçÆËøõË°åÊ∑±Â∫¶Á†îÁ©∂ ÂèØ‰ª•Âêó\n(Intranet environment Unable to connect to the extranet Want to conduct in-depth research on database data and document data?)",
      "state": "open",
      "author": "mxz112233445566778899",
      "author_type": "User",
      "created_at": "2025-06-16T05:22:39Z",
      "updated_at": "2025-06-21T20:12:36Z",
      "closed_at": null,
      "labels": [
        "question",
        "docs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/499/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/499",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/499",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:18.119579",
      "comments": [
        {
          "author": "djpetti",
          "body": "It should be possible to do this using [local document search](https://github.com/LearningCircuit/local-deep-research/wiki/Configuring-Local-Search) and configuring an LLM provider that works without external internet access, such as Ollama. It might be challenging to get LDR installed without inter",
          "created_at": "2025-06-16T13:11:35Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Sorry we dont have chinese docs yet :(",
          "created_at": "2025-06-16T15:58:20Z"
        },
        {
          "author": "mxz112233445566778899",
          "body": "\n\n\n> It should be possible to do this using [local document search](https://github.com/LearningCircuit/local-deep-research/wiki/Configuring-Local-Search) and configuring an LLM provider that works without external internet access, such as Ollama. It might be challenging to get LDR installed without ",
          "created_at": "2025-06-17T08:29:02Z"
        },
        {
          "author": "djpetti",
          "body": "Hmm, if it's really too large to go in a vector database, then there's unfortunately not a lot we can do. Local document search depends on indexing all the documents into a vector DB. If the issue is that this indexing process would take too long, you can set Ollama as your embedding provider and pe",
          "created_at": "2025-06-17T13:12:50Z"
        },
        {
          "author": "LearningCircuit",
          "body": "There is also elastic search added by the nice Chinese contributor ",
          "created_at": "2025-06-17T15:18:38Z"
        }
      ]
    },
    {
      "issue_number": 506,
      "title": "Error getting full content",
      "body": "When performing a quick summary search using SearXNG, the following error comes up in the logs. It seems like the _get_full_content is missing from the full_search class. Any ideas on this?\n\nI am on LDR 0.5.7 in a Conda environment.\n\n\n`results_with_content = self.full_search._get_full_content(\\n                           ‚îÇ    ‚îî <local_deep_research.web_search_engines.engines.full_search.FullSearchResults object at 0x123d8d7c0>\\n                           ‚îî <local_deep_research.web_search_engines.engines.search_engine_searxng.SearXNGSearchEngine object at 0x1239b06b0>\\n\\nAttributeError: 'FullSearchResults' object has no attribute '_get_full_content'\\n\"`",
      "state": "open",
      "author": "xybernaut",
      "author_type": "User",
      "created_at": "2025-06-20T04:21:33Z",
      "updated_at": "2025-06-21T20:08:38Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/506/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/506",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/506",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:18.354055",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Maybe disable full search (activate setting \"snippets only\")?\n\nNot sure if this solves your problem tbh it is more guessing.",
          "created_at": "2025-06-21T11:10:14Z"
        },
        {
          "author": "djpetti",
          "body": "Yes, this is a known issue. Searching with full content is currently broken for (at least) SearXNG. On the other hand, it usually doesn't produce better results anyway. Activating the \"search snippets only\" setting should solve it",
          "created_at": "2025-06-21T12:14:02Z"
        },
        {
          "author": "xybernaut",
          "body": "Thanks for the reply. \n\nIs ‚Äúgetting full content‚Äù meant to scrape the content of each page of the search result, convert it to markdown and pass it to the LLM for the generation of the report?",
          "created_at": "2025-06-21T13:19:10Z"
        },
        {
          "author": "djpetti",
          "body": "When getting the full content is enabled, it will attempt to extract the entire page content and feed it to the LLM as part of the report generation process. Otherwise, it will just use a small excerpt.\n\nThe basic idea here is reasonable, but in practice, it doesn't seem to work well. This is probab",
          "created_at": "2025-06-21T18:27:18Z"
        },
        {
          "author": "xybernaut",
          "body": "Gotcha. I can imagine if it simply fed the content of each page to the context for the LLM it would quickly fill up the context window and it would also run pretty slowly especially if you are hosting the LLM locally. Also if you feed a long context to an LLM it tends to forget/ignore details. So ye",
          "created_at": "2025-06-21T18:47:26Z"
        }
      ]
    },
    {
      "issue_number": 512,
      "title": "docker-compose down (and presumably docker stop) times out.",
      "body": "**Describe the bug**\nDoing a `docker-compose-down`, and I presume (but haven't tested) `docker container stop`, results in a timeout instead of a clean exit.  Docker-compose (and docker stop) will initially send `SIGTERM` to process 1 in the container, but after 10 seconds if the primary container process hasn't ended it will then forcefully destroy the container.  https://docs.docker.com/reference/cli/docker/container/stop/\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. `docker-compose-up` with a compose file that includes `ollama`, `searxng`, and `deep-research`.\n2. Wait for everything to load and work properly.\n3. `docker-compose stop`\n4. Notice that it takes just over 10 seconds to stop every time.\n\n**Expected behavior**\nThe container cleanly ends in under 10 seconds so that it doesn't get hard terminated by docker-compose.\n\n**System Information:**\n - OS: Windows, WSL2, Ubuntu 22\n - Python Version: Docker\n - Model Used: Qwen3:30b-a3b\n - Hardware Specs: nVidia RTX 5090\n",
      "state": "open",
      "author": "MicahZoltu",
      "author_type": "User",
      "created_at": "2025-06-21T19:03:06Z",
      "updated_at": "2025-06-21T19:54:58Z",
      "closed_at": null,
      "labels": [
        "bug",
        "needs-replication"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/512/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/512",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/512",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:18.532456",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @MicahZoltu,\n\nCan you share your `docker-compose.yml` file?",
          "created_at": "2025-06-21T19:04:52Z"
        },
        {
          "author": "MicahZoltu",
          "body": "```yml\nname: 'local-ai'\n\nservices:\n\n# I also have openwebui, but left it out of this for brevity as I'm pretty sure it isn't relevant and my config for it is massive\n#  openwebui:\n#    ...\n\n  ollama:\n    image: 'ollama/ollama:0.7.1@sha256:236582b2cc09b52c663ae6f2481cfe1cbe30bea2875f0f5a7ffdeca881d72",
          "created_at": "2025-06-21T19:06:41Z"
        },
        {
          "author": "djpetti",
          "body": "Hi @MicahZoltu,\n\nI tried your compose file and did not encounter any issues with LDR failing to exit. Can you perhaps try removing *everything* from the compose file except LDR and see if the issue persists?",
          "created_at": "2025-06-21T19:54:58Z"
        }
      ]
    },
    {
      "issue_number": 511,
      "title": "max_tokens cannot be set to default vaule in UI.",
      "body": "**Describe the bug**\nAttempting to set the `max_tokens` variable via the UI says it must be between 100 to 4096, but the default setting is 30,000.\n\n![Image](https://github.com/user-attachments/assets/9459a454-2422-4564-adb8-bf1916607d7b)\n![Image](https://github.com/user-attachments/assets/8d24e6fb-57f6-438f-ab95-1924f775e8af)\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run a clean docker container with default settings.\n2. Look at the configuration JSON in the UI.\n3. Notice that max_tokens is set to 30,000\n4. Try to change the value in the settings UI to 29,999 and then back to 30,000\n5. Notice you get an error.\n\n**Expected behavior**\nThe default value is a valid setting in the UI.\n\n**System Information:**\n - OS: Windows 11, WSL2, Ubuntu 22\n - Python Version: Docker image\n - Model Used: Qwen3:30b-a3b\n - Hardware Specs: nVidia RTX 5090",
      "state": "closed",
      "author": "MicahZoltu",
      "author_type": "User",
      "created_at": "2025-06-21T18:42:11Z",
      "updated_at": "2025-06-21T19:22:03Z",
      "closed_at": "2025-06-21T19:22:03Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/511/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/511",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/511",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:18.726149",
      "comments": [
        {
          "author": "djpetti",
          "body": "Whoops, this is just a typo in our settings specification. I will update.",
          "created_at": "2025-06-21T19:00:46Z"
        }
      ]
    },
    {
      "issue_number": 239,
      "title": "Progress bar is bugged for detailed reports",
      "body": "**Describe the bug**\nWhen generating a detailed report, the progress bar appears to go to 100% after the initial search completes. Then it stays there while the report sections are being generated.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Start the LDR web interface.\n2. Create a detailed report.\n\n**Expected behavior**\nThe progress bar should accurately encompass the entire report generation process.\n\n**System Information:**\n - OS: Ubuntu 22.04\n - Python Version: 3.12\n - Model Used: llama-3.3-70b-instruct\n - Hardware Specs: 32 GB of RAM, no GPU\n",
      "state": "open",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-28T15:42:02Z",
      "updated_at": "2025-06-21T18:58:46Z",
      "closed_at": null,
      "labels": [
        "bug",
        "enhancement",
        "rc/0.3.0"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/239/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/239",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/239",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:18.927350",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "This is not as straight forward to fix as one might think, because you cannot know how long the report runs",
          "created_at": "2025-04-28T22:46:03Z"
        },
        {
          "author": "djpetti",
          "body": "In that case, we probably at least want to improve the UI to communicate that there us an indeterminate amount of time left ",
          "created_at": "2025-04-29T11:50:21Z"
        },
        {
          "author": "taoeffect",
          "body": "As this issue is related to #324, I've given it some thought: one thing you could do maybe would be to give the user some ability to control how long the LLM has to deliver its report.\n\nFor example, there could be two UI elements:\n\n1. The first is a slider or dropdown that says how long the AI has t",
          "created_at": "2025-05-14T03:50:59Z"
        },
        {
          "author": "djpetti",
          "body": "Not a bad idea...\n\nPlanning for a specific amount of processing time might be a bit beyond our capabilities at the moment üòÉ \nThe \"stop now\" feature could potentially work though.",
          "created_at": "2025-05-14T22:09:14Z"
        },
        {
          "author": "MicahZoltu",
          "body": "Is the set of sections known in advance, or after some initial set of queries and processing?  Once the set of sections is known, could it then give a more accurate time estimate?",
          "created_at": "2025-06-21T18:54:35Z"
        }
      ]
    },
    {
      "issue_number": 500,
      "title": "Cannot set context length for Ollama model",
      "body": "**Describe the bug**\nI use Ollama as the LLM for my research, and have set the Context Window Size at 32768 tokens. However, when I start research, Ollama loads the model with the default context length (4096).\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Set Context Window Size to 32768\n2. Run new research\n3. Check Ollama log and see check the context size when the model is being loaded (--ctx-size)\n\n![Image](https://github.com/user-attachments/assets/fd8a2536-1bd8-48d1-b81e-c6292a065ef8)\n\n![Image](https://github.com/user-attachments/assets/cef148cb-2f92-48c8-be66-690f2e2755ca)\n",
      "state": "open",
      "author": "theodorevo",
      "author_type": "User",
      "created_at": "2025-06-18T16:38:00Z",
      "updated_at": "2025-06-19T03:20:28Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/500/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/500",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/500",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:19.133345",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "@theodorevo we implemented this already in dev branch. It will be released soonish. The num_ctx is not set by context window parameter yet.",
          "created_at": "2025-06-18T17:06:46Z"
        },
        {
          "author": "theodorevo",
          "body": "Wonderful. Looking forward to the new release!",
          "created_at": "2025-06-19T03:20:28Z"
        }
      ]
    },
    {
      "issue_number": 492,
      "title": "This is awesome. But one thng that openai's deep research does better is responding to ouput instructions. Just try a search like this:",
      "body": "This is awesome. But one thng that openai's deep research does better is responding to ouput instructions. Just try a search like this:\r\nSearch for childrenfriendly accommodations near Vienna/Austria that are easy reachable without a car. Check for the availability of a room for 2 adults and a 5 year old in the last week of june. Generate a table as response with the most important facst like prize, way to get there (and duration), list the attraktions (also the ones for kids). \r\n\r\nThere is never a table in my research results (using [llama 4:16-17b](llama4:16x17b)\r\n\r\n_Originally posted by @ArchonMegalon in https://github.com/LearningCircuit/local-deep-research/discussions/474_",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-06-11T21:17:52Z",
      "updated_at": "2025-06-12T13:42:20Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/492/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/492",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/492",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:19.337475",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @ArchonMegalon,\n\nThank you for your suggestion. We have, in fact, gotten it to follow output instructions before, but the finicky nature of LLMs makes it difficult to guarantee any behavior. Have you tried with different LLMs? That can have a significant effect on the quality of your output.\n\nIn ",
          "created_at": "2025-06-12T13:42:20Z"
        }
      ]
    },
    {
      "issue_number": 493,
      "title": "search issues",
      "body": "Spot on!\r\n\r\nI did a new research and this time it told me that it found 0 results.\r\n\r\nThen I went through the settings and found that for my searx engine, a wrong URL was set (it defaults to localhost, but mine is on another machine), despite my providing the correct URL in the .env file for docker compose to pick up. \r\n\r\nSo I changed the URL in the settings. But this had no effect. It still found 0 result.\r\n\r\nSo I switched the search engine in settings to SerpAPI and provided my API key. Again, no results found.\r\n\r\nWhat might be going wrong here?\r\n\r\n_Originally posted by @gitwittidbit in https://github.com/LearningCircuit/local-deep-research/discussions/460#discussioncomment-13413240_",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-06-11T22:08:31Z",
      "updated_at": "2025-06-12T13:39:15Z",
      "closed_at": null,
      "labels": [
        "needs-replication"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/493/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/493",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/493",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:19.513022",
      "comments": [
        {
          "author": "djpetti",
          "body": "I @gitwittidbit,\n\nCan you share a log with us from  a failed search run? You should be able to use the \"Download Logs\" button in the log panel.",
          "created_at": "2025-06-12T13:39:15Z"
        }
      ]
    },
    {
      "issue_number": 487,
      "title": "Research Failed: Technical error: Error: max_workers must be greater than 0",
      "body": "Hello! I am encountering an odd, somewhat inconsistent error after a fresh install. \n\nI ran the installer with default parameters as indicated on the github page:\n\n# Step 1: Pull and run SearXNG for optimal search results\ndocker pull searxng/searxng\ndocker run -d -p 8080:8080 --name searxng searxng/searxng\n\n# Step 2: Pull and run Local Deep Research (Please build your own docker on ARM)\ndocker pull localdeepresearch/local-deep-research\ndocker run -d -p 5000:5000 --network host --name local-deep-research localdeepresearch/local-deep-research\n\n# Start containers - Required after each reboot (can be automated with this flag in run command --restart unless-stopped)\ndocker start searxng\ndocker start local-deep-research\n\nsince I'm just running some x64 hardware I thought it would be fine. \n\nsearxng is running\nollama is running\nthe deep research backend/frontend are both running\n\nhowever when I ask certain queries (ie: Query: tell me how the soviet union collapsed)\n\nI get a long error\n\nQuick Research Summary\n\nQuery: tell me how the soviet union collapsed\n‚ö†Ô∏è Research Failed\n\nError Type: LLM Service Error\n\nWhat happened: The LLM failed to generate search questions. This usually means the LLM service isn‚Äôt responding properly.\n\nTry this:\n\n    Check if your LLM service (Ollama/LM Studio) is running\n    Restart the LLM service\n    Try a different model\n\nTechnical error: Error: max_workers must be greater than 0\n\nFor detailed error information, scroll down to the research logs and select ‚ÄúErrors‚Äù from the filter.\n\nand there is no output in research logs at all (do I need to enable them?)\n\nAlso other questions don't have any issues (ie: What is AI? produces coherent results, albeit I didn't read them.)\n\nI also have the JSON I am using to run this bad boy, i did change the deepseek-r1:8b name as I thought that was the issue, but it doesn't seem to have changed anything. \n\nSystem specs\n\nOS Arch linux, plasma, wayland\nCPU intel core ultra 7\nGPU nvidia 3070ti\n\nalthough I'm not sure how relevant those are here.\n\n[deepresearch.json](https://github.com/user-attachments/files/20682771/deepresearch.json)",
      "state": "closed",
      "author": "noah-bartlett",
      "author_type": "User",
      "created_at": "2025-06-11T01:50:59Z",
      "updated_at": "2025-06-12T13:37:25Z",
      "closed_at": "2025-06-12T13:37:25Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/487/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/487",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/487",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:19.681234",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hello,\n\nAs the error implies, this is usually an LLM connection issue. However, the fact that it works for some searches and not others doesn't fit that pattern. \n\nThe lack of logs is concerning. (might be another instance of #484?) Do you see any output in the terminal?",
          "created_at": "2025-06-11T02:12:07Z"
        },
        {
          "author": "noah-bartlett",
          "body": "It does look like it is an instance of #484 as I also dont have almost anything in the way of docker logs either. \nThere is absolutely nothing in the terminal, looks just like that. \n\nmy first inference was that the reference for the model wasn't formatted correctly, so I updated it in the JSON - bu",
          "created_at": "2025-06-11T02:40:29Z"
        },
        {
          "author": "djpetti",
          "body": "So, without logs, we're basically flying blind here. I think we need a resolution for #484, and then we can tackle this issue. Sorry about that",
          "created_at": "2025-06-11T02:43:37Z"
        },
        {
          "author": "djpetti",
          "body": "> 2025-06-11 01:01:03.075 | WARNING | local_deep_research.web.database.schema_upgrade:run_schema_upgrades:99 - Database not found at /install/.venv/lib/python3.13/src/data/ldr.db\n\nThis does not look happy. Can you `docker exec` into that container real quick and check if that file exists?\n\nAlso, do ",
          "created_at": "2025-06-11T02:48:22Z"
        },
        {
          "author": "noah-bartlett",
          "body": "here you go - looks like the whole src folder isn't there\n‚îå‚îÄ‚îÄ(noah ·ê∑ arklinux)-[~]\n‚îî‚îÄ$ sudo  docker exec -it local-deep-research sh\n# ls -l /install/.venv/lib/python3.13/src/data/ldr.db\nls: cannot access '/install/.venv/lib/python3.13/src/data/ldr.db': No such file or directory\n# ls -l /install   \nt",
          "created_at": "2025-06-11T03:28:24Z"
        }
      ]
    },
    {
      "issue_number": 484,
      "title": "Logs not showing",
      "body": "Hey, the logs are not showing while research is being conducted. Also, logs not appearing in the research report itself at the very bottom of the report. everything else is working perfectly searxng, report, etc\n\n![Image](https://github.com/user-attachments/assets/0a525145-3c54-4ed0-b32e-55853a09557a)\n![Image](https://github.com/user-attachments/assets/4c1277a9-7829-4ffa-a25c-1f24b04be7fd)",
      "state": "closed",
      "author": "bigZos",
      "author_type": "User",
      "created_at": "2025-06-10T21:29:29Z",
      "updated_at": "2025-06-11T17:14:33Z",
      "closed_at": "2025-06-11T16:53:25Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 19,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/484/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/484",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/484",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:19.877989",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi,\n\nCould you please tell me what version of LDR you are running and how you installed it (pip, Docker, etc.)? Are logs still printed in the terminal?",
          "created_at": "2025-06-10T21:35:44Z"
        },
        {
          "author": "bigZos",
          "body": "Sure, \n\nversion 0.5.5\n\nI set it up with docker compose. I ran the cookiecutter command and took it from there.  ",
          "created_at": "2025-06-10T21:42:47Z"
        },
        {
          "author": "djpetti",
          "body": "Okay, and if you use `docker logs`, do you still see the logs printed there? Also, did you enable the host networking option in the cookiecutter?",
          "created_at": "2025-06-10T21:44:27Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I have never seen this before ",
          "created_at": "2025-06-10T21:53:44Z"
        },
        {
          "author": "bigZos",
          "body": "I opened the docker logs and I dont see the research being logged. Just the below:\n\n---\n\n2025-06-11 00:08:06.604 | 2025-06-10 21:08:06.604 | INFO     | local_deep_research.web.app:<module>:25 - Running schema upgrades on existing database\n2025-06-11 00:08:06.631 | 2025-06-10 21:08:06.631 | WARNING  ",
          "created_at": "2025-06-10T21:55:10Z"
        }
      ]
    },
    {
      "issue_number": 294,
      "title": "Improve error handling",
      "body": "Users report two situations where better feedback would enhance the experience:\n\nSearXNG configured on non-default ports works without clear notification when connection fails\nInsufficient memory for LLM services leads to empty reports without helpful diagnostics\n\nSteps to reproduce:\n\nRun SearXNG on non-default port (8081 instead of 8080)\nRun with limited system resources for Ollama\n\nProposed solution:\n\nAdd SearXNG URL/port configuration option in settings\nImplement connectivity checks before research begins\nProvide helpful user notifications when services cannot be reached\nAdd graceful fallback options where possible",
      "state": "closed",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-05-06T22:44:08Z",
      "updated_at": "2025-06-11T13:19:45Z",
      "closed_at": "2025-06-11T13:19:45Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/294/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/294",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/294",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:20.110627",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "https://www.reddit.com/r/LocalLLaMA/comments/1keh382/comment/mqpyt50/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button",
          "created_at": "2025-05-06T22:50:34Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@djpetti  I think you fixed this already?",
          "created_at": "2025-05-12T17:43:39Z"
        },
        {
          "author": "djpetti",
          "body": "It's not a full fix. There's still more I want to do",
          "created_at": "2025-05-12T17:45:46Z"
        }
      ]
    },
    {
      "issue_number": 486,
      "title": "Quotation marks",
      "body": "https://github.com/LearningCircuit/local-deep-research/pull/407\n\nwe need to think about this",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-06-10T22:14:12Z",
      "updated_at": "2025-06-11T05:10:13Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/486/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/486",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/486",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:20.360061",
      "comments": []
    },
    {
      "issue_number": 461,
      "title": "how to set Local Search Configuring -Local document(/examplesÔºâ",
      "body": "May I add my own crawled files in the example folder? What database is being used here? Also, where should the file path for the example be set so that the model can look for local files in the example folder?",
      "state": "open",
      "author": "AhaZsy",
      "author_type": "User",
      "created_at": "2025-06-09T03:16:43Z",
      "updated_at": "2025-06-09T13:09:43Z",
      "closed_at": null,
      "labels": [
        "docs"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/461/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/461",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/461",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:20.360082",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @AhaZsy,\n\nWe have documentation on this [here](https://github.com/LearningCircuit/local-deep-research/wiki/Configuring-Local-Search).",
          "created_at": "2025-06-09T13:09:41Z"
        }
      ]
    },
    {
      "issue_number": 6,
      "title": "add statistics",
      "body": "would be nice to have:\n  - token count usage per model\n  - spent time on researches / analyses\n  - total spent time, token count, pages read, researches made",
      "state": "closed",
      "author": "uneuro",
      "author_type": "User",
      "created_at": "2025-02-10T12:18:16Z",
      "updated_at": "2025-06-09T12:21:58Z",
      "closed_at": "2025-06-09T12:21:26Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/6/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/6",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/6",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:20.555361",
      "comments": [
        {
          "author": "HashedViking",
          "body": "@uneuro check out the web interface `python app.py` it has time spent on research / analysis, feel free to fork and PR other metrics",
          "created_at": "2025-03-04T14:18:12Z"
        },
        {
          "author": "scottvr",
          "body": "@HashedViking and @uneuro\nI have a fork with additional logging/mwtrics for token count, costs, per-model for the same, and a metrics dfashboard template. https://github.com/scottvr/local-deep-research/tree/metrics-dashboard%2C-token-logging-and-costs\n\nI need to test in the actual web ui before I se",
          "created_at": "2025-03-23T19:30:59Z"
        },
        {
          "author": "scottvr",
          "body": "@uneuro The commit at https://github.com/LearningCircuit/local-deep-research/pull/74  gives us all of the per-model/search metrics you mentioned on the web dashboard (plus an estimated cost and ability to easily update model costs) as well as a few other things. \n\n@HashedViking it should allow you t",
          "created_at": "2025-03-23T23:32:35Z"
        },
        {
          "author": "LearningCircuit",
          "body": " Issue Resolved - Comprehensive Metrics System Implemented\n\n  This issue has been fully addressed in the v0.5.0 release with a\n  comprehensive metrics tracking system that exceeds the original\n  requirements.\n\n  ‚úÖ Original Requirements - All Implemented:\n\n  - Token count usage per model ‚úÖ - Fully tr",
          "created_at": "2025-06-09T12:21:26Z"
        }
      ]
    },
    {
      "issue_number": 451,
      "title": "Error 404 (loading research results)",
      "body": "**Describe the bug**\nResearch is being conducted, but at the end I get message \"Error loading research results: HTTP error 404\". When I open research history, all entries have the same error. I can see md files in the research_output directory, though, whose contents look like finished reports.\n\n**System Information:**\n - OS: Windows 11\n - Python Version: 3.12.2\n - Model Used: GLM4, via LM studio",
      "state": "closed",
      "author": "cenajuzod99zl",
      "author_type": "User",
      "created_at": "2025-06-07T17:45:27Z",
      "updated_at": "2025-06-08T18:00:58Z",
      "closed_at": "2025-06-08T17:43:57Z",
      "labels": [
        "bug",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/451/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti",
        "LearningCircuit"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/451",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/451",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:20.735959",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Do you think that is the package issue @djpetti ",
          "created_at": "2025-06-07T23:08:25Z"
        },
        {
          "author": "djpetti",
          "body": "Which version of LDR are you using?",
          "created_at": "2025-06-07T23:13:54Z"
        },
        {
          "author": "djpetti",
          "body": "Can confirm this is a bug that affects v0.5.1.",
          "created_at": "2025-06-08T00:21:34Z"
        },
        {
          "author": "LearningCircuit",
          "body": "this is a ollama specific issue. I tested open router it works. ",
          "created_at": "2025-06-08T06:31:27Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I think this is mostly a bad error message if something with the LLM doesnt work.",
          "created_at": "2025-06-08T06:40:35Z"
        }
      ]
    },
    {
      "issue_number": 459,
      "title": "Error loading research results: HTTP error 404",
      "body": " The query is processd 100%Ôºåwhen I view the reportÔºåit apear the ‚ÄùError loading research results: HTTP error 404‚Äú„ÄÇThe Project is running in UbuntuÔºàLinux OS)\n",
      "state": "closed",
      "author": "AhaZsy",
      "author_type": "User",
      "created_at": "2025-06-08T15:14:44Z",
      "updated_at": "2025-06-08T16:54:40Z",
      "closed_at": "2025-06-08T15:16:27Z",
      "labels": [
        "bug",
        "duplicate"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/459/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/459",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/459",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:20.916328",
      "comments": [
        {
          "author": "djpetti",
          "body": "Duplicate of #451 ",
          "created_at": "2025-06-08T15:16:27Z"
        },
        {
          "author": "AhaZsy",
          "body": "Thanks„ÄÇI duplicate 451 codeÔºåbut also meet ‚Äúview report 404‚Äù„ÄÇ Should I need return the old codeÔºü\n\n---- Replied Message ----\n| From | Daniel ***@***.***> |\n| Date | 06/08/2025 23:16 |\n| To | LearningCircuit/local-deep-research ***@***.***> |\n| Cc | AhaZsy ***@***.***>,\nAuthor ***@***.***> |\n| Subject ",
          "created_at": "2025-06-08T16:54:40Z"
        }
      ]
    },
    {
      "issue_number": 402,
      "title": "Want to enrich Wiki document:installation section",
      "body": "I have two suggestions to improve the wiki\n\n1. I suggest to add an issue for \"Torch has no CUDA\" when cookie-cutter not working in window (in my case). I had typed that part of wiki in https://github.com/lrex93497/local-deep-research/wiki/Installation\n\nI think it can be added to the wiki.\n\n2. I suggest adding a simple, complete hands-on guide on how to set up the image, specifically for using this project (local document search function) with KoboldCpp ‚Äî which supports OpenAI endpoints. I successfully deployed this combination on my own device locally. \nI can write that section for the wiki if it's considered appropriate.\nBecause I think some users may like to deploy this project this way without ollama.",
      "state": "open",
      "author": "lrex93497",
      "author_type": "User",
      "created_at": "2025-05-29T11:10:11Z",
      "updated_at": "2025-06-08T06:57:41Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "docs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/402/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/402",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/402",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:21.122059",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "@lrex93497 that would be amazing, thank you. I can also add you in discord dev chat if you are interested.",
          "created_at": "2025-06-01T08:34:17Z"
        },
        {
          "author": "lrex93497",
          "body": "Great, I will write that part tomorrow.\nAnd thank you for the invitation! I‚Äôll stay on GitHub for now. Will contribute when I can.",
          "created_at": "2025-06-01T16:29:48Z"
        },
        {
          "author": "djpetti",
          "body": "The use of local search with Docker should get a lot easier thanks to some changes that we have in the pipeline. It should basically be set up to work out-of-the-box.\n\nNot sure about the \"Torch has no CUDA\" issue you're referencing... Can you perhaps provide some more context?",
          "created_at": "2025-06-01T17:57:31Z"
        },
        {
          "author": "lrex93497",
          "body": "The error \"Torch has no CUDA\" occurs when using an embedding model. As PyTorch was compiled without CUDA support inside the container when followed the existing guidelines. I believe this is due to cookiecutter not working correctly on Windows. It isn't detecting my NVIDIA GPU. In the wiki, I've add",
          "created_at": "2025-06-02T08:19:02Z"
        },
        {
          "author": "lrex93497",
          "body": "I have add a part \"Example of usage with KoboldCPP and Local Document Search\" in https://github.com/lrex93497/local-deep-research/wiki/Installation#example-of-usage-with-koboldcpp-and-local-document-search. Feel free to merge it into existing wiki.",
          "created_at": "2025-06-02T13:15:54Z"
        }
      ]
    },
    {
      "issue_number": 456,
      "title": "Add X.com and Amazon.com!",
      "body": "\n### Discussed in https://github.com/LearningCircuit/local-deep-research/discussions/436\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **nomonkeynodeal** June  6, 2025</sup>\nI think this thing would be amazing for narrative analysis over time on social media, as well as analyzing individual accounts.\r\n\r\nI was able to have it pull Amazon links out of SearXNG, but it won't find all available items this way. I would kill to have something that can do my product research. Especially the kind that is for projects where I don't know exactly what I need.</div>",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-06-08T06:56:50Z",
      "updated_at": "2025-06-08T06:56:50Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/456/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/456",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/456",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:21.340649",
      "comments": []
    },
    {
      "issue_number": 75,
      "title": "on web interface is impossible to choose between  Quick Summary andDetailed Report using a screen reader",
      "body": "**Describe the bug**\n\nI'm using a screen reader and when I try to go on Detailed Report and press on enter or space to choose it id doesn't works. \n\n**To Reproduce**\nSteps to reproduce the behavior:\n0. using a screen reader E.G. NVDA is free and open source\n1. go on web interface of local-deep-research\n2. put  the cursor on Detailed Report\n3. press enter \n4. start the research, is the quick summary that is used\n \n**Expected behavior**\n\nbe able to choose between quick summary and dDetailed Report using a screen reader.\n\n**System Information:**\n - OS: kali-linux 2025.1\n - Python Version: 3.13.2 \n - Model Used: openai gpt-4o\n - Hardware Specs: 32GB RAM snapdragon X elite\n\n**Additional context**\n\nIs it possible to use radio button for instance ? It would be accessible with screen reader ",
      "state": "closed",
      "author": "jessy836",
      "author_type": "User",
      "created_at": "2025-03-24T12:48:12Z",
      "updated_at": "2025-06-07T07:00:08Z",
      "closed_at": "2025-06-07T07:00:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/75/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/75",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/75",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:21.340670",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "I am not entirely sure I understand this issue. So you cannot click with the cursor?",
          "created_at": "2025-05-12T18:08:23Z"
        }
      ]
    },
    {
      "issue_number": 293,
      "title": "Installation via Unraid",
      "body": "\n## Description\nUsers of Unraid with the Compose Manager plugin are unable to properly set up Local Deep Research because the project uses both a Dockerfile and docker-compose.yml. The Compose Manager plugin in Unraid supports docker-compose.yml files but users don't know how to handle the separate Dockerfile in this environment.\n\n## Impact\nThis prevents Unraid users from successfully deploying Local Deep Research.\n\n## Suggested Solutions\n1. Create an Unraid-specific setup guide in the documentation\n2. Modify the docker-compose.yml to use pre-built images rather than building from a Dockerfile\n3. Create a single-file docker-compose.yml that includes the build instructions inline\n\n## Additional Context\nFrom a user report:\n> I use the Compose Manager plugin in Unraid, so that I can add docker container with a compose file, but I have never used a dockerfile. I have no idea how to use that in combination with Unraid and chatgpt didnt know either, so I gave up üòÖ\n\n## Proposed Priority\nMedium - This affects a specific subset of users but completely blocks their usage of the application.",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-05-06T21:58:58Z",
      "updated_at": "2025-06-06T20:40:06Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/293/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/293",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/293",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:21.589213",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "https://www.reddit.com/r/LocalLLaMA/comments/1keh382/comment/mqjcsvc/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button",
          "created_at": "2025-05-06T22:04:27Z"
        },
        {
          "author": "djpetti",
          "body": "The `docker compose` configuration has changed significantly since this was posted. @LearningCircuit, can you ask OP to give it another try?",
          "created_at": "2025-05-20T22:18:15Z"
        },
        {
          "author": "nomonkeynodeal",
          "body": "I got it working, but none of the settings persist unless you manually create a variable for each of them in the unRAID docker config.",
          "created_at": "2025-06-06T20:30:19Z"
        }
      ]
    },
    {
      "issue_number": 415,
      "title": "docker-compose cookiecutter does not work on Windows.",
      "body": "**Describe the bug**\nIt appears that the post-generation hook for this cookiecutter is failing because Windows is being weird about file permissions.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Clone repo on windows.\n2. Run the cookiecutter\n3. Respond to all the prompts\n\n**Expected behavior**\nIt should generate the compose file without an error.\n\n**System Information:**\n - OS: Windows\n\n**Additional context**\nThis was reported by a user on Discord.\n\n![Image](https://github.com/user-attachments/assets/9ef51b73-0b47-4ee6-b09e-2413b94c1fae)\n\nError translation: \"The process cannot access the file because another process is currently using it.\"\n",
      "state": "open",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-06-02T13:09:37Z",
      "updated_at": "2025-06-02T13:12:08Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/415/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/415",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/415",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:23.703920",
      "comments": []
    },
    {
      "issue_number": 395,
      "title": "Failed to create search engine 'personal_notes'",
      "body": "\nHere's the log for it ",
      "state": "closed",
      "author": "haojie-hub",
      "author_type": "User",
      "created_at": "2025-05-26T10:17:26Z",
      "updated_at": "2025-05-26T23:06:26Z",
      "closed_at": "2025-05-26T23:06:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/395/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/395",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/395",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:23.703942",
      "comments": [
        {
          "author": "haojie-hub",
          "body": "[log.txt](https://github.com/user-attachments/files/20439512/log.txt)",
          "created_at": "2025-05-26T10:17:39Z"
        },
        {
          "author": "djpetti",
          "body": "This is caused by an invalid setting for the embedding model. The root cause of that is almost certainly an edge case in `SettingsManager` that is addressed in the linked PR.",
          "created_at": "2025-05-26T16:53:37Z"
        }
      ]
    },
    {
      "issue_number": 392,
      "title": "Support Max Tokens checkmark gives int multiply failure - likely type mismatch",
      "body": "**Describe the bug**\nError when running a deep search.  (Only if you enable max content search option.)\nError message - \"Error can't multiply sequence by non-int of type 'float'\"\n\nError log shows:\nlocal-deep-research-1  | > File \"/install/.venv/lib/python3.13/site-packages/local_deep_research/web/services/research_service.py\", line 285, in run_research_process\nlocal-deep-research-1  |     use_llm = get_llm(\nlocal-deep-research-1  |               ‚îî <function get_llm at 0x7ce6f98a22a0>\nlocal-deep-research-1  |   File \"/install/.venv/lib/python3.13/site-packages/local_deep_research/config/llm_config.py\", line 82, in get_llm\nlocal-deep-research-1  |     get_db_setting(\"llm.max_tokens\", 30000), int(context_window_size * 0.8)\nlocal-deep-research-1  |     ‚îÇ                                            ‚îî '128000'\nlocal-deep-research-1  |     ‚îî <function get_db_setting at 0x7ce6f549cb80>\nlocal-deep-research-1  | \nlocal-deep-research-1  | TypeError: can't multiply sequence by non-int of type 'float'\nlocal-deep-research-1  | 2025-05-25 21:24:53.949 | DEBUG    | local_deep_research.web.services.settings_manager:check_env_setting:38 - Overriding llm.provider setting from environment variable.\nlocal-deep-research-1  | 2025-05-25 21:24:53.949 | INFO     | local_deep_research.config.llm_config:get_llm:67 - Getting LLM with model: gemma3:27b, temperature: 0.7, provider: ollama\nlocal-deep-research-1  | 2025-05-25 21:24:53.949 | ERROR    | local_deep_research.web.services.research_service:run_research_process:691 - Research failed: can't multiply sequence by non-int of type 'float'\nlocal-deep-research-1  | Traceback (most recent call last):\nlocal-deep-research-1  | \nlocal-deep-research-1  |   File \"/usr/local/lib/python3.13/threading.py\", line 1012, in _bootstrap\nlocal-deep-research-1  |     self._bootstrap_inner()\nlocal-deep-research-1  |     ‚îÇ    ‚îî <function Thread._bootstrap_inner at 0x7ce6f9048680>\nlocal-deep-research-1  |     ‚îî <Thread(Thread-280 (run_research_process), started daemon 137329341986496)>\nlocal-deep-research-1  |   File \"/usr/local/lib/python3.13/threading.py\", line 1041, in _bootstrap_inner\nlocal-deep-research-1  |     self.run()\nlocal-deep-research-1  |     ‚îÇ    ‚îî <function Thread.run at 0x7ce6f9048400>\nlocal-deep-research-1  |     ‚îî <Thread(Thread-280 (run_research_process), started daemon 137329341986496)>\nlocal-deep-research-1  |   File \"/usr/local/lib/python3.13/threading.py\", line 992, in run\nlocal-deep-research-1  |     self._target(*self._args, **self._kwargs)\n\n\n\n**Expected behavior**\nI expect the model to accept a checkmark in the \"Supports 'Max Tokens'\" field and accept an integer in the corresponding entry (\"llm.context_window_size\").  This is happening somewhere around\n\n\"   context_window_size = get_db_setting(\"llm.context_window_size\", DEFAULT_CONTEXT_WINDOW_SIZE)\n-    prompt_window      = int(context_window_size * 0.8)\"\n\n**System Information:**\n - OS: Ubuntu 24.04.2 LTS\n - Model Used: Gemma3:27b on local ollama instance\n\n\n",
      "state": "closed",
      "author": "jhsmith409",
      "author_type": "User",
      "created_at": "2025-05-25T21:45:17Z",
      "updated_at": "2025-05-26T16:54:18Z",
      "closed_at": "2025-05-26T16:54:18Z",
      "labels": [
        "bug",
        "duplicate"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/392/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/392",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/392",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:23.884971",
      "comments": [
        {
          "author": "djpetti",
          "body": "I believe this is a duplicate of #382, which was fixed in 0.4.2. Can you try that version?",
          "created_at": "2025-05-26T01:41:22Z"
        },
        {
          "author": "jhsmith409",
          "body": "You are correct, same issues as 382, sorry to duplicate that.  I was on 0.4.0.  Testing 0.4.2 next.  Will open new issue of it reappears.  Closing this issue.",
          "created_at": "2025-05-26T11:35:49Z"
        }
      ]
    },
    {
      "issue_number": 380,
      "title": "RAG upload",
      "body": "\"i have a question about RAG. this might not be the right place to ask but i'd appriciate your help. im trying to figure out how to use RAG within the app. how do i upload my stuff (pdfs, etc)? i imagine im suppose to point to a folder or something, cause i cant find an upload button, but im struggling to figure it out where and how to do that. (excuse my ignorance, my background is in finance not computers)\n\napologies if this is an inappropriate ask\"",
      "state": "closed",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-05-23T20:49:15Z",
      "updated_at": "2025-05-25T19:52:53Z",
      "closed_at": "2025-05-25T19:52:53Z",
      "labels": [
        "docs"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/380/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/380",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/380",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:24.065551",
      "comments": [
        {
          "author": "djpetti",
          "body": "@LearningCircuit Can you point OP to [this wiki page](https://github.com/LearningCircuit/local-deep-research/wiki/Configuring-Local-Search)?",
          "created_at": "2025-05-25T02:35:22Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I did and it was what they needed",
          "created_at": "2025-05-25T19:52:48Z"
        }
      ]
    },
    {
      "issue_number": 382,
      "title": "TypeError Related to Context Size",
      "body": "**Describe the bug**\n\nI receive the following error: \"TypeError: can't multiply sequence by non-int of type 'float'\"\n\n![Image](https://github.com/user-attachments/assets/6642e017-3c37-49e3-9b6c-4822244a975c)\n\n**To Reproduce**\nOllama installed within Windows 11.\nOpen Webui installed via docker\nSearxng installed via docker\nlocal-deep-research installed via docker\n\nRestarting the container does not fix the issue.\nDeleting the local-deep-research container, and then running \"docker run -d -p 5000:5000 --name local-deep-research localdeepresearch/local-deep-research\" does not fix the issue.\n\n**System Information:**\n - OS: Windows 11 Home 24H2\n - Python Version: 3.12\n - Model Used: Qwen 3 30B A3B\n - Hardware Specs:\n          - 11th Gen Intel(R) Core(TM) i5-11400F @ 2.60GHz   2.59 GHz\n          - 16.0 GB (15.8 GB usable)\n          - RTX 3060 12GB\n\n**Docker Output/Logs**\n\n2025-05-24 17:16:50 > File \"/install/.venv/lib/python3.13/site-packages/local_deep_research/web/services/research_service.py\", line 284, in run_research_process\n2025-05-24 17:16:50     use_llm = get_llm(\n2025-05-24 17:16:50               ‚îî <function get_llm at 0x7f3009c36e80>\n2025-05-24 17:16:50   File \"/install/.venv/lib/python3.13/site-packages/local_deep_research/config/llm_config.py\", line 82, in get_llm\n2025-05-24 17:16:50     get_db_setting(\"llm.max_tokens\", 30000), int(context_window_size * 0.8)\n2025-05-24 17:16:50     ‚îÇ                                            ‚îî '10000.00'\n2025-05-24 17:16:50     ‚îî <function get_db_setting at 0x7f3005604360>\n2025-05-24 17:16:50 \n2025-05-24 17:16:50 **TypeError: can't multiply sequence by non-int of type 'float'**",
      "state": "closed",
      "author": "lumien",
      "author_type": "User",
      "created_at": "2025-05-24T21:29:32Z",
      "updated_at": "2025-05-25T16:47:12Z",
      "closed_at": "2025-05-25T16:47:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/382/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/382",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/382",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:24.293606",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "What happens with other models? What happens if you reset the db to defaults?",
          "created_at": "2025-05-24T21:49:36Z"
        },
        {
          "author": "LearningCircuit",
          "body": "You can reset to defaults at the bottom of the settings page.",
          "created_at": "2025-05-24T21:50:04Z"
        },
        {
          "author": "lumien",
          "body": "i have tried various other models i have installed locally, and the same issue happens. I have also selected \"reset to defaults\" on the settings page, and it doesn't fix the issue.\n\ni have also tried various context size settings, the original 128000, also 10000, and 10000.00 ; all give the same err",
          "created_at": "2025-05-24T21:56:22Z"
        },
        {
          "author": "djpetti",
          "body": "Okay, I think this is just a case of the type for that setting not being handled correctly. I've seen that happen before. We should be able to squeeze it into a patch release.",
          "created_at": "2025-05-25T02:34:04Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@djpetti but the default settings are stable",
          "created_at": "2025-05-25T07:24:29Z"
        }
      ]
    },
    {
      "issue_number": 322,
      "title": "Please help me, after configuring the route, I am unable to retrieve my local documents through local search",
      "body": "This is my log:\n<details><summary>Details</summary>\n<p>\n\nC:\\Users\\AUSU>python -m local_deep_research.web.app\nData directory already exists at: C:\\Users\\AUSU\\AppData\\Roaming\\Python\\Python312\\data\nINFO:__main__:Running schema upgrades on existing database\nINFO:local_deep_research.web.database.schema_upgrade:Running schema upgrades on C:\\Users\\AUSU\\AppData\\Roaming\\Python\\Python312\\src\\data\\ldr.db\nINFO:local_deep_research.web.database.schema_upgrade:Table 'research_log' does not exist, no action needed\nINFO:local_deep_research.web.database.schema_upgrade:Schema upgrades completed successfully\nUsing package static path: C:/Users/AUSU/AppData/Roaming/Python/Python312/site-packages/local_deep_research/web/static\nUsing package template path: C:/Users/AUSU/AppData/Roaming/Python/Python312/site-packages/local_deep_research/web/templates\nINFO:local_deep_research.web.app_factory:Using database at C:\\Users\\AUSU\\AppData\\Roaming\\Python\\Python312\\site-packages\\data\\ldr.db\nINFO:local_deep_research.web.services.socket_service:Socket.IO instance attached to socket service\nINFO:__main__:Starting web server on 0.0.0.0:5000 (debug: True)\n * Serving Flask app 'local_deep_research.web.app_factory'\n * Debug mode: on\nData directory already exists at: C:\\Users\\AUSU\\AppData\\Roaming\\Python\\Python312\\data\nINFO:__main__:Running schema upgrades on existing database\nINFO:local_deep_research.web.database.schema_upgrade:Running schema upgrades on C:\\Users\\AUSU\\AppData\\Roaming\\Python\\Python312\\src\\data\\ldr.db\nINFO:local_deep_research.web.database.schema_upgrade:Table 'research_log' does not exist, no action needed\nINFO:local_deep_research.web.database.schema_upgrade:Schema upgrades completed successfully\nUsing package static path: C:/Users/AUSU/AppData/Roaming/Python/Python312/site-packages/local_deep_research/web/static\nUsing package template path: C:/Users/AUSU/AppData/Roaming/Python/Python312/site-packages/local_deep_research/web/templates\nINFO:local_deep_research.web.app_factory:Using database at C:\\Users\\AUSU\\AppData\\Roaming\\Python\\Python312\\site-packages\\data\\ldr.db\nINFO:local_deep_research.web.services.socket_service:Socket.IO instance attached to socket service\nINFO:__main__:Starting web server on 0.0.0.0:5000 (debug: True)\nWARNING:werkzeug: * Debugger is active!\nINFO:local_deep_research.web.services.socket_service:Client connected: Sj1o5ngNSnUoT7HOAAAB\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.services.socket_service:Client connected: N3qGc43NCCQULwhrAAAD\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.app_factory:Attempting to connect to Ollama API\nDEBUG:local_deep_research.web.app_factory:Ollama API response: Status 200\nDEBUG:local_deep_research.web.app_factory:Ollama API raw response: {\"models\":[{\"name\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"model\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"modified_at\":\"2025-05-13T08:39:40.557926682Z\",\"size\":322302359,\"digest\":\"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"jina-bert-v2\",\"families\":[\"jina-bert-v2\"],\"parameter_size\":\"160.22M\",\"quantization_level\":\"F16\"}},{\"name\":\"snowflake-arctic-embed2:latest\",\"model\":\"snowflake-arctic-embed2:latest\",\"mod...\nDEBUG:local_deep_research.web.app_factory:Ollama API JSON data: {\"models\": [{\"name\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"model\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"modified_at\": \"2025-05-13T08:39:40.557926682Z\", \"size\": 322302359, \"digest\": \"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\", \"details\": {\"parent_model\": \"\", \"format\": \"gguf\", \"family\": \"jina-bert-v2\", \"families\": [\"jina-bert-v2\"], \"parameter_size\": \"160.22M\", \"quantization_level\": \"F16\"}}, {\"name\": \"snowflake-arctic-embed2:latest\", \"model\": \"snowflake...\nINFO:local_deep_research.web.app_factory:Found 8 models in newer Ollama API format\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: EntropyYue/jina-embeddings-v2-base-zh:latest -> Entropyyue Jina-embeddings-v2-base-zh Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: snowflake-arctic-embed2:latest -> Snowflake-arctic-embed2 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: linux6200/bge-reranker-v2-m3:latest -> Linux6200 Bge-reranker-v2-m3 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: qwen3:4b -> Qwen3 4b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: gemma3:12b -> Gemma3 12b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: deepseek-r1:7b -> Deepseek-r1 7b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: phi4:latest -> Phi4 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: nomic-embed-text:latest -> Nomic-embed-text Latest\nINFO:local_deep_research.web.app_factory:Final Ollama models count: 8\nINFO:local_deep_research.web.app_factory:Sample Ollama models: deepseek-r1:7b, EntropyYue/jina-embeddings-v2-base-zh:latest, gemma3:12b, linux6200/bge-reranker-v2-m3:latest, nomic-embed-text:latest\nINFO:local_deep_research.web.app_factory:Attempting to connect to Ollama API\nDEBUG:local_deep_research.web.app_factory:Ollama API response: Status 200\nDEBUG:local_deep_research.web.app_factory:Ollama API raw response: {\"models\":[{\"name\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"model\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"modified_at\":\"2025-05-13T08:39:40.557926682Z\",\"size\":322302359,\"digest\":\"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"jina-bert-v2\",\"families\":[\"jina-bert-v2\"],\"parameter_size\":\"160.22M\",\"quantization_level\":\"F16\"}},{\"name\":\"snowflake-arctic-embed2:latest\",\"model\":\"snowflake-arctic-embed2:latest\",\"mod...\nDEBUG:local_deep_research.web.app_factory:Ollama API JSON data: {\"models\": [{\"name\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"model\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"modified_at\": \"2025-05-13T08:39:40.557926682Z\", \"size\": 322302359, \"digest\": \"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\", \"details\": {\"parent_model\": \"\", \"format\": \"gguf\", \"family\": \"jina-bert-v2\", \"families\": [\"jina-bert-v2\"], \"parameter_size\": \"160.22M\", \"quantization_level\": \"F16\"}}, {\"name\": \"snowflake-arctic-embed2:latest\", \"model\": \"snowflake...\nINFO:local_deep_research.web.app_factory:Found 8 models in newer Ollama API format\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: EntropyYue/jina-embeddings-v2-base-zh:latest -> Entropyyue Jina-embeddings-v2-base-zh Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: snowflake-arctic-embed2:latest -> Snowflake-arctic-embed2 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: linux6200/bge-reranker-v2-m3:latest -> Linux6200 Bge-reranker-v2-m3 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: qwen3:4b -> Qwen3 4b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: gemma3:12b -> Gemma3 12b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: deepseek-r1:7b -> Deepseek-r1 7b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: phi4:latest -> Phi4 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: nomic-embed-text:latest -> Nomic-embed-text Latest\nINFO:local_deep_research.web.app_factory:Final Ollama models count: 8\nINFO:local_deep_research.web.app_factory:Sample Ollama models: deepseek-r1:7b, EntropyYue/jina-embeddings-v2-base-zh:latest, gemma3:12b, linux6200/bge-reranker-v2-m3:latest, nomic-embed-text:latest\nINFO:local_deep_research.web.services.socket_service:Client connected: UGIybuzsOjrkqtcBAAAF\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.app_factory:Attempting to connect to Ollama API\nDEBUG:local_deep_research.web.app_factory:Ollama API response: Status 200\nDEBUG:local_deep_research.web.app_factory:Ollama API raw response: {\"models\":[{\"name\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"model\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"modified_at\":\"2025-05-13T08:39:40.557926682Z\",\"size\":322302359,\"digest\":\"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"jina-bert-v2\",\"families\":[\"jina-bert-v2\"],\"parameter_size\":\"160.22M\",\"quantization_level\":\"F16\"}},{\"name\":\"snowflake-arctic-embed2:latest\",\"model\":\"snowflake-arctic-embed2:latest\",\"mod...\nDEBUG:local_deep_research.web.app_factory:Ollama API JSON data: {\"models\": [{\"name\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"model\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"modified_at\": \"2025-05-13T08:39:40.557926682Z\", \"size\": 322302359, \"digest\": \"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\", \"details\": {\"parent_model\": \"\", \"format\": \"gguf\", \"family\": \"jina-bert-v2\", \"families\": [\"jina-bert-v2\"], \"parameter_size\": \"160.22M\", \"quantization_level\": \"F16\"}}, {\"name\": \"snowflake-arctic-embed2:latest\", \"model\": \"snowflake...\nINFO:local_deep_research.web.app_factory:Found 8 models in newer Ollama API format\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: EntropyYue/jina-embeddings-v2-base-zh:latest -> Entropyyue Jina-embeddings-v2-base-zh Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: snowflake-arctic-embed2:latest -> Snowflake-arctic-embed2 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: linux6200/bge-reranker-v2-m3:latest -> Linux6200 Bge-reranker-v2-m3 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: qwen3:4b -> Qwen3 4b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: gemma3:12b -> Gemma3 12b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: deepseek-r1:7b -> Deepseek-r1 7b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: phi4:latest -> Phi4 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: nomic-embed-text:latest -> Nomic-embed-text Latest\nINFO:local_deep_research.web.app_factory:Final Ollama models count: 8\nINFO:local_deep_research.web.app_factory:Sample Ollama models: deepseek-r1:7b, EntropyYue/jina-embeddings-v2-base-zh:latest, gemma3:12b, linux6200/bge-reranker-v2-m3:latest, nomic-embed-text:latest\nINFO:local_deep_research.web.app_factory:Attempting to connect to Ollama API\nDEBUG:local_deep_research.web.app_factory:Ollama API response: Status 200\nDEBUG:local_deep_research.web.app_factory:Ollama API raw response: {\"models\":[{\"name\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"model\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"modified_at\":\"2025-05-13T08:39:40.557926682Z\",\"size\":322302359,\"digest\":\"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"jina-bert-v2\",\"families\":[\"jina-bert-v2\"],\"parameter_size\":\"160.22M\",\"quantization_level\":\"F16\"}},{\"name\":\"snowflake-arctic-embed2:latest\",\"model\":\"snowflake-arctic-embed2:latest\",\"mod...\nDEBUG:local_deep_research.web.app_factory:Ollama API JSON data: {\"models\": [{\"name\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"model\": \"EntropyYue/jina-embeddings-v2-base-zh:latest\", \"modified_at\": \"2025-05-13T08:39:40.557926682Z\", \"size\": 322302359, \"digest\": \"ac8a39f0f1e876456fe177f9c248f248dc8af46fc88fcc716aa821b8860ffa0c\", \"details\": {\"parent_model\": \"\", \"format\": \"gguf\", \"family\": \"jina-bert-v2\", \"families\": [\"jina-bert-v2\"], \"parameter_size\": \"160.22M\", \"quantization_level\": \"F16\"}}, {\"name\": \"snowflake-arctic-embed2:latest\", \"model\": \"snowflake...\nINFO:local_deep_research.web.app_factory:Found 8 models in newer Ollama API format\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: EntropyYue/jina-embeddings-v2-base-zh:latest -> Entropyyue Jina-embeddings-v2-base-zh Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: snowflake-arctic-embed2:latest -> Snowflake-arctic-embed2 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: linux6200/bge-reranker-v2-m3:latest -> Linux6200 Bge-reranker-v2-m3 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: qwen3:4b -> Qwen3 4b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: gemma3:12b -> Gemma3 12b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: deepseek-r1:7b -> Deepseek-r1 7b\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: phi4:latest -> Phi4 Latest\nDEBUG:local_deep_research.web.app_factory:Added Ollama model: nomic-embed-text:latest -> Nomic-embed-text Latest\nINFO:local_deep_research.web.app_factory:Final Ollama models count: 8\nINFO:local_deep_research.web.app_factory:Sample Ollama models: deepseek-r1:7b, EntropyYue/jina-embeddings-v2-base-zh:latest, gemma3:12b, linux6200/bge-reranker-v2-m3:latest, nomic-embed-text:latest\nINFO:local_deep_research.web.services.socket_service:Client connected: IuSxLs5_Pf0-w7REAAAH\nINFO:local_deep_research.web.routes.research_routes:Starting research with provider: OLLAMA, model: qwen3:4b, search engine: local_all\nINFO:local_deep_research.web.routes.research_routes:Additional parameters: max_results=None, time_period=None, iterations=1, questions=3\nINFO:local_deep_research.web.services.research_service:Starting research process for ID 38, query: How to use SLDWORKS?\nINFO:local_deep_research.web.services.research_service:Research parameters: provider=OLLAMA, model=qwen3:4b, search_engine=local_all, max_results=None, time_period=None, iterations=1, questions_per_iteration=3, custom_endpoint=https://openrouter.ai/api/v1\nINFO:local_deep_research.web.services.research_service:Overriding system settings with: provider=OLLAMA, model=qwen3:4b, search_engine=local_all\nGetting LLM with model: qwen3:4b, temperature: 0.7, provider: ollama\nINFO:local_deep_research.config.llm_config:Checking Ollama availability at http://localhost:11434/api/tags\nINFO:local_deep_research.config.llm_config:Ollama is available. Status code: 200\nINFO:local_deep_research.config.llm_config:Response preview: {\"models\":[{\"name\":\"EntropyYue/jina-embeddings-v2-base-zh:latest\",\"model\":\"EntropyYue/jina-embedding\nINFO:local_deep_research.config.llm_config:Checking if model 'qwen3:4b' exists in Ollama\nINFO:local_deep_research.config.llm_config:Available Ollama models: entropyyue/jina-embeddings-v2-base-zh:latest, snowflake-arctic-embed2:latest, linux6200/bge-reranker-v2-m3:latest, qwen3:4b, gemma3:12b and more\nINFO:local_deep_research.config.llm_config:Creating ChatOllama with model=qwen3:4b, base_url=http://localhost:11434\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.services.socket_service:Client connected: r99fM5yKN2a6jO-WAAAJ\nINFO:local_deep_research.web.services.socket_service:Client connected: 9Rn0Ny7It5keHcO2AAAL\nINFO:local_deep_research.config.llm_config:Testing Ollama model with simple invocation\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.config.llm_config:Ollama test successful. Response type: <class 'langchain_core.messages.ai.AIMessage'>\nINFO:local_deep_research.web.services.research_service:Successfully set LLM to: provider=OLLAMA, model=qwen3:4b\nINFO:local_deep_research.config.search_config:Creating search engine with tool: local_all\nINFO:local_deep_research.config.search_config:Search config: tool=local_all, max_results=50, time_period=y\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: local_all with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engines_config:Loaded 10 search engines from configuration file\nINFO:local_deep_research.web_search_engines.search_engines_config:\n  arxiv, auto, brave, github, google_pse, pubmed, searxng, serpapi, wayback, wikipedia\n\nERROR:local_deep_research.web_search_engines.search_engines_config:Invalid paths specified for local collection: [\n  \"G:\\documents\\1\",\n  \"G:\\documents\\2\",\n  \"G:\\documents\\3\"\n]\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.personal_notes.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.personal_notes.class_name' in the database.\nINFO:local_deep_research.web_search_engines.search_engines_config:Ignoring disabled local collection 'project_docs'.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.research_papers.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.research_papers.class_name' in the database.\nINFO:local_deep_research.web_search_engines.search_engines_config:Registered local document collections as search engines\nINFO:faiss.loader:Loading faiss with AVX2 support.\nINFO:faiss.loader:Successfully loaded faiss with AVX2 support.\nINFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating local_all with filtered parameters: dict_keys(['max_results', 'max_filtered_results', 'llm'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating personal_notes with filtered parameters: dict_keys(['cache_dir', 'chunk_overlap', 'chunk_size', 'description', 'embedding_device', 'embedding_model', 'embedding_model_type', 'max_filtered_results', 'max_results', 'name', 'paths', 'llm'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating research_papers with filtered parameters: dict_keys(['cache_dir', 'chunk_overlap', 'chunk_size', 'description', 'embedding_device', 'embedding_model', 'embedding_model_type', 'max_filtered_results', 'max_results', 'name', 'paths', 'llm'])\nesearch_papersdeep_research.web_search_engines.engines.search_engine_local:Folder not found or is not a directory: G:\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders found among: ['G:\\research_papers']\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:This search engine will return no results until valid folders are configured\nesearch_papersdeep_research.web_search_engines.engines.search_engine_local:Skipped 1 invalid folders: G:\nINFO:local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: LocalAllSearchEngine\nINFO:local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_local_all.LocalAllSearchEngine object at 0x000002277E1FF980>>\nINFO:local_deep_research.config.search_config:Successfully created search engine of type: LocalAllSearchEngine\nINFO:local_deep_research.search_system:Initializing AdvancedSearchSystem with strategy_name='source-based'\nINFO:local_deep_research.search_system:Creating SourceBasedSearchStrategy instance\nINFO:local_deep_research.search_system:Created strategy of type: SourceBasedSearchStrategy\nINFO:local_deep_research.config.search_config:Creating search engine with tool: local_all\nINFO:local_deep_research.config.search_config:Search config: tool=local_all, max_results=50, time_period=y\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: local_all with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating local_all with filtered parameters: dict_keys(['max_results', 'max_filtered_results', 'llm'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating personal_notes with filtered parameters: dict_keys(['cache_dir', 'chunk_overlap', 'chunk_size', 'description', 'embedding_device', 'embedding_model', 'embedding_model_type', 'max_filtered_results', 'max_results', 'name', 'paths', 'llm'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating research_papers with filtered parameters: dict_keys(['cache_dir', 'chunk_overlap', 'chunk_size', 'description', 'embedding_device', 'embedding_model', 'embedding_model_type', 'max_filtered_results', 'max_results', 'name', 'paths', 'llm'])\nesearch_papersdeep_research.web_search_engines.engines.search_engine_local:Folder not found or is not a directory: G:\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders found among: ['G:\\research_papers']\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:This search engine will return no results until valid folders are configured\nesearch_papersdeep_research.web_search_engines.engines.search_engine_local:Skipped 1 invalid folders: G:\nINFO:local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: LocalAllSearchEngine\nINFO:local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_local_all.LocalAllSearchEngine object at 0x000002277DE89FA0>>\nINFO:local_deep_research.config.search_config:Successfully created search engine of type: LocalAllSearchEngine\nINFO:local_deep_research.web.services.research_service:Successfully set search engine to: local_all\nINFO:local_deep_research.advanced_search_system.strategies.source_based_strategy:Starting source-based research on topic: How to use SLDWORKS?\nINFO:local_deep_research.advanced_search_system.questions.standard_question:Generating follow-up questions...\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.advanced_search_system.questions.standard_question:Generated 3 follow-up questions\nINFO:local_deep_research.advanced_search_system.strategies.source_based_strategy:Using questions for iteration 1: ['How to use SLDWORKS?', 'How to install and activate SLDWORKS on a new computer?', 'What are the steps to create a 3D model from scratch in SLDWORKS?', 'How to export a design from SLDWORKS in multiple file formats (e.g., STEP, IGES, PDF)?']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:Searching across all local collections for query: How to use SLDWORKS?\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:Searching across all local collections for query: How to install and activate SLDWORKS on a new computer?\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:Searching across all local collections for query: What are the steps to create a 3D model from scratch in SLDWORKS?\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:Searching across all local collections for query: How to export a design from SLDWORKS in multiple file formats (e.g., STEP, IGES, PDF)?\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:No local documents found for query: How to use SLDWORKS?\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:No local documents found for query: How to install and activate SLDWORKS on a new computer?\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nINFO:local_deep_research.web_search_engines.search_engine_base:Search engine LocalAllSearchEngine returned no preview results for query: How to use SLDWORKS?\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:No valid folders to search in collections: ['default']\nINFO:local_deep_research.web_search_engines.search_engine_base:Search engine LocalAllSearchEngine returned no preview results for query: How to install and activate SLDWORKS on a new computer?\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:No local documents found for query: What are the steps to create a 3D model from scratch in SLDWORKS?\nINFO:local_deep_research.web_search_engines.engines.search_engine_local_all:No local documents found for query: How to export a design from SLDWORKS in multiple file formats (e.g., STEP, IGES, PDF)?\nINFO:local_deep_research.web_search_engines.search_engine_base:Search engine LocalAllSearchEngine returned no preview results for query: What are the steps to create a 3D model from scratch in SLDWORKS?\nINFO:local_deep_research.web_search_engines.search_engine_base:Search engine LocalAllSearchEngine returned no preview results for query: How to export a design from SLDWORKS in multiple file formats (e.g., STEP, IGES, PDF)?\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.advanced_search_system.findings.repository:Added 0 documents to repository\nINFO:local_deep_research.advanced_search_system.findings.repository:Set questions for 1 iterations\nINFO:local_deep_research.advanced_search_system.findings.repository:Formatting final report. Number of detailed findings: 2. Synthesized content length: 2630. Number of question iterations: 1\nINFO:local_deep_research.utilities.search_utilities:Inside format_findings utility. Findings count: 2, Questions iterations: 1\nINFO:local_deep_research.utilities.search_utilities:Formatting 0 links to markdown...\nINFO:local_deep_research.utilities.search_utilities:Formatting 2 detailed finding items.\nINFO:local_deep_research.utilities.search_utilities:No unique sources found across all findings to list.\nINFO:local_deep_research.utilities.search_utilities:Finished format_findings utility.\nINFO:local_deep_research.advanced_search_system.findings.repository:Successfully formatted final report.\nINFO:local_deep_research.web.services.research_service:Found formatted_findings of length: 5928\nINFO:local_deep_research.web.services.research_service:Successfully converted to clean markdown of length: 5928\nINFO:local_deep_research.web.services.research_service:Writing report to: research_outputs\\quick_summary_how_to_use_sldworks.md\nINFO:local_deep_research.web.services.research_service:Updating database for research_id: 38\nINFO:local_deep_research.web.services.research_service:Database updated successfully for research_id: 38\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research_id: 38\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research 38\nINFO:local_deep_research.web.services.research_service:Resources cleaned up for research_id: 38\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.services.socket_service:Client connected: vSuw8eccy4hbC0CVAAAN\n\n</p>\n</details> \n\n\nThis is my web settings\n\n![Image](https://github.com/user-attachments/assets/5905fe36-38dc-4709-8748-9c4730ebcf75)\nThe 1, 2, and 3 folders in PATH contain several TXT files",
      "state": "closed",
      "author": "haojie-hub",
      "author_type": "User",
      "created_at": "2025-05-13T09:18:37Z",
      "updated_at": "2025-05-23T20:19:40Z",
      "closed_at": "2025-05-23T18:39:20Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 22,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/322/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/322",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/322",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:24.495670",
      "comments": []
    },
    {
      "issue_number": 375,
      "title": "pip is still on version 0.3.12",
      "body": "Seeing the new 0.4.0 version, I tried to update today in pip however it is still showing and installing the previous version (0.3.12).\n\n<img width=\"570\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1387e1b5-73a3-435b-a634-0733896fd619\" />\n\nPyPI already has the latest version.\n\n<img width=\"570\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/82057b74-964f-4565-b4d1-ab59a7335b7d\" />\n\nAny ideas what could be the issue?",
      "state": "closed",
      "author": "xybernaut",
      "author_type": "User",
      "created_at": "2025-05-21T03:17:19Z",
      "updated_at": "2025-05-22T21:10:13Z",
      "closed_at": "2025-05-22T21:10:12Z",
      "labels": [
        "bug",
        "needs-replication"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/375/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/375",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/375",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:24.495684",
      "comments": [
        {
          "author": "djpetti",
          "body": "Just tried installing now in a fresh `venv`, and it seemed to get the correct version.\n\nWould you mind sharing what version of Python and pip you are using? Also, just to be sure, can you try running\n```python\nimport local_deep_research\nprint(local_deep_research.__version__)\n```\nand tell me what it ",
          "created_at": "2025-05-21T14:07:16Z"
        },
        {
          "author": "xybernaut",
          "body": "I get version 0.3.12 when I run the code snippet you provided.\n\nI am using LDR in Conda environment. I also just updated pip to the latest version and it still installs LDR version 0.3.12.\n\nIf I execute pip install local-deep-research==0.4.0, I get the error below:\n\nINFO: pip is looking at multiple ",
          "created_at": "2025-05-21T16:21:44Z"
        },
        {
          "author": "LearningCircuit",
          "body": "We didn't yank that version  hmn",
          "created_at": "2025-05-21T16:26:06Z"
        },
        {
          "author": "djpetti",
          "body": "Yeah, something weird is going on. Several of those versions that pip reports don't exist, as far as we know. Can you try installing in a normal venv instead of Conda?",
          "created_at": "2025-05-21T16:42:10Z"
        },
        {
          "author": "xybernaut",
          "body": "I prefer using Conda so I will give venv a shot when I get the chance. I also just got the Docker version up and running with version 0.40.",
          "created_at": "2025-05-21T17:01:03Z"
        }
      ]
    },
    {
      "issue_number": 338,
      "title": "Searxng: Invalid value \"False\" for parameter safesearch",
      "body": "**Describe the bug**\nThere are no results received from Searxng. From LDR log: SearXNG returned status code 400. \nExcept for Searxng status code 400, everything else seems fine in LDR log. The model went on to generate the summary template without any search results.\n\n**Expected behavior**\nThere should be results from Searxng, which will be used to generate the report.\n\n**System Information:**\n- Docker Desktop on Windows 11\n- Run with existing Ollama container at http://ollama:11434\n- Run with existing Searxng container at http://host.docker.internal:8081 \n- Both Ollama and Searxng instance addresses are configured properly in LDR Settings.\n\n**Additional context**\nRunning the exact query seen in LDR's log directly in Searxng returns several results. I also use this Searxng instance with Open WebUI without any issues. This is unlikely to be an issue with Searxng instance.\nI have tried disabling Safe Search in LDR's setting, but still got the same issue.\n\n**Output/Logs from Searxng**\n```\n2025-05-13 23:47:48.106 | 2025-05-14 03:47:48,103 ERROR:searx.webapp: Invalid choice: False\n2025-05-13 23:47:48.106 | Traceback (most recent call last):\n2025-05-13 23:47:48.106 |   File \"/usr/local/searxng/searx/webapp.py\", line 459, in pre_request\n2025-05-13 23:47:48.106 |     preferences.parse_dict(sxng_request.form)\n2025-05-13 23:47:48.107 |     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:47:48.107 |   File \"/usr/local/searxng/searx/preferences.py\", line 525, in parse_dict\n2025-05-13 23:47:48.107 |     self.key_value_settings[user_setting_name].parse(user_setting)\n2025-05-13 23:47:48.107 |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n2025-05-13 23:47:48.107 |   File \"/usr/local/searxng/searx/preferences.py\", line 206, in parse\n2025-05-13 23:47:48.107 |     raise ValidationException('Invalid choice: {0}'.format(data))\n2025-05-13 23:47:48.107 | searx.preferences.ValidationException: Invalid choice: False\n2025-05-13 23:47:48.110 | 2025-05-14 03:47:48,106 ERROR:searx.webapp: Invalid choice: False\n2025-05-13 23:47:48.110 | Traceback (most recent call last):\n2025-05-13 23:47:48.110 |   File \"/usr/local/searxng/searx/webapp.py\", line 459, in pre_request\n2025-05-13 23:47:48.110 |     preferences.parse_dict(sxng_request.form)\n2025-05-13 23:47:48.110 |     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:47:48.110 |   File \"/usr/local/searxng/searx/preferences.py\", line 525, in parse_dict\n2025-05-13 23:47:48.110 |     self.key_value_settings[user_setting_name].parse(user_setting)\n2025-05-13 23:47:48.110 |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n2025-05-13 23:47:48.110 |   File \"/usr/local/searxng/searx/preferences.py\", line 206, in parse\n2025-05-13 23:47:48.110 |     raise ValidationException('Invalid choice: {0}'.format(data))\n2025-05-13 23:47:48.110 | searx.preferences.ValidationException: Invalid choice: False\n2025-05-13 23:47:48.115 | 2025-05-14 03:47:48,109 ERROR:searx.webapp: search error: SearxParameterException\n2025-05-13 23:47:48.115 | Traceback (most recent call last):\n2025-05-13 23:47:48.115 |   File \"/usr/local/searxng/searx/webapp.py\", line 618, in search\n2025-05-13 23:47:48.115 |     search_query, raw_text_query, _,_ , selected_locale = get_search_query_from_webapp(\n2025-05-13 23:47:48.115 |                                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n2025-05-13 23:47:48.115 |         sxng_request.preferences, sxng_request.form\n2025-05-13 23:47:48.115 |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:47:48.115 |     )\n2025-05-13 23:47:48.115 |     ^\n2025-05-13 23:47:48.115 |   File \"/usr/local/searxng/searx/webadapter.py\", line 259, in get_search_query_from_webapp\n2025-05-13 23:47:48.115 |     query_safesearch = parse_safesearch(preferences, form)\n2025-05-13 23:47:48.115 |   File \"/usr/local/searxng/searx/webadapter.py\", line 84, in parse_safesearch\n2025-05-13 23:47:48.115 |     raise SearxParameterException('safesearch', query_safesearch)\n2025-05-13 23:47:48.115 | searx.exceptions.SearxParameterException: Invalid value \"False\" for parameter safesearch\n2025-05-13 23:47:48.117 | 2025-05-14 03:47:48,111 ERROR:searx.webapp: Invalid choice: False\n2025-05-13 23:47:48.117 | Traceback (most recent call last):\n2025-05-13 23:47:48.117 |   File \"/usr/local/searxng/searx/webapp.py\", line 459, in pre_request\n2025-05-13 23:47:48.117 |     preferences.parse_dict(sxng_request.form)\n2025-05-13 23:47:48.117 |     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:47:48.117 |   File \"/usr/local/searxng/searx/preferences.py\", line 525, in parse_dict\n2025-05-13 23:47:48.117 |     self.key_value_settings[user_setting_name].parse(user_setting)\n2025-05-13 23:47:48.118 |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n2025-05-13 23:47:48.118 |   File \"/usr/local/searxng/searx/preferences.py\", line 206, in parse\n2025-05-13 23:47:48.118 |     raise ValidationException('Invalid choice: {0}'.format(data))\n2025-05-13 23:47:48.118 | searx.preferences.ValidationException: Invalid choice: False\n2025-05-13 23:47:48.120 | 2025-05-14 03:47:48,114 ERROR:searx.webapp: search error: SearxParameterException\n2025-05-13 23:47:48.120 | Traceback (most recent call last):\n2025-05-13 23:47:48.120 |   File \"/usr/local/searxng/searx/webapp.py\", line 618, in search\n2025-05-13 23:47:48.120 |     search_query, raw_text_query, _,_ , selected_locale = get_search_query_from_webapp(\n2025-05-13 23:47:48.120 |                                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n2025-05-13 23:47:48.120 |         sxng_request.preferences, sxng_request.form\n2025-05-13 23:47:48.120 |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:47:48.120 |     )\n2025-05-13 23:47:48.120 |     ^\n2025-05-13 23:47:48.120 |   File \"/usr/local/searxng/searx/webadapter.py\", line 259, in get_search_query_from_webapp\n2025-05-13 23:47:48.120 |     query_safesearch = parse_safesearch(preferences, form)\n2025-05-13 23:47:48.120 |   File \"/usr/local/searxng/searx/webadapter.py\", line 84, in parse_safesearch\n2025-05-13 23:47:48.120 |     raise SearxParameterException('safesearch', query_safesearch)\n2025-05-13 23:47:48.120 | searx.exceptions.SearxParameterException: Invalid value \"False\" for parameter safesearch\n2025-05-13 23:47:48.123 | 2025-05-14 03:47:48,122 ERROR:searx.webapp: search error: SearxParameterException\n2025-05-13 23:47:48.123 | Traceback (most recent call last):\n2025-05-13 23:47:48.123 |   File \"/usr/local/searxng/searx/webapp.py\", line 618, in search\n2025-05-13 23:47:48.123 |     search_query, raw_text_query, _,_ , selected_locale = get_search_query_from_webapp(\n2025-05-13 23:47:48.123 |                                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n2025-05-13 23:47:48.123 |         sxng_request.preferences, sxng_request.form\n2025-05-13 23:47:48.123 |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:47:48.123 |     )\n2025-05-13 23:47:48.123 |     ^\n2025-05-13 23:47:48.123 |   File \"/usr/local/searxng/searx/webadapter.py\", line 259, in get_search_query_from_webapp\n2025-05-13 23:47:48.123 |     query_safesearch = parse_safesearch(preferences, form)\n2025-05-13 23:47:48.123 |   File \"/usr/local/searxng/searx/webadapter.py\", line 84, in parse_safesearch\n2025-05-13 23:47:48.123 |     raise SearxParameterException('safesearch', query_safesearch)\n2025-05-13 23:47:48.123 | searx.exceptions.SearxParameterException: Invalid value \"False\" for parameter safesearch\n2025-05-13 23:48:00.664 | 2025-05-14 03:48:00,663 ERROR:searx.webapp: Invalid choice: False\n2025-05-13 23:48:00.664 | Traceback (most recent call last):\n2025-05-13 23:48:00.664 |   File \"/usr/local/searxng/searx/webapp.py\", line 459, in pre_request\n2025-05-13 23:48:00.664 |     preferences.parse_dict(sxng_request.form)\n2025-05-13 23:48:00.664 |     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:48:00.664 |   File \"/usr/local/searxng/searx/preferences.py\", line 525, in parse_dict\n2025-05-13 23:48:00.664 |     self.key_value_settings[user_setting_name].parse(user_setting)\n2025-05-13 23:48:00.664 |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n2025-05-13 23:48:00.664 |   File \"/usr/local/searxng/searx/preferences.py\", line 206, in parse\n2025-05-13 23:48:00.664 |     raise ValidationException('Invalid choice: {0}'.format(data))\n2025-05-13 23:48:00.664 | searx.preferences.ValidationException: Invalid choice: False\n2025-05-13 23:48:00.667 | 2025-05-14 03:48:00,666 ERROR:searx.webapp: search error: SearxParameterException\n2025-05-13 23:48:00.667 | Traceback (most recent call last):\n2025-05-13 23:48:00.667 |   File \"/usr/local/searxng/searx/webapp.py\", line 618, in search\n2025-05-13 23:48:00.667 |     search_query, raw_text_query, _,_ , selected_locale = get_search_query_from_webapp(\n2025-05-13 23:48:00.667 |                                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n2025-05-13 23:48:00.667 |         sxng_request.preferences, sxng_request.form\n2025-05-13 23:48:00.667 |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:48:00.667 |     )\n2025-05-13 23:48:00.667 |     ^\n2025-05-13 23:48:00.667 |   File \"/usr/local/searxng/searx/webadapter.py\", line 259, in get_search_query_from_webapp\n2025-05-13 23:48:00.667 |     query_safesearch = parse_safesearch(preferences, form)\n2025-05-13 23:48:00.667 |   File \"/usr/local/searxng/searx/webadapter.py\", line 84, in parse_safesearch\n2025-05-13 23:48:00.667 |     raise SearxParameterException('safesearch', query_safesearch)\n2025-05-13 23:48:00.667 | searx.exceptions.SearxParameterException: Invalid value \"False\" for parameter safesearch\n2025-05-13 23:48:00.670 | 2025-05-14 03:48:00,669 ERROR:searx.webapp: Invalid choice: False\n2025-05-13 23:48:00.670 | Traceback (most recent call last):\n2025-05-13 23:48:00.670 |   File \"/usr/local/searxng/searx/webapp.py\", line 459, in pre_request\n2025-05-13 23:48:00.670 |     preferences.parse_dict(sxng_request.form)\n2025-05-13 23:48:00.670 |     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:48:00.670 |   File \"/usr/local/searxng/searx/preferences.py\", line 525, in parse_dict\n2025-05-13 23:48:00.670 |     self.key_value_settings[user_setting_name].parse(user_setting)\n2025-05-13 23:48:00.670 |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n2025-05-13 23:48:00.670 |   File \"/usr/local/searxng/searx/preferences.py\", line 206, in parse\n2025-05-13 23:48:00.670 |     raise ValidationException('Invalid choice: {0}'.format(data))\n2025-05-13 23:48:00.670 | searx.preferences.ValidationException: Invalid choice: False\n2025-05-13 23:48:00.671 | 2025-05-14 03:48:00,671 ERROR:searx.webapp: search error: SearxParameterException\n2025-05-13 23:48:00.671 | Traceback (most recent call last):\n2025-05-13 23:48:00.671 |   File \"/usr/local/searxng/searx/webapp.py\", line 618, in search\n2025-05-13 23:48:00.671 |     search_query, raw_text_query, _,_ , selected_locale = get_search_query_from_webapp(\n2025-05-13 23:48:00.671 |                                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n2025-05-13 23:48:00.671 |         sxng_request.preferences, sxng_request.form\n2025-05-13 23:48:00.671 |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-05-13 23:48:00.671 |     )\n2025-05-13 23:48:00.671 |     ^\n2025-05-13 23:48:00.671 |   File \"/usr/local/searxng/searx/webadapter.py\", line 259, in get_search_query_from_webapp\n2025-05-13 23:48:00.671 |     query_safesearch = parse_safesearch(preferences, form)\n2025-05-13 23:48:00.671 |   File \"/usr/local/searxng/searx/webadapter.py\", line 84, in parse_safesearch\n2025-05-13 23:48:00.671 |     raise SearxParameterException('safesearch', query_safesearch)\n2025-05-13 23:48:00.671 | searx.exceptions.SearxParameterException: Invalid value \"False\" for parameter safesearch  \n```\n",
      "state": "closed",
      "author": "theodorevo",
      "author_type": "User",
      "created_at": "2025-05-14T04:08:21Z",
      "updated_at": "2025-05-22T03:53:20Z",
      "closed_at": "2025-05-14T16:17:38Z",
      "labels": [
        "bug",
        "needs-replication"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/338/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/338",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/338",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:24.677285",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @theodorevo,\n\nWould you mind telling me what version of `SearXNG` you are using? If you navigate to http://host.docker.internal:8081/, it should say at the bottom of the page.",
          "created_at": "2025-05-14T13:21:28Z"
        },
        {
          "author": "theodorevo",
          "body": "I am using the latest Searxng image: 2025.5.13+1a8884f. \nDocker run commands: \nSearxng: `docker run -d -v D:/Docker/searxng:/etc/searxng -p 8081:8080 --name searxng --restart always searxng/searxng`\n\nLDR: `docker run -d -p 5000:5000 --add-host=host.docker.internal:host-gateway --network openwebui-ne",
          "created_at": "2025-05-14T14:35:13Z"
        },
        {
          "author": "djpetti",
          "body": "It looks like the latest version of SearXNG is actually `2025.5.14+c73b469`. Just tested with that and everything is working fine. Hmmm....\n\nDid you change the value of the \"safe search\" setting for SearXNG at all?",
          "created_at": "2025-05-14T14:56:34Z"
        },
        {
          "author": "theodorevo",
          "body": "So they had another update from when I did the test. I updated Searxng and still got the same issue. I'm attaching additional logs. It's strange that LDR is trying to search the full query with Searxng, but I also tried searching that full query in Searxng directly and I still got results.\n\n![Image]",
          "created_at": "2025-05-14T15:14:49Z"
        },
        {
          "author": "theodorevo",
          "body": "Safesearch is turned off in both Searxng and LDR:\n\n![Image](https://github.com/user-attachments/assets/87b964ee-90e4-4870-a250-e9091c346c5b)\n\n",
          "created_at": "2025-05-14T15:17:05Z"
        }
      ]
    },
    {
      "issue_number": 377,
      "title": "Follow fixed instruction plan.",
      "body": "\"Follow fixed instruction plan.\n\nI want to create a research report with a more or less fixed template of sections and questions I needed answered. What I don't have fixed is the answers and reflection processs after the initial answer has been generated.\n\nIs there a way to use this package to make a workflow like that ? Ideally I'd like the research and review agent to have a few interactions but I'd like to limit it at say 5 or something.\n\nThanks and appreciate all this package a lot.\"\n\nhttps://www.reddit.com/r/LocalDeepResearch/comments/1kqirvz/follow_fixed_instruction_plan/",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-05-21T21:57:21Z",
      "updated_at": "2025-05-21T22:02:00Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/377/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/377",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/377",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:24.918981",
      "comments": [
        {
          "author": "djpetti",
          "body": "This functionality should be made possible by #221...",
          "created_at": "2025-05-21T22:01:59Z"
        }
      ]
    },
    {
      "issue_number": 374,
      "title": "Is there any chance of having external embedding and reranking support?",
      "body": "Currently we can run embedding and reranking on CPU or via Llama, but it would be great to allow for embedding/rerank endpoints for faster generation.",
      "state": "open",
      "author": "HumerousGorgon",
      "author_type": "User",
      "created_at": "2025-05-21T00:40:52Z",
      "updated_at": "2025-05-21T16:08:53Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "docs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/374/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/374",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/374",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:25.129918",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @HumerousGorgon,\n\nWe already support remote endpoints for embedding, but it is a little bit subtle. If you change the \"Ollama endpoint URL\" setting, it will use that endpoint for both the LLM and the embedding models. \n\nWe did not support setting separate endpoints for LLMs and embedding models, ",
          "created_at": "2025-05-21T14:09:40Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "I definitely can see a usecase for seperate URLs.\nI use Llama.cpp and run a seperate server instance for my embedding URL.\n\nThanks for the info on the embedding model, though. I wonder what would happen if I set the Ollama endpoint URL to my llama.cpp embedding instance, but kept the config using th",
          "created_at": "2025-05-21T14:25:48Z"
        },
        {
          "author": "djpetti",
          "body": "Just to clarify: are you using Llama.cpp or Ollama? We currently don't support embeddings from Llama.cpp.\n\nIf you set the ollama endpoint URL and configure the embedding model type to Ollama, but keep you main LLM provider set to an external service, it *should* use the external service for LLM requ",
          "created_at": "2025-05-21T15:54:05Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Let's keep this open until we have similar flexibility for embeddings as we have for llm",
          "created_at": "2025-05-21T16:08:52Z"
        }
      ]
    },
    {
      "issue_number": 346,
      "title": "Allow /nothink for qwen 3 models",
      "body": "Please allow /nothink as a toggle for qwen 3 models. :)",
      "state": "open",
      "author": "shaneetrain",
      "author_type": "User",
      "created_at": "2025-05-15T17:49:34Z",
      "updated_at": "2025-05-21T14:02:20Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/346/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/346",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/346",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:25.322748",
      "comments": [
        {
          "author": "HumerousGorgon",
          "body": "I installed and configured cot_proxy, which runs in a docker container and presets Qwen3 to use think or no_think with the proper temperature, top k, min p, etc.\nI'd highly recommend looking into that for this project :)",
          "created_at": "2025-05-21T00:38:12Z"
        },
        {
          "author": "djpetti",
          "body": "Thanks for that suggestion! Perhaps the best way forward is to provide a `docker compose` configuration that includes `cot_proxy`.",
          "created_at": "2025-05-21T14:02:19Z"
        }
      ]
    },
    {
      "issue_number": 241,
      "title": "Custom context size",
      "body": "# Custom context size\n\n## TL;DR\nPlease add custom context window size at least for custom models such as llama and lmstudio. \n\n## Intro\nI'm not sure how exactly is it managed now, but according to my humble investigation the tool does not use more than 7k tokens in its requests to LLMs although I have spinned a 64k context window model to ensure that it can consume the whole detailed report.\n\n## Main idea\nIdeally there should either be a manual specification of the context window that can be used (in will be particularly handy for those who spin models with huge context), or an automatic detection of it.\n\n## Quotation\nI couldn't find any mentions of that idea besides the #3 but it's very specific while I want a broader functionality. ",
      "state": "closed",
      "author": "smbrine",
      "author_type": "User",
      "created_at": "2025-04-28T18:12:16Z",
      "updated_at": "2025-05-20T22:22:42Z",
      "closed_at": "2025-05-20T22:22:42Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/241/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "LearningCircuit"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/241",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/241",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:25.494405",
      "comments": [
        {
          "author": "l0nedigit",
          "body": "I second this.   Using llama.cpp with QwQ, the response continues to error due to the context size >512.",
          "created_at": "2025-05-06T16:18:02Z"
        }
      ]
    },
    {
      "issue_number": 367,
      "title": "Has anyone had any luck with using Brave?",
      "body": "Been trying to get Brave to work for the last hour.\nI've tried entering my API key via the docker-compose file, via the settings menu, everything!\nAll I get from the logs is:\n`Failed to create search engine 'brave': '<' not supported between instances of 'str' and 'int'`\n\nAnyone got a config they can run me through?",
      "state": "closed",
      "author": "HumerousGorgon",
      "author_type": "User",
      "created_at": "2025-05-20T15:22:50Z",
      "updated_at": "2025-05-20T21:09:46Z",
      "closed_at": "2025-05-20T21:09:46Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/367/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/367",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/367",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:25.700745",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @HumerousGorgon ,\n\nThis looks like a bug in the Brave engine. In the meantime, you might want to look into SearXNG, as it is well supported by LDR and can use both Brave and other search engines on the backend. ",
          "created_at": "2025-05-20T19:19:04Z"
        }
      ]
    },
    {
      "issue_number": 323,
      "title": "Can't use on Apple ARM processors?",
      "body": "**Describe the bug**\n\n```\n-> % docker pull localdeepresearch/local-deep-research\nUsing default tag: latest\nlatest: Pulling from localdeepresearch/local-deep-research\nno matching manifest for linux/arm64/v8 in the manifest list entries\n```\n\n**To Reproduce**\n\nSteps to reproduce the behavior:\n \n1. Run command 'docker pull localdeepresearch/local-deep-research'\n2. See error\n\n**Expected behavior**\n\nFor it to pull the image.\n\n**System Information:**\n\n - OS: macOS 14.7.3\n - Python Version: Python 3.13.3\n - Model Used: N/A\n - Hardware Specs: M3\n \n",
      "state": "open",
      "author": "taoeffect",
      "author_type": "User",
      "created_at": "2025-05-13T17:23:28Z",
      "updated_at": "2025-05-19T22:20:04Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/323/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/323",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/323",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:25.904424",
      "comments": [
        {
          "author": "djpetti",
          "body": "Unfortunately, none of us have a Mac to test on so some Mac-related issues go unnoticed. Our container is currently built through a GH action, so we might need to modify that to include ARM. \n\nIn the meantime, you can:\n- Build the container locally from the Dockerfile\n- Install without Docker",
          "created_at": "2025-05-13T17:35:01Z"
        },
        {
          "author": "taoeffect",
          "body": "Thanks! Yes building it locally using the `docker-compose.yml` file seems to be working.",
          "created_at": "2025-05-13T17:43:01Z"
        },
        {
          "author": "LearningCircuit",
          "body": "ARM support seems to be important, but I dont know how we can achieve it.",
          "created_at": "2025-05-13T18:06:17Z"
        },
        {
          "author": "djpetti",
          "body": "It looks like it's possible (albeit possibly complicated) to do this with GH actions. I'll keep this issue open until that is achieved.",
          "created_at": "2025-05-13T18:59:45Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I consider this more a enhancement than a bug.",
          "created_at": "2025-05-13T20:19:58Z"
        }
      ]
    },
    {
      "issue_number": 366,
      "title": "Strategy to handle Captcha in searxng",
      "body": "During deep research when duckduckgo shows captcha, there are warnings and json decode errors in the logs;\n\n```\n File \"/usr/local/searxng/searx/engines/duckduckgo.py\", line 363, in response\nsearxng                |     raise SearxEngineCaptchaException(suspended_time=0, message=f\"CAPTCHA ({resp.search_params['data'].get('kl')})\")\nsearxng                | searx.exceptions.SearxEngineCaptchaException: CAPTCHA (us-en), suspended_time=0\nsearxng                | 2025-05-19 09:14:32,107 WARNING:searx.engines.startpage: ErrorContext('searx/engines/startpage.py', 417, 'results_json = loads(results_raw)', 'json.decoder.JSONDecodeError', None, ('Extra data',)) False\nsearxng                | 2025-05-19 09:14:32,107 ERROR:searx.engines.startpage: exception : Extra data: line 1 column 3 (char 2)\nsearxng                | Traceback (most recent call last):\nsearxng                |   File \"/usr/local/searxng/searx/search/processors/online.py\", line 160, in search\nsearxng                |     search_results = self._search_basic(query, params)\nsearxng                |   File \"/usr/local/searxng/searx/search/processors/online.py\", line 148, in _search_basic\nsearxng                |     return self.engine.response(response)\nsearxng                |            ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n```\n\nSearxng's documentation recommends solving the captcha through the same IP - https://docs.searxng.org/admin/answer-captcha.html .\n\nPerhaps we could notify the user that captcha is being displayed so that users can open the search engine in the browser and solve the captcha.",
      "state": "closed",
      "author": "abishekmuthian",
      "author_type": "User",
      "created_at": "2025-05-19T10:53:08Z",
      "updated_at": "2025-05-19T13:35:51Z",
      "closed_at": "2025-05-19T13:35:51Z",
      "labels": [
        "wontfix"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/366/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/366",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/366",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:26.148331",
      "comments": [
        {
          "author": "djpetti",
          "body": "LDR works by performing dozens of searches in the course of answering a query. With this design, it is probably impractical to have it stop frequently and ask the user to solve a CAPTCHA. In the long term, it would be nice to get around it somehow, but I think for now we're not going to address it, ",
          "created_at": "2025-05-19T13:35:29Z"
        }
      ]
    },
    {
      "issue_number": 265,
      "title": "run error",
      "body": "install following the wayÔºö\n\n# Install the package\npip install local-deep-research\n\n# Install required browser automation tools\nplaywright install\n\n# For local models, install Ollama\n# Download from https://ollama.ai and then pull a model\nollama pull gemma3:12b\nThen run:\n\n# Start the web interface (recommended)\nldr-web # (OR python -m local_deep_research.web.app)\n\nHoweverÔºå it appears error when run it runs.\n\n16:40:36\nInfo\nGenerating search questions for iteration 2\n16:40:36\nInfo\nRunning parallel searches for iteration 2\n16:40:36\nMilestone\nResearch complete\n16:40:36\nMilestone\nSearch complete, preparing to generate summary...\n16:40:36\nError\nError during synthesis. Attempting fallback...\n16:40:36\nError\nUsing fallback synthesis due to unknown error\n16:40:36\nInfo\nGenerating clean summary from research data...\n16:40:36\nMilestone\nWriting research report to file...\n16:40:36\nMilestone\nResearch completed successfully",
      "state": "closed",
      "author": "hwuscut",
      "author_type": "User",
      "created_at": "2025-05-04T08:45:15Z",
      "updated_at": "2025-05-18T09:10:47Z",
      "closed_at": "2025-05-18T09:10:46Z",
      "labels": [
        "bug",
        "needs-replication"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 25,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/265/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/265",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/265",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:26.342615",
      "comments": []
    },
    {
      "issue_number": 337,
      "title": "SearNgx could not be found from local-deep-research",
      "body": "**Describe the bug**\nNot possible to use local  SearNgx\n**To Reproduce**\nInstalled SearNgx with Docker\nIs running on http://192.168.1.102:8080/ OR http://localhost:8080 without problems\n\nInstalled local-deep-research with pip\nis running on http://192.168.1.102:5000/ OR http://localhost:5000 without problems\nis connected with local docker ollama - without problems\n\nWhen using external SearchEngines - Everything is OK.\n\nWhen trying to use SearNgx always: \n\n**System Information:**\n - OS: [e.g. Ubuntu 22.04]\n - Python Version: [e.g. 3.10.0]\n - Model Used: [e.g. qwen3:30B]\n - Hardware Specs: [e.g. 64GB RAM, Jetson orin agx]\n\n**Additional context**\nAdd any other context about the problem here.\n\n**Output/Logs**\nINFO:local_deep_research.web_search_engines.search_engines_config:Registered local document collections as search engines\nINFO:local_deep_research.web_search_engines.search_engines_config:Loaded 10 search engines from configuration file\nINFO:local_deep_research.web_search_engines.search_engines_config:\n  arxiv, auto, brave, github, google_pse, pubmed, searxng, serpapi, wayback, wikipedia \n\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.personal_notes.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.personal_notes.class_name' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.project_docs.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.project_docs.class_name' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.research_papers.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.research_papers.class_name' in the database.\nINFO:local_deep_research.web_search_engines.search_engines_config:Registered local document collections as search engines\nINFO:local_deep_research.web_search_engines.search_engine_factory:Failed to create search engine 'searxng': lxml.html.clean module is now a separate project lxml_html_clean.\nInstall lxml[html_clean] or lxml_html_clean directly.\nERROR:local_deep_research.web_search_engines.search_engine_factory:Failed to create search engine for searxng - returned None\nERROR:local_deep_research.config.search_config:Failed to create search engine for tool: searxng\nINFO:local_deep_research.web.services.research_service:Successfully set search engine to: searxng\nINFO:local_deep_research.advanced_search_system.strategies.source_based_strategy:Starting source-based research on topic: bitte funktioniere doch endlich\nWARNING:local_deep_research.web.services.research_service:Detected error in formatted findings: Error: Unable to conduct research without a search engine.... stack trace: NoneType: None\n\nWARNING:local_deep_research.web.services.research_service:Detected unknown error in synthesis\nINFO:local_deep_research.web.services.research_service:Found formatted_findings of length: 58\nINFO:local_deep_research.web.services.research_service:Successfully converted to clean markdown of length: 58\nINFO:local_deep_research.web.services.research_service:Writing report to: research_outputs/quick_summary_bitte_funktioniere_doch_endlich_2025-05-14T01:01:37.140014.md\nINFO:local_deep_research.web.services.research_service:Updating database for research_id: 9\nINFO:local_deep_research.web.services.research_service:Database updated successfully for research_id: 9\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research_id: 9\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research 9\nINFO:local_deep_research.web.services.research_service:Resources cleaned up for research_id: 9\nIf applicable, add relevant output or error logs here\n```\n",
      "state": "closed",
      "author": "telemetrieTP23",
      "author_type": "User",
      "created_at": "2025-05-13T23:10:28Z",
      "updated_at": "2025-05-18T08:41:43Z",
      "closed_at": "2025-05-18T08:41:42Z",
      "labels": [
        "bug",
        "needs-replication"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/337/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/337",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/337",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:26.342635",
      "comments": [
        {
          "author": "glahera",
          "body": "Hi @telemetrieTP23 \nHow are you spinning up your LDR instance? Using the included docker-compose.yml file on the github or something else?",
          "created_at": "2025-05-14T03:06:32Z"
        },
        {
          "author": "telemetrieTP23",
          "body": "Hi [@glahera](https://github.com/glahera) as i wrote above i Installed local-deep-research with pip (because the standard docker file dont work on arm64 like jetson orin)\n",
          "created_at": "2025-05-14T09:48:01Z"
        },
        {
          "author": "djpetti",
          "body": "Hi @telemetrieTP23,\n\nI have not seen this error before. If I had to speculate, I suspect it comes from one of our dependencies using a deprecated `lxml` API, although I'm not sure which one. Maybe you're installing an outdated version of something. Can you please try the following?\n\n1. Install LDR f",
          "created_at": "2025-05-14T13:19:16Z"
        },
        {
          "author": "telemetrieTP23",
          "body": "Here we go: (the only thing i changed in installation is using the torch 2.7 cuda arm64 wheel from nvidia directly)\n\naiofiles==24.1.0\naiohappyeyeballs==2.6.1\naiohttp==3.11.18\naiosignal==1.3.2\nannotated-types==0.7.0\nanthropic==0.51.0\nanyio==4.9.0\narxiv==2.2.0\nasync-timeout==4.0.3\nattrs==25.3.0\nbackof",
          "created_at": "2025-05-14T15:28:26Z"
        },
        {
          "author": "djpetti",
          "body": "@telemetrieTP23 \n\nThanks for the information. I forgot to ask: what version of Jetpack are you using on the Jetson?\n\nAlso, as a workaround, you should be able to build the Docker container manually, even though we don't offer a pre-built image for ARM.",
          "created_at": "2025-05-15T13:06:34Z"
        }
      ]
    },
    {
      "issue_number": 344,
      "title": "Add a \"stop now\" button for detailed report generation",
      "body": "As was suggested in #239, it could be useful to have a \"stop now\" button for detailed reports that stops the generation after the current iteration, and forces it to generate the report immediately with the information that it has gathered. Presumably, this would simply remove any sections that haven't been researched at that point from the report.",
      "state": "open",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-05-14T22:10:55Z",
      "updated_at": "2025-05-18T08:15:44Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/344/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/344",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/344",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:26.588528",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "I would extend this to that \"we want to show the current state if it crashes\".",
          "created_at": "2025-05-18T08:15:43Z"
        }
      ]
    },
    {
      "issue_number": 348,
      "title": "Request: Decrease the size of the Docker image",
      "body": "Currently, the local-deep-research Docker image weighs 12.44GB. It's crazy much just for a small web UI. It would be better for the users to have a smaller image.",
      "state": "closed",
      "author": "bkosowski",
      "author_type": "User",
      "created_at": "2025-05-16T10:40:23Z",
      "updated_at": "2025-05-16T20:39:14Z",
      "closed_at": "2025-05-16T20:39:14Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/348/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/348",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/348",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:26.785741",
      "comments": [
        {
          "author": "djpetti",
          "body": "The main limitation now is the `torch` dependencies, which significantly increase the size of the image. In the future, we plan to move away from `sentence-transformers` for embeddings, which should help.",
          "created_at": "2025-05-16T13:46:12Z"
        }
      ]
    },
    {
      "issue_number": 339,
      "title": "Filename Generation Fails on Windows Due to Invalid Characters",
      "body": "**Describe the bug**\nWhen generating a report, the application uses datetime.now().isoformat() to include a timestamp in the filename. This results in colons (:) being present in the filename, which is not allowed on Windows operating systems. As a result, file creation fails or raises an error when run on Windows\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create quick report\n2. See [Errno 22] Invalid argument: 'research_outputs\\\\detailed_report_find_out_what_you_can_about_what_early_career_empl_2025-05-14T17:35:36.345996.md'\n3. Check console log and see actual error\n\n**Expected behavior**\nThe filename should be windows conform\n\n**System Information:**\n - OS: Windows 10 Build 19045\n - Python Version: Python 3.11.5\n - Model Used: [OLlamao](gemma3:12b)\n\n**Additional context**\nProblematic source  code is in `research_service.py` line 646  `datetime.now().isoformat()`\n\n**Output/Logs**\n```\n`Lib\\site-packages\\local_deep_research\\web\\services\\research_service.py\", line 544, in run_research_process\n    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: [Errno 22] Invalid argument: 'research_outputs\\\\quick_summary_query_2025-05-14T17:43:54.869247.md'\n`\n```\n",
      "state": "closed",
      "author": "niklasdrews",
      "author_type": "User",
      "created_at": "2025-05-14T15:56:32Z",
      "updated_at": "2025-05-14T19:07:23Z",
      "closed_at": "2025-05-14T19:07:23Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/339/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/339",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/339",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:26.944384",
      "comments": []
    },
    {
      "issue_number": 319,
      "title": "No module named 'local_deep_research.config.config_files'",
      "body": "**Describe the bug**\n```\nModuleNotFoundError: No module named 'local_deep_research.config.config_files'\n```\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run command 'python -m local_deep_research.main'\n2. See error\n\n**Expected behavior**\npdm install successfully and run it first time.\n\n\n**System Information:**\n - OS: MacOS\n - Python Version: 3.11\n - Model Used: not related\n - Hardware Specs: not related\n\n**Additional context**\n\n<img width=\"630\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fe9dbf0f-9f78-40ee-b6e5-a40cbb67f103\" />\n\n\n**Output/Logs**\n```\nRequest error when checking Ollama: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1055e3290>: Failed to establish a new connection: [Errno 61] Connection refused'))\nSelected provider ollama is not available.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/Users/samuel/work/llm/local-deep-research/src/local_deep_research/main.py\", line 6, in <module>\n    from .config.config_files import settings\nModuleNotFoundError: No module named 'local_deep_research.config.config_files'```\n",
      "state": "closed",
      "author": "samuelchen",
      "author_type": "User",
      "created_at": "2025-05-12T16:06:03Z",
      "updated_at": "2025-05-14T02:48:55Z",
      "closed_at": "2025-05-13T16:32:04Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/319/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/319",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/319",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:26.944405",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @samuelchen,\n\nWe recommend using the web interface instead of the CLI. The CLI is not very well supported at this point, and will probably be removed in the future.",
          "created_at": "2025-05-12T16:50:45Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I think we can make CLI work again maybe by utilizing programmatic access and building a CLI wrapper around it. @djpetti  ... ",
          "created_at": "2025-05-12T16:58:47Z"
        },
        {
          "author": "LearningCircuit",
          "body": "but yeah at the moment it is not working I think?",
          "created_at": "2025-05-12T16:59:18Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@samuelchen are you interested to join our discord dev chat?",
          "created_at": "2025-05-12T17:02:38Z"
        },
        {
          "author": "djpetti",
          "body": "> I think we can make CLI work again maybe by utilizing programmatic access and building a CLI wrapper around it. [@djpetti](https://github.com/djpetti) ...\n\nYeah... this is possible. Although maybe not high priority. I would advocate for deprecating the broken CLI in the meantime so that people don",
          "created_at": "2025-05-12T17:03:50Z"
        }
      ]
    },
    {
      "issue_number": 324,
      "title": "Detailed report search stuck in infinite loop + search doesn't stop",
      "body": "**Describe the bug**\n\nDeep search is stuck in infinite loop.\n\nAlso, cancelling the search doesn't stop it, it continues searching in the background even though the UI shows it as suspended. I can tell because the LM Studio server logs show continued completion queries.\n\n**To Reproduce**\n\nRunning locally using docker-compose and commit 09c70e1c9024b16a1052f501e988acfeb3e02847 and using these settings with LM Studio with model `qwen3-30b-a3b`:\n\n![Image](https://github.com/user-attachments/assets/986e683f-de61-4831-b317-644946951e0a)\n\n\n**Expected behavior**\n\nNo infinite loop, and when the search is cancelled for it to really be cancelled.\n\n**System Information:**\n  \n - OS: macOS 14\n - Model Used: `qwen3-30b-a3b`\n\n**Output/Logs**\n\nLogs from UI:\n\n<details>\n\n```\n5:48:29 PM\nInfo\nResearch started\n5:48:29 PM\nInfo\nStarting research process\n5:48:29 PM\nInfo\nUsing LMSTUDIO model: qwen3-30b-a3b\n5:48:29 PM\nInfo\nUsing search tool: auto\n5:48:29 PM\nInfo\nInitializing source-based research\n5:48:29 PM\nInfo\nStarting iteration 1/2\n5:48:29 PM\nInfo\nGenerating search questions for iteration 1\n5:48:38 PM\nInfo\nRunning parallel searches for iteration 1\n5:48:58 PM\nInfo\nCompleted search 1 of 3: ***************** ...\n5:49:44 PM\nInfo\nCompleted search 2 of 3: ***************** ...\n5:50:03 PM\nInfo\nCompleted search 3 of 3: ***************** ...\n5:50:03 PM\nInfo\nCompleted iteration 1/2\n5:50:03 PM\nInfo\nStarting iteration 2/2\n5:50:03 PM\nInfo\nGenerating search questions for iteration 2\n5:50:19 PM\nInfo\nRunning parallel searches for iteration 2\n5:51:10 PM\nInfo\nCompleted search 1 of 2: ***************** ...\n5:51:27 PM\nInfo\nCompleted search 2 of 2: ***************** ...\n5:51:27 PM\nInfo\nCompleted iteration 2/2\n5:51:27 PM\nInfo\nPerforming final filtering of all results\n5:51:44 PM\nInfo\nFiltered from 21 to 6 results\n5:51:44 PM\nInfo\nGenerating final synthesis\n5:52:02 PM\nInfo\nResearch complete\n5:52:02 PM\nInfo\nSearch complete, generating output\n5:52:02 PM\nInfo\nGenerating detailed report...\n5:52:19 PM\nInfo\nUsing LMSTUDIO model: qwen3-30b-a3b\n5:52:19 PM\nInfo\nUsing search tool: auto\n5:52:19 PM\nInfo\nInitializing source-based research\n5:52:19 PM\nInfo\nStarting iteration 1/2\n5:52:19 PM\nInfo\nGenerating search questions for iteration 1\n5:52:32 PM\nInfo\nRunning parallel searches for iteration 1\n5:53:05 PM\nInfo\nCompleted search 1 of 3: ***************** ...\n5:53:06 PM\nInfo\nCompleted search 2 of 3: ***************** ...\n5:53:06 PM\nInfo\nCompleted search 3 of 3: ***************** ...\n5:53:06 PM\nInfo\nCompleted iteration 1/2\n5:53:06 PM\nInfo\nStarting iteration 2/2\n5:53:06 PM\nInfo\nGenerating search questions for iteration 2\n5:53:19 PM\nInfo\nRunning parallel searches for iteration 2\n5:54:48 PM\nInfo\nCompleted search 1 of 2: ***************** ...\n5:54:56 PM\nInfo\nCompleted search 2 of 2: ***************** ...\n5:54:56 PM\nInfo\nCompleted iteration 2/2\n5:54:56 PM\nInfo\nPerforming final filtering of all results\n5:55:22 PM\nInfo\nFiltered from 21 to 5 results\n5:55:22 PM\nInfo\nGenerating final synthesis\n5:55:47 PM\nInfo\nResearch complete\n5:55:47 PM\nInfo\nUsing LMSTUDIO model: qwen3-30b-a3b\n5:55:47 PM\nInfo\nUsing search tool: auto\n5:55:47 PM\nInfo\nInitializing source-based research\n5:55:47 PM\nInfo\nStarting iteration 1/2\n5:55:47 PM\nInfo\nGenerating search questions for iteration 1\n5:55:59 PM\nInfo\nRunning parallel searches for iteration 1\n5:56:25 PM\nInfo\nCompleted search 1 of 3: ***************** ...\n5:56:25 PM\nInfo\nCompleted search 2 of 3: ***************** ...\n5:56:27 PM\nInfo\nCompleted search 3 of 3: ***************** ...\n5:56:27 PM\nInfo\nCompleted iteration 1/2\n5:56:27 PM\nInfo\nStarting iteration 2/2\n5:56:27 PM\nInfo\nGenerating search questions for iteration 2\n5:56:39 PM\nInfo\nRunning parallel searches for iteration 2\n5:58:10 PM\nInfo\nCompleted search 1 of 2: ***************** ...\n5:58:12 PM\nInfo\nCompleted search 2 of 2: ***************** ...\n5:58:12 PM\nInfo\nCompleted iteration 2/2\n5:58:12 PM\nInfo\nPerforming final filtering of all results\n5:58:30 PM\nInfo\nFiltered from 25 to 7 results\n5:58:30 PM\nInfo\nGenerating final synthesis\n5:58:53 PM\nInfo\nResearch complete\n5:58:53 PM\nInfo\nUsing LMSTUDIO model: qwen3-30b-a3b\n(2√ó)\n5:58:53 PM\nInfo\nUsing search tool: auto\n(2√ó)\n5:58:53 PM\nInfo\nInitializing source-based research\n(2√ó)\n5:58:53 PM\nInfo\nStarting iteration 1/2\n(2√ó)\n5:58:53 PM\nInfo\nGenerating search questions for iteration 1\n(2√ó)\n5:59:03 PM\nInfo\nRunning parallel searches for iteration 1\n(2√ó)\n5:59:56 PM\nInfo\nCompleted search 1 of 3: ***************** ...\n11:00:28 AM\nInfo\nResearch started\n11:00:28 AM\nInfo\nStarting research process\n11:00:28 AM\nMilestone\nCompleted search 1 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 2 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 3 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted iteration 1/2\n11:00:28 AM\nInfo\nStarting iteration 2/2\n11:00:28 AM\nInfo\nGenerating search questions for iteration 2\n11:00:28 AM\nInfo\nRunning parallel searches for iteration 2\n11:00:28 AM\nMilestone\nCompleted search 1 of 2: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 2 of 2: ***************** ...\n11:00:28 AM\nMilestone\nCompleted iteration 2/2\n11:00:28 AM\nInfo\nPerforming final filtering of all results\n11:00:28 AM\nInfo\nFiltered from 21 to 6 results\n11:00:28 AM\nInfo\nGenerating final synthesis\n11:00:28 AM\nMilestone\nResearch complete\n11:00:28 AM\nMilestone\nSearch complete, generating output\n11:00:28 AM\nInfo\nGenerating detailed report...\n11:00:28 AM\nInfo\nUsing LMSTUDIO model: qwen3-30b-a3b\n11:00:28 AM\nInfo\nUsing search tool: auto\n11:00:28 AM\nInfo\nInitializing source-based research\n11:00:28 AM\nInfo\nStarting iteration 1/2\n11:00:28 AM\nInfo\nGenerating search questions for iteration 1\n11:00:28 AM\nInfo\nRunning parallel searches for iteration 1\n11:00:28 AM\nMilestone\nCompleted search 1 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 2 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 3 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted iteration 1/2\n11:00:28 AM\nInfo\nStarting iteration 2/2\n11:00:28 AM\nInfo\nGenerating search questions for iteration 2\n11:00:28 AM\nInfo\nRunning parallel searches for iteration 2\n11:00:28 AM\nMilestone\nCompleted search 1 of 2: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 2 of 2: ***************** ...\n11:00:28 AM\nMilestone\nCompleted iteration 2/2\n11:00:28 AM\nInfo\nPerforming final filtering of all results\n11:00:28 AM\nInfo\nFiltered from 21 to 5 results\n11:00:28 AM\nInfo\nGenerating final synthesis\n11:00:28 AM\nMilestone\nResearch complete\n11:00:28 AM\nInfo\nUsing LMSTUDIO model: qwen3-30b-a3b\n11:00:28 AM\nInfo\nUsing search tool: auto\n11:00:28 AM\nInfo\nInitializing source-based research\n11:00:28 AM\nInfo\nStarting iteration 1/2\n11:00:28 AM\nInfo\nGenerating search questions for iteration 1\n11:00:28 AM\nInfo\nRunning parallel searches for iteration 1\n11:00:28 AM\nMilestone\nCompleted search 1 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 2 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 3 of 3: ***************** ...\n11:00:28 AM\nMilestone\nCompleted iteration 1/2\n11:00:28 AM\nInfo\nStarting iteration 2/2\n11:00:28 AM\nInfo\nGenerating search questions for iteration 2\n11:00:28 AM\nInfo\nRunning parallel searches for iteration 2\n11:00:28 AM\nMilestone\nCompleted search 1 of 2: ***************** ...\n11:00:28 AM\nMilestone\nCompleted search 2 of 2: ***************** ...\n11:00:28 AM\nMilestone\nCompleted iteration 2/2\n11:00:28 AM\nInfo\nPerforming final filtering of all results\n11:00:28 AM\nInfo\nFiltered from 25 to 7 results\n11:00:28 AM\nInfo\nGenerating final synthesis\n11:00:28 AM\nMilestone\nResearch complete\n11:00:28 AM\nInfo\nUsing LMSTUDIO model: qwen3-30b-a3b\n11:00:28 AM\nInfo\nUsing search tool: auto\n11:00:28 AM\nInfo\nInitializing source-based research\n11:00:28 AM\nInfo\nStarting iteration 1/2\n11:00:28 AM\nInfo\nGenerating search questions for iteration 1\n11:00:28 AM\nInfo\nRunning parallel searches for iteration 1\n11:00:28 AM\nMilestone\nCompleted search 1 of 3: ***************** ...\n11:00:43 AM\nMilestone\nCompleted search 2 of 3: ***************** ...\n11:00:43 AM\nMilestone\nCompleted search 3 of 3: ***************** ...\n11:00:43 AM\nMilestone\nCompleted iteration 1/2\n11:00:43 AM\nInfo\nStarting iteration 2/2\n11:00:43 AM\nInfo\nGenerating search questions for iteration 2\n11:00:58 AM\nInfo\nRunning parallel searches for iteration 2\n11:01:34 AM\nMilestone\nCompleted search 1 of 2: ***************** ...\n```\n\n</details>\n\nLogs from container:\n\n<details>\n\n```\n\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.personal_notes.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.personal_notes.class_name' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.project_docs.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.project_docs.class_name' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.research_papers.module_path' in the database.\nWARNING:local_deep_research.utilities.db_utils:Could not find setting 'search.engine.local.research_papers.class_name' in the database.\nINFO:local_deep_research.web_search_engines.search_engines_config:Registered local document collections as search engines\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:SEARCH_PLAN: Will try these engines in order: github, pubmed, wikipedia\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Trying search engine: github\nINFO:local_deep_research.web_search_engines.engines.search_engine_github:Getting GitHub previews for query: ********\nERROR:local_deep_research.web_search_engines.engines.search_engine_github:GitHub API error: 422 - {\"message\":\"Validation Failed\",\"errors\":[{\"message\":\"The search is longer than 256 characters.\",\"resource\":\"Search\",\"field\":\"q\",\"code\":\"invalid\"}],\"documentation_url\":\"https://docs.github.com/v3/search/\",\"status\":\"422\"}\nWARNING:local_deep_research.web_search_engines.engines.search_engine_github:No GitHub results found for query: ********\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:github returned no previews\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Trying search engine: pubmed\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Getting PubMed previews for query: ********\nINFO:openai._base_client:Retrying request to /chat/completions in 0.433534 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.416655 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.419183 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.959431 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.903169 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.816008 seconds\nERROR:local_deep_research.web_search_engines.engines.search_engine_pubmed:Error optimizing query: ********\nERROR:local_deep_research.web_search_engines.engines.search_engine_github:Error optimizing query with LLM: Connection error.\nINFO:local_deep_research.web_search_engines.engines.search_engine_github:Final GitHub query: ********\nERROR:local_deep_research.web_search_engines.engines.search_engine_github:Error optimizing query with LLM: Connection error.\nINFO:local_deep_research.web_search_engines.engines.search_engine_github:Final GitHub query: ********\nERROR:local_deep_research.web_search_engines.engines.search_engine_github:GitHub API error: 422 - {\"message\":\"Validation Failed\",\"errors\":[{\"message\":\"The search is longer than 256 characters.\",\"resource\":\"Search\",\"field\":\"q\",\"code\":\"invalid\"}],\"documentation_url\":\"https://docs.github.com/v3/search/\",\"status\":\"422\"}\nWARNING:local_deep_research.web_search_engines.engines.search_engine_github:No GitHub results found for query: ********\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:github returned no previews\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Trying search engine: pubmed\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Getting PubMed previews for query: ********\nINFO:openai._base_client:Retrying request to /chat/completions in 0.379263 seconds\nERROR:local_deep_research.web_search_engines.engines.search_engine_github:GitHub API error: 422 - {\"message\":\"Validation Failed\",\"errors\":[{\"message\":\"The search is longer than 256 characters.\",\"resource\":\"Search\",\"field\":\"q\",\"code\":\"invalid\"}],\"documentation_url\":\"https://docs.github.com/v3/search/\",\"status\":\"422\"}\nWARNING:local_deep_research.web_search_engines.engines.search_engine_github:No GitHub results found for query: ********\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:github returned no previews\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Trying search engine: arxiv\nINFO:local_deep_research.web_search_engines.engines.search_engine_arxiv:Getting paper previews from arXiv\nINFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=**************\nINFO:openai._base_client:Retrying request to /chat/completions in 0.861025 seconds\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Query '********' has 0 total results in PubMed\nINFO:openai._base_client:Retrying request to /chat/completions in 0.425979 seconds\nERROR:local_deep_research.web_search_engines.engines.search_engine_pubmed:Error optimizing query: ********\nINFO:openai._base_client:Retrying request to /chat/completions in 0.997394 seconds\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Query '********' has 367 total results in PubMed\nINFO:openai._base_client:Retrying request to /chat/completions in 0.459220 seconds\nERROR:local_deep_research.web_search_engines.engines.search_engine_pubmed:Error determining historical focus: Connection error.\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Using adaptive search strategy: rare_topic with filter: \"last 10 years\"[pdat]\nINFO:openai._base_client:Retrying request to /chat/completions in 0.994218 seconds\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:PubMed search for '********' found 0 results\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:No results with time filter, trying without time restrictions\nERROR:local_deep_research.web_search_engines.engines.search_engine_pubmed:Error determining historical focus: Connection error.\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Using adaptive search strategy: moderate_volume with filter: \"last 5 years\"[pdat]\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:PubMed search for '********' found 50 results\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:PubMed search for '********' found 0 results\nWARNING:local_deep_research.web_search_engines.engines.search_engine_pubmed:No PubMed results found using strategy: no_time_filter\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Simplifying query: ********\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Simplified query: ********\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Trying with simplified query: ********\nINFO:arxiv:Got first page: 50 of 2732411 total results\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:ENGINE_SELECTED: arxiv\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Successfully got 50 preview results from arxiv\nINFO:openai._base_client:Retrying request to /chat/completions in 0.474212 seconds\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Query '********' has 17953 total results in PubMed\nINFO:openai._base_client:Retrying request to /chat/completions in 0.482375 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.915771 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.876524 seconds\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Found 50 PubMed previews using strategy: moderate_volume\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:ENGINE_SELECTED: pubmed\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Successfully got 50 preview results from pubmed\nINFO:openai._base_client:Retrying request to /chat/completions in 0.459198 seconds\nINFO:openai._base_client:Retrying request to /chat/completions in 0.961392 seconds\nERROR:local_deep_research.web_search_engines.search_engine_base:Relevance filtering error: Connection error.\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n        pool_request.request\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 955, in _request\n    response = self._client.send(\n        request,\n        stream=stream or self._should_stream_response_body(request=request),\n        **kwargs,\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n        request,\n    ...<2 lines>...\n        history=[],\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n        request,\n        follow_redirects=follow_redirects,\n        history=history,\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/src/local_deep_research/web_search_engines/search_engine_base.py\", line 171, in _filter_for_relevance\n    response = self.llm.invoke(prompt)\n  File \"/app/src/local_deep_research/config/llm_config.py\", line 287, in invoke\n    response = self.base_llm.invoke(*args, **kwargs)\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 307, in invoke\n    self.generate_prompt(\n    ~~~~~~~~~~~~~~~~~~~~^\n        [self._convert_input(input)],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    ).generations[0][0],\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 843, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 683, in generate\n    self._generate_with_cache(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n        m,\n        ^^\n    ...<2 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 908, in _generate_with_cache\n    result = self._generate(\n        messages, stop=stop, run_manager=run_manager, **kwargs\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 955, in _generate\n    response = self.client.create(**payload)\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n    return func(*args, **kwargs)\n  File \"/app/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 914, in create\n    return self._post(\n           ~~~~~~~~~~^\n        \"/chat/completions\",\n        ^^^^^^^^^^^^^^^^^^^^\n    ...<41 lines>...\n        stream_cls=Stream[ChatCompletionChunk],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1242, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 919, in request\n    return self._request(\n           ~~~~~~~~~~~~~^\n        cast_to=cast_to,\n        ^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        retries_taken=retries_taken,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 979, in _request\n    return self._retry_request(\n           ~~~~~~~~~~~~~~~~~~~^\n        input_options,\n        ^^^^^^^^^^^^^^\n    ...<4 lines>...\n        response_headers=None,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n    return self._request(\n           ~~~~~~~~~~~~~^\n        options=options,\n        ^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        stream_cls=stream_cls,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 979, in _request\n    return self._retry_request(\n           ~~~~~~~~~~~~~~~~~~~^\n        input_options,\n        ^^^^^^^^^^^^^^\n    ...<4 lines>...\n        response_headers=None,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n    return self._request(\n           ~~~~~~~~~~~~~^\n        options=options,\n        ^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        stream_cls=stream_cls,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 989, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nERROR:local_deep_research.web_search_engines.engines.search_engine_pubmed:Error determining historical focus: Connection error.\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Using adaptive search strategy: high_volume with filter: \"last 1 year\"[pdat]\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:PubMed search for '********' found 50 results\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Simplified query found 50 results\nERROR:local_deep_research.web_search_engines.search_engine_base:Relevance filtering error: Connection error.\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n        pool_request.request\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 955, in _request\n    response = self._client.send(\n        request,\n        stream=stream or self._should_stream_response_body(request=request),\n        **kwargs,\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n        request,\n    ...<2 lines>...\n        history=[],\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n        request,\n        follow_redirects=follow_redirects,\n        history=history,\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/src/local_deep_research/web_search_engines/search_engine_base.py\", line 171, in _filter_for_relevance\n    response = self.llm.invoke(prompt)\n  File \"/app/src/local_deep_research/config/llm_config.py\", line 287, in invoke\n    response = self.base_llm.invoke(*args, **kwargs)\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 307, in invoke\n    self.generate_prompt(\n    ~~~~~~~~~~~~~~~~~~~~^\n        [self._convert_input(input)],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    ).generations[0][0],\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 843, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 683, in generate\n    self._generate_with_cache(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n        m,\n        ^^\n    ...<2 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 908, in _generate_with_cache\n    result = self._generate(\n        messages, stop=stop, run_manager=run_manager, **kwargs\n    )\n  File \"/app/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 955, in _generate\n    response = self.client.create(**payload)\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n    return func(*args, **kwargs)\n  File \"/app/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 914, in create\n    return self._post(\n           ~~~~~~~~~~^\n        \"/chat/completions\",\n        ^^^^^^^^^^^^^^^^^^^^\n    ...<41 lines>...\n        stream_cls=Stream[ChatCompletionChunk],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1242, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 919, in request\n    return self._request(\n           ~~~~~~~~~~~~~^\n        cast_to=cast_to,\n        ^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        retries_taken=retries_taken,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 979, in _request\n    return self._retry_request(\n           ~~~~~~~~~~~~~~~~~~~^\n        input_options,\n        ^^^^^^^^^^^^^^\n    ...<4 lines>...\n        response_headers=None,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n    return self._request(\n           ~~~~~~~~~~~~~^\n        options=options,\n        ^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        stream_cls=stream_cls,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 979, in _request\n    return self._retry_request(\n           ~~~~~~~~~~~~~~~~~~~^\n        input_options,\n        ^^^^^^^^^^^^^^\n    ...<4 lines>...\n        response_headers=None,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n    return self._request(\n           ~~~~~~~~~~~~~^\n        options=options,\n        ^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        stream_cls=stream_cls,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/app/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 989, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\n```\n\n</details>\n",
      "state": "open",
      "author": "taoeffect",
      "author_type": "User",
      "created_at": "2025-05-13T18:12:55Z",
      "updated_at": "2025-05-13T19:53:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/324/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/324",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/324",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:28.953029",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @taoeffect,\n\nThere are two things here, and I suspect only one is a bug. \n\nI think the detailed report is actually working as it should. There is a known issue (#239) with the progress bar where it will go to 100% prematurely during detailed reports. As far as the \"infinite loop\", I suspect it's ",
          "created_at": "2025-05-13T18:43:53Z"
        },
        {
          "author": "taoeffect",
          "body": "Huh, maybe you're right. I edited the title to add that second bug to it.",
          "created_at": "2025-05-13T19:53:43Z"
        }
      ]
    },
    {
      "issue_number": 306,
      "title": "\"Error: ‚Äò<‚Äô not supported between instances of ‚Äòint‚Äô and ‚Äòstr‚Äô\" got included in the Detailed Report",
      "body": "**Bug Description**\nIn one rare occasion today, LDR has spewed out \"Error: ‚Äò<‚Äô not supported between instances of ‚Äòint‚Äô and ‚Äòstr‚Äô\" in the Detailed Report.\n\n**Prompt**\nResearch Prompt: Vmware, Omnissa and Broadcom\nIteration: 3\nQuestions: 7\n\n**System Information:**\n - OS: MacOS Sequoia\n - Python Version: 3.13,2\n - Model Used: hf.co/Goekdeniz-Guelmez/Josiefied-Qwen3-4B-abliterated-v1-gguf:Q5_K_M\n - Hardware Specs: M1, 16GB RAM\n\n**Additional context**\nIt seems like somewhere in the LDR side an error didn't get caught, was handled incorrectly and passed directly to the API call, resulting the following parts:\n\n![Image](https://github.com/user-attachments/assets/f1c4dd44-3f32-489e-b99b-9b1bb0375a30)\n\n![Image](https://github.com/user-attachments/assets/a39ce1f9-d132-426a-a075-58529b0ee7a7)\n\n![Image](https://github.com/user-attachments/assets/b1aa531a-9b2c-4abc-ab63-9b5b13d16347)\n\n![Image](https://github.com/user-attachments/assets/f7fc49da-8eed-4072-9096-3c7b5dfc0112)\n\n\n**Output/Logs**\nLog will be attached later. Maybe add feature to quickly export log for a research?\n",
      "state": "closed",
      "author": "glahera",
      "author_type": "User",
      "created_at": "2025-05-09T14:52:34Z",
      "updated_at": "2025-05-13T14:24:35Z",
      "closed_at": "2025-05-13T14:24:34Z",
      "labels": [
        "bug",
        "needs-replication"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/306/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/306",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/306",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:29.143749",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @glahera,\n\nWe've seen similar issues before. I will know whether this is related after seeing your complete log output. Are you using the latest version of LDR?\n\nBtw, your quick log export feature idea might be a good one üòÉ ",
          "created_at": "2025-05-09T16:47:20Z"
        },
        {
          "author": "glahera",
          "body": "Hi @djpetti, digging down the container log shows the following error:\n```\nERROR:local_deep_research.advanced_search_system.filters.cross_engine_filter:Cross-engine filtering error: '<' not supported between instances of 'int' and 'str'\nERROR:local_deep_research.advanced_search_system.strategies.sou",
          "created_at": "2025-05-12T13:33:15Z"
        },
        {
          "author": "djpetti",
          "body": "Hey @glahera,\n\nWould you mind posting the full log if possible? I'd like to see what the system was doing in the lead-up to this error.\n\nAlso, please let me know what version you are using. It should say in the bottom left corner of the web UI.",
          "created_at": "2025-05-12T13:48:10Z"
        },
        {
          "author": "glahera",
          "body": "For sure, here's the log of the research up until the failure part\n[ldr-debug.log](https://github.com/user-attachments/files/20187112/ldr-debug.log)",
          "created_at": "2025-05-13T11:45:52Z"
        },
        {
          "author": "djpetti",
          "body": "Hi @glahera,\n\nThanks for the log. Would you mind also mentioning what version you are using? I thought this bug was fixed in a previous version, but it might be a regression.",
          "created_at": "2025-05-13T13:30:15Z"
        }
      ]
    },
    {
      "issue_number": 301,
      "title": "too many links in detailed report mode",
      "body": "We got over 220000+ links when we do some research, and the link number like this: \n\n[823] Izvemhu tazuzci ku ruGiventheshifttowardsheterogeneousintegrationandchipletdesigns,howareadvancementsinadaptiveTSVplacementandrouting‚ÄìutilizingtechniqueslikemachinelearningtooptimizeTSVlocationbasedonsignalintegrity,powerdelivery,andthermalmanagement‚Äìimpactingthedensityandperformanceof3DintegratedcircuitsasofMay2025?Ace hadlomsib.\nURL: http://as.hr/kaf\n\n[824] Ezunone vusGiventheshifttowardsheterogeneousintegrationandchipletdesigns,howareadvancementsinadaptiveTSVplacementandrouting‚ÄìutilizingtechniqueslikemachinelearningtooptimizeTSVlocationbasedonsignalintegrity,powerdelivery,andthermalmanagement‚Äìimpactingthedensityandperformanceof3DintegratedcircuitsasofMay2025?Wojocujot hokipuw rukawu daonada.\nURL: http://afu.id/liuno\n\n[1649] Appropriateness is all you need!\nURL: http://arxiv.org/abs/2304.14553v1\n\n[1650] Benchmarking High Bandwidth Memory on FPGAs\nURL: http://arxiv.org/abs/2005.04324v1",
      "state": "closed",
      "author": "lixy910915",
      "author_type": "User",
      "created_at": "2025-05-08T05:54:19Z",
      "updated_at": "2025-05-12T18:43:49Z",
      "closed_at": "2025-05-12T18:43:48Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/301/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/301",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/301",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:29.335569",
      "comments": [
        {
          "author": "glahera",
          "body": "There seems to be no re-indexing on actually used reference from the search results.",
          "created_at": "2025-05-08T07:06:59Z"
        },
        {
          "author": "djpetti",
          "body": "Hi @lixy910915, would it be possible to share the settings that you used, including the query, search tool, and LLM?",
          "created_at": "2025-05-08T13:11:51Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I noticed this bug, too. I think there is a calculation issue concerning the last citation. I will try to look into it over the weekend.",
          "created_at": "2025-05-08T21:16:10Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@lixy910915 @glahera this should be fixed. Please update to the latest version and try again.",
          "created_at": "2025-05-12T18:43:31Z"
        },
        {
          "author": "LearningCircuit",
          "body": "reopen if required",
          "created_at": "2025-05-12T18:43:48Z"
        }
      ]
    },
    {
      "issue_number": 295,
      "title": "SearXNG port is in API-key",
      "body": "I think we would change SearXNG port via API key varibale. This is a bit unintuitive and there should be a normal value for searxng in the config menu.",
      "state": "closed",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-05-06T22:45:41Z",
      "updated_at": "2025-05-11T09:18:15Z",
      "closed_at": "2025-05-11T09:18:15Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/295/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/295",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/295",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:29.498493",
      "comments": [
        {
          "author": "djpetti",
          "body": "While we're at it, we should probably support connecting to SearXNG instances that are not running locally.",
          "created_at": "2025-05-07T13:07:22Z"
        }
      ]
    },
    {
      "issue_number": 308,
      "title": "Outdated env var names in docs",
      "body": "**Describe the bug**\nSince [this commit](https://github.com/LearningCircuit/local-deep-research/commit/6cd6ce3090a58831b922a7094abe91f1609d8ab4), env vars convention is different, many env vars are different (ex: `OLLAMA_BASE_URL`)",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2025-05-10T09:41:19Z",
      "updated_at": "2025-05-10T12:15:11Z",
      "closed_at": "2025-05-10T12:15:10Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/308/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/308",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/308",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:29.701389",
      "comments": [
        {
          "author": "djpetti",
          "body": "The naming convention change was intentional. I thought I got them all changed in the docs, but apparently not. I will check again. Thanks for letting me know. ",
          "created_at": "2025-05-10T12:06:53Z"
        },
        {
          "author": "av",
          "body": "Thanks for taking a look!",
          "created_at": "2025-05-10T12:15:10Z"
        }
      ]
    },
    {
      "issue_number": 223,
      "title": "RAG settings",
      "body": "Is there detailed setting instructions for local RAG? I can not load local RAG.\nI placed the files in the \"/AiData/local_deep_research/dataset/research_papers\" directory, and configured these in \"local_collections.toml\":\n```\n[research_papers]\nname = \"Research Papers\"\ndescription = \"Academic research papers and articles\"\npaths = [\"/AiData/local_deep_research/dataset/research_papers\"]\nenabled = true\nembedding_device = \"cpu\"\nembedding_model = \"mxbai-embed-large\"\nembedding_model_type = \"ollama\"\nollama_base_url = \"http://localhost:11434\"\nmax_results = 20\nmax_filtered_results = 5\nchunk_size = 800\nchunk_overlap = 150\ncache_dir = \"__CACHE_DIR__/local_search/research_papers\"\nstrengths = [\"academic research\", \"scientific papers\", \"scholarly content\"]\nweaknesses = [\"potentially outdated\", \"limited to collected papers\"]\nreliability = 0.85\n```\n",
      "state": "closed",
      "author": "1320799775",
      "author_type": "User",
      "created_at": "2025-04-24T04:03:17Z",
      "updated_at": "2025-05-09T17:14:23Z",
      "closed_at": "2025-05-09T17:14:22Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/223/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/223",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/223",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:29.892631",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Does arxiv work for you now?",
          "created_at": "2025-04-24T05:50:07Z"
        },
        {
          "author": "smbrine",
          "body": "+1",
          "created_at": "2025-04-24T08:28:40Z"
        },
        {
          "author": "1320799775",
          "body": "> Does arxiv work for you now?\n\nYes, Arxiv can work for me now. \n\nBut I still can't connect the local RAG.\nThis is the log for 'local_all':\n```\nINFO:src.local_deep_research.web_search_engines.search_engines_config:Registered local document collections as search engines\n\n...\n\nINFO:local_deep_research",
          "created_at": "2025-04-25T05:48:25Z"
        },
        {
          "author": "djpetti",
          "body": "That configuration should work, assuming the folder is valid and accessible. @1320799775 , can you share your full output logs?",
          "created_at": "2025-04-26T12:08:09Z"
        },
        {
          "author": "1320799775",
          "body": "> That configuration should work, assuming the folder is valid and accessible. [@1320799775](https://github.com/1320799775) , can you share your full output logs?\n\nThis is my full output logs under Linux Ubuntu. \nIt seems that the path has not been found. But I have confirmed the existence and the p",
          "created_at": "2025-04-27T02:30:25Z"
        }
      ]
    },
    {
      "issue_number": 299,
      "title": "Multiple searches with the same query will overwrite previous outputs",
      "body": "**Describe the bug**\nPerforming multiple searches with the same query will produce separate entries in the history view. However, clicking on any entry will always show the report from the most recent search.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Start the web UI\n2. Perform a quick summary with a particular query\n3. Perform another quick summary with the same query\n4. Go to the history view, and select the first search.\n\n**Expected behavior**\nThe outputs from both searches should be preserved.\n\n**System Information:**\n - OS: Ubuntu 22.04\n - Python Version: 3.12.0\n - Model Used: llama-3.3-70b-instruct\n - Hardware Specs: 32 GB RAM, no GPU\n\n**Additional context**\nI suspect that this has to do with the naming of report files in the outputs directory. I notice that it is named based on the query. The problem is probably that newer searches with the same query are overwriting the older files.\n",
      "state": "closed",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-05-07T18:40:55Z",
      "updated_at": "2025-05-09T15:47:18Z",
      "closed_at": "2025-05-09T15:47:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/299/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/299",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/299",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.161895",
      "comments": []
    },
    {
      "issue_number": 297,
      "title": "GUI versioning needs to be automated",
      "body": "In the bottem left in the GUI there is a versioning visible. It is not automatically synced with the internal version of the tool and therefore confuses users. \n\nWe need to automate this version. https://www.reddit.com/r/LocalLLaMA/comments/1keh382/comment/mqqctrh/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button",
      "state": "closed",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-05-06T22:53:29Z",
      "updated_at": "2025-05-07T19:01:36Z",
      "closed_at": "2025-05-07T19:01:36Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/297/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/297",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/297",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.161915",
      "comments": []
    },
    {
      "issue_number": 296,
      "title": "Investigate connection issues between Local Deep Research and Open WebUI in Docker environment",
      "body": "There appears to be a connectivity issue when trying to use Local Deep Research with Open WebUI running in a separate Docker container. When configuring the \"Custom OpenAI-compatible API\" provider to point to Open WebUI (via host.docker.internal:3000/api/models), the connection fails despite proper API key configuration. https://www.reddit.com/r/LocalLLaMA/comments/1keh382/comment/mqqbvw2/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-05-06T22:49:48Z",
      "updated_at": "2025-05-06T22:49:54Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/296/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/296",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/296",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.161921",
      "comments": []
    },
    {
      "issue_number": 275,
      "title": "Error in cross engine filtering",
      "body": "> Think I'm getting a similar error.\n> \n> INFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\n> INFO:httpx:HTTP Request: POST http://192.168.10.58:30068/api/chat \"HTTP/1.1 200 OK\"\n> ERROR:local_deep_research.advanced_search_system.filters.cross_engine_filter:Cross-engine filtering error: '<' not supported between instances of 'int' and 'str'\n> ERROR:local_deep_research.advanced_search_system.strategies.source_based_strategy:Error in research process: '<' not supported between instances of 'int' and 'str'\n> ERROR:local_deep_research.advanced_search_system.strategies.source_based_strategy:Traceback (most recent call last):\n>   File \"/usr/local/lib/python3.13/site-packages/local_deep_research/advanced_search_system/filters/cross_engine_filter.py\", line 174, in filter_results\n>     max_filtered = min(self.max_results, len(ranked_results))\n> TypeError: '<' not supported between instances of 'int' and 'str'\n> During handling of the above exception, another exception occurred:\n> Traceback (most recent call last):\n>   File \"/usr/local/lib/python3.13/site-packages/local_deep_research/advanced_search_system/strategies/source_based_strategy.py\", line 310, in analyze_topic\n>     final_filtered_results = self.cross_engine_filter.filter_results(\n>         accumulated_search_results_across_all_iterations,\n>     ...<4 lines>...\n>         start_index=len(self.all_links_of_system),\n>     )\n>   File \"/usr/local/lib/python3.13/site-packages/local_deep_research/advanced_search_system/filters/cross_engine_filter.py\", line 199, in filter_results\n>     top_results = results[: min(self.max_results, len(results))]\n>                             ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> TypeError: '<' not supported between instances of 'int' and 'str'\n> WARNING:local_deep_research.web.services.research_service:Detected error in formatted findings: Error: '<' not supported between instances of 'int' and 'str'... stack trace: NoneType: None\n> WARNING:local_deep_research.web.services.research_service:Detected unknown error in synthesis\n> INFO:local_deep_research.web.services.research_service:Using current_knowledge as fallback\n> INFO:local_deep_research.web.services.research_service:Found formatted_findings of length: 61\n> INFO:local_deep_research.web.services.research_service:Successfully converted to clean markdown of length: 61 \n\n _Originally posted by @subpixeledgroup in [#265](https://github.com/LearningCircuit/local-deep-research/issues/265#issuecomment-2849886710)_",
      "state": "closed",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-05-05T13:15:55Z",
      "updated_at": "2025-05-06T13:42:42Z",
      "closed_at": "2025-05-05T18:36:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/275/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/275",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/275",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.161929",
      "comments": [
        {
          "author": "subpixeledgroup",
          "body": "Looks like everything works great now, thanks!\n\nQuick question, when i got this bug i was using the brave api. I switched it to my searxng instance and quick summary seemed to work, but when i tried detailed report it filled up my ram, went into swap and nearly crashed the vm. Is this due to that sa",
          "created_at": "2025-05-05T21:49:32Z"
        },
        {
          "author": "StatusQuo209",
          "body": "Still having this error even reinstalling in windows ",
          "created_at": "2025-05-06T13:42:41Z"
        }
      ]
    },
    {
      "issue_number": 162,
      "title": "Auto update",
      "body": "Any way you would consider adding auto updates for the windows setup? \n\nMaybe auto update on launch or auto restart once per day and pull the latest files off GitHub?\n\n--\n\nAlso, would look forward to adding a login page and searches separated per account, for situations where the webapp is publicly exposed. ",
      "state": "open",
      "author": "StatusQuo209",
      "author_type": "User",
      "created_at": "2025-04-09T09:23:59Z",
      "updated_at": "2025-05-06T07:18:06Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "rc/0.3.0"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/162/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/162",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/162",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.317206",
      "comments": [
        {
          "author": "djpetti",
          "body": "Account management would be a useful feature, but also a complicated one that probably won't get added any time soon. Auto update is possible.",
          "created_at": "2025-04-10T19:35:15Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Basic account management might be possible for searches and results but dont expect that it is very secure (could probably done via the database).",
          "created_at": "2025-04-11T18:47:09Z"
        },
        {
          "author": "djpetti",
          "body": "I've used [Fief](https://www.fief.dev/) in the past as a sort of turnkey account management solution, but deploying it is not trivial.",
          "created_at": "2025-04-11T21:17:58Z"
        },
        {
          "author": "djpetti",
          "body": "I'm adding this to the project plan, but to be clear, I think the scope is adding a feature that notifies the user when an updated version is available. Given the way Python packaging works, I don't think there's any safe way to implement full auto-updates (unless you use Docker, in which case you p",
          "created_at": "2025-04-22T16:53:37Z"
        },
        {
          "author": "StatusQuo209",
          "body": "Just want to add that the windows installer is still installing 0.2.0, not the newer 0.3.0 build. I am on Windows and pip and manual docker install is a little outside my skill level lolol",
          "created_at": "2025-05-02T06:47:28Z"
        }
      ]
    },
    {
      "issue_number": 268,
      "title": "'str' object has no attribute 'content'",
      "body": "**Describe the bug**\nI tried to run a research but I got an error `'str' object has no attribute 'content'`. Relevant error:\n\n```\nERROR:local_deep_research.config.llm_config:Model 'mistral' not found in Ollama. Available models: gemma3:27b, qwen3:32b, qwen3:30b-a3b, linux6200/bge-reranker-v2-m3:latest, snowflake-arctic-embed2:latest\nERROR:local_deep_research.web.services.research_service:Research failed: 'str' object has no attribute 'content'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/local_deep_research/web/services/research_service.py\", line 628, in run_research_process\n    final_report = report_generator.generate_report(results, query)\n  File \"/usr/local/lib/python3.13/site-packages/local_deep_research/report_generator.py\", line 48, in generate_report\n    structure = self._determine_report_structure(initial_findings, query)\n  File \"/usr/local/lib/python3.13/site-packages/local_deep_research/report_generator.py\", line 87, in _determine_report_structure\n    response = search_utilities.remove_think_tags(self.model.invoke(prompt).content)\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'content'\nERROR:local_deep_research.web.services.research_service:Ex\n```\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n1. Created a prompt with a description of what I need to research. Settings:\n\n![Image](https://github.com/user-attachments/assets/d6243fcb-dc68-46f6-a77f-99680fda2b02)\n\n2. Ran the research.\n\n**Expected behavior**\n\nNo errors, in-depth research completed.\n\n**System Information:**\n - OS: Docker 28.0.4\n - Python Version: latest used in docker image\n - Model Used: gemma:27b from ollama\n - Hardware Specs: [RTX 5090]\n\n**Additional context**\n\nI think the model was not correctly selected but there could be some other issue.\n\n**Output/Logs**\n\nPlease see the whole log trace: https://pastebin.com/vXAXUu0w",
      "state": "closed",
      "author": "criscola",
      "author_type": "User",
      "created_at": "2025-05-04T12:16:44Z",
      "updated_at": "2025-05-04T20:26:41Z",
      "closed_at": "2025-05-04T20:26:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/268/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/268",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/268",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.520280",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Oh interesting I have not seen this one before. ",
          "created_at": "2025-05-04T12:57:14Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@djpetti  do you think that might be related to recent changes in v0.3.1?",
          "created_at": "2025-05-04T12:59:55Z"
        },
        {
          "author": "djpetti",
          "body": "It would surprise me, since those changes are pretty minor. One of the things that's odd to me is that it seems to be trying to use `mistral` even though that is not the model selected. I wonder if that might be related to the problem.\n\n@criscola Can you share the complete output?",
          "created_at": "2025-05-04T13:11:26Z"
        },
        {
          "author": "criscola",
          "body": "The behavior of the tool is quite weird @djpetti because `gemma3:27b` is loaded in memory according to ollama and nvidia-smi, but then looking at the logs:\n\n```\n Info Research started\n15:22:45 Info Starting research process\n**15:22:45 Info Using ollama model: mistral**\n15:22:45 Info Using search too",
          "created_at": "2025-05-04T13:24:17Z"
        },
        {
          "author": "djpetti",
          "body": "Yes, this is quite unusual. \n\nJust to be sure that everything is getting set right, can you try selecting a different model, and then selecting `gemma` again? You should see messages in the top right indicating that the setting was successfully changed",
          "created_at": "2025-05-04T13:29:44Z"
        }
      ]
    },
    {
      "issue_number": 262,
      "title": "0.3.0 , ldr-web ,get error",
      "body": "**Describe the bug**\nmy envs:\nubnutu 24.04 , python=3.11 , local-deep-research=0.3.0\n\nI install local-deep-research by pip . When I have installed local-deep-research and playwright  , then run ldr-web , get the error:\n-----------------------------------------------------------------------------------------------------\n(localdp) gqchen@gqchen-mpc:~/local-deep-research$ ldr-web\nTraceback (most recent call last):\n  File \"/home/gqchen/miniconda3/envs/localdp/bin/ldr-web\", line 5, in <module>\n    from local_deep_research.web.app import main\n  File \"/home/gqchen/miniconda3/envs/localdp/lib/python3.11/site-packages/local_deep_research/__init__.py\", line 9, in <module>\n    from .config.llm_config import get_llm\n  File \"/home/gqchen/miniconda3/envs/localdp/lib/python3.11/site-packages/local_deep_research/config/llm_config.py\", line 9, in <module>\n    from ..utilities.db_utils import get_db_setting\n  File \"/home/gqchen/miniconda3/envs/localdp/lib/python3.11/site-packages/local_deep_research/utilities/db_utils.py\", line 9, in <module>\n    from ..web.services.settings_manager import SettingsManager, check_env_setting\n  File \"/home/gqchen/miniconda3/envs/localdp/lib/python3.11/site-packages/local_deep_research/web/services/settings_manager.py\", line 37\n    env_variable_name = f\"LDR_{\"_\".join(key.split(\".\")).upper()}\"\n                                ^\nSyntaxError: f-string: expecting '}'\n---------------------------------------------------------------------------------------------\n\n**Expected behavior**\nldr-web can run well.\n\n**System Information:**\n - OS: [e.g. Ubuntu 24.04]\n - Python Version: 3.11\n - Model Used: qwen3 4b\n - Hardware Specs: AMD 8845HS, 96GB RAM\n\n**Additional context**\nAs I install local-deep-research **0.2.2** , all is well.\n",
      "state": "closed",
      "author": "gqchen-dz",
      "author_type": "User",
      "created_at": "2025-05-03T14:32:08Z",
      "updated_at": "2025-05-04T03:51:45Z",
      "closed_at": "2025-05-04T03:51:45Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/262/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/262",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/262",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.720403",
      "comments": [
        {
          "author": "djpetti",
          "body": "Looks like a simple issue with f-string handling, should be an easy fix...",
          "created_at": "2025-05-03T14:35:32Z"
        },
        {
          "author": "djpetti",
          "body": "As a temporary fix, this should work correctly with Python >=3.12.",
          "created_at": "2025-05-03T14:38:30Z"
        },
        {
          "author": "djpetti",
          "body": "@gqchen-dz Please confirm that this is fixed in the current `main` branch.",
          "created_at": "2025-05-03T14:42:56Z"
        },
        {
          "author": "gqchen-dz",
          "body": "thanks.\nIt has been fixed.",
          "created_at": "2025-05-04T03:50:45Z"
        }
      ]
    },
    {
      "issue_number": 216,
      "title": "Migrate all settings to the DB",
      "body": "Right now, settings are still spread around between the config files, DB, and environment variables. All settings should be transitioned to use the DB, and config files should be completely deprecated. Environment variables can be kept, but they should simply override settings in the DB. The settings U.I. should allow all these settings to be modified from the web interface.",
      "state": "closed",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-21T22:12:47Z",
      "updated_at": "2025-05-01T18:23:11Z",
      "closed_at": "2025-05-01T18:23:11Z",
      "labels": [
        "enhancement",
        "rc/0.3.0"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/216/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/216",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/216",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.939526",
      "comments": []
    },
    {
      "issue_number": 237,
      "title": "Detailed reports appear broken with certain search engines",
      "body": "**Describe the bug**\nWhen generating detailed reports with certain search engines (observed with Arxiv and Wikipedia, but not SearXNG currently), the report ends up blank. Section titles and references are generated, but no section content.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Start LDR\n2. Set search engine to Arxiv\n3. Generate a detailed report\n\n**Expected behavior**\nIt should create a correct report with content.\n\n**Output/Logs**\nExample console output:\n```\n/home/daniel/git/local-deep-research/.venv/bin/python -m src.local_deep_research.web.app \nINFO:src.local_deep_research.config.llm_config:Checking Ollama availability at http://localhost:11434/api/tags\nINFO:src.local_deep_research.config.llm_config:Ollama is available. Status code: 200\nINFO:src.local_deep_research.config.llm_config:Response preview: {\"models\":[{\"name\":\"mxbai-embed-large:latest\",\"model\":\"mxbai-embed-large:latest\",\"modified_at\":\"2025\nINFO:src.local_deep_research.config.config_files:Looking for config in: /home/daniel/.config/local_deep_research\nINFO:src.local_deep_research.config.llm_config:Available providers: ['ollama', 'vllm']\nINFO:src.local_deep_research.web_search_engines.search_engines_config:Loaded 11 search engines from configuration file\nINFO:src.local_deep_research.web_search_engines.search_engines_config:\n  arxiv, auto, brave, github, google_pse, local_all, pubmed, searxng, serpapi, wayback, wikipedia \n\nINFO:src.local_deep_research.web_search_engines.search_engines_config:Registered local document collections as search engines\nData directory already exists at: /home/daniel/git/local-deep-research/data\nUsing package static path: /home/daniel/git/local-deep-research/src/local_deep_research/web/static\nUsing package template path: /home/daniel/git/local-deep-research/src/local_deep_research/web/templates\nINFO:__main__:Running schema upgrades on existing database\nINFO:src.local_deep_research.web.database.schema_upgrade:Running schema upgrades on /home/daniel/git/local-deep-research/src/data/ldr.db\nINFO:src.local_deep_research.web.database.schema_upgrade:Table 'research_log' does not exist, no action needed\nINFO:src.local_deep_research.web.database.schema_upgrade:Schema upgrades completed successfully\nINFO:src.local_deep_research.web.app_factory:Using database at /home/daniel/git/local-deep-research/src/data/ldr.db\nINFO:src.local_deep_research.web.database.migrations:Settings table already has 50 rows, skipping import\nINFO:src.local_deep_research.web.database.migrations:Predefined settings setup complete\nINFO:src.local_deep_research.web.services.socket_service:Socket.IO instance attached to socket service\nINFO:src.local_deep_research.config.config_files:Looking for config in: /home/daniel/.config/local_deep_research\nINFO:__main__:OPENAI_API_KEY not found in environment variables, OpenAI integration disabled\nINFO:__main__:Starting web server on 0.0.0.0:5000 (debug: True)\nWARNING:werkzeug:Werkzeug appears to be used in a production deployment. Consider switching to a production web server instead.\n * Serving Flask app 'src.local_deep_research.web.app_factory'\n * Debug mode: on\nINFO:src.local_deep_research.config.llm_config:Checking Ollama availability at http://localhost:11434/api/tags\nINFO:src.local_deep_research.config.llm_config:Ollama is available. Status code: 200\nINFO:src.local_deep_research.config.llm_config:Response preview: {\"models\":[{\"name\":\"mxbai-embed-large:latest\",\"model\":\"mxbai-embed-large:latest\",\"modified_at\":\"2025\nINFO:src.local_deep_research.config.config_files:Looking for config in: /home/daniel/.config/local_deep_research\nINFO:src.local_deep_research.config.llm_config:Available providers: ['ollama', 'vllm']\nINFO:src.local_deep_research.web_search_engines.search_engines_config:Loaded 11 search engines from configuration file\nINFO:src.local_deep_research.web_search_engines.search_engines_config:\n  arxiv, auto, brave, github, google_pse, local_all, pubmed, searxng, serpapi, wayback, wikipedia \n\nINFO:src.local_deep_research.web_search_engines.search_engines_config:Registered local document collections as search engines\nINFO:__main__:Running schema upgrades on existing database\nINFO:src.local_deep_research.web.database.schema_upgrade:Running schema upgrades on /home/daniel/git/local-deep-research/src/data/ldr.db\nINFO:src.local_deep_research.web.database.schema_upgrade:Table 'research_log' does not exist, no action needed\nINFO:src.local_deep_research.web.database.schema_upgrade:Schema upgrades completed successfully\nINFO:src.local_deep_research.web.app_factory:Using database at /home/daniel/git/local-deep-research/src/data/ldr.db\nData directory already exists at: /home/daniel/git/local-deep-research/data\nUsing package static path: /home/daniel/git/local-deep-research/src/local_deep_research/web/static\nUsing package template path: /home/daniel/git/local-deep-research/src/local_deep_research/web/templates\nINFO:src.local_deep_research.web.database.migrations:Settings table already has 50 rows, skipping import\nINFO:src.local_deep_research.web.database.migrations:Predefined settings setup complete\nINFO:src.local_deep_research.web.services.socket_service:Socket.IO instance attached to socket service\nINFO:src.local_deep_research.config.config_files:Looking for config in: /home/daniel/.config/local_deep_research\nINFO:__main__:OPENAI_API_KEY not found in environment variables, OpenAI integration disabled\nINFO:__main__:Starting web server on 0.0.0.0:5000 (debug: True)\nWARNING:werkzeug:Werkzeug appears to be used in a production deployment. Consider switching to a production web server instead.\nWARNING:werkzeug: * Debugger is active!\nINFO:src.local_deep_research.web.services.socket_service:Client connected: Z08T3hXM_Nm1JwuoAAAC\nINFO:src.local_deep_research.web.services.socket_service:Client connected: j8tYLPHwc7bwVZkxAAAD\nINFO:src.local_deep_research.config.config_files:Looking for config in: /home/daniel/.config/local_deep_research\nINFO:src.local_deep_research.web.services.socket_service:Client connected: RKrlArMrU-0Hu8mFAAAF\nGetting LLM with model: llama3.2:latest, temperature: 0.7, provider: ollama\nINFO:src.local_deep_research.web.routes.research_routes:Starting research with provider: OLLAMA, model: llama3.2:latest, search engine: wikipedia\nINFO:src.local_deep_research.web.routes.research_routes:Additional parameters: max_results=None, time_period=None, iterations=2, questions=3\nINFO:src.local_deep_research.web.services.research_service:Starting research process for ID 11, query: give me a very short report on climate change max 2 headings and no subheadings\nINFO:src.local_deep_research.web.services.research_service:Research parameters: provider=OLLAMA, model=llama3.2:latest, search_engine=wikipedia, max_results=None, time_period=None, iterations=2, questions_per_iteration=3, custom_endpoint=https://openrouter.ai/api/v1\nINFO:src.local_deep_research.web.services.research_service:Overriding system settings with: provider=OLLAMA, model=llama3.2:latest, search_engine=wikipedia\nINFO:src.local_deep_research.config.llm_config:Checking Ollama availability at http://localhost:11434/api/tags\nINFO:src.local_deep_research.config.llm_config:Ollama is available. Status code: 200\nINFO:src.local_deep_research.config.llm_config:Response preview: {\"models\":[{\"name\":\"mxbai-embed-large:latest\",\"model\":\"mxbai-embed-large:latest\",\"modified_at\":\"2025\nINFO:src.local_deep_research.config.llm_config:Checking if model 'llama3.2:latest' exists in Ollama\nINFO:src.local_deep_research.config.llm_config:Available Ollama models: mxbai-embed-large:latest, deepseek-r1:14b, deepseek-coder-v2:16b, llama3.2:latest\nINFO:src.local_deep_research.config.llm_config:Creating ChatOllama with model=llama3.2:latest, base_url=http://localhost:11434\nINFO:src.local_deep_research.config.llm_config:Testing Ollama model with simple invocation\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.web.services.socket_service:Client connected: PQX43UAwIgmtk0ofAAAH\nINFO:src.local_deep_research.config.llm_config:Ollama test successful. Response type: <class 'langchain_core.messages.ai.AIMessage'>\nINFO:src.local_deep_research.web.services.research_service:Successfully set LLM to: provider=OLLAMA, model=llama3.2:latest\nINFO:src.local_deep_research.config.search_config:Creating search engine with tool: wikipedia\nINFO:src.local_deep_research.config.search_config:Search config: tool=wikipedia, max_results=10, time_period=all\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: wikipedia with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:src.local_deep_research.web.services.socket_service:Client connected: eLbTD-n9PivngyVvAAAJ\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating wikipedia with filtered parameters: dict_keys(['max_results', 'include_content', 'max_filtered_results'])\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_wikipedia.WikipediaSearchEngine object at 0x74c799994c20>>\nINFO:src.local_deep_research.config.search_config:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.search_system:Initializing AdvancedSearchSystem with strategy_name='source-based'\nINFO:src.local_deep_research.search_system:Creating SourceBasedSearchStrategy instance\nINFO:src.local_deep_research.search_system:Created strategy of type: SourceBasedSearchStrategy\nINFO:src.local_deep_research.config.search_config:Creating search engine with tool: wikipedia\nINFO:src.local_deep_research.config.search_config:Search config: tool=wikipedia, max_results=10, time_period=all\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: wikipedia with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating wikipedia with filtered parameters: dict_keys(['max_results', 'include_content', 'max_filtered_results'])\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_wikipedia.WikipediaSearchEngine object at 0x74c79a3fe490>>\nINFO:src.local_deep_research.config.search_config:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.web.services.research_service:Successfully set search engine to: wikipedia\nINFO:src.local_deep_research.advanced_search_system.strategies.source_based_strategy:Starting source-based research on topic: give me a very short report on climate change max 2 headings and no subheadings\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generating follow-up questions...\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generated 3 follow-up questions\nINFO:src.local_deep_research.advanced_search_system.strategies.source_based_strategy:Using questions for iteration 1: ['give me a very short report on climate change max 2 headings and no subheadings', 'What is the current global average temperature and how does it compare to pre-industrial levels?', 'How much have global greenhouse gas emissions increased since the Paris Agreement in 2015, and what impact has this had on climate change?', 'What is the latest scientific consensus on the causes and effects of climate change, and what steps can individuals take to mitigate their carbon footprint?']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: give me a very short report on climate change max 2 headings and no subheadings\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: What is the current global average temperature and how does it compare to pre-industrial levels?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: How much have global greenhouse gas emissions increased since the Paris Agreement in 2015, and what impact has this had on climate change?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: What is the latest scientific consensus on the causes and effects of climate change, and what steps can individuals take to mitigate their carbon footprint?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 8 Wikipedia results: ['Russian invasion of Ukraine', '2022 in science', 'Republican Party efforts to disrupt the 2024 United States presidential election', 'Attempts to overturn the 2020 United States presidential election', 'China‚ÄìUnited States relations', 'Trumpism', 'Foreign policy of the first Donald Trump administration', 'History of Mexican Americans']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Climate change in the United States', 'Economic analysis of climate change', 'Paris Agreement', 'Climate change in New Zealand', 'Effects of climate change on agriculture', 'Climate change in Canada', 'Greenhouse gas emissions', 'Climate change', 'Climate change in Australia', 'Greenhouse gas emissions from agriculture']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Climate change mitigation', 'Climate change in Australia', 'Climate change in New Zealand', 'Climate change denial', '2023 United Nations Climate Change Conference', '2023 in climate change', 'Carbon emission trading', 'Environmental policy of the Joe Biden administration', '2021 United Nations Climate Change Conference', 'Nuclear power debate']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Global surface temperature', 'Climate change', 'Climate', 'Special Report on Global Warming of 1.5 ¬∞C', 'Hockey stick graph (global temperature)', 'Sea level rise', 'Sea level', 'Temperature record of the last 2,000 years', 'Paris Agreement', 'Effects of climate change']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 8 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generating follow-up questions...\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generated 3 follow-up questions\nINFO:src.local_deep_research.advanced_search_system.strategies.source_based_strategy:Generated questions for iteration 2: ['What is the current global average temperature and how does it compare to pre-industrial levels, taking into account recent updates in climate modeling and data analysis that have been published since 2023?', 'How do greenhouse gas emissions from different sectors, such as agriculture and transportation, contribute to the overall carbon footprint of countries like New Zealand and Australia, which are mentioned in previous search results but require more specific information on their current levels and trends?', 'What is the latest scientific consensus on the causes and effects of climate change, specifically regarding the role of tipping points and feedback loops in amplifying the impacts of warming, and how do these findings inform policy decisions for mitigating and adapting to climate change?']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: What is the current global average temperature and how does it compare to pre-industrial levels, taking into account recent updates in climate modeling and data analysis that have been published since 2023?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: How do greenhouse gas emissions from different sectors, such as agriculture and transportation, contribute to the overall carbon footprint of countries like New Zealand and Australia, which are mentioned in previous search results but require more specific information on their current levels and trends?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: What is the latest scientific consensus on the causes and effects of climate change, specifically regarding the role of tipping points and feedback loops in amplifying the impacts of warming, and how do these findings inform policy decisions for mitigating and adapting to climate change?\nERROR:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Error getting Wikipedia previews: An unknown error occured: \"Search request is longer than the maximum allowed length. (Actual: 304; allowed: 300)\". Please report it on GitHub!\nINFO:local_deep_research.web_search_engines.search_engine_base:Search engine WikipediaSearchEngine returned no preview results for query: How do greenhouse gas emissions from different sectors, such as agriculture and transportation, contribute to the overall carbon footprint of countries like New Zealand and Australia, which are mentioned in previous search results but require more specific information on their current levels and trends?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 0 Wikipedia results: []\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:No Wikipedia results found for query: What is the latest scientific consensus on the causes and effects of climate change, specifically regarding the role of tipping points and feedback loops in amplifying the impacts of warming, and how do these findings inform policy decisions for mitigating and adapting to climate change?\nINFO:local_deep_research.web_search_engines.search_engine_base:Search engine WikipediaSearchEngine returned no preview results for query: What is the latest scientific consensus on the causes and effects of climate change, specifically regarding the role of tipping points and feedback loops in amplifying the impacts of warming, and how do these findings inform policy decisions for mitigating and adapting to climate change?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Global surface temperature', 'Economic analysis of climate change', 'Climate change', 'Climate change in Africa', 'Climate change in the Arctic', 'Climate change in Australia', 'Effects of climate change on oceans', 'Climate change in Europe', 'History of climate change science', 'Climate change in China']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.filters.cross_engine_filter:Cross-engine filtering kept 2 out of 48 results with reordering=True, reindex=True\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Added 2 documents to repository\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Set questions for 2 iterations\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Formatting final report. Number of detailed findings: 3. Synthesized content length: 1216. Number of question iterations: 2\nINFO:src.local_deep_research.utilities.search_utilities:Inside format_findings utility. Findings count: 3, Questions iterations: 2\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 2 links to markdown...\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 3 detailed finding items.\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 2 links to markdown...\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 2 links to markdown...\nINFO:src.local_deep_research.utilities.search_utilities:Finished format_findings utility.\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Successfully formatted final report.\nINFO:src.local_deep_research.web.services.research_service:Found formatted_findings of length: 5162\nINFO:src.local_deep_research.web.services.research_service:Successfully converted to clean markdown of length: 5162\nINFO:src.local_deep_research.web.services.research_service:Writing report to: research_outputs/quick_summary_give_me_a_very_short_report_on_climate_change_max_.md\nINFO:src.local_deep_research.web.services.research_service:Updating database for research_id: 11\nINFO:src.local_deep_research.web.services.research_service:Database updated successfully for research_id: 11\nINFO:src.local_deep_research.web.services.research_service:Cleaning up resources for research_id: 11\nINFO:src.local_deep_research.web.services.research_service:Cleaning up resources for research 11\nINFO:src.local_deep_research.web.services.research_service:Resources cleaned up for research_id: 11\nINFO:src.local_deep_research.web.services.socket_service:Client connected: X8t1x083nfrfjjj0AAAL\nINFO:src.local_deep_research.web.services.socket_service:Client connected: GNngdLw4CIbUjWKfAAAN\nINFO:src.local_deep_research.web.routes.research_routes:Starting research with provider: OLLAMA, model: llama3.2:latest, search engine: wikipedia\nINFO:src.local_deep_research.web.routes.research_routes:Additional parameters: max_results=None, time_period=None, iterations=2, questions=3\nINFO:src.local_deep_research.web.services.research_service:Starting research process for ID 12, query: give me a very short report on climate change max 2 headings and no subheadings\nINFO:src.local_deep_research.web.services.research_service:Research parameters: provider=OLLAMA, model=llama3.2:latest, search_engine=wikipedia, max_results=None, time_period=None, iterations=2, questions_per_iteration=3, custom_endpoint=https://openrouter.ai/api/v1\nINFO:src.local_deep_research.web.services.research_service:Overriding system settings with: provider=OLLAMA, model=llama3.2:latest, search_engine=wikipedia\nINFO:src.local_deep_research.config.llm_config:Checking Ollama availability at http://localhost:11434/api/tags\nINFO:src.local_deep_research.config.llm_config:Ollama is available. Status code: 200\nINFO:src.local_deep_research.config.llm_config:Response preview: {\"models\":[{\"name\":\"mxbai-embed-large:latest\",\"model\":\"mxbai-embed-large:latest\",\"modified_at\":\"2025\nINFO:src.local_deep_research.config.llm_config:Checking if model 'llama3.2:latest' exists in Ollama\nINFO:src.local_deep_research.config.llm_config:Available Ollama models: mxbai-embed-large:latest, deepseek-r1:14b, deepseek-coder-v2:16b, llama3.2:latest\nINFO:src.local_deep_research.config.llm_config:Creating ChatOllama with model=llama3.2:latest, base_url=http://localhost:11434\nGetting LLM with model: llama3.2:latest, temperature: 0.7, provider: ollama\nINFO:src.local_deep_research.config.llm_config:Testing Ollama model with simple invocation\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.web.services.socket_service:Client connected: EknYi-ekwLCIEIAoAAAP\nINFO:src.local_deep_research.config.llm_config:Ollama test successful. Response type: <class 'langchain_core.messages.ai.AIMessage'>\nINFO:src.local_deep_research.web.services.research_service:Successfully set LLM to: provider=OLLAMA, model=llama3.2:latest\nINFO:src.local_deep_research.config.search_config:Creating search engine with tool: wikipedia\nINFO:src.local_deep_research.config.search_config:Search config: tool=wikipedia, max_results=10, time_period=all\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: wikipedia with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating wikipedia with filtered parameters: dict_keys(['max_results', 'include_content', 'max_filtered_results'])\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_wikipedia.WikipediaSearchEngine object at 0x74c79b7ba490>>\nINFO:src.local_deep_research.config.search_config:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.search_system:Initializing AdvancedSearchSystem with strategy_name='source-based'\nINFO:src.local_deep_research.search_system:Creating SourceBasedSearchStrategy instance\nINFO:src.local_deep_research.search_system:Created strategy of type: SourceBasedSearchStrategy\nINFO:src.local_deep_research.config.search_config:Creating search engine with tool: wikipedia\nINFO:src.local_deep_research.config.search_config:Search config: tool=wikipedia, max_results=10, time_period=all\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: wikipedia with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Creating wikipedia with filtered parameters: dict_keys(['max_results', 'include_content', 'max_filtered_results'])\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_wikipedia.WikipediaSearchEngine object at 0x74c799975220>>\nINFO:src.local_deep_research.config.search_config:Successfully created search engine of type: WikipediaSearchEngine\nINFO:src.local_deep_research.web.services.research_service:Successfully set search engine to: wikipedia\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:src.local_deep_research.advanced_search_system.strategies.source_based_strategy:Starting source-based research on topic: give me a very short report on climate change max 2 headings and no subheadings\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generating follow-up questions...\nINFO:src.local_deep_research.web.services.socket_service:Client connected: 4WSNNr9cbqn4fJl_AAAR\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generated 3 follow-up questions\nINFO:src.local_deep_research.advanced_search_system.strategies.source_based_strategy:Using questions for iteration 1: ['give me a very short report on climate change max 2 headings and no subheadings', 'What are the current projected impacts of climate change on global sea levels by 2050?', 'How have recent advances in renewable energy affected the rate of carbon dioxide emissions from fossil fuels globally since 2015?', 'What are the most significant updates to international agreements or policies aimed at mitigating the effects of climate change as of 2024?']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: give me a very short report on climate change max 2 headings and no subheadings\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: What are the current projected impacts of climate change on global sea levels by 2050?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: How have recent advances in renewable energy affected the rate of carbon dioxide emissions from fossil fuels globally since 2015?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: What are the most significant updates to international agreements or policies aimed at mitigating the effects of climate change as of 2024?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 8 Wikipedia results: ['Russian invasion of Ukraine', '2022 in science', 'Attempts to overturn the 2020 United States presidential election', 'Republican Party efforts to disrupt the 2024 United States presidential election', 'China‚ÄìUnited States relations', 'Trumpism', 'Foreign policy of the first Donald Trump administration', 'History of Mexican Americans']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Climate change', 'Effects of climate change on oceans', 'Economic analysis of climate change', 'Climate change in Washington', 'Effects of climate change on agriculture', '2025 in climate change', 'Sea level rise', 'Climate change in Australia', 'Climate change in China', 'Effects of climate change']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Climate change', 'Climate change policy of the United States', 'Climate change in Australia', 'Economic analysis of climate change', 'Politics of climate change', 'Climate change in Europe', 'Climate change in the United States', 'Climate change mitigation', 'Wildfire', 'Climate change in China']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Renewable energy', 'Greenhouse gas emissions', 'Energy development', 'Climate change', 'Climate change in India', 'Renewable energy in the United States', '2025 in climate change', 'Nuclear power', 'Special Report on Global Warming of 1.5 ¬∞C', 'Petroleum']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 8 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generating follow-up questions...\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.questions.standard_question:Generated 2 follow-up questions\nINFO:src.local_deep_research.advanced_search_system.strategies.source_based_strategy:Generated questions for iteration 2: ['How do the authoritarian leanings and populist ideologies associated with Trumpism influence U.S. foreign policy, particularly in its relations with traditional allies and adversaries?', 'What are the implications of climate change policy in the United States, given its significant greenhouse gas emissions per person and position as a global leader, on both domestic and international levels?']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: How do the authoritarian leanings and populist ideologies associated with Trumpism influence U.S. foreign policy, particularly in its relations with traditional allies and adversaries?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Getting Wikipedia page previews for query: What are the implications of climate change policy in the United States, given its significant greenhouse gas emissions per person and position as a global leader, on both domestic and international levels?\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 6 Wikipedia results: ['Trumpism', 'Fascism and ideology', 'Far-right politics', 'Javier Milei 2023 presidential campaign', 'RT (TV network)', 'Nicolae Iorga']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Found 10 Wikipedia results: ['Climate change in the United States', 'Climate change in Australia', 'Climate change in New Zealand', 'Climate change mitigation', 'Climate change in the United Kingdom', 'Climate policy of China', 'President of the United States', 'United States', 'Impact of the COVID-19 pandemic on the environment', 'Water supply and sanitation in the United States']\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 6 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:local_deep_research.web_search_engines.engines.search_engine_wikipedia:Successfully created 10 previews from Wikipedia\nINFO:local_deep_research.web_search_engines.search_engine_base:Returning snippet-only results as per config\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.filters.cross_engine_filter:Cross-engine filtering kept 14 out of 54 results with reordering=True, reindex=True\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Added 14 documents to repository\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Set questions for 2 iterations\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Formatting final report. Number of detailed findings: 3. Synthesized content length: 1914. Number of question iterations: 2\nINFO:src.local_deep_research.utilities.search_utilities:Inside format_findings utility. Findings count: 3, Questions iterations: 2\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 16 links to markdown...\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 3 detailed finding items.\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 16 links to markdown...\nINFO:src.local_deep_research.utilities.search_utilities:Formatting 16 links to markdown...\nINFO:src.local_deep_research.utilities.search_utilities:Finished format_findings utility.\nINFO:src.local_deep_research.advanced_search_system.findings.repository:Successfully formatted final report.\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:src.local_deep_research.config.llm_config:Checking Ollama availability at http://localhost:11434/api/tags\nINFO:src.local_deep_research.config.llm_config:Ollama is available. Status code: 200\nINFO:src.local_deep_research.config.llm_config:Response preview: {\"models\":[{\"name\":\"mxbai-embed-large:latest\",\"model\":\"mxbai-embed-large:latest\",\"modified_at\":\"2025\nINFO:src.local_deep_research.config.llm_config:Checking if model 'llama3.2:latest' exists in Ollama\nINFO:src.local_deep_research.config.llm_config:Available Ollama models: mxbai-embed-large:latest, deepseek-r1:14b, deepseek-coder-v2:16b, llama3.2:latest\nINFO:src.local_deep_research.config.llm_config:Creating ChatOllama with model=llama3.2:latest, base_url=http://localhost:11434\nGetting LLM with model: llama3.2:latest, temperature: 0.7, provider: ollama\nINFO:src.local_deep_research.config.llm_config:Testing Ollama model with simple invocation\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:src.local_deep_research.config.llm_config:Ollama test successful. Response type: <class 'langchain_core.messages.ai.AIMessage'>\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.utilities.search_utilities:Formatting 16 links to markdown...\nINFO:src.local_deep_research.web.services.research_service:Cleaning up resources for research 12\nProcessing section: Climate Change Overview\nProcessing section: Climate Change Impacts\nINFO:src.local_deep_research.web.services.socket_service:Client connected: a78YxHaPISePWLEtAAAT\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nERROR:src.local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\n```\n",
      "state": "closed",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-27T21:07:31Z",
      "updated_at": "2025-05-01T18:22:31Z",
      "closed_at": "2025-05-01T18:22:30Z",
      "labels": [
        "bug",
        "rc/0.3.0"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/237/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/237",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/237",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:30.939547",
      "comments": [
        {
          "author": "djpetti",
          "body": "This is fixed in `0.3.0`.",
          "created_at": "2025-05-01T18:22:30Z"
        }
      ]
    },
    {
      "issue_number": 248,
      "title": "Unable to use external Ollama endpoint.",
      "body": "In a previous build I was able to use ollama that was installed on a different PC that was exposed to the local network. It seems like the feature was removed and now it only connects to the ollama endpoint that is on the same machine as the local deep research install. \n\nWas hoping this feature can be reinstated. ",
      "state": "closed",
      "author": "StatusQuo209",
      "author_type": "User",
      "created_at": "2025-04-30T13:49:59Z",
      "updated_at": "2025-05-01T18:22:01Z",
      "closed_at": "2025-05-01T18:21:59Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/248/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/248",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/248",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:31.176692",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @StatusQuo209,\n\nThe current version of LDR still supports this, AFAIK, either by setting `ollama_base_url` in the `[llm]` section of the main config file (which is not specified by default), or setting the `OLLAMA_BASE_URL` environment variable. I would recommend using the environment variable, p",
          "created_at": "2025-04-30T14:19:39Z"
        },
        {
          "author": "djpetti",
          "body": "Hey @StatusQuo209,\n\nPlease upgrade to `0.3.0`. This version will let you set the endpoint directly from the web U.I. We can reopen this if you have any further issues.",
          "created_at": "2025-05-01T18:22:00Z"
        }
      ]
    },
    {
      "issue_number": 214,
      "title": "Settings update requires complete restart of the application?",
      "body": "**Describe the bug**\nI think settings are sometimes only updated after the tool is restarted.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. start research tool\n2. change settings\n3. start research (old settings are used)\n\n**Expected behavior**\nResearch is conducted with settings\n",
      "state": "closed",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-04-21T21:51:28Z",
      "updated_at": "2025-05-01T18:21:20Z",
      "closed_at": "2025-05-01T18:21:18Z",
      "labels": [
        "unclear"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/214/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/214",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/214",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:31.381479",
      "comments": [
        {
          "author": "djpetti",
          "body": "You could potentially see this behavior if the DB write fails for some reason. That's because it will cache DB writes in memory, which will make it look like it worked until you restart. Maybe check your logs for DB errors?",
          "created_at": "2025-04-21T22:05:09Z"
        },
        {
          "author": "LearningCircuit",
          "body": "solved with #253 ",
          "created_at": "2025-05-01T18:21:18Z"
        }
      ]
    },
    {
      "issue_number": 207,
      "title": "Save all reference articles",
      "body": "\"Congratulations on the new version.\nI hope to try it out soon.\n\nOne thing I want to mention is that the Windows installer link in the release notes leads to a 404.\n\nSecondly, and possibly I mentioned this before, but it would be useful if there was an option to locally save the referenced articles. This would be mainly for ease of use for fact checking, but also to generate a RAG database for further sampling of the data.\n\nWhen I research a topic, medical usually, having a RAG available to query for additional clarifications would be quite helpful. \n\nI understand if this is not within the scope of your project though.\" DrAlex https://www.reddit.com/r/LocalDeepResearch/s/2SWtUrJfb0",
      "state": "open",
      "author": "LearningCircuit",
      "author_type": "User",
      "created_at": "2025-04-18T21:50:59Z",
      "updated_at": "2025-04-28T22:52:25Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "rc/0.3.0"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/207/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "LearningCircuit"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/207",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/207",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:31.628836",
      "comments": [
        {
          "author": "djpetti",
          "body": "I think this is an excellent idea. We might have to contend with search engines, though, for which this doesn't make much sense (e.g. general web search.)",
          "created_at": "2025-04-18T23:18:47Z"
        },
        {
          "author": "LearningCircuit",
          "body": "My plan is to work first on a partial solution (automatically download PDFs into a folder). There are lots of ways to make this more convenient, but I think that will be already a helpful start.",
          "created_at": "2025-04-22T23:34:20Z"
        }
      ]
    },
    {
      "issue_number": 240,
      "title": "Support Langchain for embedding models",
      "body": "Langchain has a much broader selection of embedding models available than Ollama or SentenceTransformers. It would be a good idea to support it eventually. We should investigate whether it can completely supersede the other options, or if we should just add it as a third option.",
      "state": "open",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-28T16:35:28Z",
      "updated_at": "2025-04-28T16:35:28Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/240/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/240",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/240",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:31.853180",
      "comments": []
    },
    {
      "issue_number": 212,
      "title": "openai_endpoint_url not used when provider set to \"openai_endpoint\"",
      "body": "**Describe the bug**\nI am trying to use \"Programmatic Access\" to interact with local_deep_research.\nI can use ldr-web with all my preferred settings/keys successfully. this install was from over a month ago.\nfor the script below I am doing a fresh install of local_deep_research but reusing my existing settings.toml and .env (.env only has my api key) and I believe I have correctly included the changes I made to settings.toml below.\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. pip install local_deep_research\n2. I run this short example from the documentation modifying the from statement to use .api. I am using the same settings.toml and .env file as when i run ldr-web.\nhere is the applicable/modified [llm] section in settings.toml\n```\n[llm]\nprovider = \"openai_endpoint\"\n# Model name\nmodel = \"deepseek-chat\"  #reasoner\"\n# Temperature\ntemperature = 1.0\n# Maximum tokens\nmax_tokens = 30000\n# OpenAI-compatible endpoint URL\nopenai_endpoint_url = \"https://api.deepseek.com\"\n```\nand here is the example I'm trying to run  using programtic access:\n```\n# Generate a quick research summary with custom parameters\nresults = quick_summary(\n    query=\"advances in fusion energy\",\n    search_tool=\"auto\",          # Auto-select the best search engine\n    iterations=1,                # Single research cycle for speed\n    questions_per_iteration=2,   # Generate 2 follow-up questions\n    max_results=30,              # Consider up to 30 search results\n    temperature=0.7,              # Control creativity of generation\n    progress_callback=print\n)\nprint(results[\"summary\"])\n```\n2. See error:\nError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: <<removed>>. You can find your API key at https://platform.openai.com/account/api-keys.\n\n**Expected behavior**\nopenai_endpoint_url = \"https://api.deepseek.com\" should be used an not the openai endpoint.\n\n**System Information:**\n - OS: Ubuntu 24.04\n - Python Version: 3.10.0\n - Model Used: deepseek-chat\n\n\n**Additional context**\nI do not have an understanding of the full code base so I  state the following as an observation only.   \n\nI compared my working setup from a month ago where I run ldr-web from and I noticed a different llm_config.py between then and now.\n\na quick look and i see this commit Commit 6cb2137 where it made this change to llm_config.py:\n```\n-        openai_endpoint_url = settings.llm.openai_endpoint_url\n+         if openai_endpoint_url is not None:\n+             openai_endpoint_url = get_db_setting(\n+                 \"llm.openai_endpoint_url\", settings.llm.openai_endpoint_url\n+             )\n```\nwhen  debug and step through and I undo this change and removing the 4 lines and just leave the previous line \n```\n openai_endpoint_url = settings.llm.openai_endpoint_url\n```\nthen the example script above runs successfully.\n\n",
      "state": "closed",
      "author": "giantbirdicus",
      "author_type": "User",
      "created_at": "2025-04-21T19:57:23Z",
      "updated_at": "2025-04-23T21:49:05Z",
      "closed_at": "2025-04-22T20:40:03Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/212/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/212",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/212",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:31.853202",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @giantbirdicus,\n\nThe code you pointed out is expected behavior, as it is supposed to prioritize loading this setting from the DB. *However,* for programmatic access, it probably makes sense to have this as an argument that is passed directly to the function.",
          "created_at": "2025-04-21T20:06:29Z"
        },
        {
          "author": "djpetti",
          "body": "@giantbirdicus \n\nPlease give this another try with the latest code in the `main` branch.",
          "created_at": "2025-04-22T21:11:07Z"
        },
        {
          "author": "giantbirdicus",
          "body": "thanks @djpetti \n\nAs you suggest, I gave it another try.  To me this is good/closed as well. \n\nAlso, I like this project, it is actually very helpful for me, and I am glad it is being actively developed and improved. thank you and the team.\n\nI am now able to get both api functions quick summary and ",
          "created_at": "2025-04-23T13:33:17Z"
        },
        {
          "author": "djpetti",
          "body": "Yeah, this is expected. The first example in the notebook without any additional arguments will basically just grab whatever happens to be in the database for configuration. There might be a discussion to be had about just removing that example and endorsing the practice of always specifying configu",
          "created_at": "2025-04-23T13:45:01Z"
        },
        {
          "author": "LearningCircuit",
          "body": "> Also, I like this project, it is actually very helpful for me, and I am glad it is being actively developed and improved. thank you and the team.\n> \n> I am now able to get both api functions quick summary and generate report to run without modifying the library.\n\nI am actually curious how you are ",
          "created_at": "2025-04-23T21:48:38Z"
        }
      ]
    },
    {
      "issue_number": 222,
      "title": "There was a problem when connecting to lmstudio",
      "body": "Error reporting issues may occur when using the api provided by the local lmstudio. When I load gemma-3-12b-it-qat in lmsudio and turn on the server. The terminal verification service is normal, but it cannot be used in local deep research and errors will occur.\n**********************************************************************************************************************\nWARNING:local_deep_research.web.services.research_service:Detected unknown error in synthesis\nINFO:local_deep_research.web.services.research_service:Using current_knowledge as fallback\nINFO:local_deep_research.web.services.research_service:Found formatted_findings of length: 22\nINFO:local_deep_research.web.services.research_service:Successfully converted to clean markdown of length: 22\nINFO:local_deep_research.web.services.research_service:Writing report to: research_outputs\\quick_summary_ÁîµÊú∫.md\nINFO:local_deep_research.web.services.research_service:Updating database for research_id: 21\nINFO:local_deep_research.web.services.research_service:Database updated successfully for research_id: 21\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research_id: 21\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research 21\nINFO:local_deep_research.web.services.research_service:Resources cleaned up for research_id: 21\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.services.socket_service:Client connected: XMQuisnPJ9CSHZbUAAAb\nINFO:local_deep_research.web.app_factory:Attempting to connect to Ollama API\nWARNING:local_deep_research.web.app_factory:Could not connect to Ollama API: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000029961E8A600>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\nINFO:local_deep_research.web.app_factory:Using fallback Ollama models due to connection error\nINFO:local_deep_research.web.app_factory:Final Ollama models count: 3\nINFO:local_deep_research.web.app_factory:Sample Ollama models: llama3, mistral, gemma:latest\nINFO:local_deep_research.web.app_factory:Attempting to connect to Ollama API\nWARNING:local_deep_research.web.app_factory:Could not connect to Ollama API: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002992DA1FF80>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\nINFO:local_deep_research.web.app_factory:Using fallback Ollama models due to connection error\nINFO:local_deep_research.web.app_factory:Final Ollama models count: 3\nINFO:local_deep_research.web.app_factory:Sample Ollama models: llama3, mistral, gemma:latest\nINFO:local_deep_research.web.routes.research_routes:Starting research with provider: OPENAI_ENDPOINT, model: gpt-4o, search engine: arxiv\nINFO:local_deep_research.web.routes.research_routes:Additional parameters: max_results=None, time_period=None, iterations=2, questions=3\nINFO:local_deep_research.web.services.research_service:Starting research process for ID 22, query: ÁîµÊú∫\nINFO:local_deep_research.web.services.research_service:Research parameters: provider=OPENAI_ENDPOINT, model=gpt-4o, search_engine=arxiv, max_results=None, time_period=None, iterations=2, questions_per_iteration=3, custom_endpoint=[http://127.0.0.1:1234](https://www.google.com/url?sa=E&q=http%3A%2F%2F127.0.0.1%3A1234)\nINFO:local_deep_research.web.services.research_service:Overriding system settings with: provider=OPENAI_ENDPOINT, model=gpt-4o, search_engine=arxiv\nGetting LLM with model: gpt-4o, temperature: 0.7, provider: openai_endpoint\nWARNING:local_deep_research.config.llm_config:OPENAI_ENDPOINT_API_KEY not found. Falling back to default model.\nINFO:local_deep_research.web.services.research_service:Successfully set LLM to: provider=OPENAI_ENDPOINT, model=gpt-4o\nINFO:local_deep_research.config.search_config:Creating search engine with tool: arxiv\nINFO:local_deep_research.config.search_config:Search config: tool=arxiv, max_results=10, time_period=all\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: arxiv with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating arxiv with filtered parameters: dict_keys(['max_results', 'sort_by', 'sort_order', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: ArXivSearchEngine\nINFO:local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method ArXivSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_arxiv.ArXivSearchEngine object at 0x000002992DA1C9B0>>\nINFO:local_deep_research.config.search_config:Successfully created search engine of type: ArXivSearchEngine\nINFO:local_deep_research.search_system:Initializing AdvancedSearchSystem with strategy_name='parallel'\nINFO:local_deep_research.search_system:Creating ParallelSearchStrategy instance\nINFO:local_deep_research.search_system:Created strategy of type: ParallelSearchStrategy\nINFO:local_deep_research.config.search_config:Creating search engine with tool: arxiv\nINFO:local_deep_research.config.search_config:Search config: tool=arxiv, max_results=10, time_period=all\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: arxiv with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating arxiv with filtered parameters: dict_keys(['max_results', 'sort_by', 'sort_order', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: ArXivSearchEngine\nINFO:local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method ArXivSearchEngine.run of <local_deep_research.web_search_engines.engines.search_engine_arxiv.ArXivSearchEngine object at 0x000002992DA1FE60>>\nINFO:local_deep_research.config.search_config:Successfully created search engine of type: ArXivSearchEngine\nINFO:local_deep_research.web.services.research_service:Successfully set search engine to: arxiv\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.advanced_search_system.strategies.parallel_search_strategy:Starting parallel research on topic: ÁîµÊú∫\nINFO:local_deep_research.advanced_search_system.questions.standard_question:Generating follow-up questions...\nINFO:local_deep_research.advanced_search_system.questions.standard_question:Generated 0 follow-up questions\nINFO:local_deep_research.advanced_search_system.strategies.parallel_search_strategy:Generated questions: []\nINFO:local_deep_research.web_search_engines.engines.search_engine_arxiv:---Execute a search using arXiv---\nINFO:local_deep_research.web_search_engines.engines.search_engine_arxiv:Getting paper previews from arXiv\nINFO:arxiv:Requesting page (first: True, try: 0): [https://export.arxiv.org/api/query?search_query=%E7%94%B5%E6%9C%BA&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=25](https://www.google.com/url?sa=E&q=https://export.arxiv.org/api/query?search_query=%E7%94%B5%E6%9C%BA&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=25)\nINFO:local_deep_research.web.services.socket_service:Client connected: MGuiwqtVllOsfPweAAAd\nINFO:local_deep_research.web.services.socket_service:Client connected: DBVaTcHI7LOTlma0AAAf\nINFO:arxiv:Got empty first page; stopping generation\nINFO:local_deep_research.web_search_engines.search_engine_base:Search engine ArXivSearchEngine returned no preview results for query: ÁîµÊú∫\nINFO:local_deep_research.advanced_search_system.findings.repository:Set questions for 1 iterations\nINFO:local_deep_research.advanced_search_system.findings.repository:Formatting final report. Number of detailed findings: 1. Synthesized content length: 75. Number of question iterations: 1\nINFO:local_deep_research.utilities.search_utilities:Inside format_findings utility. Findings count: 1, Questions iterations: 1\nINFO:local_deep_research.utilities.search_utilities:Formatting 1 detailed finding items.\nINFO:local_deep_research.utilities.search_utilities:No unique sources found across all findings to list.\nINFO:local_deep_research.utilities.search_utilities:Finished format_findings utility.\nINFO:local_deep_research.advanced_search_system.findings.repository:Successfully formatted final report.\nINFO:local_deep_research.advanced_search_system.findings.repository:Added 0 documents to repository\nINFO:local_deep_research.web.services.research_service:Found formatted_findings of length: 367\nINFO:local_deep_research.web.services.research_service:Successfully converted to clean markdown of length: 367\nINFO:local_deep_research.web.services.research_service:Writing report to: research_outputs\\quick_summary_ÁîµÊú∫.md\nINFO:local_deep_research.web.services.research_service:Updating database for research_id: 22\nINFO:local_deep_research.web.services.research_service:Database updated successfully for research_id: 22\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research_id: 22\nINFO:local_deep_research.web.services.research_service:Cleaning up resources for research 22\nINFO:local_deep_research.web.services.research_service:Resources cleaned up for research_id: 22\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.services.socket_service:Client connected: aPE1Hcmpako_nHt9AAAh\nERROR:local_deep_research.web.services.socket_service:Unhandled Socket.IO error: register_socket_events.<locals>.on_disconnect() takes 0 positional arguments but 1 was given\nINFO:local_deep_research.web.services.socket_service:Client connected: Hv20wA1WF0Zb0VoFAAAj\n************************************************************************************************************************\nMoreover, the lmstudio service log shows no related information or reports any errors. It seems that the connection was not successful at all",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2025-04-23T03:25:10Z",
      "updated_at": "2025-04-23T03:35:36Z",
      "closed_at": "2025-04-23T03:35:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/222/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/222",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/222",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:32.072801",
      "comments": []
    },
    {
      "issue_number": 221,
      "title": "Refactor report generation",
      "body": "Report generation can be refactored to make it easier to generate custom reports and control the output format.",
      "state": "open",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-22T13:30:48Z",
      "updated_at": "2025-04-22T23:30:21Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "rc/0.3.0"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/221/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/221",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/221",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:33.868422",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Does this require any optimizations to search?",
          "created_at": "2025-04-22T23:30:20Z"
        }
      ]
    },
    {
      "issue_number": 3,
      "title": "add parameter for the maximum context with ollama num_ctx",
      "body": "for example:\n(model=\"deepseek-r1:70b-llama-distill-q8_0\", temperature=0.7, num_ctx=3800)",
      "state": "open",
      "author": "uneuro",
      "author_type": "User",
      "created_at": "2025-02-09T18:38:59Z",
      "updated_at": "2025-04-22T23:10:23Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/3/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/3",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/3",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:34.088298",
      "comments": []
    },
    {
      "issue_number": 220,
      "title": "Option to output the report as a .tex or .qmd file?",
      "body": "Would be great to have the option for the report to be in a LaTeX .tex or Quarto .qmd format",
      "state": "open",
      "author": "kendonB",
      "author_type": "User",
      "created_at": "2025-04-21T23:50:08Z",
      "updated_at": "2025-04-22T13:31:51Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/220/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/220",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/220",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:34.088320",
      "comments": [
        {
          "author": "djpetti",
          "body": "This sounds like a good idea. We are in the process of refactoring our report generation system. Once that is done, implementing this type of feature should be relatively easy.",
          "created_at": "2025-04-22T13:31:50Z"
        }
      ]
    },
    {
      "issue_number": 219,
      "title": "Output references in a reference software format",
      "body": "Option to output all references as a RIS file (e.g., to import into Zotero).",
      "state": "open",
      "author": "kendonB",
      "author_type": "User",
      "created_at": "2025-04-21T23:48:07Z",
      "updated_at": "2025-04-22T06:20:01Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/219/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/219",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/219",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:34.319183",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "@kendonB thank you these are good ideas",
          "created_at": "2025-04-22T06:20:00Z"
        }
      ]
    },
    {
      "issue_number": 36,
      "title": "Error : 429 Resource has been exhausted (e.g. check quota) with Gemini API Free Plan",
      "body": "What can I change? ",
      "state": "open",
      "author": "Yellowbery01",
      "author_type": "User",
      "created_at": "2025-03-15T19:36:31Z",
      "updated_at": "2025-04-21T20:12:37Z",
      "closed_at": null,
      "labels": [
        "bug",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/36/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/36",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/36",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:34.661771",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "You used up all of the Quota of the LLM? Choose another model or open router?",
          "created_at": "2025-03-15T23:05:02Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I am not sure ei understand the question ",
          "created_at": "2025-03-15T23:05:18Z"
        },
        {
          "author": "Yellowbery01",
          "body": "> I am not sure ei understand the question\n\nI configured Local Deep Research to do your research via the Gemini Free API\n The limitations being 15 requests per minute, they are sometimes exceeded (I do not even know how possible) which prevents me from being able to do my research.\n\n I show you a at",
          "created_at": "2025-03-16T12:14:47Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Thank you we will need to modify the code and add retries and slow downs for this. It is important ",
          "created_at": "2025-03-16T13:28:24Z"
        }
      ]
    },
    {
      "issue_number": 187,
      "title": "Settings auto-save causes multiple calls to the backend",
      "body": "**Describe the bug**\nWhen you change a setting in the settings tab and it auto-saves, it appears to trigger multiple identical calls to the backend.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Start the web interface.\n2. Open the settings tab.\n3. Change a setting\n4. Observe multiple notifications on the right.\n\n**Expected behavior**\nThere should be only a single call to the backend.\n\n**System Information:**\n - OS: Ubuntu 24.04\n - Python Version: 3.12.0\n - Model Used: Tested with various Ollama models.\n - Hardware Specs: 32 GB RAM, RX7900XT\n\n**Additional context**\nNone.\n\n**Output/Logs**\nNone.\n",
      "state": "open",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-14T18:05:19Z",
      "updated_at": "2025-04-21T20:11:12Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "rc/0.3.0"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/187/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/187",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/187",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:34.855707",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "marked as enhancement, because while it looks funny it is not really a problem.",
          "created_at": "2025-04-21T20:09:37Z"
        }
      ]
    },
    {
      "issue_number": 150,
      "title": "Local document analysis seems shallow and will likely miss bibliography",
      "body": "I'm groking through the code and I noticed that the local document analysis only analyzes 5 documents and 1000 tokens per document:\n\nhttps://github.com/LearningCircuit/local-deep-research/blob/e934bf31459defa31e950430dcf3597e6a40d8d3/src/local_deep_research/api/research_functions.py#L246-L256\n\nThe issue is that one of the most interesting part of a paper is the bibliography, to find new sources to feed the deep research, and bibliography is unlikely to be in the first 1000 tokens.\n\nFurthermore, many documents don't have an abstract or their intro is just marketing fluff, for example corporate documents.",
      "state": "closed",
      "author": "mratsim",
      "author_type": "User",
      "created_at": "2025-04-04T19:44:11Z",
      "updated_at": "2025-04-17T19:45:29Z",
      "closed_at": "2025-04-17T19:45:29Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/150/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/150",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/150",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:35.013340",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Oh interesting that is an API problem we definitely should fix that",
          "created_at": "2025-04-04T20:41:12Z"
        },
        {
          "author": "LearningCircuit",
          "body": "You can also get local documents with the other API functions... Just change the search engines to local all. \n\nActually maybe this function should be removed from API because it is somewhat redundant and wrong ",
          "created_at": "2025-04-04T20:43:50Z"
        },
        {
          "author": "djpetti",
          "body": "This might be less bad than it looks. The way local search works is that it breaks each document up into a bunch of small chunks, and then searches for relevant chunks instead of entire documents. So the total amount of data that it's throwing away is actually a lot less than it seems. Even so, this",
          "created_at": "2025-04-04T21:43:32Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Is this still relevant or can we close @djpetti ",
          "created_at": "2025-04-17T19:00:17Z"
        },
        {
          "author": "djpetti",
          "body": "This should probably be closed. \n\n @mratsim IMO, this is more of a discussion topic than an issue. Discord would probably be a better place for this. ",
          "created_at": "2025-04-17T19:42:12Z"
        }
      ]
    },
    {
      "issue_number": 154,
      "title": "Engine cannot be changed",
      "body": "After I started the research in the default state, the engine always chose searxng, but I did not start the local server, so it could not be used. However, I set google's apikey and setting files in.env, changed auto to google_pse, and then restarted the program, and found that searxng was still used again. I think we need a more concise front end to make simple changes to the search engine",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2025-04-06T08:24:20Z",
      "updated_at": "2025-04-17T19:01:26Z",
      "closed_at": "2025-04-17T19:01:25Z",
      "labels": [
        "bug",
        "rc/0.1.*"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/154/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/154",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/154",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:35.194298",
      "comments": [
        {
          "author": "Elegant-Spider",
          "body": "+1",
          "created_at": "2025-04-06T09:23:24Z"
        },
        {
          "author": "LearningCircuit",
          "body": "This might only be a UI bug. Can you use python -m local_deep_research.main and see if it actually uses SearXGN, please?",
          "created_at": "2025-04-06T09:30:27Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Because for me it always says SearXGN when it actually is not using that @HashedViking ",
          "created_at": "2025-04-06T09:31:12Z"
        },
        {
          "author": "HashedViking",
          "body": "> Because for me it always says SearXGN when it actually is not using that @HashedViking \n\nYes, that's a UI bug in logs, always showing SearXNG, should not affect anything though, I'll look into this",
          "created_at": "2025-04-06T09:35:22Z"
        },
        {
          "author": "berataydin",
          "body": "When I am looking this SearXGN thing, I decided to use serp, uncomment required config from search_engines.toml and have valid api key in secrets.toml but it's giving me this. \n\nWARNING:local_deep_research.web_search_engines.search_engine_factory:Search engine 'serp' not found, using default: wikipe",
          "created_at": "2025-04-06T09:39:47Z"
        }
      ]
    },
    {
      "issue_number": 155,
      "title": "\"'DynaBox' object has no attribute 'max_results'\"",
      "body": "\"'DynaBox' object has no attribute 'max_results'\"",
      "state": "closed",
      "author": "Elegant-Spider",
      "author_type": "User",
      "created_at": "2025-04-06T09:22:06Z",
      "updated_at": "2025-04-17T19:00:54Z",
      "closed_at": "2025-04-17T19:00:53Z",
      "labels": [
        "bug",
        "rc/0.1.*"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/155/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/155",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/155",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:35.393679",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hello @Elegant-Spider,\n\nWhen reporting bugs, please follow our bug report issue template so we can more easily diagnose it. This looks like an issue with a corrupted config file, but I don't have enough info to know for sure. Can you please share the full logs and stack trace (if there is one)?",
          "created_at": "2025-04-06T15:14:04Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Closed due to complete rework.",
          "created_at": "2025-04-17T19:00:53Z"
        }
      ]
    },
    {
      "issue_number": 130,
      "title": "No research findings were generated. Please try again.",
      "body": null,
      "state": "closed",
      "author": "shshen-closer",
      "author_type": "User",
      "created_at": "2025-04-01T07:43:22Z",
      "updated_at": "2025-04-17T19:00:38Z",
      "closed_at": "2025-04-17T19:00:37Z",
      "labels": [
        "rc/0.1.*"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/130/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/130",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/130",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:35.560923",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "I think we need better UI handling of this. It's fine that it happens but the user needs to know why",
          "created_at": "2025-04-01T15:49:32Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Most likely you use very small LLM which is unable to generate the questions or what LLM are you using?",
          "created_at": "2025-04-01T21:27:30Z"
        },
        {
          "author": "djpetti",
          "body": "I've also seen this happen if the search engines or local collections are misconfigured. We really need the full log output to diagnose this case.",
          "created_at": "2025-04-01T23:19:53Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Closed due to complete rework.",
          "created_at": "2025-04-17T19:00:37Z"
        }
      ]
    },
    {
      "issue_number": 124,
      "title": "Quick Start (not windows installer) lack of config files",
      "body": "**Describe the bug**\nlack of config files after installation from CMD\n\n**To Reproduce**\nSteps to reproduce the behavior:\ncreate conda env with python 3.11\n\npip install local-deep-research\n\nplaywright install\n\nLm studio API\n\n\n**System Information:**\n - Windows 10\n - Python 3.11 (conda environment)\n\n\n**Output/Logs**\n```\nldr-web\n\nTraceback (most recent call last):\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\pathlib.py\", line 1116, in mkdir\n    os.mkdir(self, mode)\nFileNotFoundError: [WinError 3] The system cannot find a specific path: 'C:\\\\Users\\\\testuser\\\\Documents\\\\LearningCircuit\\\\local-deep-research\\\\config'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\pathlib.py\", line 1116, in mkdir\n    os.mkdir(self, mode)\nFileNotFoundError: [WinError 3] The system cannot find a specific path: 'C:\\\\Users\\\\testuser\\\\Documents\\\\LearningCircuit\\\\local-deep-research'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\pathlib.py\", line 1116, in mkdir\n    os.mkdir(self, mode)\nFileNotFoundError: [WinError 2] A specific file cannot be found: 'C:\\\\Users\\\\testuser\\\\Documents\\\\LearningCircuit'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Scripts\\ldr-web.exe\\__main__.py\", line 4, in <module>\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\site-packages\\local_deep_research\\__init__.py\", line 14, in <module>\n    from .search_system import AdvancedSearchSystem\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\site-packages\\local_deep_research\\search_system.py\", line 6, in <module>\n    from .config import settings, get_llm, get_search\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\site-packages\\local_deep_research\\config.py\", line 29, in <module>\n    CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\pathlib.py\", line 1120, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\pathlib.py\", line 1120, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\pathlib.py\", line 1121, in mkdir\n    self.mkdir(mode, parents=False, exist_ok=exist_ok)\n  File \"C:\\Users\\testuser\\anaconda3\\envs\\local-deep-research\\Lib\\pathlib.py\", line 1116, in mkdir\n    os.mkdir(self, mode)\nFileNotFoundError: [WinError 2] A specific file cannot be found: 'C:\\\\Users\\\\testuser\\\\Documents\\\\LearningCircuit'\n\n\n```\n",
      "state": "closed",
      "author": "tehMArs",
      "author_type": "User",
      "created_at": "2025-03-31T06:36:59Z",
      "updated_at": "2025-04-17T18:59:21Z",
      "closed_at": "2025-04-17T18:59:20Z",
      "labels": [
        "bug",
        "rc/0.1.*"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/124/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/124",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/124",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:35.744389",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Thanks for reporting and sorry.\n\n\nYou can copy the files manually from here https://github.com/LearningCircuit/local-deep-research/tree/main/src/local_deep_research/defaults and create the dir manually.\n\n\nI try to find a better long term solution ",
          "created_at": "2025-03-31T07:03:54Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Copy all files from that link in this dir",
          "created_at": "2025-03-31T07:04:19Z"
        },
        {
          "author": "tehMArs",
          "body": "> Thanks for reporting and sorry.\n> \n> You can copy the files manually from here https://github.com/LearningCircuit/local-deep-research/tree/main/src/local_deep_research/defaults and create the dir manually.\n> \n> I try to find a better long term solution\n\nstill settings.toml is missing, i just insta",
          "created_at": "2025-03-31T10:02:59Z"
        },
        {
          "author": "LearningCircuit",
          "body": "That's what we are doing each start up... We copy the files from default to your user documents if the files don't exist already.\n\nI will have to investigate why miniconda doesn't seem to support this.",
          "created_at": "2025-03-31T16:51:19Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Closed due to complete rework.",
          "created_at": "2025-04-17T18:59:20Z"
        }
      ]
    },
    {
      "issue_number": 108,
      "title": "How to set BASE_URL of OPENAI_ENDPOINT?",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\nDocument does not shows the information of BASE_URL of OPENAI_ENDPOINT.\nAnd src/default/llm_config,py seems to be not load some settings about BASE_URL.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run command '...'\n2. Enter query '....'\n3. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\nsomething like ...\n```llm_config.py\ndef is_openai_endpoint_available():\n    \"\"\"Check if OpenAI endpoint is available\"\"\"\n    try:\n        import requests\n        api_key = settings.get('OPENAI_ENDPOINT_API_KEY', '')\n        if not api_key:\n            api_key = os.getenv('OPENAI_ENDPOINT_API_KEY')\n        base_url = settings.get('OPENAI_ENDPOINT_BASE_URL', settings.llm.get('openai_endpoint_base_url', 'http://localhost:8080'))\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\",  # Add the API key as a Bearer token\n        }\n        # typically uses OpenAI-compatible endpoints\n        response = requests.get(f\"{base_url}/v1/models\", timeout=3.0, headers=headers)\n        return response.status_code == 200\n    except:\n        return False\n```\nand user set .env OPENAI_ENDPOINT_BASE_URL=...\n\n**System Information:**\n - OS: [e.g. Ubuntu 22.04]\n - Python Version: [e.g. 3.9.0]\n - Model Used: [e.g. deepseek-r1:14b]\n - Hardware Specs: [e.g. 32GB RAM, NVIDIA RTX 3080]\n\n**Additional context**\nAdd any other context about the problem here.\nSince there's a base_url setting for Ollama's endpoint, there should also be one for the OpenAI endpoint, shouldn't there?\n\n**Output/Logs**\n```\nIf applicable, add relevant output or error logs here\n```\n",
      "state": "closed",
      "author": "tosintech-web",
      "author_type": "User",
      "created_at": "2025-03-28T13:33:28Z",
      "updated_at": "2025-04-17T18:58:54Z",
      "closed_at": "2025-04-17T18:58:53Z",
      "labels": [
        "bug",
        "rc/0.1.*"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/108/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/108",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/108",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:35.941605",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hey,\n\nYou should be able to set this in the `settings.toml` file. Specifically, in the LLM section, change the `provider` to `openai_endpoint`, and set `openai_endpoint_url` accordingly.",
          "created_at": "2025-03-28T19:14:08Z"
        },
        {
          "author": "djpetti",
          "body": "Here is an example of this configuration:\n\n```toml\n[llm]\n# LLM provider (one of: ollama, openai, anthropic, vllm, openai_endpoint, lmstudio, llamacpp)\nprovider = \"openai_endpoint\"\n# Model name\nmodel = \"llama-3.3-70b-instruct\"\n# Temperature\ntemperature = 0.7\n# Maximum tokens\nmax_tokens = 30000\n# Open",
          "created_at": "2025-03-28T19:14:53Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@tosintech-web  can we close?",
          "created_at": "2025-03-30T11:58:59Z"
        },
        {
          "author": "mratsim",
          "body": "The docker usage readme seems to mention LDR_LLM__OPENAI_ENDPOINT_URL\n\nhttps://github.com/LearningCircuit/local-deep-research/blob/99aaa071b3283ae2d4896514b4c25c8585e84479/docs/docker-usage-readme.md?plain=1#L159-L167\n\nbut I agree that it's not clear from the readme, I initially thought I add to set",
          "created_at": "2025-04-06T23:01:15Z"
        },
        {
          "author": "djpetti",
          "body": "It's possible the documentation could be improved. I'll take a look. ",
          "created_at": "2025-04-07T01:38:45Z"
        }
      ]
    },
    {
      "issue_number": 32,
      "title": "Unable to use Ollama's local model",
      "body": "I can't use the model downloaded in ollama, I have modified the name attribute of the model in the config.py, but it doesn't work\nIs there anything that still needs to be adjusted?  \n\nneed help!\n\n![Image](https://github.com/user-attachments/assets/f0e62ef8-5391-453e-a843-4316f66149e3)\n\n![Image](https://github.com/user-attachments/assets/ead15188-58f6-425d-8157-6b2d3804a7f8)",
      "state": "closed",
      "author": "eastjoe",
      "author_type": "User",
      "created_at": "2025-03-14T09:50:22Z",
      "updated_at": "2025-04-17T18:58:04Z",
      "closed_at": "2025-04-17T18:58:03Z",
      "labels": [
        "bug",
        "rc/0.1.*"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/32/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/32",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/32",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:36.160947",
      "comments": [
        {
          "author": "eastjoe",
          "body": "I adjusted his ollama address to the specified local ip and it worked\n\n![Image](https://github.com/user-attachments/assets/3b45dc09-3a26-48c5-9daa-c8a17985384c)\n\nBut then, it seemed to get stuck in an infinite loop and kept sending this request        \n\n![Image](https://github.com/user-attachments/a",
          "created_at": "2025-03-14T10:17:15Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@HashedViking I think you use this? Maybe you can help?",
          "created_at": "2025-03-14T11:28:45Z"
        },
        {
          "author": "HashedViking",
          "body": "@eastjoe could you send complete logs text, also that might happen when you restart the backend, but frontend keeps trying to fetch the progress of a previous research, that has failed due to unreachable Ollama error, \n\nTry to open the web interface in a new tab, and delete failed research\n\nIn gener",
          "created_at": "2025-03-14T12:43:12Z"
        },
        {
          "author": "lamininA1",
          "body": "Have you considered editing the `OLLAMA_HOST` variable?\n\nI set the `OLLAMA_HOST` variable to `0.0.0.0:11434`, and it works fine in local-deep-research.",
          "created_at": "2025-03-16T04:05:24Z"
        },
        {
          "author": "HashedViking",
          "body": "@eastjoe also some VPNs alter local proxy settings which also leads to Ollama connection failures, try to disable/reconfigure it",
          "created_at": "2025-03-16T06:47:15Z"
        }
      ]
    },
    {
      "issue_number": 186,
      "title": "Setting temporary configs from the \"advanced settings\" panel doesn't work.",
      "body": "**Describe the bug**\nChanging these settings appears to work in the frontend, but doesn't have any effect on the parameters used for the search.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Start the web interface\n2. Open the advanced settings panel\n3. Change a setting\n4. Run the query\n\n**Expected behavior**\nIt should run the query with the setting that was changed.\n\n**System Information:**\n - OS: Ubuntu 24.04\n - Python Version: 3.12.0\n - Model Used: Tried with various Ollama models\n - Hardware Specs: 32GB RAM RX7900XT\n\n**Additional context**\nNone\n\n**Output/Logs**\nNone\n",
      "state": "closed",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-14T18:02:35Z",
      "updated_at": "2025-04-15T17:50:18Z",
      "closed_at": "2025-04-15T17:50:17Z",
      "labels": [
        "bug",
        "rc/0.2.0"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/186/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/186",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/186",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:36.342886",
      "comments": [
        {
          "author": "djpetti",
          "body": "Fixed in the RC branch.",
          "created_at": "2025-04-15T17:50:17Z"
        }
      ]
    },
    {
      "issue_number": 174,
      "title": "Engine selection dropdown breaks when scrolling",
      "body": "**Describe the bug**\nThe engine selection dropdown is not positioned correctly when you scroll the page. This can affect people using the app on smaller monitors.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run the web server\n2. Go to advanced settings\n3. Open the dropdown for selecting an engine.\n4. Scroll the page.\n\n**Expected behavior**\nThe dropdown should stay locked to the input.\n\n**System Information:**\n - OS: Ubuntu 24.04\n - Python Version: 3.12\n - Model Used: deepseek-r1:14b\n - Hardware Specs: 32GB RAM, RX7900\n\n**Additional context**\nNone\n\n**Output/Logs**\n\n![Image](https://github.com/user-attachments/assets/0962e6bb-4373-4172-bb25-d42ed492f397)\n",
      "state": "closed",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-04-12T16:39:02Z",
      "updated_at": "2025-04-14T15:25:27Z",
      "closed_at": "2025-04-14T15:25:25Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/174/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "djpetti"
      ],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/174",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/174",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:36.605575",
      "comments": [
        {
          "author": "djpetti",
          "body": "Closing because this was merged.",
          "created_at": "2025-04-14T15:25:25Z"
        }
      ]
    },
    {
      "issue_number": 111,
      "title": "Working with Reasoning Models",
      "body": "While creating queries adn using models which answers whiy think tags I presume this part of the search_system.py doesnt work properly\n\n        questions = [\n            q.replace(\"Q:\", \"\").strip()\n            for q in remove_think_tags(response.content).split(\"\\n\")\n            if q.strip().startswith(\"Q:\")\n        ][: self.questions_per_iteration]\n\nI still receive think tag parts with Deepseek and QwQ models and it makes it impossible to work with them. Even it is pretty straightforward method I am not sure why this is happening?\n\nAny ideas? Any other place we should use remove_think_tags utility function to solve this issue?\n\nAlso any ideas to manipulate Reasoning effort of reasoning models to enhance deep research performance ?",
      "state": "closed",
      "author": "EggzYy",
      "author_type": "User",
      "created_at": "2025-03-28T18:36:23Z",
      "updated_at": "2025-04-13T22:34:30Z",
      "closed_at": "2025-04-13T22:34:30Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/111/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/111",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/111",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:36.826730",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "I will fix it today. We have remove thinking tags function maybe it is not applied consistently anymore ",
          "created_at": "2025-03-28T19:31:23Z"
        },
        {
          "author": "LearningCircuit",
          "body": "try with this branch https://github.com/LearningCircuit/local-deep-research/tree/thinking-tags\n\nImportant: \n1. remove your old llm_config.py in the config area\n2. Use developer setup instead of normal install \n\nIt will take me a while to merge in main because this is a fundamental change that might ",
          "created_at": "2025-03-29T00:14:08Z"
        },
        {
          "author": "LearningCircuit",
          "body": "fixed in main. If you have any issues please tell me",
          "created_at": "2025-03-30T10:50:17Z"
        },
        {
          "author": "EggzYy",
          "body": "Thank you it seem like working but I am receiving once in a while such logs\n\nINFO:local_deep_research.web_search_engines.engines.search_engine_pubmed:Query \"think tag place holder\" has 428117 total results in PubMed\n\nNote: I couldn't write the think tag here",
          "created_at": "2025-03-30T18:39:44Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Hmm that is really strange, because the solution should apply to all LLM calls. I basically catch the output of the LLM. I will reopen ",
          "created_at": "2025-03-30T19:04:55Z"
        }
      ]
    },
    {
      "issue_number": 159,
      "title": "'research_history' is saved where the 'ldr-web' command was run, not in the global directory",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run command 'ldr-web' in a folder\n2. Look for a file `research_history.db` in the folder\n3. Go to another folder, run the command again, see the database file inside\n\n**Expected behavior**\nThe `research_history.db` file should be saved into the `Documents\\LearningCircuit\\local-deep-research\\...` dir like the rest of the files\n\n**System Information:**\n - OS: Windows 11\n - Python Version: 3.12.9\n - Model Used: Deepseek\n - Hardware Specs: 16GB RAM, NVIDIA GEFORCE MX250\n",
      "state": "closed",
      "author": "kabachuha",
      "author_type": "User",
      "created_at": "2025-04-08T12:28:56Z",
      "updated_at": "2025-04-11T13:09:09Z",
      "closed_at": "2025-04-11T13:09:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/159/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/159",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/159",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:37.004379",
      "comments": [
        {
          "author": "djpetti",
          "body": "Hi @kabachuha,\n\nThis is expected behavior. The database (and cache) are currently stored in the working directory. We find that this is the most reliable way to handle saved data on multiple operating systems. However, we understand that this might be confusing, and will probably change it in a futu",
          "created_at": "2025-04-08T14:56:50Z"
        },
        {
          "author": "HashedViking",
          "body": "@kabachuha hi, incoming release will migrate all settings, logs, history to a single local `ldr.db` (with export capabilities), no more scattered data",
          "created_at": "2025-04-08T15:11:44Z"
        },
        {
          "author": "kabachuha",
          "body": "Got it",
          "created_at": "2025-04-09T11:07:37Z"
        },
        {
          "author": "djpetti",
          "body": "Closing this for now, since it should be addressed in the next release.",
          "created_at": "2025-04-11T13:09:08Z"
        }
      ]
    },
    {
      "issue_number": 151,
      "title": "Discord invite invalid",
      "body": "The link to the Discord server is dead.",
      "state": "closed",
      "author": "faurehu",
      "author_type": "User",
      "created_at": "2025-04-04T23:53:52Z",
      "updated_at": "2025-04-06T16:11:19Z",
      "closed_at": "2025-04-06T16:11:19Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/151/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/151",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/151",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:37.317571",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "https://discord.gg/ttcqQeFcJ3",
          "created_at": "2025-04-05T00:01:35Z"
        }
      ]
    },
    {
      "issue_number": 152,
      "title": "Loading the model using lm studio failed",
      "body": "![Image](https://github.com/user-attachments/assets/222b8659-34ae-4a58-8bce-6cae95e920a6)\n\n![Image](https://github.com/user-attachments/assets/08f8db46-ab89-4a8e-b271-07758a19a74b)\n\n![Image](https://github.com/user-attachments/assets/f355eeb3-c488-4ff6-9226-045bbde17cf8)\nLoading the model using lm studio failedÔºåAll changes except for the.env file",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2025-04-05T15:19:07Z",
      "updated_at": "2025-04-06T08:20:07Z",
      "closed_at": "2025-04-06T08:20:06Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/152/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/152",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/152",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:37.532669",
      "comments": [
        {
          "author": "bigdatabart",
          "body": "isn't that a typo? usually the name contains \"3\"\nI'm running the project for the first time now with gemma-3-12b-it and lmstudio",
          "created_at": "2025-04-05T22:02:16Z"
        },
        {
          "author": "brownplayer",
          "body": "> Ëøô‰∏çÊòØÈîôÂ≠óÂêóÔºüÈÄöÂ∏∏ÂêçÁß∞ÂåÖÂê´‚Äú3‚ÄùÊàëÁé∞Âú®ÊòØÁ¨¨‰∏ÄÊ¨°‰ΩøÁî® gemma-3-12b-it Âíå lmstudio ËøêË°åÈ°πÁõÆ\n\n![Image](https://github.com/user-attachments/assets/8396c7e4-473f-48d7-8657-dd833d584168)\nIt's not a typo, I downloaded it from huggingface so there's no \"3\" in the name.",
          "created_at": "2025-04-06T01:45:37Z"
        },
        {
          "author": "brownplayer",
          "body": "After clicking start research again, I observed that lmstudio responded, but there was still an error in the Deep Research System",
          "created_at": "2025-04-06T01:49:23Z"
        }
      ]
    },
    {
      "issue_number": 120,
      "title": "Filter for logs appears to be broken",
      "body": "**Describe the bug**\nSelecting a stricter filter for the log view doesn't appear to change anything.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Start the web interface\n2. Run a new query\n3. Select \"errors\" in the log view\n\n**Expected behavior**\nIt should only show errors.\n\n**System Information:**\n - OS: Ubuntu 24.04\n - Python Version: 3.13\n - Model Used: deepseek-r1:14b\n - Hardware Specs: AMD 3900X, 32 GB RAM, AMD RX 7900 XT\n\n**Additional context**\nNone\n\n**Output/Logs**\nNone\n\nThis issue is not super high priority, but someone who knows what they're doing with the frontend more than I do could probably fix it pretty quickly.\n\n![Image](https://github.com/user-attachments/assets/4e43e08c-e5f0-4469-96d9-8dcc74f3c346)",
      "state": "closed",
      "author": "djpetti",
      "author_type": "User",
      "created_at": "2025-03-30T16:26:00Z",
      "updated_at": "2025-04-01T23:21:31Z",
      "closed_at": "2025-04-01T23:21:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/120/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/120",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/120",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:37.795174",
      "comments": [
        {
          "author": "HashedViking",
          "body": "@djpetti nice catch, should be fixed at [commit](https://github.com/LearningCircuit/local-deep-research/pull/127/commits/d0d35b4e807052f975de4d5f4c75288147921a6a)",
          "created_at": "2025-03-31T14:26:11Z"
        },
        {
          "author": "djpetti",
          "body": "Can confirm. Let's close it",
          "created_at": "2025-04-01T23:21:19Z"
        }
      ]
    },
    {
      "issue_number": 101,
      "title": "FileNotFoundError: [Errno 2] No such file or directory: '/Users/chilikevin/miniconda3/envs/ldr/lib/python3.12/site-packages/local_deep_research/defaults/.env.template'",
      "body": "I just follow the official step to build app.but deliver this issue",
      "state": "closed",
      "author": "cloga",
      "author_type": "User",
      "created_at": "2025-03-27T02:56:40Z",
      "updated_at": "2025-03-30T19:58:59Z",
      "closed_at": "2025-03-30T19:58:59Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/101/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/101",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/101",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:39.704870",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Install with pip?",
          "created_at": "2025-03-27T07:45:12Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I will try to fix later today but need to know better what you did @cloga ",
          "created_at": "2025-03-27T07:50:06Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I made minor fix .. I dont know if it helps since i cannot reproduce it @cloga ",
          "created_at": "2025-03-27T19:05:31Z"
        }
      ]
    },
    {
      "issue_number": 114,
      "title": "Post install error: Module Not Found (local_deep_research)",
      "body": "Hi LearningCircuit (@LearningCircuit),\n\n**Error:**\nOnce the install completes, the web server window displays the following error message:\n\n~~~\nC:\\Program Files\\Python312\\python.exe: Error while finding module specification for 'local_deep_research.web.app' \n\n(ModuleNotFoundError: No module named 'local_deep_research')\n\nThe application has closed unexpectedly.\nPress any key to exit...\n~~~\n\n**System Information:**\nProcessor\tIntel(R) Core(TM) i7-6700K CPU @ 4.00GHz   4.01 GHz\nInstalled RAM\t16.0 GB\nSystem type\t64-bit operating system, x64-based processor\n\nWindows 11 Pro - 24H2\nOS build\t26100.3624\n\n**Output/Logs**\nIf you can let me know where any logs are located, I would be glad to include them in this issue.\n\nKind Regards,\nLiam",
      "state": "closed",
      "author": "celtic-coder",
      "author_type": "User",
      "created_at": "2025-03-29T20:10:14Z",
      "updated_at": "2025-03-29T21:57:09Z",
      "closed_at": "2025-03-29T21:56:29Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/114/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/114",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/114",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:39.871854",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "What happens if you close the application window and try to run the application again (by pressing start button and typing \"Local Deep Research\")?\n\nI tested this installer on 2 computer, but that doesnt seem to be enough hmn. Sorry about this.",
          "created_at": "2025-03-29T20:18:43Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Can you try to run the application a second time? I think you didnt have python installed on your system and although the installer seems to installs it. It seems that it cannot directly use it in the installer. \n\nI will investigate why, but you should be able to just run the installer again and tha",
          "created_at": "2025-03-29T20:22:02Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Actually, I was to fast I dont think it works so easily. I will investigate your error and reply once I figured it out.",
          "created_at": "2025-03-29T20:23:58Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I think installing it twice might help, but I am not 100% sure.",
          "created_at": "2025-03-29T20:27:49Z"
        },
        {
          "author": "celtic-coder",
          "body": "Hi Learning Circuit (@LearningCircuit),\n\nI think that I missed this error the first time that I did the install. On the subsequent installation, this was the error message. It appears that the application is attempting to use the Python version previously installed with the Scoop (https://scoop.sh/)",
          "created_at": "2025-03-29T20:58:13Z"
        }
      ]
    },
    {
      "issue_number": 76,
      "title": "Local search never finds relevant data in Documents",
      "body": "Local search never finds relevant data in documents even I am asking about the main information written in documents which is over 10 times repeated in the document. \nI am using PDF and MD documents in the file and I opened reindexing and injected local search in auto by index[0] by modfiying code, however when I use only local search the result doesnt change. \nWhen I follow the DEBUG:\nThe folder is indexed as intended. \nThe indexed data is sent to served as intended\nA new .chache folder with faiss database is created as intended\n\nHowever with whatever query goes to the local_search_engine  I receive \"No local documents found for the query\" log. \n\n\n\nFor example I have 3 documents related to POCT methods also contains LF and ELISA in the folder and one of the documents are a full local-deep-research detailed report for the same query so there are answers for the queries even I search in the file with Ctrl+F  among which are indexed:\n\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Saving index to .cache\\local_search\\index_516f5460176afed7b77b7a536e69c2a5\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Indexed 3 files in 21.69 seconds\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Successfully indexed 1 folders: C:/Users/Emre/Documents/LearningCircuit/local-deep-research/project_documents\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Trying search engine: local\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Searching local documents in collections: ['default']\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:Diagnostic for C:/Users/Emre/Documents/LearningCircuit/local-deep-research/project_documents:\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:  - Folder hash: c1c90263717fac020c2646cc80cbe402\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:  - Index path: .cache\\local_search\\index_c1c90263717fac020c2646cc80cbe402\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:  - Index exists on disk: False\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:  - Is in indexed_folders: False\nWARNING:local_deep_research.web_search_engines.engines.search_engine_local:Folder C:/Users/Emre/Documents/LearningCircuit/local-deep-research/project_documents has not been indexed\nINFO:local_deep_research.web_search_engines.engines.search_engine_local:No local documents found for query: Given the rapidly evolving landscape of point-of-care diagnostics (POCT), what are the *current* (Q4 2024 - Q1 2025) projected market penetration rates and adoption barriers for *non-amplification* direct pathogen detection technologies specifically in resource-limited settings, and how do these compare to established POCT methods like lateral flow assays and basic ELISA? (This moves beyond funding to *actual* market traction).\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:local returned no previews",
      "state": "closed",
      "author": "EggzYy",
      "author_type": "User",
      "created_at": "2025-03-24T15:31:18Z",
      "updated_at": "2025-03-27T19:33:31Z",
      "closed_at": "2025-03-27T19:33:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/76/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/76",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/76",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:40.085971",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "I think the best solution would be to use a stronger embedding model. The one that is included here is rather weak. ",
          "created_at": "2025-03-24T17:19:38Z"
        },
        {
          "author": "LearningCircuit",
          "body": "But you should be able to change it in the config",
          "created_at": "2025-03-24T17:19:52Z"
        },
        {
          "author": "djpetti",
          "body": "Hi, I notice that you appear to be using Ollama to generate embeddings? I think that I encountered the same issue, and [fixed it](https://github.com/LearningCircuit/local-deep-research/pull/78). Can you pull the main branch again and give it another shot?",
          "created_at": "2025-03-25T13:16:43Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@djpetti can we close this?",
          "created_at": "2025-03-27T19:24:20Z"
        },
        {
          "author": "djpetti",
          "body": "@LearningCircuit Yes, I think this should be resolved. @EggzYy can reopen if the problem persists",
          "created_at": "2025-03-27T19:26:04Z"
        }
      ]
    },
    {
      "issue_number": 82,
      "title": "'search' error",
      "body": "**Describe the bug**\nEven a bsic search query returns an error and produces no ouput.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run command ldr -web\n2. Enter query 'what is a start?'\n3. no output is shown in WebUI or CLI, but a correct 'research_output' is generated.\n\n**Expected behavior**\nI expected a basic or elaborate definition of a star.\n\n**System Information:**\n - OS: Win11 Pro with WSL2 Ubuntu 22.04\n - Python Version: 3.10.12\n - Model Used: gemma3:12B\n - Hardware Specs: 12900K, 32GiB DDR5, RTX5090\n\n**Additional context**\nAdd any other context about the problem here.\n\n**Output/Logs**\n```\nIf applicable, add relevant output or error logs here:\n\nINFO:llm_config:Available providers: ['OLLAMA', 'VLLM']\nINFO:local_deep_research.config:Search config params: {'search_tool': 'auto', 'llm_instance': ChatOllama(model='gemma3:12b', temperature=0.7, base_url='http://localhost:11434'), 'max_results': 50, 'region': 'us', 'time_period': 'y', 'safe_search': True, 'search_snippets_only': False, 'search_language': 'English', 'max_filtered_results': 5}\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: auto with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating auto with filtered parameters: dict_keys(['use_api_key_services', 'max_engines_to_try', 'max_results', 'max_filtered_results', 'llm'])\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Meta Search Engine initialized with 10 available engines: wikipedia, arxiv, pubmed, github, wayback, local_all, semantic_scholar, project_docs, research_papers, personal_notes\nINFO:local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: MetaSearchEngine\nINFO:local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.meta_search_engine.MetaSearchEngine object at 0x7fb796b32e60>>\nINFO:llm_config:Available providers: ['OLLAMA', 'VLLM']\nINFO:local_deep_research.main:Starting with settings: iterations=2, questions_per_iteration=2\nINFO:local_deep_research.main:Initializing configuration...\nINFO:local_deep_research.config:Search tool is: auto\nINFO:llm_config:Available providers: ['OLLAMA', 'VLLM']\nINFO:local_deep_research.config:Search config params: {'search_tool': 'auto', 'llm_instance': ChatOllama(model='gemma3:12b', temperature=0.7, base_url='http://localhost:11434'), 'max_results': 50, 'region': 'us', 'time_period': 'y', 'safe_search': True, 'search_snippets_only': False, 'search_language': 'English', 'max_filtered_results': 5}\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating search engine for tool: auto with params: dict_keys(['max_results', 'llm', 'max_filtered_results'])\nINFO:local_deep_research.web_search_engines.search_engine_factory:Creating auto with filtered parameters: dict_keys(['use_api_key_services', 'max_engines_to_try', 'max_results', 'max_filtered_results', 'llm'])\nINFO:local_deep_research.web_search_engines.engines.meta_search_engine:Meta Search Engine initialized with 10 available engines: wikipedia, arxiv, pubmed, github, wayback, local_all, semantic_scholar, project_docs, research_papers, personal_notes\nINFO:local_deep_research.web_search_engines.search_engine_factory:Successfully created search engine of type: MetaSearchEngine\nINFO:local_deep_research.web_search_engines.search_engine_factory:Engine has 'run' method: <bound method BaseSearchEngine.run of <local_deep_research.web_search_engines.engines.meta_search_engine.MetaSearchEngine object at 0x7fb796872ad0>>\nINFO:llm_config:Available providers: ['OLLAMA', 'VLLM']\nWelcome to the Advanced Research System\nType 'quit' to exit\n\nSelect output type:\n1) Quick Summary (Generated in a few minutes)\n2) Detailed Research Report (Recommended for deeper analysis - may take several hours)\nEnter number (1 or 2): 1\n\nEnter your research query: What are the key differences between stars and planets? Provide references.\n\nResearching... This may take a few minutes.\n\nINFO:local_deep_research.search_system:Starting research on topic: What are the key differences between stars and planets? Provide references.\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.search_system:Generated questions: ['\"Compare and contrast stars and planets: luminosity, composition, formation, and orbital characteristics, updated 2024-2025\"', '\"What are the fundamental differences between stars and planets regarding nuclear fusion, mass, and light emission - recent astronomical discoveries\"']\nINFO:local_deep_research.search_system:SEARCH ERROR: Error during search: 'strengths'\nINFO:local_deep_research.search_system:len search: 0\nINFO:local_deep_research.search_system:SEARCH ERROR: Error during search: 'strengths'\nINFO:local_deep_research.search_system:len search: 0\nINFO:local_deep_research.search_system:1\nINFO:local_deep_research.search_system:ITERATION\nINFO:local_deep_research.search_system:ITERATION\nINFO:local_deep_research.search_system:ITERATION - Compressing Knowledge\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.search_system:FINISHED ITERATION - Compressing Knowledge\nINFO:local_deep_research.search_system:Saving findings ...\nINFO:local_deep_research.config:Looking for config in: /home/jmb/.config/local_deep_research\nINFO:local_deep_research.search_system:Saved findings\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.search_system:Generated questions: ['\"How do recent observations of exoplanetary atmospheres, particularly those using JWST, challenge or refine the traditional compositional models of ice giants versus gas giants, and what implications does this have for understanding planet formation differences compared to star formation processes?\" (This question pushes beyond the general compositional differences already outlined and delves into the complexities revealed by recent, high-resolution atmospheric observations, probing for nuanced differences in formation that might blur the lines between planet types.)', '\"Considering the increasing evidence for brown dwarfs and objects on the star-planet mass continuum, what revised definitions or observational criteria are astronomers currently employing to definitively classify these objects, and how do these evolving classification schemes impact the fundamental distinctions drawn between stars and planets?\" (This question addresses the fuzzy boundary between stars and planets, acknowledging the existence of brown dwarfs and objects that defy easy categorization, and seeks to understand how current research is refining our understanding of these boundaries.)']\nINFO:local_deep_research.search_system:SEARCH ERROR: Error during search: 'strengths'\nINFO:local_deep_research.search_system:len search: 0\nINFO:local_deep_research.search_system:SEARCH ERROR: Error during search: 'strengths'\nINFO:local_deep_research.search_system:len search: 0\nINFO:local_deep_research.search_system:2\nINFO:local_deep_research.search_system:ITERATION\nINFO:local_deep_research.search_system:ITERATION\nINFO:local_deep_research.search_system:ITERATION - Compressing Knowledge\nINFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\nINFO:local_deep_research.search_system:FINISHED ITERATION - Compressing Knowledge\nINFO:local_deep_research.search_system:Saving findings ...\nINFO:local_deep_research.config:Looking for config in: /home/jmb/.config/local_deep_research\nINFO:local_deep_research.search_system:Saved findings\n\n=== QUICK SUMMARY ===\n\n\n",
      "state": "closed",
      "author": "twack3r",
      "author_type": "User",
      "created_at": "2025-03-25T14:48:30Z",
      "updated_at": "2025-03-26T11:49:23Z",
      "closed_at": "2025-03-25T19:39:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/82/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/82",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/82",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:40.276869",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Unfortunately I merged a PR of a contributor to early (yesterday). Will be fixed today. It will not happen again ",
          "created_at": "2025-03-25T16:08:28Z"
        },
        {
          "author": "LearningCircuit",
          "body": "Please use pip install for the moment or go back a few commits ",
          "created_at": "2025-03-25T16:09:41Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@twack3r ",
          "created_at": "2025-03-25T16:18:50Z"
        },
        {
          "author": "djpetti",
          "body": "@twack3r Can you try the code from the linked PR? I'm hoping it fixes your problem.",
          "created_at": "2025-03-25T17:31:21Z"
        },
        {
          "author": "LearningCircuit",
          "body": "@djpetti  ah didnt see and made similar fix :(",
          "created_at": "2025-03-25T19:39:19Z"
        }
      ]
    },
    {
      "issue_number": 39,
      "title": "New research always returns error 'NoneType' object has no attribute 'run'",
      "body": "Any new research that I try returns the error 'NoneType' object has no attribute 'run'",
      "state": "closed",
      "author": "lecharblanc",
      "author_type": "User",
      "created_at": "2025-03-16T14:39:41Z",
      "updated_at": "2025-03-21T16:21:08Z",
      "closed_at": "2025-03-21T16:21:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/39/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/39",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/39",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:40.496302",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "Does it also happen if you use python main.py?",
          "created_at": "2025-03-16T16:03:54Z"
        },
        {
          "author": "lecharblanc",
          "body": "Thanks for the pointer. Turns out I was missing the Wikipedia package. I installed it and the error went away.  ",
          "created_at": "2025-03-16T20:02:20Z"
        }
      ]
    },
    {
      "issue_number": 60,
      "title": "`pypdf` package not found, please install it with `pip install pypdf`",
      "body": "I wanted to use the **RAG** functionality, but before it worked, I had to install pypdf additionally. It seems like PyPDF2 is either not needed or not sufficient.\n\nThis was the output before installing pypdf:\n\n```bash\nERROR:web_search_engines.engines.search_engine_local:Error loading /home/XXX/local-deep-research/\nlocal_search_files/research_papers/When_things_get_older_Exploring_circuit_aging_in_IoT_applications.pdf: `pypdf`\npackage not found, please install it with `pip install pypdf`\n```\n\nAfter installing it, everything worked like a charm!\nGenerally, a really nice project with great results so far, thanks a lot!",
      "state": "closed",
      "author": "Dr-Shab",
      "author_type": "User",
      "created_at": "2025-03-19T20:50:08Z",
      "updated_at": "2025-03-20T08:13:30Z",
      "closed_at": "2025-03-20T08:13:30Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/60/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/60",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/60",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:40.709225",
      "comments": [
        {
          "author": "LearningCircuit",
          "body": "If you want add it to the requirements and make a PR. Than you will also be a contributor to the project \n\nOtherwise I will do it over the weekend ",
          "created_at": "2025-03-19T21:02:09Z"
        },
        {
          "author": "LearningCircuit",
          "body": "I just added pypdf to requirements https://github.com/LearningCircuit/local-deep-research/blob/a422959dc2a571d9a37b2c5ac89769e7f37a5ec5/requirements.txt#L23\n\nfeel free to contribut if you want.\n\nAnd thank you",
          "created_at": "2025-03-19T23:33:36Z"
        }
      ]
    },
    {
      "issue_number": 54,
      "title": "LLM makes things up",
      "body": "The results from the Research contain many papers, but when I searched for these paper titles, I found that these papers were completely made up by the model.",
      "state": "closed",
      "author": "Seraphli",
      "author_type": "User",
      "created_at": "2025-03-18T16:44:22Z",
      "updated_at": "2025-03-18T20:18:10Z",
      "closed_at": "2025-03-18T20:13:01Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/54/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/54",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/54",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:40.890481",
      "comments": [
        {
          "author": "Seraphli",
          "body": "I initially tried a smaller model, using mistral-nemo:12b. Now I'm using a larger model qwq:32b-q4, which is noticeably better, and the provided link can be opened as well. This issue seems to be resolvable by increasing the model size.",
          "created_at": "2025-03-18T20:13:01Z"
        },
        {
          "author": "Seraphli",
          "body": "Additionally, the obtained content seems to have a low relevance to the inquiry, and it may be necessary to adjust the prompt",
          "created_at": "2025-03-18T20:18:09Z"
        }
      ]
    },
    {
      "issue_number": 41,
      "title": "Install instructions need `playwright install`",
      "body": "`playwright install` is required after `pip install -r requirements.txt`",
      "state": "closed",
      "author": "bjj",
      "author_type": "User",
      "created_at": "2025-03-17T02:36:20Z",
      "updated_at": "2025-03-17T17:02:50Z",
      "closed_at": "2025-03-17T17:02:49Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/41/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/41",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/41",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:41.077666",
      "comments": [
        {
          "author": "HashedViking",
          "body": "Nice catch, updated the readme #43 ",
          "created_at": "2025-03-17T14:54:31Z"
        },
        {
          "author": "LearningCircuit",
          "body": "readme updated",
          "created_at": "2025-03-17T17:02:49Z"
        }
      ]
    },
    {
      "issue_number": 29,
      "title": "generated PDF are largely oversized",
      "body": "**Describe the bug**\nHello, you're doing a nice job here ! \n \nI've successfully done a quick search, but the generated PDF is largely oversized.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run command 'Quick Search' \n2. Enter query 'who is napoleon ?'\n3. Search went successfully, cheers, then i want download the pdf file\n\n**Expected behavior**\ngenerated pdf file size 102.5Mb ! \nAlso some texts seems to be rendered as images.\n\nBy getting query markdown answer directly from browser console and pull it in a markdown that i export myself to pdf i get a normal 115Kb sized file\n\n**System Information:**\n - OS: [e.g. Arch Linux Kernel Zen 6.1.3]\n - Python Version: [e.g. 3.13.2]\n - Model Used: [e.g. gemma1:3b]\n - Hardware Specs: [e.g. 32GB RAM, i9 9900k]\n\n\n",
      "state": "closed",
      "author": "Theo-codatte",
      "author_type": "User",
      "created_at": "2025-03-13T14:51:20Z",
      "updated_at": "2025-03-16T11:21:47Z",
      "closed_at": "2025-03-16T11:21:47Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/29/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/LearningCircuit/local-deep-research/issues/29",
      "api_url": "https://api.github.com/repos/LearningCircuit/local-deep-research/issues/29",
      "repository": "LearningCircuit/local-deep-research",
      "extraction_date": "2025-06-22T00:44:41.301919",
      "comments": [
        {
          "author": "HashedViking",
          "body": "Yeah, the whole PDF is just a crazy big image, I'll look into this when find some time, thanks for pointing it out",
          "created_at": "2025-03-13T17:11:26Z"
        },
        {
          "author": "HashedViking",
          "body": "#31 fixes it",
          "created_at": "2025-03-14T09:22:03Z"
        },
        {
          "author": "HashedViking",
          "body": "@LearningCircuit close the issue, please, this bug is fixed ",
          "created_at": "2025-03-16T06:44:30Z"
        }
      ]
    }
  ]
}