{
  "repository": "av/harbor",
  "repository_info": {
    "repo": "av/harbor",
    "stars": 1863,
    "language": "Python",
    "description": "Effortlessly run LLM backends, APIs, frontends, and services with one command.",
    "url": "https://github.com/av/harbor",
    "topics": [
      "ai",
      "bash",
      "cli",
      "container",
      "docker",
      "docker-compose",
      "exl2",
      "gguf",
      "llm",
      "local",
      "mcp",
      "npm",
      "package",
      "pypi",
      "safetensors",
      "self-hosted",
      "tool",
      "tools"
    ],
    "created_at": "2024-07-27T18:46:59Z",
    "updated_at": "2025-06-22T01:52:38Z",
    "search_query": "local llm language:python stars:>2",
    "total_issues_estimate": 158,
    "labeled_issues_estimate": 117,
    "labeling_rate": 74.4,
    "sample_labeled": 32,
    "sample_total": 43,
    "has_issues": true,
    "repo_id": 834595923,
    "default_branch": "main",
    "size": 25784
  },
  "extraction_date": "2025-06-22T00:49:36.769606",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 121,
  "issues": [
    {
      "issue_number": 61,
      "title": "Harbor App does not work under Wayland",
      "body": "Hello,\r\n\r\ngreat job with the development of this amazing project!\r\n\r\nI wanted to try the Harbor App under my Linux system that uses Wayland instead of X11. I get the following error however:\r\n```\r\nWarning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\r\nCannot get default EGL display: EGL_SUCCESS\r\nCannot create EGL context: invalid display (last error: EGL_SUCCESS)\r\n```\r\n\r\nEven ignoring the error by setting `QT_QPA_PLATFORM=wayland` does not help:\r\n```\r\nWarning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\r\nQSocketNotifier: Can only be used with threads started with QThread\r\nCannot get default EGL display: EGL_SUCCESS\r\nCannot create EGL context: invalid display (last error: EGL_SUCCESS)\r\n```",
      "state": "closed",
      "author": "maeyounes",
      "author_type": "User",
      "created_at": "2024-10-14T17:57:24Z",
      "updated_at": "2025-06-11T17:06:26Z",
      "closed_at": "2024-10-27T09:35:37Z",
      "labels": [
        "question",
        "OS:Linux"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/61/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/61",
      "api_url": "https://api.github.com/repos/av/harbor/issues/61",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:08.731559",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for the kind words and for trying it out!\r\n\r\nThe `EGL_SUCCESS` should be related to the `webkitgtk` and should be fixed by them in the upcoming releases, meanwhile disabling compositing via env vars could work:\r\n```bash\r\n# Try this first\r\nWEBKIT_DISABLE_DMABUF_RENDERER=1\r\n\r\n# If the first doe",
          "created_at": "2024-10-14T18:07:22Z"
        },
        {
          "author": "maeyounes",
          "body": "Thank you for the reply.\r\n\r\nUsing `WEBKIT_DISABLE_DMABUF_RENDERER=1` helps, while `WEBKIT_DISABLE_COMPOSITING_MODE=1` doesn't.\r\n\r\nHowever, although the app is rendered now, there is another issue that might not be related. The app does not show any service: Unexpected error: No such file or director",
          "created_at": "2024-10-14T19:00:27Z"
        },
        {
          "author": "av",
          "body": "Sorry for a delayed reply \r\n\r\nThis looks like a problem executing `harbor` command, so let's check the very basic things first - is Harbor CLI installed? If not - please see the [installation guide](https://github.com/av/harbor/wiki/1.0.-Installing-Harbor#harbor-cli). \r\n\r\nThe next thing to verify wo",
          "created_at": "2024-10-27T08:59:32Z"
        },
        {
          "author": "maeyounes",
          "body": "Thanks for the answer.\r\n\r\nThe problem was fixed after installing the CLI. I assumed that the App is independent of the CLI, so it was my bad.",
          "created_at": "2024-10-27T09:05:31Z"
        },
        {
          "author": "av",
          "body": "No worries, thanks for confirming it wasn't some obscure platform bug!\r\n\r\nIf I'll ever have enough time - there are plans for the app to be self-contained and to manage the CLI installation (not soon, that's for sure though)",
          "created_at": "2024-10-27T09:20:16Z"
        }
      ]
    },
    {
      "issue_number": 140,
      "title": "Fail to download JSR package",
      "body": "Hello,\n\nI just updated to the latest main branch and I have trouble to get Harbor started, and the error message is like this:\n``` bash\nDownload https://jsr.io/@std/yaml/meta.json\nDownload https://jsr.io/@std/collections/meta.json\nDownload https://jsr.io/@std/fmt/meta.json\nerror: JSR package manifest for '@std/yaml' failed to load. Import 'https://jsr.io/@std/yaml/meta.json' failed.\n    at file:///mnt/data/apps/harbor/routines/mergeComposeFiles.js:1:23\nexit status 1\n```\n\nIt looks like the new `routines` functions require some connect with `jsr.io` and I have tried `curl https://jsr.io/@std/yaml/meta.json` it produce some result:\n```  bash\n{\n  \"scope\": \"std\",\n  \"name\": \"yaml\",\n  \"latest\": \"1.0.5\",\n  \"versions\": {\n    \"1.0.2\": {},\n    \"0.210.0\": {},\n    \"0.209.0\": {},\n    \"0.202.0\": {},\n    \"0.204.0\": {},\n    \"0.211.0\": {},\n    \"0.224.1\": {},\n    \"1.0.1\": {},\n    \"0.222.1\": {},\n    \"1.0.5\": {},\n    \"0.220.1\": {},\n    \"0.218.1\": {},\n    \"0.199.0\": {},\n    \"1.0.3\": {},\n    \"1.0.4\": {},\n    \"0.205.0\": {},\n    \"0.217.0\": {},\n    \"1.0.0-rc.3\": {},\n    \"0.224.3\": {},\n    \"0.221.0\": {},\n    \"0.206.0\": {},\n    \"0.196.0\": {},\n    \"0.218.0\": {},\n    \"0.224.2\": {},\n    \"0.224.0\": {},\n    \"0.216.0\": {},\n    \"0.214.0\": {},\n    \"0.215.0\": {},\n    \"0.198.0\": {},\n    \"0.208.0\": {},\n    \"0.197.0\": {},\n    \"0.219.0\": {},\n    \"1.0.0-rc.1\": {},\n    \"0.212.0\": {},\n    \"0.213.1\": {},\n    \"0.222.0\": {},\n    \"0.207.0\": {},\n    \"0.201.0\": {},\n    \"0.219.1\": {},\n    \"0.200.0\": {},\n    \"0.218.2\": {},\n    \"1.0.0-rc.2\": {},\n    \"0.223.0\": {},\n    \"0.213.0\": {},\n    \"0.203.0\": {},\n    \"1.0.0-rc.4\": {},\n    \"1.0.0\": {}\n  }\n```\n\nI am behind the proxy and has self-signed certificate and I don't know if it matters. \nThanks for your help!\n\nRegards,",
      "state": "open",
      "author": "shenhai-ran",
      "author_type": "User",
      "created_at": "2025-03-04T09:41:20Z",
      "updated_at": "2025-06-09T12:19:53Z",
      "closed_at": null,
      "labels": [
        "bug",
        "CLI"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/140/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/140",
      "api_url": "https://api.github.com/repos/av/harbor/issues/140",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:08.952669",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for the detailed report and sorry that the update wasn't smooth in your instance!\n\nYes, it's related to the new routines functionality that has lazy/dynamic dependency install from JSR during the cold start. \n\n1. From the looks of it - you do have access to JSR from behind your proxy (gra",
          "created_at": "2025-03-04T13:49:12Z"
        },
        {
          "author": "shenhai-ran",
          "body": "Hi,\n\nThanks for your quick feedback!\n\n1. Actually this might be the problem, I tried to attach `harbor.ollama` container, and has trouble with curl connection:\n``` bash\n# curl https://jsr.io/@std/yaml/meta.json\ncurl: (60) SSL certificate problem: self signed certificate in certificate chain\nMore det",
          "created_at": "2025-03-06T08:14:26Z"
        },
        {
          "author": "av",
          "body": "> there should be some docker services for the new routines\n\nYes, but it's fully [inline in the CLI here](https://github.com/av/harbor/blob/main/harbor.sh#L342), so any modifications of the volumes needs to happen there, I think another issue would be that it's a `distroless` image, so it might not ",
          "created_at": "2025-04-19T14:07:19Z"
        },
        {
          "author": "boriskroeger",
          "body": "I have the same issue but without a proxy.\nOnly fallback to legacy cli let me start harbor.",
          "created_at": "2025-06-09T12:19:52Z"
        }
      ]
    },
    {
      "issue_number": 158,
      "title": "Bug: harbor.webui is unhealthy",
      "body": " ```\nharbor up\n[+] Running 3/4\n âœ” Network harbor_harbor-network  Created                                                                                                                                              0.0s\n âœ” Container harbor.ollama        Healthy                                                                                                                                              2.5s\n â ¹ Container harbor.webui         Waiting                                                                                                                                             15.0s\n âœ” Container harbor.ollama-init   Healthy                                                                                                                                              2.4s\ncontainer harbor.webui is unhealthy\n```\n```\n harbor logs webui\n^Bvharbor.webui  | Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nFetching 30 files:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:02<00:07,  3.09it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nharbor.webui  | WARNI [huggingface_hub.file_download] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nFetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:08<00:00,  3.50it/s]\nharbor.webui  | INFO:     Started server process [10]\nharbor.webui  | INFO:     Waiting for application startup.\nharbor.webui  | 2025-04-30 12:50:29.773 | INFO     | open_webui.utils.logger:start_logger:140 - GLOBAL_LOG_LEVEL: INFO - {}\n```",
      "state": "open",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2025-04-30T15:53:16Z",
      "updated_at": "2025-06-06T15:36:54Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/158/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/158",
      "api_url": "https://api.github.com/repos/av/harbor/issues/158",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:09.140784",
      "comments": [
        {
          "author": "av",
          "body": "I tightened up `webui` healthcheck recently, to avoid always waiting full ten seconds for its start. \n\nBy the look of the logs - open webui starts normally, it just takes longer than healthcheck is configured to wait. Is this something that happens to you consistently, or sporadically?",
          "created_at": "2025-05-10T08:44:22Z"
        },
        {
          "author": "FiredMercury21",
          "body": "I'm getting this bug sometimes. Not sure why, but if I try the command a couple more times it seems to work. Can't really make out a pattern.",
          "created_at": "2025-06-06T15:36:53Z"
        }
      ]
    },
    {
      "issue_number": 57,
      "title": "LLM Tools integrations",
      "body": "https://github.com/underlines/awesome-ml/blob/master/llm-tools.md\r\n",
      "state": "open",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-10-07T18:05:02Z",
      "updated_at": "2025-05-27T10:09:01Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 24,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/57/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/57",
      "api_url": "https://api.github.com/repos/av/harbor/issues/57",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:09.323914",
      "comments": []
    },
    {
      "issue_number": 164,
      "title": "Updating container / tool / service versions?",
      "body": "Is there a straightforward and easy way for users to try updating containers and reverting those updates, or to jump to the latest? Particularly if there are ways to go back to the safe tested defaults harbor supplies.\n\nFor example, the current harbor openwebui is 0.6.7 and the latest release is 0.6.9. This is similarly true for others like openhands where there have been some significant fixes recently.\n\nI checked locations I thought would be relevant in the wiki but mentions of updates concerned updating harbor itself, rather than the packages:\nhttps://github.com/av/harbor/wiki/3.-Harbor-CLI-Reference\nhttps://github.com/av/harbor/wiki\n\nThanks!",
      "state": "open",
      "author": "ahundt",
      "author_type": "User",
      "created_at": "2025-05-11T21:49:41Z",
      "updated_at": "2025-05-19T00:07:50Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/164/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/164",
      "api_url": "https://api.github.com/repos/av/harbor/issues/164",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:09.323937",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for trying out Harbor and for your recent reports! \n\nHarbor's service version strategy is to use latest version until the service is known too break too often or require manual patching to update.\n\nIn all service wikis, the \"Starting\" section's first command outlines how service can  be u",
          "created_at": "2025-05-15T06:06:01Z"
        },
        {
          "author": "av",
          "body": "For example, for the Open WebUI:\n```bash\n# List Open WebUI-related configs\nharbor config ls | grep WEBUI\n\nWEBUI_HOST_PORT                33801\nWEBUI_SECRET                   h@rb0r\nWEBUI_NAME                     Harbor\nWEBUI_LOG_LEVEL                INFO\nWEBUI_VERSION                  main\nWEBUI_IMA",
          "created_at": "2025-05-17T09:51:01Z"
        },
        {
          "author": "ahundt",
          "body": "There is a specific failure in openhands I was hoping to test with an update. \n\nhttps://github.com/All-Hands-AI/OpenHands/issues/8317",
          "created_at": "2025-05-18T21:12:44Z"
        },
        {
          "author": "av",
          "body": "That can be set with:\n```bash\nharbor config set openhands.version main\n```\n\nAnd then rebuild/restart for new image to take place",
          "created_at": "2025-05-19T00:07:49Z"
        }
      ]
    },
    {
      "issue_number": 163,
      "title": "boost tools appearing but not working in fresh openwebui setup",
      "body": "Hi, harbor is a very cool system that saves a ton of setup work. Thanks!\n\nI started openwebui where on the first harbor up contained both webui and boost (and the command in the current run).\n\nThe tools appear in the model list, however, if I send commands to the boost versions, such as rcn it stalls out.\n\nWorking example with a regular model:\n\n```\ngemma3:4b-it-qat\nOkay, I received your \"test.\"\n\nIs there anything specific you wanted me to do or respond to? Do you want me to:\n\nRespond with a simple \"Test received!\"?\nAnswer a question?\nPerform a task (like writing a sentence, summarizing something, or translating text)?\nLet me know what you're expecting!\n```\n\n\nFailed Example with rcn on the same model, this is a very fast and small model and I've waited several minutes:\n```\nrcn gemma3:4b-it-qat\nToday at 5:34 PM\n\n3/4\n```\n\nBasically it is flashing that initial text gen background.\n\n![Image](https://github.com/user-attachments/assets/0a6549da-fcad-4f4f-9604-f18111e2904b)\n\n\n\n\n\n",
      "state": "open",
      "author": "ahundt",
      "author_type": "User",
      "created_at": "2025-05-11T21:39:50Z",
      "updated_at": "2025-05-17T13:38:59Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/163/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/163",
      "api_url": "https://api.github.com/repos/av/harbor/issues/163",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:09.577218",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for the report (and for trying out Boost)!\nIt was a regression in the `chat.advance` in one of the relatively recent releases. The fix is in `main` and will be a part of next Harbor release",
          "created_at": "2025-05-17T09:43:51Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.3.12](https://github.com/av/harbor/releases/tag/v0.3.12)",
          "created_at": "2025-05-17T13:38:58Z"
        }
      ]
    },
    {
      "issue_number": 159,
      "title": "Unable to run MetaMPC -> MCPO Unsupported protocol version from the server: 2025-03-26",
      "body": "Upon a fresh install of harbor on a fresh installation of ubuntu I'm getting the following errors. MCPO will not stay running and open-webui cannot connect to the tools.\n\nSteps:\n- Install Ubuntu && Update to latest packages\n- Install harbor using the unsafe script\n- Run  `harbor update` to double check latest\n- Run `harbor up` for initialization\n- Run `harbor up metampc` to perform initialization (error from metampc-migrate initially)\n- Rerun `harbor metampc` no issues in metampc-migrate logs, successful migration\n- Open metampc interface and add `Time and Timezone` and `Sequential Thinking`\n- Run `harbor up metampc mcpo`  Error starting metampc-sse (No Api-Key defined)\n- Run `harbor down`\n- Run `harbor up ollama open-webui metampc mcpo` no errors from docker, mcpo quietly errors and shuts down\n\nAt this point open-webui was configured for the mcpo server, but is unable to connect to it. Logs attached below.\n\n\nHarbor Version:\n```\nHarbor CLI version: 0.3.10\n```\n\nMCPO Logs:\n```\nHarbor: MCPO Entrypoint\nPython 3.11.2\nJSON Merger is starting...\nMerged JSON files matching '.json' into '/app/config.json' with environment variables rendered\nMerged Configs:\n{\n  \"mcpServers\": {\n    \"metamcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"supergateway\",\n        \"--sse\",\n        \"http://metamcp-sse:12006/sse\"\n      ]\n    }\n  }\n}\nStarting MCPO...\nINFO:     Started server process [33]\nINFO:     Waiting for application startup.\n[supergateway] Starting...\n[supergateway] Supergateway is supported by Supermachine (hosted MCPs) - https://supermachine.ai\n[supergateway]   - outputTransport: stdio\n[supergateway]   - sse: http://metamcp-sse:12006/sse\n[supergateway]   - Headers: (none)\n[supergateway] Connecting to SSE...\n[supergateway] Stdio server listening\n[supergateway] Stdio \\u2192 SSE: {\n  jsonrpc: '2.0',\n  id: 0,\n  method: 'initialize',\n  params: {\n    protocolVersion: '2024-11-05',\n    capabilities: { sampling: {}, roots: [Object] },\n    clientInfo: { name: 'mcp', version: '0.1.0' }\n  }\n}\n[supergateway] SSE connected\n[supergateway] Response: {\n  jsonrpc: '2.0',\n  id: 0,\n  result: {\n    protocolVersion: '2025-03-26',\n    capabilities: { prompts: {}, resources: {}, tools: {} },\n    serverInfo: { name: 'MetaMCP', version: '0.5.0' }\n  }\n}\n[supergateway] stdin closed. Exiting...\nERROR:      + Exception Group Traceback (most recent call last):\n  |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/starlette/routing.py\", line 692, in lifespan\n  |     async with self.lifespan_context(app) as maybe_state:\n  |   File \"/usr/lib/python3.11/contextlib.py\", line 204, in __aenter__\n  |     return await anext(self.gen)\n  |            ^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcpo/main.py\", line 92, in lifespan\n  |     await stack.enter_async_context(\n  |   File \"/usr/lib/python3.11/contextlib.py\", line 638, in enter_async_context\n  |     result = await _enter(cm)\n  |              ^^^^^^^^^^^^^^^^\n  |   File \"/usr/lib/python3.11/contextlib.py\", line 204, in __aenter__\n  |     return await anext(self.gen)\n  |            ^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcpo/main.py\", line 104, in lifespan\n  |     async with stdio_client(server_params) as (reader, writer):\n  |   File \"/usr/lib/python3.11/contextlib.py\", line 222, in __aexit__\n  |     await self.gen.athrow(typ, value, traceback)\n  |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 166, in stdio_client\n  |     async with (\n  |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Exception Group Traceback (most recent call last):\n    |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 173, in stdio_client\n    |     yield read_stream, write_stream\n    |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcpo/main.py\", line 105, in lifespan\n    |     async with ClientSession(reader, writer) as session:\n    |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n    |     raise BaseExceptionGroup(\n    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n    +-+---------------- 1 ----------------\n      | Traceback (most recent call last):\n      |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcpo/main.py\", line 107, in lifespan\n      |     await create_dynamic_endpoints(app, api_dependency=api_dependency)\n      |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcpo/main.py\", line 24, in create_dynamic_endpoints\n      |     result = await session.initialize()\n      |              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"/app/cache/uv/archive-v0/mKsbP1XMmZ_gjV5LL0vPC/lib/python3.11/site-packages/mcp/client/session.py\", line 145, in initialize\n      |     raise RuntimeError(\n      | RuntimeError: Unsupported protocol version from the server: 2025-03-26\n      +------------------------------------\n\nERROR:    Application startup failed. Exiting.\nStarting MCP OpenAPI Proxy with config file: /app/config.json\n```\n\nMetaMPC Logs\n```\n   \\u25b2 Next.js 15.2.3\n   - Local:        http://localhost:3000\n   - Network:      http://0.0.0.0:3000\n\n \\u2713 Starting...\n \\u2713 Ready in 184ms\n```\n\nHarbor PS\n```\nNAME                      IMAGE                                COMMAND                  SERVICE            CREATED          STATUS                    PORTS\nharbor.metamcp            harbor-metamcp                       \"docker-entrypoint.sâ€¦\"   metamcp            15 minutes ago   Up 15 minutes             0.0.0.0:34421->3000/tcp, [::]:34421->3000/tcp\nharbor.metamcp-postgres   postgres:16.2-alpine3.18             \"docker-entrypoint.sâ€¦\"   metamcp-postgres   15 minutes ago   Up 15 minutes             0.0.0.0:34422->5432/tcp, [::]:34422->5432/tcp\nharbor.metamcp-sse        harbor-metamcp-sse                   \"node /app/start-sseâ€¦\"   metamcp-sse        15 minutes ago   Up 15 minutes (healthy)   0.0.0.0:34423->12006/tcp, [::]:34423->12006/tcp\nharbor.ollama             ollama/ollama:latest                 \"/bin/ollama serve\"      ollama             15 minutes ago   Up 15 minutes (healthy)   0.0.0.0:33821->11434/tcp, [::]:33821->11434/tcp\nharbor.webui              ghcr.io/open-webui/open-webui:main   \"/app/start_webui.sh\"    webui              15 minutes ago   Up 15 minutes (healthy)   0.0.0.0:33801->8080/tcp, [::]:33801->8080/tcp\n```\n\nServer Info\n```\n            .-/+oossssoo+/-.               nachtraben@ai \n        `:+ssssssssssssssssss+:`           ------------- \n      -+ssssssssssssssssssyyssss+-         OS: Ubuntu 24.04.2 LTS x86_64 \n    .ossssssssssssssssssdMMMNysssso.       Host: KVM/QEMU (Standard PC (Q35 + ICH9, 2009) pc-q35-9.2) \n   /ssssssssssshdmmNNmmyNMMMMhssssss/      Kernel: 6.11.0-24-generic \n  +ssssssssshmydMMMMMMMNddddyssssssss+     Uptime: 4 hours, 42 mins \n /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/    Packages: 1732 (dpkg), 11 (snap) \n.ssssssssdMMMNhsssssssssshNMMMdssssssss.   Shell: bash 5.2.21 \n+sssshhhyNMMNyssssssssssssyNMMMysssssss+   Resolution: 2560x1342 \nossyNMMMNyMMhsssssssssssssshmmmhssssssso   DE: GNOME 46.0 \nossyNMMMNyMMhsssssssssssssshmmmhssssssso   WM: Mutter \n+sssshhhyNMMNyssssssssssssyNMMMysssssss+   WM Theme: Adwaita \n.ssssssssdMMMNhsssssssssshNMMMdssssssss.   Theme: Yaru-dark [GTK2/3] \n /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/    Icons: Yaru [GTK2/3] \n  +sssssssssdmydMMMMMMMMddddyssssssss+     Terminal: gnome-terminal \n   /ssssssssssshdmNNNNmyNMMMMhssssss/      CPU: Intel Xeon E5-2697A v4 (48) @ 2.599GHz \n    .ossssssssssssssssssdMMMNysssso.       GPU: 00:01.0 Red Hat, Inc. QXL paravirtual graphic card \n      -+sssssssssssssssssyyyssss+-         Memory: 3329MiB / 128777MiB \n        `:+ssssssssssssssssss+:`\n            .-/+oossssoo+/-.                                       \n                                                                   \n```\n\n",
      "state": "open",
      "author": "NachtRaben",
      "author_type": "User",
      "created_at": "2025-05-01T23:19:36Z",
      "updated_at": "2025-05-12T16:33:15Z",
      "closed_at": null,
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/159/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/159",
      "api_url": "https://api.github.com/repos/av/harbor/issues/159",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:09.766968",
      "comments": [
        {
          "author": "nickgnat",
          "body": "Same issue here - it appears that mcpo (via supergateway) sends protocolVersion 2024-11-05, which creates a mismatch with MetaMPC:\n\n```\n[supergateway] Connecting to SSE...\n[supergateway] Stdio server listening\n[supergateway] Stdio â†’ SSE: {\n  jsonrpc: '2.0',\n  id: 0,\n  method: 'initialize',\n  params:",
          "created_at": "2025-05-10T01:28:56Z"
        },
        {
          "author": "av",
          "body": "Hi, thank you for such a detailed report\n\nI tried to reproduce today (sadly unsuccessfully) with latest versions of the services and a clean build.\n\nOne important note about MetaMCP git ref:\n```bash\n# We're building from a fixed git hash\n# You can update it to build a different version\nharbor config",
          "created_at": "2025-05-10T10:27:51Z"
        },
        {
          "author": "nickgnat",
          "body": "@av Following your steps exactly leads to #162 for me. I'm able to get around the undefined 'api_key'  by going into MetaMCP's webUI and generating an API key in the /api-keys page.\n\nThen if I `harbor restart metamcp mcpo`, the `metamcp-sse` container comes up successfully, but I get the issue expla",
          "created_at": "2025-05-10T23:11:24Z"
        },
        {
          "author": "ahundt",
          "body": "@nickgnat could you share the details of that workaround?",
          "created_at": "2025-05-11T06:11:33Z"
        },
        {
          "author": "scarecr0w12",
          "body": "Just confirmed running the MetaMCP 0.5.3 (as well as doing @nickgnat  fix for the API key) both MetaMCP and MCPO work on fresh pulls and build\n",
          "created_at": "2025-05-12T00:37:33Z"
        }
      ]
    },
    {
      "issue_number": 162,
      "title": "MetaMCP error due to unresolvable api_key",
      "body": "When trying to run `harbor up ollama metamcp mcpo` (With WebUI as default) I get the following error:\n\n```\nharbor.metamcp-sse  | TypeError: Cannot read properties of undefined (reading 'api_key')\nharbor.metamcp-sse  |     at resolveApiKey (file:///app/start-sse.mjs:22:25)\nharbor.metamcp-sse  |     at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\nharbor.metamcp-sse  |     at async main (file:///app/start-sse.mjs:79:15)\n\n```\n",
      "state": "open",
      "author": "scarecr0w12",
      "author_type": "User",
      "created_at": "2025-05-09T18:18:18Z",
      "updated_at": "2025-05-11T21:54:07Z",
      "closed_at": null,
      "labels": [
        "bug",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/162/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/162",
      "api_url": "https://api.github.com/repos/av/harbor/issues/162",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:09.977222",
      "comments": [
        {
          "author": "av",
          "body": "MetaMCP requires an API key to use its MCP server, normally it's created automatically and stored in the service's DB, which `metamcp-sse` connects to and picks it up from.\n\nThe error you shared indicates that the key is missing due to some reason. Could you please verify that it's there in the Meta",
          "created_at": "2025-05-10T10:42:50Z"
        },
        {
          "author": "scarecr0w12",
          "body": "> MetaMCP requires an API key to use its MCP server, normally it's created automatically and stored in the service's DB, which `metamcp-sse` connects to and picks it up from.\n> \n> The error you shared indicates that the key is missing due to some reason. Could you please verify that it's there in th",
          "created_at": "2025-05-10T17:19:24Z"
        },
        {
          "author": "ahundt",
          "body": "I'm seeing the exact same issue when it tries to run harbor up metamcp:\n\n```\nÂ± harbor logs metamcp-sse\nharbor.metamcp-sse  | TypeError: Cannot read properties of undefined (reading 'api_key')\nharbor.metamcp-sse  |     at resolveApiKey (file:///app/start-sse.mjs:22:25)\nharbor.metamcp-sse  |     at pr",
          "created_at": "2025-05-11T06:10:02Z"
        },
        {
          "author": "nickgnat",
          "body": "@ahundt What seems to work for me is after I get that error message:\nGo to the MetaMCP webui (http://localhost:34421/)\nClick \"API Keys\" and generate a key\nThen `harbor restart metamcp`",
          "created_at": "2025-05-11T07:05:21Z"
        },
        {
          "author": "scarecr0w12",
          "body": "> [@ahundt](https://github.com/ahundt) What seems to work for me is after I get that error message: Go to the MetaMCP webui (http://localhost:34421/) Click \"API Keys\" and generate a key Then `harbor restart metamcp`\n\nCan confirm this works",
          "created_at": "2025-05-11T08:15:24Z"
        }
      ]
    },
    {
      "issue_number": 161,
      "title": "storing harbor and its apps state someplace safe? setting that location?",
      "body": "I'm trying out harbor and I installed it with uv, so right now I believe the state is in an .env directory. Is there a way to configure a main state directory?\n\nsimilarly, I also have an existing open-webui install and that state in a local directory. I know ollama can be configured to use the local install, is that possible with open webui? It would also be ok if open webui ran in docker but read and wrote to the local state data directory.\n\nIdeally I'd like to generally keep my data somewhere known and safe so I can backup and recover. I'm less worried about losing the docker installs since those are already versionsed.\n\nThanks!",
      "state": "closed",
      "author": "ahundt",
      "author_type": "User",
      "created_at": "2025-05-07T06:16:36Z",
      "updated_at": "2025-05-11T21:52:22Z",
      "closed_at": "2025-05-11T21:35:46Z",
      "labels": [
        "documentation",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/161/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/161",
      "api_url": "https://api.github.com/repos/av/harbor/issues/161",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:10.199297",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for trying out Harbor,\n\nInstalling from PyPi or NPM is mostly for automated scenarios:\n```bash\nuvx install llm-harbor\ncp custom_profile.env $(harbor home)/profiles/custom_profile.env\nharbor profile use custom_profile\n```\n\nThis installation configuration will be lost during an upgrade (due",
          "created_at": "2025-05-10T10:40:13Z"
        },
        {
          "author": "ahundt",
          "body": "Thanks, I've been able to migrate!",
          "created_at": "2025-05-11T21:35:36Z"
        },
        {
          "author": "ahundt",
          "body": "Although, one thought is it might be a good idea to note the risk involved with the package manager installs in the setup instructions, and why the manual clone and the one liner are suggested, could be worth reopening this until that's added.\n\nWhatever the case, thanks for the awesome tool!",
          "created_at": "2025-05-11T21:51:43Z"
        }
      ]
    },
    {
      "issue_number": 40,
      "title": "Docker for HarborCLI",
      "body": "been testing this project on my desktop and its been great! wonderful job.\r\n\r\nI was wanting to know if its possible to have the HarborCLI in a docker environment, would love to have this as a all in one replacement for my current server setup.\r\n\r\nI looked through the wiki and couldn't find anything\r\n\r\nthanks for the wonderful work.",
      "state": "open",
      "author": "Steel-skull",
      "author_type": "User",
      "created_at": "2024-09-28T21:39:47Z",
      "updated_at": "2025-05-10T10:48:59Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/40/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/40",
      "api_url": "https://api.github.com/repos/av/harbor/issues/40",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:10.391387",
      "comments": [
        {
          "author": "av",
          "body": "Thank you for trying out Harbor and for the kind words!\r\n\r\nIndeed, Harbor itself is not available as a dockerized CLI. However, It should be possible to dockerize it:\r\n- Mount `harbor home` as a workspace\r\n- Expose access to docker socket\r\n- ? Docker Compose CLI\r\n- ? Commands working with global cac",
          "created_at": "2024-09-28T22:33:13Z"
        },
        {
          "author": "av",
          "body": "[v0.3.0](https://github.com/av/harbor/releases/tag/v0.3.0) starts making first steps in this direction. We'll start to slowly translating the CLI over to Deno which happens to run dockerized. In the final target state most of the code will run within the container except the small portion on the hos",
          "created_at": "2025-03-01T23:09:02Z"
        },
        {
          "author": "Steel-skull",
          "body": "awesome to hear! ill keep tabs as yall progress!\n",
          "created_at": "2025-03-24T12:25:49Z"
        },
        {
          "author": "av",
          "body": "#161 is related, dockerizing Harbor CLI requires ability to preconfigure install/symlink location",
          "created_at": "2025-05-10T10:48:58Z"
        }
      ]
    },
    {
      "issue_number": 38,
      "title": "BionicGPT configuration for ollama backend",
      "body": "In the documentation the Bionic GPT it is mentioned that it works with ollama and OpenAPI compatible backends and it is demonstrated running a local gemma model. I could not find information on how to properly configure the settings of the Bionic GPT frontend, only thing I could find was the official documentation ([https://bionic-gpt.com/docs/running-locally/ollama/](https://bionic-gpt.com/docs/running-locally/ollama/)). I tried to follow the steps by adding a model with name listed by `ollama list`, domain from `harbor url ollama` and set api key to `olllama`. Then I added an assistant with this LLM as a backend. When I submit a message into the chat I am getting `connection refused` error:\r\n```\r\nTransport(\r\n    reqwest::Error {\r\n        kind: Request,\r\n        url: Url {\r\n            scheme: \"http\",\r\n            cannot_be_a_base: false,\r\n            username: \"\",\r\n            password: None,\r\n            host: Some(\r\n                Domain(\r\n                    \"localhost\",\r\n                ),\r\n            ),\r\n            port: Some(\r\n                33821,\r\n            ),\r\n            path: \"/chat/completions\",\r\n            query: None,\r\n            fragment: None,\r\n        },\r\n        source: Error {\r\n            kind: Connect,\r\n            source: Some(\r\n                ConnectError(\r\n                    \"tcp connect error\",\r\n                    Os {\r\n                        code: 111,\r\n                        kind: ConnectionRefused,\r\n                        message: \"Connection refused\",\r\n                    },\r\n                ),\r\n            ),\r\n        },\r\n    },\r\n)\r\n```\r\nand `harbor logs bionicgpt` outputs:\r\n```\r\nWARN[0000] The \"HARBOR_WHISPER_VERSION\" variable is not set. Defaulting to a blank string. \r\nWARN[0000] The \"HARBOR_WHISPER_HOST_PORT\" variable is not set. Defaulting to a blank string. \r\nharbor.bionicgpt  |   - name: base\r\nharbor.bionicgpt  |     static_layer:\r\nharbor.bionicgpt  |       {}\r\nharbor.bionicgpt  |   - name: admin\r\nharbor.bionicgpt  |     admin_layer:\r\nharbor.bionicgpt  |       {}\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.326][11][info][config] [source/server/configuration_impl.cc:125] loading tracing configuration\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.327][11][info][config] [source/server/configuration_impl.cc:85] loading 0 static secret(s)\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.327][11][info][config] [source/server/configuration_impl.cc:91] loading 2 cluster(s)\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.328][11][info][config] [source/server/configuration_impl.cc:95] loading 1 listener(s)\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.333][11][warning][misc] [source/common/protobuf/message_validator_impl.cc:21] Deprecated field: type envoy.extensions.filters.http.ext_authz.v3.ExtAuthz Using the default now-deprecated value AUTO for enum 'envoy.extensions.filters.http.ext_authz.v3.ExtAuthz.transport_api_version' from file ext_authz.proto. This enum value will be removed from Envoy soon so a non-default value must now be explicitly set. Please see https://www.envoyproxy.io/docs/envoy/latest/version_history/version_history for details. If continued use of this field is absolutely necessary, see https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/runtime#using-runtime-overrides-for-deprecated-features for how to apply a temporary and highly discouraged override.\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.333][11][info][lua] [source/extensions/filters/http/lua/lua_filter.cc:170] envoy_on_request() function not found. Lua filter will not hook requests.\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.333][11][info][lua] [source/extensions/filters/http/lua/lua_filter.cc:170] envoy_on_request() function not found. Lua filter will not hook requests.\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.334][11][info][config] [source/server/configuration_impl.cc:107] loading stats configuration\r\nharbor.bionicgpt  | [2024-09-28 17:31:17.334][11][info][main] [source/server/server.cc:732] starting main dispatch loop\r\nharbor.bionicgpt  | [2024-09-28 17:31:20.795][11][info][runtime] [source/common/runtime/runtime_impl.cc:425] RTDS has finished initialization\r\nharbor.bionicgpt  | [2024-09-28 17:31:20.795][11][info][upstream] [source/common/upstream/cluster_manager_impl.cc:191] cm init: all clusters initialized\r\nharbor.bionicgpt  | [2024-09-28 17:31:20.795][11][info][main] [source/server/server.cc:713] all clusters initialized. initializing init manager\r\nharbor.bionicgpt  | [2024-09-28 17:31:20.795][11][info][config] [source/server/listener_manager_impl.cc:888] all dependencies initialized. starting workers\r\nharbor.bionicgpt  | [2024-09-28 17:31:20.796][11][warning][main] [source/server/server.cc:610] there is no configured limit to the number of allowed active connections. Set a limit via the runtime key overload.global_downstream_max_connections\r\n```",
      "state": "open",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-09-28T17:35:19Z",
      "updated_at": "2025-05-10T10:48:11Z",
      "closed_at": null,
      "labels": [
        "documentation",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/38/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/38",
      "api_url": "https://api.github.com/repos/av/harbor/issues/38",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:10.599962",
      "comments": [
        {
          "author": "av",
          "body": "This was one of the most, if not the most complicated services to setup ðŸ˜…\r\n\r\nThere's a fully-fledged reverse proxy and lots of sub-services requiring carefull routing. All-in-all, I wouldn't be surprising if that setup stopped working completely, just out of the drift with upstream docker images and",
          "created_at": "2024-09-28T22:55:37Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank you again, I was able to get the chat working by setting the domain of model to `http://harbor.ollama:11434/v1` which is the output of `url -i ollama` with added `/v1`, without it I keep getting the following error:\r\n```\r\nInvalidStatusCode(\r\n    404,\r\n    Response {\r\n        url: Url {\r\n      ",
          "created_at": "2024-09-29T19:48:18Z"
        }
      ]
    },
    {
      "issue_number": 36,
      "title": "Librechat RAG errors",
      "body": "When I tried to submit a document to Librechat, I encountered error. `harbor logs librechat` gives the following output:\r\n```\r\nWARN[0000] The \"HARBOR_WHISPER_VERSION\" variable is not set. Defaulting to a blank string. \r\nWARN[0000] The \"HARBOR_WHISPER_HOST_PORT\" variable is not set. Defaulting to a blank string. \r\nharbor.librechat  | }\r\nharbor.librechat  | 2024-09-28 17:21:06 info: [deleteNullOrEmptyConversations] Deleted 0 conversations and 0 messages\r\nharbor.librechat  | 2024-09-28 17:21:11 warn: RAG API is either not running or not reachable at http://lc-rag:33892, you may experience errors with file uploads.\r\nharbor.librechat  | 2024-09-28 17:21:11 info: No changes needed for 'USER' role permissions\r\nharbor.librechat  | 2024-09-28 17:21:11 info: No changes needed for 'ADMIN' role permissions\r\nharbor.librechat  | 2024-09-28 17:21:11 info: \r\nharbor.librechat  | Outdated Config version: 1.1.5\r\nharbor.librechat  | Latest version: 1.1.7\r\nharbor.librechat  | \r\nharbor.librechat  |       Check out the Config changelogs for the latest options and features added.\r\nharbor.librechat  | \r\nharbor.librechat  |       https://www.librechat.ai/changelog\r\nharbor.librechat  | \r\nharbor.librechat  | \r\nharbor.librechat  | 2024-09-28 17:21:11 info: Server listening on all interfaces at port 33891. Use http://localhost:33891 to access it\r\nharbor.librechat  | 2024-09-28 17:21:50 error: Failed to fetch models from SGLang API\r\nharbor.librechat  | The request either timed out or was unsuccessful. Error message:\r\nharbor.librechat  |  Cannot read properties of undefined (reading 'status')\r\nharbor.librechat  | 2024-09-28 17:21:59 error: Error embedding file Cannot read properties of undefined (reading 'status')\r\nharbor.librechat  | 2024-09-28 17:21:59 error: [/files] Error processing file: Cannot read properties of undefined (reading 'status')\r\n```\r\nand looking into logs of the RAG container with  I am getting:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/app/main.py\", line 46, in <module>\r\n    from psql import PSQLDatabase, ensure_custom_id_index_on_embedding, pg_health_check\r\n  File \"/app/psql.py\", line 3, in <module>\r\n    from config import DSN, logger\r\n  File \"/app/config.py\", line 5, in <module>\r\n    import boto3\r\nModuleNotFoundError: No module named 'boto3'\r\n```\r\nThis problem persists even after manual removal of containers and images and complete \"reinstall\" of Librechat using `harbor`. The problem seams to be caused by the RAG container, but I could not find if it is caused by the upstream image or configuration.\r\n\r\n-------------\r\nI am running harbor version 1.35, on Bluefin DX (Fedora Silverblue 40).",
      "state": "closed",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-09-28T17:24:09Z",
      "updated_at": "2025-05-10T10:47:47Z",
      "closed_at": "2025-05-10T10:47:46Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/36/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/36",
      "api_url": "https://api.github.com/repos/av/harbor/issues/36",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:10.803106",
      "comments": [
        {
          "author": "pesschap",
          "body": "I'm experiencing this issue as well.",
          "created_at": "2025-02-16T11:35:26Z"
        },
        {
          "author": "pesschap",
          "body": "Solved this issue by changing RAG provider to OpenAI as I am running it on a VPS and Ollama is not an option.",
          "created_at": "2025-02-17T13:03:08Z"
        },
        {
          "author": "av",
          "body": "Closing as a part of cleanup, please feel free to open a new issue if needed",
          "created_at": "2025-05-10T10:47:46Z"
        }
      ]
    },
    {
      "issue_number": 160,
      "title": "Local deep research always fails",
      "body": "No matter what search engine I use, I always get the following error:\n\n```\nERROR:local_deep_research.advanced_search_system.filters.cross_engine_filter:Cross-engine filtering error: '<' not supported between instances of 'int' and'str'\nERROR:local_deep_research.advanced_search_system.strategies.source_based_strategy:Error in research process: '<' not supported between instances of 'int' and 'str'\nERROR:local_deep_research.advanced_search_system.strategies.source_based_strategy:Traceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/local_deep_research/advanced_search_system/filters/cross_engine_filter.py\", line 174, in filter_results\n    max_filtered = min(self.max_results, len(ranked_results))\nTypeError: '<' not supported between instances of 'int' and 'str'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/local_deep_research/advanced_search_system/strategies/source_based_strategy.py\", line 310, in analyze_topic\n    final_filtered_results = self.cross_engine_filter.filter_results(\n        accumulated_search_results_across_all_iterations,\n    ...<4 lines>...\n        start_index=len(self.all_links_of_system),\n    )\n  File \"/usr/local/lib/python3.13/site-packages/local_deep_research/advanced_search_system/filters/cross_engine_filter.py\", line 199, in filter_results\n    top_results = results[: min(self.max_results, len(results))]\n                            ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: '<' not supported between instances of 'int' and 'str'\n\nWARNING:local_deep_research.web.services.research_service:Detected error in formatted findings: Error: '<' not supported between instances of 'int' and 'str'... stack trace: NoneType: None\n\nWARNING:local_deep_research.web.services.research_service:Detected unknown error in synthesis\n```\n\nThis does not happen when I run LDR via its origin repository using their docker compose file.",
      "state": "open",
      "author": "genevera",
      "author_type": "User",
      "created_at": "2025-05-04T03:32:38Z",
      "updated_at": "2025-05-10T09:57:07Z",
      "closed_at": null,
      "labels": [
        "bug",
        "upstream"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/160/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/160",
      "api_url": "https://api.github.com/repos/av/harbor/issues/160",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:11.014096",
      "comments": [
        {
          "author": "av",
          "body": "There are multiple things:\n\n- Tested with `v0.2.0` (the one initially introduced to Harbor - worked as expected\n- Rebuilding from scratch (`harbor build ldr --no-cache`) - switches to `v0.3.6`, it started to fail with a different error compared to what you reported: `ValueError: max_workers must be ",
          "created_at": "2025-05-10T09:54:59Z"
        }
      ]
    },
    {
      "issue_number": 157,
      "title": "Litellm Documentation",
      "body": "I see that the litellm service merges configuration files, but I haven't been able to configure it myself. Is there a chance we could get an example of a configuration in the documentation or the commented out in the default config.yaml?\n\nThanks!",
      "state": "open",
      "author": "DIGist",
      "author_type": "User",
      "created_at": "2025-04-29T01:16:55Z",
      "updated_at": "2025-05-10T08:42:07Z",
      "closed_at": null,
      "labels": [
        "documentation",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/157/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/157",
      "api_url": "https://api.github.com/repos/av/harbor/issues/157",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:11.174071",
      "comments": [
        {
          "author": "av",
          "body": "Hi! There's a [small example](https://github.com/av/harbor/wiki/2.3.5-Satellite:-LiteLLM#configuration) in the wiki, just added a few more tips there\n\n[Official reference](https://docs.litellm.ai/docs/proxy/configs) is the best place to see possible configuration values\n\nIs there anything specific t",
          "created_at": "2025-05-10T08:42:00Z"
        }
      ]
    },
    {
      "issue_number": 23,
      "title": "Integration: Agent Zero",
      "body": "Hey this is quite cool project and quite active. Let me know if this will be feasible\r\n\r\n[https://github.com/frdel/agent-zero](https://github.com/frdel/agent-zero)",
      "state": "closed",
      "author": "bhupesh-sf",
      "author_type": "User",
      "created_at": "2024-09-18T17:57:04Z",
      "updated_at": "2025-04-26T15:43:25Z",
      "closed_at": "2025-04-26T10:48:17Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/23/reactions",
        "total_count": 3,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/23",
      "api_url": "https://api.github.com/repos/av/harbor/issues/23",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:11.358501",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for bringing it to my attention, it should be doable from the first glance, I'll be able to understand better once started with the integration",
          "created_at": "2024-09-18T21:38:54Z"
        },
        {
          "author": "av",
          "body": "I did a few attempts since this issue opened - all unsuccessful, the tool really wants to be running on the host and launching containers by itself",
          "created_at": "2024-10-11T07:04:00Z"
        },
        {
          "author": "bhupesh-sf",
          "body": "yeah, agreed because that is by design. not sure what can be done in this case",
          "created_at": "2024-10-11T16:22:21Z"
        },
        {
          "author": "av",
          "body": "Been a while!\nJust revisited Agent Zero - they have proper docker support since v0.8.0, added to [Harbor v0.3.9](https://github.com/av/harbor/releases/tag/v0.3.9)",
          "created_at": "2025-04-26T10:48:17Z"
        },
        {
          "author": "bhupesh-sf",
          "body": "Thanks a lot",
          "created_at": "2025-04-26T15:43:25Z"
        }
      ]
    },
    {
      "issue_number": 69,
      "title": "Fabric installation error",
      "body": "Hi,\r\nTrying to install Fabric, this is the error:\r\n\r\n![image](https://github.com/user-attachments/assets/c5fc0c12-7284-41f7-a56a-0b3b5876bdef)\r\n\r\nWhat did I miss?\r\nThank you",
      "state": "closed",
      "author": "PieBru",
      "author_type": "User",
      "created_at": "2024-10-29T10:18:13Z",
      "updated_at": "2025-04-26T10:56:22Z",
      "closed_at": "2025-04-26T10:56:21Z",
      "labels": [
        "documentation",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/69/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/69",
      "api_url": "https://api.github.com/repos/av/harbor/issues/69",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:11.570949",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for the report!\r\n\r\n`fabric` is a CLI service (one of the oldest integrated ones as well) - so you don't need to `up` it (there's no web-service in the background). It's mentioned in the [docs](https://github.com/av/harbor/wiki/2.3.10-Satellite:-fabric#starting), but I guess the whole situatio",
          "created_at": "2024-11-02T13:51:18Z"
        },
        {
          "author": "av",
          "body": "I hope my comment above clarified the issue, feel free to reopen or create a new one if needed",
          "created_at": "2025-04-26T10:56:21Z"
        }
      ]
    },
    {
      "issue_number": 156,
      "title": "MetaMCP fails to build",
      "body": "When trying to start MetaMCP through Harbor, the build process fails due to version mismatch.\n\n```\nroot@harbor:~# harbor up \nCompose can now delegate builds to bake for better performance.\n To do so, set COMPOSE_BAKE=true.\n[+] Building 0.9s (8/8) FINISHED                                                                                                                                       docker:default\n => CACHED [metamcp internal] load git source https://github.com/metatool-ai/metatool-app.git                                                                                    0.7s\n => [metamcp-sse internal] load build definition from Dockerfile                                                                                                                 0.0s\n => => transferring dockerfile: 148B                                                                                                                                             0.0s\n => [metamcp-sse internal] load metadata for ghcr.io/av/tools:latest                                                                                                             0.0s\n => [metamcp-sse internal] load .dockerignore                                                                                                                                    0.0s\n => => transferring context: 2B                                                                                                                                                  0.0s\n => [metamcp-sse 1/3] FROM ghcr.io/av/tools:latest                                                                                                                               0.0s\n => CACHED [metamcp-sse 2/3] RUN echo '{ \"dependencies\": { \"pg\": \"^18.0.0\" } }' > package.json                                                                                   0.0s\n => ERROR [metamcp-sse 3/3] RUN npm install                                                                                                                                      0.8s\n => CANCELED [metamcp internal] load metadata for docker.io/library/node:20-alpine                                                                                               0.2s\n------                                                                                                                                                                                \n > [metamcp-sse 3/3] RUN npm install:                                                                                                                                                 \n0.674 npm error code ETARGET\n0.674 npm error notarget No matching version found for pg@^18.0.0.\n0.674 npm error notarget In most cases you or one of your dependencies are requesting\n0.674 npm error notarget a package version that doesn't exist.\n0.675 npm error A complete log of this run can be found in: /app/cache/npm/_logs/2025-04-25T17_25_30_916Z-debug-0.log\n------\nfailed to solve: process \"/bin/sh -c npm install\" did not complete successfully: exit code: 1\n``` ",
      "state": "open",
      "author": "bannert1337",
      "author_type": "User",
      "created_at": "2025-04-25T17:26:47Z",
      "updated_at": "2025-04-26T10:55:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/156/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/156",
      "api_url": "https://api.github.com/repos/av/harbor/issues/156",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:11.762539",
      "comments": [
        {
          "author": "av",
          "body": "There was a similar issue in the original service, with versions find-replace in package-lock.json\nhttps://github.com/metatool-ai/mcp-server-metamcp/issues/9\n\nMaybe it's that again?\nOne way to workaround is to build from a specific git ref from one of the previous MetaMCP releases to see if it works",
          "created_at": "2025-04-25T17:38:40Z"
        },
        {
          "author": "av",
          "body": "Ok, got to debug this. \n\nFirst of all - sorry, this specific bug was actually on Harbor's end in a custom service that exposes MetaMCP as SSE-capable MCP server. I did a very obvious mistake there and used pg version `18` which doesn't exists, it was obscured by the fact I had a correct local image ",
          "created_at": "2025-04-26T08:03:55Z"
        },
        {
          "author": "av",
          "body": "Fixes were released in v0.3.9\n\nNote that you still need to:\n```bash\nharbor config set metamcp.git_ref https://github.com/metatool-ai/metatool-app.git#v0.4.3\n```\nafter update (or reset the whole config)",
          "created_at": "2025-04-26T10:55:31Z"
        }
      ]
    },
    {
      "issue_number": 49,
      "title": "command wrappers",
      "body": "I made this little script:\r\n\r\n```\r\n$ cat harbor-cmd-alias\r\n#!/bin/bash\r\n\r\nPROGNAME=$(basename \"$0\")\r\n\r\nexec harbor \"$PROGNAME\" \"$@\"\r\n```\r\n\r\nand in `~/.local/bin/`, I have the following symlinks:\r\n\r\n```\r\naichat->harbor-cmd-alias\r\naider->harbor-cmd-alias\r\nbench->harbor-cmd-alias\r\ncmdh->harbor-cmd-alias\r\nfabric->harbor-cmd-alias\r\nharbor-cmd-alias\r\nhf->harbor-cmd-alias\r\nollama->harbor-cmd-alias\r\nopenhands->harbor-cmd-alias\r\nopint->harbor-cmd-alias\r\nparllama->harbor-cmd-alias\r\nplandex->harbor-cmd-alias\r\n```\r\n\r\nWhich seems to work just fine.  Would suggest something similar as a built-in feature, with automatic symlink updates.",
      "state": "open",
      "author": "lee-b",
      "author_type": "User",
      "created_at": "2024-09-30T13:08:49Z",
      "updated_at": "2025-04-26T10:54:26Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/49/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/49",
      "api_url": "https://api.github.com/repos/av/harbor/issues/49",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:11.980796",
      "comments": [
        {
          "author": "av",
          "body": "This is a good idea and is something I've been considering to add on the opt-in basis, in order to avoid messing up with any of the binaries the user might have installed natively",
          "created_at": "2024-09-30T13:37:41Z"
        }
      ]
    },
    {
      "issue_number": 41,
      "title": "DeepLiveCam",
      "body": "Requested on Reddit:\r\nhttps://www.reddit.com/r/LocalLLaMA/s/ErrpBnD8YW\r\n\r\nProject:\r\nhttps://github.com/hacksider/Deep-Live-Cam\r\n\r\nPossible implementation path:\r\nhttps://github.com/hacksider/Deep-Live-Cam/issues/208#issuecomment-2285872895",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-09-29T14:06:39Z",
      "updated_at": "2025-04-26T10:50:10Z",
      "closed_at": "2025-04-26T10:50:09Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/41/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/41",
      "api_url": "https://api.github.com/repos/av/harbor/issues/41",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:12.187052",
      "comments": [
        {
          "author": "av",
          "body": "Docker support is very unlikely, so closing this",
          "created_at": "2025-04-26T10:50:09Z"
        }
      ]
    },
    {
      "issue_number": 81,
      "title": "Add Perplexideez",
      "body": "https://github.com/brunostjohn/perplexideez\r\n\r\nA multi-user perplexity clone that supports both ollama and docker. Thought it might be interesting.",
      "state": "closed",
      "author": "nullnuller",
      "author_type": "User",
      "created_at": "2024-11-16T00:26:25Z",
      "updated_at": "2025-04-26T09:50:42Z",
      "closed_at": "2025-04-26T09:50:41Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/81/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/81",
      "api_url": "https://api.github.com/repos/av/harbor/issues/81",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:12.416389",
      "comments": [
        {
          "author": "av",
          "body": "Yup, thanks! I've seen it on Reddit and also noted it down for possible integration ",
          "created_at": "2024-11-16T00:28:14Z"
        },
        {
          "author": "av",
          "body": "I tried, but [failed](https://github.com/av/harbor/blob/main/compose.perplexideez.yml). We'll have to wait for some changes on the upstream to make it run on `localhost` without issues",
          "created_at": "2024-11-17T18:45:22Z"
        },
        {
          "author": "av",
          "body": "It looks like Perplexideez development completely stopped 5 months ago, I'm open to contributions with this service, but won't work on it myself - hence closing the issue",
          "created_at": "2025-04-26T09:50:41Z"
        }
      ]
    },
    {
      "issue_number": 80,
      "title": "Add SurfSense ?",
      "body": "A Personal NotebookLM and Perplexity-like AI Assistant for Everyone. Research and Never forget anything.\r\n[Github](https://github.com/MODSetter/SurfSense)\r\nKey Features\r\nðŸ’¡ Idea: Have your own private NotebookLM and Perplexity with better integrations.\r\nâš™ï¸ Cross Browser Extension: Save your dynamic content bookmarks from your favourite browser.\r\nðŸ“ Multiple File Format Uploading Support: Save content from your own personal files(Documents, images and more) to your own personal knowledge base .\r\nðŸ” Powerful Search: Quickly research or find anything in your saved content.\r\nðŸ’¬ Chat with your Saved Content: Interact in Natural Language with your saved Web Browsing Sessions and get cited answers.\r\nðŸŽ¤ Podcasts your Saved Content: Create podcasts over your saved content in SurfSense knowledge base.\r\nðŸ“„ Cited Answers: Get Cited answers just like Perplexity.\r\nðŸ”” Local LLM Support: Works Flawlessly with Ollama local LLMs.\r\nðŸ  Self Hostable: Open source and easy to deploy locally.\r\nðŸ“Š Advanced RAG Techniques: Utilize the power of Hierarchical Indices RAG.\r\nðŸ”Ÿ% Cheap On Wallet: Works Flawlessly with OpenAI gpt-4o-mini model and Ollama local LLMs.\r\nðŸ•¸ï¸ No WebScraping: Extension directly reads the data from DOM to get accurate data.",
      "state": "open",
      "author": "nullnuller",
      "author_type": "User",
      "created_at": "2024-11-11T12:34:09Z",
      "updated_at": "2025-04-25T17:07:14Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/80/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/80",
      "api_url": "https://api.github.com/repos/av/harbor/issues/80",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:13.631864",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for the pointer!\r\n\r\nWill be much easier to integrate once they start publishing official Docker images somewhere",
          "created_at": "2024-11-11T18:35:52Z"
        },
        {
          "author": "bannert1337",
          "body": "They added Docker images:\nhttps://www.surfsense.net/docs/docker-installation",
          "created_at": "2025-04-25T16:59:48Z"
        },
        {
          "author": "av",
          "body": "Very nice, thanks for pinging me!",
          "created_at": "2025-04-25T17:07:12Z"
        }
      ]
    },
    {
      "issue_number": 155,
      "title": "New App req -- WilmerAI",
      "body": "Key Features:\nPrompt Routing: Allows prompts to be directed to specialized workflows based on the domain or task.\n\nMulti-Model Collaboration: Supports combining responses from multiple LLMs to generate a single, cohesive output.\n\nCustom Workflows: Extendable with Python scripts for tailored functionality.\n\n**Compatibility: Exposes OpenAI and Ollama-compatible endpoints, making it connectable to most tools and front ends.**\n\nMemory Management: Tracks conversations across hundreds of thousands of tokens while maintaining high-level context.\n\n\nSo effectively it could act as a configurable and extensible proxy for requests from any LLM tool (that uses ollama or openai custom endpoints), perform additional modification or logic and route it to their api of choice.\nSolving the problem of worrying about which tools talk to which api (ollama vs openai) and providing powerful workflows for those that want to tackle more.\n\nhttps://github.com/SomeOddCodeGuy/WilmerAI",
      "state": "open",
      "author": "DIGist",
      "author_type": "User",
      "created_at": "2025-04-21T06:23:10Z",
      "updated_at": "2025-04-24T20:50:34Z",
      "closed_at": null,
      "labels": [
        "help wanted",
        "new service"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/155/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/155",
      "api_url": "https://api.github.com/repos/av/harbor/issues/155",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:13.838956",
      "comments": [
        {
          "author": "av",
          "body": "Thank for the suggestion, I know about Wilmer and read a lot of posts/comments from it's author on r/LocalLLaMA \n\nTo be added to Harbor, a project should have a way to be run in Docker as well as maintained images published somewhere. Unfortunately it's not the case with Wilmer as of now, integratio",
          "created_at": "2025-04-21T06:49:38Z"
        },
        {
          "author": "DIGist",
          "body": "Oh, Apologies, I wasn't aware of Harbor boost. Thanks! Does it currently have the ability to provide an ollama endpoint and relay that to another endpoint or are there any plans to do so in the future for tools that haven't adopted the openai api standard yet and only talk to ollama?",
          "created_at": "2025-04-24T20:36:18Z"
        },
        {
          "author": "av",
          "body": "No worries, thanks for taking the time to learn about the project!\n\nProxying for Ollama endpoints is out of scope for Boost. That said - it should be relatively straightforward to query ollama endpoints from within custom modules wirh included LiteLLM dependency, but it'll be a big technical challen",
          "created_at": "2025-04-24T20:50:34Z"
        }
      ]
    },
    {
      "issue_number": 153,
      "title": "suggestions: Researchers, LLM-based Database frontend",
      "body": "my first suggestions but i think these would leverage harbor's concept of integrating tools\n\n\n[weaviate](https://weaviate.io/blog/hybrid-search-explained) or QDRANT for standalone hybrid/reranked RAG(they integrate embedding/rerank LLM models on the database itself so you don't need to rely on your app to support SOTA RAG techniques)\n\n\n[vectoradmin](https://vectoradmin.com) db frontend allows one to interact with Vector Databases directly \n\nresearch tools\nhttps://github.com/mshumer/OpenDeepResearcher\n\nhttps://github.com/LearningCircuit/local-deep-research",
      "state": "closed",
      "author": "VinnyG9",
      "author_type": "User",
      "created_at": "2025-03-30T05:48:03Z",
      "updated_at": "2025-04-19T13:56:57Z",
      "closed_at": "2025-04-19T13:56:49Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/153/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/153",
      "api_url": "https://api.github.com/repos/av/harbor/issues/153",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:14.052027",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for the suggestions!\n\n- https://github.com/LearningCircuit/local-deep-research was added in [v0.3.8](https://github.com/av/harbor/releases/tag/v0.3.8)\n- [vectoradmin](https://vectoradmin.com/) looks to be on pause\n- we have [qdrant](https://github.com/av/harbor/wiki/2.3.26-Satellite:-Qdrant) ",
          "created_at": "2025-04-19T13:56:49Z"
        }
      ]
    },
    {
      "issue_number": 64,
      "title": "Harbor App (Appimage) cannot load config",
      "body": "After making a clean install of `harbor` CLI (`harbor doctor` shows everything OK), I tried the Harbor App via AppImage. No AppImage wrapper was used, only `chmod +x HarborApp.AppImage` and `./HarborApp.AppImage`.\r\nThe problem is only the `Profiles` tab which outputs an error \"Unexpected error loading configuration\".\r\n![Screenshot from 2024-10-23 14-21-22](https://github.com/user-attachments/assets/46bfeb55-4bf1-4078-924b-04da02f31338)\r\nOther tabs are unaffected.",
      "state": "open",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-10-23T13:45:53Z",
      "updated_at": "2025-04-06T22:28:51Z",
      "closed_at": null,
      "labels": [
        "bug",
        "OS:Linux",
        "App"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 18,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/64/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/64",
      "api_url": "https://api.github.com/repos/av/harbor/issues/64",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:14.247300",
      "comments": [
        {
          "author": "av",
          "body": "Thank you so much for another report!\r\n\r\nUnfortunately I can't reproduce it right away, so we'll have to debug\r\n\r\nJust released v0.2.12 comes with a couple of things that should help:\r\n- `harbor doctor` now prints location of the Harbor Home, it should be present in the App in the \"CLI\" page, could ",
          "created_at": "2024-10-27T11:49:36Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "As before thank you for quick update of `harbor`, however the problem persists. I followed the troubleshooting you have suggested and here are the results.\r\n\r\nHarbor App doctor (the values match between App and CLI):\r\nâœ” Docker is installed and running\r\nâœ” Docker Compose (v2) is installed\r\nâœ” Harbor ho",
          "created_at": "2024-10-27T14:19:25Z"
        },
        {
          "author": "av",
          "body": "Yeah, nothing in these looks suspicious in any way. The `--no-sandbox` seems to be related to the Electron apps (Harbor runs with Tauri). Granted the homedir location, could you please try running it with: `--filesystem=/var`? Very thin chance, but still\r\n\r\nI'm certain that this happens because of s",
          "created_at": "2024-10-27T15:43:05Z"
        },
        {
          "author": "av",
          "body": "I've added a \"guess\" fix to https://github.com/av/harbor/releases/tag/v0.2.14 with an explicit permission to read/write to the User's home folder (previously was under the wildcard). Also a thin chance that it fixes the problem for you, but please try running the AppImage from that release",
          "created_at": "2024-10-27T16:06:09Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank you for incredibly fast update, I updated CLI and the App. Nevertheless, the problem remains the same in version 0.2.14 with or without `--filesystem=/var` flag.\r\nI noticed that the button create profile is clickable despite the error. The profile creation dialog opens and upon clicking create",
          "created_at": "2024-10-27T20:55:19Z"
        }
      ]
    },
    {
      "issue_number": 154,
      "title": "Suggestion: add Clara",
      "body": "Add Clara to the list\nhttps://github.com/badboysm890/ClaraVerse",
      "state": "open",
      "author": "asmoura",
      "author_type": "User",
      "created_at": "2025-04-01T07:13:39Z",
      "updated_at": "2025-04-01T07:40:42Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/154/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/154",
      "api_url": "https://api.github.com/repos/av/harbor/issues/154",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:14.474056",
      "comments": []
    },
    {
      "issue_number": 139,
      "title": "Suggestion: RAGflow",
      "body": "It would fit well with the stack. It is a fairly complete RAG solution. https://github.com/infiniflow/ragflow",
      "state": "open",
      "author": "freddyholms",
      "author_type": "User",
      "created_at": "2025-03-03T06:59:17Z",
      "updated_at": "2025-03-30T13:39:11Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/139/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/139",
      "api_url": "https://api.github.com/repos/av/harbor/issues/139",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:14.474077",
      "comments": [
        {
          "author": "VinnyG9",
          "body": "yes, ragflow and kotaemon are both brilliant ",
          "created_at": "2025-03-30T05:37:58Z"
        },
        {
          "author": "VinnyG9",
          "body": "also check these out\n\nhttps://github.com/av/harbor/issues/153",
          "created_at": "2025-03-30T13:39:11Z"
        }
      ]
    },
    {
      "issue_number": 146,
      "title": "docker Volumes as HARBOR_HF_CACHE",
      "body": "Hi,\nIs there a recommended way to replace the default HARBOR_HF_CACHE path (currently ~/.cache/huggingface) with a docker Volume? I tried modifying one of compose files and received an error `__harbor.yml: services.volumes Additional property models_folder is not allowed.`  (models_folder being the docker Volume where i keep my downloaded models). What i'm trying to accomplish is heaving a single models folder as a volume that could be made available to all the other containers which might need it. This way we can avoid multiple copies of the same model. Thank you.",
      "state": "closed",
      "author": "mpetruc",
      "author_type": "User",
      "created_at": "2025-03-17T02:13:46Z",
      "updated_at": "2025-03-29T11:12:22Z",
      "closed_at": "2025-03-29T11:12:21Z",
      "labels": [
        "enhancement",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/146/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/146",
      "api_url": "https://api.github.com/repos/av/harbor/issues/146",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:14.726724",
      "comments": [
        {
          "author": "av",
          "body": "Hi, you can modify this and any other config values via [harbor config](https://github.com/av/harbor/wiki/1.-Harbor-User-Guide#harbor-config):\n\n```bash\nharbor config set hf.cache ~/path/to/my/cache\n```\n\nSetting this to a volume is not currently supported, unless you're ready to also modify the compo",
          "created_at": "2025-03-22T08:17:44Z"
        },
        {
          "author": "av",
          "body": "I hope my reply above covers the question, please feel free to open another issue if needed",
          "created_at": "2025-03-29T11:12:21Z"
        }
      ]
    },
    {
      "issue_number": 147,
      "title": "Aphrodite error",
      "body": "`File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 43, in import_protobuf\n    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))\nImportError: \nThe new behaviour of LlamaTokenizer (with `self.legacy = False`) requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.`\n\nis this related to harbor somehow? or should rather be reported on aphrodite? Thanks.",
      "state": "closed",
      "author": "mpetruc",
      "author_type": "User",
      "created_at": "2025-03-17T04:10:40Z",
      "updated_at": "2025-03-29T11:11:30Z",
      "closed_at": "2025-03-29T11:11:29Z",
      "labels": [
        "documentation",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/147/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/147",
      "api_url": "https://api.github.com/repos/av/harbor/issues/147",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:14.911578",
      "comments": [
        {
          "author": "av",
          "body": "Reproduced the same error as you, however, it seems to be fixed in the latest versions of the engine. You can switch with:\n\n```bash\n# Set aphrodite docker tag\nharbor config set aphrodite.version latest\n\n# Pull the set version\nharbor pull aphrodite\n```\n\nThat said - Aphrodite is one of the more niche ",
          "created_at": "2025-03-22T09:05:53Z"
        },
        {
          "author": "av",
          "body": "Closing since the error seems to be relate to Aphrodite, please feel free to open another one if needed",
          "created_at": "2025-03-29T11:11:29Z"
        }
      ]
    },
    {
      "issue_number": 151,
      "title": "Fabric install fail on Harbor v0.3.5: requires go version 1.23.4",
      "body": "I'm experiencing an issue installing the Harbor fabric component after upgrading to the latest Harbor release. The installation process fails with the following error message (or similar):\n\n```\n => ERROR [fabric 3/3] RUN go install github.com/danielmiessler/fabric@latest                                                  22.8s\n------\n > [fabric 3/3] RUN go install github.com/danielmiessler/fabric@latest:\n5.193 go: downloading github.com/danielmiessler/fabric v1.4.164\n22.54 go: github.com/danielmiessler/fabric@latest: github.com/danielmiessler/fabric@v1.4.164 requires go >= 1.23.4 (running go 1.23.1; GOTOOLCHAIN=local)\n------\nfailed to solve: executor failed running [/bin/sh -c go install github.com/danielmiessler/fabric@latest]: exit code: 1\n```\n\nThe error indicates that the fabric component requires Go version 1.23.4 or higher. \n\nI've traced the issue to the `~/.harbor/fabric/Dockerfile` file, which specifies an older Go version. I'm resolving this locally by modifying the FROM instruction in that file to use a more recent Go version (e.g. `FROM golang:1.23.5`).\n\n## Steps to Reproduce:\n\n-  Install/Upgrade to Harbor version 0.3.5.\n- Attempt to install the fabric component using the standard installation procedure.\n- Observe the error message indicating the Go version requirement.\n\n### Expected Behavior: \nThe fabric component should install successfully.\n### Actual Behavior: \nThe fabric component installation fails due to a Go version mismatch.\n### Workaround:\nI'm temporarily resolving the issue by updating the FROM instruction in `~/.harbor/fabric/Dockerfile`.\n### Environment:\n Harbor Version: `0.3.5`\n Operating System: Debian `12.10`\n Go Version: `1.23.1`\n Docker Version: `20.10.24+dfsg1`\n",
      "state": "closed",
      "author": "asmoura",
      "author_type": "User",
      "created_at": "2025-03-25T13:55:08Z",
      "updated_at": "2025-03-29T11:10:39Z",
      "closed_at": "2025-03-29T11:10:37Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/151/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/151",
      "api_url": "https://api.github.com/repos/av/harbor/issues/151",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:15.086911",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for giving it a try and for the report!\n\nWe prefer to use pre-built images from service maintainers where possible. Unfortunately it wasn't possible with `fabric`, so such shifts in dependencies happen from time to time.\n\n[Bumped Harbor's Dockerfile](https://github.com/av/harbor/commit/0573f0",
          "created_at": "2025-03-29T10:25:44Z"
        },
        {
          "author": "av",
          "body": "Fixed in [v0.3.6](https://github.com/av/harbor/releases/tag/v0.3.6)",
          "created_at": "2025-03-29T11:10:37Z"
        }
      ]
    },
    {
      "issue_number": 148,
      "title": "Ensuring Persistent Configurations & App Management in Harbor",
      "body": "I am really enjoying Harbor, and it seems like the way to go. However, I need to figure out a few things regarding configurations to ensure they persist, giving me peace of mind.\n\n1. Uninstalling Apps:\nIf I install an app for testing and later decide I don't need it, what is the correct way to remove it? For example, Harbor shows that Ollama is installed, but I have my own Ollama instance with multiple models that I want to connect. How can I remove the pre-installed Ollama? Perhaps you could add an option in the Harbor app to uninstall it, similar to how apps are installed. The same applies to any other app I test and later want to remove.\n\n2. Persistent Settings in Open WebUI:\nEvery time I restart Open WebUI, all my settings are reset. This is not just about API connectionsâ€”for instance, I use OpenRouter API and only need 5 to 7 models. However, when I add the API in the .env file, it displays all the models, and I need a way to filter them to show only the ones I need.\n\nAdditionally, I need my chat history to be transferred from Open WebUI (OWUI) to Harbor OWUI before I fully migrate. Many configurations Iâ€™ve set over time in the admin panel should also persist in Harbor OWUI rather than resetting after each restart.\n\n3. I have it running on localhost:33801 and for example markov works perfectly on  http://localhost:33801 however, when I configure it with Cloudflare with SSL and my own domain it does not render the artifact, it just open plank artifact while using https://somedomain.com\nI have open webui configured exactly the same but all artifacts work using domain name not localhost. what am I missing here? \n\n\nUsing http://localhost:33801/\n\n![Image](https://github.com/user-attachments/assets/3066711b-6e1a-449c-9dac-9dd72352dac8)\n\n\nUsing https://some-domain.com/\n\n![Image](https://github.com/user-attachments/assets/2560c601-3b68-44f3-acb1-9c5db2a6618f)\n\n\n\n\n",
      "state": "closed",
      "author": "Tobe2d",
      "author_type": "User",
      "created_at": "2025-03-18T16:06:08Z",
      "updated_at": "2025-03-23T13:38:45Z",
      "closed_at": "2025-03-23T13:38:44Z",
      "labels": [
        "enhancement",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/148/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/148",
      "api_url": "https://api.github.com/repos/av/harbor/issues/148",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:15.260802",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying our Harbor and for the kind words!\n\n## 1. Uninstalling Apps\n\na) Harbor works on top of Docker, so you can cleanup all things that are no longer in use with \n\n```bash\ndocker system prune\n```\n\nb) However, this will still keep your local caches in tact (including that of ollama, for e",
          "created_at": "2025-03-22T09:42:59Z"
        },
        {
          "author": "Tobe2d",
          "body": "Thanks for the detailed reply! it is much more clear now ;-)",
          "created_at": "2025-03-23T13:38:44Z"
        }
      ]
    },
    {
      "issue_number": 141,
      "title": "New App Req -- Llama-Swap",
      "body": "[https://github.com/mostlygeek/llama-swap.git](https://github.com/mostlygeek/llama-swap.git)\n\nLlama-swap will provide a lot of utility for the various inference frameworks that Harbor supports. Here's an example I read-\n\n_\"You can also use [llama-swap](https://github.com/mostlygeek/llama-swap) as a proxy. It launches llama.cpp (or any other command) on your behalf based on the model selected via the API, waits for it to start up, and proxies your HTTP requests to it. That way you can have a hundred different models (or quants, or llama.cpp configs) set up and it just hot-swaps them as needed by your apps.\n\nFor example, I have a workflow (using WilmerAI) that uses Command R, Phi 4, Mistral, and Qwen Coder, along with some embedding models (nomic). I can't fit all 5 of them in VRAM, and launching/stopping each manually would be ridiculous. I have Wilmer pointed at the proxy, and it automatically loads and unloads the models it requests via API.\"_\n\nOriginal posts here:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1j417qh/comment/mg899l3/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button](https://www.reddit.com/r/LocalLLaMA/comments/1j417qh/comment/mg899l3/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
      "state": "closed",
      "author": "ColumbusAI",
      "author_type": "User",
      "created_at": "2025-03-05T22:39:00Z",
      "updated_at": "2025-03-23T00:04:28Z",
      "closed_at": "2025-03-23T00:04:27Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/141/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/141",
      "api_url": "https://api.github.com/repos/av/harbor/issues/141",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:15.469067",
      "comments": [
        {
          "author": "DIGist",
          "body": "Hi! So I was going to open a new app request  for Wilmer to be added to Harbor and found this post. \n\nSo Apparently Wilmer already provides a Ollama compatible proxy endpoint, So reallly we just need to get Wilmer into Harbor and ideally llama-swap as well. ",
          "created_at": "2025-03-08T07:10:07Z"
        },
        {
          "author": "av",
          "body": "Thank you so much for the suggestion! I'll add it to the list of upcoming integrations ðŸ‘ ",
          "created_at": "2025-03-09T20:46:21Z"
        },
        {
          "author": "av",
          "body": "Added llama-swap in [v0.3.4](https://github.com/av/harbor/releases/tag/v0.3.4)",
          "created_at": "2025-03-22T11:10:53Z"
        },
        {
          "author": "ColumbusAI",
          "body": "Wooooo!!! you ROCK! ",
          "created_at": "2025-03-23T00:04:27Z"
        }
      ]
    },
    {
      "issue_number": 135,
      "title": "Suggestion: add caddy server as a reverse proxy",
      "body": "In the spirit of getting things up and running quickly, I suggest adding caddy (https://github.com/caddyserver/caddy; https://caddyserver.com/) as a reverse proxy serving other services",
      "state": "open",
      "author": "odeda",
      "author_type": "User",
      "created_at": "2025-02-23T10:01:59Z",
      "updated_at": "2025-03-03T10:18:16Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/135/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/135",
      "api_url": "https://api.github.com/repos/av/harbor/issues/135",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:15.688975",
      "comments": [
        {
          "author": "beyondmeat",
          "body": "traefik is much better imo. Hoping for support for that as well.",
          "created_at": "2025-02-23T12:45:59Z"
        },
        {
          "author": "av",
          "body": "Added `traefik` in [v0.3.0](https://github.com/av/harbor/releases/tag/v0.3.0), however I'm sure that the given integration version will not be sufficient for any kind of scenario. \n\nIf you'll have a chance, please try it out, I'm curious to learn more about use-cases and what's needed from the servi",
          "created_at": "2025-03-01T23:05:35Z"
        },
        {
          "author": "beyondmeat",
          "body": "@av Is there a reason why it's v2 and not v3?",
          "created_at": "2025-03-03T10:12:42Z"
        },
        {
          "author": "av",
          "body": "No specific reason, I just based on a relatively recent (as I thought) example that I had at hand. Granted config compatibility, v3 could work out of the box (in theory):\n```bash\n# Update to latest\nharbor config set traefik.version latest\nharbor pull traefik\n\n# Test\nharbor up traefik\nharbor open tra",
          "created_at": "2025-03-03T10:18:14Z"
        }
      ]
    },
    {
      "issue_number": 115,
      "title": "Service: LibreChat Â» \"Permission denied error\" after service start",
      "body": "# Harbor / LibreChat / Bug Reporting\n\n## ðŸ–¥ï¸ Setup\n\n### System\n\n- Computer: Apple M3 Max Macbook Pro\n- RAM: 128 GB\n- OS: macOS Sequoia 15.3\n\n---\n\n### Docker\n\n- CPU limit: 16\n- Memory limit: 96 GB\n- Swap: 4 GB\n- Disk Usage Limit: 128 GB\n- Engine: v27.4.0\n- Docker Desktop 4.37.2 (179585)\n\n---\n\n### Harbor\n\n- Default settings\n- App: Version 0.2.25 (20250126.003743)\n\n\tâœ… Docker is installed and running\n\tâœ… Docker Compose (v2) is installed\n\tâœ… Docker Compose (v2) version is newer than 2.23.1\n\tâœ… Harbor home: /Users/(UsernameXY)/harbor\n\tâœ… Default profile exists and is readable\n\tâœ… Current profile (.env) exists and is readable\n\tâœ… CLI is linked\n\tâŒ NVIDIA GPU is not available. NVIDIA GPU support may not work.\n\tâŒ NVIDIA Container Toolkit is not installed. NVIDIA GPU support may not work\n\n---\n\n## ðŸª² Bug / Additional Info\n\nServices started via Harbor App.\nPermission denied errors inside following containers:\n\n* harbor.librechat-rag (exited)\n* harbor.librechat (exited)\n\nDefault settings. No additional changes.\n\n---\n## ðŸ“’ Logs\n### harbor.librechat-rag âŒ\n\n``` bash\n2025-01-31 16:20:45 USER_AGENT environment variable not set, consider setting it to identify your requests.\n2025-01-31 16:20:45 Traceback (most recent call last):\n2025-01-31 16:20:45   File \"/app/main.py\", line 49, in <module>\n2025-01-31 16:20:45     from psql import PSQLDatabase, ensure_custom_id_index_on_embedding, pg_health_check\n2025-01-31 16:20:45   File \"/app/psql.py\", line 3, in <module>\n2025-01-31 16:20:45     from config import DSN, logger\n2025-01-31 16:20:45   File \"/app/config.py\", line 45, in <module>\n2025-01-31 16:20:45     os.makedirs(RAG_UPLOAD_DIR, exist_ok=True)\n2025-01-31 16:20:45   File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n2025-01-31 16:20:45     mkdir(name, mode)\n2025-01-31 16:20:45 PermissionError: [Errno 13] Permission denied: './uploads/'\n```\n\n---\n\n### harbor.librechat âŒ\n\n``` bash\n2025-01-31 16:20:44 exec /app/start_librechat.sh: permission denied\n```\n\n---\n\n### harbor.librechat-search âœ…\n\n``` bash\n2025-01-31 16:20:44 \n2025-01-31 16:20:44 2025-01-31T15:20:44.420958Z  INFO actix_server::builder: starting 16 workers\n2025-01-31 16:20:44 2025-01-31T15:20:44.420971Z  INFO actix_server::server: Actix runtime found; starting in Actix runtime\n```\n\n---\n\n### harbor.librechat-db âœ…\n\n``` bash\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.437+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23285,   \"ctx\":\"main\",\"msg\":\"Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'\"}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.438+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":5945603, \"ctx\":\"main\",\"msg\":\"Multi threading initialized\"}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.438+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4648601, \"ctx\":\"main\",\"msg\":\"Implicit TCP FastOpen unavailable. If TCP FastOpen is required, set at least one of the related parameters\",\"attr\":{\"relatedParameters\":[\"tcpFastOpenServer\",\"tcpFastOpenClient\",\"tcpFastOpenQueueSize\"]}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.438+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4915701, \"ctx\":\"main\",\"msg\":\"Initialized wire specification\",\"attr\":{\"spec\":{\"incomingExternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":25},\"incomingInternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":25},\"outgoing\":{\"minWireVersion\":6,\"maxWireVersion\":25},\"isInternalClient\":true}}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.439+00:00\"},\"s\":\"I\",  \"c\":\"TENANT_M\", \"id\":7091600, \"ctx\":\"main\",\"msg\":\"Starting TenantMigrationAccessBlockerRegistry\"}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.439+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":4615611, \"ctx\":\"initandlisten\",\"msg\":\"MongoDB starting\",\"attr\":{\"pid\":1,\"port\":27017,\"dbPath\":\"/data/db\",\"architecture\":\"64-bit\",\"host\":\"5a226268a926\"}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.439+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23403,   \"ctx\":\"initandlisten\",\"msg\":\"Build Info\",\"attr\":{\"buildInfo\":{\"version\":\"8.0.4\",\"gitVersion\":\"bc35ab4305d9920d9d0491c1c9ef9b72383d31f9\",\"openSSLVersion\":\"OpenSSL 3.0.13 30 Jan 2024\",\"modules\":[],\"allocator\":\"tcmalloc-google\",\"environment\":{\"distmod\":\"ubuntu2404\",\"distarch\":\"aarch64\",\"target_arch\":\"aarch64\"}}}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.439+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":51765,   \"ctx\":\"initandlisten\",\"msg\":\"Operating System\",\"attr\":{\"os\":{\"name\":\"Ubuntu\",\"version\":\"24.04\"}}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.439+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":21951,   \"ctx\":\"initandlisten\",\"msg\":\"Options set by command line\",\"attr\":{\"options\":{\"net\":{\"bindIp\":\"*\"},\"security\":{\"authorization\":\"disabled\"}}}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.440+00:00\"},\"s\":\"W\",  \"c\":\"STORAGE\",  \"id\":22271,   \"ctx\":\"initandlisten\",\"msg\":\"Detected unclean shutdown - Lock file is not empty\",\"attr\":{\"lockFile\":\"/data/db/mongod.lock\"}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.440+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22270,   \"ctx\":\"initandlisten\",\"msg\":\"Storage engine to use detected by data files\",\"attr\":{\"dbpath\":\"/data/db\",\"storageEngine\":\"wiredTiger\"}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.440+00:00\"},\"s\":\"W\",  \"c\":\"STORAGE\",  \"id\":22302,   \"ctx\":\"initandlisten\",\"msg\":\"Recovering data from the last clean checkpoint.\"}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.440+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22315,   \"ctx\":\"initandlisten\",\"msg\":\"Opening WiredTiger\",\"attr\":{\"config\":\"create,cache_size=47677M,session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,remove=true,path=journal,compressor=snappy),builtin_extension_config=(zstd=(compression_level=6)),file_manager=(close_idle_time=600,close_scan_interval=10,close_handle_minimum=2000),statistics_log=(wait=0),json_output=(error,message),verbose=[recovery_progress:1,checkpoint_progress:1,compact_progress:1,backup:0,checkpoint:0,compact:0,evict:0,history_store:0,recovery:0,rts:0,salvage:0,tiered:0,timestamp:0,transaction:0,verify:0,log:0],prefetch=(available=true,default=false),\"}}\n2025-01-31 16:20:44 {\"t\":{\"$date\":\"2025-01-31T15:20:44.941+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336844,\"ts_usec\":941085,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"Recovering log 3 through 4\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.018+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":17982,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"Recovering log 4 through 4\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.099+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":99228,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"Main recovery loop: starting at 3/5888 to 4/256\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.139+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":139431,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"Recovering log 3 through 4\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.238+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":238640,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"Recovering log 4 through 4\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.265+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":265867,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"recovery log replay has successfully finished and ran for 325 milliseconds\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.265+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":265944,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"Set global recovery timestamp: (0, 0)\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.265+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":265953,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"Set global oldest timestamp: (0, 0)\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.266+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":266453,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"recovery rollback to stable has successfully finished and ran for 0 milliseconds\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.266+00:00\"},\"s\":\"I\",  \"c\":\"WTCHKPT\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":266894,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"WT_SESSION.checkpoint\",\"category\":\"WT_VERB_CHECKPOINT_PROGRESS\",\"category_id\":7,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"saving checkpoint snapshot min: 3, snapshot max: 3 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0) base write gen: 189\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.268+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":268628,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"recovery checkpoint has successfully finished and ran for 2 milliseconds\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.268+00:00\"},\"s\":\"I\",  \"c\":\"WTRECOV\",  \"id\":22430,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336845,\"ts_usec\":268863,\"thread\":\"1:0xffff98d78040\",\"session_name\":\"txn-recover\",\"category\":\"WT_VERB_RECOVERY_PROGRESS\",\"category_id\":34,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"recovery was completed successfully and took 328ms, including 325ms for the log replay, 0ms for the rollback to stable, and 2ms for the checkpoint.\"}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.269+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4795906, \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger opened\",\"attr\":{\"durationMillis\":829}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.269+00:00\"},\"s\":\"I\",  \"c\":\"RECOVERY\", \"id\":23987,   \"ctx\":\"initandlisten\",\"msg\":\"WiredTiger recoveryTimestamp\",\"attr\":{\"recoveryTimestamp\":{\"$timestamp\":{\"t\":0,\"i\":0}}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.284+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":9068900, \"ctx\":\"initandlisten\",\"msg\":\"For customers running the current memory allocator, we suggest changing the contents of the following sysfsFile\",\"attr\":{\"allocator\":\"tcmalloc-google\",\"sysfsFile\":\"/sys/kernel/mm/transparent_hugepage/defrag\",\"currentValue\":\"madvise\",\"desiredValue\":\"defer+madvise\"},\"tags\":[\"startupWarnings\"]}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.285+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":8640302, \"ctx\":\"initandlisten\",\"msg\":\"We suggest setting the contents of sysfsFile to 0.\",\"attr\":{\"sysfsFile\":\"/sys/kernel/mm/transparent_hugepage/khugepaged/max_ptes_none\",\"currentValue\":511},\"tags\":[\"startupWarnings\"]}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.285+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":8718500, \"ctx\":\"initandlisten\",\"msg\":\"Your system has glibc support for rseq built in, which is not yet supported by tcmalloc-google and has critical performance implications. Please set the environment variable GLIBC_TUNABLES=glibc.pthread.rseq=0\",\"tags\":[\"startupWarnings\"]}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.285+00:00\"},\"s\":\"W\",  \"c\":\"NETWORK\",  \"id\":5123300, \"ctx\":\"initandlisten\",\"msg\":\"vm.max_map_count is too low\",\"attr\":{\"currentValue\":262144,\"recommendedMinimum\":1677720,\"maxConns\":838860},\"tags\":[\"startupWarnings\"]}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.285+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":8386700, \"ctx\":\"initandlisten\",\"msg\":\"We suggest setting swappiness to 0 or 1, as swapping can cause performance problems.\",\"attr\":{\"sysfsFile\":\"/proc/sys/vm/swappiness\",\"currentValue\":60},\"tags\":[\"startupWarnings\"]}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.286+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4915702, \"ctx\":\"initandlisten\",\"msg\":\"Updated wire specification\",\"attr\":{\"oldSpec\":{\"incomingExternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":25},\"incomingInternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":25},\"outgoing\":{\"minWireVersion\":6,\"maxWireVersion\":25},\"isInternalClient\":true},\"newSpec\":{\"incomingExternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":25},\"incomingInternalClient\":{\"minWireVersion\":25,\"maxWireVersion\":25},\"outgoing\":{\"minWireVersion\":25,\"maxWireVersion\":25},\"isInternalClient\":true}}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.286+00:00\"},\"s\":\"I\",  \"c\":\"REPL\",     \"id\":5853300, \"ctx\":\"initandlisten\",\"msg\":\"current featureCompatibilityVersion value\",\"attr\":{\"featureCompatibilityVersion\":\"8.0\",\"context\":\"startup\"}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.286+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":5071100, \"ctx\":\"initandlisten\",\"msg\":\"Clearing temp directory\"}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.287+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":6608200, \"ctx\":\"initandlisten\",\"msg\":\"Initializing cluster server parameters from disk\"}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.287+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":20536,   \"ctx\":\"initandlisten\",\"msg\":\"Flow Control is enabled on this deployment\"}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.287+00:00\"},\"s\":\"I\",  \"c\":\"FTDC\",     \"id\":20625,   \"ctx\":\"initandlisten\",\"msg\":\"Initializing full-time diagnostic data capture\",\"attr\":{\"dataDirectory\":\"/data/db/diagnostic.data\"}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.288+00:00\"},\"s\":\"I\",  \"c\":\"REPL\",     \"id\":6015317, \"ctx\":\"initandlisten\",\"msg\":\"Setting new configuration state\",\"attr\":{\"newState\":\"ConfigReplicationDisabled\",\"oldState\":\"ConfigPreStart\"}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.288+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22262,   \"ctx\":\"initandlisten\",\"msg\":\"Timestamp monitor starting\"}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.288+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":7333401, \"ctx\":\"initandlisten\",\"msg\":\"Starting the DiskSpaceMonitor\"}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.288+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":23015,   \"ctx\":\"listener\",\"msg\":\"Listening on\",\"attr\":{\"address\":\"/tmp/mongodb-27017.sock\"}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.288+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":23015,   \"ctx\":\"listener\",\"msg\":\"Listening on\",\"attr\":{\"address\":\"0.0.0.0:27017\"}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.288+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":23016,   \"ctx\":\"listener\",\"msg\":\"Waiting for connections\",\"attr\":{\"port\":27017,\"ssl\":\"off\"}}\n2025-01-31 16:20:45 {\"t\":{\"$date\":\"2025-01-31T15:20:45.288+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":8423403, \"ctx\":\"initandlisten\",\"msg\":\"mongod startup complete\",\"attr\":{\"Summary of time elapsed\":{\"Startup from clean shutdown?\":false,\"Statistics\":{\"Set up periodic runner\":\"0 ms\",\"Set up online certificate status protocol manager\":\"0 ms\",\"Transport layer setup\":\"0 ms\",\"Run initial syncer crash recovery\":\"0 ms\",\"Create storage engine lock file in the data directory\":\"1 ms\",\"Get metadata describing storage engine\":\"0 ms\",\"Validate options in metadata against current startup options\":\"0 ms\",\"Create storage engine\":\"841 ms\",\"Write current PID to file\":\"0 ms\",\"Initialize FCV before rebuilding indexes\":\"1 ms\",\"Drop abandoned idents and get back indexes that need to be rebuilt or builds that need to be restarted\":\"0 ms\",\"Rebuild indexes for collections\":\"0 ms\",\"Load cluster parameters from disk for a standalone\":\"0 ms\",\"Build user and roles graph\":\"0 ms\",\"Set up the background thread pool responsible for waiting for opTimes to be majority committed\":\"0 ms\",\"Start up the replication coordinator\":\"1 ms\",\"Ensure the change stream collections on startup contain consistent data\":\"0 ms\",\"Write startup options to the audit log\":\"0 ms\",\"Start transport layer\":\"0 ms\",\"_initAndListen total elapsed time\":\"849 ms\"}}}}\n2025-01-31 16:20:46 {\"t\":{\"$date\":\"2025-01-31T15:20:46.016+00:00\"},\"s\":\"I\",  \"c\":\"FTDC\",     \"id\":20631,   \"ctx\":\"ftdc\",\"msg\":\"Unclean full-time diagnostic data capture shutdown detected, found interim file, some metrics may have been lost\",\"attr\":{\"error\":{\"code\":0,\"codeName\":\"OK\"}}}\n2025-01-31 16:20:46 {\"t\":{\"$date\":\"2025-01-31T15:20:46.094+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":636300,  \"ctx\":\"ftdc\",\"msg\":\"Use of deprecated server parameter name\",\"attr\":{\"deprecatedName\":\"internalQueryCacheSize\",\"canonicalName\":\"internalQueryCacheMaxEntriesPerCollection\"}}\n2025-01-31 16:20:46 {\"t\":{\"$date\":\"2025-01-31T15:20:46.094+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":636300,  \"ctx\":\"ftdc\",\"msg\":\"Use of deprecated server parameter name\",\"attr\":{\"deprecatedName\":\"oplogSamplingLogIntervalSeconds\",\"canonicalName\":\"collectionSamplingLogIntervalSeconds\"}}\n2025-01-31 16:20:46 {\"t\":{\"$date\":\"2025-01-31T15:20:46.095+00:00\"},\"s\":\"W\",  \"c\":\"NETWORK\",  \"id\":23803,   \"ctx\":\"ftdc\",\"msg\":\"Use of deprecated server parameter 'sslMode', please use 'tlsMode' instead.\"}\n2025-01-31 16:20:46 {\"t\":{\"$date\":\"2025-01-31T15:20:46.095+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":636300,  \"ctx\":\"ftdc\",\"msg\":\"Use of deprecated server parameter name\",\"attr\":{\"deprecatedName\":\"wiredTigerConcurrentReadTransactions\",\"canonicalName\":\"storageEngineConcurrentReadTransactions\"}}\n2025-01-31 16:20:46 {\"t\":{\"$date\":\"2025-01-31T15:20:46.095+00:00\"},\"s\":\"W\",  \"c\":\"CONTROL\",  \"id\":636300,  \"ctx\":\"ftdc\",\"msg\":\"Use of deprecated server parameter name\",\"attr\":{\"deprecatedName\":\"wiredTigerConcurrentWriteTransactions\",\"canonicalName\":\"storageEngineConcurrentWriteTransactions\"}}\n2025-01-31 16:21:45 {\"t\":{\"$date\":\"2025-01-31T15:21:45.291+00:00\"},\"s\":\"I\",  \"c\":\"WTCHKPT\",  \"id\":22430,   \"ctx\":\"Checkpointer\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336905,\"ts_usec\":291878,\"thread\":\"1:0xffff8540e6c0\",\"session_name\":\"WT_SESSION.checkpoint\",\"category\":\"WT_VERB_CHECKPOINT_PROGRESS\",\"category_id\":7,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"saving checkpoint snapshot min: 6, snapshot max: 6 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0) base write gen: 189\"}}}\n2025-01-31 16:22:45 {\"t\":{\"$date\":\"2025-01-31T15:22:45.296+00:00\"},\"s\":\"I\",  \"c\":\"WTCHKPT\",  \"id\":22430,   \"ctx\":\"Checkpointer\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738336965,\"ts_usec\":296122,\"thread\":\"1:0xffff8540e6c0\",\"session_name\":\"WT_SESSION.checkpoint\",\"category\":\"WT_VERB_CHECKPOINT_PROGRESS\",\"category_id\":7,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"saving checkpoint snapshot min: 7, snapshot max: 7 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0) base write gen: 189\"}}}\n2025-01-31 16:23:45 {\"t\":{\"$date\":\"2025-01-31T15:23:45.299+00:00\"},\"s\":\"I\",  \"c\":\"WTCHKPT\",  \"id\":22430,   \"ctx\":\"Checkpointer\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738337025,\"ts_usec\":299678,\"thread\":\"1:0xffff8540e6c0\",\"session_name\":\"WT_SESSION.checkpoint\",\"category\":\"WT_VERB_CHECKPOINT_PROGRESS\",\"category_id\":7,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"saving checkpoint snapshot min: 8, snapshot max: 8 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0) base write gen: 189\"}}}\n2025-01-31 16:24:45 {\"t\":{\"$date\":\"2025-01-31T15:24:45.302+00:00\"},\"s\":\"I\",  \"c\":\"WTCHKPT\",  \"id\":22430,   \"ctx\":\"Checkpointer\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738337085,\"ts_usec\":302597,\"thread\":\"1:0xffff8540e6c0\",\"session_name\":\"WT_SESSION.checkpoint\",\"category\":\"WT_VERB_CHECKPOINT_PROGRESS\",\"category_id\":7,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"saving checkpoint snapshot min: 9, snapshot max: 9 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0) base write gen: 189\"}}}\n2025-01-31 16:25:45 {\"t\":{\"$date\":\"2025-01-31T15:25:45.307+00:00\"},\"s\":\"I\",  \"c\":\"WTCHKPT\",  \"id\":22430,   \"ctx\":\"Checkpointer\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738337145,\"ts_usec\":307293,\"thread\":\"1:0xffff8540e6c0\",\"session_name\":\"WT_SESSION.checkpoint\",\"category\":\"WT_VERB_CHECKPOINT_PROGRESS\",\"category_id\":7,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"saving checkpoint snapshot min: 10, snapshot max: 10 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0) base write gen: 189\"}}}\n2025-01-31 16:26:45 {\"t\":{\"$date\":\"2025-01-31T15:26:45.311+00:00\"},\"s\":\"I\",  \"c\":\"WTCHKPT\",  \"id\":22430,   \"ctx\":\"Checkpointer\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":{\"ts_sec\":1738337205,\"ts_usec\":311008,\"thread\":\"1:0xffff8540e6c0\",\"session_name\":\"WT_SESSION.checkpoint\",\"category\":\"WT_VERB_CHECKPOINT_PROGRESS\",\"category_id\":7,\"verbose_level\":\"DEBUG_1\",\"verbose_level_id\":1,\"msg\":\"saving checkpoint snapshot min: 11, snapshot max: 11 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0) base write gen: 189\"}}}\n```\n\n---\n\n### harbor.librechat-vector âœ…\n\n``` bash\n2025-01-31 16:20:44 2025-01-31 15:20:44.601 UTC [1] LOG:  starting PostgreSQL 15.4 (Debian 15.4-2.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\n2025-01-31 16:20:44 2025-01-31 15:20:44.616 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2025-01-31 16:20:44 2025-01-31 15:20:44.616 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2025-01-31 16:20:44 2025-01-31 15:20:44.617 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2025-01-31 16:20:44 2025-01-31 15:20:44.621 UTC [30] LOG:  database system was interrupted; last known up at 2025-01-31 15:09:58 UTC\n2025-01-31 16:20:45 2025-01-31 15:20:45.187 UTC [30] LOG:  database system was not properly shut down; automatic recovery in progress\n2025-01-31 16:20:45 2025-01-31 15:20:45.194 UTC [30] LOG:  redo starts at 0/1960D18\n2025-01-31 16:20:45 2025-01-31 15:20:45.194 UTC [30] LOG:  invalid record length at 0/1960D50: wanted 24, got 0\n2025-01-31 16:20:45 2025-01-31 15:20:45.194 UTC [30] LOG:  redo done at 0/1960D18 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s\n2025-01-31 16:20:45 2025-01-31 15:20:45.203 UTC [28] LOG:  checkpoint starting: end-of-recovery immediate wait\n2025-01-31 16:20:45 2025-01-31 15:20:45.209 UTC [28] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.001 s, total=0.007 s; sync files=2, longest=0.001 s, average=0.001 s; distance=0 kB, estimate=0 kB\n2025-01-31 16:20:45 2025-01-31 15:20:45.211 UTC [1] LOG:  database system is ready to accept connections\n```\n\n---\n",
      "state": "closed",
      "author": "jschmdt",
      "author_type": "User",
      "created_at": "2025-01-31T17:09:27Z",
      "updated_at": "2025-03-03T07:46:58Z",
      "closed_at": "2025-03-03T07:46:57Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/115/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/115",
      "api_url": "https://api.github.com/repos/av/harbor/issues/115",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:15.893979",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for trying out Harbor and for such a readable report!\n\nAfter debugging for some time, I was sure it's related to the classic user permissions issue with bind mounts on MacOS vs Linux in Docker. However, it turned out to be much simpler, it appears that somehow the `start_librechat.sh` in ",
          "created_at": "2025-02-01T16:38:39Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.2.26](https://github.com/av/harbor/releases/tag/v0.2.26), please let me know if it fixes the problem",
          "created_at": "2025-02-01T18:59:46Z"
        },
        {
          "author": "av",
          "body": "Please feel free to follow up here or in a new one if needed",
          "created_at": "2025-03-03T07:46:57Z"
        }
      ]
    },
    {
      "issue_number": 123,
      "title": "suggestion: have fewer items at the root of the repo",
      "body": "You have to scroll for ever to get to the body of the repo and all these files at the root feels pretty messy. I'd put all those compose files in a single directory and roll up some of those other directories into some categories",
      "state": "open",
      "author": "byjlw",
      "author_type": "User",
      "created_at": "2025-02-06T20:26:49Z",
      "updated_at": "2025-03-02T20:47:12Z",
      "closed_at": null,
      "labels": [
        "help wanted"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/123/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/123",
      "api_url": "https://api.github.com/repos/av/harbor/issues/123",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:16.075091",
      "comments": [
        {
          "author": "av",
          "body": "I understand the frustration, however it's quite an invasive change in terms of the compose file locations and relative paths to volumes/workspaces and caches and would come with a lot of testing being required, so I'm a bit hesitant to invest the time right now (compared to some other work)",
          "created_at": "2025-02-06T22:20:51Z"
        },
        {
          "author": "av",
          "body": "[v0.3.0](https://github.com/av/harbor/releases/tag/v0.3.0) enables removing all compose files from the root of the repo, however it'll be a breaking change as it won't be compatible with legacy mode for the CLI, so the cleanup is likely to happen in v0.4.0",
          "created_at": "2025-03-01T23:11:46Z"
        },
        {
          "author": "ColumbusAI",
          "body": "> [v0.3.0](https://github.com/av/harbor/releases/tag/v0.3.0) enables removing all compose files from the root of the repo, however it'll be a breaking change as it won't be compatible with legacy mode for the CLI, so the cleanup is likely to happen in v0.4.0\n\nI like the FAST startup speed of v0.3.0!",
          "created_at": "2025-03-02T01:04:16Z"
        },
        {
          "author": "av",
          "body": "ðŸ¤” The delay we're talking about happens before the `docker compose` own output, while it's loading the service files for Harbor. \n\n`harbor down` works identically to `harbor up` in this regard now and docker starts processing the command within ~400ms on my machine.\n\nPlease fix me if I'm wrong - you",
          "created_at": "2025-03-02T08:44:51Z"
        },
        {
          "author": "ColumbusAI",
          "body": "@av here's what I see -- [https://youtu.be/9P6BLS3WLbU](https://youtu.be/9P6BLS3WLbU)\n\nI'm not sure why there's this big delay in 'harbor down' compared to 'harbor up'... perhaps it's as you mentioned and Harbor is waiting for the 10s timeout before forcefully closing?",
          "created_at": "2025-03-02T20:04:43Z"
        }
      ]
    },
    {
      "issue_number": 68,
      "title": "n8n on `--latest` version",
      "body": "Hi,\r\nI installed n8n, then its homepage asks for `N8N_SECURE_COOKIE=false`.\r\n\r\n![image](https://github.com/user-attachments/assets/a8bdae61-cc98-402d-a303-c6b8a736d6df)\r\n\r\nSetting it with `harbor config set N8N_SECURE_COOKIE false` doesn't have effect.\r\n\r\nThank you",
      "state": "closed",
      "author": "PieBru",
      "author_type": "User",
      "created_at": "2024-10-29T10:12:53Z",
      "updated_at": "2025-03-01T23:38:30Z",
      "closed_at": "2025-03-01T23:38:28Z",
      "labels": [
        "bug",
        "OS:MacOS"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/68/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/68",
      "api_url": "https://api.github.com/repos/av/harbor/issues/68",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:16.278692",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for the reports!\r\n\r\nThat's a thing I definitely missed in the baseline confg for `n8n`. [v0.2.15](https://github.com/av/harbor/releases/tag/v0.2.15) will ship with secure cookies disabled out-of-the-box since `http://localhost` is where the service will run in most supported scenarios\r\n\r\n",
          "created_at": "2024-11-02T13:47:39Z"
        },
        {
          "author": "av",
          "body": "Please feel free to follow up here or in a new issue if needed, thanks!",
          "created_at": "2025-03-01T23:38:28Z"
        }
      ]
    },
    {
      "issue_number": 127,
      "title": "Timezone not inherited from OS on Windows",
      "body": "Was just trying this tool https://openwebui.com/t/technotrekker/gettime but it assumes timezone is set correctly in the container.",
      "state": "closed",
      "author": "bjj",
      "author_type": "User",
      "created_at": "2025-02-13T06:47:45Z",
      "updated_at": "2025-03-01T23:06:56Z",
      "closed_at": "2025-03-01T23:06:55Z",
      "labels": [
        "bug",
        "OS:Windows"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/127/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/127",
      "api_url": "https://api.github.com/repos/av/harbor/issues/127",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:16.462568",
      "comments": [
        {
          "author": "av",
          "body": "Added [a small fix](https://github.com/av/harbor/commit/7b2f892df9328c0ac7be69e5eed4f7d994df6f8e) that should help in this specific instance. However, when it comes to Windows it'll be also important to check that WSL2 has correct time by itself.\n\nI'm a bit hesitant to apply the fix to all container",
          "created_at": "2025-02-22T09:37:23Z"
        },
        {
          "author": "av",
          "body": "Closing as the fix was released, please feel free to follow up or open a new issue as needed",
          "created_at": "2025-03-01T23:06:55Z"
        }
      ]
    },
    {
      "issue_number": 132,
      "title": ".com/api/keys 502 error",
      "body": "Hi, I am able to set API keys through the config / .env files, but through the front end when I try to set API keys, I get a .com/api/keys 502 error in the console logs.\n\nInspecting harbor logs I get this error too.\n\nharbor.librechat  | 2025-02-18 16:54:58 error: There was an uncaught error: Invalid key length\nharbor.librechat exited with code 0\n\nI would like to be able to set API keys from the front end, how may I do so?",
      "state": "closed",
      "author": "pesschap",
      "author_type": "User",
      "created_at": "2025-02-18T17:05:11Z",
      "updated_at": "2025-03-01T23:06:23Z",
      "closed_at": "2025-03-01T23:06:22Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/132/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/132",
      "api_url": "https://api.github.com/repos/av/harbor/issues/132",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:16.646708",
      "comments": [
        {
          "author": "av",
          "body": "Hi, it appears to be caused by [this issue](https://github.com/danny-avila/LibreChat/discussions/2024#discussioncomment-10454097), just pushed updated default configuration to `main` that doesn't seem to cause the problem anymore",
          "created_at": "2025-02-22T11:06:53Z"
        },
        {
          "author": "av",
          "body": "Closing as the fix was released, please feel free to follow up or open a new issue as needed",
          "created_at": "2025-03-01T23:06:22Z"
        }
      ]
    },
    {
      "issue_number": 136,
      "title": "Error when building qrgen docker image",
      "body": "# Description\nI was experimenting with using the tunnel functionality in Harbor to access my Open WebUI instance outside of my local network. While I am able to get a cloudflare URL that works, I notice that the qrgen image that is supposed to run to render a QR code of the URL\nfails to properly build. \n\n# Steps to Reproduce\nRun the following commands:\n```\nharbor up\nharbor tunnel webui\n```\n\n# Expected Behaviour\nThe qrgen image should build successfully and print out the QR code for the cloudflare tunnel.\n\n\n# Actual Behaviour\nWhen building the qrgen image, the following error is observed:\n```\nâ¯ harbor tunnel webui\n19:10:26 [INFO] Starting new tunnel\n19:10:26 [INFO] Container name: harbor.cfd.tunnel.1740741026\n19:10:26 [INFO] Intra URL: http://harbor.webui:8080\nd20e8ee9fb42c65eb881a295b1d3a7cf5f48eec6228b5a58f29b76f9e82b159d\n19:10:27 [INFO] Waiting for tunnel URL...\n19:10:28 [INFO] Waiting for tunnel URL...\n19:10:29 [INFO] Waiting for tunnel URL...\n19:10:30 [INFO] Waiting for tunnel URL...\n^[[23~19:10:31 [INFO] Waiting for tunnel URL...\n19:10:32 [INFO] Waiting for tunnel URL...\n19:10:32 [INFO] Tunnel URL: https://discussion-alice-sue-uri.trycloudflare.com\nCompose now can delegate build to bake for better performances\nJust set COMPOSE_BAKE=true\n[+] Building 4.8s (9/9) FINISHED                                                                                                                                                                                                                                                                                                                                                docker:default\n => [qrgen internal] load build definition from Dockerfile                                                                                                                                                                                                                                                                                                                                0.0s\n => => transferring dockerfile: 276B                                                                                                                                                                                                                                                                                                                                                      0.0s\n => [qrgen internal] load metadata for docker.io/pkgxdev/pkgx:latest                                                                                                                                                                                                                                                                                                                      1.6s\n => [qrgen internal] load .dockerignore                                                                                                                                                                                                                                                                                                                                                   0.0s\n => => transferring context: 2B                                                                                                                                                                                                                                                                                                                                                           0.0s\n => [qrgen 1/5] FROM docker.io/pkgxdev/pkgx:latest@sha256:4f7d70c2568ecbedab0fb2f60264629e9b0f2b6939f498ecfc35400e59a89f3a                                                                                                                                                                                                                                                                0.0s\n => [qrgen internal] load build context                                                                                                                                                                                                                                                                                                                                                   0.0s\n => => transferring context: 87B                                                                                                                                                                                                                                                                                                                                                          0.0s\n => CACHED [qrgen 2/5] WORKDIR /app                                                                                                                                                                                                                                                                                                                                                       0.0s\n => CACHED [qrgen 3/5] RUN pkgx +node@20 npm install qrcode-terminal                                                                                                                                                                                                                                                                                                                      0.0s\n => CACHED [qrgen 4/5] COPY ./gen.ts /app/gen.ts                                                                                                                                                                                                                                                                                                                                          0.0s\n => ERROR [qrgen 5/5] RUN pkgx gen.ts test                                                                                                                                                                                                                                                                                                                                                3.2s\n------                                                                                                                                                                                                                                                                                                                                                                                         \n > [qrgen 5/5] RUN pkgx gen.ts test:\n3.064 Error: CmdNotFound(\"gen.ts\")\n------\nfailed to solve: process \"/bin/bash -c pkgx gen.ts test\" did not complete successfully: exit code: 1\n19:10:37 [ERROR] Failed to print QR code\n\n```\n\n# Additional Details\n- OS: Fedora Workstation 41\n- Docker Version: 28.0.1, build 068a01e\n- Shell: zsh",
      "state": "closed",
      "author": "Tien-Cheng",
      "author_type": "User",
      "created_at": "2025-02-28T11:11:57Z",
      "updated_at": "2025-03-01T23:03:56Z",
      "closed_at": "2025-03-01T23:03:55Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/136/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/136",
      "api_url": "https://api.github.com/repos/av/harbor/issues/136",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:16.839306",
      "comments": [
        {
          "author": "Tien-Cheng",
          "body": "I found that I could fix it by changing the Dockerfile as follows:\nBefore:\n```\nFROM pkgxdev/pkgx\n\nWORKDIR /app\n\nRUN pkgx +node@20 npm install qrcode-terminal\nCOPY ./gen.ts /app/gen.ts\n# Activate pkgx env\nRUN pkgx gen.ts test\n\nENTRYPOINT [ \"pkgx\", \"gen.ts\" ]\n```\n\nAfter:\n```\nFROM pkgxdev/pkgx\n\nWORKDIR",
          "created_at": "2025-02-28T11:19:35Z"
        },
        {
          "author": "av",
          "body": "Thank you so much for the detailed report and for the fix ðŸ™Œ \nAs mentioned in the PR - the preferred way is to drop `pkgx` altogether, which might be out of scope for you, so I already did that and it'll be a part of the incoming release\n\nI'm still very thankful for your work here, so will mention th",
          "created_at": "2025-03-01T22:57:45Z"
        },
        {
          "author": "av",
          "body": "The fix was released in [v0.3.0](https://github.com/av/harbor/releases/tag/v0.3.0), please feel free to follow up here or open a new issue as needed",
          "created_at": "2025-03-01T23:03:55Z"
        }
      ]
    },
    {
      "issue_number": 129,
      "title": "Ollama share models between host and container",
      "body": "Hello, I notice that `~/.ollama` on host is mounted to `/root/.ollama` in the container, however from container I can't access the models have been downloaded by the host, is that on purpose? \n\nAnything I can do to reuse the models from the host?\n\nThanks!",
      "state": "closed",
      "author": "shenhai-ran",
      "author_type": "User",
      "created_at": "2025-02-15T04:24:30Z",
      "updated_at": "2025-02-24T09:09:48Z",
      "closed_at": "2025-02-24T09:09:46Z",
      "labels": [
        "documentation",
        "question",
        "OS:Linux",
        "OS:Windows"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/129/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/129",
      "api_url": "https://api.github.com/repos/av/harbor/issues/129",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:17.019269",
      "comments": [
        {
          "author": "av",
          "body": "Hi, the idea is definitely to share the models with the host, the current default location is from the Ollama's defaults on MacOS\n\nThe path is configurable and you can point it to your actual cache path like this:\n```bash\nharbor config set ollama.cache /usr/share/ollama/.ollama\n\n# Test: \n# start oll",
          "created_at": "2025-02-22T10:16:57Z"
        },
        {
          "author": "shenhai-ran",
          "body": "Thanks! It works on Linux",
          "created_at": "2025-02-24T09:09:46Z"
        }
      ]
    },
    {
      "issue_number": 93,
      "title": "Open WebUI config resets on restart",
      "body": "From the wiki:\r\n\r\n> You can configure Open WebUI in three ways:\r\n> - Via WebUI itself: changes are saved in the webui/config.json file, Harbor may override them on restart\r\n>   - Copy config changes to the webui/configs/config.override.json in order to persist them over Harbor's default config\r\n\r\nIs this really how it should be or maybe there's something I don't understand (wchich absolutely may be the case ðŸ˜‰). If I want to make the changes made via UI persist after restart, I should make a copy of my settings first? If so, that's really inconvenient as I'd like to be able to regularly change some settings in the UI and I can't be trusted with remembering to merge the files ðŸ˜‰ Is there a reason it's designed this way in Harbor?\r\n\r\nBesides, I thing the path changed as there isn't a `webui/config.json` file.",
      "state": "open",
      "author": "dkodr",
      "author_type": "User",
      "created_at": "2024-12-31T13:32:48Z",
      "updated_at": "2025-02-18T02:22:21Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/93/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/93",
      "api_url": "https://api.github.com/repos/av/harbor/issues/93",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:17.280421",
      "comments": [
        {
          "author": "av",
          "body": "Hey ðŸ‘‹ \r\n\r\nThanks for trying out Harbor. The overwrite is applicable only to portions that are pre-configured by Harbor (Web RAG, TTS, Image Generation, embeddings), in such cases using an override config is a current workaround.  For API connections - see `openai` in the config. \r\n\r\nYou can find the",
          "created_at": "2024-12-31T13:40:16Z"
        },
        {
          "author": "cartergrobinson",
          "body": "What format should be used for the override.env file to persist ComfyUI running on a remote host?\n\nI tried this, but it doesn't seem to show up in webui:\n\n```\n# This file can be used for additional environment variables\n# specifically for the \"webui\" service.\n# You can also use the \"harbor env\" comm",
          "created_at": "2025-01-23T07:42:27Z"
        },
        {
          "author": "av",
          "body": "Hi ðŸ‘‹ \n\nIn order to persist ComfyUI and other Open WebUI-related config overrides - you can use config.override.json in the open-webui workspace:\nhttps://github.com/av/harbor/blob/main/open-webui/configs/config.override.json\n\nAny settings from there will override whatever is set by Harbor automatical",
          "created_at": "2025-02-01T18:57:51Z"
        },
        {
          "author": "pesschap",
          "body": "@av I'm trying to change some settings, e.g. I have some OpenAI API Urls added but want to only show 1 or 2 models instead of the whole list. However the settings do not save and I cannot find the file that contains those settings. The settings get overriden every time I restart the container. How d",
          "created_at": "2025-02-18T02:22:20Z"
        }
      ]
    },
    {
      "issue_number": 122,
      "title": "Add Browser Use",
      "body": "https://github.com/browser-use/web-ui\n\nopen source computer use dockerized setup would fit well in this stack",
      "state": "open",
      "author": "drmbt",
      "author_type": "User",
      "created_at": "2025-02-04T23:45:58Z",
      "updated_at": "2025-02-05T12:47:05Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/122/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/122",
      "api_url": "https://api.github.com/repos/av/harbor/issues/122",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:17.464449",
      "comments": [
        {
          "author": "av",
          "body": "Yes, it's one of my candidates for the next releases as well. The thing that stopped me last time was lack of pre-built images, but I'll see if there's a way around that",
          "created_at": "2025-02-05T00:21:08Z"
        },
        {
          "author": "drmbt",
          "body": "awesome! how about something to do speaker diarization? Been looking into this one: https://github.com/juanmc2005/diart",
          "created_at": "2025-02-05T12:47:04Z"
        }
      ]
    },
    {
      "issue_number": 101,
      "title": "Nvidia CDI driver support and Podman",
      "body": "If I understand correctly the current support for Nvidia GPUs relies on using adding appropriate `compose.x.<service>.nvidia.yml` file to the compose files loaded with docker-compose, these contain the device description:\r\n```\r\ndevices:\r\n  - driver: nvidia\r\n    count: all\r\n    capabilities: [gpu]\r\n```\r\nThis is a problem for systems relying on CDI driver for the Nvidia support for containers, such as NixOS. Allowing the user to change this section based on the system configuration would also (partialy) enable use of Podman instead of Docker, since the former relies on CDI. The section for CDI system would look like:\r\n```\r\ndevices:\r\n  - driver: cdo\r\n    capabilities: [gpu]\r\n    device_ids:\r\n    - nvidia.com/gpu=all\r\n```\r\nI have tested this change for ollama + webui and it does work.\r\n\r\nAnother problem is that on CDI system such as NixOS the detection of `nvidia-ctk` fails even though `nvidia-container-toolkit` works. This could be solved by adding an \"force-nvidia\" variable which would allow to force Nvidia even though the checks fail.\r\n\r\nRegarding Podman, the only place where it is currently not compatible apart from Nvidia seems to be the `--wait` flag for `docker compose`, this may be fixed in future releases of Podman.",
      "state": "open",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2025-01-15T20:36:32Z",
      "updated_at": "2025-02-03T12:25:57Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "OS:Linux"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 20,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/101/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "FrantaNautilus"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/101",
      "api_url": "https://api.github.com/repos/av/harbor/issues/101",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:17.660273",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thank you so much as always for the detailed analysis!\n\nYou're correct that Nvidia CTK and resource sharing is setup via the [Cross Service files](https://github.com/av/harbor/wiki/6.-Harbor-Compose-Setup#cross-service-file) (I also call them `.x.` files, haha).\n\nIn such a file `nvidia` is a tag",
          "created_at": "2025-01-18T12:13:35Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank you for considering this enhancement and of course for continuing to develop `harbor`. I do not want to burden you with additional work, so I will do my best to find how to detect CDI and differentiate it from CTK approach. I will add information I find to this issue.\nI am also not sure if I u",
          "created_at": "2025-01-18T14:59:05Z"
        },
        {
          "author": "av",
          "body": "> I will do my best to find how to detect CDI and differentiate it from CTK approach\n\nThank you so much for the kind words and for the suggestion to scope this out, it'll make things much simpler, as I don't have a NIX system setup and only vaguely familiar with such setups atm, I hope there's a sim",
          "created_at": "2025-01-18T16:40:38Z"
        },
        {
          "author": "av",
          "body": "I decided to make things simpler right away, latest `main` has configurable [capability detection](https://github.com/av/harbor/wiki/3.-Harbor-CLI-Reference#capabilities-detection), so you can turn it off immediately and use that instead of the `harbor defaults` for adding `cdi` cross files ",
          "created_at": "2025-01-18T16:59:03Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank you so much for both the extensive reply (I am sorry if my ideas are not much useful, in programming I am just an amateur, yet I am always happy to learn.) and for the new feature, this will make the support much easier to implement.\n\nI should be able to finish my research of CDI detection by ",
          "created_at": "2025-01-18T17:19:37Z"
        }
      ]
    },
    {
      "issue_number": 120,
      "title": "Add R2R",
      "body": "R2R</strong></a><span>Â </span>â€” an all-in-one open source AI Retrieval-Augmented Generation system thatâ€™s easy to self-host and super flexible for a range of use cases. R2R lets you ingest documents (PDFs, images, audio, JSON, etc.) into a local or cloud-based knowledge store, and then query them using advanced hybrid or graph-based search. It even supports multi-step â€œagenticâ€ reasoning if you want more powerful question answering, coding hints, or domain-specific Q&amp;A on your private data.</p><p style=\"margin-bottom: 1rem; color: rgb(184, 197, 201); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(11, 20, 22); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">\n\n\nOwners Website\thttps://sciphi.ai/\nGitHub\thttps://github.com/SciPhi-AI/R2R\nDocker & Full Installation Guide\t[Self-Hosting (Docker)](https://r2r-docs.sciphi.ai/self-hosting/installation/full/docker)\nQuickstart Docs\t[R2R Quickstart](https://r2r-docs.sciphi.ai/self-hosting/quickstart)",
      "state": "open",
      "author": "nullnuller",
      "author_type": "User",
      "created_at": "2025-02-03T00:43:14Z",
      "updated_at": "2025-02-03T12:24:56Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/120/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/120",
      "api_url": "https://api.github.com/repos/av/harbor/issues/120",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:17.908045",
      "comments": [
        {
          "author": "av",
          "body": "Awesome tool, thank you for sharing!\n\nThe official CLI path is unlikely as it needs install on the host or Docker-in-Docker otherwise, but I'll see if there's a way to make it work.\n\nOtherwise, the full compose is quite convoluted, so similarly to Dify it'll be a large chunk of work to adopt it to H",
          "created_at": "2025-02-03T12:24:41Z"
        }
      ]
    },
    {
      "issue_number": 111,
      "title": "aider doesnt load due to endpoint",
      "body": "hello, \n\nfirstly - the app is the dopestest. \n\n\nhave one small issue in testing: \ncurrent docker-compose for aider shows the following entrypoint\n\n\n    entrypoint: [\"/root/.aider/start_aider.sh\"]\n\n\nand gives this failure \n\nharbor.aider   | /bin/bash: /root/.aider/start_aider.sh: Permission denied\nharbor.aider exited with code 126\n\nif you install via pip - this is a nightmare to fix. \n\nfor some reason its not mounting the volume\n\n",
      "state": "closed",
      "author": "puccaso",
      "author_type": "User",
      "created_at": "2025-01-26T23:06:06Z",
      "updated_at": "2025-02-02T19:46:37Z",
      "closed_at": "2025-02-01T18:58:54Z",
      "labels": [
        "bug",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/111/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/111",
      "api_url": "https://api.github.com/repos/av/harbor/issues/111",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:18.104410",
      "comments": [
        {
          "author": "av",
          "body": "Hi ðŸ‘‹ \n\nThanks for the positive feedback and for trying it out! Aider and some other services do have permission-related conflicts, could you please try:\n```bash\nharbor fixfs \n```\nIt'll normalise the permissions across locations managed by Harbor. Please let me know if that helps, I'll add a note to ",
          "created_at": "2025-01-27T00:47:18Z"
        },
        {
          "author": "puccaso",
          "body": "Hello av, \n\n```\n$> harbor fixfs\nCreating .env file...\nsetfacl: /home/user/.parllama: No such file or directory\nsetfacl: /home/user/.config/fabric: No such file or directory\nsetfacl: /home/user/.cache/txtai: No such file or directory\nsetfacl: /home/user/.cache/nexa: No such file or directory\n\n\n$> har",
          "created_at": "2025-01-27T23:22:52Z"
        },
        {
          "author": "av",
          "body": "Thanks for the follow up, I finally got to debug this. It appears to only be happening on Mac OS.",
          "created_at": "2025-02-01T12:30:00Z"
        },
        {
          "author": "av",
          "body": "Sorry, in your specific instance it's likely not MacOS, but the reason is likely to be the same/similar - you're running on ARM, right?",
          "created_at": "2025-02-01T13:36:18Z"
        },
        {
          "author": "av",
          "body": "Ok, I got to the bottom of it, it's not specific to ARM, base Aider image was updated to stop using `root` as the default user - both for ARM and AMD64, so Harbor's path related to the `root` user being default became incorrect.\n\nI've switched relevant places to use the new `appuser` by default, it'",
          "created_at": "2025-02-01T13:54:28Z"
        }
      ]
    },
    {
      "issue_number": 92,
      "title": "MACOS : Unexpected error: No such file or directory (os error 2)",
      "body": "Hello,\r\n\r\nI can't get HARBOR services, here's what I have to display :\r\n![Capture dâ€™eÌcran 2024-12-25 aÌ€ 16 39 01](https://github.com/user-attachments/assets/1144f18d-8ea4-467c-bd03-9071a7a79003)\r\n\r\n![Capture dâ€™eÌcran 2024-12-25 aÌ€ 16 39 09](https://github.com/user-attachments/assets/2c9de47e-c427-4b7f-b24c-b6a4c67bf552)\r\n![Capture dâ€™eÌcran 2024-12-25 aÌ€ 16 39 19](https://github.com/user-attachments/assets/5ea58e50-1fd3-47d6-b446-f6aaa8eb9d2d)\r\n\r\n![Capture dâ€™eÌcran 2024-12-25 aÌ€ 16 39 27](https://github.com/user-attachments/assets/fd010bec-64e5-41d4-bde6-03a82d030853)\r\nIf it's a bug, then sorry for the programmers...",
      "state": "closed",
      "author": "DaSNyB",
      "author_type": "User",
      "created_at": "2024-12-25T15:43:25Z",
      "updated_at": "2025-02-01T19:54:01Z",
      "closed_at": "2025-02-01T19:54:00Z",
      "labels": [
        "documentation",
        "OS:MacOS"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/92/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/92",
      "api_url": "https://api.github.com/repos/av/harbor/issues/92",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:18.330006",
      "comments": [
        {
          "author": "av",
          "body": "Hey, thanks for trying out Harbor!\r\n\r\nFrom the first glance it looks like you might be running the app [without the CLI installed](https://github.com/av/harbor/wiki/1.0.-Installing-Harbor#harbor-app), which causes the (os error 2) errors. When the CLI installed, MacOS version might still have issues",
          "created_at": "2024-12-25T19:30:17Z"
        },
        {
          "author": "av",
          "body": "Closing, but please feel free to follow up or open a new one if needed",
          "created_at": "2025-02-01T19:54:00Z"
        }
      ]
    },
    {
      "issue_number": 28,
      "title": "[Request] Direct Windows Support",
      "body": "Hi there, is it possible for you to support windows directly.\r\nOllama has it on windows.\r\nIf possible in the future an installation script shall be perfect.\r\n\r\nThanks for the project.",
      "state": "closed",
      "author": "FlowDownTheRiver",
      "author_type": "User",
      "created_at": "2024-09-23T11:44:23Z",
      "updated_at": "2025-02-01T19:52:23Z",
      "closed_at": "2025-02-01T19:51:52Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/28/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/28",
      "api_url": "https://api.github.com/repos/av/harbor/issues/28",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:18.544571",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thank you for your interest in Harbor! \r\nDirect support for Windows (outside of running in WSL2) is not planned, as most of the Harbor relies on a POSIX compliant unix shell atm. I have plans on making it more portable, but it's very far away and likely along with a full rewrite ",
          "created_at": "2024-09-23T11:48:55Z"
        },
        {
          "author": "FlowDownTheRiver",
          "body": "@av Thank you very much anyway. This is a great project. Would like to see how it goes in the future. \r\nTake what your priorities to the front... However I wish that was possible...",
          "created_at": "2024-09-23T12:12:22Z"
        },
        {
          "author": "av",
          "body": "We have [a partial support by now](https://github.com/av/harbor/releases/tag/v0.2.24) - via WSL2, there are plans towards having a native CLI that should theoretically open up a door towards an app-like install for the Harbor in the future, we'll see if we ever get there",
          "created_at": "2025-02-01T19:51:52Z"
        }
      ]
    },
    {
      "issue_number": 71,
      "title": "[Request] Regarding Compatibility of Visual Tree of Thoughts function with OpenWebUI and Litellm ",
      "body": "Reddit Discussion = https://www.reddit.com/r/LocalLLaMA/comments/1fnjnm0/visual_tree_of_thoughts_for_webui/\r\n\r\nCode = https://pastebin.com/QuyrcqZC   \r\n\r\nThis is what i found in the reddit discussion which i mentioned above which makes the openwebui Visual Tree of Thoughts PIPE function compatible with openai api endpoint which means we can use any model through litellm and this function would work on those models too and it did too and i was able to see several of my litellm routed proprietary llm's like google gemini family models and anthropic models and groq models and it created clone of orignal litellm models with 'mcts' prefix as you can see in below images :\r\n\r\n![image](https://github.com/user-attachments/assets/6b99d42f-3911-4b28-be33-c4fd059258d2)\r\n![image](https://github.com/user-attachments/assets/69480abf-a91d-4c5d-b7ff-ca4e382d7538)\r\n\r\nBut when i use those mcts models they give me this error  -` 'Depends' object has no attribute 'name'`\r\n\r\n![image](https://github.com/user-attachments/assets/4f21d2b3-f428-4f66-a5d0-6ff5beb5d5bb)\r\n\r\n\r\nThis i guess is related to how function works in my opinion correct me if i am wrong . Function only has 'name' attribute which i guess is not understandable by Litellm because of which it is not processing the request ? so either function working has to be modified or litellm forking and modifying some stuff can be the solution . This much is what i can think of so if it is possible for you to fix it then do guide me and other openweb ui users on this so that we can use Visual Tree of Thoughts function not just with ollama models but with other models too which are supported by Litellm.\r\n\r\nBtw thankyou very much for this awesome function , it feels great to see llm thought process in that way and we can understand alot more things on how it thinks and process info",
      "state": "closed",
      "author": "Greatz08",
      "author_type": "User",
      "created_at": "2024-10-30T03:51:14Z",
      "updated_at": "2025-02-01T19:32:55Z",
      "closed_at": "2025-02-01T19:32:54Z",
      "labels": [
        "documentation",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/71/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/71",
      "api_url": "https://api.github.com/repos/av/harbor/issues/71",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:18.991898",
      "comments": [
        {
          "author": "av",
          "body": "Hey, thanks for reaching out! Always nice to see someone from r/LocalLLaMa here :) \r\n\r\nOne huge issue with the original `mcts` was the fact it only works with Ollama backend in the WebUI ([see here](https://github.com/av/harbor/blob/main/open-webui/extras/mcts.py#L26))\r\n\r\nUnfortunately Ollama/OpenAI",
          "created_at": "2024-11-02T14:10:10Z"
        },
        {
          "author": "Greatz08",
          "body": "@av  I have tried first workarounds and it wont work as i mentioned in my long message . In starting i mentioned Code link which redirect to same pastebin link which you mentioned in your replied :-)) . Screenshots which i added showed clearly the error i faced when using that patched version , So c",
          "created_at": "2024-11-05T01:58:01Z"
        },
        {
          "author": "av",
          "body": "Harbor [v0.2.22](https://github.com/av/harbor/releases/tag/v0.2.22) had an update for MCTS (compatible with at least v0.5.4). This is a very old issue, so I don't assume you're likely to actively test this, so closing as is, but please feel free to open a new one or follow up if needed",
          "created_at": "2025-02-01T19:32:54Z"
        }
      ]
    },
    {
      "issue_number": 106,
      "title": "Necessary comfyui models not installed for webui",
      "body": "```\nharbor.comfyui  | got prompt\nharbor.comfyui  | Failed to validate prompt for output 9:\nharbor.comfyui  | * UNETLoader 12:\nharbor.comfyui  |   - Value not in list: unet_name: 'flux1-dev.safetensors' not in []\nharbor.comfyui  | * DualCLIPLoader 11:\nharbor.comfyui  |   - Value not in list: clip_name1: 't5xxl_fp16.safetensors' not in []\nharbor.comfyui  |   - Value not in list: clip_name2: 'clip_l.safetensors' not in []\nharbor.comfyui  | * VAELoader 10:\nharbor.comfyui  |   - Value not in list: vae_name: 'ae.safetensors' not in []\nharbor.comfyui  | Output will be ignored\n```\n\nTo work through this, I manually exported the \"ComfyUI Workflow\" JSON provided by harbor and dragged it into comfyui to try to satisfy the dependencies. It's not directly possible. For example, `ae.safetensors` installed through the model manager in comfyui is `FLUX1/ae.safetensors`. I assume the others will be similar (e.g. `t5`). There isn't a direct `flux1-dev.safetensors` in the model list.",
      "state": "open",
      "author": "bjj",
      "author_type": "User",
      "created_at": "2025-01-24T09:05:41Z",
      "updated_at": "2025-02-01T19:28:19Z",
      "closed_at": null,
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/106/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bjj"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/106",
      "api_url": "https://api.github.com/repos/av/harbor/issues/106",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:19.227138",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for another report ðŸ‘ \n\nHarbor pre-configures `comfyui` container to run a provisioning script:\nhttps://github.com/av/harbor/blob/main/profiles/default.env#L268-L269\n\nIt should theoretically download necessary models as well as pre-install the default workflow for WebUI, however it's also enti",
          "created_at": "2025-01-24T12:21:55Z"
        },
        {
          "author": "bjj",
          "body": "I can confirm that the provisioning script ran. Here are the two copies of `t5xxl` on my system:\n\n```\nroot@4ceeaaa86d13:/var/log# find / -name t5xxl_fp16.safetensors\n/workspace/ComfyUI/models/text_encoders/t5/t5xxl_fp16.safetensors\n/workspace/storage/stable_diffusion/models/clip/t5xxl_fp16.safetenso",
          "created_at": "2025-01-24T16:51:31Z"
        },
        {
          "author": "bjj",
          "body": "If I run `/opt/ai-dock/storage_monitor/bin/storage-monitor` it makes symlinks, which made me realize that there appear to be two different comfyui installations in this container, one at `/opt/ComfyUI` and one in `/workspace/ComfyUI`. The `storage-manager` script made links in `/opt/ComfyUI` but the",
          "created_at": "2025-01-25T04:03:15Z"
        },
        {
          "author": "av",
          "body": "Ok, I got to the testing of the setup from scratch and noted a few issues that I missed previously:\n- The initial download is even lengthier than I remember it - I think that interrupting it might be the reason initial start didn't work for you, [added a note above \"Starting\" docs on that](https://g",
          "created_at": "2025-01-25T10:59:48Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.2.25](https://github.com/av/harbor/releases/tag/v0.2.25)",
          "created_at": "2025-01-25T12:37:13Z"
        }
      ]
    },
    {
      "issue_number": 105,
      "title": "open-webui needs embedding to do RAG, including searxng search",
      "body": "The default embedding is not installed by default. I was able to make searxng work with no additional config besides `harbor ollama pull mxbai-embed-large:latest` (based on the default RAG embedding config)",
      "state": "closed",
      "author": "bjj",
      "author_type": "User",
      "created_at": "2025-01-24T06:35:50Z",
      "updated_at": "2025-02-01T19:24:42Z",
      "closed_at": "2025-02-01T19:24:40Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/105/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bjj"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/105",
      "api_url": "https://api.github.com/repos/av/harbor/issues/105",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:19.508514",
      "comments": [
        {
          "author": "av",
          "body": "Hi ðŸ‘‹, thanks for trying out Harbor!\n\nIndeed, this part doesn't work quite as advertised right now. \n\nI'm thinking about adding `OLLAMA_DEFAULT_MODEL` and `OLLAMA_DEFAULT_EMBEDDINGS_MODEL` config options + custom entrypoint that'll pre-pull those during ollama start + being able to inject them into o",
          "created_at": "2025-01-24T11:41:32Z"
        },
        {
          "author": "av",
          "body": "I ended up with a slightly different approach, with a single `HARBOR_OLLAMA_DEFAULT_MODELS` config, but the gist is the same - there's a sidecar service that'll pre-pull these models whenever `ollama` service runs, ensuring that they are available for dependent services (to be released shortly)",
          "created_at": "2025-01-25T12:17:32Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.2.25](https://github.com/av/harbor/releases/tag/v0.2.25)",
          "created_at": "2025-01-25T12:36:55Z"
        },
        {
          "author": "av",
          "body": "Please feel free to open a new one of follow up if needed",
          "created_at": "2025-02-01T19:24:40Z"
        }
      ]
    },
    {
      "issue_number": 102,
      "title": "Ability to add Multiple Dify.AI Models into OpenWebui",
      "body": "Currently, from what I can tell we can only add one model from Dify.ai into the OpenWebui Connections. It would be great/useful if we can add multiple",
      "state": "closed",
      "author": "ZacharyKehlGEAppliances",
      "author_type": "User",
      "created_at": "2025-01-17T16:39:20Z",
      "updated_at": "2025-02-01T19:24:19Z",
      "closed_at": "2025-02-01T19:24:18Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/102/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/102",
      "api_url": "https://api.github.com/repos/av/harbor/issues/102",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:19.761798",
      "comments": [
        {
          "author": "ZacharyKehlGEAppliances",
          "body": "I think something like this would work?\n\nNot sure, can create PR to test or if there is a better idea...\n\n```\n# compose.dify.yml  \ndify-weaviate:\n    image: semitechnologies/weaviate:${HARBOR_DIFY_WEAVIATE_VERSION}\n    container_name: ${HARBOR_CONTAINER_PREFIX}.dify-weaviate\n    volumes:\n      - ${H",
          "created_at": "2025-01-17T16:53:30Z"
        },
        {
          "author": "av",
          "body": "Hey ðŸ‘‹ thanks for the report!\n\nIndeed, it's currently only possible to configure a single specific workflow to be proxied.\n\nThe whole thing setup in such a way out of a few things:\n- Dify doesn't have an OpenAI-compatible API, so I had to write a proxy on my own, none of the existing ones were workin",
          "created_at": "2025-01-18T12:43:13Z"
        },
        {
          "author": "av",
          "body": "I hope that the answer above adresses the issue, please feel free to open a new one or follow up if needed",
          "created_at": "2025-02-01T19:24:18Z"
        }
      ]
    },
    {
      "issue_number": 117,
      "title": "Service: Bolt Â» Unresponsive frontend, several minutes until first GET & workerd \"Connection\" error",
      "body": "# Harbor / Bolt / Bug Reporting \n## ðŸ–¥ï¸ Setup\n\n### System\n\n- Computer: Apple M3 Max Macbook Pro\n- RAM: 128 GB\n- OS: macOS Sequoia 15.3\n\n---\n\n### Docker\n\n- CPU limit: 16\n- Memory limit: 96 GB\n- Swap: 4 GB\n- Disk Usage Limit: 128 GB\n- Engine: v27.4.0\n- Docker Desktop 4.37.2 (179585)\n\n---\n\n### Harbor\n\n- Default settings\n- App: Version 0.2.25 (20250126.003743)\n\n\tâœ… Docker is installed and running\n\tâœ… Docker Compose (v2) is installed\n\tâœ… Docker Compose (v2) version is newer than 2.23.1\n\tâœ… Harbor home: /Users/(UsernameXY)/harbor\n\tâœ… Default profile exists and is readable\n\tâœ… Current profile (.env) exists and is readable\n\tâœ… CLI is linked\n\tâŒ NVIDIA GPU is not available. NVIDIA GPU support may not work.\n\tâŒ NVIDIA Container Toolkit is not installed. NVIDIA GPU support may not work\n\n---\n\n## ðŸª² Bug / Additional Info\n\nServices started via Harbor App.\nContainer is started, but Bolt frontend takes long time from init until first GET (133,13 seconds), frontend is shown after additional 2 minutes â€“ components react fast, but page GET stays slow.\n\n* harbor.bolt (running)\n\t* ERROR] workerd/jsg/util.c++:276: error: e = kj/async-io-unix.c++:1524: overloaded: connect(): Connection timed out\n\t- Local server slow (frontend stays unreachable) ðŸ‘‰ first `GET / 200 OK` after 133.130 ms\n- Tried to set OLLAMA_API_BASE_URL as shown in Wiki ðŸ‘‰ no difference in above behavior\n\nDefault settings. No additional changes.\n\n---\n## ðŸ“’ Logs\n### harbor.bolt ðŸ« \n\nBolt service needs around 3 minutes from init to first GET\n\n``` bash\n2025-01-31 17:36:26 > bolt@ build /app\n2025-01-31 17:36:26 > remix vite:build\n2025-01-31 17:36:26 \n2025-01-31 17:36:29 vite v5.3.1 building for production...\n2025-01-31 17:36:29 transforming...\n2025-01-31 17:36:36 âœ“ 1583 modules transformed.\n2025-01-31 17:36:37 rendering chunks...\n2025-01-31 17:36:37 computing gzip size...\n2025-01-31 17:36:37 build/client/.vite/manifest.json                            130.22 kB â”‚ gzip:  10.12 kB\n2025-01-31 17:36:37 build/client/assets/tailwind-compat-CC20SAMN.css              2.25 kB â”‚ gzip:   1.00 kB\n2025-01-31 17:36:37 build/client/assets/_index-D_NZK3VS.css                       2.61 kB â”‚ gzip:   0.96 kB\n2025-01-31 17:36:37 build/client/assets/xterm-lQO2bNqs.css                        4.08 kB â”‚ gzip:   1.67 kB\n2025-01-31 17:36:37 build/client/assets/ReactToastify-CYivYX3d.css               14.19 kB â”‚ gzip:   2.71 kB\n2025-01-31 17:36:36 Generated an empty chunk: \"api.enhancer\".\n2025-01-31 17:36:36 Generated an empty chunk: \"api.models\".\n2025-01-31 17:36:36 Generated an empty chunk: \"api.chat\".\n2025-01-31 17:36:37 build/client/assets/index-CPTzpSUP.css                       17.01 kB â”‚ gzip:   2.78 kB\n2025-01-31 17:36:37 build/client/assets/root-kD2MswjW.css                        53.84 kB â”‚ gzip:  10.37 kB\n2025-01-31 17:36:37 build/client/assets/api.enhancer-l0sNRNKZ.js                  0.00 kB â”‚ gzip:   0.02 kB\n2025-01-31 17:36:37 build/client/assets/api.chat-l0sNRNKZ.js                      0.00 kB â”‚ gzip:   0.02 kB\n2025-01-31 17:36:37 build/client/assets/api.models-l0sNRNKZ.js                    0.00 kB â”‚ gzip:   0.02 kB\n2025-01-31 17:36:37 build/client/assets/chat._id-CwxbN1RB.js                      0.12 kB â”‚ gzip:   0.12 kB\n2025-01-31 17:36:37 build/client/assets/_index-_Urxur0L.js                        0.14 kB â”‚ gzip:   0.13 kB\n2025-01-31 17:36:37 build/client/assets/codeowners-CGmujMTu.js                    0.48 kB â”‚ gzip:   0.30 kB\n2025-01-31 17:36:37 build/client/assets/tsv-DnLUQrgA.js                           0.63 kB â”‚ gzip:   0.33 kB\n2025-01-31 17:36:37 build/client/assets/shellsession-4WldjxEd.js                  0.68 kB â”‚ gzip:   0.43 kB\n2025-01-31 17:36:37 build/client/assets/qmldir-MS3qTAOR.js                        0.88 kB â”‚ gzip:   0.43 kB\n2025-01-31 17:36:37 build/client/assets/git-rebase-CtmYztAk.js                    0.89 kB â”‚ gzip:   0.41 kB\n2025-01-31 17:36:37 build/client/assets/html-derivative-CvQVKW9i.js               0.91 kB â”‚ gzip:   0.51 kB\n2025-01-31 17:36:37 build/client/assets/fortran-fixed-form-CdVjoHtb.js            1.01 kB â”‚ gzip:   0.53 kB\n2025-01-31 17:36:37 build/client/assets/csv-B2DkETJQ.js                           1.04 kB â”‚ gzip:   0.35 kB\n2025-01-31 17:36:37 build/client/assets/theme-C4y9B7zS.js                         1.22 kB â”‚ gzip:   0.67 kB\n2025-01-31 17:36:37 build/client/assets/xsl-bm8RKyr3.js                           1.26 kB â”‚ gzip:   0.50 kB\n2025-01-31 17:36:37 build/client/assets/ini-DeVv6D4_.js                           1.36 kB â”‚ gzip:   0.48 kB\n2025-01-31 17:36:37 build/client/assets/git-commit-BleTlbuD.js                    1.42 kB â”‚ gzip:   0.64 kB\n2025-01-31 17:36:37 build/client/assets/sparql-BgtC_-ln.js                        1.43 kB â”‚ gzip:   0.81 kB\n2025-01-31 17:36:37 build/client/assets/docker-DNR26wTC.js                        1.52 kB â”‚ gzip:   0.56 kB\n2025-01-31 17:36:37 build/client/assets/wenyan-D3VXSfF0.js                        1.55 kB â”‚ gzip:   1.09 kB\n2025-01-31 17:36:37 build/client/assets/hxml-C5imjkyf.js                          1.72 kB â”‚ gzip:   0.88 kB\n2025-01-31 17:36:37 build/client/assets/desktop-D71BffLY.js                       1.81 kB â”‚ gzip:   0.76 kB\n2025-01-31 17:36:37 build/client/assets/berry-CxrokwfH.js                         2.08 kB â”‚ gzip:   0.77 kB\n2025-01-31 17:36:37 build/client/assets/reg-CrhH3_Og.js                           2.15 kB â”‚ gzip:   0.69 kB\n2025-01-31 17:36:37 build/client/assets/erb-C4FIxXpw.js                           2.16 kB â”‚ gzip:   0.71 kB\n2025-01-31 17:36:37 build/client/assets/diff-DvyTQcux.js                          2.38 kB â”‚ gzip:   0.71 kB\n2025-01-31 17:36:37 build/client/assets/gleam-Dd6f7Z5P.js                         2.44 kB â”‚ gzip:   0.83 kB\n2025-01-31 17:36:37 build/client/assets/index-DHbqgi-g.js                         2.48 kB â”‚ gzip:   1.53 kB\n2025-01-31 17:36:37 build/client/assets/log-Ksn5IXup.js                           2.50 kB â”‚ gzip:   0.86 kB\n2025-01-31 17:36:37 build/client/assets/json-CupVZNk8.js                          2.64 kB â”‚ gzip:   0.80 kB\n2025-01-31 17:36:37 build/client/assets/index-DgVbUpA9.js                         2.69 kB â”‚ gzip:   1.54 kB\n2025-01-31 17:36:37 build/client/assets/jssm-Dble9ECP.js                          2.74 kB â”‚ gzip:   0.68 kB\n2025-01-31 17:36:37 build/client/assets/jsonl-BUpeXbsf.js                         2.83 kB â”‚ gzip:   0.82 kB\n2025-01-31 17:36:37 build/client/assets/jsonc-DYI1rfmx.js                         2.93 kB â”‚ gzip:   0.82 kB\n2025-01-31 17:36:37 build/client/assets/po-HrnDn_2Q.js                            2.97 kB â”‚ gzip:   0.96 kB\n2025-01-31 17:36:37 build/client/assets/tasl-BxwAa5i0.js                          3.03 kB â”‚ gzip:   0.85 kB\n2025-01-31 17:36:37 build/client/assets/genie-DAfrLhwG.js                         3.05 kB â”‚ gzip:   1.19 kB\n2025-01-31 17:36:37 build/client/assets/vala-CO5hpdkB.js                          3.08 kB â”‚ gzip:   1.17 kB\n2025-01-31 17:36:37 build/client/assets/rel-6Kuza3Wr.js                           3.08 kB â”‚ gzip:   1.10 kB\n2025-01-31 17:36:37 build/client/assets/hy-C3qJFuQy.js                            3.08 kB â”‚ gzip:   1.58 kB\n2025-01-31 17:36:37 build/client/assets/logo-DdacRhvC.js                          3.09 kB â”‚ gzip:   1.46 kB\n2025-01-31 17:36:37 build/client/assets/json5-Bh8mriwU.js                         3.20 kB â”‚ gzip:   0.93 kB\n2025-01-31 17:36:37 build/client/assets/jsonnet-C9d3aiqh.js                       3.25 kB â”‚ gzip:   1.06 kB\n2025-01-31 17:36:37 build/client/assets/fluent-KPqz0Sb3.js                        3.32 kB â”‚ gzip:   0.87 kB\n2025-01-31 17:36:37 build/client/assets/narrat-B9CT-1u6.js                        3.33 kB â”‚ gzip:   1.10 kB\n2025-01-31 17:36:37 build/client/assets/turtle-C15OxdQ5.js                        3.36 kB â”‚ gzip:   0.95 kB\n2025-01-31 17:36:37 build/client/assets/ssh-config-BH1M7C1g.js                    3.46 kB â”‚ gzip:   1.57 kB\n2025-01-31 17:36:37 build/client/assets/splunk-6XBPEST2.js                        3.65 kB â”‚ gzip:   1.63 kB\n2025-01-31 17:36:37 build/client/assets/smalltalk-DSsji4Hu.js                     3.81 kB â”‚ gzip:   1.22 kB\n2025-01-31 17:36:37 build/client/assets/glsl-DNg5e6rY.js                          3.84 kB â”‚ gzip:   1.42 kB\n2025-01-31 17:36:37 build/client/assets/bicep-3ghuYFLd.js                         3.87 kB â”‚ gzip:   1.05 kB\n2025-01-31 17:36:37 build/client/assets/pascal-BvCdDh2R.js                        3.90 kB â”‚ gzip:   1.65 kB\n2025-01-31 17:36:37 build/client/assets/zenscript-B1nm99XP.js                     3.93 kB â”‚ gzip:   1.40 kB\n2025-01-31 17:36:37 build/client/assets/entry.client-ydwSl9ol.js                  4.06 kB â”‚ gzip:   1.53 kB\n2025-01-31 17:36:37 build/client/assets/http-mWhOz2-8.js                          4.14 kB â”‚ gzip:   1.11 kB\n2025-01-31 17:36:37 build/client/assets/nextflow-DBxHOdLe.js                      4.30 kB â”‚ gzip:   1.24 kB\n2025-01-31 17:36:37 build/client/assets/fennel-C6XIsc4F.js                        4.48 kB â”‚ gzip:   1.53 kB\n2025-01-31 17:36:37 build/client/assets/bibtex-BaedD2tq.js                        4.49 kB â”‚ gzip:   0.78 kB\n2025-01-31 17:36:37 build/client/assets/tcl-C_8Fx7bH.js                           4.52 kB â”‚ gzip:   1.67 kB\n2025-01-31 17:36:37 build/client/assets/qml-DcMLa_hy.js                           4.79 kB â”‚ gzip:   1.35 kB\n2025-01-31 17:36:37 build/client/assets/gdresource-nSffpn2A.js                    4.86 kB â”‚ gzip:   1.34 kB\n2025-01-31 17:36:37 build/client/assets/zig-xtV5iK4E.js                           4.90 kB â”‚ gzip:   1.57 kB\n2025-01-31 17:36:37 build/client/assets/xml-KWQaRJyt.js                           4.91 kB â”‚ gzip:   1.19 kB\n2025-01-31 17:36:37 build/client/assets/awk-i0IPvypD.js                           4.97 kB â”‚ gzip:   1.37 kB\n2025-01-31 17:36:37 build/client/assets/fish-DfeQjIbs.js                          5.02 kB â”‚ gzip:   1.69 kB\n2025-01-31 17:36:37 build/client/assets/jinja-DwshyRC8.js                         5.14 kB â”‚ gzip:   1.39 kB\n2025-01-31 17:36:37 build/client/assets/powerquery-CApMHEaB.js                    5.30 kB â”‚ gzip:   1.49 kB\n2025-01-31 17:36:37 build/client/assets/dax-DsfXcHUZ.js                           5.32 kB â”‚ gzip:   2.29 kB\n2025-01-31 17:36:37 build/client/assets/verilog-DVfdqzEq.js                       5.44 kB â”‚ gzip:   1.85 kB\n2025-01-31 17:36:37 build/client/assets/prisma-BBJYjQ0k.js                        5.51 kB â”‚ gzip:   1.34 kB\n2025-01-31 17:36:37 build/client/assets/gdshader-B_SUYfiV.js                      5.73 kB â”‚ gzip:   1.72 kB\n2025-01-31 17:36:37 build/client/assets/proto-DOtRmeXT.js                         5.77 kB â”‚ gzip:   1.38 kB\n2025-01-31 17:36:37 build/client/assets/vb-beD-FUib.js                            5.79 kB â”‚ gzip:   2.33 kB\n2025-01-31 17:36:37 build/client/assets/red-jaXbsbtS.js                           5.90 kB â”‚ gzip:   1.55 kB\n2025-01-31 17:36:37 build/client/assets/wgsl-BZz1Hhek.js                          5.90 kB â”‚ gzip:   1.67 kB\n2025-01-31 17:36:37 build/client/assets/shaderlab-DBpbMEBh.js                     5.90 kB â”‚ gzip:   2.08 kB\n2025-01-31 17:36:37 build/client/assets/toml-BT9ZzGyQ.js                          5.91 kB â”‚ gzip:   1.36 kB\n2025-01-31 17:36:37 build/client/assets/postcss-DXT9h7v2.js                       5.96 kB â”‚ gzip:   1.89 kB\n2025-01-31 17:36:37 build/client/assets/soy-DhLnWwFA.js                           6.11 kB â”‚ gzip:   1.64 kB\n2025-01-31 17:36:37 build/client/assets/min-dark-iSbrOpM4.js                      6.12 kB â”‚ gzip:   1.70 kB\n2025-01-31 17:36:37 build/client/assets/clojure-BF6G6X0H.js                       6.14 kB â”‚ gzip:   1.46 kB\n2025-01-31 17:36:37 build/client/assets/solarized-light-xPNGhBYe.js               6.15 kB â”‚ gzip:   1.69 kB\n2025-01-31 17:36:37 build/client/assets/cypher-ByMv4Xf1.js                        6.21 kB â”‚ gzip:   1.81 kB\n2025-01-31 17:36:37 build/client/assets/solarized-dark-C86elO-m.js                6.52 kB â”‚ gzip:   1.77 kB\n2025-01-31 17:36:37 build/client/assets/ara-CG4fK2Nq.js                           6.56 kB â”‚ gzip:   2.11 kB\n2025-01-31 17:36:37 build/client/assets/dart-DZLoTQm4.js                          6.64 kB â”‚ gzip:   1.83 kB\n2025-01-31 17:36:37 build/client/assets/min-light-BITGhEdf.js                     6.81 kB â”‚ gzip:   1.86 kB\n2025-01-31 17:36:37 build/client/assets/riscv-AgnqFTPX.js                         6.91 kB â”‚ gzip:   2.20 kB\n2025-01-31 17:36:37 build/client/assets/hlsl-DmDrTTlz.js                          7.16 kB â”‚ gzip:   2.22 kB\n2025-01-31 17:36:37 build/client/assets/systemd-CuJfdYLG.js                       7.26 kB â”‚ gzip:   2.53 kB\n2025-01-31 17:36:37 build/client/assets/qss-FJDVp-XM.js                           7.43 kB â”‚ gzip:   2.57 kB\n2025-01-31 17:36:37 build/client/assets/regexp-C_ZPRiAj.js                        7.46 kB â”‚ gzip:   1.48 kB\n2025-01-31 17:36:37 build/client/assets/monokai-sMI-pExk.js                       7.47 kB â”‚ gzip:   1.86 kB\n2025-01-31 17:36:37 build/client/assets/typst-I4qd5QHW.js                         7.62 kB â”‚ gzip:   1.68 kB\n2025-01-31 17:36:37 build/client/assets/haml-BsKmeTIz.js                          7.80 kB â”‚ gzip:   2.00 kB\n2025-01-31 17:36:37 build/client/assets/scheme-1Je9_Reo.js                        7.92 kB â”‚ gzip:   2.64 kB\n2025-01-31 17:36:37 build/client/assets/kotlin-BIxS-Weu.js                        7.92 kB â”‚ gzip:   2.10 kB\n2025-01-31 17:36:37 build/client/assets/plsql-BbJj1K1w.js                         7.98 kB â”‚ gzip:   2.96 kB\n2025-01-31 17:36:37 build/client/assets/make-B9S9BZZh.js                          8.14 kB â”‚ gzip:   1.74 kB\n2025-01-31 17:36:37 build/client/assets/vue-html-BQKlc1uT.js                      8.34 kB â”‚ gzip:   1.92 kB\n2025-01-31 17:36:37 build/client/assets/dark-plus-KEYLhlmT.js                     8.47 kB â”‚ gzip:   2.03 kB\n2025-01-31 17:36:37 build/client/assets/sass-CMDmr8et.js                          8.52 kB â”‚ gzip:   2.46 kB\n2025-01-31 17:36:37 build/client/assets/slack-dark-C7oZ9nno.js                    8.69 kB â”‚ gzip:   1.94 kB\n2025-01-31 17:36:37 build/client/assets/tex-Dk885XYG.js                           8.71 kB â”‚ gzip:   2.89 kB\n2025-01-31 17:36:37 build/client/assets/andromeeda-YxQm0tCS.js                    8.75 kB â”‚ gzip:   2.30 kB\n2025-01-31 17:36:37 build/client/assets/jison-5Ig0VrXv.js                         8.76 kB â”‚ gzip:   1.84 kB\n2025-01-31 17:36:37 build/client/assets/slack-ochin-hXH8Gyq8.js                   9.09 kB â”‚ gzip:   2.07 kB\n2025-01-31 17:36:37 build/client/assets/rst-5AUMhdsS.js                           9.23 kB â”‚ gzip:   2.30 kB\n2025-01-31 17:36:37 build/client/assets/sas-WQZNIjpJ.js                           9.33 kB â”‚ gzip:   4.05 kB\n2025-01-31 17:36:37 build/client/assets/light-plus-BsvsQ1iS.js                    9.33 kB â”‚ gzip:   2.20 kB\n2025-01-31 17:36:37 build/client/assets/dream-maker-Eh5U-gDp.js                   9.54 kB â”‚ gzip:   2.34 kB\n2025-01-31 17:36:37 build/client/assets/beancount-Urb1RsFe.js                     9.64 kB â”‚ gzip:   1.49 kB\n2025-01-31 17:36:37 build/client/assets/gherkin-DjTlIhuc.js                       9.89 kB â”‚ gzip:   5.11 kB\n2025-01-31 17:36:37 build/client/assets/cadence-Bgpqy2XC.js                       9.89 kB â”‚ gzip:   2.27 kB\n2025-01-31 17:36:37 build/client/assets/raku-D384ylkT.js                         10.02 kB â”‚ gzip:   2.93 kB\n2025-01-31 17:36:37 build/client/assets/yaml-C5gCGmDW.js                         10.08 kB â”‚ gzip:   2.32 kB\n2025-01-31 17:36:37 build/client/assets/elm-CTSLo1i4.js                          10.09 kB â”‚ gzip:   2.18 kB\n2025-01-31 17:36:37 build/client/assets/cmake-DXZpi2gR.js                        10.13 kB â”‚ gzip:   3.56 kB\n2025-01-31 17:36:37 build/client/assets/hcl-CpAANOdC.js                          10.67 kB â”‚ gzip:   2.45 kB\n2025-01-31 17:36:37 build/client/assets/github-light-CRlnGVMD.js                 10.84 kB â”‚ gzip:   2.48 kB\n2025-01-31 17:36:37 build/client/assets/puppet-37ic6j3l.js                       10.85 kB â”‚ gzip:   2.25 kB\n2025-01-31 17:36:37 build/client/assets/prolog-BH_RS3WO.js                       11.04 kB â”‚ gzip:   3.82 kB\n2025-01-31 17:36:37 build/client/assets/github-dark-CzPA46E-.js                  11.06 kB â”‚ gzip:   2.53 kB\n2025-01-31 17:36:37 build/client/assets/handlebars-C0vBBVRa.js                   11.12 kB â”‚ gzip:   2.38 kB\n2025-01-31 17:36:37 build/client/assets/hjson-DZqG9GXz.js                        11.58 kB â”‚ gzip:   1.84 kB\n2025-01-31 17:36:37 build/client/assets/bat-BPiaQZfK.js                          11.95 kB â”‚ gzip:   3.24 kB\n2025-01-31 17:36:37 build/client/assets/terraform-DBeuZS66.js                    12.07 kB â”‚ gzip:   3.08 kB\n2025-01-31 17:36:37 build/client/assets/v-C2TBxDwV.js                            12.13 kB â”‚ gzip:   2.80 kB\n2025-01-31 17:36:37 build/client/assets/vesper-BSB_bK09.js                       12.27 kB â”‚ gzip:   1.95 kB\n2025-01-31 17:36:37 build/client/assets/apache-rS0jd3Ly.js                       12.35 kB â”‚ gzip:   3.76 kB\n2025-01-31 17:36:37 build/client/assets/vitesse-light-Br6ll-O0.js                12.98 kB â”‚ gzip:   2.99 kB\n2025-01-31 17:36:37 build/client/assets/vitesse-black-B3g-KkBK.js                13.04 kB â”‚ gzip:   3.02 kB\n2025-01-31 17:36:37 build/client/assets/vitesse-dark-Bxkoe-BC.js                 13.12 kB â”‚ gzip:   3.02 kB\n2025-01-31 17:36:37 build/client/assets/clarity-CIekO_uJ.js                      13.21 kB â”‚ gzip:   2.44 kB\n2025-01-31 17:36:37 build/client/assets/aurora-x-BaWyeHV_.js                     13.25 kB â”‚ gzip:   2.26 kB\n2025-01-31 17:36:37 build/client/assets/actionscript-3-BigF1UXR.js               13.25 kB â”‚ gzip:   2.78 kB\n2025-01-31 17:36:37 build/client/assets/pug-BmZh5kCX.js                          13.39 kB â”‚ gzip:   2.91 kB\n2025-01-31 17:36:37 build/client/assets/synthwave-84-BBDuFDsq.js                 13.44 kB â”‚ gzip:   2.83 kB\n2025-01-31 17:36:37 build/client/assets/nix-B7rNE5kf.js                          13.64 kB â”‚ gzip:   2.31 kB\n2025-01-31 17:36:37 build/client/assets/github-light-default-UREJT2Bw.js         13.75 kB â”‚ gzip:   3.02 kB\n2025-01-31 17:36:37 build/client/assets/lua-BPmF2VOm.js                          13.85 kB â”‚ gzip:   3.13 kB\n2025-01-31 17:36:37 build/client/assets/github-dark-dimmed-CRDKj6ck.js           14.03 kB â”‚ gzip:   3.10 kB\n2025-01-31 17:36:37 build/client/assets/github-dark-default-BXF7Vm5l.js          14.03 kB â”‚ gzip:   3.10 kB\n2025-01-31 17:36:37 build/client/assets/solidity-CThH5sBG.js                     14.48 kB â”‚ gzip:   3.06 kB\n2025-01-31 17:36:37 build/client/assets/ayu-dark-Bn5gmY5k.js                     14.51 kB â”‚ gzip:   3.05 kB\n2025-01-31 17:36:37 build/client/assets/wasm-Cicx_DS6.js                         14.51 kB â”‚ gzip:   2.85 kB\n2025-01-31 17:36:37 build/client/assets/liquid-Dj-jsJFu.js                       14.86 kB â”‚ gzip:   2.90 kB\n2025-01-31 17:36:37 build/client/assets/angular-html-DZQ5UQWW.js                 14.91 kB â”‚ gzip:   3.39 kB\n2025-01-31 17:36:37 build/client/assets/purescript-CfPQhs6g.js                   14.97 kB â”‚ gzip:   2.61 kB\n2025-01-31 17:36:37 build/client/assets/cue-C6Aznpr-.js                          15.16 kB â”‚ gzip:   2.05 kB\n2025-01-31 17:36:37 build/client/assets/svelte-DavKDhWY.js                       15.60 kB â”‚ gzip:   3.05 kB\n2025-01-31 17:36:37 build/client/assets/gnuplot-nclm9rTJ.js                      15.73 kB â”‚ gzip:   3.59 kB\n2025-01-31 17:36:37 build/client/assets/rust-DGxQkqYo.js                         15.98 kB â”‚ gzip:   3.32 kB\n2025-01-31 17:36:37 build/client/assets/abap-DXFkqnOI.js                         16.05 kB â”‚ gzip:   5.98 kB\n2025-01-31 17:36:37 build/client/assets/elixir-DjCTzIqv.js                       16.33 kB â”‚ gzip:   3.17 kB\n2025-01-31 17:36:37 build/client/assets/gdscript-B474tPdy.js                     16.44 kB â”‚ gzip:   3.68 kB\n2025-01-31 17:36:37 build/client/assets/graphql-Cac5VMXA.js                      16.59 kB â”‚ gzip:   2.62 kB\n2025-01-31 17:36:37 build/client/assets/move-DOHBU7tp.js                         17.44 kB â”‚ gzip:   3.90 kB\n2025-01-31 17:36:37 build/client/assets/marko-DTTVzMRa.js                        17.61 kB â”‚ gzip:   3.18 kB\n2025-01-31 17:36:37 build/client/assets/groovy-wChcbJ1V.js                       17.70 kB â”‚ gzip:   3.76 kB\n2025-01-31 17:36:37 build/client/assets/nushell-CrgTADc5.js                      17.74 kB â”‚ gzip:   4.91 kB\n2025-01-31 17:36:37 build/client/assets/material-theme-B2BuIiKK.js               18.08 kB â”‚ gzip:   3.09 kB\n2025-01-31 17:36:37 build/client/assets/material-theme-darker-BrGg7AAd.js        18.09 kB â”‚ gzip:   3.09 kB\n2025-01-31 17:36:37 build/client/assets/material-theme-ocean-CBL0qBdF.js         18.09 kB â”‚ gzip:   3.11 kB\n2025-01-31 17:36:37 build/client/assets/material-theme-lighter-DDRuGeQH.js       18.10 kB â”‚ gzip:   3.08 kB\n2025-01-31 17:36:37 build/client/assets/material-theme-palenight-D7gg1Usp.js     18.10 kB â”‚ gzip:   3.10 kB\n2025-01-31 17:36:37 build/client/assets/glimmer-ts-61dai_V5.js                   18.32 kB â”‚ gzip:   2.94 kB\n2025-01-31 17:36:37 build/client/assets/glimmer-js-Bhx_sfM5.js                   18.32 kB â”‚ gzip:   2.95 kB\n2025-01-31 17:36:37 build/client/assets/matlab-Btshr8M_.js                       18.44 kB â”‚ gzip:   3.98 kB\n2025-01-31 17:36:37 build/client/assets/mdc-Bn_uTtpR.js                          19.16 kB â”‚ gzip:   6.66 kB\n2025-01-31 17:36:37 build/client/assets/vue-DwKRC_TW.js                          19.38 kB â”‚ gzip:   3.02 kB\n2025-01-31 17:36:37 build/client/assets/kusto-DGEpfOTx.js                        19.39 kB â”‚ gzip:   4.52 kB\n2025-01-31 17:36:37 build/client/assets/snazzy-light-CA9nliXM.js                 19.75 kB â”‚ gzip:   3.79 kB\n2025-01-31 17:36:37 build/client/assets/viml-BLluXI4E.js                         20.30 kB â”‚ gzip:   7.22 kB\n2025-01-31 17:36:37 build/client/assets/twig-llAgFoxS.js                         20.30 kB â”‚ gzip:   3.95 kB\n2025-01-31 17:36:37 build/client/assets/dracula-DGO8GyiP.js                      20.47 kB â”‚ gzip:   3.97 kB\n2025-01-31 17:36:37 build/client/assets/dracula-soft-9B1nZgL-.js                 20.48 kB â”‚ gzip:   4.00 kB\n2025-01-31 17:36:37 build/client/assets/powershell-9ZOzOPqN.js                   20.53 kB â”‚ gzip:   4.65 kB\n2025-01-31 17:36:37 build/client/assets/nim-0XdZC7BR.js                          20.93 kB â”‚ gzip:   3.47 kB\n2025-01-31 17:36:37 build/client/assets/rose-pine-CX_FIdg1.js                    21.22 kB â”‚ gzip:   3.80 kB\n2025-01-31 17:36:37 build/client/assets/rose-pine-moon-CdedUr_-.js               21.23 kB â”‚ gzip:   3.82 kB\n2025-01-31 17:36:37 build/client/assets/rose-pine-dawn-Bj5xdiaE.js               21.23 kB â”‚ gzip:   3.82 kB\n2025-01-31 17:36:37 build/client/assets/index-CmC7uV4_.js                        21.71 kB â”‚ gzip:   9.79 kB\n2025-01-31 17:36:37 build/client/assets/mermaid-Dkb1Nx48.js                      22.10 kB â”‚ gzip:   3.87 kB\n2025-01-31 17:36:37 build/client/assets/vhdl-QZ3jNtnE.js                         22.21 kB â”‚ gzip:   3.97 kB\n2025-01-31 17:36:37 build/client/assets/index-BoUgl1nU.js                        22.56 kB â”‚ gzip:  10.03 kB\n2025-01-31 17:36:37 build/client/assets/sql-DbK06e1c.js                          22.73 kB â”‚ gzip:   7.53 kB\n2025-01-31 17:36:37 build/client/assets/fsharp-C-VjhQSu.js                       22.97 kB â”‚ gzip:   4.12 kB\n2025-01-31 17:36:37 build/client/assets/common-lisp-C3qUB5O8.js                  23.25 kB â”‚ gzip:   6.31 kB\n2025-01-31 17:36:37 build/client/assets/astro-U9VgAVQT.js                        23.26 kB â”‚ gzip:   7.57 kB\n2025-01-31 17:36:37 build/client/assets/razor-CtZ59qoM.js                        23.50 kB â”‚ gzip:   3.45 kB\n2025-01-31 17:36:37 build/client/assets/apl-D9TNySCV.js                          23.76 kB â”‚ gzip:   4.74 kB\n2025-01-31 17:36:37 build/client/assets/system-verilog-BscxmKrE.js               23.84 kB â”‚ gzip:   4.81 kB\n2025-01-31 17:36:37 build/client/assets/one-light-alpzPJ78.js                    23.99 kB â”‚ gzip:   3.64 kB\n2025-01-31 17:36:37 build/client/assets/java-ClXEvkw9.js                         24.78 kB â”‚ gzip:   4.36 kB\n2025-01-31 17:36:37 build/client/assets/scss-DqznleHU.js                         24.91 kB â”‚ gzip:   4.25 kB\n2025-01-31 17:36:37 build/client/assets/typespec-DAP_O-zg.js                     25.22 kB â”‚ gzip:   2.79 kB\n2025-01-31 17:36:37 build/client/assets/coffee-B4DZ7swD.js                       25.62 kB â”‚ gzip:   6.50 kB\n2025-01-31 17:36:37 build/client/assets/julia-CnoVwgV8.js                        25.75 kB â”‚ gzip:   6.04 kB\n2025-01-31 17:36:37 build/client/assets/nord-CsyjKwr8.js                         25.83 kB â”‚ gzip:   4.36 kB\n2025-01-31 17:36:37 build/client/assets/scala-vMsNTMhM.js                        26.09 kB â”‚ gzip:   4.20 kB\n2025-01-31 17:36:37 build/client/assets/index-CSEtdpwT.js                        26.17 kB â”‚ gzip:   8.72 kB\n2025-01-31 17:36:37 build/client/assets/night-owl-BeocmOPF.js                    27.48 kB â”‚ gzip:   5.07 kB\n2025-01-31 17:36:37 build/client/assets/applescript-B4yE-MfL.js                  27.91 kB â”‚ gzip:   6.30 kB\n2025-01-31 17:36:37 build/client/assets/index-gF1nlaqG.js                        28.75 kB â”‚ gzip:  11.70 kB\n2025-01-31 17:36:37 build/client/assets/stylus-DSrLtGYv.js                       29.66 kB â”‚ gzip:   8.10 kB\n2025-01-31 17:36:37 build/client/assets/crystal-DwllXkRF.js                      30.65 kB â”‚ gzip:   5.53 kB\n2025-01-31 17:36:37 build/client/assets/one-dark-pro-BTtaZsq5.js                 31.68 kB â”‚ gzip:   5.40 kB\n2025-01-31 17:36:37 build/client/assets/haxe-Dit6kIrv.js                         31.99 kB â”‚ gzip:   5.87 kB\n2025-01-31 17:36:37 build/client/assets/codeql-BOpLLL-w.js                       32.08 kB â”‚ gzip:   4.03 kB\n2025-01-31 17:36:37 build/client/assets/nginx-Bo7Ko850.js                        32.50 kB â”‚ gzip:   4.45 kB\n2025-01-31 17:36:37 build/client/assets/poimandres-Cda-MJFk.js                   32.82 kB â”‚ gzip:   5.47 kB\n2025-01-31 17:36:37 build/client/assets/tokyo-night-eJfcURhx.js                  32.89 kB â”‚ gzip:   5.87 kB\n2025-01-31 17:36:37 build/client/assets/erlang-DS9ZWoKD.js                       33.08 kB â”‚ gzip:   4.40 kB\n2025-01-31 17:36:37 build/client/assets/houston-CZZ6oYdA.js                      33.91 kB â”‚ gzip:   5.74 kB\n2025-01-31 17:36:37 build/client/assets/r-BXfENWL6.js                            34.03 kB â”‚ gzip:  11.07 kB\n2025-01-31 17:36:37 build/client/assets/ruby-CPHW1Myo.js                         34.49 kB â”‚ gzip:   5.71 kB\n2025-01-31 17:36:37 build/client/assets/asm-PWN5J14X.js                          37.93 kB â”‚ gzip:   8.09 kB\n2025-01-31 17:36:37 build/client/assets/cobol-DgBJixdi.js                        38.34 kB â”‚ gzip:  11.06 kB\n2025-01-31 17:36:37 build/client/assets/d-PifQWv0n.js                            38.49 kB â”‚ gzip:   8.21 kB\n2025-01-31 17:36:37 build/client/assets/shellscript-BZfs-ost.js                  38.50 kB â”‚ gzip:   6.18 kB\n2025-01-31 17:36:37 build/client/assets/haskell-DAGYewaG.js                      39.07 kB â”‚ gzip:   7.13 kB\n2025-01-31 17:36:37 build/client/assets/perl-InL218rs.js                         40.08 kB â”‚ gzip:   4.78 kB\n2025-01-31 17:36:37 build/client/assets/apex-Sfo2eW0G.js                         42.54 kB â”‚ gzip:   6.74 kB\n2025-01-31 17:36:37 build/client/assets/ada-CowR2XfX.js                          42.76 kB â”‚ gzip:   5.87 kB\n2025-01-31 17:36:37 build/client/assets/index-B2IRPuJt.js                        42.99 kB â”‚ gzip:  14.81 kB\n2025-01-31 17:36:37 build/client/assets/go-BJn7Ek5W.js                           43.86 kB â”‚ gzip:   6.26 kB\n2025-01-31 17:36:37 build/client/assets/catppuccin-mocha-CEfge3mM.js             44.19 kB â”‚ gzip:   7.63 kB\n2025-01-31 17:36:37 build/client/assets/catppuccin-latte-BYdKNJ10.js             44.19 kB â”‚ gzip:   7.64 kB\n2025-01-31 17:36:37 build/client/assets/catppuccin-frappe-CSPeAESR.js            44.19 kB â”‚ gzip:   7.65 kB\n2025-01-31 17:36:37 build/client/assets/catppuccin-macchiato-DVLwECkk.js         44.20 kB â”‚ gzip:   7.64 kB\n2025-01-31 17:36:37 build/client/assets/index-C9u02mIW.js                        44.69 kB â”‚ gzip:  19.07 kB\n2025-01-31 17:36:37 build/client/assets/imba-BXg-Svbq.js                         47.16 kB â”‚ gzip:   9.92 kB\n2025-01-31 17:36:37 build/client/assets/css-CbYhyuC0.js                          47.37 kB â”‚ gzip:  12.56 kB\n2025-01-31 17:36:37 build/client/assets/markdown-BdfWgkoX.js                     50.81 kB â”‚ gzip:   5.64 kB\n2025-01-31 17:36:37 build/client/assets/latex-DhY63DBA.js                        52.73 kB â”‚ gzip:   6.19 kB\n2025-01-31 17:36:37 build/client/assets/wikitext-2Gt4HDrj.js                     53.58 kB â”‚ gzip:   5.79 kB\n2025-01-31 17:36:37 build/client/assets/ballerina-T9ysyp6P.js                    54.33 kB â”‚ gzip:   8.17 kB\n2025-01-31 17:36:37 build/client/assets/html-CqhC7HHo.js                         55.14 kB â”‚ gzip:  11.79 kB\n2025-01-31 17:36:37 build/client/assets/stata-8O9LehIm.js                        56.54 kB â”‚ gzip:  13.66 kB\n2025-01-31 17:36:37 build/client/assets/ocaml-RqY_Nz63.js                        60.33 kB â”‚ gzip:   5.09 kB\n2025-01-31 17:36:37 build/client/assets/c-RCJZWN-0.js                            67.59 kB â”‚ gzip:  10.47 kB\n2025-01-31 17:36:37 build/client/assets/mojo--7WWnkdy.js                         69.93 kB â”‚ gzip:  11.22 kB\n2025-01-31 17:36:37 build/client/assets/python-DwuVtWc2.js                       70.76 kB â”‚ gzip:  11.29 kB\n2025-01-31 17:36:37 build/client/assets/root-DSvoOs_7.js                         72.31 kB â”‚ gzip:  23.70 kB\n2025-01-31 17:36:37 build/client/assets/less-Du6_OKDb.js                         76.35 kB â”‚ gzip:  12.91 kB\n2025-01-31 17:36:37 build/client/assets/csharp-DISxKEhY.js                       78.97 kB â”‚ gzip:  10.39 kB\n2025-01-31 17:36:37 build/client/assets/hack-OhUXFOZr.js                         79.92 kB â”‚ gzip:  27.46 kB\n2025-01-31 17:36:37 build/client/assets/index-BP5Fp6so.js                        84.26 kB â”‚ gzip:  33.42 kB\n2025-01-31 17:36:37 build/client/assets/asciidoc-CF5eOCvB.js                     85.17 kB â”‚ gzip:   8.17 kB\n2025-01-31 17:36:37 build/client/assets/fortran-free-form-D6pmzCqS.js            89.19 kB â”‚ gzip:  12.14 kB\n2025-01-31 17:36:37 build/client/assets/swift-Dez-Qvcc.js                        90.59 kB â”‚ gzip:  17.15 kB\n2025-01-31 17:36:37 build/client/assets/vyper-o-cPXEvd.js                        93.22 kB â”‚ gzip:  13.14 kB\n2025-01-31 17:36:37 build/client/assets/blade-HiqCVk8k.js                        98.76 kB â”‚ gzip:  28.39 kB\n2025-01-31 17:36:37 build/client/assets/racket-Cp2HGa90.js                      101.40 kB â”‚ gzip:  17.31 kB\n2025-01-31 17:36:37 build/client/assets/index-Ck7wz1pD.js                       101.46 kB â”‚ gzip:  33.75 kB\n2025-01-31 17:36:37 build/client/assets/objective-c-DHmGyzbM.js                 102.03 kB â”‚ gzip:  23.50 kB\n2025-01-31 17:36:37 build/client/assets/php-DdJTc9Za.js                         103.33 kB â”‚ gzip:  28.40 kB\n2025-01-31 17:36:37 build/client/assets/mdx-CBPJd_fO.js                         118.30 kB â”‚ gzip:  23.40 kB\n2025-01-31 17:36:37 build/client/assets/objective-cpp-BWKJ1FCf.js               164.63 kB â”‚ gzip:  30.77 kB\n2025-01-31 17:36:37 build/client/assets/javascript-Dch3xQiY.js                  167.86 kB â”‚ gzip:  17.12 kB\n2025-01-31 17:36:37 build/client/assets/tsx-BlxWTfDV.js                         168.58 kB â”‚ gzip:  17.18 kB\n2025-01-31 17:36:37 build/client/assets/jsx-CsyrCbsw.js                         170.83 kB â”‚ gzip:  17.21 kB\n2025-01-31 17:36:37 build/client/assets/typescript-DC8MraHL.js                  174.98 kB â”‚ gzip:  16.64 kB\n2025-01-31 17:36:37 build/client/assets/angular-ts-DOuPvTiZ.js                  177.43 kB â”‚ gzip:  17.22 kB\n2025-01-31 17:36:37 build/client/assets/components-bNrEeZYQ.js                  248.72 kB â”‚ gzip:  79.94 kB\n2025-01-31 17:36:37 build/client/assets/wolfram-BICIrM8O.js                     264.33 kB â”‚ gzip:  76.98 kB\n2025-01-31 17:36:37 build/client/assets/wasm-CsTmP73Z.js                        622.30 kB â”‚ gzip: 230.27 kB\n2025-01-31 17:36:37 build/client/assets/cpp-B9__le0e.js                         628.29 kB â”‚ gzip:  48.49 kB\n2025-01-31 17:36:37 build/client/assets/emacs-lisp-BEjL32p1.js                  807.64 kB â”‚ gzip: 202.93 kB\n2025-01-31 17:36:37 build/client/assets/_index-sTTYchLv.js                    1,732.71 kB â”‚ gzip: 525.34 kB\n2025-01-31 17:36:37 âœ“ built in 8.87s\n2025-01-31 17:36:38 vite v5.3.1 building SSR bundle for production...\n2025-01-31 17:36:38 transforming...\n2025-01-31 17:36:38 âœ“ 42 modules transformed.\n2025-01-31 17:36:38 rendering chunks...\n2025-01-31 17:36:38 build/server/.vite/manifest.json                   1.18 kB\n2025-01-31 17:36:38 build/server/assets/tailwind-compat-CC20SAMN.css   2.25 kB\n2025-01-31 17:36:38 build/server/assets/xterm-lQO2bNqs.css             4.08 kB\n2025-01-31 17:36:38 build/server/assets/ReactToastify-CYivYX3d.css    14.19 kB\n2025-01-31 17:36:38 build/server/assets/index-CPTzpSUP.css            17.01 kB\n2025-01-31 17:36:38 build/server/assets/server-build-DKuU_kZm.css     32.28 kB\n2025-01-31 17:36:38 build/server/index.js                             60.23 kB\n2025-01-31 17:36:38 âœ“ built in 275ms\n2025-01-31 17:36:39 \n2025-01-31 17:36:39 > bolt@ dockerstart /app\n2025-01-31 17:36:39 > bindings=$(./bindings.sh) && wrangler pages dev ./build/client $bindings --ip 0.0.0.0 --port 5173 --no-show-interactive-dev-session\n2025-01-31 17:36:39 \n2025-01-31 17:36:39 \n2025-01-31 17:36:39  â›…ï¸ wrangler 3.63.2 (update available 3.106.0)\n2025-01-31 17:36:39 ----------------------------------------------\n2025-01-31 17:36:39 \n2025-01-31 17:36:39 âœ¨ Compiled Worker successfully\n2025-01-31 17:36:39 Your worker has access to the following bindings:\n2025-01-31 17:36:39 - Vars:\n2025-01-31 17:36:39   - GROQ_API_KEY: \"(hidden)\"\n2025-01-31 17:36:39   - OPENAI_API_KEY: \"(hidden)\"\n2025-01-31 17:36:39   - ANTHROPIC_API_KEY: \"(hidden)\"\n2025-01-31 17:36:39   - OPEN_ROUTER_API_KEY: \"(hidden)\"\n2025-01-31 17:36:39   - GOOGLE_GENERATIVE_AI_API_KEY: \"(hidden)\"\n2025-01-31 17:36:39   - OLLAMA_API_BASE_URL: \"(hidden)\"\n2025-01-31 17:36:39   - OPENAI_LIKE_API_BASE_URL: \"(hidden)\"\n2025-01-31 17:36:39   - OPENAI_LIKE_API_KEY: \"(hidden)\"\n2025-01-31 17:36:39   - MISTRAL_API_KEY: \"(hidden)\"\n2025-01-31 17:36:39   - VITE_LOG_LEVEL: \"(hidden)\"\n2025-01-31 17:36:39 [wrangler:inf] Ready on http://0.0.0.0:5173\n[wrangler:inf] - http://127.0.0.1:5173\n[wrangler:inf] - http://172.18.0.2:5173\nâŽ” Starting local server...\n2025-01-31 17:36:40 \n2025-01-31 17:36:37 \n2025-01-31 17:36:37 (!) Some chunks are larger than 500 kB after minification. Consider:\n2025-01-31 17:36:37 - Using dynamic import() to code-split the application\n2025-01-31 17:36:37 - Use build.rollupOptions.output.manualChunks to improve chunking: https://rollupjs.org/configuration-options/#output-manualchunks\n2025-01-31 17:36:37 - Adjust chunk size limit for this warning via build.chunkSizeWarningLimit.\n[wrangler:inf] GET / 200 OK (133130ms)\n\n2025-01-31 17:39:48 âœ˜ [ERROR] workerd/jsg/util.c++:276: error: e = kj/async-io-unix.c++:1524: overloaded: connect(): Connection timed out\n2025-01-31 17:39:48 \n2025-01-31 17:39:48   stack: /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@565195b /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@5651c67 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@5613ebb /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@564897c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@564915c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@563d7d4 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@563db74 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fcb1a7 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fcba6f /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fcd527 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fad32c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fb35ec /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fc7ee4 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@34f329b /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@2f51c20 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@366cf3c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@356bca8; sentryErrorContext = jsgInternalError\n2025-01-31 17:39:48 \n[wrangler:inf] GET / 200 OK (133951ms)\n\n```\n\n---\n\nAfter some trial & error and several reboots of the bolt service Â» Frontend initializes about 2 minutes after first launch with the follwing log:\n\n``` bash\n2025-01-31 17:52:42 âœ˜ [ERROR] workerd/jsg/util.c++:276: error: e = kj/async-io-unix.c++:1524: overloaded: connect(): Connection timed out\n2025-01-31 17:52:42 \n2025-01-31 17:52:42   stack: /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@565195b /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@5651c67 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@5613ebb /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@564897c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@564915c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@563d7d4 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@563db74 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fcb1a7 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fcba6f /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fcd527 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fad32c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fb35ec /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@3fc7ee4 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@34f329b /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@2f51c20 /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@366cf3c /app/node_modules/.pnpm/@cloudflare+workerd-linux-arm64@1.20240701.0/node_modules/@cloudflare/workerd-linux-arm64/bin/workerd@356bca8; sentryErrorContext = jsgInternalError\n2025-01-31 17:52:42 \n[wrangler:inf] GET / 200 OK (134904ms)\n[wrangler:inf] GET /assets/root-kD2MswjW.css 200 OK (3ms)\n[wrangler:inf] GET /assets/tailwind-compat-CC20SAMN.css 200 OK (5ms)\n[wrangler:inf] GET /assets/ReactToastify-CYivYX3d.css 200 OK (6ms)\n[wrangler:inf] GET /assets/index-CPTzpSUP.css 200 OK (8ms)\n[wrangler:inf] GET /assets/manifest-f032aa0a.js 200 OK (19ms)\n[wrangler:inf] GET /assets/theme-C4y9B7zS.js 200 OK (13ms)\n[wrangler:inf] GET /assets/xterm-lQO2bNqs.css 200 OK (19ms)\n[wrangler:inf] GET /assets/root-DSvoOs_7.js 200 OK (14ms)\n[wrangler:inf] GET /assets/entry.client-ydwSl9ol.js 200 OK (4ms)\n[wrangler:inf] GET /assets/components-bNrEeZYQ.js 200 OK (6ms)\n[wrangler:inf] GET /assets/_index-D_NZK3VS.css 200 OK (50ms)\n[wrangler:inf] GET /assets/_index-_Urxur0L.js 200 OK (50ms)\n[wrangler:inf] GET /favicon.svg 200 OK (3ms)\n[wrangler:inf] GET /assets/_index-sTTYchLv.js 200 OK (18ms)\n[wrangler:inf] GET /assets/dark-plus-KEYLhlmT.js 200 OK (6ms)\n[wrangler:inf] GET /assets/light-plus-BsvsQ1iS.js 200 OK (7ms)\n[wrangler:inf] GET /assets/shellscript-BZfs-ost.js 200 OK (6ms)\n[wrangler:inf] GET /assets/wasm-CsTmP73Z.js 200 OK (12ms)\n```",
      "state": "open",
      "author": "jschmdt",
      "author_type": "User",
      "created_at": "2025-01-31T17:15:32Z",
      "updated_at": "2025-02-01T19:00:48Z",
      "closed_at": null,
      "labels": [
        "bug",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/117/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/117",
      "api_url": "https://api.github.com/repos/av/harbor/issues/117",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:20.008526",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for another beautiful report!\n\nI can reproduce the problem on Mac OS, unfortuinately the specifics lie on how `bolt.new` is implemented itself, on top of workerd and Wrangler from CloudFlare, not even Node.js, there's [something in the image](https://github.com/cloudflare/workerd/issues/1401)",
          "created_at": "2025-02-01T17:32:59Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.2.26](https://github.com/av/harbor/releases/tag/v0.2.26), please let me know if the official image improves the performance (unlikely as per my tests, but still)",
          "created_at": "2025-02-01T19:00:47Z"
        }
      ]
    },
    {
      "issue_number": 116,
      "title": "Service: ChatUI Â» Connection ended (DB), no such file \"app/final.yaml\" & MONGODB_URL missing (Client)",
      "body": "# Harbor / ChatUI / Bug Reporting\n\n## ðŸ–¥ï¸ Setup\n\n### System\n\n- Computer: Apple M3 Max Macbook Pro\n- RAM: 128 GB\n- OS: macOS Sequoia 15.3\n\n---\n\n### Docker\n\n- CPU limit: 16\n- Memory limit: 96 GB\n- Swap: 4 GB\n- Disk Usage Limit: 128 GB\n- Engine: v27.4.0\n- Docker Desktop 4.37.2 (179585)\n\n---\n\n### Harbor\n\n- Default settings\n- App: Version 0.2.25 (20250126.003743)\n\n\tâœ… Docker is installed and running\n\tâœ… Docker Compose (v2) is installed\n\tâœ… Docker Compose (v2) version is newer than 2.23.1\n\tâœ… Harbor home: /Users/(UsernameXY)/harbor\n\tâœ… Default profile exists and is readable\n\tâœ… Current profile (.env) exists and is readable\n\tâœ… CLI is linked\n\tâŒ NVIDIA GPU is not available. NVIDIA GPU support may not work.\n\tâŒ NVIDIA Container Toolkit is not installed. NVIDIA GPU support may not work\n\n---\n\n## ðŸª² Bug / Additional Info\n\nServices started via Harbor App. \nError messages inside the container logs:\n\n*  harbor.chatui-db (running)\n\t* â€¦ Connection ended â€¦\n* harbor.chatui (exited)\n\t* Error: ENOENT: no such file or directory, open '/app/final.yaml'\n\t* Error: Please specify the MONGODB_URL environment variable inside .env.local. Set it to mongodb://localhost:27017 if you are running MongoDB locally, or to a MongoDB Atlas free instance for example.\n\n---\n## ðŸ“’ Logs\n### ### harbor.chatui-db âš ï¸\n\n``` bash\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.886+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"127.0.0.1:56606\",\"uuid\":{\"uuid\":{\"$uuid\":\"908820b5-6163-49ad-b9a0-6f640c9c4a5d\"}},\"connectionId\":137,\"connectionCount\":1}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.887+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":51800,   \"ctx\":\"conn137\",\"msg\":\"client metadata\",\"attr\":{\"remote\":\"127.0.0.1:56606\",\"client\":\"conn137\",\"negotiatedCompressors\":[],\"doc\":{\"application\":{\"name\":\"mongosh 2.3.4\"},\"driver\":{\"name\":\"nodejs|mongosh\",\"version\":\"6.11.0|2.3.4\"},\"platform\":\"Node.js v20.18.1, LE\",\"os\":{\"name\":\"linux\",\"architecture\":\"arm64\",\"version\":\"4.14.209-160.339.amzn2.aarch64\",\"type\":\"Linux\"},\"env\":{\"container\":{\"runtime\":\"docker\"}}}}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.911+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"127.0.0.1:56618\",\"uuid\":{\"uuid\":{\"$uuid\":\"17562bba-dad0-4f0c-884d-7e61c2ff4ef0\"}},\"connectionId\":138,\"connectionCount\":2}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.911+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"127.0.0.1:56630\",\"uuid\":{\"uuid\":{\"$uuid\":\"9c016580-0391-42c5-aeec-0ab4f101aa08\"}},\"connectionId\":139,\"connectionCount\":3}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.912+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":51800,   \"ctx\":\"conn138\",\"msg\":\"client metadata\",\"attr\":{\"remote\":\"127.0.0.1:56618\",\"client\":\"conn138\",\"negotiatedCompressors\":[],\"doc\":{\"application\":{\"name\":\"mongosh 2.3.4\"},\"driver\":{\"name\":\"nodejs|mongosh\",\"version\":\"6.11.0|2.3.4\"},\"platform\":\"Node.js v20.18.1, LE\",\"os\":{\"name\":\"linux\",\"architecture\":\"arm64\",\"version\":\"4.14.209-160.339.amzn2.aarch64\",\"type\":\"Linux\"},\"env\":{\"container\":{\"runtime\":\"docker\"}}}}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.912+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":51800,   \"ctx\":\"conn139\",\"msg\":\"client metadata\",\"attr\":{\"remote\":\"127.0.0.1:56630\",\"client\":\"conn139\",\"negotiatedCompressors\":[],\"doc\":{\"application\":{\"name\":\"mongosh 2.3.4\"},\"driver\":{\"name\":\"nodejs|mongosh\",\"version\":\"6.11.0|2.3.4\"},\"platform\":\"Node.js v20.18.1, LE\",\"os\":{\"name\":\"linux\",\"architecture\":\"arm64\",\"version\":\"4.14.209-160.339.amzn2.aarch64\",\"type\":\"Linux\"},\"env\":{\"container\":{\"runtime\":\"docker\"}}}}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.914+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":6788700, \"ctx\":\"conn138\",\"msg\":\"Received first command on ingress connection since session start or auth handshake\",\"attr\":{\"elapsedMillis\":1}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.914+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"127.0.0.1:56644\",\"uuid\":{\"uuid\":{\"$uuid\":\"fdd8db4e-29ef-48a5-8054-a57c404daccf\"}},\"connectionId\":140,\"connectionCount\":4}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.916+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":51800,   \"ctx\":\"conn140\",\"msg\":\"client metadata\",\"attr\":{\"remote\":\"127.0.0.1:56644\",\"client\":\"conn140\",\"negotiatedCompressors\":[],\"doc\":{\"application\":{\"name\":\"mongosh 2.3.4\"},\"driver\":{\"name\":\"nodejs|mongosh\",\"version\":\"6.11.0|2.3.4\"},\"platform\":\"Node.js v20.18.1, LE\",\"os\":{\"name\":\"linux\",\"architecture\":\"arm64\",\"version\":\"4.14.209-160.339.amzn2.aarch64\",\"type\":\"Linux\"},\"env\":{\"container\":{\"runtime\":\"docker\"}}}}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.917+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":6788700, \"ctx\":\"conn140\",\"msg\":\"Received first command on ingress connection since session start or auth handshake\",\"attr\":{\"elapsedMillis\":0}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.979+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22944,   \"ctx\":\"conn137\",\"msg\":\"Connection ended\",\"attr\":{\"remote\":\"127.0.0.1:56606\",\"uuid\":{\"uuid\":{\"$uuid\":\"908820b5-6163-49ad-b9a0-6f640c9c4a5d\"}},\"connectionId\":137,\"connectionCount\":3}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.979+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22944,   \"ctx\":\"conn140\",\"msg\":\"Connection ended\",\"attr\":{\"remote\":\"127.0.0.1:56644\",\"uuid\":{\"uuid\":{\"$uuid\":\"fdd8db4e-29ef-48a5-8054-a57c404daccf\"}},\"connectionId\":140,\"connectionCount\":2}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.979+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22944,   \"ctx\":\"conn139\",\"msg\":\"Connection ended\",\"attr\":{\"remote\":\"127.0.0.1:56630\",\"uuid\":{\"uuid\":{\"$uuid\":\"9c016580-0391-42c5-aeec-0ab4f101aa08\"}},\"connectionId\":139,\"connectionCount\":1}}\n2025-01-31 16:42:58 {\"t\":{\"$date\":\"2025-01-31T15:42:58.979+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22944,   \"ctx\":\"conn138\",\"msg\":\"Connection ended\",\"attr\":{\"remote\":\"127.0.0.1:56618\",\"uuid\":{\"uuid\":{\"$uuid\":\"17562bba-dad0-4f0c-884d-7e61c2ff4ef0\"}},\"connectionId\":138,\"connectionCount\":0}}\n```\n\n---\n\n### ### harbor.chatui âŒ\n\n``` bash\n2025-01-31 16:42:15 Starting ChatUI...\n2025-01-31 16:44:02 Harbor: Custom ChatUI Entrypoint\n2025-01-31 16:44:02 v20.18.2\n2025-01-31 16:44:02 YAML Merger is starting...\n2025-01-31 16:44:02 Merged Configs:\n2025-01-31 16:44:02 Transforming to .env.local...\n2025-01-31 16:44:02 node:fs:448\n2025-01-31 16:44:02     return binding.readFileUtf8(path, stringToFlags(options.flag));\n2025-01-31 16:44:02                    ^\n2025-01-31 16:44:02 \n2025-01-31 16:44:02 Error: ENOENT: no such file or directory, open '/app/final.yaml'\n2025-01-31 16:44:02     at Object.readFileSync (node:fs:448:20)\n2025-01-31 16:44:02     at file:///app/envify.js:10:17\n2025-01-31 16:44:02     at ModuleJob.run (node:internal/modules/esm/module_job:234:25)\n2025-01-31 16:44:02     at async ModuleLoader.import (node:internal/modules/esm/loader:473:24)\n2025-01-31 16:44:02     at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:122:5) {\n2025-01-31 16:44:02   errno: -2,\n2025-01-31 16:44:02   code: 'ENOENT',\n2025-01-31 16:44:02   syscall: 'open',\n2025-01-31 16:44:02   path: '/app/final.yaml'\n2025-01-31 16:44:02 }\n2025-01-31 16:44:02 \n2025-01-31 16:44:02 Node.js v20.18.2\n2025-01-31 16:44:02 Final .env.local:\n2025-01-31 16:44:02 \n2025-01-31 16:44:02 Starting ChatUI...\n2025-01-31 16:44:02 file:///app/build/server/chunks/database-x_dXPQvf.js:59\n2025-01-31 16:44:02       throw new Error(\n2025-01-31 16:44:02             ^\n2025-01-31 16:44:02 \n2025-01-31 16:44:02 Error: Please specify the MONGODB_URL environment variable inside .env.local. Set it to mongodb://localhost:27017 if you are running MongoDB locally, or to a MongoDB Atlas free instance for example.\n2025-01-31 16:44:02     at new Database (file:///app/build/server/chunks/database-x_dXPQvf.js:59:13)\n2025-01-31 16:44:02     at Database.getInstance (file:///app/build/server/chunks/database-x_dXPQvf.js:76:27)\n2025-01-31 16:44:02     at file:///app/build/server/chunks/database-x_dXPQvf.js:216:30\n2025-01-31 16:44:02     at ModuleJob.run (node:internal/modules/esm/module_job:234:25)\n2025-01-31 16:44:02     at async ModuleLoader.import (node:internal/modules/esm/loader:473:24)\n2025-01-31 16:44:02     at async get_hooks (file:///app/build/server/index.js:1522:8)\n2025-01-31 16:44:02     at async Server.init (file:///app/build/server/index.js:4342:24)\n2025-01-31 16:44:02     at async file:///app/build/handler.js:1185:1\n2025-01-31 16:44:02 \n2025-01-31 16:44:02 Node.js v20.18.2\n```\n\n---\n\n",
      "state": "closed",
      "author": "jschmdt",
      "author_type": "User",
      "created_at": "2025-01-31T17:11:56Z",
      "updated_at": "2025-02-01T19:00:07Z",
      "closed_at": "2025-02-01T19:00:06Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/116/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/116",
      "api_url": "https://api.github.com/repos/av/harbor/issues/116",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:20.234333",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for another great report!\n\nThis was broken after one of `chatui` updates resulting in Harbor's config merger stopping to work, so the application wasn't able to find the config where it was supposed to be. I reworked the config merger so now it should be compatible with `chatui`'s image. The ",
          "created_at": "2025-02-01T16:49:33Z"
        },
        {
          "author": "av",
          "body": "The fix was released in [v0.2.26](https://github.com/av/harbor/releases/tag/v0.2.26)",
          "created_at": "2025-02-01T19:00:06Z"
        }
      ]
    },
    {
      "issue_number": 113,
      "title": "OpenHands not launching",
      "body": "I pulled openhands and tried to start on my local repo\n\nharbor openhands\n\nWARN[0000] Found orphan containers ([harbor-opint-run-334ab2bc72b4 harbor-opint-run-495860abb84c harbor-opint-run-ca7f10ff86a1 harbor-opint-run-51d933a91977 harbor-opint-run-bc54998d0bb8 harbor-aider-run-ddf5e0860cd5]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\nStarting OpenHands...\nSetting up enduser with id 1000\nDocker socket group id: 964\nCreating group with id 964\nRunning as enduser\n/app/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nERROR:root:  File \"/app/.venv/bin/uvicorn\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/click/core.py\", line 1161, in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/click/core.py\", line 1082, in main\n    rv = self.invoke(ctx)\n         ^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/click/core.py\", line 1443, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/click/core.py\", line 788, in invoke\n    return __callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/uvicorn/main.py\", line 412, in main\n    run(\n  File \"/app/.venv/lib/python3.12/site-packages/uvicorn/main.py\", line 579, in run\n    server.run()\n  File \"/app/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 66, in run\n    return asyncio.run(self.serve(sockets=sockets))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 70, in serve\n    await self._serve(sockets)\n  File \"/app/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 77, in _serve\n    config.load()\n  File \"/app/.venv/lib/python3.12/site-packages/uvicorn/config.py\", line 435, in load\n    self.loaded_app = import_from_string(self.app)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/uvicorn/importer.py\", line 19, in import_from_string\n    module = importlib.import_module(module_str)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/app/openhands/server/listen.py\", line 3, in <module>\n    from openhands.server.app import app as base_app\n  File \"/app/openhands/server/app.py\", line 15, in <module>\n    from openhands.server.routes.files import app as files_api_router\n  File \"/app/openhands/server/routes/files.py\", line 28, in <module>\n    from openhands.server.file_config import (\n  File \"/app/openhands/server/file_config.py\", line 6, in <module>\n    from openhands.server.shared import config as shared_config\n  File \"/app/openhands/server/shared.py\", line 16, in <module>\n    config = load_app_config()\n             ^^^^^^^^^^^^^^^^^\n  File \"/app/openhands/core/config/utils.py\", line 489, in load_app_config\n    finalize_config(config)\n  File \"/app/openhands/core/config/utils.py\", line 291, in finalize_config\n    get_or_create_jwt_secret(\n  File \"/app/openhands/core/config/utils.py\", line 257, in get_or_create_jwt_secret\n    file_store.write(JWT_SECRET, new_secret)\n  File \"/app/openhands/storage/local.py\", line 24, in write\n    with open(full_path, mode) as f:\n         ^^^^^^^^^^^^^^^^^^^^^\n\nERROR:root:<class 'PermissionError'>: [Errno 13] Permission denied: '/.openhands-state/.jwt_secret\n\nEDIT: I runned harbor fixfs",
      "state": "closed",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2025-01-30T16:41:18Z",
      "updated_at": "2025-02-01T18:59:17Z",
      "closed_at": "2025-02-01T18:59:16Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/113/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/113",
      "api_url": "https://api.github.com/repos/av/harbor/issues/113",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:20.560078",
      "comments": [
        {
          "author": "av",
          "body": "Glad to see you again and thanks for the report!\n\nIndeed, `openhands` has updated since the last tests and now requires a local volume to be present (`/.openhands-state` above is the expected location). Additionally, it looks like the image configuration has changed and it's no longer possible to pr",
          "created_at": "2025-02-01T14:52:28Z"
        },
        {
          "author": "alsoasnerd",
          "body": "bro is cooking ðŸ”¥ðŸ”¥",
          "created_at": "2025-02-01T16:47:08Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.2.26](https://github.com/av/harbor/releases/tag/v0.2.26)",
          "created_at": "2025-02-01T18:59:16Z"
        }
      ]
    },
    {
      "issue_number": 118,
      "title": "[Request] LobeChat Â» Deploying Server Database Version",
      "body": "# Enhance LobeChat with DB functionality\n\n## ðŸ’¡Idea / Background\nI love LobeChat's modern UI, but unfortunately the chats aren't permanently stored in a database. Looking for a Docker-friendly solution, I found a compose-setup from the LobeChat developers themselves: [LobeChat Server DB with Docker Compose](https://lobehub.com/de/docs/self-hosting/server-database/docker-compose) Furthermore, with the added integrated data storage, LobeChats' feature set is expanded with Knowledge Base (file upload / knowledge management / RAG) functionalities.\n\n## ðŸ¤” Request\nWould it be possible to integrate the LobeChat Server DB-variant with the above setup inside of Harbor?\n\n## ðŸ”— Useful links\n[Deploy LobeChat Server Database Version with Docker Compose](https://lobehub.com/de/docs/self-hosting/server-database/docker-compose)",
      "state": "open",
      "author": "jschmdt",
      "author_type": "User",
      "created_at": "2025-01-31T19:09:11Z",
      "updated_at": "2025-02-01T18:41:50Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/118/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/118",
      "api_url": "https://api.github.com/repos/av/harbor/issues/118",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:20.903656",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for another beautifully prepared report!\n\nI made [an attempt](https://github.com/av/harbor/compare/experiment/lobechat-db?expand=1) to integrate with their more complete compose setup, however it comes not only with the DB, but also with Minio and Casdoor/Logto for Auth, unfortunately their s",
          "created_at": "2025-02-01T18:41:05Z"
        }
      ]
    },
    {
      "issue_number": 114,
      "title": "Add lollms-webui",
      "body": "Hey, please add this one more UI, this is really cool.\n\n[https://github.com/ParisNeo/lollms-webui](https://github.com/ParisNeo/lollms-webui)",
      "state": "closed",
      "author": "bhupesh-sf",
      "author_type": "User",
      "created_at": "2025-01-31T08:21:25Z",
      "updated_at": "2025-02-01T15:18:02Z",
      "closed_at": "2025-02-01T15:17:32Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/114/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/114",
      "api_url": "https://api.github.com/repos/av/harbor/issues/114",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:21.173850",
      "comments": [
        {
          "author": "av",
          "body": "Hey, thank you for the suggestion!\n\nI did some quick tests:\n- No official images - was able to build from the repo\n- Official dockerfile doesn't work, there was initiative to fix it: https://github.com/ParisNeo/lollms-webui/pull/583\n- I tried to build from the fixed image in the PR above, but it doe",
          "created_at": "2025-02-01T15:17:32Z"
        }
      ]
    },
    {
      "issue_number": 108,
      "title": "Harbor How Error - Will not successfully build the container on first use",
      "body": "I am using Harbor in a WSL2 container and when I try to use harbor how for the first-time, I get this on several attempts:\n\n harbor how\n[+] Building 2.2s (7/11)                                                                                 docker:default\n => [cmdh internal] load build definition from Dockerfile                                                          0.0s\n => => transferring dockerfile: 389B                                                                               0.0s\n => [cmdh internal] load metadata for docker.io/pkgxdev/pkgx:latest                                                0.4s\n => [cmdh internal] load .dockerignore                                                                             0.0s\n => => transferring context: 2B                                                                                    0.0s\n => [cmdh 1/7] FROM docker.io/pkgxdev/pkgx:latest@sha256:3ec497e47fa662eef62718bfd92908cc969a4928eba8bbf3ba064ed5  0.0s\n => => resolve docker.io/pkgxdev/pkgx:latest@sha256:3ec497e47fa662eef62718bfd92908cc969a4928eba8bbf3ba064ed571e31  0.0s\n => [cmdh internal] load build context                                                                             0.0s\n => => transferring context: 31B                                                                                   0.0s\n => CACHED [cmdh 2/7] WORKDIR /app                                                                                 0.0s\n => ERROR [cmdh 3/7] RUN pkgx install node@20 npm git                                                              1.7s\n------\n > [cmdh 3/7] RUN pkgx install node@20 npm git:\n1.584 install: target 'git': No such file or directory\n------\nfailed to solve: process \"/bin/bash -c pkgx install node@20 npm git\" did not complete successfully: exit code: 1",
      "state": "closed",
      "author": "ColumbusAI",
      "author_type": "User",
      "created_at": "2025-01-25T18:23:14Z",
      "updated_at": "2025-01-27T07:33:21Z",
      "closed_at": "2025-01-27T07:17:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/108/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ColumbusAI"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/108",
      "api_url": "https://api.github.com/repos/av/harbor/issues/108",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:21.516298",
      "comments": [
        {
          "author": "av",
          "body": "Thank you for the report, I can reproduce it with:\n\n```bash\nharbor build cmdh\n```\n\nUsing `pkgx` was one of the earlier choices, there was another instance where images based on it stopped working like this.\n\nI switched the image to use `node:lts` instead and made some extra updates that should make ",
          "created_at": "2025-01-26T00:34:13Z"
        },
        {
          "author": "ColumbusAI",
          "body": "Here's the new result based on the latest commit. Does Harbor How **only** work with model llama3.1?\n\nAs a test, I installed llama3.1 through Ollama and I still get the same error.\n\n```\n$ harbor how\n(node:17) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland al",
          "created_at": "2025-01-26T03:04:04Z"
        },
        {
          "author": "av",
          "body": "`llama3.1` is the default for `cmdh` service that is behind the `harbor how`, it can be configured with `harbor cmdh model`, I'm hesitant to include any larger LLMs into default downloads as it might not be something that user's system is ready to accommodate. There's definitely a room for improveme",
          "created_at": "2025-01-26T10:36:09Z"
        },
        {
          "author": "av",
          "body": "The check for `harbor cmdh model` to be available is in `main`",
          "created_at": "2025-01-26T12:35:26Z"
        },
        {
          "author": "ColumbusAI",
          "body": "it's working! I'm gonna close this out thanks @av! you rock man",
          "created_at": "2025-01-27T07:17:38Z"
        }
      ]
    },
    {
      "issue_number": 110,
      "title": "How can I have Harbor build an image from the latest Github commit, e.g. VLLM?",
      "body": "Even thinking if this is possible, is it also possible to have a PR release added to the image?",
      "state": "closed",
      "author": "ColumbusAI",
      "author_type": "User",
      "created_at": "2025-01-26T00:06:58Z",
      "updated_at": "2025-01-27T02:02:12Z",
      "closed_at": "2025-01-27T02:02:11Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/110/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/110",
      "api_url": "https://api.github.com/repos/av/harbor/issues/110",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:21.880300",
      "comments": [
        {
          "author": "av",
          "body": "In theory you can, in practice it'll be complicated since you'll have to replicate service's required build environment locally. There are a couple of approaches, all based on using Docker's support for `git` context.\n\n**Approach 1**\n\nReplace `build` with git repo context, I took the build args from",
          "created_at": "2025-01-26T13:02:07Z"
        },
        {
          "author": "ColumbusAI",
          "body": "Understood, this is something I will play with and if I have a solution, I will report back for others who may be interested. Thanks @av !",
          "created_at": "2025-01-27T02:02:11Z"
        }
      ]
    },
    {
      "issue_number": 104,
      "title": "N8N needs environment variable WEBHOOK_URL defined to correctly perform OAuth.",
      "body": "At this time if you are  trying to setup an OAuth. The callback function defaults to the following and cannot be changed.\n`http://localhost:5678/rest/oauth2-credential/callback`\n\n![Image](https://github.com/user-attachments/assets/b4856e2a-b5e9-4a7b-bf56-67c6dc5e7b23)\n\nThe solution appears to be adding the webhook url described below.\nhttps://docs.n8n.io/hosting/configuration/configuration-examples/webhook-url/\nhttps://community.n8n.io/t/replacing-localhost-5678-with-server-address-in-oauth-credentials/9907\n\nI believe we need to add the following line:\n`WEBHOOK_URL=http://localhost:34191/\n`\nWhat would be the correct Harbor way to do this in a clean fashion so that\nit is always set when the container starts up.\n\nI experimented without luck. Probably because I added prefix N8N\nand no quotes around the url.\n` harbor config set N8N_WEBHOOK_URL=https://localhost:34191`\nAlso export of the environment variable did not fix it for me.\n\nThanks for creating harbor. It is awesome and I am going to put my\ntime and effort into learning it and using it.\n",
      "state": "closed",
      "author": "polarisJet",
      "author_type": "User",
      "created_at": "2025-01-22T17:43:07Z",
      "updated_at": "2025-01-26T10:36:50Z",
      "closed_at": "2025-01-26T10:36:49Z",
      "labels": [
        "documentation",
        "enhancement",
        "question"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/104/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/104",
      "api_url": "https://api.github.com/repos/av/harbor/issues/104",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:22.206378",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for trying Harbor out and for the positive feedback!\n\nIt's possible to [specify arbitrary env variables ](https://github.com/av/harbor/wiki/1.-Harbor-User-Guide#environment-variables)for a specific service by:\n- [`harbor env`](https://github.com/av/harbor/wiki/3.-Harbor-CLI-Reference#harb",
          "created_at": "2025-01-22T18:15:47Z"
        },
        {
          "author": "polarisJet",
          "body": "This is perfect. Works exactly like you described. Have a great , great day !!!!\n",
          "created_at": "2025-01-22T19:50:01Z"
        },
        {
          "author": "av",
          "body": "Closing as this seems to be resolved, please feel free to open a new one or follow up here if needed",
          "created_at": "2025-01-26T10:36:49Z"
        }
      ]
    },
    {
      "issue_number": 107,
      "title": "Harbor Doctor Error - fails to recognize newest docker compose",
      "body": "$ harbor doctor\n10:17:57 [INFO] Running Harbor Doctor...\n10:17:58 [INFO] âœ” Docker is installed and running\n10:17:58 [INFO] âœ” Docker Compose (v2) is installed\n10:17:58 [ERROR] âœ˜ Docker Compose version is older than 2.23.1. Please update Docker Compose (v2).\n10:17:58 [INFO] âœ” Harbor home: /home/admin/source/harbor\n10:17:58 [INFO] âœ” Default profile exists and is readable\n10:17:58 [INFO] âœ” Current profile (.env) exists and is readable\n10:17:58 [INFO] âœ” CLI is linked\n10:17:58 [INFO] âœ” NVIDIA GPU is available\n10:17:58 [INFO] âœ” NVIDIA Container Toolkit is installed\n10:17:58 [ERROR] Harbor Doctor checks failed. Please resolve the issues above.\n\n$ docker-compose version\nDocker Compose version v2.31.0-desktop.2\n",
      "state": "closed",
      "author": "ColumbusAI",
      "author_type": "User",
      "created_at": "2025-01-25T18:20:51Z",
      "updated_at": "2025-01-26T03:07:26Z",
      "closed_at": "2025-01-26T03:07:25Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/107/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/107",
      "api_url": "https://api.github.com/repos/av/harbor/issues/107",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:22.447530",
      "comments": [
        {
          "author": "av",
          "body": "Hey, great to see you back!\n\nCould you please also check the compose plugin v2, as that's what Harbor uses?\n\n```bash\n# Older compose, standalone binary\ndocker-compose version\n\n# Newer compose, used by harbor, note no dash\ndocker compose version\n```\n\nWith `docker compose` specifically, I see the foll",
          "created_at": "2025-01-25T23:42:26Z"
        },
        {
          "author": "ColumbusAI",
          "body": "you remembered me! Love it. Here's my docker compose version: \n\n$ docker-compose version\nDocker Compose version v2.31.0-desktop.2",
          "created_at": "2025-01-26T00:02:34Z"
        },
        {
          "author": "av",
          "body": "Yes, that's the older standalone binary, Harbor uses v2 plugin for docker CLI - the two can have different versions on same system. \n\nThe difference is in the dash. \n\nI suspect that the v2 plugin could be older on your setup (or that Harbor has issues with \"desktop\" portion).",
          "created_at": "2025-01-26T01:02:39Z"
        },
        {
          "author": "ColumbusAI",
          "body": "I traced the issue to this code snippet. When I commented out the last comparison, everything works as it should. The issue is that that the comparison was expecting an integer and received a string because of the \"-desktop.2\" but now we're all good! Closing this out.\n\n```\n # log_debug \"Docker Compo",
          "created_at": "2025-01-26T03:07:25Z"
        }
      ]
    },
    {
      "issue_number": 99,
      "title": "Add KoboldCpp",
      "body": "I think it would be nice to have KoboldCpp supported by the project, its both a backend and has its own UI.\r\nThe undelying engine of KoboldCpp is a fork of llamacpp, stable-diffusion.cpp, whisper.cpp and others.\r\n\r\nThe official docker-compose for the project is as follows:\r\n```\r\nversion: \"3.2\"\r\nservices:\r\n  koboldcpp:\r\n    container_name: koboldcpp\r\n    image: koboldai/koboldcpp:latest\r\n    volumes:\r\n      - ./kcpp-data:/workspace/:rw\r\n    deploy: # You can remove this section if you do not wish to use an Nvidia GPU\r\n      resources:\r\n        reservations:\r\n          devices:\r\n          - driver: nvidia\r\n            device_ids: ['0']\r\n            capabilities: [gpu]\r\n    environment:\r\n      - KCPP_MODEL=https://huggingface.co/concedo/KobbleTinyV2-1.1B-GGUF/resolve/main/KobbleTiny-Q4_K.gguf?download=true # Remove this line if you wish to supply your own model offline\r\n      - KCPP_DONT_REMOVE_MODELS=true\r\n      - KCPP_DONT_UPDATE=false\r\n      - KCPP_DONT_TUNNEL=true\r\n      - KCPP_ARGS=--model model.gguf --usecublas --gpulayers 99 --multiuser 20 --quiet # Remove --model model.gguf if you are using a comma split model with the KCPP_MODEL argument, remove --usecublas --gpulayers 99 --multiuser 20 for CPU mode\r\n    ports:\r\n      - \"5001:5001\"\r\n ```\r\n\r\nIf this project needs a different design I can help with that, but I am not familair enough with this project to do a PR from scratch.\r\n\r\nKoboldCpp supports its own API, OpenAI's API (multiple including stuff like image recognition) and even has some basic ollama emulation. Should make it work with a lot of the frontends in this project.",
      "state": "closed",
      "author": "henk717",
      "author_type": "User",
      "created_at": "2025-01-11T13:05:11Z",
      "updated_at": "2025-01-18T12:05:43Z",
      "closed_at": "2025-01-18T12:05:42Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/99/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/99",
      "api_url": "https://api.github.com/repos/av/harbor/issues/99",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:22.680408",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks so much for the suggestion! I wasn't aware that koboldcpp has a docker-friendly setup, the integration is available in [Harbor v0.2.22](https://github.com/av/harbor/releases/tag/v0.2.22)",
          "created_at": "2025-01-11T16:52:54Z"
        },
        {
          "author": "henk717",
          "body": "Thanks for the quick add, at first glance from the readme the only debatable default is this one:\r\n```\r\n--usecublas --gpulayers 99 --multiuser 20\r\n```\r\nDoing that forces a full offload on nvidia GPU's, but I am unsure if that is ideal for harbor users. Its essential for multi GPU to work well howeve",
          "created_at": "2025-01-11T17:27:29Z"
        },
        {
          "author": "av",
          "body": "> --usecublas --gpulayers 99 --multiuser 20\r\n\r\nThanks for the clarifications! I incorrectly assumed that these are recommended defaults for the docker version in general and used them throughout the testing, removing these params in the upcoming update. It's awesome that kobold automatically infers ",
          "created_at": "2025-01-12T16:23:43Z"
        },
        {
          "author": "av",
          "body": "Closing, since we now have the integration and docs/defaults have been fixed according to the suggestions. Please feel free to follow up or open a new issue if needed, cheers!",
          "created_at": "2025-01-18T12:05:42Z"
        }
      ]
    },
    {
      "issue_number": 100,
      "title": "AnythingLLM Exited (1) Error & Solution",
      "body": "Hi, I would like to inform about any issue present regarding AnythingLLM.\r\nOn a clean installation of harbor when installing and trying to run AnythingLLM I'm presented with the message:\r\n\r\n`container harbor.anythingllm exited (1)`\r\n\r\nRunning `harbor logs anythingllm`\r\n\r\nI'm provided with this logs:\r\n\r\nharbor.anythingllm  | ```\r\nharbor.anythingllm  | import { PrismaClient } from '@prisma/client'\r\nharbor.anythingllm  | const prisma = new PrismaClient()\r\nharbor.anythingllm  | ```\r\nharbor.anythingllm  | or start using Prisma Client at the edge (See: https://pris.ly/d/accelerate)\r\nharbor.anythingllm  | ```\r\nharbor.anythingllm  | import { PrismaClient } from '@prisma/client/edge'\r\nharbor.anythingllm  | const prisma = new PrismaClient()\r\nharbor.anythingllm  | ```\r\nharbor.anythingllm  |\r\nharbor.anythingllm  | See other ways of importing Prisma Client: http://pris.ly/d/importing-client\r\nharbor.anythingllm  |\r\nharbor.anythingllm  | Environment variables loaded from .env\r\nharbor.anythingllm  | Prisma schema loaded from prisma/schema.prisma\r\nharbor.anythingllm  | Datasource \"db\": SQLite database \"anythingllm.db\" at \"file:../storage/anythingllm.db\"\r\nharbor.anythingllm  |\r\nharbor.anythingllm  | Error: Schema engine error:\r\nharbor.anythingllm  | SQLite database error\r\nharbor.anythingllm  | unable to open database file: ../storage/anythingllm.db\r\nharbor.anythingllm  |\r\n\r\n\r\nBut after searching around and checking around the documentation of AnythingLLM's website i found that usually this is cause by a permission issue that anythingllm is not able to access the storage location.\r\nSo the possible two solutions are: or change the location of the storage to a location that it doesn't require root permission or chmod the current location.\r\n\r\nTo test it i run: `sudo chown -R 1000:1000 /home/vllm/.harbor/anythingllm`\r\nNote: vllm is just the name of my user as i'm running on LXC to play with harbor.\r\n\r\nAnd after that I was able to run and access anythingllm succesfully.\r\n\r\nThe solution I found from another github link: https://github.com/Mintplex-Labs/anything-llm/issues/2077\r\n\r\nIn case someone faces the same issue as me.\r\n\r\nRegards !\r\n",
      "state": "closed",
      "author": "Lensman89",
      "author_type": "User",
      "created_at": "2025-01-12T19:11:49Z",
      "updated_at": "2025-01-18T12:04:36Z",
      "closed_at": "2025-01-18T12:04:35Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/100/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/100",
      "api_url": "https://api.github.com/repos/av/harbor/issues/100",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:22.867303",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for trying out Harbor!\n\nIndeed, there are also many other services that require specific file access permissions on the volumes mounted from the host. Unfortunately, sometimes services are also not self-consistent, or the permissions are changed on the host due to some automated process.\n",
          "created_at": "2025-01-18T12:04:35Z"
        }
      ]
    },
    {
      "issue_number": 96,
      "title": "perplexideez causing trouble when building other services",
      "body": "In a fresh installation of harbor:\r\n\r\nâžœ harbor build webtop\r\nvalidating /home/xxx/.harbor/compose.perplexideez.yml: configs.perplexideez_proxy_config Additional property content is not allowed\r\nvalidating /home/xxx/.harbor/compose.perplexideez.yml: configs.perplexideez_proxy_config Additional property content is not allowed\r\n\r\nI had to delete anything related to perplexideez in order to build webtop. Something may be wrong in the compose file of perplexideez, in the content section, and it doesn't allow other services to run successfully. ",
      "state": "closed",
      "author": "SimonBlancoE",
      "author_type": "User",
      "created_at": "2025-01-08T18:25:59Z",
      "updated_at": "2025-01-11T17:03:00Z",
      "closed_at": "2025-01-10T12:59:34Z",
      "labels": [
        "enhancement",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/96/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/96",
      "api_url": "https://api.github.com/repos/av/harbor/issues/96",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:23.114795",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for reporting!\r\n\r\nThis feature requires [Docker 2.23.1](https://docs.docker.com/compose/releases/release-notes/#2231) and above, could you please check if your local one is below that? (then it's expected and we need to be more explicit about the version in the [installation docs](https://git",
          "created_at": "2025-01-10T10:40:51Z"
        },
        {
          "author": "SimonBlancoE",
          "body": "Yes, I am using 2.22.0 right now. That should explain the issue. \r\nI use OpenSUSE Leap in my server, which is not bleeding edge but also not super outdated as well. ",
          "created_at": "2025-01-10T12:22:27Z"
        },
        {
          "author": "av",
          "body": "I agree that such a small feature definitely not worth of raising the version requirements. A preferred workflows  would be only this service to stop working when the version is older. I have an idea how to implement it.",
          "created_at": "2025-01-10T13:24:16Z"
        },
        {
          "author": "av",
          "body": "JFYI, [v0.2.22](https://github.com/av/harbor/releases/tag/v0.2.22) comes with a mentioned change and should no longer fail on compose older than v2.23.1, the installation docs now also mention the required version",
          "created_at": "2025-01-11T17:02:47Z"
        }
      ]
    },
    {
      "issue_number": 95,
      "title": "#!/usr/bin/env bash instead of #!/bin/bash ?",
      "body": "Is there a reason why `#!/bin/bash` is used instead of `#!/usr/bin/env bash`? The reason I ask is because bin/bash doesn't work with unconventional setups like NixOS.",
      "state": "closed",
      "author": "bndlfm",
      "author_type": "User",
      "created_at": "2025-01-07T06:20:54Z",
      "updated_at": "2025-01-11T17:00:21Z",
      "closed_at": "2025-01-11T17:00:21Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/95/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/95",
      "api_url": "https://api.github.com/repos/av/harbor/issues/95",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:23.354291",
      "comments": [
        {
          "author": "av",
          "body": "No specific reason, just a missing on my end, thank you for raising this up! [The fix](https://github.com/av/harbor/commit/68084f6fc100ee1e31fb03d1ec89661c09aa5174) is added to `main` and will be a part of the next release",
          "created_at": "2025-01-07T15:41:07Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.2.22](https://github.com/av/harbor/releases/tag/v0.2.22), please feel free to re-open or follow up if needed and thanks again!",
          "created_at": "2025-01-11T17:00:21Z"
        }
      ]
    },
    {
      "issue_number": 94,
      "title": "Add Khoj",
      "body": "Khoj is an open source, personal AI\r\nYou can [chat](https://docs.khoj.dev/features/chat) with it about anything. It'll use files you shared with it to respond, when relevant. It can also access information from the public internet.\r\nQuickly [find](https://docs.khoj.dev/features/search) relevant notes and documents using natural language\r\nIt understands pdf, plaintext, markdown, org-mode files, [notion pages](https://docs.khoj.dev/data-sources/notion_integration) and [github repositories](https://docs.khoj.dev/data-sources/github_integration)\r\nAccess it from your [Emacs](https://docs.khoj.dev/clients/emacs), [Obsidian](https://docs.khoj.dev/clients/obsidian), the [Khoj desktop app](https://docs.khoj.dev/clients/desktop), or [any web browser](https://docs.khoj.dev/clients/web)\r\n\r\nhttps://docs.khoj.dev/get-started/setup/\r\n\r\nLooks possible? I can start a branch for this today if interested.",
      "state": "open",
      "author": "ZacharyKehlGEAppliances",
      "author_type": "User",
      "created_at": "2025-01-06T16:27:11Z",
      "updated_at": "2025-01-06T17:57:46Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/94/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/94",
      "api_url": "https://api.github.com/repos/av/harbor/issues/94",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:23.539481",
      "comments": [
        {
          "author": "ZacharyKehlGEAppliances",
          "body": "https://github.com/ZacharyKehlGEAppliances/harbor-khoj\r\n\r\nWorking on it here",
          "created_at": "2025-01-06T17:36:50Z"
        },
        {
          "author": "av",
          "body": "Hey, thanks for the suggestion and the initiative! ðŸ’¯ \r\n\r\nLooks like a useful all-rounder, would be a great addition to the other frontends, please feel free to reach out in case any help is needed",
          "created_at": "2025-01-06T17:57:44Z"
        }
      ]
    },
    {
      "issue_number": 26,
      "title": "OptiLLM Integration",
      "body": "https://github.com/codelion/optillm",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-09-20T22:18:50Z",
      "updated_at": "2024-12-29T10:34:12Z",
      "closed_at": "2024-12-29T10:34:12Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/26/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 2,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/26",
      "api_url": "https://api.github.com/repos/av/harbor/issues/26",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:23.732730",
      "comments": [
        {
          "author": "av",
          "body": "Added for the upcoming release, however it requires a couple of workarounds to work with Open WebUI",
          "created_at": "2024-12-29T10:34:09Z"
        }
      ]
    },
    {
      "issue_number": 27,
      "title": "AICI",
      "body": "https://github.com/microsoft/aici?tab=readme-ov-file#build-and-start-rllm-server-and-aici-runtime",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-09-22T06:37:21Z",
      "updated_at": "2024-12-28T08:36:35Z",
      "closed_at": "2024-12-28T08:36:35Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/27/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/27",
      "api_url": "https://api.github.com/repos/av/harbor/issues/27",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:23.927734",
      "comments": [
        {
          "author": "av",
          "body": "It's likely that other services will work better",
          "created_at": "2024-12-28T08:36:35Z"
        }
      ]
    },
    {
      "issue_number": 70,
      "title": "Perplexica UI doesn't terminate initialization",
      "body": "Hi,\r\nTrying Perplexica, it installs without errors, but the UI is stuck before writing its \"prompt\". It seems its settings must be adjusted, but also the settings page remains stuck waiting with the `UPLOAD` button disabled:\r\n![image](https://github.com/user-attachments/assets/0483632a-389e-43f7-b37c-6bca12f064ee)\r\n\r\nIt seems also that Perplexica doesn't take advantage of the Harbor ecosystem (Ollama, SearXNG).\r\n\r\nI got Perplexica working in a dedicated LXC container after setting its `config.toml` before starting it:\r\n```toml\r\n[API_KEYS]\r\nGROQ = \"gsk_******\"\r\n\r\n[API_ENDPOINTS]\r\nOLLAMA = \"http://10.4.0.100:11434\"\r\nSEARXNG = \"http://10.4.0.105:4000\"\r\n```\r\n\r\nThe only harbor config I'm aware of is this:\r\n```bash\r\n[root@HarborAI ~]# harbor config ls | grep -i perplexica\r\nPERPLEXICA_HOST_PORT           34041\r\nPERPLEXICA_BACKEND_HOST_PORT   34042\r\n```\r\n\r\nHow to solve?\r\nThank you",
      "state": "open",
      "author": "PieBru",
      "author_type": "User",
      "created_at": "2024-10-29T14:56:28Z",
      "updated_at": "2024-12-27T22:16:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/70/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/70",
      "api_url": "https://api.github.com/repos/av/harbor/issues/70",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:25.960598",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for the report, it's not an expected state, since Perplexica should:\r\n- Start from scratch with `harbor up perplexica`\r\n- Connect to built-in Ollama and Searxng out of the box\r\n\r\nUnfortunately, Perplexica is one of the services that are a tiny bit \"picky\" in terms of configuration, I've outli",
          "created_at": "2024-11-02T14:02:37Z"
        },
        {
          "author": "nullnuller",
          "body": "<s>I still have the same issue https://github.com/av/harbor/issues/60#issuecomment-2411864309 </s> The issues seems to be resolved for me.\r\nAlso, I noticed that perplexica config survives rm -rf harbor or even harbor config reset",
          "created_at": "2024-11-04T11:48:26Z"
        },
        {
          "author": "PieBru",
          "body": "I managed to solve this same issue I had with Perplexica in a LXC container on Proxmox VE, but I don't know how to apply this solution to Perplexica on Harbor.\r\n\r\nAfter finding this https://github.com/ItzCrazyKns/Perplexica/issues/467, the solution was substituting `127.0.0.1` with the LAN IP of the",
          "created_at": "2024-12-04T16:28:25Z"
        },
        {
          "author": "ecker00",
          "body": "This requires rebuilding the image, if you are lazy override it on runtime:\r\n\r\n```\r\n  perplexica-frontend:\r\n    image: itzcrazykns1337/perplexica-frontend:main\r\n    restart: unless-stopped\r\n    depends_on:\r\n      - perplexica-backend\r\n    ports:\r\n      - 3000:3000\r\n    networks:\r\n      - perplexica-",
          "created_at": "2024-12-27T22:16:19Z"
        }
      ]
    },
    {
      "issue_number": 90,
      "title": "can't recognize system's self signed certificate in chain",
      "body": "I am using Harbor to host applications such as dify, n8n. However since I have self-signed certificate, some applications have trouble to communicate with outside. \r\n\r\nFor example, if I add Groq API keys in `n8n`, I have error for this\r\n![image](https://github.com/user-attachments/assets/e5baa63f-9abd-44d6-8718-f03ec78d12ec)\r\n\r\nFor the environment, I am using WSL2, and the self-signed certificate is added to the system already.\r\n\r\nHow can I let Harbor and its services recognize the system certificate?\r\n\r\nThanks for your help.",
      "state": "closed",
      "author": "shenhai-ran",
      "author_type": "User",
      "created_at": "2024-12-13T11:24:26Z",
      "updated_at": "2024-12-27T16:45:14Z",
      "closed_at": "2024-12-27T16:45:14Z",
      "labels": [
        "documentation",
        "question",
        "OS:Windows"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/90/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/90",
      "api_url": "https://api.github.com/repos/av/harbor/issues/90",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:26.153589",
      "comments": [
        {
          "author": "av",
          "body": "Hey, thanks for trying out Harbor!\r\n\r\nAs per my understanding it happens because the custom certificates are not installed in the service containers, so the container OS doesn't know about them and won't mark them as trusted.\r\n\r\nOne way to verify if that's the case is to add a custom mount to one of",
          "created_at": "2024-12-25T19:44:57Z"
        },
        {
          "author": "shenhai-ran",
          "body": "Hello,\r\n\r\nThanks for your feedback, I found similar results online :).  However particularly for n8n, besides update volumes, I also need to add one more environment variable in the yml file as:\r\n``` yaml\r\n n8n:\r\n  # ...\r\n    volumes:\r\n      # ...\r\n      -  /etc/ssl/certs/ca-certificates.crt:/etc/ss",
          "created_at": "2024-12-27T16:45:14Z"
        }
      ]
    },
    {
      "issue_number": 73,
      "title": "glm-4 voice support?",
      "body": "when?",
      "state": "open",
      "author": "VinnyG9",
      "author_type": "User",
      "created_at": "2024-11-05T02:20:11Z",
      "updated_at": "2024-12-25T19:49:22Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/73/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/73",
      "api_url": "https://api.github.com/repos/av/harbor/issues/73",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:26.331235",
      "comments": [
        {
          "author": "av",
          "body": "Sorry, this issue was lost for a very long time\r\n\r\nI did some brief investigation and can't see if there're any developer-friendly ways to run inference for GLM-4-Voice, with OpenAI-compatible APIs. I'm open to the integration when any such service is available",
          "created_at": "2024-12-25T19:49:21Z"
        }
      ]
    },
    {
      "issue_number": 84,
      "title": "Add ogem",
      "body": "Key Features\r\n\r\n- One API for Everything: Use OpenAI's API format for all models (GPT-4, Claude, Gemini)\r\n- Smart Routing: Automatic latency-based routing\r\n- Fallback Support: Chain multiple models (gpt-4,claude-3-opus,gemini-1.5-pro)\r\n- Multi-Architecture Docker: Ready for both ARM64 and AMD64\r\n\r\nLinks\r\nGitHub: [https://github.com/yanolja/ogem](https://github.com/yanolja/ogem)\r\n\r\nDocker Hub: [ynext/ogem](https://hub.docker.com/repository/docker/ynext/ogem/general)\r\n\r\nAlthough I haven't tried it, the failover support looks interesting.",
      "state": "closed",
      "author": "nullnuller",
      "author_type": "User",
      "created_at": "2024-11-24T13:26:33Z",
      "updated_at": "2024-12-25T19:35:40Z",
      "closed_at": "2024-12-25T19:35:40Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/84/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/84",
      "api_url": "https://api.github.com/repos/av/harbor/issues/84",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:26.495698",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for bringing this in! I also saw the post on r/LocalLLaMa when it was announced. I decided to wait for some time to see if it'll gain any usage, as it overlaps a lot with LiteLLM/OptiLLM",
          "created_at": "2024-11-29T22:34:21Z"
        },
        {
          "author": "av",
          "body": "After some time in the oven - it doesn't look like ogem is getting traction or being developed further, so I won't be actively working on this integration. \r\n\r\nWith that said, I'm of course open to the contributions in the future and will help in any way I can.",
          "created_at": "2024-12-25T19:35:40Z"
        }
      ]
    },
    {
      "issue_number": 82,
      "title": "Add Langflow",
      "body": "Langflow is a low-code app builder for RAG and multi-agent AI applications. Itâ€™s Python-based and agnostic to any model, API, or database.\r\n\r\n- Python-based and agnostic to models, APIs, data sources, or databases.\r\n- Visual IDE for drag-and-drop building and testing of workflows.\r\n- Playground to immediately test and iterate workflows with step-by-step control.\r\n- Multi-agent orchestration and conversation management and retrieval.\r\n- Free cloud service to get started in minutes with no setup.\r\n- Publish as an API or export as a Python application.\r\n- Observability with LangSmith, LangFuse, or LangWatch integration.\r\n- Enterprise-grade security and scalability with free DataStax Langflow cloud service.\r\n- Customize workflows or create flows entirely just using Python.\r\n- Ecosystem integrations as reusable components for any model, API or database.",
      "state": "closed",
      "author": "ZacharyKehlGEAppliances",
      "author_type": "User",
      "created_at": "2024-11-20T22:41:44Z",
      "updated_at": "2024-12-25T16:53:09Z",
      "closed_at": "2024-12-25T16:53:08Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/82/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/82",
      "api_url": "https://api.github.com/repos/av/harbor/issues/82",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:26.671906",
      "comments": [
        {
          "author": "nullnuller",
          "body": "Also, how about Flowise? Which one is better as a completely free alternative (with lots of connecters)? n8n and dify are already included?",
          "created_at": "2024-11-21T13:17:06Z"
        },
        {
          "author": "ZacharyKehlGEAppliances",
          "body": "Honestly right now I have been using Dify.ai and love it. It just has a lack of connectors. Langflow has some really great connectors and is supported by the langchain team. It also seems that their RAG implementation is a bit more advanced than Dify but I havent looked into it too much. \r\n\r\nThe onl",
          "created_at": "2024-11-21T14:45:35Z"
        },
        {
          "author": "ZacharyKehlGEAppliances",
          "body": "Also I have started work on importing it here. It seems pretty successful, I am just having issues handling harbor url and harbor open.\r\n\r\nhttps://github.com/ZacharyKehlGEAppliances/harbor-langflow",
          "created_at": "2024-11-21T14:46:36Z"
        },
        {
          "author": "av",
          "body": "Hey ðŸ‘‹ðŸ» \r\n\r\n- Thanks for trying out Harbor and for the suggestion! \r\n- Langflow was planned eventually, and Flowise as well, we do want to expand on satellites with visual LLM workflows, I've been using Dify/LitLytics quite a bit, we also have n8n (a bit cumbersome to configure), and omnichain alread",
          "created_at": "2024-11-21T17:32:43Z"
        },
        {
          "author": "av",
          "body": "JFYI, Flowise was just added in [v0.2.18](https://github.com/av/harbor/releases/tag/v0.2.18) (very nice DevX, functionality is great too), @ZacharyKehlGEAppliances , granted you have a WIP for LangFlow integration - please feel free to open a PR whenever ready, I'll avoid adding it myself to avoid c",
          "created_at": "2024-11-24T12:08:17Z"
        }
      ]
    },
    {
      "issue_number": 56,
      "title": "Holy Docker ComposeðŸ’€",
      "body": "idk what the reason is for all of those compose files to be in the root of the project, ðŸ˜­ and maybe it was stated somewhere in the docs. But it is lagging the page and looks incredibly messy. Think that should be a sign to put all of that into a folder.ðŸ‘€",
      "state": "closed",
      "author": "HeavyLvy",
      "author_type": "User",
      "created_at": "2024-10-07T08:09:26Z",
      "updated_at": "2024-12-25T03:25:10Z",
      "closed_at": "2024-12-25T03:25:10Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/56/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/56",
      "api_url": "https://api.github.com/repos/av/harbor/issues/56",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:26.883177",
      "comments": [
        {
          "author": "av",
          "body": "It's no scalable, agree! \r\n\r\nI was considering to do this, but it'd require a pretty meticulous and large-scale check of all the paths still matching. Worst case - every single service will have to be re-tested. So I decided to not do that (yet).",
          "created_at": "2024-10-07T17:44:05Z"
        },
        {
          "author": "HeavyLvy",
          "body": "Itd be worth it",
          "created_at": "2024-10-21T21:13:56Z"
        }
      ]
    },
    {
      "issue_number": 66,
      "title": "`cmdh` installation fails",
      "body": "Hi,\r\nI just installed `harbor` in a dedicated Arch Linux LXC container on a Proxmox VE standalone server node.\r\nThe default apps plus `searxng` and `litellm` installed flawlessy, but `harbor up cmdh` shows an installation error:\r\n\r\n![image](https://github.com/user-attachments/assets/941ff821-13c1-4fe0-a7fb-dcad28e5d1f6)\r\n\r\nLXC container CPU, RAM and Disk are more than enough.\r\nDid I miss something?\r\n\r\nThank you.\r\nPiero",
      "state": "closed",
      "author": "PieBru",
      "author_type": "User",
      "created_at": "2024-10-28T11:39:53Z",
      "updated_at": "2024-11-24T13:55:36Z",
      "closed_at": "2024-11-24T13:55:35Z",
      "labels": [
        "bug",
        "OS:Linux"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/66/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/66",
      "api_url": "https://api.github.com/repos/av/harbor/issues/66",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:27.060346",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for the report, looks like `cmdh` integration went stale, will be fixed in the upcoming release.\r\n\r\nMeanwhile - `opint`, `aider`, `aichat` offer similar functionalities, also `fabric` works quite well for the shell-related tasks",
          "created_at": "2024-10-28T11:45:49Z"
        },
        {
          "author": "av",
          "body": "Finally got to it!\r\n\r\nShould be fixed in [v0.2.15](https://github.com/av/harbor/releases/tag/v0.2.15)",
          "created_at": "2024-11-02T13:35:58Z"
        },
        {
          "author": "av",
          "body": "Closing, please feel free to follow up or create a new issue if needed ",
          "created_at": "2024-11-24T13:55:35Z"
        }
      ]
    },
    {
      "issue_number": 21,
      "title": "Harbor install script / update defaults to 1.9",
      "body": "Hey. Just wanted to let you know when using that script, the installation (and updates using 'harbor update') list version 1.9 as the latest.\r\nI'm running this on Ubuntu 22.04 LTS. Git listing versions sorts them alphabetically, so 1.24 is lower than 1.9 when I run the command.\r\n\r\n\r\nMinor issue that can be circumvented by running the install manually + replacing instead of script/update.\r\n\r\n\r\nPS. This project is amazing. Makes trying stuff out so easy and clean!",
      "state": "closed",
      "author": "Pokora22",
      "author_type": "User",
      "created_at": "2024-09-17T11:13:49Z",
      "updated_at": "2024-11-24T12:15:17Z",
      "closed_at": "2024-11-24T12:15:17Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/21/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/21",
      "api_url": "https://api.github.com/repos/av/harbor/issues/21",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:27.286688",
      "comments": [
        {
          "author": "av",
          "body": "Thank you so much for the feedback and for trying out Harbor!\r\n\r\nCurrent behavior is less than ideal and needs to be fixed, there are also problems with the shallow clone that is done by the intstall",
          "created_at": "2024-09-17T13:00:26Z"
        },
        {
          "author": "av",
          "body": "I've prepared a possible fix [here](https://github.com/av/harbor/commit/6c61d1a5ae3d9952f120b7fbaca22c79e1e5629f), the only question is MacOS compat",
          "created_at": "2024-09-17T13:10:57Z"
        },
        {
          "author": "av",
          "body": "The fix was released in scope of [v0.1.27](https://github.com/av/harbor/releases/tag/v0.1.27)\r\n\r\nI'll keep this issue open for visibility, since it essentially prevents anyone from upgrading unless they know that there's a problem.\r\n\r\nWorkarounds:\r\n- Try updating with `harbor update --latest`\r\n- Man",
          "created_at": "2024-09-22T16:24:48Z"
        },
        {
          "author": "av",
          "body": "Closing, since I think enough time passed for most of the users to be on versions where this problem was fixed",
          "created_at": "2024-11-24T12:15:17Z"
        }
      ]
    },
    {
      "issue_number": 65,
      "title": "FEATURE: even more useful AI tools",
      "body": "Hi,\r\nI just \"discovered\" harbor and I'm so exited that, even before deep diving into it, I would like to propose some additions, all can run fully-local and eventually use SearXNG.\r\n\r\n**n8n**, a one-catches-all \"flows\" tools, with LLM agents support.\r\nhttps://github.com/n8n-io/n8n\r\nhttps://docs.n8n.io/hosting/installation/docker/\r\n\r\n**Bolt.new fork**, IMO one of the best AI coding tools available now.\r\nhttps://github.com/coleam00/bolt.new-any-llm\r\nhttps://github.com/coleam00/bolt.new-any-llm/blob/main/docker-compose.yml\r\n\r\n**LangFlow**, a no/low code agentic app builder.\r\nhttps://github.com/langflow-ai/langflow\r\nhttps://github.com/langflow-ai/langflow/tree/main/docker_example\r\nIt will natively support Ollama agents after solving this issue: https://github.com/langflow-ai/langflow/issues/4178\r\n\r\n**Flowise**, similar to LangFlow, kinda simpler but also less powerful. I don't use it, but IMO it deserves an honorable mention here.\r\nhttps://github.com/FlowiseAI/Flowise\r\nhttps://github.com/FlowiseAI/Flowise?tab=readme-ov-file#-docker\r\n\r\n**OpenCanvas**, by LangChain-AI, an awesome open source version of OpenAI-Canvas.\r\nWaiting for the fully-local version, use its free cloud instance: https://opencanvas.langchain.com/\r\nhttps://github.com/langchain-ai/open-canvas\r\nIt's in heavy development, I'm just waiting to solve this issue https://github.com/langchain-ai/open-canvas/issues/109\r\n\r\n**AutoGEN Studio**, by Microsoft, Web UI to \"Interactively Explore Multi-Agent Workflows\".\r\nhttps://microsoft.github.io/autogen/0.2/blog/2023/12/01/AutoGenStudio/\r\nhttps://microsoft.github.io/autogen/0.2/docs/installation/Docker/\r\nThe stable release is based on AutoGEN v2, but the new version based on AutoGEN v4 will arive (hopefully) soon.\r\n\r\nOn YT there are some recent videos that cover these tools.\r\n\r\nThank you,\r\nPiero",
      "state": "open",
      "author": "PieBru",
      "author_type": "User",
      "created_at": "2024-10-26T10:22:50Z",
      "updated_at": "2024-11-24T12:09:41Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "new service"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/65/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/65",
      "api_url": "https://api.github.com/repos/av/harbor/issues/65",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:27.497959",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying out Harbor and for the suggestions!\r\n\r\nn8n and Bolt.new were already on the table, so I [tried to integrate both today](https://github.com/av/harbor/releases/tag/v0.2.13). `n8n` works, but `bolt` unfortunately does not and requires some fixes in the upstream. I'll return to the oth",
          "created_at": "2024-10-27T15:10:42Z"
        },
        {
          "author": "av",
          "body": "Flowise and Bolt.new were added in [v0.2.18](https://github.com/av/harbor/releases)",
          "created_at": "2024-11-24T12:09:40Z"
        }
      ]
    },
    {
      "issue_number": 83,
      "title": "k6 future functions",
      "body": "I hope K6 can add real-time monitoring and dashboard export functions, which can view dashboard data based on test ID instead of time, and then make the data persistently stored.",
      "state": "open",
      "author": "z1054136399",
      "author_type": "User",
      "created_at": "2024-11-21T08:43:21Z",
      "updated_at": "2024-11-24T12:01:45Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/83/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/83",
      "api_url": "https://api.github.com/repos/av/harbor/issues/83",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:27.700750",
      "comments": [
        {
          "author": "av",
          "body": "Hi ðŸ‘‹ðŸ» \r\n\r\nThanks for the suggestion! \r\n\r\n> real-time monitoring\r\n\r\nIndeed, the current `k6` setup is designed more around one-off performance tests, but I understand the desire for continuous monitoring. Is it more about uptime or more about continuous performance monitoring? \r\n\r\nFor uptime, we migh",
          "created_at": "2024-11-24T12:01:43Z"
        }
      ]
    },
    {
      "issue_number": 74,
      "title": "Integration - Chat Nio",
      "body": "Seems quite good options and a nice UI.\r\n\r\n[https://github.com/zmh-program/chatnio](https://github.com/zmh-program/chatnio)",
      "state": "closed",
      "author": "bhupesh-sf",
      "author_type": "User",
      "created_at": "2024-11-06T15:47:12Z",
      "updated_at": "2024-11-09T22:05:06Z",
      "closed_at": "2024-11-09T22:05:06Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/74/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/74",
      "api_url": "https://api.github.com/repos/av/harbor/issues/74",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:27.880034",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for bringing this to my attention. The service configuration is quite poorly documented, but I think I managed to produce something working.\r\n\r\nWill be available in upcoming Harbor release",
          "created_at": "2024-11-09T19:26:22Z"
        },
        {
          "author": "av",
          "body": "Released in [v0.2.16](https://github.com/av/harbor/releases/tag/v0.2.16)",
          "created_at": "2024-11-09T22:05:02Z"
        }
      ]
    },
    {
      "issue_number": 67,
      "title": "FEATURE: ComfyUI default config",
      "body": "Hi, this is very low priority. Harbor is so useful that it forces my laziness out of me :)\r\n\r\nI just installed ComfyUI, and I think a newcomer (like me) user experience can be even better.\r\n\r\n1. As mentioned in #50 a LAN server URL redirects to `localhost`. I tried `harbor config set COMFYUI_URL http://harborai.lan:34032/` (both 34031 and 34032), but it continues to redirec to to `localhost`.\r\n\r\n2. After manually setting the hostname in the URL, here is the second CTF puzzle: the login. It's quite obvious to the experienced Harbor user, but IMO newcomers will appreciate if the installation phase shows a follow-up message to the user, something like:\r\n\r\n```\r\nBefore accessing ComfyUI, please have a look at, and eventually modify, its configuration:\r\nharbor config ls | grep -i comfyui\r\nharbor config set COMFYUI_USER IamComfy\r\nharbor config set COMFYUI_PASSWORD my_S3cret!\r\n```\r\n\r\nThank you",
      "state": "open",
      "author": "PieBru",
      "author_type": "User",
      "created_at": "2024-10-29T10:00:04Z",
      "updated_at": "2024-11-04T11:54:01Z",
      "closed_at": null,
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/67/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/67",
      "api_url": "https://api.github.com/repos/av/harbor/issues/67",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:28.091193",
      "comments": [
        {
          "author": "av",
          "body": "Hi, sorry for a delayed response.\r\n\r\nFirstly, huge thanks for this and the other reports you made! It's a great help for the project!\r\n\r\n1. I think that authentication redirect can be configured using `DIRECT_ADDRESS` env var from the [`ai-dock`](https://github.com/ai-dock/base-image/wiki/2.0-Enviro",
          "created_at": "2024-11-02T13:41:07Z"
        },
        {
          "author": "kosteva",
          "body": "> 1. As mentioned in [comfyui expects localhostÂ #50](https://github.com/av/harbor/issues/50) a LAN server URL redirects to `localhost`. I tried `harbor config set COMFYUI_URL http://harborai.lan:34032/` (both 34031 and 34032), but it continues to redirec to to `localhost`.\r\n\r\nfor my money, i solved ",
          "created_at": "2024-11-04T11:53:59Z"
        }
      ]
    },
    {
      "issue_number": 63,
      "title": "Changing cache location isn't working in tabbyapi, vllm",
      "body": "Hey. I installed this on Linux and changed the model for tabby to 'bullerwins_gemma-2-2b-it-exl2_8.0bpw', and then used the hf dl'er to d/l the model to here.  I've confirmed it was dl'ed to the folder below.\r\n\r\n`(~/.cache/huggingface/hub/models--bullerwins--gemma-2-2b-it-exl2_8.0bpw)`\r\n\r\nWhen I run `harbor up tabbyapi` I get the error below.\r\n\r\n  File \"/app/backends/exllamav2/model.py\", line 216, in __init__\r\n    self.config.prepare()\r\n  File \"/usr/local/lib/python3.11/dist-packages/exllamav2/config.py\", line 166, in prepare\r\n    assert os.path.exists(self.model_dir), \"Can't find \" + self.model_dir\r\n`**AssertionError: Can't find /models/hf/bullerwins_gemma-2-2b-it-exl2_8.0bpw**`\r\n\r\nWhat step am I missing? Where is the `/models/hf` folder that its looking for?\r\n\r\nGreat project. Thx.\r\n",
      "state": "closed",
      "author": "zheroz00",
      "author_type": "User",
      "created_at": "2024-10-23T03:10:06Z",
      "updated_at": "2024-11-02T10:38:13Z",
      "closed_at": "2024-11-02T10:38:13Z",
      "labels": [
        "documentation",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/63/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/63",
      "api_url": "https://api.github.com/repos/av/harbor/issues/63",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:28.297803",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for another report!\r\n\r\nCurrent TabbyAPI integration is indeed a bit strict about the model placement, but it's designed to also be straightforward. Here's a sample set of commands to run the model above:\r\n\r\n```bash\r\n# 1. Use HuggingFaceDownloader CLI to get the files from the HF Hub\r\n$ harbor",
          "created_at": "2024-10-27T10:42:01Z"
        },
        {
          "author": "av",
          "body": "I've added more detailed explanations in the [TabbyAPI service guide](https://github.com/av/harbor/wiki/2.2.4-Backend:-TabbyAPI#huggingfacedownloader)",
          "created_at": "2024-10-27T10:50:17Z"
        },
        {
          "author": "av",
          "body": "Closing for now, please feel free to follow up or open a new issue if needed",
          "created_at": "2024-11-02T10:38:13Z"
        }
      ]
    },
    {
      "issue_number": 54,
      "title": "Bolt.ai integration",
      "body": "https://hub.docker.com/r/mickysharam/bolt-ai",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-10-06T17:37:47Z",
      "updated_at": "2024-10-27T16:23:37Z",
      "closed_at": "2024-10-27T16:23:37Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/54/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/54",
      "api_url": "https://api.github.com/repos/av/harbor/issues/54",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:28.487815",
      "comments": [
        {
          "author": "av",
          "body": "Available in [v0.2.13](https://github.com/av/harbor/releases/tag/v0.2.13), hopefully upstream will fix the problem with API configurability",
          "created_at": "2024-10-27T16:23:37Z"
        }
      ]
    },
    {
      "issue_number": 59,
      "title": "New service: Latitude",
      "body": "https://github.com/latitude-dev/latitude-llm",
      "state": "open",
      "author": "mrtkrcm",
      "author_type": "User",
      "created_at": "2024-10-11T06:53:20Z",
      "updated_at": "2024-10-27T09:04:20Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/59/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/59",
      "api_url": "https://api.github.com/repos/av/harbor/issues/59",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:28.696896",
      "comments": [
        {
          "author": "av",
          "body": "Great tool, thanks for bringing it to my attention ",
          "created_at": "2024-10-11T07:01:57Z"
        },
        {
          "author": "av",
          "body": "As a small update - the only thing I'm waiting for atm are pre-built docker images for their setup to be hosted somewhere",
          "created_at": "2024-10-27T09:04:19Z"
        }
      ]
    },
    {
      "issue_number": 51,
      "title": "Unable to start harbor from WSL (Arch)",
      "body": "Hi there!\r\nYour project seems really interesting, especially that you provided a way to easily enable LLM or modules (satellites) in a very modular way.\r\nI am running on Windows so I decided to get into WSL (Windows Subsystem Linux) which is a Linux kernel running inside Windows.\r\nI have docker running on my host and available inside my WSL (socket is shared), hosted in my windows host.\r\n```console\r\nâ¯ docker ps\r\nCONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS     NAMES\r\ne10ee223a4e9   edc812b8e25d                 \"entry\"                  18 minutes ago   Up 18 minutes             k8s_lb-t\r\n... <truncated>\r\n```\r\n\r\n```console\r\nâ¯ docker version\r\nClient:\r\n Version:           27.1.1\r\n API version:       1.45 (downgraded from 1.46)\r\n Go version:        go1.22.5\r\n Git commit:        63125853e3\r\n Built:             Thu Jul 25 17:06:22 2024\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          26.1.5\r\n  API version:      1.45 (minimum version 1.24)\r\n  Go version:       go1.22.5\r\n  Git commit:       411e817ddf710ff8e08fa193da80cb78af708191\r\n  Built:            Fri Jul 26 17:51:06 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.17\r\n  GitCommit:        3a4de459a68952ffb703bbe7f2290861a75b6b67\r\n runc:\r\n  Version:          1.1.14\r\n  GitCommit:        2c9f5602f0ba3d9da1c2596322dfc4e156844890\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\nI tried to start harbor, after installing it succesfully, unfortunately, It suddenly stops for a reason I'm unable to diagnose:\r\n```console\r\nâ¯ ./harbor.sh up\r\n[+] Running 0/2\r\n â ‹ ollama Pulling                                                                                                  0.1s\r\n â ‹ webui Pulling                                                                                                   0.1s\r\nerror getting credentials - err: exit status 1, out: `Could not connect: No such file or directory`\r\n```\r\n\r\nWould you have any idea what's going on?\r\nAny way to move forward?",
      "state": "closed",
      "author": "cazzoo",
      "author_type": "User",
      "created_at": "2024-10-03T05:39:33Z",
      "updated_at": "2024-10-27T09:02:35Z",
      "closed_at": "2024-10-27T09:02:35Z",
      "labels": [
        "bug",
        "documentation",
        "OS:Windows"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/51/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/51",
      "api_url": "https://api.github.com/repos/av/harbor/issues/51",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:28.944340",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying out Harbor!\r\n\r\nUnfortunately, my own experience with WSL is rather limited and Harbor isn't tested there explicitly (but known to run as per users reports). Hence, I can offer only somewhat generic guidance based on the online research (sorry if these are something you already trie",
          "created_at": "2024-10-03T06:42:22Z"
        },
        {
          "author": "av",
          "body": "Closing, if the above didn't help - please feel free to follow up or open a new issue",
          "created_at": "2024-10-27T09:02:35Z"
        }
      ]
    },
    {
      "issue_number": 60,
      "title": "Searxng and Perplexica are not working properly",
      "body": "All config are default, nothing changed after git clone and habor installation steps.\r\nI am using ollama instance running outside of docker (not the harbor ollama), which can be accessed by webui when used for chat.\r\n\r\n```\r\n(base) harbor$ harbor doctor\r\n19:48:52 [INFO] Running Harbor Doctor...\r\n19:48:52 [INFO] âœ” Docker is installed and running\r\n19:48:52 [INFO] âœ” Docker Compose (v2) is installed\r\n19:48:52 [INFO] âœ” .env file exists and is readable\r\n19:48:52 [INFO] âœ” default profile exists and is readable\r\n19:48:52 [INFO] âœ” Harbor workspace directory exists\r\n19:48:52 [INFO] âœ” CLI is linked\r\n19:48:52 [WARN] âœ˜ NVIDIA Container Toolkit is not installed. NVIDIA GPU support may not work.\r\n19:48:52 [INFO] Harbor Doctor checks completed successfully.\r\n```\r\n\r\nharbor info\r\n```\r\nHarbor CLI version: 0.2.11\r\n==========================\r\n19:48:47 [INFO] Harbor active services:\r\nboost\r\nollama\r\nperplexica\r\nperplexica-be\r\nsearxng\r\nstt\r\ntts\r\nwebui\r\n==========================\r\nClient: Docker Engine - Community\r\n Version:    27.3.1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.17.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.29.7\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 13\r\n  Running: 8\r\n  Paused: 0\r\n  Stopped: 5\r\n Images: 56\r\n Server Version: 27.3.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c\r\n runc version: v1.1.14-0-g2c9f560\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.8.0-41-generic\r\n Operating System: Ubuntu 24.04.1 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 56\r\n Total Memory: 251.8GiB\r\n Name: \r\n ID:\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n```\r\nSearxng seem to return some results but they don't get passed to any LLM I try.\r\n![image](https://github.com/user-attachments/assets/c031897e-07d7-4ea8-b0ee-9e20ae86c830)\r\n\r\nsearxng log\r\n```\r\n(base) nulled@mail:~/Downloads/LLM_Applications/harbor$ harbor logs searxng\r\nharbor.searxng  |   File \"/usr/lib/python3.12/site-packages/httpx/_client.py\", line 1617, in send\r\nharbor.searxng  |     response = await self._send_handling_auth(\r\nharbor.searxng  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nharbor.searxng  |   File \"/usr/lib/python3.12/site-packages/httpx/_client.py\", line 1645, in _send_handling_auth\r\nharbor.searxng  |     response = await self._send_handling_redirects(\r\nharbor.searxng  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nharbor.searxng  |   File \"/usr/lib/python3.12/site-packages/httpx/_client.py\", line 1682, in _send_handling_redirects\r\nharbor.searxng  |     response = await self._send_single_request(request)\r\nharbor.searxng  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nharbor.searxng  |   File \"/usr/lib/python3.12/site-packages/httpx/_client.py\", line 1719, in _send_single_request\r\nharbor.searxng  |     response = await transport.handle_async_request(request)\r\nharbor.searxng  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nharbor.searxng  |   File \"/usr/lib/python3.12/site-packages/httpx/_transports/default.py\", line 352, in handle_async_request\r\nharbor.searxng  |     with map_httpcore_exceptions():\r\nharbor.searxng  |          ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nharbor.searxng  |   File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\r\nharbor.searxng  |     self.gen.throw(value)\r\nharbor.searxng  |   File \"/usr/lib/python3.12/site-packages/httpx/_transports/default.py\", line 77, in map_httpcore_exceptions\r\nharbor.searxng  |     raise mapped_exc(message) from exc\r\nharbor.searxng  | httpx.ConnectTimeout\r\n```\r\n\r\nPerplexica keeps looking for an answer, never ends.\r\n\r\n![image](https://github.com/user-attachments/assets/faa63745-1ee8-4558-b9cd-237d47d4b9ee)\r\n\r\n```\r\nharbor.perplexica  | yarn run v1.22.22\r\nharbor.perplexica  | $ next start\r\nharbor.perplexica  |    â–² Next.js 14.1.4\r\nharbor.perplexica  |    - Local:        http://localhost:3000\r\nharbor.perplexica  | \r\nharbor.perplexica  |  âœ“ Ready in 1223ms\r\n```",
      "state": "closed",
      "author": "nullnuller",
      "author_type": "User",
      "created_at": "2024-10-14T09:00:21Z",
      "updated_at": "2024-10-27T08:50:22Z",
      "closed_at": "2024-10-27T08:50:21Z",
      "labels": [
        "good first issue",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/60/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/60",
      "api_url": "https://api.github.com/repos/av/harbor/issues/60",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:29.137429",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for the detailed report!\r\n\r\nThere are a few items, I'll try to cover them one-by-one:\r\n\r\n> I am using ollama instance running outside of docker\r\n\r\nThat's perfectly fine, but I'd recommend removing `ollama` from the default services in Harbor in such case:\r\n```bash\r\nharbor defaults # see c",
          "created_at": "2024-10-14T17:36:09Z"
        },
        {
          "author": "av",
          "body": "Closing, feel free to follow up or open a new one if needed",
          "created_at": "2024-10-27T08:50:21Z"
        }
      ]
    },
    {
      "issue_number": 50,
      "title": "comfyui expects localhost",
      "body": "Is there a way to set the comfyui host?  I don't see anything in the docs or the .env file for the COMFYUI_HOSTNAME, for example.\r\n\r\nIf not, then there is an issue, since ComfyUI seems to expect to be served at localhost.  If I connect to it as http://server:34031/ or http://server:34032/, I get redirected to http://localhost:34032/login, which breaks of course, since I'm remote.  The server is headless, of course, so I can't just use that as the client too.",
      "state": "closed",
      "author": "lee-b",
      "author_type": "User",
      "created_at": "2024-09-30T18:53:12Z",
      "updated_at": "2024-10-21T22:51:47Z",
      "closed_at": "2024-10-21T22:51:47Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/50/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/50",
      "api_url": "https://api.github.com/repos/av/harbor/issues/50",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:29.341629",
      "comments": [
        {
          "author": "av",
          "body": "The login screen is provided by the integration called `ai-dock`, its documentation is available here:\r\nhttps://github.com/ai-dock/comfyui?tab=readme-ov-file#additional-environment-variables\r\n\r\nSpecifically, it seems that you can specify `COMFYUI_URL` as a full URL including port or `DIRECT_ADDRESS`",
          "created_at": "2024-09-30T20:43:35Z"
        },
        {
          "author": "lee-b",
          "body": "Thanks, @av! :)",
          "created_at": "2024-10-21T22:51:46Z"
        }
      ]
    },
    {
      "issue_number": 58,
      "title": "How to set arbitrary environment variables for service containers?",
      "body": "Many of the supported services use environment variables for further configuration. For example you can run `harbor ollama serve -h` to see variables for Ollama. I was looking for a way to set the `OLLAMA_DEBUG` to 1.\r\n\r\nI tried to use `harbor config set ollama.debug 1` and while it creates the correct looking variable to the `harbor config list`, but  inside the ollama container it has `HARBOR_` prefix and thus ollama does not use it. \r\n",
      "state": "closed",
      "author": "kumpulak",
      "author_type": "User",
      "created_at": "2024-10-08T18:34:11Z",
      "updated_at": "2024-10-11T20:49:03Z",
      "closed_at": "2024-10-11T20:48:59Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/58/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/58",
      "api_url": "https://api.github.com/repos/av/harbor/issues/58",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:29.526552",
      "comments": [
        {
          "author": "av",
          "body": "Kudos on trying the config command for that, indeed it's intended for options that Harbor controls directly. \r\n\r\nFor arbitrary env vars, see this section in the User guide:\r\nhttps://github.com/av/harbor/wiki/1.-Harbor-User-Guide#using-configuration-files\r\n\r\nYou can add them to the `.env`, there are ",
          "created_at": "2024-10-08T18:55:49Z"
        },
        {
          "author": "av",
          "body": "Closing this for now. Feel free to follow up or open a new issue if needed.",
          "created_at": "2024-10-11T20:48:59Z"
        }
      ]
    },
    {
      "issue_number": 55,
      "title": "harbor up -> \"unknown shorthand flag: 'f' in -f\" error",
      "body": "_system: Arch Linux with KDE, 5900X, 64GB, 6800XT_\r\n\r\nNo matter how I install, unsafe script or manual cloning, every time I try to launch harbor, is throws the following error:\r\n\r\n```\r\n[ProtoBelisarius@ProtoBelisarius harbor]$ harbor up\r\nunknown shorthand flag: 'f' in -f\r\nSee 'docker --help'.\r\n\r\nUsage:  docker [OPTIONS] COMMAND\r\n\r\nA self-sufficient runtime for containers\r\n\r\nCommon Commands:\r\n  run         Create and run a new container from an image\r\n  exec        Execute a command in a running container\r\n  ps          List containers\r\n  build       Build an image from a Dockerfile\r\n  pull        Download an image from a registry\r\n  push        Upload an image to a registry\r\n  images      List images\r\n  login       Authenticate to a registry\r\n  logout      Log out from a registry\r\n  search      Search Docker Hub for images\r\n  version     Show the Docker version information\r\n  info        Display system-wide information\r\n\r\nManagement Commands:\r\n  builder     Manage builds\r\n  checkpoint  Manage checkpoints\r\n  container   Manage containers\r\n  context     Manage contexts\r\n  image       Manage images\r\n  manifest    Manage Docker image manifests and manifest lists\r\n  network     Manage networks\r\n  plugin      Manage plugins\r\n  system      Manage Docker\r\n  trust       Manage trust on Docker images\r\n  volume      Manage volumes\r\n\r\nSwarm Commands:\r\n  config      Manage Swarm configs\r\n  node        Manage Swarm nodes\r\n  secret      Manage Swarm secrets\r\n  service     Manage Swarm services\r\n  stack       Manage Swarm stacks\r\n  swarm       Manage Swarm\r\n\r\nCommands:\r\n  attach      Attach local standard input, output, and error streams to a running container\r\n  commit      Create a new image from a container's changes\r\n  cp          Copy files/folders between a container and the local filesystem\r\n  create      Create a new container\r\n  diff        Inspect changes to files or directories on a container's filesystem\r\n  events      Get real time events from the server\r\n  export      Export a container's filesystem as a tar archive\r\n  history     Show the history of an image\r\n  import      Import the contents from a tarball to create a filesystem image\r\n  inspect     Return low-level information on Docker objects\r\n  kill        Kill one or more running containers\r\n  load        Load an image from a tar archive or STDIN\r\n  logs        Fetch the logs of a container\r\n  pause       Pause all processes within one or more containers\r\n  port        List port mappings or a specific mapping for the container\r\n  rename      Rename a container\r\n  restart     Restart one or more containers\r\n  rm          Remove one or more containers\r\n  rmi         Remove one or more images\r\n  save        Save one or more images to a tar archive (streamed to STDOUT by default)\r\n  start       Start one or more stopped containers\r\n  stats       Display a live stream of container(s) resource usage statistics\r\n  stop        Stop one or more running containers\r\n  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\r\n  top         Display the running processes of a container\r\n  unpause     Unpause all processes within one or more containers\r\n  update      Update configuration of one or more containers\r\n  wait        Block until one or more containers stop, then print their exit codes\r\n\r\nGlobal Options:\r\n      --config string      Location of client config files (default \"/home/belisarius/.docker\")\r\n  -c, --context string     Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with \"docker context use\")\r\n  -D, --debug              Enable debug mode\r\n  -H, --host list          Daemon socket to connect to\r\n  -l, --log-level string   Set the logging level (\"debug\", \"info\", \"warn\", \"error\", \"fatal\") (default \"info\")\r\n      --tls                Use TLS; implied by --tlsverify\r\n      --tlscacert string   Trust certs signed only by this CA (default \"/home/belisarius/.docker/ca.pem\")\r\n      --tlscert string     Path to TLS certificate file (default \"/home/belisarius/.docker/cert.pem\")\r\n      --tlskey string      Path to TLS key file (default \"/home/belisarius/.docker/key.pem\")\r\n      --tlsverify          Use TLS and verify the remote\r\n  -v, --version            Print version information and quit\r\n\r\nRun 'docker COMMAND --help' for more information on a command.\r\n\r\nFor more help on how to use Docker, head to https://docs.docker.com/go/guides/\r\n\r\n\r\n```\r\nIm quite baffled by this error and have no idea how to solve it. No similar issues either, its really weird.\r\n\r\nSymlink was successfully created:\r\n\r\n```\r\n[ProtoBelisarius@ProtoBelisarius harbor]$ ./harbor.sh ln\r\nCreating .env file...\r\n19:41:52 [INFO] Symlink created: /home/username/.local/bin/harbor -> /home/username/Git/harbor/harbor.sh\r\n19:41:52 [INFO] You may need to reload your shell or run 'source /home/username/.bash_profile' for changes to take effect.\r\n\r\n```",
      "state": "closed",
      "author": "ProtoBelisarius",
      "author_type": "User",
      "created_at": "2024-10-06T17:51:46Z",
      "updated_at": "2024-10-06T19:06:32Z",
      "closed_at": "2024-10-06T19:06:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/55/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/55",
      "api_url": "https://api.github.com/repos/av/harbor/issues/55",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:29.751573",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying out Harbor!\r\n\r\nCould you please run `harbor info` and `harbor doctor`?\r\n\r\nFrom the first glance, it looks like either older version of GNU libs (`cut -f`), or older `docker` with standalone compose (not supported by Harbor)",
          "created_at": "2024-10-06T17:59:58Z"
        },
        {
          "author": "ProtoBelisarius",
          "body": "**harbor doctor**\r\n```\r\n[ProtoBelisarius@ProtoBelisarius harbor]$ harbor doctor\r\n20:02:49 [INFO] Running Harbor Doctor...\r\n20:02:49 [ERROR] âœ˜ Docker is not installed or not running. Please install or start Docker.\r\n```\r\n\r\n**harbor info**\r\n```\r\n[ProtoBelisarius@ProtoBelisarius harbor]$ harbor info\r\nH",
          "created_at": "2024-10-06T18:05:12Z"
        },
        {
          "author": "av",
          "body": "It looks like docker is there, next couple of things to check:\r\n\r\n- do you have a setup to run it without `sudo`? See the [postinstall instructions](https://docs.docker.com/engine/install/linux-postinstall/) on setting that up\r\n- Ensure you have compose plugin installed: https://docs.docker.com/comp",
          "created_at": "2024-10-06T18:11:57Z"
        },
        {
          "author": "ProtoBelisarius",
          "body": "I think I solved it, its downloading 1.448GB now.\r\n\r\nI installed the docker-compose package from extras reop, idk if that was actually necessary.\r\n\r\nMy current docker version is: `Docker version 27.3.1, build ce1223035a`\r\n\r\nI then started the service: `sudo systemctl start docker`\r\n\r\nthen it said:\r\n",
          "created_at": "2024-10-06T18:14:46Z"
        },
        {
          "author": "av",
          "body": "Glad to hear it works now!\r\n\r\nI'm not sure about the package, it depends on if it contains compose plugin or a standalone compose bin (maybe both? not sure). Running the service and having your user in the docker group are both prerequisites to having Harbor running, I'll reflect these in the instal",
          "created_at": "2024-10-06T18:25:02Z"
        }
      ]
    },
    {
      "issue_number": 39,
      "title": "Add AnythingLLM",
      "body": "Hello,\r\nFirstly let me thank you for the great work on this project and all the effort you have put into maintaining it.\r\nI understand that `harbor` currently supports many frontends, but I would like to bring your attention to AnythingLLM. They are oriented towards RAG and agents. The development of this project is very active and they already support ollama integration. The advantage compared to currently available frontends is feature completeness and support for many local and online backends out of the box, and also the support for browser extension.\r\nI can try to implement this service, but I would like to ask if there are any reasons for it not to be implemented to this moment.\r\nBest regards.",
      "state": "closed",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-09-28T17:51:54Z",
      "updated_at": "2024-10-05T18:18:14Z",
      "closed_at": "2024-10-05T18:18:13Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/39/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/39",
      "api_url": "https://api.github.com/repos/av/harbor/issues/39",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:29.967734",
      "comments": [
        {
          "author": "av",
          "body": "Thank you for the kind words!\r\n\r\nThere're no specific reasons against AnythingLLM integration, any contribution is warmly welcomed. There's [a short overview](https://github.com/av/harbor/wiki/7.-Adding-A-New-Service) of necessary steps to add a new service in the Wiki, but please feel free to reach",
          "created_at": "2024-09-28T22:27:42Z"
        },
        {
          "author": "av",
          "body": "Added a simple integration in [v0.2.4](https://github.com/av/harbor/releases/tag/v0.2.4), let me know if you'll be able to see if it functions as expected",
          "created_at": "2024-10-02T20:36:33Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank you. You were finished before I even finished experiment on local branch. I will test it tomorrow.",
          "created_at": "2024-10-02T20:43:00Z"
        },
        {
          "author": "av",
          "body": "I'm sure that you'll find released version pretty basic and there'll be things to improve on based on your local work, so don't think your effort is lost, I'll be around in such case, thanks!",
          "created_at": "2024-10-02T20:51:29Z"
        },
        {
          "author": "av",
          "body": "I'm closing this for now that initial support is in place, please feel free to follow-up or open a new issue if needed",
          "created_at": "2024-10-05T18:18:13Z"
        }
      ]
    },
    {
      "issue_number": 53,
      "title": "Integrate RepoPack",
      "body": "https://github.com/yamadashy/repopack",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-10-04T09:29:52Z",
      "updated_at": "2024-10-04T22:49:40Z",
      "closed_at": "2024-10-04T22:49:40Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/53/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/53",
      "api_url": "https://api.github.com/repos/av/harbor/issues/53",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:30.147060",
      "comments": [
        {
          "author": "av",
          "body": "Done in v0.2.6",
          "created_at": "2024-10-04T22:49:40Z"
        }
      ]
    },
    {
      "issue_number": 43,
      "title": "Please dont use colons in file names. It will cause issues in windows!",
      "body": "https://github.com/av/harbor/blob/main/ollama/modelfiles/llama3.1%3A8b.Modelfile#L8",
      "state": "closed",
      "author": "shaneholloman",
      "author_type": "User",
      "created_at": "2024-09-30T08:05:20Z",
      "updated_at": "2024-10-04T09:34:48Z",
      "closed_at": "2024-10-04T09:34:47Z",
      "labels": [
        "bug",
        "OS:Windows"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/43/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/43",
      "api_url": "https://api.github.com/repos/av/harbor/issues/43",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:30.321451",
      "comments": [
        {
          "author": "shaneholloman",
          "body": "```\r\nshane @ moa â¯ _repos â¯ gh repo clone shaneholloman/harbor\r\nCloning into 'harbor'...\r\nremote: Enumerating objects: 2168, done.\r\nremote: Counting objects: 100% (726/726), done.\r\nremote: Compressing objects: 100% (447/447), done.\r\nRremote: Total 2168 (delta 266), reused 684 (delta 229), pack-reuse",
          "created_at": "2024-09-30T08:08:38Z"
        },
        {
          "author": "av",
          "body": "Thanks for bringing this to my attention (and for the PR!), I'll keep an eye on cases like these from now on",
          "created_at": "2024-09-30T10:05:57Z"
        },
        {
          "author": "av",
          "body": "Closing this for now, since this was resolved with your PR (thanks again!)\r\n\r\nPlease, feel free to follow-up or open a new issue if anything similar happens again. ",
          "created_at": "2024-10-04T09:34:47Z"
        }
      ]
    },
    {
      "issue_number": 46,
      "title": "ollama create can't access local files",
      "body": "`ollama create -f some_file_path some_name` should be able to access `some_file_path`, in order to import the files.  However, since it's a client to the docker instance, that doesn't work.\r\n\r\nThis is far from perfect, but I wrote this little script a while ago, before switching to harbor from my own setup.  It works by mapping the current folder to its equivalent that is mounted inside the ollama docker instance, and then dumping the appropriate ollama command to run:\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport os\r\nimport os.path\r\nimport sys\r\nfrom pathlib import Path\r\nimport re\r\n\r\ndef main():\r\n    this_dir = Path(sys.argv[0]).parent.absolute()\r\n\r\n    try:\r\n        gguf_path = Path(sys.argv[1]).absolute()\r\n    except IndexError:\r\n        print(\"No gguf file given to import!\", file=sys.stderr)\r\n        return 20\r\n\r\n    if not gguf_path.exists():\r\n        print(f\"ERROR: expected {gguf_path!r} to be a valid, existing gguf file\", file=sys.stderr)\r\n        return 20\r\n\r\n    if Path(os.path.commonpath((gguf_path, this_dir))) != this_dir:\r\n        print(f\"ERROR: expected {gguf_path!r} to be under {this_dir!r}\", file=sys.stderr)\r\n        return 20\r\n\r\n    model_name = gguf_path.stem\r\n\r\n    if \"-of-0\" in model_name:\r\n        model_name = re.sub(r'-\\d+-of-\\d+', '', model_name)\r\n\r\n    rel_gguf_path = Path(gguf_path.relative_to(this_dir))\r\n    rel_model_path = rel_gguf_path.parent / f\"{model_name}.model\"\r\n\r\n    container_root = \"/data/llms/\"\r\n\r\n    container_gguf_path = container_root / rel_gguf_path\r\n    container_model_path = container_root / rel_model_path\r\n\r\n    with open(rel_model_path, 'w') as model_fp:\r\n        model_fp.write(f\"\"\"FROM {container_gguf_path}\\n\"\"\")\r\n\r\n    print(f\"Model file created/updated. To import, run:\\n\\nollama create -f '{container_model_path}' '{model_name}'\\n\")\r\n\r\n    return 0\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n```\r\n\r\nI think harbor does some similar llm folder mapping to share llm files with as many services as possible.  Perhaps not for ollama since it stores models in its own custom format for most files, but if mapping that llm folder in a standard way, then harbor ollama create could map the create file, if it's in that shared folder, at least.\r\n\r\n\r\nNOTE: being able to create ollama models from modelfiles is very important, to be able to tune models with large contexts so that they use less context and therefore fit into VRAM.",
      "state": "closed",
      "author": "lee-b",
      "author_type": "User",
      "created_at": "2024-09-30T12:28:39Z",
      "updated_at": "2024-10-04T09:33:35Z",
      "closed_at": "2024-10-04T09:33:35Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/46/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/46",
      "api_url": "https://api.github.com/repos/av/harbor/issues/46",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:30.497273",
      "comments": [
        {
          "author": "av",
          "body": "Hi, ollama is one of the older services so it lacks the feature you're describing, many of the CLI services, however, do have it, I agree that allowing ollama CLI access to PWD is a good enhancement for usability when working with modelfiles",
          "created_at": "2024-09-30T13:40:13Z"
        },
        {
          "author": "av",
          "body": "This was added in the [v0.2.1](https://github.com/av/harbor/releases/tag/v0.2.1), let me know if you'll manage to try it out!",
          "created_at": "2024-09-30T20:47:15Z"
        },
        {
          "author": "av",
          "body": "Closing this for now, please feel free to follow-up or open a new issue if needed.",
          "created_at": "2024-10-04T09:33:35Z"
        }
      ]
    },
    {
      "issue_number": 37,
      "title": "Auxiliary containers left running after harbor down",
      "body": "When command `harbor down librechat` or `harbor down bionicgpt` is issued the \"auxiliary\" containers, such as `harbor.lc-db` `harbor.lc-vector` `harbor.lc-search` for Librechat are left running according to `docker ps`. Is this expected behavior? I could not find details in the documentation. I also noticed that these containers are not being removed upon `harbor down` and are still listed in `docker containers list --all`, unlike the \"main\" containers of the respective frontends.\r\n\r\n-------------\r\nI am running harbor version 1.35, on Bluefin DX (Fedora Silverblue 40).",
      "state": "closed",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-09-28T17:24:58Z",
      "updated_at": "2024-10-04T09:32:54Z",
      "closed_at": "2024-10-04T09:32:54Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/37/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/37",
      "api_url": "https://api.github.com/repos/av/harbor/issues/37",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:30.693687",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying things out!\r\n\r\nNo, it's not expected for those containers to remain running after `harbor down`, however that could happen if the down is cancelled prematurely.\r\n\r\nI just tried following workflow locally and it doesn't seem to leave dangling containers:\r\n\r\n```bash\r\n everlier@pop-os",
          "created_at": "2024-09-28T22:46:15Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank  you for helping me with this issue. Trying this again and I found that the `harbor down`  works as expected, however its the command `harbor down <handle>`  which leaves containers running. The containers remain running even after 56 minutes.\r\n```\r\n$harbor down librechat\r\n```\r\n```\r\n[+] Runnin",
          "created_at": "2024-09-29T19:01:33Z"
        },
        {
          "author": "av",
          "body": "Indeed, this is a bug in how the containers to stop are resolved, thank you for additional tests!",
          "created_at": "2024-09-29T19:14:16Z"
        },
        {
          "author": "lee-b",
          "body": "Is this not due to `docker compose up` rather than `docker compose up --remove-orphans` (or the compose down equivalent)?",
          "created_at": "2024-09-30T12:22:00Z"
        },
        {
          "author": "av",
          "body": "The fix was released in one of the recent versions - down command now tries to match running sub-services as well. I'm closing this for now, but please feel free to follow-up or open a new one if needed.",
          "created_at": "2024-10-04T09:32:54Z"
        }
      ]
    },
    {
      "issue_number": 45,
      "title": "comfyui won't start due to :rshared",
      "body": "Not sure what the purpose of rshared is in the comfy ui config -- multiple instances?  I got the following error with it:\r\n\r\n```\r\n$ harbor up comfyui\r\nWARN[0000] Found orphan containers ([harbor.plandex-server harbor.plandex-db harbor.plandex]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\r\n[+] Running 2/3\r\n â § Container harbor.comfyui  Starting                                                                                                                                                              1.1s\r\n âœ” Container harbor.ollama   Running                                                                                                                                                               0.0s\r\n âœ” Container harbor.webui    Started                                                                                                                                                               0.9s\r\nError response from daemon: path /mnt/nvme1/home/lb/AI/harbor/comfyui/workspace is mounted on /mnt/nvme1 but it is not a shared mount\r\n```\r\n\r\nThis patch fixes it:\r\n\r\n```\r\ndiff --git a/compose.comfyui.yml b/compose.comfyui.yml\r\nindex 895508f..d97cd43 100644\r\n--- a/compose.comfyui.yml\r\n+++ b/compose.comfyui.yml\r\n@@ -33,7 +33,7 @@ services:\r\n       - ${HARBOR_COMFYUI_PORTAL_HOST_PORT}:${HARBOR_COMFYUI_PORTAL_HOST_PORT}\r\n       - ${HARBOR_COMFYUI_SYNCTHING_HOST_PORT}:${HARBOR_COMFYUI_SYNCTHING_HOST_PORT}\r\n     volumes:\r\n-      - ./comfyui/workspace:/workspace:rshared\r\n+      - ./comfyui/workspace:/workspace\r\n       # - ./comfyui/storage:/storage\r\n     networks:\r\n       - harbor-network\r\n```\r\n\r\nBut may break some other goal.",
      "state": "closed",
      "author": "lee-b",
      "author_type": "User",
      "created_at": "2024-09-30T12:20:25Z",
      "updated_at": "2024-10-04T09:31:15Z",
      "closed_at": "2024-10-04T09:31:15Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/45/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/45",
      "api_url": "https://api.github.com/repos/av/harbor/issues/45",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:30.909171",
      "comments": [
        {
          "author": "lee-b",
          "body": "Hmm. This may be a red herring, warning, not fatal error.  Seems it won't ACTUALLY start regardless.  Confirming.",
          "created_at": "2024-09-30T18:41:13Z"
        },
        {
          "author": "av",
          "body": "Thanks for reporting nonetheless, this was a leftover from the initial testing, I removed it [v0.2.1](https://github.com/av/harbor/releases/tag/v0.2.1)",
          "created_at": "2024-09-30T20:26:13Z"
        },
        {
          "author": "av",
          "body": "Closing for now, feel free to follow-up if needed",
          "created_at": "2024-10-04T09:31:15Z"
        }
      ]
    },
    {
      "issue_number": 47,
      "title": "Unexpected error: No such file or directory (os error 2)",
      "body": "I tried the appimage and deb version, I just see this on every page except settings\r\n![image](https://github.com/user-attachments/assets/8bdc30d7-9c27-4452-b302-c8d8cd1a365a)\r\n",
      "state": "closed",
      "author": "uninstall-your-browser",
      "author_type": "User",
      "created_at": "2024-09-30T12:31:49Z",
      "updated_at": "2024-10-04T09:30:48Z",
      "closed_at": "2024-10-04T09:30:48Z",
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/47/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/47",
      "api_url": "https://api.github.com/repos/av/harbor/issues/47",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:31.145244",
      "comments": [
        {
          "author": "av",
          "body": "Do you have Harbor CLI installed? The app is the companion for the CLI and won't work without it. There'll be a better detection and warning / settings for this in the future versions.",
          "created_at": "2024-09-30T13:28:43Z"
        },
        {
          "author": "uninstall-your-browser",
          "body": "I don't, I didn't realise it was needed",
          "created_at": "2024-09-30T23:55:40Z"
        },
        {
          "author": "av",
          "body": "I'm closing this for now, please feel free to open another one if needed",
          "created_at": "2024-10-04T09:30:48Z"
        }
      ]
    },
    {
      "issue_number": 24,
      "title": "NexaAI integration",
      "body": "Another interesting backend:\r\nhttps://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#docker-usage",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-09-19T11:40:31Z",
      "updated_at": "2024-10-03T22:08:05Z",
      "closed_at": "2024-10-03T22:08:05Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/24/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/24",
      "api_url": "https://api.github.com/repos/av/harbor/issues/24",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:31.353579",
      "comments": [
        {
          "author": "bhupesh-sf",
          "body": "Love to see it here, today only I was planning to experiment with it and here I found it's already in the works. Thank you very much",
          "created_at": "2024-09-24T16:37:08Z"
        },
        {
          "author": "av",
          "body": "Added in v0.2.5, unfortunately Nexa server isn't feature-complete yet, so it's only partial at this stage",
          "created_at": "2024-10-03T22:08:05Z"
        }
      ]
    },
    {
      "issue_number": 32,
      "title": "macOS `sed` incompatibility in harbor.sh",
      "body": "When I first ran `./harbor.sh ln` on macOS, I got an error message:\r\n```\r\nsed: 1: \".env\": invalid command code .\r\n```\r\n\r\nLooks like it was caused by a [difference in GNU sed and macOS sed](https://stackoverflow.com/questions/4247068/sed-command-with-i-option-failing-on-mac-but-works-on-linux).\r\n\r\nI was able to get it running with:\r\n```bash\r\nbrew install gnu-sed\r\nPATH=\"$(brew --prefix gnu-sed)/libexec/gnubin:$PATH\" ./harbor.sh link\r\n```",
      "state": "closed",
      "author": "montasaurus",
      "author_type": "User",
      "created_at": "2024-09-25T15:51:04Z",
      "updated_at": "2024-09-30T20:48:25Z",
      "closed_at": "2024-09-30T20:48:25Z",
      "labels": [
        "bug",
        "OS:MacOS"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/32/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/32",
      "api_url": "https://api.github.com/repos/av/harbor/issues/32",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:31.535511",
      "comments": [
        {
          "author": "av",
          "body": "Thank you for trying out Harbor!\r\n\r\nShell portability made me rethink initial design choices multiple time. I though I covered cases [like this](https://github.com/av/harbor/blob/main/harbor.sh#L776), but apparently not all of them yet.\r\n\r\nI pushed a hotfix, it should be available via `harbor update",
          "created_at": "2024-09-25T18:17:19Z"
        },
        {
          "author": "av",
          "body": "Closing, [v0.1.32](https://github.com/av/harbor/releases/tag/v0.1.32) included a few other compatibility fixes for MacOS",
          "created_at": "2024-09-30T20:48:21Z"
        }
      ]
    },
    {
      "issue_number": 48,
      "title": "Name conflict",
      "body": "`harbor` is a very well-known name in containerisation circles, for `https://goharbor.io/`.  Difficult step, but I'd suggest renaming this (even in a small, subtle way, like to `harbor-ai`, to avoid confusion/conflict.  I think it would be better to do this sooner rather than waiting until the project gets bigger, as it will only get harder to change as more people know the name.",
      "state": "open",
      "author": "lee-b",
      "author_type": "User",
      "created_at": "2024-09-30T12:52:05Z",
      "updated_at": "2024-09-30T13:34:48Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/48/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/48",
      "api_url": "https://api.github.com/repos/av/harbor/issues/48",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:31.713413",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for raising this, I'm considering a few possible options:\r\n- domainify the name: harbor.sh for example, exactly after the CLI name\r\n- use a self-expanding acronym that'll include Harbor in it's name, for example HBR - Harbor Bash Resource\r\n\r\nI agree that it could be a concern, but I doubt tha",
          "created_at": "2024-09-30T13:34:23Z"
        }
      ]
    },
    {
      "issue_number": 17,
      "title": "This project is insanely amazing !!!",
      "body": "Hey,\r\n\r\nThanks for putting this up. This is something I face regularly. Every time, there is a tool I want to try I have to take the pain of all the setup and issues faced during that. \r\n\r\nI always wondered, if there could be a unified solution that can help to simply plug and play any tool and this is what I got here.\r\n\r\nI hope we will get the apple silicon support soon and be able to try out the stuff which I still cannot.",
      "state": "open",
      "author": "bhupesh-sf",
      "author_type": "User",
      "created_at": "2024-09-16T17:42:15Z",
      "updated_at": "2024-09-30T12:31:43Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/17/reactions",
        "total_count": 7,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 3,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/17",
      "api_url": "https://api.github.com/repos/av/harbor/issues/17",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:31.876365",
      "comments": [
        {
          "author": "av",
          "body": "Thank you for the feedback, I really appreciate it!",
          "created_at": "2024-09-16T17:46:57Z"
        },
        {
          "author": "lee-b",
          "body": "Agree, I had a similar set up with lots of docker compose files, but this is very nicely polished and integrated.  I'll be using it in future, and contributing where I can.",
          "created_at": "2024-09-30T12:31:41Z"
        }
      ]
    },
    {
      "issue_number": 33,
      "title": "Update to latest version causes git conflict",
      "body": "Upon clean installation of `harbor` the version 1.9 is installed and when the command `harbor update --latest` is issued a merge conflict is reported by git:\r\n```\r\nUpdating to the bleeding edge version...\r\nremote: Enumerating objects: 302, done.\r\nremote: Counting objects: 100% (302/302), done.\r\nremote: Compressing objects: 100% (216/216), done.\r\nremote: Total 247 (delta 44), reused 176 (delta 15), pack-reused 0 (from 0)\r\nReceiving objects: 100% (247/247), 110.92 KiB | 1.71 MiB/s, done.\r\nResolving deltas: 100% (44/44), completed with 31 local objects.\r\nFrom https://github.com/av/harbor\r\n * [new branch]      main       -> main\r\nPrevious HEAD position was 6ccc5c9 chore: v0.1.9 bump\r\nSwitched to branch 'main'\r\nhint: You have divergent branches and need to specify how to reconcile them.\r\nhint: You can do so by running one of the following commands sometime before\r\nhint: your next pull:\r\nhint:\r\nhint:   git config pull.rebase false  # merge\r\nhint:   git config pull.rebase true   # rebase\r\nhint:   git config pull.ff only       # fast-forward only\r\nhint:\r\nhint: You can replace \"git config\" with \"git config --global\" to set a default\r\nhint: preference for all repositories. You can also pass --rebase, --no-rebase,\r\nhint: or --ff-only on the command line to override the configured default per\r\nhint: invocation.\r\nfatal: Need to specify how to reconcile divergent branches.\r\nMerging .env files...\r\nMerged content from default.env into .env, preserving order and structure\r\nHarbor updated successfully.\r\n```\r\nEven though the `harbor` reports successful update, it is effectively broken because the profile in `default.env` is not updated to contain new environment variables. This leads to `harbor` commands being broken, e.g.\r\n```\r\nharbor version\r\n/var/home/FrantaNautilus/.local/bin/harbor: line 1791: : No such file or directory\r\n/var/home/FrantaNautilus/.local/bin/harbor: line 1794: : No such file or directory\r\n/var/home/FrantaNautilus/.local/bin/harbor: line 1794: [: : integer expression expected\r\nHarbor CLI version: 0.1.31\r\n```\r\nIn order to get a fully working latest installation I had to perform it manually and it seams that manual update is currently the only working option.",
      "state": "closed",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-09-27T16:41:54Z",
      "updated_at": "2024-09-29T19:13:02Z",
      "closed_at": "2024-09-29T19:03:54Z",
      "labels": [
        "bug",
        "OS:Linux"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/33/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/33",
      "api_url": "https://api.github.com/repos/av/harbor/issues/33",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:32.121895",
      "comments": [
        {
          "author": "av",
          "body": "Thank you so much for trying out Harbor and for reporting the issue!\r\n\r\nThe current install/update is indeed poorly implemented. We'll be switching away from using shell for this in the future. The current iteration of the issue was caused by multiple things adding up together:\r\n\r\n- Install script w",
          "created_at": "2024-09-27T19:09:05Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank you for a quick fix. Only now I have realized I forgot to mention my OS, I am not on Mac OS but on Fedora Silverblue (derivative called Bluefin DX, this is the reason my home directory is under `/var`). I don't know if that changes anything about the causes of the problem, but I don't want to ",
          "created_at": "2024-09-27T19:34:40Z"
        },
        {
          "author": "av",
          "body": "Thank you for pointing that out, I was completely blinded by having this problem being related to MacOS setup specifically. But indeed, it was broken more generally after the previous fix attempts",
          "created_at": "2024-09-27T22:55:22Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Tested updating to CLI version: 0.2.0 via `harbor update -l` and no problems appeared. Config merging works fine.",
          "created_at": "2024-09-29T19:03:54Z"
        },
        {
          "author": "av",
          "body": "That's great to hear! Thank you so much for the additional tests and a follow-up! Sorry for not getting back to you on the other issues, I really wanted to release the app before this Monday, as I'll have drastically less time for Harbor now",
          "created_at": "2024-09-29T19:13:01Z"
        }
      ]
    },
    {
      "issue_number": 35,
      "title": "Librechat permission issues and not picking up Ollama",
      "body": "I am trying to use Librechat with `harbor` and there are two issues that prevent it from working. I am reporting these together, because I am not sure these are not linked. OS: Bluefin DX (Fedora Silverblue 40, with docker and docker compose)\r\n\r\n#### sub-issue 1\r\nWhen opening Librechat with `harbor librechat`, it intermediately crashes, here is the output of `harbor logs librechat`:\r\n```\r\nWARN[0000] The \"HARBOR_WHISPER_HOST_PORT\" variable is not set. Defaulting to a blank string. \r\nWARN[0000] The \"HARBOR_WHISPER_VERSION\" variable is not set. Defaulting to a blank string. \r\nharbor.librechat  | \r\nharbor.librechat  | > LibreChat@v0.7.5-rc2 backend\r\nharbor.librechat  | > cross-env NODE_ENV=production node api/server/index.js\r\nharbor.librechat  | \r\nharbor.librechat  | 2024-09-27 19:07:28 info: [Optional] Redis not initialized. Note: Redis support is experimental.\r\nharbor.librechat  | 2024-09-27 19:07:29 error: There was an uncaught error: EACCES: permission denied, open '/app/api/logs/meiliSync-2024-09-27.log'\r\n```\r\nI tried to fix this by changing the ownership of the files in `.harbor/librechat` to my user (specifically `sudo chmod -R user:user .harbor/librechat`) and I was able to get Librechat started.\r\n\r\n#### sub-issue 2\r\nLibrechat opened with `harbor up librechat ollama` does not pick up the running `ollama` and seams to have a problem finding the RAG container. Specifically, when the `librachat` is opened and Ollama provider is selected, there are no models listed. Additionally when a message is submitted expecting default model to be running, an error is displayed that `ollama` cannot be found.\r\nLog from `harbor logs librechat`\r\n```\r\nWARN[0000] The \"HARBOR_WHISPER_VERSION\" variable is not set. Defaulting to a blank string. \r\nWARN[0000] The \"HARBOR_WHISPER_HOST_PORT\" variable is not set. Defaulting to a blank string. \r\nharbor.librechat  | 2024-09-27 19:11:46 warn: RAG API is either not running or not reachable at http://lcrag:33892, you may experience errors with file uploads.\r\nharbor.librechat  | 2024-09-27 19:11:46 info: Updating 'USER' role MULTI_CONVO 'USE' permission from false to: true\r\nharbor.librechat  | 2024-09-27 19:11:46 info: Updated 'USER' role permissions\r\nharbor.librechat  | 2024-09-27 19:11:46 info: No changes needed for 'ADMIN' role permissions\r\nharbor.librechat  | 2024-09-27 19:11:46 info: \r\nharbor.librechat  | Outdated Config version: 1.1.5\r\nharbor.librechat  | Latest version: 1.1.7\r\nharbor.librechat  | \r\nharbor.librechat  |       Check out the Config changelogs for the latest options and features added.\r\nharbor.librechat  | \r\nharbor.librechat  |       https://www.librechat.ai/changelog\r\nharbor.librechat  | \r\nharbor.librechat  | \r\nharbor.librechat  | 2024-09-27 19:11:46 info: Server listening on all interfaces at port 33891. Use http://localhost:33891 to access it\r\nharbor.librechat  | {\"level\":\"info\",\"message\":\"[Login] [Login successful] [Username: user] [Request-IP: 172.19.0.1]\",\"timestamp\":\"2024-09-27T19:13:15.650Z\"}\r\nharbor.librechat  | 2024-09-27 19:13:15 error: Failed to derive base URL Invalid URL\r\nharbor.librechat  | 2024-09-27 19:13:15 error: Failed to fetch models from Ollama API. If you are not using Ollama directly, and instead, through some aggregator or reverse proxy that handles fetching via OpenAI spec, ensure the name of the endpoint doesn't start with `ollama` (case-insensitive). Cannot read properties of undefined (reading 'status')\r\nharbor.librechat  | 2024-09-27 19:13:20 error: Failed to fetch models from SGLang API\r\nharbor.librechat  | The request either timed out or was unsuccessful. Error message:\r\nharbor.librechat  |  Cannot read properties of undefined (reading 'status')\r\nharbor.librechat  | 2024-09-27 19:26:56 error: [handleAbortError] AI response error; aborting request: {\"type\":\"no_user_key\"}\r\nharbor.librechat  | 2024-09-27 19:26:56 warn: Invalid conversation ID: null\r\n```\r\nLooking at the config file `librechat.yml`, it looks like the Librechat is expecting Ollama to be running at default URL, that being  `http://ollama:11434`, however in my case Ollama is automatically started on `http://localhost:33821`.",
      "state": "closed",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-09-27T19:54:54Z",
      "updated_at": "2024-09-28T00:35:15Z",
      "closed_at": "2024-09-28T00:35:14Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/35/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/35",
      "api_url": "https://api.github.com/repos/av/harbor/issues/35",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:32.309431",
      "comments": [
        {
          "author": "av",
          "body": "> When opening Librechat with `harbor librechat`, it intermediately crashes\r\n\r\nThis is happening because of the poorly arranged dependencies between sub-services, it wasn't caught during development as the compose file was created gradually, so the same problem wasn't surfaced\r\n\r\n```bash\r\n# Use this",
          "created_at": "2024-09-27T22:46:02Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Thank you so much for detailed reply and quick fix. I confirm that Librechat now works with ollama without problems.\r\nJust reading through the documentation and code feels like a masterclass in Docker and Bash.",
          "created_at": "2024-09-28T00:35:15Z"
        }
      ]
    },
    {
      "issue_number": 34,
      "title": "harbor doctor does not detect docker compose",
      "body": "Upon clean installation of `harbor` I noticed that upon running the `harbor doctor` the docker compose is not detected, although it is present. This seams to be caused by using command `docker-compose` instead of `docker compose`. According to in formation in Docker documentation ([https://docs.docker.com/compose/releases/migrate/](https://docs.docker.com/compose/releases/migrate/)), `docker-compose` was superseded by  `docker compose`, and in consequence the command `docker-compose` is not available on all system with working Docker compose.",
      "state": "closed",
      "author": "FrantaNautilus",
      "author_type": "User",
      "created_at": "2024-09-27T17:47:34Z",
      "updated_at": "2024-09-28T00:27:25Z",
      "closed_at": "2024-09-28T00:27:25Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/34/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/34",
      "api_url": "https://api.github.com/repos/av/harbor/issues/34",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:32.508211",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for being thorough with the reports! You noticing and using `harbor doctor` also makes me a very happy maintainer! \r\n\r\nLuckily, it's just a leftover in doctor script itself, the rest of the CLI was already using `docker compose` as expected. \r\n\r\nI've pushed the fix with v0.1.34",
          "created_at": "2024-09-27T19:28:05Z"
        },
        {
          "author": "FrantaNautilus",
          "body": "Just tested the command and report is now correct. Thank you for incredibly fast fix. Closing the issue.",
          "created_at": "2024-09-28T00:27:25Z"
        }
      ]
    },
    {
      "issue_number": 31,
      "title": "Clarification on Frontend and Backend Compatibility with Multiple Models",
      "body": "I have a few questions regarding frontend and backend compatibility:\r\n\r\n1. Frontend-Backend Compatibility: Does any frontend work with any backend? I noticed that most backends are designed to work with GGUF models, but this project seems to support various backends, including vLLM, Mistral.rs, etc. If I select one of these backends, will it be compatible with frontends like Open WebUI and other similar platforms?\r\n2. Closed-Source Frontend Support: Currently, I have a closed-source frontend that is only compatible with GGUF models. If I choose to use this project, is it possible to expose a backend that my closed-source frontend can consume, allowing it to support multiple types of backends?\r\n\r\nIâ€™d appreciate any guidance or clarification on these points!",
      "state": "closed",
      "author": "bhupesh-sf",
      "author_type": "User",
      "created_at": "2024-09-24T16:40:13Z",
      "updated_at": "2024-09-27T06:48:11Z",
      "closed_at": "2024-09-27T06:48:11Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/31/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/31",
      "api_url": "https://api.github.com/repos/av/harbor/issues/31",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:32.751596",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying out Harbor!\r\n\r\n1. It's only mentioned in the docs implicitly, but the support is not complete (nor it could be, to be fair)\r\n\r\nThe policy is to support everything for the main default UI and default Backend. I.e. - integrate all UIs with the default backend and all UIs with the def",
          "created_at": "2024-09-24T16:59:23Z"
        },
        {
          "author": "bhupesh-sf",
          "body": "Thanks for the answer, it does clarifies ðŸ‘ðŸ» ",
          "created_at": "2024-09-27T01:32:57Z"
        },
        {
          "author": "av",
          "body": "Awesome, feel free to reopen if needed",
          "created_at": "2024-09-27T06:48:11Z"
        }
      ]
    },
    {
      "issue_number": 30,
      "title": "Boost - failing when streaming is disabled in the Open WebUI",
      "body": "- Disable streaming in the Open WebUI\r\n- Use external models via boost \r\n- ?",
      "state": "closed",
      "author": "av",
      "author_type": "User",
      "created_at": "2024-09-24T15:03:21Z",
      "updated_at": "2024-09-25T18:19:15Z",
      "closed_at": "2024-09-25T18:19:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/30/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/30",
      "api_url": "https://api.github.com/repos/av/harbor/issues/30",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:32.928117",
      "comments": [
        {
          "author": "av",
          "body": "Was fixed in the v0.1.31",
          "created_at": "2024-09-25T18:19:15Z"
        }
      ]
    },
    {
      "issue_number": 22,
      "title": "Support Standalone Speech to Text Backends",
      "body": "Hello,\r\n\r\nIt seems that the project only support text-to-speech backends such as Parler and openedai-speech (offering piped and coqai tts). However there is no speech-to-text backends (I think the only one to exist is ones offering Whisper). Some frontends like Open WebUI have Whisper integrated, but other like LibreChat do not. I think it would add to the flexibility of the project if a Whisper server can be a standalone backend\r\n \r\nI wonder if you can add support to stt modules and maybe also allow choosing between the different model sizes. \r\n\r\nAfter some digging, I found that similar to Parler TTS, this project https://github.com/fedirz/faster-whisper-server also offers an openAI compatible server with a docker image, so it shouldn't be hard to add. While it is not the same original whisper model, it is the same one used by Open WebUI according to https://github.com/open-webui/open-webui/blob/b64c9d966a6498d4f2ffe12ab1498fab298afb37/Dockerfile#L129 .\r\n\r\nBest regards,\r\nMY",
      "state": "closed",
      "author": "maeyounes",
      "author_type": "User",
      "created_at": "2024-09-17T19:38:58Z",
      "updated_at": "2024-09-24T17:44:37Z",
      "closed_at": "2024-09-24T17:44:37Z",
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/22/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/22",
      "api_url": "https://api.github.com/repos/av/harbor/issues/22",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:33.095949",
      "comments": [
        {
          "author": "av",
          "body": "Yes, I agree that a STT is a missing piece. It makes perfect sense to have that available as a service. I also hope that openedai-speech (or another abstract project) would allow for a more encompassing setup later.",
          "created_at": "2024-09-17T19:49:37Z"
        },
        {
          "author": "av",
          "body": "Added `stt` service in [v0.1.28](https://github.com/av/harbor/releases/tag/v0.1.28)",
          "created_at": "2024-09-23T11:03:26Z"
        },
        {
          "author": "av",
          "body": "Closing for now, feel free to follow up if there's anything to add with the STT ",
          "created_at": "2024-09-24T17:44:11Z"
        }
      ]
    },
    {
      "issue_number": 29,
      "title": "Problem building Fabric Satellite",
      "body": "I am facing an issue when running fabric:\r\n\r\n```\r\nocto@kame-house:~> harbor build fabric\r\n[+] Building 10.0s (12/14)                                                               docker:default\r\n => [fabric internal] load build definition from Dockerfile                                        0.0s\r\n => => transferring dockerfile: 592B                                                               0.0s\r\n => [fabric internal] load metadata for docker.io/pkgxdev/pkgx:latest                              0.9s\r\n => [fabric internal] load .dockerignore                                                           0.0s\r\n => => transferring context: 2B                                                                    0.0s\r\n => [fabric  1/10] FROM docker.io/pkgxdev/pkgx:latest@sha256:b8042d48820c079898d69e024b1d38711d8a  0.0s\r\n => [fabric internal] load build context                                                           0.0s\r\n => => transferring context: 36B                                                                   0.0s\r\n => CACHED [fabric  2/10] WORKDIR /app                                                             0.0s\r\n => CACHED [fabric  3/10] RUN pkgx install git pipx python ffmpeg                                  0.0s\r\n => CACHED [fabric  4/10] RUN python --version && pipx --version && ffmpeg -version                0.0s\r\n => CACHED [fabric  5/10] RUN git clone https://github.com/danielmiessler/fabric.git               0.0s\r\n => CACHED [fabric  6/10] RUN echo 'export LD_LIBRARY_PATH=$(find / -name \"*.so\" -exec dirname {}  0.0s\r\n => CACHED [fabric  7/10] WORKDIR /app/fabric                                                      0.0s\r\n => ERROR [fabric  8/10] RUN pipx install .                                                        8.9s\r\n------                                                                                                  \r\n > [fabric  8/10] RUN pipx install .:                                                                   \r\n1.882 creating virtual environment...                                                                   \r\n1.940 creating shared libraries...                                                                      \r\n6.072 upgrading shared libraries...                                                                     \r\n8.050 determining package name from '/app/fabric'...                                                    \r\n8.651 ERROR: Directory '/app/fabric' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\r\n8.652 Cannot determine package name from spec '/app/fabric'. Check package spec for\r\n8.652 errors.\r\n------\r\nfailed to solve: process \"/bin/bash -c pipx install .\" did not complete successfully: exit code: 1\r\n\r\n\r\n```\r\n\r\nApparently the setup.py and project.toml files are missing from the /app/fabric directory. I have checked the original fabric repository, and also the files `compose.x.fabric.ollama.yml `, `compose.fabric.yml `and `fabric/start_fabric.sh` but I can't really spot anything. ",
      "state": "closed",
      "author": "SimonBlancoE",
      "author_type": "User",
      "created_at": "2024-09-23T12:31:20Z",
      "updated_at": "2024-09-23T20:48:48Z",
      "closed_at": "2024-09-23T20:46:19Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/29/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/29",
      "api_url": "https://api.github.com/repos/av/harbor/issues/29",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:33.271296",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for bringing this to my attention.\r\n\r\nIndeed, `fabric` was updated and is now completely different compared to the version that was integrated with Harbor.\r\n\r\nI've updated the integration in https://github.com/av/harbor/releases/tag/v0.1.30",
          "created_at": "2024-09-23T14:01:18Z"
        },
        {
          "author": "SimonBlancoE",
          "body": "No, thank you for developing this amazing tool!\r\n\r\nNow, I could build fabric, but it gets stuck for me when using `harbor fabric --setup`\r\n\r\n```\r\nocto@kame-house:~/go> harbor fabric --setup\r\n[+] Building 0.0s (0/0)                                                                                      ",
          "created_at": "2024-09-23T16:58:46Z"
        },
        {
          "author": "av",
          "body": "@SimonBlancoE , oh, sorry! I know exactly what it is, I missed `--setup` as one of the commands that should be exempt from TTY flag in docker.\r\n\r\nI just pushed a hotfix, until it's released you can try it by upgrading to the latest, via:\r\n```bash\r\nharbor update --latest\r\n```",
          "created_at": "2024-09-23T17:03:01Z"
        },
        {
          "author": "SimonBlancoE",
          "body": "I was just going to edit my latest message, I added the `--setup `command to the exempt list and made it work.  xD",
          "created_at": "2024-09-23T17:04:58Z"
        },
        {
          "author": "SimonBlancoE",
          "body": "One more quirk, and a workaround:\r\n\r\n1.- Although I could detect the ollama url and select `llama3.1:latest `in the fabric setup, if you try a command like `cat $(h home)/harbor.sh | head -n 100 | h fabric -sp create_npc`, the console will say `model \"llama3.1:8b\" not found, try pulling it first`. \r",
          "created_at": "2024-09-23T17:34:05Z"
        }
      ]
    },
    {
      "issue_number": 19,
      "title": "Using a different Ollama URL?",
      "body": "Hey there. Great project, very handy.\r\n\r\nI have ollama installed on my LLM server and its already integrated with a ton of other services. Is it possible to have harbor use the ollama service I already have installed, while still maintaining the rest of the integration?\r\n\r\nThanks",
      "state": "closed",
      "author": "zheroz00",
      "author_type": "User",
      "created_at": "2024-09-17T01:04:02Z",
      "updated_at": "2024-09-22T16:25:41Z",
      "closed_at": "2024-09-22T16:25:41Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/19/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/19",
      "api_url": "https://api.github.com/repos/av/harbor/issues/19",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:33.448767",
      "comments": [
        {
          "author": "maeyounes",
          "body": "Isn't it possible in this scenario to just stop the old ollama server and change the volume directories in the harbor ollama docker compose file to point to your old ollama server files (model files and other configuration files) ?",
          "created_at": "2024-09-17T11:09:53Z"
        },
        {
          "author": "av",
          "body": "Yes, right now it's already possible to specify the custom cache location, to at least share the downloaded blobs:\r\n```bash\r\nharbor config get ollama.cache\r\nharbor config set ollama.cache ~/path/to/cache\r\n```\r\n\r\nHowever, I agree that you might want to use a completely external Ollama instance. I've ",
          "created_at": "2024-09-17T12:51:08Z"
        },
        {
          "author": "zheroz00",
          "body": "@av This is great! Ty, So if I already have Ollama installed as a service at 10.10.10.158:11434, should I be able to run 'harbor config set ollama.internal_url http://10.10.10.158:11434' and it should use that by default? How does this affect the yaml files and the ollama containers they typically s",
          "created_at": "2024-09-17T14:37:39Z"
        },
        {
          "author": "av",
          "body": "I assume that `10.10.10.158:11434` is ping-able from the host, what you want to use a version that is ping-able in the same way from Harbor containers.\r\n\r\nOne way to test that is:\r\n```bash\r\n# spin up some services\r\nharbor up\r\n\r\n# Enter webui container\r\nharbor exec webui bash\r\n\r\n# Try to find where y",
          "created_at": "2024-09-17T14:59:43Z"
        },
        {
          "author": "av",
          "body": "Closing the issue, please let me know how the tests go - we'll reopen if needed",
          "created_at": "2024-09-22T16:25:41Z"
        }
      ]
    },
    {
      "issue_number": 25,
      "title": "[question] Unable to upgrade Dify.AI",
      "body": "When I attempt to pull the latest Dify.AI build I only pull` 0.6.16`, am I missing a command or process to update it to the latest?\r\n\r\nAlso just want to say I am in awe with how easy it is to use this software. Incredible work!",
      "state": "closed",
      "author": "ZacharyKehlGEAppliances",
      "author_type": "User",
      "created_at": "2024-09-20T13:50:41Z",
      "updated_at": "2024-09-20T13:58:42Z",
      "closed_at": "2024-09-20T13:56:33Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/25/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/25",
      "api_url": "https://api.github.com/repos/av/harbor/issues/25",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:33.641314",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying out Harbor!\r\n\r\nDify was one of the more complex projects to integrate, so unfortunately it comes with a pinned version. It can be bumped manually via `harbor config set dify.version <version number>`. Unfortunately, due to how project is setup it's not guaranteed to continue workin",
          "created_at": "2024-09-20T13:54:55Z"
        },
        {
          "author": "ZacharyKehlGEAppliances",
          "body": "Ahh understandable. Thank you for the quick response! ",
          "created_at": "2024-09-20T13:56:33Z"
        },
        {
          "author": "av",
          "body": "No pbs, mentioned this in Dify docs:\r\nhttps://github.com/av/harbor/wiki/2.3.3-Satellite:-Dify#updating-dify",
          "created_at": "2024-09-20T13:58:41Z"
        }
      ]
    },
    {
      "issue_number": 13,
      "title": "Add MLX Backend",
      "body": "Is it possible to add MLX (eg. MLX-LM or FastMLX) backends to Harbor?",
      "state": "open",
      "author": "parthpat12",
      "author_type": "User",
      "created_at": "2024-09-13T11:05:24Z",
      "updated_at": "2024-09-19T11:41:04Z",
      "closed_at": null,
      "labels": [
        "new service"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/13/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/13",
      "api_url": "https://api.github.com/repos/av/harbor/issues/13",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:33.848544",
      "comments": [
        {
          "author": "av",
          "body": "Unfortunately the GPU support is not possible with Docker on MacOS at the moment\r\n\r\nhttps://docs.docker.com/desktop/gpu/\r\n\r\nSo, it's possible to integrate these, but they won't be utilised to their full potential ",
          "created_at": "2024-09-13T15:41:00Z"
        }
      ]
    },
    {
      "issue_number": 20,
      "title": "Permission issue with LibreChat Deployment",
      "body": "Hello,\r\n\r\nI'm running into a permission issue when deploying LibreChat under Harbor. I get the following error when starting the container:\r\n```\r\n> LibreChat@v0.7.5-rc2 backend\r\n> cross-env NODE_ENV=production node api/server/index.js\r\n\r\n2024-09-17 11:37:21 info: [Optional] Redis not initialized. Note: Redis support is experimental.\r\n2024-09-17 11:37:23 error: There was an uncaught error: EACCES: permission denied, open '/app/api/logs/meiliSync-2024-09-17.log'\r\n```\r\n\r\nThen the container stops and exits.\r\n\r\nI tried reverting to an older image version than the latest one but I still get the same error.\r\nI also tried setting the variables PUID and PGID to 1000.\r\nNone of this worked.\r\n\r\nDeploying with the docker-compose file from the original LibreChat repository does not have this issue.",
      "state": "closed",
      "author": "maeyounes",
      "author_type": "User",
      "created_at": "2024-09-17T09:43:02Z",
      "updated_at": "2024-09-17T13:36:10Z",
      "closed_at": "2024-09-17T11:05:49Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/20/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/20",
      "api_url": "https://api.github.com/repos/av/harbor/issues/20",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:34.055911",
      "comments": [
        {
          "author": "maeyounes",
          "body": "I solved this problem by adding the line `user: \":\"` to the docker compose file as the original repository had `user: \"${UID}:${GID}\"`. It does not work with value 1000 for `UID` and `GID` for some reason.",
          "created_at": "2024-09-17T11:05:49Z"
        },
        {
          "author": "av",
          "body": "Sorry, you were so quick I didn't manage to provide the feedback in time!\r\n\r\nFS permissions are a classic issue when mounting host volumes with docker. So, there's a built-in (underdocumented, though) util to fix:\r\n\r\n```bash\r\nharbor fixfs\r\n```\r\n\r\nIt'll prompt you for the user password and will run a",
          "created_at": "2024-09-17T13:35:12Z"
        }
      ]
    },
    {
      "issue_number": 18,
      "title": "Support for KTransformers",
      "body": "\r\nHello,\r\n\r\nThanks for putting the work into this amazing project. I really enjoy its flexibility.\r\n\r\nI wonder if there are plans to add support for the backend KTransformers https://github.com/kvcache-ai/ktransformers as it probably the best framework to serve mixture of expert type models such as Mixtral and DeepSeek-Coder models locally. \r\n\r\nBest regards,\r\nMY",
      "state": "closed",
      "author": "maeyounes",
      "author_type": "User",
      "created_at": "2024-09-16T18:55:49Z",
      "updated_at": "2024-09-17T12:56:20Z",
      "closed_at": "2024-09-17T12:27:45Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/18/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/18",
      "api_url": "https://api.github.com/repos/av/harbor/issues/18",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:34.257588",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for bringing it to my attention, I'm working on the integration now",
          "created_at": "2024-09-16T20:44:47Z"
        },
        {
          "author": "av",
          "body": "Integrated in [v0.1.25](https://github.com/av/harbor/releases/tag/v0.1.25)\r\n\r\nWhile working on this, I remembered that I already tried to integrate it in the past and failed. This time, after putting more effort, I worked around the issues, however, I'd advice on using the service cautiously. The pr",
          "created_at": "2024-09-17T12:27:45Z"
        },
        {
          "author": "maeyounes",
          "body": "Thank you very much for this. I will take your advice into account. ",
          "created_at": "2024-09-17T12:56:19Z"
        }
      ]
    },
    {
      "issue_number": 15,
      "title": "Using a different inference engine with ChatUI or WebUI",
      "body": "For a few reasons, I need to use a non-llama.cpp based inference engine. I couldn't figure out how to get a working UI using Harbor, VLLM and ChatUI or WebUI (other than just VLLM serve by itself). Could you share a few more examples / demos that don't depend on Ollama / Llama.cpp?",
      "state": "closed",
      "author": "kinchahoy",
      "author_type": "User",
      "created_at": "2024-09-14T07:51:40Z",
      "updated_at": "2024-09-17T12:29:24Z",
      "closed_at": "2024-09-17T12:29:23Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/15/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/15",
      "api_url": "https://api.github.com/repos/av/harbor/issues/15",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:34.482626",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for trying out Harbor!\r\n\r\nTo permanently switch from  `ollama` to `vllm`:\r\n```bash\r\nharbor defaults rm ollama\r\nharbor defaults add vllm\r\n```\r\n\r\nI've mentioned this in the [high-level user guide](https://github.com/av/harbor/wiki/1.-Harbor-User-Guide#default-services)\r\n\r\nThis will come wit",
          "created_at": "2024-09-14T09:00:55Z"
        },
        {
          "author": "av",
          "body": "Just released `harbor profile` functionality in [v0.1.21](https://github.com/av/harbor/releases/tag/v0.1.21), could be helpful in this context",
          "created_at": "2024-09-14T12:12:44Z"
        },
        {
          "author": "av",
          "body": "Feel free to reopen if any more follow-ups are needed",
          "created_at": "2024-09-17T12:29:23Z"
        }
      ]
    },
    {
      "issue_number": 16,
      "title": "No such service : ol1",
      "body": "![image](https://github.com/user-attachments/assets/523c8c48-2256-43be-bde5-b92c5d1f2e31)\r\n\r\nEnv:\r\n```\r\nHarbor CLI version: 0.1.9\r\n==========================\r\nHarbor active services:\r\nwebui\r\n==========================\r\nClient: Docker Engine - Community\r\n Version:    27.2.1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.16.2\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.29.2\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 27.2.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c\r\n runc version: v1.1.14-0-g2c9f560\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.153.1-microsoft-standard-WSL2\r\n Operating System: Ubuntu 24.04.1 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 20\r\n Total Memory: 15.49GiB\r\n Name: Tom2024\r\n ID: 1ec0703a-6d34-4969-ae93-6d711f141527\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```",
      "state": "closed",
      "author": "avcode-exe",
      "author_type": "User",
      "created_at": "2024-09-16T16:01:01Z",
      "updated_at": "2024-09-16T16:20:19Z",
      "closed_at": "2024-09-16T16:20:19Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/16/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/16",
      "api_url": "https://api.github.com/repos/av/harbor/issues/16",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:34.716772",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for trying out Harbor!\r\n\r\n```\r\nHarbor CLI version: 0.1.9\r\n```\r\n\r\n`ol1` was just released today in scope of v0.1.24, so please try updating to the most recent version. Update might not work smoothly yet, in that instance resort to manual checkout of the `v0.1.24` tag (or simply latest `mai",
          "created_at": "2024-09-16T16:07:37Z"
        },
        {
          "author": "avcode-exe",
          "body": "Somehow it works now, maybe some time delay.",
          "created_at": "2024-09-16T16:20:07Z"
        }
      ]
    },
    {
      "issue_number": 14,
      "title": "Error \"no matching manifest for linux/arm64/v8\"",
      "body": "Was able to get Ollama and webui running. But when I try to install \"harbor up llamacpp litellm vllm\", I get error message.\r\n\r\nharbor up works:\r\n`Mac-Studio ~ % harbor up\r\n[+] Running 21/21\r\n âœ” webui Pulled                                                           59.6s \r\n   âœ” 92c3b3500be6 Pull complete                                            2.6s \r\n   âœ” 89790d4ca55c Pull complete                                            2.7s \r\n   âœ” 01ac286af1cc Pull complete                                            3.3s \r\n   âœ” 4ba8b3df285e Pull complete                                            3.3s \r\n   âœ” 39530234331f Pull complete                                            3.3s \r\n   âœ” 4f4fb700ef54 Pull complete                                            3.3s \r\n   âœ” 7ea6b4f15cc2 Pull complete                                            3.3s \r\n   âœ” cd34c4ad28ac Pull complete                                            3.3s \r\n   âœ” eb06520c5bae Pull complete                                           25.4s \r\n   âœ” e8d4b7d1f622 Pull complete                                           25.4s \r\n   âœ” 1391d428a531 Pull complete                                           58.1s \r\n   âœ” 695e24f3f07c Pull complete                                           58.8s \r\n   âœ” a0a6c53e8519 Pull complete                                           58.8s \r\n   âœ” b27529f13b89 Pull complete                                           58.8s \r\n   âœ” 7cc46888e662 Pull complete                                           59.1s \r\n âœ” ollama Pulled                                                          48.3s \r\n   âœ” e63ce922f022 Pull complete                                            5.3s \r\n   âœ” f8b8e7f3f328 Pull complete                                           25.7s \r\n   âœ” f6ad96f729db Pull complete                                           25.8s \r\n   âœ” 744f4f23d145 Pull complete                                           47.8s \r\n[+] Running 3/3\r\n âœ” Network harbor_harbor-network  Creat...                                 0.0s \r\n âœ” Container harbor.ollama        Healthy                                  0.7s \r\n âœ” Container harbor.webui         Healthy                                 10.7s `\r\n\r\nharbor up llamacpp litellm vllm fails:\r\n`Mac-Studio ~ % harbor up llamacpp litellm vllm   \r\n[+] Running 1/3\r\n â ¼ llamacpp Pulling                                                        0.4s \r\n â ¼ litellm Pulling                                                         0.4s \r\n âœ˜ litellmdb Error  context canceled                                       0.4s \r\nno matching manifest for linux/arm64/v8 in the manifest list entries`\r\n\r\nTrying individually: harbor up llamacpp also fails\r\n`Mac-Studio ~ % harbor up llamacpp             \r\n[+] Running 0/1\r\n â ¼ llamacpp Pulling                                                        0.4s \r\nno matching manifest for linux/arm64/v8 in the manifest list entries`\r\n\r\nTrying individually: harbor up vllm also fails after package download\r\n`Mac-Studio ~ % harbor up vllm    \r\n[+] Building 147.0s (8/8) FINISHED                         docker:desktop-linux\r\n => [vllm internal] load build definition from Dockerfile                  0.0s\r\n => => transferring dockerfile: 204B                                       0.0s\r\n => [vllm internal] load metadata for docker.io/vllm/vllm-openai:v0.6.0    5.8s\r\n => [vllm auth] vllm/vllm-openai:pull token for registry-1.docker.io       0.0s\r\n => [vllm internal] load .dockerignore                                     0.0s\r\n => => transferring context: 2B                                            0.0s\r\n => [vllm 1/2] FROM docker.io/vllm/vllm-openai:v0.6.0@sha256:072427aa6f  133.8s\r\n => => resolve docker.io/vllm/vllm-openai:v0.6.0@sha256:072427aa6f95c7478  0.0s\r\n => => sha256:43cfb69dbb464ebad014cd4687bf02ee4f5011d54 27.51MB / 27.51MB  0.7s\r\n => => sha256:072427aa6f95c74782a9bc3fe1d1fcd1e1aa3fe47b3 2.84kB / 2.84kB  0.0s\r\n => => sha256:fbcd35dc5bc3a7bda41926aadd083020f942b001eba 7.94MB / 7.94MB  1.7s\r\n => => sha256:c7232af9ae05f7de83f8d6171bd0c35a4dd0a85eb 57.59MB / 57.59MB  1.8s\r\n => => sha256:714424fc682cec74d71394991384f2ebd3897b390 11.99kB / 11.99kB  0.0s\r\n => => sha256:db6cdef1932a0d9ca6ef9a539e08d491f66d1b1ed81926a 185B / 185B  0.7s\r\n => => extracting sha256:43cfb69dbb464ebad014cd4687bf02ee4f5011d540916c65  0.7s\r\n => => sha256:56dc8550293751a1604e97ac949cfae82ba20cb2a28 6.89kB / 6.89kB  0.8s\r\n => => sha256:9f61b3db38d69dddb7c6f05c299c875b3c7b6d97869e2f9 104B / 104B  0.9s\r\n => => sha256:0bd39d0469a8457b392113410f3125a0859d6e5f6d48ad8 230B / 230B  1.0s\r\n => => sha256:d22ff5f4aac61b315f6054a8a18ecbe9dcd3af 231.18MB / 231.18MB  10.4s\r\n => => extracting sha256:fbcd35dc5bc3a7bda41926aadd083020f942b001ebac6f1d  0.2s\r\n => => sha256:d866993704f54dbc8fb066ffc01f4e0e0645617600b 6.90kB / 6.90kB  1.8s\r\n => => sha256:b4918d864665c1c0b4ab2df2c6951e0b7ec94e90b4 3.54GB / 3.54GB  84.0s\r\n => => sha256:e93cc01aab8b268f0f4cc7c62ca1401d317824b9c7 1.34GB / 1.34GB  41.5s\r\n => => extracting sha256:c7232af9ae05f7de83f8d6171bd0c35a4dd0a85ebafb15b9  0.7s\r\n => => extracting sha256:db6cdef1932a0d9ca6ef9a539e08d491f66d1b1ed81926ae  0.0s\r\n => => extracting sha256:56dc8550293751a1604e97ac949cfae82ba20cb2a28e0347  0.0s\r\n => => extracting sha256:9f61b3db38d69dddb7c6f05c299c875b3c7b6d97869e2f96  0.0s\r\n => => extracting sha256:0bd39d0469a8457b392113410f3125a0859d6e5f6d48ad85  0.0s\r\n => => sha256:fd30840e514de4a28db901cf841543e45fc26221 15.12MB / 15.12MB  11.3s\r\n => => extracting sha256:d22ff5f4aac61b315f6054a8a18ecbe9dcd3afad842ea601  5.8s\r\n => => extracting sha256:d866993704f54dbc8fb066ffc01f4e0e0645617600bba305  0.0s\r\n => => extracting sha256:b4918d864665c1c0b4ab2df2c6951e0b7ec94e90b425d15  37.9s\r\n => => extracting sha256:e93cc01aab8b268f0f4cc7c62ca1401d317824b9c76b99c  10.2s\r\n => => extracting sha256:fd30840e514de4a28db901cf841543e45fc262213c5eb1de  1.2s\r\n => [vllm 2/2] RUN pip install bitsandbytes                                6.6s\r\n => [vllm] exporting to image                                              0.7s \r\n => => exporting layers                                                    0.7s \r\n => => writing image sha256:40b832b504decfe262b9569b4f5cabd345c0693005859  0.0s \r\n => => naming to docker.io/library/harbor-vllm                             0.0s \r\n => [vllm] resolving provenance for metadata file                          0.0s \r\n[+] Running 3/0                                                                 \r\n âœ” Container harbor.vllm                                                                                                                               Created0.1s \r\n âœ” Container harbor.ollama                                                                                                                             Running0.0s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Created0.1s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Running0.0s \r\n[+] Running 2/4rbor.webui                                                        â ™ Container harbor.vllm                                                                                                                               Starting0.3s llm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Running0.0s \r\n[+] Running 1/4rbor.webui                                                        â ¹ Container harbor.vllm                                                                                                                               Waiting0.4s vllm The requested image's platform (linux/amd64) does not match the detected â ™ Container harbor.ollama                                                                                                                             Waiting0.4s \r\n[+] Running 1/4rbor.webui                                                        â ¸ Container harbor.vllm                                                                                                                               Waiting0.5s vllm The requested image's platform (linux/amd64) does not match the detected â ¹ Container harbor.ollama                                                                                                                             Waiting0.5s \r\n[+] Running 1/4rbor.webui                                                        â ¼ Container harbor.vllm                                                                                                                               Waiting0.6s vllm The requested image's platform (linux/amd64) does not match the detected â ¸ Container harbor.ollama                                                                                                                             Waiting0.6s \r\n[+] Running 1/4rbor.webui                                                        â ´ Container harbor.vllm                                                                                                                               Waiting0.7s vllm The requested image's platform (linux/amd64) does not match the detected â ¼ Container harbor.ollama                                                                                                                             Waiting0.7s \r\n[+] Running 1/4rbor.webui                                                        â ¦ Container harbor.vllm                                                                                                                               Waiting0.8s vllm The requested image's platform (linux/amd64) does not match the detected â ´ Container harbor.ollama                                                                                                                             Waiting0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 3/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n[+] Running 4/4rbor.webui                                                        âœ” Container harbor.vllm                                                                                                                               Healthy0.8s vllm The requested image's platform (linux/amd64) does not match the detected âœ” Container harbor.ollama                                                                                                                             Healthy0.8s \r\n âœ” Container harbor.webui                                                                                                                              Healthy10.8s \r\n ! vllm The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested 0.0s `",
      "state": "closed",
      "author": "parthpat12",
      "author_type": "User",
      "created_at": "2024-09-13T16:02:21Z",
      "updated_at": "2024-09-13T16:45:44Z",
      "closed_at": "2024-09-13T16:18:40Z",
      "labels": [
        "wontfix"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/14/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/14",
      "api_url": "https://api.github.com/repos/av/harbor/issues/14",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:34.906890",
      "comments": [
        {
          "author": "av",
          "body": "> The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n\r\nAre you running on Raspberry PI? Not all services will have docker images with ARM arch, unfortunately, unlike MacOS there's also no x86 translation lay",
          "created_at": "2024-09-13T16:15:45Z"
        },
        {
          "author": "av",
          "body": "I've mentioned ARM builds in the Requirements section in the README",
          "created_at": "2024-09-13T16:18:08Z"
        },
        {
          "author": "parthpat12",
          "body": "I maybe missing something but according to llama.cpp GH page:\r\n\r\nApple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\r\n\r\nIs the issue occurring because it's running from docker?",
          "created_at": "2024-09-13T16:27:53Z"
        },
        {
          "author": "av",
          "body": "Yes, that's an Apple and Docker thing, llama.cpp itself can be compiled and run on MacOS, however Apple doesn't support GPU passthrough in their hypervisor, so any kind of GPU optimisation targeting Apple silicon wouldn't work in the container. CPU-only inference would still work quite well compared",
          "created_at": "2024-09-13T16:45:43Z"
        }
      ]
    },
    {
      "issue_number": 11,
      "title": "how do I fix (harbor cmd) not working? ",
      "body": "![image](https://github.com/user-attachments/assets/7e3dbb3c-a0ee-4693-8eba-c2a71195cbc1)\r\n\r\nMy goal is to modify the VLLM docker to mount my local model folder so it can see those models. I am using Debian in WSL2.\r\n",
      "state": "closed",
      "author": "ColumbusAI",
      "author_type": "User",
      "created_at": "2024-08-28T21:03:13Z",
      "updated_at": "2024-08-28T21:40:28Z",
      "closed_at": "2024-08-28T21:40:28Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/11/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/11",
      "api_url": "https://api.github.com/repos/av/harbor/issues/11",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:35.121736",
      "comments": [
        {
          "author": "av",
          "body": "Hey, you'll need to use the command substitution for these to work as expected:\r\n\r\n```bash\r\nharbor cmd litellm\r\necho $(harbor cmd litellm)\r\neval \"$(harbor cmd litellm) run litellm --help\"\r\n```\r\n\r\n`vllm` service already has a configurable cache folder which can be modified via Harbor CLI:\r\n```bash\r\n#",
          "created_at": "2024-08-28T21:37:23Z"
        },
        {
          "author": "ColumbusAI",
          "body": "@av you are the man! thanks Ivan",
          "created_at": "2024-08-28T21:40:28Z"
        }
      ]
    },
    {
      "issue_number": 10,
      "title": "xtts-v2 error config.json not found",
      "body": "Hello,\r\n\r\nFirst of all, thank you for your hard work, this tool has been incredibly useful and has saved me a lot of time with manual configurations. Keep up the great work!\r\n\r\nI'm encountering an issue with the `xtts-v2` setup. Here's what I've done so far:\r\n\r\n1. Ran `harbor down` to remove all running instances and start fresh.\r\n2. Executed `harbor pull tts` to pull the latest TTS image.\r\n3. Ran `harbor up tts` to start the TTS container.\r\n4. Accessed the interface via `harbor open`.\r\n\r\nIn the open-webui, I configured the system to use `xtts-v2` by selecting `tts-1-hd` in the Admin Panel under the Audio tab. However, when I attempt to generate TTS responses, I receive a \"Server connection error\" message.\r\n\r\nAfter checking the logs of the TTS container, I noticed the following error:\r\n\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: '/app/voices/tts/tts_models--multilingual--multi-dataset--xtts/config.json'\r\n```\r\n\r\nI navigated to the specified folder and confirmed that the `config.json` file is indeed missing. I'm unsure why this file is missing or how to resolve the issue, but I wanted to bring it to your attention in case this is a more widespread problem that others might be experiencing as well.\r\n\r\nFor reference, I found a potentially related issue in the Coqui repo ([Issue #3064](https://github.com/coqui-ai/TTS/issues/3064)), but it doesn't seem useful for this case since the image pull process was never interrupted in my case.\r\n\r\nAny guidance would be greatly appreciated.\r\n\r\nThank you!",
      "state": "closed",
      "author": "FaintWhisper",
      "author_type": "User",
      "created_at": "2024-08-25T20:31:02Z",
      "updated_at": "2024-08-27T14:49:16Z",
      "closed_at": "2024-08-27T14:49:16Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/10/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/10",
      "api_url": "https://api.github.com/repos/av/harbor/issues/10",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:35.308223",
      "comments": [
        {
          "author": "av",
          "body": "Hi, thanks for giving Harbor a spin!\r\n\r\nThere are two things to consider in this situation\r\n\r\n### Webui configuration\r\n\r\nWebUI has a cache for the audio assets produced by TTS, so to ensure that `openedai-speech` is actually hit with the `tts-1-hd` request - you need to ensure that the test in quest",
          "created_at": "2024-08-26T09:13:16Z"
        },
        {
          "author": "av",
          "body": "Added these to the wiki:\r\n- [WebUI config override](https://github.com/av/harbor/wiki/Services#configuration)\r\n- [WebUI Audio cache](https://github.com/av/harbor/wiki/Compatibility#webui---same-audio-after-audio-config-change)\r\n- [openedai-speech xtts-v2 download](https://github.com/av/harbor/wiki/C",
          "created_at": "2024-08-26T09:44:08Z"
        },
        {
          "author": "FaintWhisper",
          "body": "Thank you for your response!\r\n\r\nI noticed that the previously prompted responses for TTS were cached and not regenerated with the new TTS settings. However, despite generating new messages and prompting for TTS after updating the settings to configure xtts-v2, I still encountered the error I mention",
          "created_at": "2024-08-26T21:20:26Z"
        },
        {
          "author": "av",
          "body": "> Iâ€™m wondering if there might be a specific issue with the opendai-speech package, possibly related to how it handles the configuration file or pulls the model. However, based on your logs, it seems to be working correctly on your end, so Iâ€™m not sure if this issue is unique to my setup.\r\n\r\nMy theo",
          "created_at": "2024-08-27T08:17:12Z"
        },
        {
          "author": "FaintWhisper",
          "body": "That explanation makes sense. I realize now that the `openai-speech` image pulls the model during container creation, and itâ€™s saved within a volume ([ref](https://github.com/matatonic/openedai-speech/blob/main/docker-compose.yml)). Itâ€™s possible that because I didnâ€™t delete the volume, the model di",
          "created_at": "2024-08-27T14:49:16Z"
        }
      ]
    },
    {
      "issue_number": 9,
      "title": "didn't see in docs -- how do I fix if a container is unhealthy?",
      "body": "$ harbor up vllm litellm\r\n[+] Running 4/5\r\n âœ” Network harbor_harbor-network  Created                                     0.0s\r\n â ¸ Container harbor.webui         Waiting                                     6.1s\r\n âœ” Container harbor.vllm          Healthy                                     5.1s\r\n âœ” Container harbor.litellmdb     Healthy                                     5.1s\r\n âœ” Container harbor.litellm       Healthy                                     5.1s\r\ncontainer harbor.webui is unhealthy",
      "state": "closed",
      "author": "ColumbusAI",
      "author_type": "User",
      "created_at": "2024-08-25T20:04:45Z",
      "updated_at": "2024-08-26T09:21:33Z",
      "closed_at": "2024-08-25T23:41:19Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/9/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/9",
      "api_url": "https://api.github.com/repos/av/harbor/issues/9",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:35.571562",
      "comments": [
        {
          "author": "av",
          "body": "Thanks for trying out Harbor!\r\n\r\nRight now WebUI health check is a bit too aggressive in order to reduce the service startup wait, it may often result in webui service not passing it in time. The service itself should be healthy once started, though. I'll relax the health check parameters with next ",
          "created_at": "2024-08-25T20:09:56Z"
        },
        {
          "author": "av",
          "body": "One other thing not reflected in docs - vllm doesn't require litellm anymore, can run directly with the webui",
          "created_at": "2024-08-25T20:11:28Z"
        },
        {
          "author": "ColumbusAI",
          "body": "thank you! you rock :-D",
          "created_at": "2024-08-25T23:41:19Z"
        },
        {
          "author": "av",
          "body": "Implemented in [v0.1.8](https://github.com/av/harbor/releases/tag/v0.1.8)",
          "created_at": "2024-08-26T09:21:06Z"
        }
      ]
    },
    {
      "issue_number": 7,
      "title": "Suggestion: Open interpreter integration ",
      "body": "Is this even possible with containers?",
      "state": "closed",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2024-08-04T16:51:01Z",
      "updated_at": "2024-08-07T13:48:20Z",
      "closed_at": "2024-08-07T13:48:19Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/7/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/7",
      "api_url": "https://api.github.com/repos/av/harbor/issues/7",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:35.778715",
      "comments": [
        {
          "author": "av",
          "body": "Yes, \r\n\r\nI was thinking about integrating it just today. It should be possible with a caveat of a subfolder-only file access down from the $PWD, I need to learn more about it's internals and specifically how the context is constructed, though ",
          "created_at": "2024-08-04T17:03:55Z"
        },
        {
          "author": "av",
          "body": "Just pushed initial version of the integration, writing some docs now. The (yet) undocumented CLI interface is [here](https://github.com/av/harbor/blob/main/harbor.sh#L1121)",
          "created_at": "2024-08-04T22:13:27Z"
        },
        {
          "author": "av",
          "body": "released in [v0.0.17](https://github.com/av/harbor/releases/tag/v0.0.17)",
          "created_at": "2024-08-04T23:51:22Z"
        },
        {
          "author": "av",
          "body": "Closing as the integration was made",
          "created_at": "2024-08-07T13:48:19Z"
        }
      ]
    },
    {
      "issue_number": 8,
      "title": "Bug: Harbor ollama <parameter>",
      "body": "OCI runtime exec failed: exec failed: unable to start container process: exec: \"ollama list\": executable file not found in $PATH: unknown\r\n\r\nI removed my host ollama (rm /usr/local/bin/ollama, userdel ollama...). Maybe that caused this issue, idk.\r\n\r\nEdit: entering on container and running \"ollama list\" works, just \"harbor ollama list\" or \"harbor exec ollama \"ollama list\" doesn't works.",
      "state": "closed",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2024-08-04T20:30:40Z",
      "updated_at": "2024-08-07T13:48:07Z",
      "closed_at": "2024-08-07T13:48:06Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/8/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "av"
      ],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/8",
      "api_url": "https://api.github.com/repos/av/harbor/issues/8",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:35.944560",
      "comments": [
        {
          "author": "av",
          "body": "Thank you for reporting! It's a regression after resolving some of the linter messages in the main CLI file - went a tad wrong (sorry for the inconvenience)\r\n\r\nPlease run `harbor update` and it should be fixed",
          "created_at": "2024-08-04T20:48:16Z"
        },
        {
          "author": "av",
          "body": "Closing as config override was fixed",
          "created_at": "2024-08-07T13:48:06Z"
        }
      ]
    },
    {
      "issue_number": 6,
      "title": "How to change default server parameters on ollama?",
      "body": "Example: I want to set OLLAMA_KEEP_ALIVE permanently, how I can do that? ",
      "state": "closed",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2024-08-03T18:18:42Z",
      "updated_at": "2024-08-04T21:26:28Z",
      "closed_at": "2024-08-04T21:26:28Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/6/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/6",
      "api_url": "https://api.github.com/repos/av/harbor/issues/6",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:36.147967",
      "comments": [
        {
          "author": "av",
          "body": "Just pushed an update (also planned in advance, haha)\r\n\r\nNow, `.env` file is no longer tracked by git, so any modifications to it are not conflicting with upstream changes. Previous defaults were moved to `default.env` and are used for the initial population of the `.env`",
          "created_at": "2024-08-03T20:02:19Z"
        },
        {
          "author": "av",
          "body": "Also added the `harbor config reset` to bring the `.env` back to current defaults",
          "created_at": "2024-08-03T20:12:36Z"
        },
        {
          "author": "av",
          "body": "To your original question, with the latest update, you can specify such global persistent vars in the `.env` in the Harbor workspace without the risk of having a conflict with an update from the upstream.\r\n\r\nI've also left a pointer on where ollama vars can go in the `default.env`",
          "created_at": "2024-08-03T20:16:43Z"
        },
        {
          "author": "av",
          "body": "released in [v0.0.15](https://github.com/av/harbor/releases/tag/v0.0.15)",
          "created_at": "2024-08-03T22:01:52Z"
        },
        {
          "author": "av",
          "body": "Closing as this is now possible after `.env` was detached from the pre-provisioned defaults",
          "created_at": "2024-08-04T21:26:22Z"
        }
      ]
    },
    {
      "issue_number": 5,
      "title": "How to override default embedding model?",
      "body": "I want to use other embedding model, but harbor overrides my changes on webui after harbor.sh down && harbor.sh up\r\n",
      "state": "closed",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2024-08-03T14:43:30Z",
      "updated_at": "2024-08-04T21:25:45Z",
      "closed_at": "2024-08-04T21:25:09Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/5/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/5",
      "api_url": "https://api.github.com/repos/av/harbor/issues/5",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:36.325003",
      "comments": [
        {
          "author": "av",
          "body": "Sorry for the inconvenience! We can parameterise it, are you talking about embeddings for the web RAG?\r\n\r\nA quick workaround before the parametrisation is available - once you saved your settings in the WebUI, open `open-webui/config.json` and move the settings from there over to `./open-webui/confi",
          "created_at": "2024-08-03T15:32:19Z"
        },
        {
          "author": "alsoasnerd",
          "body": "Move my settings to config.override.json doesn't works.",
          "created_at": "2024-08-03T16:32:50Z"
        },
        {
          "author": "alsoasnerd",
          "body": "Change config.x.searxng.ollama.json works.",
          "created_at": "2024-08-03T16:39:07Z"
        },
        {
          "author": "av",
          "body": "Yes, sorry for the inconvenience again, that's due to the ordering of the configs during merging. The next version will have correct order of precedence, so that overrides are always the last",
          "created_at": "2024-08-03T18:10:56Z"
        },
        {
          "author": "alsoasnerd",
          "body": "no problem, thanks for the help",
          "created_at": "2024-08-03T18:12:54Z"
        }
      ]
    },
    {
      "issue_number": 4,
      "title": "How to update",
      "body": null,
      "state": "closed",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2024-08-03T13:24:29Z",
      "updated_at": "2024-08-04T21:24:41Z",
      "closed_at": "2024-08-04T21:24:41Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/4/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/4",
      "api_url": "https://api.github.com/repos/av/harbor/issues/4",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:36.578147",
      "comments": [
        {
          "author": "av",
          "body": "Right now, it's `git pull` when you are in the harbor folder, we'll add more convenient way to do this along with the one-line installation script later",
          "created_at": "2024-08-03T13:30:48Z"
        },
        {
          "author": "av",
          "body": "The latest version comes with a `harbor update` helper (no release tag yet), this is a stub until the mentioned functionality is in place",
          "created_at": "2024-08-03T15:22:20Z"
        },
        {
          "author": "av",
          "body": "officially in [v0.0.15](https://github.com/av/harbor/releases/tag/v0.0.15)",
          "created_at": "2024-08-03T22:01:36Z"
        },
        {
          "author": "av",
          "body": "I'm closing since we now have poor man's update, and mentioned one-line installer",
          "created_at": "2024-08-04T21:24:41Z"
        }
      ]
    },
    {
      "issue_number": 1,
      "title": "Open WebUI doesn't works",
      "body": "I just runned sudo ./harbor.sh up.\r\nOpenWebUI doesn't open\r\n![image](https://github.com/user-attachments/assets/63996c4f-e347-47f9-bee7-a26787818322)\r\n\r\nOutput of sudo ./harbor.sh logs:\r\n![image](https://github.com/user-attachments/assets/a21c946a-cec8-4415-9bcb-8a437daf7168)\r\nI pulled gemma2:2b in ollama, but doesn't fix the issue.\r\n\r\nEdit: Sorry for my english and thanks for open this project.",
      "state": "closed",
      "author": "alsoasnerd",
      "author_type": "User",
      "created_at": "2024-08-01T20:56:57Z",
      "updated_at": "2024-08-02T17:21:26Z",
      "closed_at": "2024-08-02T17:13:42Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 24,
      "reactions": {
        "url": "https://api.github.com/repos/av/harbor/issues/1/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/av/harbor/issues/1",
      "api_url": "https://api.github.com/repos/av/harbor/issues/1",
      "repository": "av/harbor",
      "extraction_date": "2025-06-22T00:49:36.769049",
      "comments": []
    }
  ]
}