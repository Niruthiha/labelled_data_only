{
  "repository": "openlit/openlit",
  "repository_info": {
    "repo": "openlit/openlit",
    "stars": 1646,
    "language": "Python",
    "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. üöÄüíª Integrates with 50+ LLM Providers, Ve",
    "url": "https://github.com/openlit/openlit",
    "topics": [
      "ai-observability",
      "amd-gpu",
      "clickhouse",
      "distributed-tracing",
      "genai",
      "gpu-monitoring",
      "grafana",
      "langchain",
      "llmops",
      "llms",
      "metrics",
      "monitoring-tool",
      "nvidia-smi",
      "observability",
      "open-source",
      "openai",
      "opentelemetry",
      "otlp",
      "python",
      "tracing"
    ],
    "created_at": "2024-01-23T17:40:59Z",
    "updated_at": "2025-06-21T17:16:18Z",
    "search_query": "llm integration language:python stars:>2 created:>2023-01-01",
    "total_issues_estimate": 98,
    "labeled_issues_estimate": 98,
    "labeling_rate": 100.0,
    "sample_labeled": 37,
    "sample_total": 37,
    "has_issues": true,
    "repo_id": 747319327,
    "default_branch": "main",
    "size": 47196
  },
  "extraction_date": "2025-06-22T00:51:40.991753",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 223,
  "issues": [
    {
      "issue_number": 760,
      "title": "[Bug]:Logs Missing for openai client.beta.chat.completions.parse with OpenLit",
      "body": "When I use the standard client.chat.completions.create call, OpenLit correctly generates trace logs, including telemetry details and spans:\n\n```\nfrom openai import OpenAI\nimport openlit\n\nopenlit.init()\n\nclient = OpenAI(api_key=\"\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n    ]\n)\n```\n\nThis produces detailed trace logs with OpenLit (e.g., trace_id, gen_ai.request.model, etc.).\n\n\nHowever, when I switch to the structured output using client.beta.chat.completions.parse, trace logs are not generated:\n\n\n```\nfrom openai import OpenAI\nimport openlit\nfrom pydantic import BaseModel\n\nopenlit.init()\n\nclient = OpenAI(api_key=\"\")\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n    ],\n    response_format=MathReasoning,\n    temperature=0,\n)\n```\n\n\nIn this structured output case, OpenLit doesn‚Äôt emit any trace information or span logs.\nIs this expected behavior? Should OpenLit currently support tracing for structured output calls via .parse()? If not, is support planned?\n",
      "state": "open",
      "author": "obaidurrehman1",
      "author_type": "User",
      "created_at": "2025-06-18T06:01:38Z",
      "updated_at": "2025-06-18T06:01:38Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/760/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/760",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/760",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.431742",
      "comments": []
    },
    {
      "issue_number": 697,
      "title": "[Bug]: File \"/opt/projsetup/.venv/lib/python3.11/site-packages/openlit/instrumentation/transformers/transformers.py\", line 70, in wrapper prompt_tokens = general_tokens(prompt[0]) ~~~~~~^^^ IndexError: string index out of range",
      "body": "### üêõ What's Going Wrong?\n> Trying to collect and get the traces for my sample chatqna application which utilized \"HuggingFacePipeline\" module langchain. However when i try to view the traces it shown error above for span \"huggingface.text_generation\"\n\n### üïµÔ∏è Steps to Reproduce\n>  Initialize LLM using HuggingFacePipeline\n\n### üéØ What Did You Expect?\n>  Traces able to view.\n\n### üì∏ Any Screenshots?\n>  N/A\n\n### üíª Your Setup\n- OpenLIT Version: [latest]\n- OpenLIT SDK Version: [latest]\n- Deployment Method: [Docker]\n\n### üìù Additional Notes\n>  Got more to say? Tell us here.\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "hteeyeoh",
      "author_type": "User",
      "created_at": "2025-04-10T06:59:40Z",
      "updated_at": "2025-06-18T02:33:54Z",
      "closed_at": "2025-06-17T18:53:00Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/697/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/697",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/697",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.431769",
      "comments": [
        {
          "author": "patcher9",
          "body": "possible to share a snippet of the relevant code?",
          "created_at": "2025-04-24T05:26:17Z"
        },
        {
          "author": "hteeyeoh",
          "body": "Sorry for late response. More or less like below:\nllm = HuggingFacePipeline.from_model_id(\n    model_id=str(model_dir),\n    task=\"text-generation\",\n    backend=\"openvino\",\n    model_kwargs={\n        \"device\": llm_device.value,\n        \"ov_config\": ov_config,\n        \"trust_remote_code\": True,\n    },",
          "created_at": "2025-04-28T01:25:39Z"
        },
        {
          "author": "hteeyeoh",
          "body": "@patcher9 , want to check for the follow up of this issue, does openlit support  HuggingFacePipeline.from_model_id module? i think openlit does not support non-openai standard right?\n",
          "created_at": "2025-05-19T02:59:45Z"
        },
        {
          "author": "patcher9",
          "body": "We support HuggingFace [Trasnformers](https://github.com/huggingface/transformers) as of now. Not sure what langchain is using to call the LLM but I can double check and make sure its supported one way or another",
          "created_at": "2025-05-23T04:10:33Z"
        },
        {
          "author": "hteeyeoh",
          "body": "alright thanks :)",
          "created_at": "2025-05-23T08:01:46Z"
        }
      ]
    },
    {
      "issue_number": 758,
      "title": "[Feat]: Refactor All OpenTelemetry Instrumentations",
      "body": "### üöÄ What's the Problem?\nCurrently, there are several issues in the codebase:\n1. Duplicate code across different integrations, especially in telemetry and monitoring logic\n2. Inconsistent code style and patterns across different integrations\n3. Heavy reliance on events API which adds complexity and potential points of failure\n4. Lack of standardization in how integrations handle common operations like error handling, metrics collection, and tracing\n\n### üí° Your Dream Solution\nA streamlined, maintainable codebase where:\n1. Common functionality is centralized in shared utilities\n2. All integrations follow consistent patterns and code styles\n3. Events API is replaced with direct OpenTelemetry instrumentation\n4. Each integration has:\n   - Clear, consistent docstrings\n   - Standardized error handling\n   - Unified approach to metrics and tracing\n   - Similar structure for wrapper functions\n   - Common utility functions for shared operations\n\n### ü§î Seen anything similar?\n1. Current `utils.py` pattern in Pydantic AI integration that centralizes common functionality\n2. OpenTelemetry's own instrumentation patterns for different services\n3. LangChain's approach to standardizing integrations through base classes and mixins\n4. LiteLLM's unified interface for different LLM providers\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n\n\n# OpenLit Integrations Checklist\n\n## LLM Providers\n- [ ] OpenAI\n- [ ] Anthropic\n- [ ] Cohere\n- [ ] Mistral\n- [ ] Bedrock\n- [ ] Vertex AI\n- [ ] Groq\n- [ ] Ollama\n- [ ] GPT4All\n- [ ] ElevenLabs\n- [ ] Reka\n- [ ] PremAI\n- [ ] AssemblyAI\n- [ ] Azure AI Inference\n- [ ] Together\n- [ ] AI21\n- [x] VLLM\n- [x] Google AI Studio\n\n## Vector Stores & Databases\n- [ ] Chroma\n- [ ] Pinecone\n- [ ] Qdrant\n- [ ] Milvus\n- [ ] Astra\n- [ ] Mem0\n\n## Frameworks & Tools\n- [ ] LangChain\n- [ ] LlamaIndex\n- [ ] Haystack\n- [ ] EmbedChain\n- [ ] Transformers\n- [ ] LiteLLM\n- [ ] CrewAI\n- [ ] AG2\n- [ ] Multion\n- [ ] Dynamiq\n- [ ] Phidata\n- [ ] Julep\n- [ ] ControlFlow\n- [ ] Crawl4AI\n- [ ] Firecrawl\n- [ ] Letta\n- [ ] Pydantic AI\n- [ ] OpenAI Agents\n\n## Infrastructure\n- [ ] GPU\n",
      "state": "open",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-06-17T19:29:35Z",
      "updated_at": "2025-06-17T21:16:35Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/758/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/758",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/758",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.645729",
      "comments": []
    },
    {
      "issue_number": 471,
      "title": "[Feat]: Human Feedback for LLM Events in OpenLIT",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nUsers cant manually rate the LLM response in OpenLIT\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nManually be able to set the LLM response score\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-10-25T18:46:08Z",
      "updated_at": "2025-06-17T19:09:06Z",
      "closed_at": "2025-06-17T19:09:06Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/471/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/471",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/471",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.645752",
      "comments": []
    },
    {
      "issue_number": 288,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring OpenSearch",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor OpenSearch LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to Pinecone, Have a auto instrumentation for OpenSearch \r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-14T08:10:27Z",
      "updated_at": "2025-06-17T19:08:47Z",
      "closed_at": "2025-06-17T19:08:46Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/288/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/288",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/288",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.645760",
      "comments": []
    },
    {
      "issue_number": 287,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring Weaviate",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor Weaviate LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to Pinecone, Have a auto instrumentation for Weaviate \r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-14T07:51:26Z",
      "updated_at": "2025-06-17T19:08:46Z",
      "closed_at": "2025-06-17T19:08:46Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/287/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "subash54"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/287",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/287",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.645768",
      "comments": [
        {
          "author": "subash54",
          "body": "@patcher9  Can I pick this one?",
          "created_at": "2024-09-27T12:04:32Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @subash54, Sure go ahead!\r\n",
          "created_at": "2024-09-30T15:15:05Z"
        }
      ]
    },
    {
      "issue_number": 284,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring LanceDB",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor LanceDB LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to Pinecone, Have a auto instrumentation for LanceDB \r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:57:20Z",
      "updated_at": "2025-06-17T19:08:46Z",
      "closed_at": "2025-06-17T19:08:46Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/284/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/284",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/284",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.873990",
      "comments": []
    },
    {
      "issue_number": 282,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring pgvector",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor pgvector based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to Pinecone, Have a auto instrumentation for pgvector\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:52:07Z",
      "updated_at": "2025-06-17T19:08:46Z",
      "closed_at": "2025-06-17T19:08:46Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/282/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/282",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/282",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:10.874013",
      "comments": [
        {
          "author": "k4kum4r",
          "body": "Hi @patcher9 ,\r\nI'm new to OSS and would love to make a contribution. Can I take this up?",
          "created_at": "2024-09-14T14:02:19Z"
        },
        {
          "author": "patcher9",
          "body": "For sure you can pick this up!\r\n\r\nI have assigned you the issue, Incase you wanna brainstorm/discuss/ask questions about the implementation, Feel free to ask here or on our [community slack](https://join.slack.com/t/openlit/shared_invite/zt-2etnfttwg-TjP_7BZXfYg84oAukY8QRQ), ",
          "created_at": "2024-09-14T16:40:22Z"
        },
        {
          "author": "k4kum4r",
          "body": "Is there a reference implementation of setting up instrumentation for other databases?\r\nFrom what I can see [this](https://github.com/openlit/openlit/blob/main/sdk/python/src/openlit/instrumentation/pinecone/pinecone.py) seems to be for Pinecone, correct?",
          "created_at": "2024-09-15T05:40:21Z"
        },
        {
          "author": "patcher9",
          "body": "Yup, Pinecone is a good example, you can also check out [ChromaDB Instrumentation](https://github.com/openlit/openlit/tree/main/sdk/python/src/openlit/instrumentation/chroma), [Milvus Instrumentation](https://github.com/openlit/openlit/tree/main/sdk/python/src/openlit/instrumentation/milvus), [Qdran",
          "created_at": "2024-09-16T07:21:06Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @k4kum4r, following up to know if you have an update/blockers/anything I can help with",
          "created_at": "2024-09-30T15:15:52Z"
        }
      ]
    },
    {
      "issue_number": 443,
      "title": "[Feat]: Add pre-built LLM Observability dashboard for Chronosphere",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nWe don't currently have a pre-built dashboard for users exporting LLM monitoring data collected by OpenLIT to Chronosphere which puts the burden on the user to see traces and metrics manually and build a dashboard\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to pre-built dashboard for Grafana Cloud, Signoz etc, Built a Chronosphere OOTB dashboard for users to monitor their LLM Applications\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-10-04T12:30:17Z",
      "updated_at": "2025-06-17T19:08:22Z",
      "closed_at": "2025-06-17T19:08:22Z",
      "labels": [
        ":book: Documentation",
        ":rocket: Feature",
        ":heavy_plus_sign:  help wanted",
        ":raised_hand: Up for Grabs",
        "Connections"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/443/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/443",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/443",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093594",
      "comments": []
    },
    {
      "issue_number": 406,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Ola Krutrim in typescript sdk",
      "body": "### üöÄ What's the Problem?\r\nOla Krutrim support is not present\r\n\r\n\r\n### üí° Your Dream Solution\r\nAfter the integration is completed, I would be able to monitor application using Ola Krutrim\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-09-03T20:14:15Z",
      "updated_at": "2025-06-17T19:08:15Z",
      "closed_at": "2025-06-17T19:08:15Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        "openlit-js",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/406/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/406",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/406",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093614",
      "comments": []
    },
    {
      "issue_number": 405,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Mistral in typescript sdk",
      "body": "### üöÄ What's the Problem?\r\nMistral support is not present\r\n\r\n\r\n### üí° Your Dream Solution\r\nAfter the integration is completed, I would be able to monitor application using Mistral AI\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-09-03T20:13:18Z",
      "updated_at": "2025-06-17T19:08:15Z",
      "closed_at": "2025-06-17T19:08:15Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        "openlit-js",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/405/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/405",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/405",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093620",
      "comments": []
    },
    {
      "issue_number": 404,
      "title": "[Feat]: Add OpenTelemetry instrumentation for GPT4All in typescript sdk",
      "body": "### üöÄ What's the Problem?\r\nGPT4All support is not present\r\n\r\n\r\n### üí° Your Dream Solution\r\nAfter the integration is completed, I would be able to monitor application using GPT4All\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-09-03T20:12:27Z",
      "updated_at": "2025-06-17T19:08:15Z",
      "closed_at": "2025-06-17T19:08:15Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        "openlit-js",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/404/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/404",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/404",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093627",
      "comments": []
    },
    {
      "issue_number": 403,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Ollama in typescript sdk",
      "body": "### üöÄ What's the Problem?\r\nOllama support is not present\r\n\r\n\r\n### üí° Your Dream Solution\r\nAfter the integration is completed, I would be able to monitor application using Ollama\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-09-03T20:11:27Z",
      "updated_at": "2025-06-17T19:08:14Z",
      "closed_at": "2025-06-17T19:08:14Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        "openlit-js",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/403/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/403",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/403",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093633",
      "comments": []
    },
    {
      "issue_number": 680,
      "title": "[Feat]: Use OpenTelemetry Events API for vLLM Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:50:28Z",
      "updated_at": "2025-06-17T19:07:26Z",
      "closed_at": "2025-06-17T19:07:26Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/680/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/680",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/680",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093638",
      "comments": []
    },
    {
      "issue_number": 679,
      "title": "[Feat]: Use OpenTelemetry Events API for VertexAI Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:50:12Z",
      "updated_at": "2025-06-17T19:07:26Z",
      "closed_at": "2025-06-17T19:07:26Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/679/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/679",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/679",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093643",
      "comments": []
    },
    {
      "issue_number": 678,
      "title": "[Feat]: Use OpenTelemetry Events API for Transformers Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:49:59Z",
      "updated_at": "2025-06-17T19:07:25Z",
      "closed_at": "2025-06-17T19:07:25Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/678/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/678",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/678",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093648",
      "comments": []
    },
    {
      "issue_number": 677,
      "title": "[Feat]: Use OpenTelemetry Events API for Together AI Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:49:47Z",
      "updated_at": "2025-06-17T19:07:25Z",
      "closed_at": "2025-06-17T19:07:25Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/677/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/677",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/677",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093653",
      "comments": []
    },
    {
      "issue_number": 676,
      "title": "[Feat]: Use OpenTelemetry Events API for Reka AI Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:49:34Z",
      "updated_at": "2025-06-17T19:07:25Z",
      "closed_at": "2025-06-17T19:07:25Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/676/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/676",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/676",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093659",
      "comments": []
    },
    {
      "issue_number": 675,
      "title": "[Feat]: Use OpenTelemetry Events API for PremAI Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:49:21Z",
      "updated_at": "2025-06-17T19:07:24Z",
      "closed_at": "2025-06-17T19:07:24Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/675/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/675",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/675",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093664",
      "comments": []
    },
    {
      "issue_number": 674,
      "title": "[Feat]: Use OpenTelemetry Events API for OpenAI Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:46:09Z",
      "updated_at": "2025-06-17T19:07:24Z",
      "closed_at": "2025-06-17T19:07:24Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/674/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/674",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/674",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093670",
      "comments": []
    },
    {
      "issue_number": 673,
      "title": "[Feat]: Use OpenTelemetry Events API for Mistral Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:45:53Z",
      "updated_at": "2025-06-17T19:07:24Z",
      "closed_at": "2025-06-17T19:07:24Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/673/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/673",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/673",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093675",
      "comments": []
    },
    {
      "issue_number": 672,
      "title": "[Feat]: Use OpenTelemetry Events API for LiteLLM Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:45:39Z",
      "updated_at": "2025-06-17T19:07:23Z",
      "closed_at": "2025-06-17T19:07:23Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/672/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/672",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/672",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093681",
      "comments": []
    },
    {
      "issue_number": 671,
      "title": "[Feat]: Use OpenTelemetry Events API for LangChain Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:45:26Z",
      "updated_at": "2025-06-17T19:07:23Z",
      "closed_at": "2025-06-17T19:07:23Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/671/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/671",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/671",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093687",
      "comments": []
    },
    {
      "issue_number": 670,
      "title": "[Feat]: Use OpenTelemetry Events API for Groq Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:45:07Z",
      "updated_at": "2025-06-17T19:07:23Z",
      "closed_at": "2025-06-17T19:07:23Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/670/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/670",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/670",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093693",
      "comments": []
    },
    {
      "issue_number": 669,
      "title": "[Feat]: Use OpenTelemetry Events API for GPT4All Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:44:56Z",
      "updated_at": "2025-06-17T19:07:23Z",
      "closed_at": "2025-06-17T19:07:23Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/669/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/669",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/669",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093698",
      "comments": []
    },
    {
      "issue_number": 668,
      "title": "[Feat]: Use OpenTelemetry Events API for Google AI Studio Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:44:41Z",
      "updated_at": "2025-06-17T19:07:22Z",
      "closed_at": "2025-06-17T19:07:22Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/668/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/668",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/668",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093703",
      "comments": []
    },
    {
      "issue_number": 666,
      "title": "[Feat]: Use OpenTelemetry Events API for Cohere Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:43:59Z",
      "updated_at": "2025-06-17T19:07:22Z",
      "closed_at": "2025-06-17T19:07:22Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/666/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/666",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/666",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093708",
      "comments": []
    },
    {
      "issue_number": 667,
      "title": "[Feat]: Use OpenTelemetry Events API for ElevenLabs Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:44:25Z",
      "updated_at": "2025-06-17T19:06:57Z",
      "closed_at": "2025-06-17T19:06:57Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/667/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/667",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/667",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093713",
      "comments": []
    },
    {
      "issue_number": 740,
      "title": "[Bug]: 1.33.23 - Gemini tracing error throws Exception",
      "body": "Following up on https://github.com/openlit/openlit/issues/734 ...\n\n\n```\n  File \"/app/.venv/lib/python3.11/site-packages/openlit/instrumentation/google_ai_studio/google_ai_studio.py\", line 42, in wrapper\n    response = process_chat_response(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.11/site-packages/openlit/instrumentation/google_ai_studio/utils.py\", line 246, in process_chat_response\n    common_chat_logic(self, pricing_info, environment, application_name, metrics,\n  File \"/app/.venv/lib/python3.11/site-packages/openlit/instrumentation/google_ai_studio/utils.py\", line 158, in common_chat_logic\n    scope._input_tokens + scope._output_tokens + scope._reasoning_tokens)\n    ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\n```\n\nSeems like exceptions being thrown in the non-streaming option are also thrown upwards, rather than being previously just logged. ",
      "state": "closed",
      "author": "dani29",
      "author_type": "User",
      "created_at": "2025-06-03T07:22:55Z",
      "updated_at": "2025-06-17T19:06:16Z",
      "closed_at": "2025-06-17T19:06:16Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/740/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/740",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/740",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.093720",
      "comments": [
        {
          "author": "patcher9",
          "body": "I see, I can fix this",
          "created_at": "2025-06-09T18:43:04Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @dani29  could you share the code you used to get this?",
          "created_at": "2025-06-17T18:46:13Z"
        }
      ]
    },
    {
      "issue_number": 756,
      "title": "[Feat]: Add OpenTelemetry Instrumentation for HuggingFace Hub",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "open",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-06-17T18:34:56Z",
      "updated_at": "2025-06-17T18:35:04Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/756/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/756",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/756",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.286572",
      "comments": []
    },
    {
      "issue_number": 752,
      "title": "[Feat]: Add Featherless AI LLM provider (access to 7,900+) open source models",
      "body": "### üöÄ What's the Problem?\nSome niche models are not provided by current providers\n\n### üí° Your Dream Solution\nI would love to see Featherless AI, an LLM Provider providing access to over 7,900 access added so users can use openlit for eval of those models\n\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "DarinVerheijke",
      "author_type": "User",
      "created_at": "2025-06-13T13:16:39Z",
      "updated_at": "2025-06-17T18:32:25Z",
      "closed_at": "2025-06-17T18:32:25Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/752/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/752",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/752",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.286593",
      "comments": [
        {
          "author": "patcher9",
          "body": "Thanks @DarinVerheijke for raising this!\n\nI have tested and using featherless via the OpenAI SDK works great with OpenLIT!\n\nhttps://docs.openlit.io/latest/integrations/featherless and also added to our Integrations catalog page in docs.\n",
          "created_at": "2025-06-17T18:32:25Z"
        }
      ]
    },
    {
      "issue_number": 749,
      "title": "[Feat]: Add Monitoring Integration for pydantic ai",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2025-06-10T11:40:18Z",
      "updated_at": "2025-06-17T17:56:09Z",
      "closed_at": "2025-06-17T17:55:25Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/749/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/749",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/749",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.483476",
      "comments": [
        {
          "author": "patcher9",
          "body": "Added in version `1.34.0` of the Python SDK",
          "created_at": "2025-06-17T17:55:25Z"
        }
      ]
    },
    {
      "issue_number": 743,
      "title": "[Bug]: OpenLit isn't sending information to our deployed collector",
      "body": "### üêõ What's Going Wrong?\n\nOpenLit isn't sending information to our deployed collector\n\n### üïµÔ∏è Steps to Reproduce\n\nWhen I run everything with localhost it is working. When the nodejs app is running in our k8s cluster it only logs traces to std out.\n\n### üéØ What Did You Expect?\n\nTraces to be sent to our otel collector\n\n### üì∏ Any Screenshots?\n\nopenlit.options logged:\n\n```\nINFO 2025-06-06T22:51:01.835518871Z [resource.labels.containerName: nodejsAPP] 2025-06-06T22:51:01.830Z [\u001b[32minfo\u001b[39m]: openlit.options {\"otlpEndpoint\":\"http://our-otel-collector:4318/v1/traces\",\"instrumentations\":{},\"otlpHeaders\":{},\"disableBatch\":true}\n```\n\n### üíª Your Setup\n- OpenLIT Version: Not used\n- OpenLIT SDK Version:  1.4.1\n- Deployment Method: Deploying a nodejs app via helm\n\n### üìù Additional Notes\n\nWe aren't using the Openlit app. Just the SDK to send traces to our existing otel collector\n\n",
      "state": "open",
      "author": "vordimous",
      "author_type": "User",
      "created_at": "2025-06-07T13:27:44Z",
      "updated_at": "2025-06-13T18:59:17Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/743/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/743",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/743",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.724507",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey are you passing the otlp_endpoint or the related ENV to the application?",
          "created_at": "2025-06-09T18:41:21Z"
        },
        {
          "author": "vordimous",
          "body": "Yes, you can see the logged options object from the imported OpenLIT SDK. \n\n> openlit.options {\"otlpEndpoint\":\"http://our-otel-collector:4318/v1/traces\"...\n\nThe `OTEL_EXPORTER_OTLP_ENDPOINT` env var is set and i am reading that env var to manually set the param on init.\n\n```js\nOpenlit.init({\n  envir",
          "created_at": "2025-06-11T19:55:14Z"
        },
        {
          "author": "patcher9",
          "body": "@AmanAgarwal041 can you check it out,its related to the JS SDK",
          "created_at": "2025-06-12T11:30:13Z"
        },
        {
          "author": "vordimous",
          "body": "I am using the latest version for the OpenTelemetry node auto-instrumentation, which, as I understand, is a bit newer than what the Openlit JS SDK is using.\n",
          "created_at": "2025-06-12T14:01:09Z"
        },
        {
          "author": "vordimous",
          "body": "I added debugging and see this error:\n```\n{\ncode: \"400\"\ndata: \"{\"code\":3,\"message\":\"ReadUint64: unsupported value type, error found in #10 byte of ...|bleValue\\\":null}},{\\\"k|..., bigger context ...|{\\\"key\\\":\\\"gen_ai.usage.cost\\\",\\\"value\\\":{\\\"doubleValue\\\":null}},{\\\"key\\\":\\\"gen_ai.request.embedding_f",
          "created_at": "2025-06-13T18:59:17Z"
        }
      ]
    },
    {
      "issue_number": 741,
      "title": "[Feat]: Update Pricing File with gpt-4.1 info",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\nllm models pricing file doesn't contain gpt-4.1 models pricing info\nhttps://github.com/openlit/openlit/blob/main/assets/pricing.json\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\nAdd gpt-4.1 pricing info https://platform.openai.com/docs/pricing\n<img width=\"787\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f924cb92-fca1-40bf-9add-96664e9e2e3f\" />\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "akiseleva-dev",
      "author_type": "User",
      "created_at": "2025-06-05T07:40:49Z",
      "updated_at": "2025-06-10T18:41:20Z",
      "closed_at": "2025-06-10T18:41:20Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/741/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/741",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/741",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:11.917001",
      "comments": [
        {
          "author": "patcher9",
          "body": "@akiseleva-dev Are you gonna be pushing a PR to update the file?",
          "created_at": "2025-06-09T18:42:30Z"
        },
        {
          "author": "akiseleva-dev",
          "body": "> [@akiseleva-dev](https://github.com/akiseleva-dev) Are you gonna be pushing a PR to update the file?\n\n@patcher9 yeah, have it here: https://github.com/openlit/openlit/pull/748",
          "created_at": "2025-06-10T05:34:01Z"
        }
      ]
    },
    {
      "issue_number": 747,
      "title": "[Feat]: Support Elastic APM v2 Endpoint",
      "body": "### üöÄ What's the Problem?\nThe current openlit sends to elastic endpoint `/v1/metrics`, and the latest Elastic APM uses `/v2` endpoints based on https://www.elastic.co/docs/solutions/observability/apm/elastic-apm-events-intake-api\n\n\nName | Endpoint\n-- | --\nAPM agent event intake | /intake/v2/events\nRUM event intake (v2) | /intake/v2/rum/events\nRUM event intake (v3) | /intake/v3/rum/events\n\n\n### üí° Your Dream Solution\nTo support Elastic V2 intake endpoint\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "chongzhang",
      "author_type": "User",
      "created_at": "2025-06-09T16:49:46Z",
      "updated_at": "2025-06-10T14:55:54Z",
      "closed_at": "2025-06-10T14:55:54Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/747/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/747",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/747",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:12.156020",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey, The SDK just sends data to an OTLP endpoint. We have not fixed the Elastic APM endpoint ",
          "created_at": "2025-06-09T18:40:37Z"
        },
        {
          "author": "chongzhang",
          "body": "I was able to set env variable OTEL_EXPORTER_OTLP_TRACES_ENDPOINT to `intake/v2/events` and openlit sends to the correct endpoint, but it returns 400 `invalid content type: 'application/x-protobuf'`. I didn't look at further how to fix it, but I'm wondering if you have suggestions to support it.",
          "created_at": "2025-06-10T13:20:24Z"
        }
      ]
    },
    {
      "issue_number": 737,
      "title": "[Feat]: Add support for Agent Squad framework",
      "body": "### üöÄ What's the Problem?\n We are using aws labs open source framework within our enterprise for agentic framework as it pairs well with Bedrock and Open AI and this solution currently doesn‚Äôt support this framework, please note we are building production grade agents with this framework and it‚Äôs very popular inhouse\n\nhttps://awslabs.github.io/agent-squad/\n\n\n### üí° Your Dream Solution\nAdd support for this framework\n",
      "state": "open",
      "author": "raghavgrover13",
      "author_type": "User",
      "created_at": "2025-05-28T16:40:43Z",
      "updated_at": "2025-06-09T18:43:32Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/737/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/737",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/737",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:12.381759",
      "comments": [
        {
          "author": "patcher9",
          "body": "@raghavgrover13 Thanks, We can take a look at adding this!",
          "created_at": "2025-06-09T18:43:31Z"
        }
      ]
    },
    {
      "issue_number": 742,
      "title": "[Feat]: Support Custom API Endpoints for Local/Ollama Models in Evaluations",
      "body": "### üöÄ What's the Problem?\nWe‚Äôre trying to use the Evaluations feature with models running locally via Ollama; however, OpenLit currently doesn‚Äôt allow configuring a custom API endpoint or supporting different API formats for connecting to on-premise models. The system seems to only support the public OpenAI API format, which prevents integration with local solutions like Ollama. As a result, we‚Äôre unable to run evaluations using our internally hosted models.\n\n\n### üí° Your Dream Solution\nIdeally, we would like to configure:\n\n- A custom API endpoint URL to connect to the Ollama API (e.g. http://localhost:11434 or an internal network endpoint).\n\n- Optional fields to specify the model name and any additional generation parameters (such as temperature, top_p, etc.).\n\n- Full support for the request/response format used by the Ollama API, so evaluations can be performed locally just like they would with the OpenAI API.\n\n- A toggle/flag to indicate the evaluation backend is on-premise, to better support hybrid environments.\n\nThis added flexibility would greatly enhance the power of the OpenLit platform and allow teams with strict data privacy or custom model requirements to fully utilize the Evaluations feature.\n\n\n### ü§î Seen anything similar?\nWe've also seen similar feature requests for Azure OpenAI support, which point to a broader need for customizable backends.\n\n\n### üñºÔ∏è Pictures or Drawings\n![Image](https://github.com/user-attachments/assets/44ec127d-c02a-47fb-a181-f64964d7c4e1)\n",
      "state": "open",
      "author": "alaorjr",
      "author_type": "User",
      "created_at": "2025-06-05T17:57:05Z",
      "updated_at": "2025-06-09T18:42:05Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/742/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/742",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/742",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:12.602308",
      "comments": [
        {
          "author": "patcher9",
          "body": "@AmanAgarwal041 Can you take a pass at this along with the other change you were planning to make around this?",
          "created_at": "2025-06-09T18:42:04Z"
        }
      ]
    },
    {
      "issue_number": 734,
      "title": "[Bug]: v 1.33.22 - Google Gemini instrumentation error & regression",
      "body": "Following up on https://github.com/openlit/openlit/issues/728 , v1.33.22 was released but some of the issues are still persisting.:\n\n1. Instrumentation is still broken with using Gemini with provided files. The issue is likely here:\n\n```\ndef format_content(messages):\n    \"\"\"\n    Process a list of messages to extract content, categorize them by role,\n    and concatenate all 'content' fields into a single string with role: content format.\n    \"\"\"\n\n    formatted_messages = []\n    prompt = \"\"\n\n    if isinstance(messages, list):\n        for content in messages:\n            **role = content.role**\n            parts = content.parts\n\n```\ncontent is a google.genai.types.Part object (per the Google [documentation](https://googleapis.github.io/python-genai/#how-to-structure-contents-argument-for-generate-content)), which does not have .role field.\n\nHere's an example dump of our content object, if it helps debugging.\n\n```\n{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': {'data': b'%PDF-1.7\\n%\\xf0\\x9f\\x96\\xa4\\n6 0\\n%%....EOF\\n', 'mime_type': 'application/pdf'}, 'text': None}\n```\n\n2. This version introduced an actual regression, compared to the previous behavior.\n\n- Previously, the error in my previous comment threw an Exception that was logged.\n- However, in the new implementation, the exception in the generate() function is thrown all the way up, breaking completely the application.\n- The streaming impl (generate_stream()) does catch the Exception.\n",
      "state": "closed",
      "author": "dani29",
      "author_type": "User",
      "created_at": "2025-05-27T12:02:54Z",
      "updated_at": "2025-05-28T18:19:59Z",
      "closed_at": "2025-05-28T14:41:42Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/734/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/734",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/734",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:12.835349",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey I have raised a PR, Put the whole block in a try and except. I see Gemini has a few different ways of passing input so gonna catach the most common one first. Then if that fails gonna atleast track the whole content block. ",
          "created_at": "2025-05-28T09:58:45Z"
        },
        {
          "author": "dani29",
          "body": "thank you! seems to work much better in 1.33.23 üëç \nappreciate the quick turnaround.",
          "created_at": "2025-05-28T18:19:58Z"
        }
      ]
    },
    {
      "issue_number": 735,
      "title": "[Feat]: Support for Azure OpenAI Custom Endpoint and API Key in Evaluations",
      "body": "### üöÄ What's the Problem?\nWe're trying to use the Evaluations feature with Azure OpenAI models, but the platform currently doesn't allow configuring a custom endpoint or using an Azure API key. The system appears to expect the OpenAI public API format and returns an error when we attempt to use our Azure credentials. Using a public OpenAI key works, but due to internal security and compliance policies, we‚Äôre required to use our private Azure OpenAI deployment.\n\n\n### üí° Your Dream Solution\nIdeally, we would be able to configure:\n\n- A custom endpoint URL for Azure OpenAI.\n- Optional fields such as deployment name, API version, and model name.\n\nFull compatibility with Azure‚Äôs API structure so that evaluations run seamlessly just like they would with the public OpenAI API.\n\n\n### üñºÔ∏è Pictures or Drawings\n\n![Image](https://github.com/user-attachments/assets/c05413e2-4131-47af-9c70-5f5a8c73ed92)\n",
      "state": "open",
      "author": "vanessaike",
      "author_type": "User",
      "created_at": "2025-05-27T16:42:33Z",
      "updated_at": "2025-05-27T16:42:33Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/735/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/735",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/735",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:13.019449",
      "comments": []
    },
    {
      "issue_number": 728,
      "title": "[Bug]: OpenLit does not generate automatic traces or metrics with Gemini SDK",
      "body": "\nüêõ What's Going Wrong?\nOpenLit is not generating automatic traces or metrics when used with the google.generativeai (Gemini) SDK. Although OpenLit is initialized correctly, no telemetry is being emitted ‚Äî neither metrics like token usage or latency, nor traces for the model call.\n\nüïµÔ∏è Steps to Reproduce\nSet up a collector (e.g. Tempo) listening for OTLP HTTP.\n\nRun the following Python script:\n```python\nimport os\nimport google.generativeai as genai\nimport openlit\n\n# Initialize openlit\nopenlit.init(\n    environment=\"openlit-testing\",\n    application_name=\"openlit-python-test\",\n    otlp_endpoint=os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://collector:4318\"),\n    disable_batch=True  # For immediate dispatch in demo\n)\n\ndef main():\n    api_key = os.getenv(\"GOOGLE_API_KEY\")\n    if not api_key:\n        raise ValueError(\"GOOGLE_API_KEY environment variable not set\")\n    \n    genai.configure(api_key=api_key)\n    model = genai.GenerativeModel('gemini-1.5-flash')\n  \n    response = model.generate_content(\"Explain the difference between a cat and a dog\")\n    \n    print(f\"Response: {response.text}\")\n    print(\"FINAL RESPONSE\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nObserve that no traces or metrics appear in your collector.\n\nüéØ What Did You Expect?\nI expected OpenLit to automatically capture and eport to otel traces and metrics of the LLM call; such as latency and token usage-\n\nüì∏ Any Screenshots?\nNo screenshots available ‚Äî issue is in missing telemetry data.\n\nüíª Your Setup\n\nOpenLIT SDK Version: 1.33.19\n\nDeployment Method: Docker\n\nüìù Additional Notes\nLet me know if Gemini support is planned or requires special integration/configuration. We'd love to use OpenLit with Gemini for observability but currently rely on manual instrumentation.\n",
      "state": "closed",
      "author": "adriamarcoval",
      "author_type": "User",
      "created_at": "2025-05-22T11:38:59Z",
      "updated_at": "2025-05-27T12:03:07Z",
      "closed_at": "2025-05-24T08:27:56Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/728/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/728",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/728",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:13.019471",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey so when using the Google AI Studio, It is recommended now to use the `google-genai` SDK. If you use that you should see telemetry being generated.",
          "created_at": "2025-05-23T04:07:05Z"
        },
        {
          "author": "t00mas",
          "body": "> Hey so when using the Google AI Studio, It is recommended now to use the `google-genai` SDK. If you use that you should see telemetry being generated.\n\nI think this has to be it. Replacing `import google.generativeai as genai` with `from google import genai` now triggers https://github.com/openlit",
          "created_at": "2025-05-23T13:25:54Z"
        },
        {
          "author": "patcher9",
          "body": "yup @t00mas and I have a fix that should be merged by tommorow for it",
          "created_at": "2025-05-23T17:08:48Z"
        },
        {
          "author": "dani29",
          "body": "Unfortunately this is still not fixed in **1.33.22**.  Throws the same error when using Gemini with file uploads.\n\n**.venv/lib/python3.11/site-packages/openlit/instrumentation/google_ai_studio/utils.py(30)**\n```\ndef format_content(messages):\n    \"\"\"\n    Process a list of messages to extract content,",
          "created_at": "2025-05-25T08:38:00Z"
        },
        {
          "author": "dani29",
          "body": "1.33.22 actually introduced an additional regression in Google tracing:\n\n- Previously, the error in my previous comment threw an Exception that was logged.\n- However, in the new implementation, the exception in the `generate()` function is thrown all the way up, breaking completely the application. ",
          "created_at": "2025-05-27T09:33:08Z"
        }
      ]
    },
    {
      "issue_number": 729,
      "title": "[Feat]: Support setting resource attributes at initialization",
      "body": "### üöÄ What's the Problem?\n**It's only a proposal.**\n\nNow we can't set OTel resource attributes directly in `openlit.init()`, which is needed(like setting `host.name` in attributes) in some scenarios.\n\n\n### üí° Your Dream Solution\nAdd a input parameter `resource_attributes` in `openlit.init()`, and set it inside the initialization function.\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n\n<img width=\"1698\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e5925a41-56d0-4b6e-b6ba-2bac85b58419\" />\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "minimAluminiumalism",
      "author_type": "User",
      "created_at": "2025-05-23T07:52:12Z",
      "updated_at": "2025-05-26T03:17:25Z",
      "closed_at": "2025-05-26T03:17:25Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/729/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/729",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/729",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:13.229466",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @minimAluminiumalism \n\nYou can set custom resource attributes uisng the Environment variable `OTEL_RESOURCE_ATTRIBUTES`. \nThis change was implemented a while back - https://github.com/openlit/openlit/pull/445",
          "created_at": "2025-05-24T07:07:49Z"
        }
      ]
    },
    {
      "issue_number": 691,
      "title": "[Bug]:Error in trace creation: 'str' object has no attribute 'role'",
      "body": "While sending the prompt it throws, Also the auto instrumentation of metrics not working , I am using google gemni\n",
      "state": "closed",
      "author": "sriramraja-krish",
      "author_type": "User",
      "created_at": "2025-04-03T14:33:19Z",
      "updated_at": "2025-05-24T08:29:11Z",
      "closed_at": "2025-05-24T08:29:10Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/691/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/691",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/691",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:13.446360",
      "comments": [
        {
          "author": "dani29",
          "body": "Happens to me as well. \n\nRelevant trace:\n```\n  File \"/Users/dani29/foo/.venv/lib/python3.11/site-packages/openlit/instrumentation/google_ai_studio/google_ai_studio.py\", line 79, in wrapper\n    role = content.role\n           ^^^^^^^^^^^^\n\n  File \"/Users/dani29/foo/.venv/lib/python3.11/site-packages/p",
          "created_at": "2025-04-20T21:05:28Z"
        },
        {
          "author": "stavmaz",
          "body": "Same here. can you please send a fix?",
          "created_at": "2025-04-21T11:26:22Z"
        },
        {
          "author": "oritmosko",
          "body": "Got the same error, can you please give an update on this?\nThanks!",
          "created_at": "2025-04-21T11:45:05Z"
        },
        {
          "author": "patcher9",
          "body": "> The fix itself seem to be pretty trivial but I'm not sure about the contribution policy, backward compatibility concerns as well as the desired output, so raising this all here.\n\n@dani29 If you are interested happy to let you make the change, No big concers around backward compatibility on this as",
          "created_at": "2025-04-24T05:28:02Z"
        },
        {
          "author": "dani29",
          "body": "Thanks for the reply @patcher9 . Unfortunately I'm not sure I'll have capacity to thoroughly fix and test it in the next month - seems like if this issue impacting others then please don't block on my availability.",
          "created_at": "2025-04-29T18:13:23Z"
        }
      ]
    },
    {
      "issue_number": 707,
      "title": "[Bug]: google ai studio gemini async_generate output token NoneType",
      "body": "### üêõ What's Going Wrong?\n>  There are cases when the output token is of NoneType. So input_tokens + output_tokens gives int + NoneType error \n\n`exception.stacktrace\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.12/site-packages/openlit/instrumentation/google_ai_studio/async_google_ai_studio.py\", line 161, in wrapper\n    input_tokens + output_tokens)\n    ~~~~~~~~~~~~~^~~~~~~~~~~~~~~\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\n`\n\n### üïµÔ∏è Steps to Reproduce\n>  Call gemini with [gen ai sdk ](https://googleapis.github.io/python-genai/genai.html)\n\n### üéØ What Did You Expect?\n>  NoneType should have been handled here.\n\n### üì∏ Any Screenshots?\n>  https://drive.google.com/file/d/13rKlWShX-Q4jXvmGjMET41JipBf2plR0/view?usp=sharing\n\n### üíª Your Setup\n- OpenLIT Version: [1.33.11]\n\n### üìù Additional Notes\n>  NoneType handle should resolve this\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "thunderbolt06",
      "author_type": "User",
      "created_at": "2025-04-27T16:46:38Z",
      "updated_at": "2025-05-24T08:28:48Z",
      "closed_at": "2025-05-24T08:28:47Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/707/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/707",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/707",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:13.668170",
      "comments": [
        {
          "author": "patcher9",
          "body": "Thanks @thunderbolt06 I can take a pass at this",
          "created_at": "2025-05-04T06:56:03Z"
        },
        {
          "author": "patcher9",
          "body": "Fixed in the latest version",
          "created_at": "2025-05-24T08:28:47Z"
        }
      ]
    },
    {
      "issue_number": 730,
      "title": "[Bug]: openlit not working with open webui + pipelines(litellm)",
      "body": "### üêõ What's Going Wrong?\n>  Open initialization fails\n\n### üïµÔ∏è Steps to Reproduce\n>  Deploy openlit and initialize in openwebui pipelines\n\n### üéØ What Did You Expect?\n> Openlit should capture metrics\n\n### üì∏ Any Screenshots?\n>  Pictures can say a thousand words and can be super helpful!\n\n### üíª Your Setup\n- OpenLIT Version: latest image\n- OpenLIT SDK Version: 1.33.21\n- Deployment Method: Docker\n\n### üìù Additional Notes\n>  Getting initialization logs but metrics are not visible in openlit UI\n\n\n### üëê Want to Help Make It Happen?\n- ‚úÖ Yes, I'd like to volunteer and help out with this!\n",
      "state": "open",
      "author": "NikhilKr872",
      "author_type": "User",
      "created_at": "2025-05-23T17:05:44Z",
      "updated_at": "2025-05-24T07:23:37Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/730/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/730",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/730",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:13.841175",
      "comments": [
        {
          "author": "patcher9",
          "body": "Did you try the solution in https://github.com/orgs/openlit/discussions/560 ?\ncc @wolfgangsmdt\n",
          "created_at": "2025-05-24T07:09:12Z"
        },
        {
          "author": "NikhilKr872",
          "body": "Getting this error in logs\n```ERROR:openlit:OpenLIT metrics setup failed. Metrics will not be available: 'NoneType' object has no attribute 'create_histogram'```",
          "created_at": "2025-05-24T07:23:36Z"
        }
      ]
    },
    {
      "issue_number": 731,
      "title": "[Bug]: Grafana Dashboard Broken ‚Äì Incorrect Metric Names Used",
      "body": "üêõ What's Going Wrong?\n\nThe Grafana dashboards published on the official OpenLIT documentation are broken. When imported into Grafana (either on-prem or via Grafana Cloud), the panels display \"No data\" because they reference metric names that don't exist.\n\nFor example, the dashboard expects:\n\n    gen_ai_total_requests\n\n    gen_ai_usage_cost_sum\n\n    gen_ai_usage_total_tokens\n\nBut OpenLIT actually exposes:\n\n    gen_ai_requests_total\n\n    gen_ai_usage_cost_USD_bucket\n\n    gen_ai_client_token_usage_bucket\n\nThis results in all panels being empty.\n\nüïµÔ∏è Steps to Reproduce\n\n    1. Go to:\n\n        https://docs.openlit.io/latest/connections/prometheus-tempo\n\n        https://docs.openlit.io/latest/connections/grafanacloud\n\n    2. Download/import the provided Grafana dashboard JSON.\n\n    3. Load it into Grafana (tested both on-premise and cloud).\n\n    4. Observe that all panels show \"No data\".\n\nüéØ What Did You Expect?\n\nI expected the dashboard to correctly visualize OpenLIT metrics and display relevant data. The metric names in the dashboard should match those actually emitted by OpenLIT.\n\nüì∏ Any Screenshots?\n\nAttached screenshots show the dashboard panels with \"No data\" and the mismatched metric names.\n\n![Image](https://github.com/user-attachments/assets/8cb43c81-81e0-406d-a2f6-12b11fe1ea21)\n\nüíª Your Setup\n\n    OpenLIT SDK Version: [e.g., 0.0.3]\n    Deployment Method: Helm\n    Grafana: On-Premise\n    Prometheus + Tempo used for metrics and traces\n\nüìù Additional Notes\n\n    The issue may be partially related to differences in how Grafana Cloud regions handle OpenTelemetry attributes, but I‚Äôm encountering this on-premise as well.\n\n    OpenLIT appears to send metrics in OTel format correctly; it's the dashboard that seems outdated or misaligned.\n\n\n",
      "state": "open",
      "author": "adriamarcoval",
      "author_type": "User",
      "created_at": "2025-05-23T20:35:37Z",
      "updated_at": "2025-05-23T20:36:38Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/731/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/731",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/731",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:14.020422",
      "comments": []
    },
    {
      "issue_number": 722,
      "title": "[Feat]: Add agno auto instrument",
      "body": "### üöÄ What's the Problem?\nPhidata become agno https://github.com/agno-agi/agno\nThere is instrumentation for Phidata here https://github.com/openlit/openlit/tree/main/sdk/python/src/openlit/instrumentation/phidata\nBut there is no agno instrumentation\n\n### üí° Your Dream Solution\nAdd agno instrumentation\n\n\n### ü§î Seen anything similar?\n- https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-agno\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this! I will start python sdk first if time allows me\n",
      "state": "open",
      "author": "duynguyenhoang",
      "author_type": "User",
      "created_at": "2025-05-15T10:34:21Z",
      "updated_at": "2025-05-23T19:38:38Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/722/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/722",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/722",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:14.020446",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @duynguyenhoang can I assign this to you since you marked you wanted to help?",
          "created_at": "2025-05-23T04:08:01Z"
        },
        {
          "author": "duynguyenhoang",
          "body": "That is okie @patcher9 \nI will try to allocate my time next month and start working on it.",
          "created_at": "2025-05-23T19:38:37Z"
        }
      ]
    },
    {
      "issue_number": 713,
      "title": "[Bug]:GPU collector is using 100% of CPU",
      "body": "### üêõ What's Going Wrong?\nGPU collector sits in a tight loop and uses 100% of CPU.\n\nIn collector.py a tight loop is used.\n```        # Keep the script running indefinitely\n        global keep_running\n        while keep_running:\n            pass\n```\n\n\n### üïµÔ∏è Steps to Reproduce\n1. Start GPU collector using default arguments.\n2. Run docker stats. CPU usage is close to 100.\n\n### üéØ What Did You Expect?\nCPU usage should be less than 1%.\n\n### üì∏ Any Screenshots?\n>  Pictures can say a thousand words and can be super helpful!\n\n### üíª Your Setup\n- OpenLIT Version: 0.0.2\n- Deployment Method: Docker\n\n### üìù Additional Notes\n>  Got more to say? Tell us here.\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "mamcgrath",
      "author_type": "User",
      "created_at": "2025-04-29T14:57:31Z",
      "updated_at": "2025-05-23T04:08:18Z",
      "closed_at": "2025-05-23T04:08:17Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/713/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/713",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/713",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:14.299690",
      "comments": [
        {
          "author": "patcher9",
          "body": "Yes, What you can do I think is directly openlit python library with `collect_gpu_metrics=True` and then you could control the runtime. Would that work for you?\n\nInstead I can see a future where we can upgrade the current GPU Collector approach to be based on Go (I would love to understand signals t",
          "created_at": "2025-05-04T06:54:47Z"
        },
        {
          "author": "mamcgrath",
          "body": "I was able to work around this issue using dockers CPU quota command.   --cpu-quota=1000",
          "created_at": "2025-05-21T18:41:00Z"
        },
        {
          "author": "patcher9",
          "body": "I see, Thanks!",
          "created_at": "2025-05-23T04:08:17Z"
        }
      ]
    },
    {
      "issue_number": 727,
      "title": "[Bug]: gpu collector is not collecting",
      "body": "### üêõ What's Going Wrong?\ngpu collector is not collecting gpu data on nvidia jetson nano\n\n### üïµÔ∏è Steps to Reproduce\n- setup nvidia jetson\n- Setup ollama\n- Setup ollama proxy\n- Setup openlit in proxy\n- Enable gpu collector\n- Call the proxy to pass through the openlit collector\n- See the message that gpu info cannot be fetched\n\n### üéØ What Did You Expect?\nthat the gpu collector is collecting data for the gpu\n\n### üì∏ Any Screenshots?\n>  Pictures can say a thousand words and can be super helpful!\n\n### üíª Your Setup\n- OpenLIT Version: [e.g., 0.0.1]\n- OpenLIT SDK Version: [e.g., 0.0.3]\n- Deployment Method: docker\n\n### üìù Additional Notes\n>  Got more to say? Tell us here.\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!",
      "state": "open",
      "author": "TheHunterDog",
      "author_type": "User",
      "created_at": "2025-05-20T16:02:38Z",
      "updated_at": "2025-05-23T04:07:33Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/727/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/727",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/727",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:14.502549",
      "comments": [
        {
          "author": "patcher9",
          "body": "Could you share the logs of the collector?\n",
          "created_at": "2025-05-23T04:07:33Z"
        }
      ]
    },
    {
      "issue_number": 696,
      "title": "[Bug]: Invalid type NoneType for attribute 'gen_ai.response.service_tier' with autogen's AzureOpenAIChatCompletionClient",
      "body": "### üêõ What's Going Wrong?\n\nusing autogen's AzureOpenAIChatCompletionClient and openlit's tracing capability, I get the following warning constantly in the logs:\n\n\"Invalid type NoneType for attribute 'gen_ai.response.service_tier' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\"\n\npreviously an issue was opened regarding this on the langfuse github, where it was found out that it might be a bug in opentelemetry: https://github.com/orgs/langfuse/discussions/6185\n\n### üïµÔ∏è Steps to Reproduce\n\nminimal project example using openlit running with your docker-compose openlit setup:\n\nrequirements.txt\n\n```\npython-dotenv==1.0.1\nopenai==1.68.2\nautogen==0.8.3\nautogen-agentchat==0.4.9.2\nautogen-core==0.4.9.2\nautogen-ext==0.4.9.2\nopenlit==1.33.19\nag2 >= 0.3.2\n```\n\nminimal code example with two agents:\n\n```python\nimport asyncio\nimport os\n\nimport openlit\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.base import TaskResult\nfrom autogen_agentchat.conditions import ExternalTermination\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom dotenv import load_dotenv\n# Sets the global default tracer provider\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nif __name__ == \"__main__\":\n\n    load_dotenv()\n\n    os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://127.0.0.1:4318\"\n\n    trace_provider = TracerProvider()\n    trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\n\n    trace.set_tracer_provider(trace_provider)\n\n    # Creates a tracer from the global tracer provider\n    tracer = trace.get_tracer(__name__)\n    # Initialize OpenLIT instrumentation. The disable_batch flag is set to true to process traces immediately.\n    openlit.init(tracer=tracer, disable_batch=True)\n\n    azure_openai_client = AzureOpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        azure_deployment=os.environ['AZURE_OPENAI_GPT_DEPLOYMENT_NAME'],\n        api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n        azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n        api_key=os.environ['AZURE_OPENAI_API_KEY'],\n        temperature=0.2,\n        seed=42\n    )\n\n    cancellation_token = CancellationToken()\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    external_termination = ExternalTermination()\n    termination = text_mention_termination | external_termination\n\n    ping_agent = AssistantAgent(\n        name=\"PingAgent\",\n        model_client=azure_openai_client,\n        description=\"\"\"\n            respond to pong with ping.\n        \"\"\",\n        system_message=\"\"\"\n            respond to pong with ping.\n        \"\"\"\n    )\n\n    pong_agent = AssistantAgent(\n        name=\"PongAgent\",\n        model_client=azure_openai_client,\n        description=\"\"\"\n            respond to ping with pong.\n        \"\"\",\n        system_message=\"\"\"\n            respond to ping with pong. end the chat after 10 pongs with the keyword TERMINATE.\n        \"\"\"\n    )\n\n    agents = [ping_agent, pong_agent]\n    group_chat = SelectorGroupChat(\n        agents,\n        model_client=azure_openai_client,\n        termination_condition=termination,\n    )\n\n\n    async def solve_user_prompt(user_prompt: str):\n        with tracer.start_as_current_span(name=\"Test\",\n                                          end_on_exit=True,\n                                          record_exception=True, set_status_on_exception=True\n                                          ) as span_obj:\n            span_obj.set_attribute(\"langfuse.user.id\", \"1\")\n            span_obj.set_attribute(\"langfuse.tags\", [\"2\"])\n            return_message = None\n            try:\n                async for message in group_chat.run_stream(task=user_prompt):\n                    if isinstance(message, TaskResult):\n                        return_message = message\n                    else:  # intermediate\n                        print(f\" --- {str(message.source).upper()} \\n{message.content}\\n\")\n                return return_message\n            except ValueError as e:\n                print(f\"got ValueError {e}\")\n\n    async def do_task(user_prompt: str):\n        result = await solve_user_prompt(user_prompt)\n        if result is not None:\n            last_msg = result.messages[-1].content.replace(\"TERMINATE\", \"\")\n        else:\n            last_msg = \"result was None\"\n        print(last_msg)\n\n    asyncio.run( do_task(\"ping\") )\n```\n\n### üéØ What Did You Expect?\n\nno warnings\n\n### üíª Setup\n\n- python package openlit==1.33.19\n- for other packages, see requirements.txt above\n- Deployment Method: Docker, locally with your docker-compose\n\n",
      "state": "closed",
      "author": "mencker",
      "author_type": "User",
      "created_at": "2025-04-08T11:29:03Z",
      "updated_at": "2025-05-20T11:15:08Z",
      "closed_at": "2025-04-14T09:52:36Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/696/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/696",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/696",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:15.005431",
      "comments": [
        {
          "author": "patcher9",
          "body": "Solved in my recent PR! https://github.com/openlit/openlit/pull/699/commits/4479a37344168928f1ca99b281987defac108593\n\nThanks for finding and reporting it. Looking forward to more!",
          "created_at": "2025-04-14T09:52:36Z"
        },
        {
          "author": "swayson",
          "body": "Hi @patcher9 , I think this bug still exists or I am losing the plot. I am usibng 1.33.21 which apparently has the fix for ollama. \n\n\n```\nopenlit                                  1.33.21\nopentelemetry-api                        1.33.1\nopentelemetry-exporter-otlp              1.33.1\nopentelemetry-exp",
          "created_at": "2025-05-20T11:14:40Z"
        }
      ]
    },
    {
      "issue_number": 712,
      "title": "[Bug]: Ollama instrumentation doesn't work when using symbol imports",
      "body": "### üêõ What's Going Wrong?\n\nOllama instrumentation doesn't work when using direct symbol imports like `from ollama import chat`.\n\n\n### üïµÔ∏è Steps to Reproduce\nCode to reproduce this issue:\n\n```\nfrom ollama import chat # import the func `chat` beforehand\n\nopenlit.init(\n    otlp_endpoint=\"http://127.0.0.1:4318\",\n    application_name=\"openai-openlit-instr\",\n)\n\n# Call the `chat()` function here as following\nstream_response = chat(\n        model=base_model,\n        messages=[\n            {\n                'role': 'user',\n                'content': 'Tell a joke about opentelemetry'\n            },\n        ],\n        stream=True\n    )\n\n```\n\n### üéØ What Did You Expect?\n\nThe instrumentation works with traces and metrics sent to the endpoint.\n\n### üì∏ Any Screenshots?\n\n\n### üíª Your Setup\n- OpenLIT Version: latest \n- OpenLIT SDK Version: latest\n- Deployment Method: macOS Sonoma on Apple Silicon with Python 3.13.1\n\n### üìù Additional Notes\n\n**Cause**\n\nLooking at `packages/sample-app/.venv/lib/python3.12/site-packages/ollama/__init__.py`, the module exports a `chat` name which is simply an alias for `_client.chat`, established via `chat = _client.chat`. Since this assignment happens during the module's initialization, code using from ollama import chat effectively captures the current reference of `_client.chat` and binds it to the chat name in its own scope.\n\n```\n_client = Client()\n\ngenerate = _client.generate\nchat = _client.chat\nembed = _client.embed\n```\nLater modifications(applying wrappers for instrumentation) to the source method (Client.chat) do not propagate to these pre-existing imported references. The imported chat continues to point to the object it referenced at the time of import.\n\n**When does instrumentation take effect?**\nImporting the function in the following ways will allow the instrumentation to take effect.\n```\nimport ollama\n\nopenlit.init()\n\nollama.chat()\n```\nThe unit test case uses this, so it can be instrument successfully. And the another style is following.\n\n```\nopenlit.init()\n\nfrom ollama import chat\n\nchat()\n```\n\n### How to solve this\n\nMaybe in the instrumentation of ollama we can add a snippet of code which scan every loaded module and find any attribute named `chat` (or `generate`, `embeddings`, etc.) whose implementation lives in the ollama package, and replace it with our dynamic proxy. That proxy then delegates to the latest `ollama.chat` under the hood, so no matter how the user imported the function, their calls still get instrumented.\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "minimAluminiumalism",
      "author_type": "User",
      "created_at": "2025-04-29T11:34:21Z",
      "updated_at": "2025-05-15T16:17:08Z",
      "closed_at": "2025-05-15T16:17:08Z",
      "labels": [
        ":bug: Bug",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/712/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "minimAluminiumalism"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/712",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/712",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:15.219005",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @minimAluminiumalism Since you checked ye sto contributing the fix for this, Any help I can give to get you started?\n",
          "created_at": "2025-05-04T06:55:38Z"
        },
        {
          "author": "minimAluminiumalism",
          "body": "> Hey [@minimAluminiumalism](https://github.com/minimAluminiumalism) Since you checked ye sto contributing the fix for this, Any help I can give to get you started?\n\nJust assign it to me :)",
          "created_at": "2025-05-04T13:29:39Z"
        },
        {
          "author": "patcher9",
          "body": "Thanks @minimAluminiumalism for helping out, Just assigned\n",
          "created_at": "2025-05-05T06:09:24Z"
        }
      ]
    },
    {
      "issue_number": 692,
      "title": "Ollama ai system as Anthropic?",
      "body": "Ollama wrapper sets ai system to Anthropic in sdk/typescript/instrumentions/ollama/wrapper.ts",
      "state": "open",
      "author": "pelikhan",
      "author_type": "User",
      "created_at": "2025-04-06T19:52:28Z",
      "updated_at": "2025-05-04T06:56:55Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/692/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/692",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/692",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:15.442119",
      "comments": [
        {
          "author": "patcher9",
          "body": "@pelikhan Would you be interested in fixing this in a PR, Seems like a small change?\n\ncc @AmanAgarwal041 ",
          "created_at": "2025-04-14T07:00:51Z"
        },
        {
          "author": "pelikhan",
          "body": "sounds like work for an ai agent ;)",
          "created_at": "2025-04-14T15:23:47Z"
        },
        {
          "author": "patcher9",
          "body": "haha!",
          "created_at": "2025-05-04T06:56:54Z"
        }
      ]
    },
    {
      "issue_number": 693,
      "title": "[Feat]: Need to track the LLM model, Invoked through request API.",
      "body": "I am trying to invoke the model as request api, openlit not tracking that.",
      "state": "closed",
      "author": "sriramraja-krish",
      "author_type": "User",
      "created_at": "2025-04-07T09:41:11Z",
      "updated_at": "2025-04-24T05:26:33Z",
      "closed_at": "2025-04-24T05:26:33Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/693/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/693",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/693",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:15.648122",
      "comments": [
        {
          "author": "scottnzuk",
          "body": "yes i want to track roo code / cline inside vsc\n",
          "created_at": "2025-04-14T02:13:15Z"
        },
        {
          "author": "patcher9",
          "body": "Hey If you are making LLM calls via `requests` rather than using an LLM SDK or any framework then OpenLIT would not be able to track the model calls. \n\nI would recommend using either one of the two as its helps in error management aswell. If you want to stick to using `requests` I would recommend ma",
          "created_at": "2025-04-14T06:59:50Z"
        }
      ]
    },
    {
      "issue_number": 702,
      "title": "[Bug]: exception",
      "body": "# bug i m getting while using Openlit\nwhen i m trying to check results in openlit my Model is giving the result but in the openlit it is not taking it as a request, it taking it as an exception and this the exception its giving its occurring for latest version for openlit.\n\nerror:\n\"Events.Attributes\":[\n0:{\n\"exception.escaped\":\"False\"\n\"exception.type\":\"AttributeError\"\n\"exception.message\":\"'GenerationConfig' object has no attribute 'get'\"\n\"exception.stacktrace\":\"Traceback (most recent call last): File \"/venv/lib/python3.11/site-packages/openlit/instrumentation/vertexai/vertexai.py\", line 93, in __next__ chunk = self.__wrapped__.__next__() ^^^^^^^^^^^^^^^^^^^^^^^^^^^ StopIteration During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/venv/lib/python3.11/site-packages/openlit/instrumentation/vertexai/vertexai.py\", line 167, in __next__ value = inference_config.get(key) ^^^^^^^^^^^^^^^^^^^^ AttributeError: 'GenerationConfig' object has no attribute 'get' \"\n}\n]\n\n# latest version of open lit that i m using",
      "state": "open",
      "author": "Akshata990418",
      "author_type": "User",
      "created_at": "2025-04-18T07:35:13Z",
      "updated_at": "2025-04-24T05:25:53Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/702/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/702",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/702",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:15.849762",
      "comments": [
        {
          "author": "patcher9",
          "body": "Interesting, Could you share the code that you are using? Like just the vertexai llm call part",
          "created_at": "2025-04-24T05:25:52Z"
        }
      ]
    },
    {
      "issue_number": 705,
      "title": "[Feat]: Google-Genai support",
      "body": "### üöÄ What's the Problem?\nOur team is using Gemini using google-genai python SDK, which is the new unified SDK to interact with Google's Gemini model using both developer studio or VertexAI.  It seems like currently both Google's instrumentation are using \"google-generativeai\" ([deprecated](https://pypi.org/project/google-generativeai/)) or the veretxai package, which is not the primary way to consume models.\n\nCurrently, it's an adoption blocker for our team. \n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\nSupport google [Python SDK](https://github.com/googleapis/python-genai) out of the box, natively. \n\n",
      "state": "closed",
      "author": "dani29",
      "author_type": "User",
      "created_at": "2025-04-20T13:52:14Z",
      "updated_at": "2025-04-20T14:43:29Z",
      "closed_at": "2025-04-20T14:43:28Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/705/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/705",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/705",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.063774",
      "comments": [
        {
          "author": "dani29",
          "body": "my bad, used a stale SDK version ü§¶‚Äç‚ôÇ ",
          "created_at": "2025-04-20T14:43:28Z"
        }
      ]
    },
    {
      "issue_number": 704,
      "title": "[Bug]: Not able to detect GPU, even though ollama detects it.",
      "body": "### üêõ What's Going Wrong?\n> Not able to detect GPU, even though ollama detects it.\n\n### üïµÔ∏è Steps to Reproduce\n>  i am using docker image `nvidia/cuda:12.8.1-base-ubuntu24.04`\n\n### üéØ What Did You Expect?\n>  Describe what you thought would happen.\n\n### üì∏ Any Screenshots?\n```\n2025/04/19 07:21:24 routes.go:1231: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-04-19T07:21:24.063Z level=INFO source=images.go:458 msg=\"total blobs: 9\"\ntime=2025-04-19T07:21:24.063Z level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\ntime=2025-04-19T07:21:24.063Z level=INFO source=routes.go:1298 msg=\"Listening on 127.0.0.1:11434 (version 0.6.5)\"\ntime=2025-04-19T07:21:24.063Z level=DEBUG source=sched.go:107 msg=\"starting llm scheduler\"\ntime=2025-04-19T07:21:24.063Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-04-19T07:21:24.064Z level=DEBUG source=gpu.go:98 msg=\"searching for GPU discovery libraries for NVIDIA\"\ntime=2025-04-19T07:21:24.064Z level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=libcuda.so*\ntime=2025-04-19T07:21:24.064Z level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[/usr/local/lib/ollama/libcuda.so* /usr/local/cuda/lib64/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]\"\ntime=2025-04-19T07:21:24.064Z level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[/usr/lib/x86_64-linux-gnu/libcuda.so.570.86.15]\ninitializing /usr/lib/x86_64-linux-gnu/libcuda.so.570.86.15\ndlsym: cuInit - 0x7fbabfd0de00\ndlsym: cuDriverGetVersion - 0x7fbabfd0de20\ndlsym: cuDeviceGetCount - 0x7fbabfd0de60\ndlsym: cuDeviceGet - 0x7fbabfd0de40\ndlsym: cuDeviceGetAttribute - 0x7fbabfd0df40\ndlsym: cuDeviceGetUuid - 0x7fbabfd0dea0\ndlsym: cuDeviceGetName - 0x7fbabfd0de80\ndlsym: cuCtxCreate_v3 - 0x7fbabfd0e120\ndlsym: cuMemGetInfo_v2 - 0x7fbabfd0e8a0\ndlsym: cuCtxDestroy - 0x7fbabfd6c9f0\ncalling cuInit\ncalling cuDriverGetVersion\nraw version 0x2f30\nCUDA driver version: 12.8\ncalling cuDeviceGetCount\ndevice count 1\ntime=2025-04-19T07:21:24.124Z level=DEBUG source=gpu.go:125 msg=\"detected GPUs\" count=1 library=/usr/lib/x86_64-linux-gnu/libcuda.so.570.86.15\n[GPU-bd4533c9-c880-0ab8-27e7-6c5248646113] CUDA totalMem 22565 mb\n[GPU-bd4533c9-c880-0ab8-27e7-6c5248646113] CUDA freeMem 22073 mb\n[GPU-bd4533c9-c880-0ab8-27e7-6c5248646113] Compute Capability 8.9\ntime=2025-04-19T07:21:24.231Z level=DEBUG source=amd_linux.go:419 msg=\"amdgpu driver not detected /sys/module/amdgpu\"\nreleasing cuda driver library\ntime=2025-04-19T07:21:24.231Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-bd4533c9-c880-0ab8-27e7-6c5248646113 library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4090\" total=\"22.0 GiB\" available=\"21.6 GiB\"\nOpenLIT GPU Instrumentation Error: No supported GPUs found.If this is a non-GPU host, set `collect_gpu_stats=False` to disable GPU stats.\n```\n### üíª Your Setup\n - OpenLIT SDK Version:  `pip list` gives `openlit 1.33.19`\n- Deployment Method: Nomad , docker driver\n\n\nNOTE: `nvidia-smi` works",
      "state": "closed",
      "author": "devashishraj",
      "author_type": "User",
      "created_at": "2025-04-19T07:29:34Z",
      "updated_at": "2025-04-19T10:35:22Z",
      "closed_at": "2025-04-19T10:35:21Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/704/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/704",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/704",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.275718",
      "comments": [
        {
          "author": "devashishraj",
          "body": "It needs these python packages : \n- psutil \n- nvidia-ml-py \n- amdsmi ",
          "created_at": "2025-04-19T10:35:21Z"
        }
      ]
    },
    {
      "issue_number": 567,
      "title": "[Feat]: Add support for bedrock invoke api",
      "body": "### üöÄ What's the Problem?\r\nBedrock Instrumentor only supports convert API right now. It will be good to have support for the invoke endpoint as not all the models are supported in converse",
      "state": "closed",
      "author": "kujjwal02",
      "author_type": "User",
      "created_at": "2025-01-12T10:43:28Z",
      "updated_at": "2025-04-16T06:49:26Z",
      "closed_at": "2025-04-16T06:49:26Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/567/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/567",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/567",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.467298",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey we did support the Invoke API but since the converse API was made a recommended choice, we removed support for invoke. You could try openlit sdk version before `1.16.1` and see if it works. Based on what more you need, I can add it back to the latest version of the sdk",
          "created_at": "2025-01-20T05:26:57Z"
        }
      ]
    },
    {
      "issue_number": 280,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring xAI",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor xAI LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to OpenAI, Have a auto instrumentation for xAI\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:48:08Z",
      "updated_at": "2025-04-16T06:49:01Z",
      "closed_at": "2025-04-16T06:49:01Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/280/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/280",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/280",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.672860",
      "comments": []
    },
    {
      "issue_number": 278,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring Titan ML",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor Titan ML LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to OpenAI, Have a auto instrumentation for Titan ML\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:46:48Z",
      "updated_at": "2025-04-16T06:48:56Z",
      "closed_at": "2025-04-16T06:48:56Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-js",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/278/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/278",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/278",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.672881",
      "comments": [
        {
          "author": "rohitmore07",
          "body": "Hey @patcher9, I came across the issue titled #278 and would love to contribute. However, I‚Äôd appreciate it if you could provide more details or elaborate on specific requirements. ",
          "created_at": "2024-12-07T19:07:10Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @rohitmore07 This would be for JS as TitanML seems to use the OpenAI SDK in python that we already covered.\r\n\r\nhttps://docs.openlit.io/latest/integrations/titan-ml",
          "created_at": "2024-12-08T04:09:36Z"
        },
        {
          "author": "patcher9",
          "body": "lemme know if you want to contribute for the JS SDK?",
          "created_at": "2024-12-09T06:33:43Z"
        },
        {
          "author": "rohitmore07",
          "body": "I‚Äôm definitely interested in contributing to the JS SDK. Please share the details or let me know how I can get started.",
          "created_at": "2024-12-09T08:21:08Z"
        },
        {
          "author": "patcher9",
          "body": "Perfect!, I'd recommend starting with adding an Ollama Instrumentation\r\n\r\nhttps://github.com/openlit/openlit/issues/403\r\n\r\nThis is a [reference PR](https://github.com/openlit/openlit/pull/352/files) for adding OpenAI and Anthropic Instrumentation in the JS/TS SDK\r\n\r\n\r\nNow How you can get started\r\n\r\n",
          "created_at": "2024-12-09T08:58:01Z"
        }
      ]
    },
    {
      "issue_number": 262,
      "title": "[Feat]: Add pre-built LLM Observability dashboard for DataDog",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nWe don't currently have a pre-built dashboard for users exporting LLM monitoring data collected by OpenLIT to DataDog which puts the burden on the user to see traces and metrics manually and build a dashboard\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to pre-built dashboard for Grafana Cloud, Signoz etc, Built a DataDog  OOTB dashboard for users to monitor their LLM Applications\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-02T09:30:51Z",
      "updated_at": "2025-04-16T06:48:47Z",
      "closed_at": "2025-04-16T06:48:47Z",
      "labels": [
        ":book: Documentation",
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        ":raised_hand: Up for Grabs",
        "Connections"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/262/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/262",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/262",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.899775",
      "comments": []
    },
    {
      "issue_number": 78,
      "title": "[Feat]: Add caching layer to fetch active db configs and other settings",
      "body": "### üöÄ What's the Problem?\r\nRecommended tool : [lru-cache](https://www.npmjs.com/package/lru-cache)\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-29T21:38:35Z",
      "updated_at": "2025-04-16T06:48:36Z",
      "closed_at": "2025-04-16T06:48:36Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/78/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/78",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/78",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.899799",
      "comments": []
    },
    {
      "issue_number": 69,
      "title": "[Bug]: Check load times for the request page, it seems to be a little slow. ",
      "body": "### üêõ What's Going Wrong?\r\nExplain the bug. Tell us what's happening that shouldn't be.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Start at '...'\r\n2. Click '...'\r\n3. Look for '...'\r\n4. Oops, there's the issue!\r\n\r\n### üéØ What Did You Expect?\r\nDescribe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\nPictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- Doku Version: [e.g., 0.0.1]\r\n- DokuMetry SDK: [Python or NodeJS]\r\n- DokuMetry SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Linux, Windows, Docker]\r\n\r\n### üìù Additional Notes\r\nGot more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-22T09:56:23Z",
      "updated_at": "2025-04-16T06:48:31Z",
      "closed_at": "2025-04-16T06:48:31Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/69/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/69",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/69",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:16.899806",
      "comments": [
        {
          "author": "patcher99",
          "body": "@AmanAgarwal041  Is this still a valid issue? Everything seems fast enough for now",
          "created_at": "2024-03-12T06:12:59Z"
        }
      ]
    },
    {
      "issue_number": 68,
      "title": "[Bug]: Time interval should be carried forward between the dashboard and request page",
      "body": "### üêõ What's Going Wrong?\r\nWhen we update the time filter on dashboard and switch to request page, the filter value goes back to default.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Go to dashboard page\r\n2. Change time filter\r\n3. Go to request page via navigation\r\n4. The filter goes back to default\r\n\r\n### üéØ What Did You Expect?\r\nIt should ideally be carried on the navigation\r\n\r\n### üì∏ Any Screenshots?\r\nPictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- Doku Version: [e.g., 0.0.1]\r\n- DokuMetry SDK: [Python or NodeJS]\r\n- DokuMetry SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Linux, Windows, Docker]\r\n\r\n### üìù Additional Notes\r\nGot more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-22T09:54:06Z",
      "updated_at": "2025-04-16T06:48:25Z",
      "closed_at": "2025-04-16T06:48:25Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/68/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/68",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/68",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.115254",
      "comments": []
    },
    {
      "issue_number": 37,
      "title": "[Feat]: Add Alerting support",
      "body": "### üöÄ What's the Problem?\r\nIn our current setup with the Doku, and monitoring OpenAI API Calls, we lack the ability to proactively monitor and react to critical metrics such as usage spikes, approaching request limit thresholds, and other relevant statistics that are pivotal for maintaining operational stability and cost management. Without real-time alerts, teams are often reactive rather than proactive, leading to disrupted services or unnecessary expenses.\r\n\r\n### üí° Your Dream Solution\r\nA robust, configurable alerting system within LLM Observability that allows users to:\r\n\r\n1. **Pre-built Alerts:**Alerts should be prebuilt into Doku with proper default thresholds\r\n1. **Set Custom Thresholds:** Users should be able to define custom thresholds for various metrics (e.g., number of requests, usage spikes) beyond which alerts would be triggered.\r\n1. **Multi-Channel Alerts:** Alerts should be dispatchable through multiple channels, including email, SMS, Slack, or webhooks, to ensure prompt notifications. We can start with email and Slack maybe?\r\n1. **Comprehensive Dashboard:** An integrated dashboard that displays current metrics, historical data, and alert configuration settings for easy monitoring and management.\r\n\r\n### ü§î Seen anything similar?\r\nSimilar to Prometheus AlertManager where user can silence alerts, send to different channels etc.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-06T23:17:46Z",
      "updated_at": "2025-04-16T06:48:17Z",
      "closed_at": "2025-04-16T06:48:17Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client",
        "Alerts"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/37/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/37",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/37",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.115278",
      "comments": []
    },
    {
      "issue_number": 644,
      "title": "[Feat]: Loosen Dependency Constraints",
      "body": "### üöÄ What's the Problem?\nGetting depency conflicts when using latest opentelemetry-sdk (1.30.0)\n\n\n### üí° Your Dream Solution\nMake the dependencies in  `sdk/python/pyproject.toml` a bit more loose (lock the major version, but allow upgardes for minor versions).\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "kujjwal02",
      "author_type": "User",
      "created_at": "2025-03-08T16:51:05Z",
      "updated_at": "2025-04-16T06:47:12Z",
      "closed_at": "2025-04-16T06:47:12Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/644/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/644",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/644",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.115284",
      "comments": [
        {
          "author": "patcher9",
          "body": "Thanks for raising the issue @kujjwal02 \n\nI see you wanna volunteer, Lemme know if you need any help with the PR",
          "created_at": "2025-03-09T08:28:34Z"
        }
      ]
    },
    {
      "issue_number": 685,
      "title": "[Bug]: typo in python sdk",
      "body": "### üêõ What's Going Wrong?\n> There is a typo in sdk/python/src/openlit/semcov/__init__.py. the class is called: SemanticConvetion and should be SemanticConvention\n\n### üïµÔ∏è Steps to Reproduce\n>  you can go to the file and see the typo\n\n### üéØ What Did You Expect?\n>  I will submit a PR to fix this issue \n\n\n### üì∏ Any Screenshots?\n>  Pictures can say a thousand words and can be super helpful!\n\n### üíª Your Setup\n- OpenLIT Version: [e.g., 1.33.19]\n- OpenLIT SDK Version: [e.g., 1.33.19]\n- Deployment Method: Docker\n\n### üìù Additional Notes\n>  \n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "fazd",
      "author_type": "User",
      "created_at": "2025-03-27T01:32:13Z",
      "updated_at": "2025-04-14T23:44:27Z",
      "closed_at": "2025-04-14T23:44:27Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/685/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/685",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/685",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.382135",
      "comments": []
    },
    {
      "issue_number": 660,
      "title": "[Bug]: ModuleNotFoundError: No module named 'opentelemetry.sdk._events'",
      "body": "### üêõ What's Going Wrong?\n>  **I'm working on crewai project, and trying to integrate it with openlit. I'm facing following error**\n\n/crew/py_env/lib/python3.11/site-packages/openlit/otel/events.py\", line 9, in <module>\n    from opentelemetry.sdk._events import EventLoggerProvider\nModuleNotFoundError: No module named 'opentelemetry.sdk._events'\n\n\n\n\n### üíª Your Setup\ndependencies = [\n    \"crewai (>=0.105.0,<0.106.0)\",\n    \"crewai-tools (>=0.37.0,<0.38.0)\",\n    \"openlit (>=1.33.16,<2.0.0)\",\n    \"langtrace-python-sdk (>=3.8.6,<4.0.0)\",\n    \"python-dotenv (>=1.0.1,<2.0.0)\"\n]\n\n### üìù Additional Notes\n>  When I further move and check site packages I found \n\n![Image](https://github.com/user-attachments/assets/43df4c50-2d39-4a8d-a316-3a92fa215fb5)\n\nopenlit depends of this package(opentelementry) - from opentelemetry.sdk._events import EventLoggerProvider \n\n\nIn opentelementry there was no pacakge **_events** in opentelementry.sdk\n\n![Image](https://github.com/user-attachments/assets/645a6b77-acc7-4eaf-b346-c0f589f90f6c)\n",
      "state": "open",
      "author": "Akram12-06",
      "author_type": "User",
      "created_at": "2025-03-14T05:33:46Z",
      "updated_at": "2025-04-10T22:10:08Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/660/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/660",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/660",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.382158",
      "comments": [
        {
          "author": "RajeswariKumaran",
          "body": "I am facing the same issue",
          "created_at": "2025-03-14T06:51:32Z"
        },
        {
          "author": "patcher9",
          "body": "Which version of the opentelemetry sdk are you using?\n Mostly need to bump up the version constraint for it in pyproject.toml",
          "created_at": "2025-03-14T08:50:16Z"
        },
        {
          "author": "Akram12-06",
          "body": "using opentelementry version 1.27.0",
          "created_at": "2025-03-14T09:20:51Z"
        },
        {
          "author": "patcher9",
          "body": "Could you upgrade it to 1.30.0 or above and then test?",
          "created_at": "2025-03-14T09:45:49Z"
        },
        {
          "author": "Akram12-06",
          "body": "I'm working with crewai, any avaliable version of crewai does not support opentelemetry sdk 1.30.0 or above",
          "created_at": "2025-03-14T10:19:47Z"
        }
      ]
    },
    {
      "issue_number": 559,
      "title": "[Bug]: Openlit initialized but AI metrics not displayed on openlit dashboard",
      "body": "### üêõ What's Going Wrong?\r\n>  I have initialized openlit in my application, but I am not seeing the metrics collected and displayed on the dashboard. I have composed up the openlit docker image and I have enabled python debug logging to try track the issue but the errors are not issues  I am able to fix.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  The change I have implemented in my application that has stopped the application from working are the following:\r\n- I have modularized my application which runs using api calls to different files using argparse and subprocess modules. Would the use of these modules prevent openlit from tracking and collecting the application AI metrics? \r\n- I have also implemented a streamlit UI - would the use of streamlit prevent openlit from working?\r\n\r\n### üéØ What Did You Expect?\r\n>  I expect the AI metrics to be collected and displayed on the openlit dashboard. \r\n\r\n### üì∏ Any Screenshots?\r\n> <img width=\"1202\" alt=\"Screenshot 2025-01-03 at 3 40 16‚ÄØPM\" src=\"https://github.com/user-attachments/assets/f4d277a0-fe42-487c-bbc9-01a68c52f58a\" />\r\n\r\n### üíª Your Setup\r\n- openlit: 1.30.5\r\n- opentelemetry-api: 1.28.1\r\n- opentelemetry-distro: 0.49b1\r\n- opentelemetry-exporter-otlp: 1.28.1\r\n- opentelemetry-exporter-otlp-proto-common: 1.28.1\r\n- opentelemetry-exporter-otlp-proto-grpc: 1.28.1\r\n- opentelemetry-exporter-otlp-proto-http: 1.28.1\r\n- opentelemetry-instrumentation: 0.49b1\r\n- opentelemetry-instrumentation-asgi: 0.49b1\r\n- opentelemetry-instrumentation-fastapi: 0.49b1\r\n- opentelemetry-proto: 1.28.1\r\n- opentelemetry-sdk:1.28.1\r\n- streamlit: 1.41.1\r\n- runs locally on MacOS Sonoma 14.7.1 operating system using an Apple M3 Pro chip\r\n\r\n### üìù Additional Notes\r\n>  Is there additional information you require or are you able to suggest changes I should make? Thank you\r\n",
      "state": "closed",
      "author": "ishachinniah-hds",
      "author_type": "User",
      "created_at": "2025-01-03T10:47:25Z",
      "updated_at": "2025-03-21T22:31:17Z",
      "closed_at": "2025-01-20T05:30:25Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/559/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/559",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/559",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.653700",
      "comments": [
        {
          "author": "patcher9",
          "body": "> Try turning off metrics, It works for me sometimes\r\n\r\n@ishanjainn that might not be the correct way to debug\r\n\r\n\r\nHey @ishachinniah-hds are you already using OpenTelemetry instrumentations or sdks?\r\n\r\n",
          "created_at": "2025-01-06T07:20:21Z"
        },
        {
          "author": "ishachinniah-hds",
          "body": "I am simply initializing openlit using the following:\r\n\r\nopenlit.init(\r\n    # openlit dashboard endpoint\r\n    otlp_endpoint=\"http://127.0.0.1:4318\", \r\n)\r\n\r\nThis was previously working, but with the UI implementation the metrics are no longer being reflected on the dashboard. \r\n\r\nI have commented out",
          "created_at": "2025-01-06T11:05:30Z"
        },
        {
          "author": "patcher9",
          "body": "try passing `disable_metrics=True` in openlit.init()\r\n\r\n> This was previously working, but with the UI implementation the metrics are no longer being reflected on the dashboard.\r\n\r\nThe dashboard in OpenLIT should not be affected as that is based on traces and not the metrics",
          "created_at": "2025-01-07T07:25:46Z"
        },
        {
          "author": "ishachinniah-hds",
          "body": "I attempted that - it doesn't seem to be making a difference. \r\n\r\nOne thing I noticed is that when I have docker composed down, I used to get a network connection error when I tried to run the application. Currently, if I have composed down the docker image in openlit, the openlit initialization is ",
          "created_at": "2025-01-09T17:19:33Z"
        },
        {
          "author": "patcher9",
          "body": "can you just do openlit.init() and send me the output?",
          "created_at": "2025-01-11T09:36:11Z"
        }
      ]
    },
    {
      "issue_number": 470,
      "title": "[Feat]: Auto-Evaluation Metrics Based for every LLM Request",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\nRight now OpenLIT can show the LLM events but their is no automated way to see if a event performed good or bad. This can be done using Eval metrics that generally run against a dataset\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\nAuto Evalutaion scoring for all LLM requests traced and stored in OpenLIT\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\nNA\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\nNA\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-10-25T18:29:06Z",
      "updated_at": "2025-03-17T08:43:41Z",
      "closed_at": "2025-03-17T08:43:41Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/470/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/470",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/470",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.889932",
      "comments": []
    },
    {
      "issue_number": 665,
      "title": "[Feat]: Use OpenTelemetry Events API for AWS Bedrock Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-17T06:43:47Z",
      "updated_at": "2025-03-17T07:00:08Z",
      "closed_at": "2025-03-17T07:00:08Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/665/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/665",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/665",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.889953",
      "comments": []
    },
    {
      "issue_number": 663,
      "title": "[Feat]: Use OpenTelemetry Events API for Azure AI Inference Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-16T12:27:08Z",
      "updated_at": "2025-03-16T12:37:27Z",
      "closed_at": "2025-03-16T12:37:27Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/663/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/663",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/663",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.889960",
      "comments": []
    },
    {
      "issue_number": 639,
      "title": "[Feat]: Using OpenTelemetry Events API to capture AI content",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\nThe GenAI OpenTelemetry SIG has moved to Events API as the dedicated format for content format from Span Events\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\nTight alignment with OpenTelemetry SIG Semantic Conventions.\n\nContinue emitting content in span events aswell as events API is not yet supported in JS and we want to keep the same data format across the two\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\nNA\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\nNA\n\n### üëê Want to Help Make It Happen?\n- [.] Yes, I'd like to volunteer and help out with this!\n",
      "state": "open",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-07T08:23:36Z",
      "updated_at": "2025-03-16T12:26:42Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/639/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/639",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/639",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.889967",
      "comments": []
    },
    {
      "issue_number": 654,
      "title": "[Bug]: 'SystemMessage' object is not subscriptable",
      "body": "### üêõ What's Going Wrong?\n\nWhen using OpenLIT with my LangChain application, I get the error `Error in trace creation: 'SystemMessage' object is not subscriptable`. The details are:\n\n```json\n{\n\"events\": [\n        {\n            \"name\": \"exception\",\n            \"attributes\": {\n                \"exception.type\": \"TypeError\",\n                \"exception.message\": \"'SystemMessage' object is not subscriptable\",\n                \"exception.stacktrace\": \"Traceback (most recent call last):\\n  File \\\"/lib/python3.10/site-packages/openlit/instrumentation/langchain/langchain.py\\\", line 269, in wrapper\\n    content_str = \\\", \\\".join(\\n  File \\\"/lib/python3.10/site-packages/openlit/instrumentation/langchain/langchain.py\\\", line 271, in <genexpr>\\n    if \\\"type\\\" in item else f'text: {item[\\\"text\\\"]}'\\nTypeError: 'SystemMessage' object is not subscriptable\\n\",\n                \"exception.escaped\": \"False\"\n            }\n        }\n    ]\n}\n```\n\nIt seems that, somehow, the object `content` obtained from `message` is a list of [LangChain System Message](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) objects, which do not follow the structure expected in `item`. Perhaps `SystemMessage.type` and `SystemMessage.text()` are what was intended to be called?\n\n### üïµÔ∏è Steps to Reproduce\n\nCreate any LangChain application, and add OpenLIT.\n\n### üéØ What Did You Expect?\n\nThe bug not to happen.\n\n### üíª Your Setup\n- OpenLIT Version: 1.33.14\n\n### üìù Additional Notes\n\nI had first mentioned this error in a [comment in a different issue](https://github.com/openlit/openlit/issues/635#issuecomment-2698769956), for a different version of the library. Reopened here to keep this independent from bugs already solved.\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!",
      "state": "closed",
      "author": "nirogu",
      "author_type": "User",
      "created_at": "2025-03-11T05:10:43Z",
      "updated_at": "2025-03-16T11:39:37Z",
      "closed_at": "2025-03-16T11:39:37Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/654/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/654",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/654",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:19.889974",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey can you share a snippet of your langchain code?",
          "created_at": "2025-03-11T06:52:29Z"
        },
        {
          "author": "nirogu",
          "body": "I can't share the exact code that threw that log because it's company owned code. I'll try to replicate it in a dummy imitation that I can post here, but it might take some time because I'm not sure of what part caused this",
          "created_at": "2025-03-11T15:35:48Z"
        },
        {
          "author": "patcher9",
          "body": "I essentially just need to know the langchain functions you used and versions pf langchain sdks",
          "created_at": "2025-03-11T15:52:28Z"
        },
        {
          "author": "mabehiro",
          "body": "Hello, I am also encountering this error, and I am also seeing the message: \"'HumanMessage' object is not subscriptable.\" This issue occurs when I use ChatPromptTemplate to create system and human prompt messages in my case. I apologize if this isn‚Äôt relevant to the issue discussed in this thread.  ",
          "created_at": "2025-03-13T18:15:59Z"
        },
        {
          "author": "patcher9",
          "body": "Woudl be great if anyone can share an example snippet so that I can use it for testing",
          "created_at": "2025-03-15T07:31:41Z"
        }
      ]
    },
    {
      "issue_number": 657,
      "title": "[Bug]: Openlit.init is not a function",
      "body": "### üêõ What's Going Wrong?\n>  Explain the bug. Tell us what's happening that shouldn't be.\nThe monitoring cannot be started with the most minimal code in my electron setup.\n```import Openlit from \"openlit\"\n\nOpenlit.init({ otlpEndpoint: \"http://127.0.0.1:4318/v1/traces\" })```\n\nActually openlit.default.init({ otlpEndpoint: \"http://127.0.0.1:4318/v1/traces\" }); solved it, but this causes is a TS error.\n\n### üïµÔ∏è Steps to Reproduce\n>  To see the bug, what steps should we follow?\nStart the project:\n[hb.tar.gz](https://github.com/user-attachments/files/19210970/hb.tar.gz)\n\n### üéØ What Did You Expect?\n>  Describe what you thought would happen.\n\n### üì∏ Any Screenshots?\n>  Pictures can say a thousand words and can be super helpful!\n\n```TypeError: openlit.init is not a function\n    at Object.<anonymous> (/Users/ferencdenes/hermes-proto/out/main/index.js:3:9)\n    at Module._compile (node:internal/modules/cjs/loader:1484:14)\n    at Module._extensions..js (node:internal/modules/cjs/loader:1564:10)\n    at Module.load (node:internal/modules/cjs/loader:1295:32)\n    at Module._load (node:internal/modules/cjs/loader:1111:12)\n    at c._load (node:electron/js2c/node_init:2:16955)\n    at cjsLoader (node:internal/modules/esm/translators:350:17)\n    at ModuleWrap.<anonymous> (node:internal/modules/esm/translators:286:7)\n    at ModuleJob.run (node:internal/modules/esm/module_job:234:25)\n    at async ModuleLoader.import (node:internal/modules/esm/loader:473:24)```\n\n### üíª Your Setup\n- OpenLIT Version: 1.4.1\n- OpenLIT SDK Version: [e.g., 0.0.3]\n- Deployment Method: Docker, but it is agnostic of that\n\n### üìù Additional Notes\n>  Got more to say? Tell us here.\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "open",
      "author": "FerencDenes",
      "author_type": "User",
      "created_at": "2025-03-12T13:44:51Z",
      "updated_at": "2025-03-12T13:49:26Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/657/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/657",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/657",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.101027",
      "comments": []
    },
    {
      "issue_number": 656,
      "title": "[Feat]: Refactor AstraDB OpenTelemetry Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-11T06:38:21Z",
      "updated_at": "2025-03-11T06:55:14Z",
      "closed_at": "2025-03-11T06:55:14Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/656/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/656",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/656",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.101721",
      "comments": []
    },
    {
      "issue_number": 580,
      "title": "[Bug]: Error in trace creation for AzureChatOpenAI",
      "body": "### üêõ What's Going Wrong?\nWhen using AzureChatOpenAI as the llm provider for my local GenAI RAG application - I have been getting the following error message in my terminal:\n    - Error in trace creation: 'AzureChatOpenAI' object has no attribute 'model_id'\n I am still seeing data being exported and displayed on the openLIT dashboard, however I am concerned that information is being lost given the use of AzureChatOpenAI. \n\n### üéØ What Did You Expect?\nIt would be ideal if trace creation with the use of AzureChatOpenAI would not raise an error. \n\n### üì∏ Any Screenshots?\n\n<img width=\"1120\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eeca2232-ea24-41e0-ae31-146b80738a5e\" />\n\n<img width=\"755\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e341cc63-1016-4568-940a-71a59a49e9c7\" />\n\n### üíª Your Setup\n- openlit: 1.30.5\n- opentelemetry-api: 1.28.1\n- opentelemetry-distro: 0.49b1\n- opentelemetry-exporter-otlp: 1.28.1\n- opentelemetry-exporter-otlp-proto-common: 1.28.1\n- opentelemetry-exporter-otlp-proto-grpc: 1.28.1\n- opentelemetry-exporter-otlp-proto-http: 1.28.1\n- opentelemetry-instrumentation: 0.49b1\n- opentelemetry-instrumentation-asgi: 0.49b1\n- opentelemetry-instrumentation-fastapi: 0.49b1\n- opentelemetry-proto: 1.28.1\n- opentelemetry-sdk:1.28.1\n- runs locally on MacOS Sonoma 14.7.1 operating system using an Apple M3 Pro chip\n",
      "state": "closed",
      "author": "ishachinniah-hds",
      "author_type": "User",
      "created_at": "2025-01-24T21:43:50Z",
      "updated_at": "2025-03-11T06:43:46Z",
      "closed_at": "2025-03-11T06:43:46Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/580/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/580",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/580",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.101732",
      "comments": [
        {
          "author": "patcher9",
          "body": "My bad for not getting back on this quicker, The Azure account I had has been closed so havent been able to update this instrumentation for a while. Can you send me the full LLM response you get, The instrumentation needs an update in parsing the response object fields",
          "created_at": "2025-01-28T08:14:13Z"
        },
        {
          "author": "ishachinniah-hds",
          "body": "This is an example output from running my application in the terminal:\n\nStarting the conversation. Type 'exit' or 'quit' to end the conversation.\nYou: what is the jimmy scale?\nError in trace creation: 'AzureChatOpenAI' object has no attribute 'model_id'\nError in trace creation: 'AzureChatOpenAI' obj",
          "created_at": "2025-01-28T20:38:12Z"
        },
        {
          "author": "patcher9",
          "body": "sorry my bad, Can you share the complete response object json that you get in llm response (Not just the text)?\n\nJust need to check the keys in the response object so that I can add changes for this bug",
          "created_at": "2025-01-29T11:10:29Z"
        },
        {
          "author": "ishachinniah-hds",
          "body": "I attempted to create the response object json, is this what you were looking for?\n\n{\n  'content': \"Please provide the log entries and your specific question for analysis.\",\n  'additional_kwargs': {'refusal': None},\n  'response_metadata': {\n    'token_usage': {\n      'completion_tokens': 12,\n      '",
          "created_at": "2025-01-29T14:55:27Z"
        },
        {
          "author": "ishachinniah-hds",
          "body": "@patcher9 Could you confirm what this error message indicates? Although I get the following error message, I do see metrics from these llm runs showing up on the openLIT dashboard. Is the error message incorrect or is some data being omitted? \n\n<img width=\"606\" alt=\"Image\" src=\"https://github.com/us",
          "created_at": "2025-02-20T19:05:01Z"
        }
      ]
    },
    {
      "issue_number": 601,
      "title": "[Bug]: LiteLLM Router Streaming Issue with Azure OpenAI",
      "body": "### üêõ What's Going Wrong?\n>  When im trying to use litellm router with openai chat acompletion with stream=True getting error \n\n```\n\"RuntimeWarning: Enable tracemalloc to get the object allocation traceback\nError occurred: litellm.APIError: AzureException APIError - 'coroutine' object is not iterable\"\n```\n\n### üïµÔ∏è Steps to Reproduce\n\n```python\nimport asyncio\nfrom litellm import Router\nimport openlit\n\nasync def main():\n    try:\n        model_list = [{ \n            \"model_name\": \"gpt-4o\",\n            \"litellm_params\": {\n                \"model\": \"azure/gpt-4o\",\n                \"api_key\": \"key\",\n                \"api_version\": \"version\",\n                \"api_base\": \"base_url\"\n            }\n        }]\n\n        router = Router(model_list=model_list)\n        response = await router.acompletion(\n            model=\"azure/gpt-4o\", \n            messages=[{\"role\": \"user\", \"content\": \"Hey, how's it going?\"}],\n            stream=True\n        )\n        async for chunk in response:\n            if hasattr(chunk.choices[0], 'delta') and chunk.choices[0].delta.content is not None:\n                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n\n    except Exception as e:\n        print(f\"Error occurred: {str(e)}\")\n\n\nopenlit.init()\nasyncio.run(main())\n```\n\n\n### üéØ What Did You Expect?\n>  LLM call should succeed and collect metrics and traces properly.\n\n\n### üíª Your Setup\n- OpenLIT Version: 1.33.8\n- OpenLIT SDK Version: 1.33.8\n- Deployment Method: Docker\n\n### üìù Additional Notes\n>  Got more to say? Tell us here.\nThis happens only while enabling stream=True\n\nHere is the whole traceback\n\n```\npython3.12/site-packages/litellm/llms/azure/azure.py:320: RuntimeWarning: coroutine 'async_chat_completions.<locals>.TracedAsyncStream.__getattr__' was never awaited\n headers = dict(raw_response.headers)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nError occurred: litellm.APIError: AzureException APIError - 'coroutine' object is not iterable\nReceived Model Group=azure/gpt4o\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n```\n\n### üëê Want to Help Make It Happen?\n- Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "karthiguna3",
      "author_type": "User",
      "created_at": "2025-02-13T04:56:24Z",
      "updated_at": "2025-03-11T06:41:38Z",
      "closed_at": "2025-03-11T06:41:38Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/601/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/601",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/601",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.341849",
      "comments": [
        {
          "author": "patcher9",
          "body": "Thanks @karthiguna3 for finding the bug\n\n> Yes, I'd like to volunteer and help out with this!\n\nLemme know if you need any help getting started with the fix",
          "created_at": "2025-02-13T06:53:21Z"
        },
        {
          "author": "karthiguna3",
          "body": "I had gone through the code but unable to spot the issue. would like to get this validated by someone who knows the code flow better.\n",
          "created_at": "2025-02-13T07:09:36Z"
        },
        {
          "author": "patcher9",
          "body": "https://github.com/openlit/openlit/blob/304087543713e4d0b615278372ebd3925f2f3b23/sdk/python/src/openlit/instrumentation/litellm/async_litellm.py#L39\n\nIm away from my laptop (Travelling) but think the fix should be in this class.\n\nCan you test against say OpenAI ? I havent tested this against router ",
          "created_at": "2025-02-13T11:28:36Z"
        },
        {
          "author": "patcher9",
          "body": "https://github.com/openlit/openlit/blob/304087543713e4d0b615278372ebd3925f2f3b23/sdk/python/src/openlit/instrumentation/litellm/async_litellm.py#L98\n\nMaybe this needs to be async for insteaf of just for",
          "created_at": "2025-02-13T11:30:05Z"
        },
        {
          "author": "karthiguna3",
          "body": "Here we are looping through the prompt right?, it has nothing to do with the response got.\n\nBut Anyway i tried it, still not working for me.\n\nCan you check once you are available?",
          "created_at": "2025-02-13T16:01:00Z"
        }
      ]
    },
    {
      "issue_number": 635,
      "title": "[Bug]: Error in trace creation: 'genai_total_tokens'",
      "body": "### üêõ What's Going Wrong?\n\nWhen using OpenLIT with my LangChain application, I always get:\n```\nInvalid type StringPromptValue for attribute 'gen_ai.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\nError in trace creation: 'genai_total_tokens'\n```\n\nThe error details are:\n```json\n{\n\"events\": [\n        {\n            \"name\": \"gen_ai.content.prompt\",\n            \"attributes\": {}\n        },\n        {\n            \"name\": \"gen_ai.content.completion\",\n            \"attributes\": {\n                \"gen_ai.completion\": \"\"\n            }\n        },\n        {\n            \"name\": \"exception\",\n            \"attributes\": {\n                \"exception.type\": \"KeyError\",\n                \"exception.message\": \"'genai_total_tokens'\",\n                \"exception.stacktrace\": \"Traceback (most recent call last):\\n  File \\\"/lib/python3.10/site-packages/openlit/instrumentation/langchain/langchain.py\\\", line 602, in wrapper\\n    metrics[\\\"genai_total_tokens\\\"].add(\\nKeyError: 'genai_total_tokens'\\n\",\n                \"exception.escaped\": \"False\"\n            }\n        }\n    ]\n}\n```\n\n### üïµÔ∏è Steps to Reproduce\n\nCreate any LangChain application, and add OpenLIT.\n\n### üéØ What Did You Expect?\n\nThe OpenLIT integration with Langchain to work smoothly and not throwing errors.\n\n### üíª Your Setup\n\n- OpenLIT Version: 1.33.9\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "nirogu",
      "author_type": "User",
      "created_at": "2025-03-04T16:38:07Z",
      "updated_at": "2025-03-11T04:47:27Z",
      "closed_at": "2025-03-07T08:29:56Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/635/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/635",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/635",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.531586",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey, sorrt about that. That was cruft which I missed to remove. I have released a new version 1.33.10. Also version till 1.33.8 work fine incase you see similar issue in this.I will try to fix it if you find",
          "created_at": "2025-03-04T18:32:01Z"
        },
        {
          "author": "nirogu",
          "body": "Hey, thanks for the quick response. Just tested the new release and this particular `genai_total_tokens` error message disappeared ü•≥ \nHowever, I still get a warning saying that `Invalid type AIMessage for attribute 'gen_ai.completion' value`, and a new error message appeared:\n```json\n{\n\"events\": [\n ",
          "created_at": "2025-03-04T20:01:02Z"
        },
        {
          "author": "patcher9",
          "body": "could you share a snippet of your langchain invoke function and the prompt structure?",
          "created_at": "2025-03-05T14:02:49Z"
        },
        {
          "author": "patcher9",
          "body": "Okay I figured the error, When We use say openai in langchain we get a different response format compared to us using say Ollama. I am adding a fix and should be released in an hour",
          "created_at": "2025-03-07T04:22:29Z"
        },
        {
          "author": "nirogu",
          "body": "Hey. I was just able to check the new version, and indeed the `Invalid type AIMessage for attribute 'gen_ai.completion' value` bug is gone. Thanks!",
          "created_at": "2025-03-11T04:41:45Z"
        }
      ]
    },
    {
      "issue_number": 648,
      "title": "[Feat]: Use OpenTelemetry Events for AssemblyAI Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-09T10:07:46Z",
      "updated_at": "2025-03-10T06:13:15Z",
      "closed_at": "2025-03-10T06:13:11Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/648/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/648",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/648",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.712911",
      "comments": []
    },
    {
      "issue_number": 633,
      "title": "[Bug]: Cannot install openlit in python13.3",
      "body": "### üêõ What's Going Wrong?\n>  Explain the bug. Tell us what's happening that shouldn't be.\n  Building wheel for tiktoken (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n  \n  √ó Building wheel for tiktoken (pyproject.toml) did not run successfully.\n  ‚îÇ exit code: 1\n  ‚ï∞‚îÄ> [32 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      copying tiktoken/registry.py -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      copying tiktoken/__init__.py -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      copying tiktoken/core.py -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      copying tiktoken/model.py -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      copying tiktoken/load.py -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      copying tiktoken/_educational.py -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      creating build/lib.macosx-15.0-arm64-cpython-313/tiktoken_ext\n      copying tiktoken_ext/openai_public.py -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken_ext\n      running egg_info\n      writing tiktoken.egg-info/PKG-INFO\n      writing dependency_links to tiktoken.egg-info/dependency_links.txt\n      writing requirements to tiktoken.egg-info/requires.txt\n      writing top-level names to tiktoken.egg-info/top_level.txt\n      reading manifest file 'tiktoken.egg-info/SOURCES.txt'\n      reading manifest template 'MANIFEST.in'\n      warning: no files found matching 'Makefile'\n      adding license file 'LICENSE'\n      writing manifest file 'tiktoken.egg-info/SOURCES.txt'\n      copying tiktoken/py.typed -> build/lib.macosx-15.0-arm64-cpython-313/tiktoken\n      running build_ext\n      running build_rust\n          Updating crates.io index\n      cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -C link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/_tiktoken.cpython-313-darwin.so\n      error: package `bstr v1.11.3` cannot be built because it requires rustc 1.73 or newer, while the currently active rustc version is 1.70.0-nightly\n      Either upgrade to rustc 1.73 or newer, or use\n      cargo update -p bstr@1.11.3 --precise ver\n      where `ver` is the latest version of `bstr` supporting rustc 1.70.0-nightly\n      error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -C 'link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/_tiktoken.cpython-313-darwin.so'` failed with code 101\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for tiktoken\nFailed to build tiktoken\n\n\n\n### üïµÔ∏è Steps to Reproduce\n>  To see the bug, what steps should we follow?\n\npip3 install openlit  # using Python13.3\n\n### üéØ What Did You Expect?\n>  Describe what you thought would happen.\n\nInstall successfully\n\n### üì∏ Any Screenshots?\n>  Pictures can say a thousand words and can be super helpful!\n\n### üíª Your Setup\n- OpenLIT Version: [e.g., 0.0.1]\n- OpenLIT SDK Version: [e.g., 0.0.3]\n- Deployment Method: [Helm, Docker]\n\n### üìù Additional Notes\n>  Got more to say? Tell us here.\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "jinjiaKarl",
      "author_type": "User",
      "created_at": "2025-03-04T14:00:32Z",
      "updated_at": "2025-03-09T13:15:37Z",
      "closed_at": "2025-03-09T09:30:20Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/633/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/633",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/633",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.712932",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @jinjiaKarl yeah seems like tiktoken is failing to build and seems to be affecting a few libraries, Any solutions or workarounds you know?",
          "created_at": "2025-03-06T11:01:40Z"
        },
        {
          "author": "jinjiaKarl",
          "body": "Sorry for the late reply. I wasn't able to figure it out. Thanks for fixing it.",
          "created_at": "2025-03-09T12:28:02Z"
        },
        {
          "author": "patcher9",
          "body": "No worries, Thanks for raising the issue!\nAlways welcome more of them!",
          "created_at": "2025-03-09T13:15:35Z"
        }
      ]
    },
    {
      "issue_number": 641,
      "title": "[Feat]: Use OpenTelemetry Events for AI21 Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-07T08:25:00Z",
      "updated_at": "2025-03-09T10:08:18Z",
      "closed_at": "2025-03-07T08:29:54Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/641/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/641",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/641",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939534",
      "comments": []
    },
    {
      "issue_number": 640,
      "title": "[Feat]: Use OpenTelemetry Events for Ollama Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-07T08:24:13Z",
      "updated_at": "2025-03-09T10:08:10Z",
      "closed_at": "2025-03-07T08:29:56Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/640/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/640",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/640",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939557",
      "comments": []
    },
    {
      "issue_number": 642,
      "title": "[Feat]: Use OpenTelemetry Events API for Anthropic Instrumentation",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-07T08:51:48Z",
      "updated_at": "2025-03-09T08:32:22Z",
      "closed_at": "2025-03-09T08:32:22Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/642/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/642",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/642",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939565",
      "comments": []
    },
    {
      "issue_number": 623,
      "title": "[Feat]: Support Groq Audio",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "open",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-03-03T10:17:09Z",
      "updated_at": "2025-03-03T10:17:09Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/623/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/623",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/623",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939576",
      "comments": []
    },
    {
      "issue_number": 611,
      "title": "[Bug]: Attribute does not exist in `ChatOpenAI` (LangChain)",
      "body": "### üêõ What's Going Wrong?\n\nWhen using OpenLIT with my LangChain application, I always get `Error in trace creation: 'ChatOpenAI' object has no attribute 'model_id'`, with the following details:\n```json\n{\n\"events\": [\n        {\n            \"name\": \"exception\",\n            \"attributes\": {\n                \"exception.type\": \"AttributeError\",\n                \"exception.message\": \"'ChatOpenAI' object has no attribute 'model_id'\",\n                \"exception.stacktrace\": \"Traceback (most recent call last):\\n  File \\\"/lib/python3.10/site-packages/openlit/instrumentation/langchain/langchain.py\\\", line 515, in wrapper\\n    model = instance.model_id\\n  File \\\"/lib/python3.10/site-packages/pydantic/main.py\\\", line 892, in __getattr__\\n    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\\nAttributeError: 'ChatOpenAI' object has no attribute 'model_id'\\n\",\n                \"exception.escaped\": \"False\"\n            }\n        }\n    ]\n}\n```\n\n### üïµÔ∏è Steps to Reproduce\n\nCreate any LangChain application that uses `ChatOpenAI`, and add OpenLIT.\n\n### üéØ What Did You Expect?\n\nThis error should not occur as indeed the `ChatOpenAI` object does not have a `model_id` attribute. [The class documentation](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) mentions `model_name` which likely is what is intended to be retrieved.\n\n### üíª Your Setup\n\n- OpenLIT Version: 1.33.8\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!",
      "state": "closed",
      "author": "nirogu",
      "author_type": "User",
      "created_at": "2025-02-26T22:51:43Z",
      "updated_at": "2025-03-03T06:16:51Z",
      "closed_at": "2025-03-03T06:16:51Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/611/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/611",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/611",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939583",
      "comments": []
    },
    {
      "issue_number": 602,
      "title": "[Bug]: SyntaxError: Cannot use import statement outside a module in OpenLIT SDK with CommonJS",
      "body": "### üêõ What's Going Wrong?\nI recently tried to install OpenLIT SDK in my Node.js application using `ts-node`, but I encountered the following error:\n\n```sh\n2025-02-13 19:13:20 import { Resource } from '@opentelemetry/resources';\n2025-02-13 19:13:20 ^^^^^^\n2025-02-13 19:13:20 SyntaxError: Cannot use import statement outside a module\n```\n\nIt seems that OpenLIT SDK (`/app/node_modules/openlit/src/index.ts`) is using ES module syntax. However, my project is configured with `\"module\": \"commonjs\"` in `tsconfig.json`, causing a compatibility issue.\n\n---\n\n### üîç Steps to Reproduce\n1. Set up a Node.js application with TypeScript.\n2. Install `openlit` using:\n\n   ```sh\n   npm install openlit\n   ```\n3. Ensure `tsconfig.json` contains:\n\n   ```json\n   {\n     \"compilerOptions\": {\n       \"module\": \"commonjs\"\n     }\n   }\n   ```\n\n4. Call `Openlit.init()` in `index.js`:\n\n   ```sh\n   Openlit.init();\n   ```\n\n5. Run the application using `ts-node`:\n\n   ```sh\n   npx ts-node index.ts\n   ```\n6. Observe the **SyntaxError**.\n\n---\n\n### üéØ What Did You Expect?\nI expected OpenLIT to work seamlessly in a CommonJS project without module-related issues.\n\n---\n\n### üñºÔ∏è Any Screenshots?\n\n![Image](https://github.com/user-attachments/assets/0808240c-5bf6-4ad0-8d6b-aec4f49abd88)\n\n---\n\n### üíª Your Setup\n- **OpenLIT SDK Version:** ^1.3.1\n\n\n---\n\n### üìù Additional Notes\nTo ensure compatibility with both CommonJS and ES modules, OpenLIT could provide separate builds in its package:\n\n```json\n{\n  \"main\": \"dist/index.cjs.js\", // CommonJS build\n  \"module\": \"dist/index.esm.js\" // ES Module build\n}\n```\n\nWould it be possible to update OpenLIT to support both module systems?\n\n---\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "afan-polyrific",
      "author_type": "User",
      "created_at": "2025-02-14T10:25:31Z",
      "updated_at": "2025-02-17T19:39:42Z",
      "closed_at": "2025-02-17T19:39:41Z",
      "labels": [
        ":bug: Bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/602/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/602",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/602",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939593",
      "comments": []
    },
    {
      "issue_number": 590,
      "title": "[Feat]: Helm Chart for OTel GPU Collector",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\n\nCurrently there is only a Docker image available to deploy the OTel GPU Collector. When deploying in Kubernetes, We have to write the Kubernetes manifest ourselves which is not very convenient.\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\n\nHelm Chart just like the OpenLIT one for the OTel GPU Collector\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\n\nNA\n\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\n\nNA\n\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "open",
      "author": "ishanjainn",
      "author_type": "User",
      "created_at": "2025-01-30T06:17:40Z",
      "updated_at": "2025-01-30T12:48:41Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        "Helm",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/590/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/590",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/590",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939603",
      "comments": []
    },
    {
      "issue_number": 573,
      "title": "[Docs]: Ports and Firewall",
      "body": "The `docker-compose.yaml` file exposes several ports, but I couldn't find in the documentation which ports are made for what. My microservices runs in other servers and I'm gonna have to put OpenLIT behind a Nginx Proxy. \r\n\r\nPlease, correct me if im wrong. What i could find is:\r\n\r\n| Service | Port | Description | External Use |\r\n|---|---|---|---|\r\n| openlit | 3000   | Used to access UI via Web Browser | Yes |\r\n| otel-collector |  4317 | GRPC server | No |\r\n| otel-collector | 4318 | HTTP server | Yes |\r\n| otel-collector |  8888 | Metrics | Optional |\r\n| otel-collector |  55679 | zPages | No |\r\n| clickhouse |  8123 | DB for Opentelemetry Collector | No |\r\n| clickhouse |  9000 | DB for OpenLIT | No |\r\n\r\nI'm trying to secure services in docker creating an internal network, but i need to know wich ports must be exposed to host, and wich ports dont. \r\n\r\nHere is my `docker-compose.yaml` file.\r\n```yaml\r\nservices:\r\n  clickhouse:\r\n    image: clickhouse/clickhouse-server:24.4.1\r\n    container_name: clickhouse\r\n    environment:\r\n      CLICKHOUSE_PASSWORD: ${OPENLIT_DB_PASSWORD:-OPENLIT}\r\n      CLICKHOUSE_USER: ${OPENLIT_DB_USER:-default}\r\n    volumes:\r\n      - clickhouse-data:/var/lib/clickhouse\r\n    #ports:\r\n      #- \"9000:9000\"\r\n      #- \"8123:8123\"\r\n    restart: unless-stopped\r\n    networks:\r\n      - openlit\r\n\r\n  openlit:\r\n    image: ghcr.io/openlit/openlit:latest\r\n    container_name: openlit\r\n    environment:\r\n      TELEMETRY_ENABLED: true\r\n      INIT_DB_HOST: clickhouse\r\n      INIT_DB_PORT: 8123\r\n      INIT_DB_DATABASE: ${OPENLIT_DB_NAME:-openlit}\r\n      INIT_DB_USERNAME: ${OPENLIT_DB_USER:-default}\r\n      INIT_DB_PASSWORD: ${OPENLIT_DB_PASSWORD:-OPENLIT}\r\n      SQLITE_DATABASE_URL: file:/app/client/data/data.db\r\n    ports:\r\n      - \"3000:3000\"\r\n    depends_on:\r\n      - clickhouse\r\n    volumes:\r\n      - openlit-data:/app/client/data\r\n    restart: unless-stopped\r\n    networks:\r\n      - openlit\r\n\r\n  otel-collector:\r\n    image: otel/opentelemetry-collector-contrib:0.94.0\r\n    container_name: otel-collector\r\n    environment:\r\n      INIT_DB_HOST: clickhouse\r\n      INIT_DB_PORT: 9000\r\n      INIT_DB_DATABASE: ${OPENLIT_DB_NAME:-openlit}\r\n      INIT_DB_USERNAME: ${OPENLIT_DB_USER:-default}\r\n      INIT_DB_PASSWORD: ${OPENLIT_DB_PASSWORD:-OPENLIT}\r\n    ports:\r\n      #- \"4317:4317\"\r\n      - \"4318:4318\"\r\n      - \"8888:8888\"\r\n      #- \"55679:55679\"\r\n    depends_on:\r\n      - clickhouse\r\n    volumes:\r\n      - ./assets/otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml\r\n    restart: unless-stopped\r\n    networks:\r\n      - openlit\r\n\r\nvolumes:\r\n  clickhouse-data:\r\n  openlit-data:\r\n\r\nnetworks:\r\n  openlit:\r\n    driver: bridge\r\n```\r\n\r\nCan you help me with this?",
      "state": "closed",
      "author": "felipeavilis",
      "author_type": "User",
      "created_at": "2025-01-15T18:37:57Z",
      "updated_at": "2025-01-30T06:36:30Z",
      "closed_at": "2025-01-30T06:36:30Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/573/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/573",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/573",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:20.939611",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @felipeavilis \n\n\n- `4318` for Otel Collector to receive metrics and traces\n- `8123` for Otel collector to send and store data in clickhouse\n- `3000` for the OpenLIT UI access\n\nI think these are the most important ones\n\n",
          "created_at": "2025-01-20T05:28:59Z"
        }
      ]
    },
    {
      "issue_number": 581,
      "title": "[Bug]: unwanted logs",
      "body": "### üêõ What's Going Wrong?\n>  When I try to import the openlit package in my Python code, it logs traces in the terminal. I tried to set the Openlit logger level to ERROR or disable it, but didn't work, even tried to stop all loggers in the environment, didn't help either.\n\n### üïµÔ∏è Steps to Reproduce\n> The following snipet would reproduce the situation:\n\n```\nfrom openai import OpenAI\nimport openlit\nopenlit.init()\n\nclient = OpenAI(api_key=\"your_api_key\")\nresponse = client.chat.completions.create(model=\"gpt-4o-mini-2024-07-18\",\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"hi, how are you?\"},\n])\n\nprint(response.choices[0].message.content)\n```\n\n### üéØ What Did You Expect?\n>  I want it to track the traces but not logging them\n\n### üì∏ Any Screenshots?\n\n![Image](https://github.com/user-attachments/assets/6fe36ae1-df45-4b3b-993f-add18e562549)\n\n### üíª Your Setup\n- OpenLIT Version: 1.33.2\n- OpenLIT SDK Version: [e.g., 0.0.3]\n- Deployment Method: Docker\n",
      "state": "closed",
      "author": "Alijanloo",
      "author_type": "User",
      "created_at": "2025-01-27T10:33:56Z",
      "updated_at": "2025-01-30T06:36:00Z",
      "closed_at": "2025-01-30T06:36:00Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/581/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/581",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/581",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:21.126423",
      "comments": [
        {
          "author": "patcher9",
          "body": "If the `otlp_endpoint` or `OTEL_EXPORTER_OTLP_ENDPOINT` is not provided, the OpenLIT SDK will output traces directly to your console, which is recommended during the development phase.\n\nTo send the traces ad store them, Configure the telemetry data destination as follows (parameter is passed to `ope",
          "created_at": "2025-01-27T13:02:48Z"
        }
      ]
    },
    {
      "issue_number": 587,
      "title": "[Bug]: Missing License in meta for Python package",
      "body": "### üêõ What's Going Wrong?\n>  Explain the bug. Tell us what's happening that shouldn't be.\nHello, \nI am working in a firm where we can procure packages as per the need using an automated procurement tool. It validates the use of package based on the license the package has, but Openlit python package doesn't have it mentioned in Pyproject.toml as \"Apache-2.0\", so we are getting LICENSE_NOT_FOUND error in our automated procurement tool.\n\n### üïµÔ∏è Steps to Reproduce\n>  To see the bug, what steps should we follow?\nWe can see in Pypi for Openlit where we can't see the License mentioned.\n\n### üéØ What Did You Expect?\n>  Describe what you thought would happen.\nUpdate the License to Apache-2.0 in pyproject.toml\n\n### üì∏ Any Screenshots?\n>  Pictures can say a thousand words and can be super helpful!\n\n![Image](https://github.com/user-attachments/assets/3f76f0e3-6ca5-4b2b-8a06-fe36f83fdce3)\n\n### üíª Your Setup\n- OpenLIT Version: [e.g., 0.0.1]\n- OpenLIT SDK Version: [e.g., 0.0.3]\n- Deployment Method: [Helm, Docker]\n\n### üìù Additional Notes\n>  Got more to say? Tell us here.\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "NaveenKumarReddy8",
      "author_type": "User",
      "created_at": "2025-01-29T14:11:41Z",
      "updated_at": "2025-01-29T15:58:05Z",
      "closed_at": "2025-01-29T15:58:05Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/587/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/587",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/587",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:21.322824",
      "comments": []
    },
    {
      "issue_number": 565,
      "title": "[Bug]: Manual Tracing ignoring exceptions",
      "body": "### üêõ What's Going Wrong?\r\n>  When manually tracing using `openlit.trace` decorator, exceptions thrown in the function are getting ignored and None is returned\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n```python\r\nimport openlit\r\n\r\nopenlit.init()\r\n\r\n@openlit.trace\r\ndef call_lc_openai():\r\n    raise ValueError(\"This is a test error\")\r\n\r\ntry:\r\n    call_lc_openai()\r\n    print(\"Function call succeeded\")\r\nexcept Exception as e:\r\n    print(\"Caught exception: \", e)\r\n```\r\n\r\n#### Output:\r\n```\r\nERROR:root:Error in call_lc_openai: This is a test error\r\nTraceback (most recent call last):\r\n  File \"/home/<user>/mambaforge/envs/openlit/lib/python3.11/site-packages/openlit/__init__.py\", line 531, in wrapper\r\n    response = wrapped(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/tmp/ipykernel_64608/3877127017.py\", line 7, in call_lc_openai\r\n    raise ValueError(\"This is a test error\")\r\nValueError: This is a test error\r\nFunction call succeeded\r\n```\r\n\r\n\r\n### üéØ What Did You Expect?\r\n>  Was expecting it to record the status in the span and reraise the exception\r\n\r\n### üíª Your Setup\r\n- OpenLIT SDK Version: 1.33.2\r\n- Deployment Method: Dockwe\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "open",
      "author": "kujjwal02",
      "author_type": "User",
      "created_at": "2025-01-11T19:03:28Z",
      "updated_at": "2025-01-20T07:00:21Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/565/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "chibuike-okpara"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/565",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/565",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:23.350558",
      "comments": [
        {
          "author": "patcher9",
          "body": "cc @murnitur any thoughts on this?",
          "created_at": "2025-01-12T06:51:54Z"
        },
        {
          "author": "chibuike-okpara",
          "body": "I‚Äôll sort it out and push",
          "created_at": "2025-01-12T09:12:18Z"
        },
        {
          "author": "patcher9",
          "body": "@murnitur any update on this?",
          "created_at": "2025-01-20T05:21:55Z"
        },
        {
          "author": "chibuike-okpara",
          "body": "@patcher9 On it now",
          "created_at": "2025-01-20T05:58:33Z"
        },
        {
          "author": "chibuike-okpara",
          "body": "@patcher9 I have fixed the issue and created a PR #575 \nThere are issues for it not deploying, please help me check it out",
          "created_at": "2025-01-20T07:00:20Z"
        }
      ]
    },
    {
      "issue_number": 574,
      "title": "[Bug]: Close Button in Openground",
      "body": "### üêõ What's Going Wrong?\r\nWhen creating a New Openground, and in Dark Theme, the Close Cross button color is black, and probably it should be more light grey or white.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n1. Open UI\r\n2. Change to Dark Theme\r\n3. Select Openground menu\r\n4. Click `+ New` button\r\n5. Clikc `Select Provider`\r\n\r\n### üéØ What Did You Expect?\r\nA light grey or white cross to close.\r\n\r\n### üì∏ Any Screenshots?\r\nLight Theme\r\n![image](https://github.com/user-attachments/assets/ad1cddad-9857-474c-afae-9867d5dcb8fe)\r\n\r\nDark Theme\r\n![image](https://github.com/user-attachments/assets/fe95cddf-7d6b-41a3-afce-a2c05703459e)\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: 1.12.0\r\n- Deployment Method: Docker\r\n",
      "state": "open",
      "author": "felipeavilis",
      "author_type": "User",
      "created_at": "2025-01-15T20:35:19Z",
      "updated_at": "2025-01-20T05:29:42Z",
      "closed_at": null,
      "labels": [
        ":bug: Bug",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/574/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kishan-Patel-dev"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/574",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/574",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:23.602713",
      "comments": [
        {
          "author": "Kishan-Patel-dev",
          "body": "Can I work on this issue",
          "created_at": "2025-01-18T12:17:58Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @Kishan-Patel-dev yes you can, Thanks for helping out!",
          "created_at": "2025-01-20T05:29:30Z"
        }
      ]
    },
    {
      "issue_number": 564,
      "title": "[Bug]: Token counts are zero for LLM not following Openai's response schema",
      "body": "### üêõ What's Going Wrong?\r\n>  When using bedrock with langchain, langchain's span attributes has all the token count as zero  and `model` attribute is set as gpt-4\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n```python\r\nimport openlit\r\nfrom langchain_aws import ChatBedrock\r\n\r\nopenlit.init(otlp_endpoint=\"http://127.0.0.1:4318\")\r\n\r\nllm = ChatBedrock(\r\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\r\n    model_kwargs=dict(temperature=0),\r\n    beta_use_converse_api=True,\r\n)\r\n\r\nresult = llm.invoke(\"Hello\")\r\n```\r\n\r\n### üéØ What Did You Expect?\r\n>  Expected the token counts to be more than 0 and model name as `anthropic.claude-3-sonnet-20240229-v1:0`\r\n\r\n### üì∏ Any Screenshots?\r\n![image](https://github.com/user-attachments/assets/927e54fa-429a-4db9-9f4d-01fbe063761a)\r\n\r\n### üíª Your Setup\r\n- OpenLIT SDK Version: 1.33.2\r\n- Deployment Method: Docker\r\n\r\n### üìù Additional Notes\r\n* Langchain's `AIMessage` has `usage_metadata` in addition to `response_metadata`, I believe it will be better to use `usage_metadata`  attribute for token statistics as structure of `response_metadata` can change based on the model\r\n* I think it is probably a good idea to set the model to None if we are not able to find the model in `response_metadata`\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "kujjwal02",
      "author_type": "User",
      "created_at": "2025-01-11T17:38:54Z",
      "updated_at": "2025-01-12T09:00:28Z",
      "closed_at": "2025-01-12T08:41:08Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/564/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/564",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/564",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:23.852836",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey, @kujjwal02 I know you volunteered for the fix but I was making a related change and gonna fix it my issue itself, That works?",
          "created_at": "2025-01-12T07:45:33Z"
        },
        {
          "author": "kujjwal02",
          "body": "Thanks @patcher9 \r\nThat works",
          "created_at": "2025-01-12T08:46:55Z"
        },
        {
          "author": "patcher9",
          "body": "Fixed in https://pypi.org/project/openlit/1.33.4/\r\n\r\nIf its still and issue, Feel free to reopen this issue ",
          "created_at": "2025-01-12T09:00:27Z"
        }
      ]
    },
    {
      "issue_number": 563,
      "title": "[Improvement]: Update Request, Exception Page UI and Trace details overlay",
      "body": "### üêõ What's Going Wrong?\r\nCurrently in the requests page we show cost, tokens as values up front. But for VectorDB and Frameworks calls, These will always be empty which creates a bit odd look when checking out traces. It can also be bit wierd when someone decides to send any random OTel Trace)\r\n\r\nThe following issues can be seen in the trace details overlay\r\n\r\n- The heading is App name, So if a trace is sent by some other method (Not OpenLIT SDK), The UI breaks.\r\n- The box showing request trace solves the usecase but is not the best experience.\r\n- The key attributes shown on top in orange ovals, They can be removed.\r\n\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  To see the bug, what steps should we follow?\r\n\r\n### üéØ What Did You Expect?\r\n>  Describe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\n>  Pictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [e.g., 0.0.1]\r\n- OpenLIT SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Docker]\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2025-01-09T12:58:21Z",
      "updated_at": "2025-01-09T12:59:32Z",
      "closed_at": "2025-01-09T12:59:32Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/563/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/563",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/563",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.072974",
      "comments": []
    },
    {
      "issue_number": 562,
      "title": "[Feat]: Allow hosting OpenLIT without needing any authentication or Sign In",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nHosting OpenLIT for sandbox accounts is tough as we always have to share username and password to people.\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\n\r\n- Allow hosting OpenLIT such that no username or password is needed. \r\n- Users should not be able to change the database config\r\n- Users should not be able to delete any prompts or secrets\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "open",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-01-09T04:36:41Z",
      "updated_at": "2025-01-09T04:36:42Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/562/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/562",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/562",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.072997",
      "comments": []
    },
    {
      "issue_number": 540,
      "title": "[Bug]:Trace logs dont respect newlines",
      "body": "When viewing trace logs the viewer does not respect newlines, which are important in complex responses. Not sure if this is a limitation in how the data is being stored/retrieved or displayed. \r\n\r\nTo reproduce, have a trace logged which contains input and/or output which have new lines in them",
      "state": "closed",
      "author": "Kyle-DeepHealth",
      "author_type": "User",
      "created_at": "2024-12-08T03:19:49Z",
      "updated_at": "2025-01-07T12:29:41Z",
      "closed_at": "2025-01-07T12:29:41Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/540/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/540",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/540",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.073005",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @Kyle-DeepHealth , Mind posting a screenshot?\r\n\r\nAlso if you do openlit.init() ithout passing the otel_endpoint, The trace should be logged to your console, If you can send that it might help we get clarity",
          "created_at": "2024-12-08T04:10:44Z"
        }
      ]
    },
    {
      "issue_number": 327,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring OpenWEB-UI ",
      "body": "### üöÄ What's the Problem?\r\nOpenLIT is not suitable for the Open WebUI web application.\r\n\r\n\r\n### üí° Your Dream Solution\r\nWhen this innovation is introduced, openLIT will definitely become an indispensable product range.\r\n\r\n\r\n### ü§î Seen anything similar?\r\nNo\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNo\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\nI will try, but I'm junior software engineer. 3 months have passed since my graduation.",
      "state": "open",
      "author": "AlbertoManuel07",
      "author_type": "User",
      "created_at": "2024-07-11T15:45:53Z",
      "updated_at": "2025-01-02T16:54:43Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/327/reactions",
        "total_count": 9,
        "+1": 6,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 3
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/327",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/327",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.238607",
      "comments": [
        {
          "author": "wolfgangsmdt",
          "body": "Any news?\r\n@AlbertoManuel07 ",
          "created_at": "2025-01-02T16:54:42Z"
        }
      ]
    },
    {
      "issue_number": 545,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring SwarmZero",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor SwarmZero based AI Agents\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nFull Stack Observability for AI Agents built using SwarmZero\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-12-09T10:35:44Z",
      "updated_at": "2025-01-02T06:37:30Z",
      "closed_at": "2025-01-02T06:37:30Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/545/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/545",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/545",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.424473",
      "comments": []
    },
    {
      "issue_number": 531,
      "title": "[Feat]: Export Metrics to JSON / TXT file",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nI am currently running a GenAI application which has openlit initialized. I want to be able to export the metrics (request duration, total tokens, etc) for different runs on this application. \r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nCurrently the dashboard shows an average of all the runs. I can see individual runs on the requests page and want to be able to export the metrics from those runs to a json/txt file. \r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nI have seen Kibana dashboards that allow users to export these outputs to a file (image attached below) and wanted to request a similar feature. \r\nI also see opentelemetry is working on a file exporter feature: https://opentelemetry.io/docs/specs/otel/protocol/file-exporter/\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n![image](https://github.com/user-attachments/assets/5143e683-bc68-4515-be7e-05c5dc1ef699)\r\n",
      "state": "open",
      "author": "ishachinniah-hds",
      "author_type": "User",
      "created_at": "2024-11-27T21:15:12Z",
      "updated_at": "2024-12-11T16:49:06Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/531/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/531",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/531",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.424507",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @ishachinniah-hds \r\n\r\n> I can see individual runs on the requests page and want to be able to export the metrics from those runs to a json/txt file.\r\n\r\nThese are actually already captured in Traces. Like each trace has data for individual request. Not sure if Kibana can view traces but we do hav",
          "created_at": "2024-11-27T22:12:15Z"
        },
        {
          "author": "ishachinniah-hds",
          "body": "Thank you! I was able to receive the metrics on Elastic ",
          "created_at": "2024-12-11T16:49:05Z"
        }
      ]
    },
    {
      "issue_number": 548,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring FireCrawl",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor FireCrawl based AI Crawling Agents\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nFull Stack Observability for AI Agents built using FireCrawl\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-12-10T05:22:32Z",
      "updated_at": "2024-12-10T05:26:29Z",
      "closed_at": "2024-12-10T05:26:29Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/548/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/548",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/548",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.667326",
      "comments": []
    },
    {
      "issue_number": 544,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring Crawl4AI",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor Crawl4AI based AI Agents\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nFull Stack Observability for AI Agents built using Crawl4AI\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-12-09T10:34:53Z",
      "updated_at": "2024-12-09T18:20:07Z",
      "closed_at": "2024-12-09T18:20:07Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/544/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/544",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/544",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.667350",
      "comments": []
    },
    {
      "issue_number": 543,
      "title": "[Feat]: Update Icons in Side nav",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nThe icons in side nav are not the best used\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nUpdated Icons that more closely resemble the page\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "open",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-12-09T10:30:57Z",
      "updated_at": "2024-12-09T10:30:58Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/543/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041",
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/543",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/543",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.667359",
      "comments": []
    },
    {
      "issue_number": 281,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring Julep AI",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor Julep AI LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to OpenAI, Have a auto instrumentation for Julep AI\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:48:49Z",
      "updated_at": "2024-12-08T04:10:02Z",
      "closed_at": "2024-12-08T04:10:02Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/281/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/281",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/281",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.667368",
      "comments": []
    },
    {
      "issue_number": 279,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring Prem AI",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor Prem AI LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to OpenAI, Have a auto instrumentation for Prem AI\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:47:29Z",
      "updated_at": "2024-12-08T04:08:36Z",
      "closed_at": "2024-12-08T04:08:36Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/279/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/279",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/279",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.667376",
      "comments": []
    },
    {
      "issue_number": 277,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring Reka AI",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor Reka AI LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to OpenAI, Have a auto instrumentation for Reka AI\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:45:58Z",
      "updated_at": "2024-12-08T04:08:29Z",
      "closed_at": "2024-12-08T04:08:29Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/277/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/277",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/277",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.667385",
      "comments": []
    },
    {
      "issue_number": 530,
      "title": "[Bug]: Newline Escape issue with Prompt Hub",
      "body": "When you paste a prompt into prompt him which has new lines in it, save, then load the prompt and attempt to edit, the app ends up replacing all \\n new line chats with \\\\n. And it will continue to do this as many times as you open and save the prompt. \r\n\r\nWorkaround for now, edit the prompt in another tool and just store it in prompt hub. ",
      "state": "closed",
      "author": "Kyle-DeepHealth",
      "author_type": "User",
      "created_at": "2024-11-27T04:23:06Z",
      "updated_at": "2024-12-05T15:22:55Z",
      "closed_at": "2024-12-05T15:20:53Z",
      "labels": [
        ":bug: Bug",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/530/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/530",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/530",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.667393",
      "comments": [
        {
          "author": "patcher9",
          "body": "@AmanAgarwal041 Can you take this up?",
          "created_at": "2024-11-27T13:33:50Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @Kyle-DeepHealth , This should be fixed in OpenLIT-1.11.6.\r\n\r\nThanks for identifying the issue and letting us know, Really Appreciate it.",
          "created_at": "2024-12-05T15:22:53Z"
        }
      ]
    },
    {
      "issue_number": 501,
      "title": "[Bug]: Error when Tracing bedrock.converse with Image Message",
      "body": "### üêõ What's Going Wrong?\r\nIn the `bedrock` instrumentor, attempting to trace a `converse` function containing an image message triggers an error. The issue occurs at this [line](https://github.com/openlit/openlit/blob/0c41338bb53e20a3a4aa956769645146f3b504c6/sdk/python/src/openlit/instrumentation/bedrock/bedrock.py#L108), where the code tries to access `item[\"text\"]` in message content. According to the [`converse`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) specification, content items do not contain a `type` key, causing a `KeyError` when the item is an image without a `text` key.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n1. Call the `converse` function with a `messages` payload that includes both text and image content:\r\n    ```python\r\n    response = client.converse(\r\n        modelId='string',\r\n        messages=[\r\n            {\r\n                'role': 'user',\r\n                'content': [\r\n                    {\"text\": \"What is in the image?\"},\r\n                    {\r\n                        'image': {\r\n                            'format': 'png',\r\n                            'source': {\r\n                                'bytes': b'bytes'\r\n                            }\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\n        ]\r\n    )\r\n    ```\r\n2. Observe that an error is raised due to the absence of a `type` key and the lack of a `text` key for the image item.\r\n\r\n![image](https://github.com/user-attachments/assets/dc416663-7559-4c9e-adbe-827dc9b8980d)\r\n\r\n### üéØ What Did You Expect?\r\nI expected the `converse` function to handle non-text content items gracefully, logging only the text messages and ignoring items in byte format (e.g., images).\r\n\r\n### üíª Your Setup\r\n- OpenLIT SDK Version: 1.29.3\r\n\r\n### üìù Additional Notes\r\nProposed solution: Update the code to log only text messages, ensuring that items in byte format are excluded. Here‚Äôs a code snippet to address the issue:\r\n\r\n```python\r\ncontent_str = \", \".join(f'text: {item[\"text\"]}' for item in content if \"text\" in item)\r\n```\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "chriskhanhtran",
      "author_type": "User",
      "created_at": "2024-11-13T18:58:28Z",
      "updated_at": "2024-12-03T15:44:33Z",
      "closed_at": "2024-12-03T05:24:49Z",
      "labels": [
        ":bug: Bug",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/501/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "chriskhanhtran"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/501",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/501",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:24.892248",
      "comments": [
        {
          "author": "patcher9",
          "body": "Ahh I see, Yeah my bad for this. \r\nI see you wanna raise a fix yourself for this, incase you need help lemme know!",
          "created_at": "2024-11-13T23:57:38Z"
        },
        {
          "author": "chriskhanhtran",
          "body": "@patcher9 thanks! I have opened PR #532 to handle this bug as well as #502.",
          "created_at": "2024-11-27T22:33:48Z"
        },
        {
          "author": "patcher9",
          "body": "Thanks @chriskhanhtran for the PR and the fix!\r\n\r\n",
          "created_at": "2024-12-03T15:44:31Z"
        }
      ]
    },
    {
      "issue_number": 502,
      "title": "[Feat]: Add Option to Disable Logging of Image in Instrumentors",
      "body": "### üöÄ What's the Problem?\r\nCurrently, the instrumentors log base64-encoded image URLs to OpenTelemetry (OTel).\r\n\r\nExample messages:\r\n```\r\nmessages = {\r\n    \"role\": \"user\",\r\n    \"content\": [\r\n        {\"type\": \"text\", \"text\": \"Describe the image for me\"},\r\n        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\r\n    ]\r\n}\r\n```\r\n\r\n[Code](https://github.com/openlit/openlit/blob/0c41338bb53e20a3a4aa956769645146f3b504c6/sdk/python/src/openlit/instrumentation/openai/openai.py#L85):\r\n```\r\ncontent_str = \", \".join(\r\n    # pylint: disable=line-too-long\r\n    f'{item[\"type\"]}: {item[\"text\"] if \"text\" in item else item[\"image_url\"]}'\r\n    if \"type\" in item else f'text: {item[\"text\"]}'\r\n    for item in content\r\n)\r\n```\r\n\r\n\r\nThis creates two issues:\r\n\r\n1. If the image is large, the OTel exporter can fail with a 413 (Payload Too Large) error.\r\n2. Logging image bytes or base64 data doesn‚Äôt provide meaningful insights and can add unnecessary load.\r\n\r\n### üí° Your Dream Solution\r\nIntroduce a configurable flag to disable logging of images, or modify the behavior to log only text messages by default. This would prevent large binary data from being sent to OTel and streamline the logging process.\r\n\r\n#### Proposed Code Change:\r\n- Add a flag to control content logging behavior.\r\n- Or log only text messages:\r\n\r\nOriginal ([link to code](https://github.com/openlit/openlit/blob/0c41338bb53e20a3a4aa956769645146f3b504c6/sdk/python/src/openlit/instrumentation/openai/openai.py#L87)):\r\n```python\r\nif isinstance(content, list):\r\n    content_str = \", \".join(\r\n        f'{item[\"type\"]}: {item[\"text\"] if \"text\" in item else item[\"image_url\"]}'\r\n        if \"type\" in item else f'text: {item[\"text\"]}'\r\n        for item in content\r\n    )\r\n    formatted_messages.append(f\"{role}: {content_str}\")\r\nelse:\r\n    formatted_messages.append(f\"{role}: {content}\")\r\n```\r\n\r\nProposed Solution:\r\n```python\r\nif isinstance(content, list):\r\n    content_str = \", \".join(\r\n        f'text: {item[\"text\"]}' for item in content if \"text\" in item\r\n    )\r\n    formatted_messages.append(f\"{role}: {content_str}\")\r\nelse:\r\n    formatted_messages.append(f\"{role}: {content}\")\r\n```\r\n\r\nThis update would ensure that only text content is logged, preventing large, unnecessary data from being exported.\r\n",
      "state": "closed",
      "author": "chriskhanhtran-verisk",
      "author_type": "User",
      "created_at": "2024-11-13T19:24:13Z",
      "updated_at": "2024-12-03T15:44:08Z",
      "closed_at": "2024-12-03T05:27:12Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/502/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "chriskhanhtran-verisk"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/502",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/502",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:25.116287",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @chriskhanhtran-verisk \r\nYeah makes sense toa dd this, Since you already know the change, want to raise the PR aswell? (If not I can surely do it too :) )",
          "created_at": "2024-11-13T23:56:02Z"
        },
        {
          "author": "patcher9",
          "body": "Thanks @chriskhanhtran-verisk for the PR and the fix!",
          "created_at": "2024-12-03T15:44:06Z"
        }
      ]
    },
    {
      "issue_number": 522,
      "title": "Support for OCI GenAI.",
      "body": "### üöÄ What's the Problem?\r\n<!-OCI GenAI does not seem to work with openlit. Don't see any metrics coming in .\r\n\r\n\r\nTried with langchain  for simple chat with OCIGenAI i am getting some metrics like request and duration. Other details are coming wrong since it treats it as a gpt-4 model . The model_id is passed as an argument. You can find simple example here.\r\nhttps://python.langchain.com/docs/integrations/chat/oci_generative_ai/\r\n",
      "state": "open",
      "author": "karthicgit",
      "author_type": "User",
      "created_at": "2024-11-23T07:15:38Z",
      "updated_at": "2024-11-23T08:17:06Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/522/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/522",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/522",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:25.361175",
      "comments": []
    },
    {
      "issue_number": 515,
      "title": "[Bug]: Incorrect Token Usage representation in the Area Chart",
      "body": "### üêõ What's Going Wrong?\r\n>  There are a couple of issues with the Token Usage graph:\r\n- Value on Y axis does not correspond to the Max value (Total Tokens). This is because all 3 token usage values(prompt tokens, completion tokens and Total tokens) are stacked. Consequently Y axis displays a sum of all 3 stacked values. Ideally only 2 token usage values should be stacked and total tokens will be an implied value. in my screenshot , Y axis value is 56189. This value is determined by summing up Total tokens(28087), prompt tokens(18326) and completion tokens(9761) + 15.\r\n-  The token usage is not stacked in the proper order. Lowest value should be dynamically placed first. In my case Completion tokens should be first, followed by prompt tokens. Sum of these should imply/indicate Total tokens.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n- Instrument any LLM app with OpenLIT\r\n- Check the token usage graph\r\n\r\n### üéØ What Did You Expect?\r\n> For issue 1 : Y axis value should be total tokens + 15 instead of total tokens + prompt tokens + completion tokens + 15\r\n> For issue 2: Lowest value (completion tokens) is placed at the bottom followed by the next higher value (prompt tokens). Stacking order should be dynamically determined\r\n\r\n### üì∏ Any Screenshots?\r\n![image](https://github.com/user-attachments/assets/c699141f-7cb8-4ebb-a096-f2a82bff5ae9)\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: 1.11.2\r\n- OpenLIT SDK Version: 1.24.1\r\n- Deployment Method: [Docker]\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "Nidhin117",
      "author_type": "User",
      "created_at": "2024-11-20T05:39:38Z",
      "updated_at": "2024-11-22T06:54:01Z",
      "closed_at": "2024-11-22T00:32:29Z",
      "labels": [
        ":bug: Bug",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/515/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/515",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/515",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:25.361203",
      "comments": [
        {
          "author": "patcher9",
          "body": "Thanks @Nidhin117 for finding this, Yeah @AmanAgarwal041 will try to get a fix in for this.",
          "created_at": "2024-11-21T02:47:35Z"
        },
        {
          "author": "AmanAgarwal041",
          "body": "Thanks @Nidhin117 for pointing this out. \r\nArea charts work pretty this way : \r\n\r\n> let's say if there are prompt token 4 , completion tokens 2 and total tokens 6\r\n> then prompt tokens area points will start from 0-4, completion tokens will be from 4-6, and total tokens will be from 6-12.\r\n> which i",
          "created_at": "2024-11-21T19:16:15Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @Nidhin117 The fix is now available in OpenLIT 1.11.3",
          "created_at": "2024-11-22T00:57:59Z"
        },
        {
          "author": "Nidhin117",
          "body": "Hey @AmanAgarwal041  and @patcher9 , thanks for fixing this. It works as expected now.",
          "created_at": "2024-11-22T06:54:00Z"
        }
      ]
    },
    {
      "issue_number": 283,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring DSPy",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor DSPy LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to OpenAI, Have a auto instrumentation for DSPy\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-13T05:53:01Z",
      "updated_at": "2024-11-22T00:58:10Z",
      "closed_at": "2024-11-22T00:58:10Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        "openlit-python",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/283/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/283",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/283",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:25.611174",
      "comments": []
    },
    {
      "issue_number": 474,
      "title": "[Bug]: Unexpected API Tracing in OpenTelemetry with Grafana and Tempo",
      "body": "### üêõ What's Going Wrong?\r\n>  I'm using OpenTelemetry's auto-instrumentation with Grafana and Tempo to trace my application. However, when I run OpenLit, it starts logging traces for all API requests going to Grafana, rather than focusing solely on requests related to the LLM, vector database, or GPU.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  Start the application with OpenTelemetry auto-instrumentation configured.\r\nRun OpenLit.\r\n\r\n### üéØ What Did You Expect?\r\n>  I expected it to log only the traces from the file where I initialized OpenLit, focusing on specific components like the LLM, vector database, or GPU‚Äîrather than logging every API request going to Grafana.\r\n\r\n### üìù Additional Notes\r\n>  If you have any sample configurations or recommendations to limit tracing data to the specific components (e.g., only LLM, vector database, or GPU-related requests), it would be greatly appreciated.",
      "state": "closed",
      "author": "krishnarastogi",
      "author_type": "User",
      "created_at": "2024-10-27T08:20:46Z",
      "updated_at": "2024-11-21T02:48:26Z",
      "closed_at": "2024-11-21T02:48:26Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/474/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/474",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/474",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:25.611205",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @krishnarastogi \r\nOpenTelemetry auto-instrumentation traces all API requests\r\nOpenLIT can only trace LLM, VectorDB and GPU requests\r\n\r\nSo if you just keep OpenLIT and remove OpenTelemetry auto-instrumentation, You should just see logging of LLM, VectorDB and GPU requests in Tempo\r\n\r\nHappy to hel",
          "created_at": "2024-10-27T14:43:31Z"
        },
        {
          "author": "krishnarastogi",
          "body": "Hey @patcher9 \r\n\r\nThanks so much for your reply!\r\n\r\nI‚Äôm currently using Django/wsgi auto-instrumentation for logging, metrics, and traces specifically for the API requests in my backend application. Additionally, I‚Äôm exploring OpenLIT to log LLM observability separately, excluding the application‚Äôs ",
          "created_at": "2024-10-27T14:57:32Z"
        },
        {
          "author": "patcher9",
          "body": "If you want to filter LLM Observability traces in tempo, OpenLIT traces have the `telemetry.sdk.name =\"openlit\"` in resource attributes as well as the span attributes. So you can filter based on that\r\n\r\nThe dashboard available in this doc already has the filter built in for traces\r\nhttps://docs.open",
          "created_at": "2024-10-28T13:28:01Z"
        }
      ]
    },
    {
      "issue_number": 513,
      "title": "[Bug]: Hugging Face Transformers models tokens not displayed",
      "body": "### üêõ What's Going Wrong?\r\n> I am encountering an issue where OpenLIT is not able to pick up and display token metrics when using Hugging Face transformers models. Specifically, I am using the meta-llama/Llama-3.2-3B-Instruct model from Hugging Face, and when passing this model through a transformers pipeline and into a HuggingFacePipeline, followed by a RAG chain, OpenLIT does not show token usage information on the dashboard. Instead, it only displays the average request duration.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  I am using the \"meta-llama/Llama-3.2-3B-Instruct\" model from hugging face. I then pass the model into a transformers pipeline and HuggingFacePipeline before it is passed into a RAG chain. With OTEL_LOG_LEVEL=debug, I see the following error: Error in trace creation: expected string or buffer. Error in trace creation: 'HuggingFacePipeline' object has no attribute 'model'. Additionally the openlit dashboard displays average request duration, but no token usage information.\r\n\r\n\r\n### üéØ What Did You Expect?\r\n>  I expected OpenLIT to display token usage metrics (e.g., tokens per request, tokens processed) alongside the average request duration, since OpenLIT is integrated with Hugging Face Transformers and supports token tracking.\r\n\r\n### üì∏ Any Screenshots?\r\n>  \r\n<img width=\"718\" alt=\"Screenshot 2024-11-18 at 4 51 30‚ÄØPM\" src=\"https://github.com/user-attachments/assets/7fdfae69-38ee-4a7f-8158-d578a730a6b8\">\r\n\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: 1.29.3\r\n- Hugging Face Model: meta-llama/Llama-3.2-3B-Instruct\r\n",
      "state": "closed",
      "author": "ishachinniah-hds",
      "author_type": "User",
      "created_at": "2024-11-18T23:01:51Z",
      "updated_at": "2024-11-19T15:29:14Z",
      "closed_at": "2024-11-19T15:26:41Z",
      "labels": [
        ":bug: Bug",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/513/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/513",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/513",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:25.825462",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @ishachinniah-hds \r\n\r\nWorking on a fix in this PR - https://github.com/openlit/openlit/pull/514",
          "created_at": "2024-11-19T15:09:00Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @ishachinniah-hds \r\n\r\nI have released the fix in the version [1.30.5](https://pypi.org/project/openlit/1.30.5/). Thanks for finding and reporting the issue. \r\n\r\nIncase you still have issues with this, feel free to reopen this (or a new) issue.",
          "created_at": "2024-11-19T15:29:11Z"
        }
      ]
    },
    {
      "issue_number": 486,
      "title": "[Feat]: Events tracking for understanding the usage among users",
      "body": "### üöÄ What's the Problem?\r\n- Should be able to track events for different user interactions\r\n- The users should be able to disable tracking via env variable `TELEMETRY_ENABLED=false`\r\n\r\n\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-11-06T18:00:36Z",
      "updated_at": "2024-11-12T20:24:25Z",
      "closed_at": "2024-11-12T20:24:25Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/486/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/486",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/486",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:26.037142",
      "comments": []
    },
    {
      "issue_number": 391,
      "title": "[Bug]:TypeError: 'generator' object does not support the context manager protocol",
      "body": "### üêõ What's Going Wrong?\r\nI'm encountering an error when attempting to use agent_executor.invoke() in my application with OpenLIT. The error occurs during the invocation of the agent and appears to be related to a generator object not supporting the context manager protocol.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nAdd OpenLIT to the application via:\r\n```\r\nopenlit.init(\r\n    otlp_endpoint=\"http://127.0.0.1:4318\",\r\n)\r\n```\r\nSet up the AgentExecutor:\r\n```\r\nagent_executor = AgentExecutor(agent=agent, tools=functions, verbose=True)\r\n```\r\nAttempt to invoke the agent with the following code:\r\n```\r\nagent_output = agent_executor.invoke(\r\n    {\r\n        \"input\": message,\r\n        \"chat_history\": chat_history,\r\n    }\r\n)\r\n```\r\nThe error is raised on this line during execution.\r\n\r\n### üéØ What Did You Expect?\r\nI expected the agent_executor.invoke() function to run without errors, properly invoking the agent and returning the expected output.\r\n\r\n### üì∏ Any Screenshots?\r\nUnfortunately, I don't have any relevant screenshots at the moment, but I have provided the traceback below.\r\n\r\n### üìù Additional Notes\r\nThe full traceback of the error is as follows:\r\n\r\n```File \"/home/mikef0x/slots.gd/app/utils/llm.py\", line 78, in llm_response_openai\r\n    agent_output = agent_executor.invoke(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\r\n    raise e\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain/agents/agent.py\", line 1608, in _call\r\n    next_step_output = self._take_next_step(\r\n                       ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain/agents/agent.py\", line 1314, in _take_next_step\r\n    [\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain/agents/agent.py\", line 1342, in _iter_next_step\r\n    output = self.agent.plan(\r\n             ^^^^^^^^^^^^^^^^\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain/agents/agent.py\", line 576, in plan\r\n    for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3262, in stream\r\n    yield from self.transform(iter([input]), config, **kwargs)\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3249, in transform\r\n    yield from self._transform_stream_with_config(\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2054, in _transform_stream_with_config\r\n    chunk: Output = context.run(next, iterator)  # type: ignore\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3211, in _transform\r\n    for output in final_pipeline:\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1272, in transform\r\n    for ichunk in input:\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5301, in transform\r\n    yield from self.bound.transform(\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1290, in transform\r\n    yield from self.stream(final, config, **kwargs)\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 425, in stream\r\n    raise e\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 405, in stream\r\n    for chunk in self._stream(messages, stop=stop, **kwargs):\r\n  File \"/home/mikef0x/slots.gd/env/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 559, in _stream\r\n    with response:\r\nTypeError: 'generator' object does not support the context manager protocol```",
      "state": "closed",
      "author": "mike2505",
      "author_type": "User",
      "created_at": "2024-08-27T13:08:27Z",
      "updated_at": "2024-11-12T19:36:03Z",
      "closed_at": "2024-11-12T19:36:02Z",
      "labels": [
        ":bug: Bug",
        "openlit-python",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/391/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/391",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/391",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:26.037165",
      "comments": [
        {
          "author": "masnec",
          "body": "Run into the same issue, it's caused by the context manager change in langchain - https://github.com/langchain-ai/langchain/pull/18013 . May need a fix in https://github.com/openlit/openlit/blob/main/sdk/python/src/openlit/instrumentation/openai/async_openai.py",
          "created_at": "2024-08-29T03:53:14Z"
        },
        {
          "author": "patcher9",
          "body": "Ack\r\nI am looking for a fix on this as directly openai works but calling an agent in this way fails. Ill keep you posted on this\r\n\r\nAny ideas @masnec why you think async openai is being used here?",
          "created_at": "2024-08-29T14:00:58Z"
        },
        {
          "author": "masnec",
          "body": "@patcher9 the issue posted here doesn't use async openai. But I think we need a fix for both sync and async flows since https://github.com/langchain-ai/langchain/pull/18013 updated both code paths. ",
          "created_at": "2024-09-02T01:54:48Z"
        },
        {
          "author": "patcher9",
          "body": "Alright thanks, This might mean we would need a different way to capture the streaming response. Thanks for pointing it out",
          "created_at": "2024-09-03T11:09:45Z"
        },
        {
          "author": "zacchenit",
          "body": "Could you please provide any updates on this issue?",
          "created_at": "2024-10-17T08:00:06Z"
        }
      ]
    },
    {
      "issue_number": 485,
      "title": "[Feat]: Enable on OpenLit UI elements in dashboards or screens so they can be embedded in other projects.",
      "body": "### üöÄ What's the Problem?\r\nI want to embed and customise, otel dashboard screens in my frontend project, but I am not able to do so or find any documentation. Being an open source project this should not be a hinderance. \r\n\r\n\r\n### üí° Your Dream Solution\r\nI should be able to pick and choose screens or custom components of the screens and use them as I wish.",
      "state": "open",
      "author": "S1LV3RJ1NX",
      "author_type": "User",
      "created_at": "2024-11-05T10:49:41Z",
      "updated_at": "2024-11-05T11:12:57Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/485/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/485",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/485",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:26.262943",
      "comments": []
    },
    {
      "issue_number": 472,
      "title": "[Feat]: Dataset Generation Based on LLM Events",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nBased on the Manual feedback from the user, The prompt and response should be captured into a dataset\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nAutomatically build a dataset based on human feedback on LLM event\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "open",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-10-25T18:47:15Z",
      "updated_at": "2024-10-25T18:47:15Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/472/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/472",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/472",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.188248",
      "comments": []
    },
    {
      "issue_number": 465,
      "title": "[Feat]: Vault to manage your secrets",
      "body": "### üöÄ What's the Problem?\r\nTool to manage secrets in OpenLIT UI\r\n\r\n\r\n### üí° Your Dream Solution\r\n- Allow users to store secrets directly in OpenLIT UI and they can then be fetched using the OpenLIT SDK directly in code.\r\n- Should be allowed to set the secrets as environment variables for the application through sdk\r\n\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-10-22T22:06:13Z",
      "updated_at": "2024-10-24T19:04:44Z",
      "closed_at": "2024-10-24T19:04:44Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/465/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/465",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/465",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.188273",
      "comments": []
    },
    {
      "issue_number": 456,
      "title": "[Feat]: GPU Collector does not works properly",
      "body": "### üöÄ What's the Problem?\r\n GPU Collector does not works properly if **Openlit-UI** runs via **_Docker-compose_** and GPU Collector run separately (outside of docker-compose.yml) \r\n\r\n\r\n### üí° Your Dream Solution\r\nWe need to update the README to make it clearer.\r\n\r\nThe user must use the host's IP address (either public or private):\r\nExample:\r\n```\r\nOTEL_EXPORTER_OTLP_ENDPOINT=\"http://192.168.10.15:4318\"\r\n```\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "pitonic",
      "author_type": "User",
      "created_at": "2024-10-14T00:21:59Z",
      "updated_at": "2024-10-15T13:29:31Z",
      "closed_at": "2024-10-15T13:29:31Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/456/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/456",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/456",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.188282",
      "comments": []
    },
    {
      "issue_number": 441,
      "title": "[Feat]: Prompt Hub",
      "body": "### üöÄ What's the Problem?\r\nTool to manage Prompts in OpenLIT UI\r\n\r\n\r\n### üí° Your Dream Solution\r\n\r\n1. Allow users to store Prompts directly in OpenLIT UI or the OpenLIT SDK and they can then be received/fetched using the OpenLIT SDK directly in code.\r\n2. Should be allowed to version their prompt\r\n3. In the Prompt Hub, When the user is creating a prompt, We should add a button to OpenGround and directly take the added prompt and allow the user to test it against different model.\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-10-03T20:10:19Z",
      "updated_at": "2024-10-15T13:13:58Z",
      "closed_at": "2024-10-15T13:13:58Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js",
        "client"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/441/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/441",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/441",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.188288",
      "comments": []
    },
    {
      "issue_number": 139,
      "title": "[Feat]: Add Monitoring Integration for LangChain framework",
      "body": "### üöÄ What's the Problem?\r\nCurrently we support LLM Monitoring integration when directly using official SDKs. LangChain is a popular choice when building RAG based applications but Doku doesn't support monitoring it currently\r\n\r\n### üí° Your Dream Solution\r\nSimilar setup as OpenAI Monitoring, Build a LLM monitoring integration for LangChain\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-23T18:22:32Z",
      "updated_at": "2024-10-14T03:55:05Z",
      "closed_at": "2024-04-18T19:56:04Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/139/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041",
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/139",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/139",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.188296",
      "comments": [
        {
          "author": "codefromthecrypt",
          "body": "I see this tagged for openlit-js, but looks like it isn't implemented yet, right? should we re-open this, or add a new issue?",
          "created_at": "2024-10-14T03:55:04Z"
        }
      ]
    },
    {
      "issue_number": 402,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Cohere in typescript sdk",
      "body": "### üöÄ What's the Problem?\r\nCohere support is not present\r\n\r\n\r\n### üí° Your Dream Solution\r\nAfter the integration is completed, I would be able to monitor application using Cohere AI\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-09-03T20:06:27Z",
      "updated_at": "2024-10-08T16:29:53Z",
      "closed_at": "2024-10-08T16:29:52Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        "openlit-js",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/402/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/402",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/402",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.403922",
      "comments": []
    },
    {
      "issue_number": 372,
      "title": "[Bug]: the \"Avg Utilization Percentage (%)\" and \"Fan Speed (0-100)\" fields do not display any output values",
      "body": "### üêõ What's Going Wrong?\r\n>  In the OpenLIT Dashboard GPU monitoring screen, the \"Avg Utilization Percentage (%)\" and \"Fan Speed (0-100)\" fields do not display any output values.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n\r\n1. Deploy the provided setup on a GCP instance and deploy OpenLIT by docker-compose.\r\n2. Deploy OTEL-GUP-COLLECTOR\r\n3. Write OpenLIT Python SDK code: `openlit.init(\r\n  otlp_endpoint=\"YOUR_OTEL_ENDPOINT\",\r\n  collect_gpu_stats=True \r\n)`\r\n4. Navigate to the OpenLIT Dashboard GPU monitoring screen.\r\n5. Observe the \"Avg Utilization Percentage (%)\" and \"Fan Speed (0-100)\" fields.\r\n\r\n### üéØ What Did You Expect?\r\n>  I expected to see the \"Avg Utilization Percentage (%)\" and \"Fan Speed (0-100)\" fields populated with real-time GPU data.\r\n\r\n### üì∏ Any Screenshots?\r\n>  \r\n![Screenshot_2024-08-16-10-23-17-051_com android chrome-edit](https://github.com/user-attachments/assets/d32a2f23-407b-460b-928b-62984b8475fb)\r\n\r\n\r\n### üíª Your Setup\r\n\r\n- GCP: Ubuntu 20.04.6 LTS 5.15.0-1066-gcp\r\n- Docker Compose Version: v2.20.2\r\n- ClickHouse Version: clickhouse/clickhouse-server:24.2.2\r\n- OpenLIT Container: ghcr.io/openlit/openlit\r\n- OpenTelemetry Collector: otel/opentelemetry-collector-contrib:0.94.0\r\n- GPU: Nvidia Tesla T4\r\n- Driver Version: 535.183.06\r\n- CUDA Version: 12.2\r\n\r\n### üìù Additional Notes\r\nIt appears that the metrics for \"Avg Utilization Percentage (%)\" and \"Fan Speed (0-100)\" are not being collected or displayed correctly. This could be related to the GPU instrumentation or data collection process.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "ian2014",
      "author_type": "User",
      "created_at": "2024-08-16T02:47:13Z",
      "updated_at": "2024-10-07T19:34:54Z",
      "closed_at": "2024-10-07T19:34:54Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/372/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041",
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/372",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/372",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.403943",
      "comments": [
        {
          "author": "patcher9",
          "body": "@ian2014 to confirm you wanna contribute on a fix?\r\n\r\nAlso we definitely do send the metric but can check the value for it if its actually 0. which is why we see 0 here",
          "created_at": "2024-08-16T08:56:50Z"
        },
        {
          "author": "ian2014",
          "body": "I misunderstood the question \"Want to Help Make It Happen?\"‚ÄîI only meant to indicate that I can reproduce the issue.\r\n\r\nApologies, as I am a rookie at GPU monitoring. It seems that the GPU utilization is indeed showing as 0. Is there a way to get it to display a valid value? Am I missing something? ",
          "created_at": "2024-08-16T14:19:57Z"
        },
        {
          "author": "patcher9",
          "body": "No worries @ian2014 \r\n\r\nI just tried it out and we are picking up the GPU Utilization metric and fan speed (That removes the possibility that we were not collecting it)\r\n\r\nThough in my case you can see the Utilization is actually 0% and Fan Speed too. \r\n\r\n\r\n<details>\r\n<summary>OTel Metric logged to ",
          "created_at": "2024-08-18T07:51:13Z"
        },
        {
          "author": "patcher9",
          "body": "Are you using Google Cloud for your GPUs, If yes you should be able to see the Utilization in GCP aswell, Have you tried comparing to it and seeing if it shows 0? (Completely possible that we have an issue too so glad you raised the issue)\r\n\r\nIt showed 0 for me for Utilization as in the screenshot b",
          "created_at": "2024-08-18T07:54:26Z"
        },
        {
          "author": "ian2014",
          "body": "Thank you @patcher9  for your reply.\r\n\r\nI‚Äôve confirmed that monitoring fan speed isn't possible in the GCP environment. However, using the `nvidia-smi` command, I was able to retrieve the GPU utilization(%) value. Below are the command outputs:\r\n\r\n**First command:**\r\n```\r\nnvidia-smi --query-gpu=time",
          "created_at": "2024-08-18T14:01:10Z"
        }
      ]
    },
    {
      "issue_number": 375,
      "title": "[Feat]: Add version to the OpenLIT UI ",
      "body": "### üöÄ What's the Problem?\r\nAdd an element to show current OpenLIT UI version installed\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nHave a icon that can show the OpenLIT UI version\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\nGrafana does a pretty good job in showing the version details\r\n<img width=\"242\" alt=\"Screenshot 2024-08-18 at 1 44 28‚ÄØPM\" src=\"https://github.com/user-attachments/assets/41b2a048-5342-44b0-b512-8ff444766109\">\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-08-17T02:49:19Z",
      "updated_at": "2024-09-30T15:00:18Z",
      "closed_at": "2024-09-30T15:00:18Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/375/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/375",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/375",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.631495",
      "comments": []
    },
    {
      "issue_number": 374,
      "title": "[Feat]: Enable real time updates on the UI",
      "body": "### üöÄ What's the Problem?\nRight now when there is an entry for the traces in the clickhouse db, the UI doesn't reflect it instantly,  hence the user has to refresh the page to reflect the changes. Enable real time updates on the UI\n\n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\nThe UI polls Clickhouse at 15s, 30s, 60s which can be set by the user. Default poll interval should be 30s\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\nNA\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\nNA\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-08-17T02:37:30Z",
      "updated_at": "2024-09-26T12:15:34Z",
      "closed_at": "2024-09-26T12:15:34Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/374/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/374",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/374",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.631522",
      "comments": []
    },
    {
      "issue_number": 425,
      "title": "[Feat]:Track output tokens with cost associated in OpenLit UI for chat completions",
      "body": "### üöÄ What's the Problem?\r\nI am not sure if output tokens are being considered when calculating the price in OpenLit UI.\r\n\r\n\r\n### üí° Your Dream Solution\r\nSee breakup of Input and Output token for chat completions along with cost associated.\r\n\r\n",
      "state": "closed",
      "author": "pranavchaturved",
      "author_type": "User",
      "created_at": "2024-09-17T12:32:36Z",
      "updated_at": "2024-09-17T12:46:05Z",
      "closed_at": "2024-09-17T12:45:56Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/425/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/425",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/425",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.631533",
      "comments": [
        {
          "author": "pranavchaturved",
          "body": "I see the separation of input and output tokens is already present. My bad. Closing this. ",
          "created_at": "2024-09-17T12:45:56Z"
        }
      ]
    },
    {
      "issue_number": 416,
      "title": "[Feat]: Possibility to group metrics by a tag",
      "body": "### üöÄ What's the Problem?\r\nHi!\r\n\r\nThe current metrics sent by openLit for the gen_ai component do not allow grouping by tag. It could be a good idea to introduce custom tags to enable monitoring by group. For example, my application manages many clients simultaneously, and all the cost monitoring metrics allow me to see the total cost spent by my application, but only per model (as my application uses several LLM models).\r\n\r\nIt would be very useful to be able to define additional custom tags for the metrics, configurable via sdk. \r\nIn my case, I would add the client ID, so I could monitor costs for individual user (and total cost as well).\r\n",
      "state": "open",
      "author": "ercucchiaio",
      "author_type": "User",
      "created_at": "2024-09-10T13:54:05Z",
      "updated_at": "2024-09-14T16:40:44Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":heavy_plus_sign:  help wanted",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/416/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/416",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/416",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.844054",
      "comments": []
    },
    {
      "issue_number": 373,
      "title": "[Feat]: Display request times of LLM and Vector DB requests in local timezone",
      "body": "### üöÄ What's the Problem?\r\nOn the OpenLIT Requests tab, all LLM calls are displayed in GMT, ignoring the browser's or system's default time zone. This causes issues when filtering requests by predefined or custom time ranges and can be confusing for first-time users of the dashboard.\r\n\r\n### üí° Your Dream Solution\r\nDisplay the list of requests in local time zone or make this a configurable parameter.\r\n\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "Nidhin117",
      "author_type": "User",
      "created_at": "2024-08-16T12:18:34Z",
      "updated_at": "2024-09-06T20:56:59Z",
      "closed_at": "2024-09-06T20:56:59Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/373/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/373",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/373",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.844080",
      "comments": []
    },
    {
      "issue_number": 253,
      "title": "[Feat]: Add Blogsite on `openlit.io` website",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCurrently OpenLIT mainatiners and contribute can share their work with OpenLIT blog.\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nCreate a blogsite for OpenLIT along with a contribution guideline for Blogs for contributors\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nhttps://mindsdb.com/blog and https://mintlify.com/blog are pretty good examples of a nice Blogsite\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-26T05:45:57Z",
      "updated_at": "2024-09-05T09:26:05Z",
      "closed_at": "2024-09-05T09:26:05Z",
      "labels": [
        ":rocket: Feature",
        "Blog"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/253/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/253",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/253",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:28.844088",
      "comments": [
        {
          "author": "patcher9",
          "body": "@AmanAgarwal041 This is partially complete or still in works?",
          "created_at": "2024-07-30T05:00:29Z"
        }
      ]
    },
    {
      "issue_number": 395,
      "title": "[Feat]:Add Monitoring for Google Gemini AI Integration ",
      "body": "üöÄ What's the Problem?\r\nCurrently, there is no feature in Openlit for monitoring API keys specifically related to Google Gemini AI (Google AI Studio). This lack of monitoring can lead to issues such as unauthorized token capture, unexpected costs, and overall lack of security around API usage.\r\n\r\nüí° Your Dream Solution\r\nI propose adding a robust API key monitoring feature within Openlit. \r\nü§î Seen anything similar?\r\nI've seen similar API monitoring tools that provide real-time tracking and alerting, but none specifically integrated with Google Gemini AI within a single platform like Openlit. Integrating such a feature would greatly enhance security and cost management for users relying on these AI services.\r\n\r\n",
      "state": "closed",
      "author": "viipuljain",
      "author_type": "User",
      "created_at": "2024-08-29T06:57:33Z",
      "updated_at": "2024-09-04T10:02:49Z",
      "closed_at": "2024-09-04T10:02:49Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/395/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/395",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/395",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:29.064863",
      "comments": []
    },
    {
      "issue_number": 252,
      "title": "[Feat]: Add JS library version of the OpenLIT Python SDK for LLM Observability based on OpenTelemetry",
      "body": "### üöÄ What's the Problem?\n<!-- Tell us what's not working right or what's getting in your way -->\nOpenLIT doesnt yet have a JS/TS library for LLM Observability. \n\n### üí° Your Dream Solution\n<!-- Tell us what you wish for, with as much detail as you like -->\nSimilar to the OpenLIT Python SDK, Users need a library for OTel based monitoring of LLM Observability\nIt should support \n- OpenAI\n- Anthropic\n- VertexAI\n- Google AI Platform\n\n### ü§î Seen anything similar?\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\nNA\n\n### üñºÔ∏è Pictures or Drawings\n<!-- Share any picture or drawing that shows your idea if you have -->\nNA\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-26T05:37:29Z",
      "updated_at": "2024-08-27T22:18:10Z",
      "closed_at": "2024-08-27T22:18:09Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        "openlit-js",
        ":raised_hand: Up for Grabs",
        "javascript"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/252/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/252",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/252",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:29.064902",
      "comments": []
    },
    {
      "issue_number": 386,
      "title": "[Feat]: Support for langchain_community.chat_models",
      "body": "### üöÄ What's the Problem?\r\nOpenlit doesn't provide autoinstrumentation for chat models from `langchain_community.chat_models`. A similar problem related to `langchain_community.llms` was successfully addressed by @patcher9 in the https://github.com/openlit/openlit/issues/301.\r\n\r\nThe following code gets traces generated:\r\n\r\n```python\r\nfrom langchain_community.llms import Ollama\r\nimport openlit\r\n\r\nopenlit.init( otlp_endpoint=\"http://127.0.0.1:4318\" )\r\n\r\nllm = Ollama(model=\"aya:8b\", base_url=\"http://localhost:11434\" )\r\nresponse = llm.invoke(\"Tell me a joke\")\r\nprint(response )\r\n```\r\n\r\nWhilst this one doesn't\r\n\r\n\r\n```python\r\nfrom langchain_community.chat_models import ChatOllama\r\nimport openlit\r\n\r\nopenlit.init( otlp_endpoint=\"http://127.0.0.1:4318\" )\r\n\r\nllm = ChatOllama(model=\"aya:8b\", base_url=\"http://localhost:11434\" )\r\nresponse = llm.invoke(\"Tell me a joke\")\r\nprint(response )\r\n```\r\n\r\n### üí° Your Dream Solution\r\n`chat_models` module should be supported \r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "jradikk",
      "author_type": "User",
      "created_at": "2024-08-23T14:38:07Z",
      "updated_at": "2024-08-26T13:55:24Z",
      "closed_at": "2024-08-26T12:23:18Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/386/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/386",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/386",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:29.064909",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey - What langchain/langchain-community version is this? I somehow get an error \r\n\r\n```ImportError: cannot import name 'Ollama' from 'langchain_community.chat_models' (/Users/ishanjain/Library/Python/3.9/lib/python/site-packages/langchain_community/chat_models/__init__.py)```",
          "created_at": "2024-08-24T04:56:43Z"
        },
        {
          "author": "jradikk",
          "body": "That's my bad. Updated the description. It has to be `from langchain_community.chat_models import ChatOllama`. The `langchain_community` module version i use is 0.2.7",
          "created_at": "2024-08-26T06:06:31Z"
        },
        {
          "author": "patcher9",
          "body": "Okay I think I know what to do, Ill try to create a new release with this added by Tommorow",
          "created_at": "2024-08-26T11:27:39Z"
        },
        {
          "author": "patcher9",
          "body": "I have it almost ready, Will be creating a new release for the SDK `1.19.0`. \r\n\r\n```python\r\nfrom langchain_community.chat_models import ChatOllama\r\nimport openlit\r\n\r\nopenlit.init( otlp_endpoint=\"http://127.0.0.1:4318\" )\r\n\r\nllm = ChatOllama(model=\"gemma:2b\", base_url=\"http://localhost:11434\" )\r\nrespo",
          "created_at": "2024-08-26T12:17:02Z"
        },
        {
          "author": "patcher9",
          "body": "Made a new release `1.19.0` -> https://pypi.org/project/openlit/1.19.0/\r\n\r\nFeel free to join our [**community Slack**](https://join.slack.com/t/openlit/shared_invite/zt-2etnfttwg-TjP_7BZXfYg84oAukY8QRQ) too incase you need any further discussions or help",
          "created_at": "2024-08-26T12:23:12Z"
        }
      ]
    },
    {
      "issue_number": 370,
      "title": "[Bug]: LangChain test fails because of LangSmith API kye issue",
      "body": "### üêõ What's Going Wrong?\r\n>  Explain the bug. Tell us what's happening that shouldn't be.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  To see the bug, what steps should we follow?\r\n\r\n### üéØ What Did You Expect?\r\n>  Describe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\n>  Pictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [e.g., 0.0.1]\r\n- OpenLIT SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Docker]\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-08-13T11:36:49Z",
      "updated_at": "2024-08-26T12:16:57Z",
      "closed_at": "2024-08-26T12:16:57Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/370/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/370",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/370",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:29.298275",
      "comments": []
    },
    {
      "issue_number": 301,
      "title": "[Bug]: No Traces on langchain_community.llms.Ollama",
      "body": "Hi, somehow I get no traces if using Langchain' Ollama, but all seems fine with raw Ollama package:\r\n\r\n \r\n```\r\nfrom ollama import Client\r\nimport openlit\r\n\r\nopenlit.init( otlp_endpoint=\"http://127.0.0.1:4318\" )\r\nclient = Client(host='http://localhost:11434') \r\nresponse = client.chat(model=\"aya:8b\"  , messages=[\r\n  { 'role': 'user', 'content': 'Why is the sky blue?', },\r\n])\r\n#### That produce traces, all good\r\n```\r\n\r\n```\r\nfrom langchain_community.llms import Ollama\r\nimport openlit\r\n\r\nopenlit.init( otlp_endpoint=\"http://127.0.0.1:4318\" )\r\n\r\nllm = Ollama(model=\"aya:8b\", base_url=\"http://localhost:11434\" )\r\nresponse = llm.invoke(\"Tell me a joke\")\r\nprint(response )\r\n#### ?? no traces\r\n```\r\n\r\nI use Win11, and I've tried different versions of LangChain and OpenLit - does not work. \r\nAlso checked it on machine with Win10 - same story.",
      "state": "closed",
      "author": "moonbaseDelta",
      "author_type": "User",
      "created_at": "2024-06-23T17:24:25Z",
      "updated_at": "2024-08-23T12:24:17Z",
      "closed_at": "2024-08-23T12:24:03Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/301/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/301",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/301",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:29.298300",
      "comments": [
        {
          "author": "moonbaseDelta",
          "body": "As far as I understand, current wrapping over LangChain Ollamas is not implemented: https://github.com/openlit/openlit/blob/main/sdk/python/src/openlit/instrumentation/langchain/__init__.py\r\n\r\nI've read over LangChain code and I agree that it's complicated way too much.\r\n\r\n\r\nHowever there're `langch",
          "created_at": "2024-06-24T03:18:35Z"
        },
        {
          "author": "patcher9",
          "body": "Yeah, LangChain seems to not be using the Ollama SDK underneath which causes this issue. \r\n\r\nTracers wont work I think here as they are not in OpenTelemetry format (recommended format for Tracing), Though I can create wrappers around the LLM Invoke function directly (That might lead to duplicate tra",
          "created_at": "2024-06-25T12:58:00Z"
        },
        {
          "author": "patcher9",
          "body": "Update:\r\nAble to figure out a way to trace all LLMs under langchain_community. I will hopefully be making a release today or early tommorow. Will update you here in the thread",
          "created_at": "2024-07-01T14:48:18Z"
        },
        {
          "author": "patcher9",
          "body": "Closing this issues as fixed in OpenLIT sdk version `1.14.2`\r\n\r\nIf its still relevant, Feel free to reopen",
          "created_at": "2024-07-01T16:53:14Z"
        },
        {
          "author": "jradikk",
          "body": "@patcher9 is it possible to also add support for langchain_community.chat_models module? If i use the following module, openlit doesn't generate traces as opposed to using langchain_community.llms module\r\n\r\n```python\r\nfrom langchain_community.chat_models import ChatOllama\r\n\r\nllm = ChatOllama(\r\n     ",
          "created_at": "2024-08-23T10:42:06Z"
        }
      ]
    },
    {
      "issue_number": 377,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring OLA Krutrim",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCannot monitor OLA Krutrim AI LLMs based Applications\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to OpenAI, Have a auto instrumentation for OLA Krutrim\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-08-18T09:42:05Z",
      "updated_at": "2024-08-23T12:21:17Z",
      "closed_at": "2024-08-23T12:21:17Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/377/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/377",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/377",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:29.542642",
      "comments": []
    },
    {
      "issue_number": 382,
      "title": "[Feat]: Support for fastapi post request (Ollama-server)",
      "body": "### üöÄ What's the Problem?\r\nCurrently working with openwebui, their way of sending requests to the ollama-server is not with using the 'standard' way of using the ollama py package. But instead using requests, see below for more detail.\r\n\r\n\r\n### üí° Your Dream Solution\r\nThat there is support for this kind of api request as well, beside ollama.chat :\r\n\r\ncurl http://localhost:11434/api/chat -d '{ \r\n  \"model\": \"llama3\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"why is the sky blue?\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"due to rayleigh scattering.\"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"how is that different than mie scattering?\"\r\n    }\r\n  ],\r\n\"stream\": true\r\n}'\r\n\r\n\r\n\r\n",
      "state": "open",
      "author": "MarcRevo",
      "author_type": "User",
      "created_at": "2024-08-21T07:34:37Z",
      "updated_at": "2024-08-23T06:23:44Z",
      "closed_at": null,
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/382/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/382",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/382",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:29.542660",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @MarcRevo \r\n\r\nAutomatically tracking post requests is a bit tough as most often we dont know whats happening. Ill try it out and if we can make it work but in the meantime, Ill recommend using [Manual Tracing in OpenLIT](https://docs.openlit.io/latest/features/tracing#manual-tracing). \r\n\r\nSometh",
          "created_at": "2024-08-23T06:23:42Z"
        }
      ]
    },
    {
      "issue_number": 371,
      "title": "[Feat]: Show the OpenLIT UI version in the UI",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\n\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-08-14T11:36:14Z",
      "updated_at": "2024-08-18T07:54:53Z",
      "closed_at": "2024-08-18T07:54:53Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/371/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/371",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/371",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:31.573478",
      "comments": []
    },
    {
      "issue_number": 369,
      "title": "[Feat]: Support Mistral AI Python SDK v1.0.0",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nUpdates the existing Mistral Instrumentation to support Mistral's Python SDK > 1.0.0. This will allow Monitoring LLM Applications using Mistral Python SDK > 1.0.0\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar experience as to the current Mistral Instrumentation for Observability \r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-08-13T11:22:19Z",
      "updated_at": "2024-08-18T07:40:53Z",
      "closed_at": "2024-08-18T07:40:53Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/369/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/369",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/369",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:31.573507",
      "comments": []
    },
    {
      "issue_number": 367,
      "title": "[Feat]: Add Tracking Costs by using a finetuned model on Ollama Server ",
      "body": "### üöÄ What's the Problem?\r\nWe are currently running an Ollama server with a fine-tuned LLM in our private cloud environment. However adding our own model to the **pricing.json** does not work. Seems like it is not yet supported.\r\n\r\n\r\n### üí° Your Dream Solution\r\nAdding our finetuned models to the pricing.json with their own specs.\r\n\r\n",
      "state": "closed",
      "author": "MarcRevo",
      "author_type": "User",
      "created_at": "2024-08-13T10:49:40Z",
      "updated_at": "2024-08-13T11:43:09Z",
      "closed_at": "2024-08-13T11:43:09Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/367/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/367",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/367",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:31.573517",
      "comments": []
    },
    {
      "issue_number": 246,
      "title": "[Feat]: Add Testing for OpenLIT UI",
      "body": "### üöÄ What's the Problem?\r\nThere is no testing workflow for OpenLIT UI\r\n\r\n### üí° Your Dream Solution\r\nAn e2e Testing workflow for OpenLIT UI\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-23T06:58:00Z",
      "updated_at": "2024-08-10T10:29:48Z",
      "closed_at": "2024-08-10T10:29:48Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":raised_hand: Up for Grabs",
        "client",
        "github_actions"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/246/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/246",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/246",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:31.573525",
      "comments": [
        {
          "author": "patcher9",
          "body": "resolved in #360 ",
          "created_at": "2024-08-10T10:29:38Z"
        }
      ]
    },
    {
      "issue_number": 299,
      "title": "[Feat]: Adopt OTel Semantic Conventions for Resource Attributes",
      "body": "### üöÄ What's the Problem?\r\nThe application name and instrumentation library are currently included in each span and metric as attributes (`gen_ai.application_name`, `telemetry.sdk.name`), which is a non-standard strategy to name and deliver such information.\r\n\r\n### üí° Your Dream Solution\r\nOpenTelemetry provides a `service.name` standard attribute to hold the application name. It's included in the resource attributes instead of span attributes. See: https://opentelemetry.io/docs/specs/semconv/resource/#service. It's actually present in the spans produced by OpenLit, but it's set to `default` rather than to the application name. About the metrics, having the `service.name` as a resource attribute instead of metric attribute is also the standard strategy. See: https://opentelemetry.io/docs/specs/otel/compatibility/prometheus_and_openmetrics/#resource-attributes\r\n\r\nAbout the instrumentation library name (`telemetry.sdk.name`), it's currently included as a span attribute, as a resource attribute, and as a metric attribute. Is it required to have it as a span attribute and metric attribute? Would it work having it just as a resource attribute, as defined in the semantic conventions? See: https://opentelemetry.io/docs/specs/semconv/resource/#telemetry-sdk. It's also required to provide `telemetry.sdk.language` and `telemetry.sdk.version` to fullfill the conventions.\r\n\r\n### üìñ Additional Context\r\nI really like OpenLit, congratulations on such a great library and thanks for following the OpenTelemetry path. It makes it possible to integrate even more systems with OpenLit. I'm currently working on integrating Java applications with OpenLit, and thanks to the OpenTelemetry adoption is more approachable. The challenge I'm experiencing is about the application name and telemetry SDK, which is why I raised this issue.",
      "state": "closed",
      "author": "ThomasVitale",
      "author_type": "User",
      "created_at": "2024-06-22T14:30:07Z",
      "updated_at": "2024-08-07T11:19:04Z",
      "closed_at": "2024-08-07T11:19:03Z",
      "labels": [
        ":rocket: Feature"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/299/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/299",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/299",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:31.824985",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @ThomasVitale \r\n\r\nThanks for opening up the issue\r\n\r\nSo if you initialize OpenLIT like\r\n\r\n```\r\nopenlit.init(application_name=\"some_name\", environment=\"some_env\")\r\n```\r\n\r\nthen you should have the applied value available in resource attributes aswell. Same for telemetry SDK, It should be available",
          "created_at": "2024-06-22T21:42:37Z"
        },
        {
          "author": "ThomasVitale",
          "body": "Thanks a lot for the reply, @patcher9, it helped clarifying things. I understand now why both pieces of information are included as span and metrics attributes. Also, I hadn't realised I could pass the application name via the `openlit.init()` method, I should have read the documentation more carefu",
          "created_at": "2024-06-23T16:28:56Z"
        },
        {
          "author": "patcher9",
          "body": "> If I instrument an application via OpenTelemetry directly, would the OpenLit UI work if the application name was only included as a service.name resource attribute and the telemetry.sdk.name was only included as a resource attribute and set to opentelemetry?\r\n\r\nYes, The UI should work (except the ",
          "created_at": "2024-06-25T13:00:53Z"
        },
        {
          "author": "ThomasVitale",
          "body": "Thanks for the additional info, much appreciated. I'll join the conversation on Slack.",
          "created_at": "2024-06-25T23:21:01Z"
        },
        {
          "author": "patcher9",
          "body": "Started a thread on Slack to discuss, See you there üëã !",
          "created_at": "2024-07-01T16:06:36Z"
        }
      ]
    },
    {
      "issue_number": 317,
      "title": "[Feat]: Capture span attribute `gen_ai.prompt` and `gen_ai.completion` in Event Attributes",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nhttps://github.com/openlit/openlit/issues/316 - @codefromthecrypt makes a good point that the `gen_ai.prompt` and `gen_ai.completion` should be captured in Event attributes of the Trace.\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\n`gen_ai.prompt` and `gen_ai.completion` in Event Attributes\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-07-07T17:40:31Z",
      "updated_at": "2024-08-07T11:17:59Z",
      "closed_at": "2024-08-07T11:17:59Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/317/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/317",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/317",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:32.036219",
      "comments": []
    },
    {
      "issue_number": 328,
      "title": "[Feat]: Add OpenTelemetry instrumentation for monitoring vLLM",
      "body": "### üöÄ What's the Problem?\r\nLack of vLLM integration.\r\n\r\n\r\n### üí° Your Dream Solution\r\nAfter the integration is completed, it will be a monitoring application that works on a larger scale.\r\n\r\n\r\n### ü§î Seen anything similar?\r\nNo\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNo\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AlbertoManuel07",
      "author_type": "User",
      "created_at": "2024-07-14T11:38:13Z",
      "updated_at": "2024-08-07T11:03:15Z",
      "closed_at": "2024-08-07T11:03:14Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/328/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/328",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/328",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:32.036253",
      "comments": [
        {
          "author": "patcher9",
          "body": "Thanks @AlbertoManuel07 for thsi feature request, I am now starting to work on this and will keep you updated as I go through this. In the meanwhile if you have any specific usecases or if you could share your workflow with vLLM, I can try to make sure that those are covered as I add this feature to",
          "created_at": "2024-07-17T10:26:20Z"
        },
        {
          "author": "patcher9",
          "body": "Also I just realised, You should be able to send traces directly from vLLM with `--otlp-traces-endpoint` aswell",
          "created_at": "2024-07-17T12:15:55Z"
        }
      ]
    },
    {
      "issue_number": 297,
      "title": "[Bug]: App fails when OpenLIT is used",
      "body": "### üêõ What's Going Wrong?\r\nI have a small LangChain-based app that works. When I add `openlit.init()`, it fails to start with the following exceptions:\r\n\r\n```\r\nFailed to instrument transformers: No module named 'transformers.pipelines'; 'transformers' is not a package\r\n...\r\n  File \"/Users/User/Library/Caches/pypoetry/virtualenvs/librefactor-k7sXJnK--py3.12/lib/python3.12/site-packages/langchain_community/embeddings/bedrock.py\", line 150, in _embedding_func\r\n    raise ValueError(f\"Error raised by inference endpoint: {e}\")\r\nValueError: Error raised by inference endpoint: 'SSO' object has no attribute 'invoke_model'\r\n```\r\nIf I remove `transformers` package it complains about the absense:\r\n```\r\nDependencyConflict: requested: \"transformers >= 4.39.3\" but found: \"None\"\r\n```\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nI have an app that makes use of the following:\r\n- langchain 0.2.5\r\n- chromadb 0.5.3\r\n- Amazon Bedrock with Claude 3 and Titan v2 Embeddings\r\n\r\nHere is the code with logic stripped for brevity:\r\n```\r\nimport openlit\r\n\r\nfrom langchain_community.embeddings.bedrock import BedrockEmbeddings\r\nfrom langchain_community.vectorstores.chroma import Chroma\r\nfrom langchain_aws import ChatBedrock\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom langchain_core.prompts import ChatPromptTemplate\r\n\r\nopenlit.init()\r\n\r\ntext_for_search = \"\"\r\n\r\nbe = BedrockEmbeddings(model_id='amazon.titan-embed-text-v2:0')\r\nch = Chroma(persist_directory='./chroma', embedding_function=be)\r\nsres = ch.similarity_search_with_score(query=text_for_search)\r\n\r\nprompt = \"\"\"prompt {input} {search_result}\"\"\"\r\n\r\nllm = ChatBedrock(model_id=\"anthropic.claude-3-opus-20240229-v1:0\")\r\nprompt = ChatPromptTemplate.from_template(prompt)\r\noutput_parser = StrOutputParser()\r\nchain = prompt | llm | output_parser\r\n\r\nr = chain.invoke({\"input\": \"some input\", \"search_result\": sres[0][0].page_content})\r\n```\r\n\r\nWhen I comment out `openlit.init()`, it works.\r\n\r\n### üéØ What Did You Expect?\r\nThe app gets instrumented with traces.\r\n\r\n### üì∏ Any Screenshots?\r\n>  Pictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: 1.14.1\r\n- Deployment Method: Docker\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "gevial",
      "author_type": "User",
      "created_at": "2024-06-21T12:52:28Z",
      "updated_at": "2024-08-05T09:52:00Z",
      "closed_at": "2024-07-31T17:54:26Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/297/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/297",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/297",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:32.247390",
      "comments": [
        {
          "author": "AmanAgarwal041",
          "body": "Thanks for letting us know. We will fix it soon @gevial . ",
          "created_at": "2024-06-22T09:03:10Z"
        },
        {
          "author": "patcher9",
          "body": "Hey @gevial Whats the SDK version you are using?\r\n\r\nA quick fix would be to ignore the HuggingFace instrumentation\r\n\r\n```\r\nopenlit.init(disabled_instrumentors=[\"transformers\"])\r\n```\r\n\r\nAlso I do see\r\nValueError: Error raised by inference endpoint: 'SSO' object has no attribute 'invoke_model'\r\nwhich ",
          "created_at": "2024-06-22T11:42:14Z"
        },
        {
          "author": "gevial",
          "body": "Hi @patcher9 \r\nIt's version 1.14.1\r\n\r\nI tried the fix you suggested but unfortunately the app fails in the same way.\r\nWhat you're saying about instrumentation makes total sense, but the fact is the app works just fine until I add a single line of `openlit.init()` (with the import statement of course",
          "created_at": "2024-06-24T15:29:18Z"
        },
        {
          "author": "patcher9",
          "body": "qq do you also have transformers installed as a library? do you know the version that you have installed?",
          "created_at": "2024-07-01T13:09:57Z"
        },
        {
          "author": "gevial",
          "body": "@patcher9 I had had to add it (4.42.3) because otherwise the instrumentation failed with \r\n\r\n`DependencyConflict: requested: \"transformers >= 4.39.3\" but found: \"None\"`\r\n\r\nBut then I changed `openlit.init()` to `openlit.init(disabled_instrumentors=[\"transformers\"])`, and removed transformers. I don'",
          "created_at": "2024-07-01T13:21:18Z"
        }
      ]
    },
    {
      "issue_number": 300,
      "title": "[Feat]: Remove the Span `StatusCode=OK` requirement to follow OTel Trace spec",
      "body": "### üöÄ What's the Problem?\r\nTelemetry from instrumented applications is only included in the OpenLit UI if the related span `StatusCode` is `OK`. When using the OpenLit library, that's not a problem since that is done as part of the instrumentation. However, when integrating other libraries/applications, it becomes a problem if they follow the OTel Trace specification and keep the `StatusCode` unset in successful scenarios.\r\n\r\n### üí° Your Dream Solution\r\nIt would be great if the span `StatusCode=OK` requirement could be removed, perhaps converting that into a filter to exclude spans with an explicit `StatusCode=ERROR`. The OpenTelemetry Trace Specification ([spec](https://github.com/open-telemetry/opentelemetry-specification/blob/v1.34.0/specification/trace/api.md#set-status)) includes the following about the status:\r\n\r\n> Generally, Instrumentation Libraries SHOULD NOT set the status code to Ok, unless explicitly configured to do so. Instrumentation Libraries SHOULD leave the status code as Unset unless there is an error, as described above.\r\n> \r\n> Application developers and Operators may set the status code to Ok.\r\n\r\nFurthermore, the [OTel semantic conventions](https://opentelemetry.io/docs/specs/semconv/http/http-spans/#status) includes the following about the status of HTTP interactions: \r\n\r\n> [Span Status](https://github.com/open-telemetry/opentelemetry-specification/tree/v1.33.0/specification/trace/api.md#set-status) MUST be left unset if HTTP status code was in the 1xx, 2xx or 3xx ranges, unless there was another error (e.g., network error receiving the response body; or 3xx codes with max redirects exceeded), in which case status MUST be set to Error.\r\n\r\n### üìñ Additional Context\r\nI really like OpenLit, congratulations on such a great library and thanks for following the OpenTelemetry path. It makes it possible to integrate even more systems with OpenLit. I'm currently working on integrating Java applications with OpenLit, and thanks to the OpenTelemetry adoption is more approachable. The main challenge I'm experiencing is about the `unset` span status code instead of `OK`, which is why I raised this issue.",
      "state": "closed",
      "author": "ThomasVitale",
      "author_type": "User",
      "created_at": "2024-06-22T21:45:31Z",
      "updated_at": "2024-08-02T15:22:31Z",
      "closed_at": "2024-07-25T08:00:43Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/300/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/300",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/300",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:32.455989",
      "comments": [
        {
          "author": "patcher9",
          "body": "Good call out, really appreciate this one. \r\n\r\nThis makes sense and I like the idea of keeping it configurable from the user's end.\r\n\r\nLemme know if you would like to raise a PR for this",
          "created_at": "2024-06-22T22:32:14Z"
        },
        {
          "author": "ThomasVitale",
          "body": "@patcher9 Thanks for the quick reply. I'd be happy to contribute a PR!\r\n\r\n* I can update the default filter in https://github.com/openlit/openlit/blob/main/src/client/src/helpers/platform.ts#L228 so that if a StatusCode is specified, that will be used. If not, it would include all spans where Status",
          "created_at": "2024-06-23T05:59:53Z"
        },
        {
          "author": "patcher9",
          "body": "For the UI, We can query for both OK and UNSET. Kinda like an or condition I think.\r\n\r\nFor the SDK Instrumentation I think, We can add a flag (bool) if the user wants status as OK in spans, If that is a True we set status as OK, else default will be UNSET\r\n```\r\nif set_ok:\r\n  span.set_status(Status(S",
          "created_at": "2024-06-23T16:10:18Z"
        },
        {
          "author": "ThomasVitale",
          "body": "Yes, it does, thank you! How should the flag for the span status be provided by users? Via the `openlit.init()` method?",
          "created_at": "2024-06-23T16:17:13Z"
        },
        {
          "author": "patcher9",
          "body": "yup, via `openlit.init()`",
          "created_at": "2024-06-25T12:58:22Z"
        }
      ]
    },
    {
      "issue_number": 342,
      "title": "[Bug]: Incompatibilities with OpenTelemetry LLM semantics pending release",
      "body": "### üêõ What's Going Wrong?\r\n\r\nI work on the OpenTelemetry LLM semantics SIG, and did an evaluation of the SDK based on the following sample code and what the semantics pending release 1.27.0 will define.\r\n\r\nThe sample code produces one span and five metrics.\r\n\r\nThe span has several exact matches with the pending semantics, but also numerous areas where data is collected as different attributes, different value types or different model locations (e.g. attribute where defined as an event). That said, many differences feel like linting vs major differences. \r\n\r\nThe metrics currently have little in common with what's pending release, either by accident or by intent of collecting different signals or collecting it differently.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nrun the following against an ollama hosted codegemma:2b-code model and look at the collector output.\r\n\r\n```python\r\nimport os\r\nimport openlit\r\nfrom openai import OpenAI\r\n\r\n# Set the service name such that it is different from other experiments\r\napplication_name = \"openlit-python-ollama\"\r\n# Default the SDK endpoint ENV variable to localhost\r\notlp_endpoint = os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://localhost:4318\")\r\n# Don't batch spans, as this is a demo\r\nopenlit.init(application_name=application_name, otlp_endpoint=otlp_endpoint, disable_batch=True)\r\n\r\ndef main():\r\n    ollama_host = os.getenv('OLLAMA_HOST', 'localhost')\r\n    # Use the OpenAI endpoint, not the Ollama API.\r\n    base_url = 'http://' + ollama_host + ':11434/v1'\r\n    client = OpenAI(base_url=base_url, api_key='unused')\r\n    messages = [\r\n      {\r\n        'role': 'user',\r\n        'content': '<|fim_prefix|>def hello_world():<|fim_suffix|><|fim_middle|>',\r\n      },\r\n    ]\r\n    chat_completion = client.chat.completions.create(model='codegemma:2b-code', messages=messages)\r\n    print(chat_completion.choices[0].message.content)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n### üéØ What Did You Expect?\r\n\r\nSemantic evaluation on [spans](https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/gen-ai-spans.md).\r\n\r\ncompatible:\r\n* kind=client\r\n* attributes['gen_ai.system']='openai'\r\n* attributes['gen_ai.operation.name']='chat'\r\n* attributes['gen_ai.response.id']='chatcmpl-411'\r\n* attributes['gen_ai.request.model']='codegemma:2b-code'\r\n\r\ndeprecated fields:\r\n* attributes['gen_ai.usage.completion_tokens']=60 (deprecated for gen_ai.usage.output_tokens)\r\n* attributes['gen_ai.usage.prompt_tokens']=24 (deprecated for gen_ai.usage.input_tokens)\r\n\r\nincompatible:\r\n* name='openai.chat.completions' (should be 'chat codegemma:2b-code')\r\n* attributes['gen_ai.request.top_p']=1 (should be a double 1.0, not int 1)\r\n* attributes['gen_ai.request.max_tokens']='' (should be an int, not a string)\r\n* attributes['gen_ai.request.temperature']=1 (should be a double 1.0, not int 1)\r\n* attributes['gen_ai.request.presence_penalty']=0 (should be a double 0.0, not int 0)\r\n* attributes['gen_ai.request.frequency_penalty']=0 (should be a double 0.0, not int 0)\r\n* attributes['gen_ai.prompt']='<|fim_prefix|>def hello_world():<|fim_suffix|><|fim_middle|>'  (should be 'content' in the event attribute `gen_ai.prompt`)\r\n* attributes['gen_ai.response.finish_reason']='stop' (should be 'gen_ai.response.finish_reasons' and an array)\r\n* attributes['gen_ai.completion']='...'  (should be 'content' in the event attribute `gen_ai.completion`)\r\n\r\ndefined in other conventions:\r\n* attributes['telemetry.sdk.name']='openlit' (Resource)\r\n\r\nnot yet defined in the standard:\r\n* attributes['gen_ai.endpoint']='openai.chat.completions'\r\n* attributes['gen_ai.environment']='default'\r\n* attributes['gen_ai.application_name']='openlit-python-ollama'\r\n* attributes['gen_ai.request.user']=''\r\n* attributes['gen_ai.request.seed']=''\r\n* attributes['gen_ai.request.is_stream']=false\r\n* attributes['gen_ai.usage.total_tokens']=84\r\n* attributes['gen_ai.usage.cost']=0\r\n\r\n\r\nSemantic evaluation on\r\n[metrics](https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/gen-ai-metrics.md):\r\n\r\n#### 'gen_ai.total.requests' (custom)\r\n\r\nThis is not yet defined in the spec\r\n\r\n#### 'gen_ai.usage.total_tokens' (custom)\r\n\r\nThis is not yet defined in the spec\r\n\r\n#### 'gen_ai.usage.completion_tokens' (custom or incompatible)\r\n\r\nTwo options:\r\n* It meant to be cumulative and is just custom\r\n* It should be a histogram, then should match 'gen_ai.client.token.usage' with 'output' and matching attributes\r\n\r\n\r\n#### 'gen_ai.usage.prompt_tokens' (custom or incompatible)\r\n\r\nTwo options:\r\n* It meant to be cumulative and is just custom\r\n* It should be a histogram, then should match 'gen_ai.client.token.usage' with 'input' and matching attributes\r\n\r\n#### 'gen_ai.usage.cost' (custom)\r\n\r\nThis is not yet defined in the spec\r\n\r\n### üì∏ Any Screenshots?\r\n\r\nExample collector log\r\n\r\n```\r\notel-collector      | 2024-07-24T12:01:00.601Z  info    TracesExporter  {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\", \"resource spans\": 1, \"spans\": 1}\r\notel-collector      | 2024-07-24T12:01:00.602Z  info    ResourceSpans #0\r\notel-collector      | Resource SchemaURL: \r\notel-collector      | Resource attributes:\r\notel-collector      |      -> service.name: Str(openlit-python-ollama)\r\notel-collector      |      -> deployment.environment: Str(default)\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      | ScopeSpans #0\r\notel-collector      | ScopeSpans SchemaURL: \r\notel-collector      | InstrumentationScope openlit.otel.tracing \r\notel-collector      | Span #0\r\notel-collector      |     Trace ID       : 51b590ffb975457834cbd6edec5ad881\r\notel-collector      |     Parent ID      : \r\notel-collector      |     ID             : dc5fa811205a8925\r\notel-collector      |     Name           : openai.chat.completions\r\notel-collector      |     Kind           : Client\r\notel-collector      |     Start time     : 2024-07-24 12:00:57.632786259 +0000 UTC\r\notel-collector      |     End time       : 2024-07-24 12:01:00.598742222 +0000 UTC\r\notel-collector      |     Status code    : Ok\r\notel-collector      |     Status message : \r\notel-collector      | Attributes:\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      |      -> gen_ai.system: Str(openai)\r\notel-collector      |      -> gen_ai.operation.name: Str(chat)\r\notel-collector      |      -> gen_ai.endpoint: Str(openai.chat.completions)\r\notel-collector      |      -> gen_ai.response.id: Str(chatcmpl-411)\r\notel-collector      |      -> gen_ai.environment: Str(default)\r\notel-collector      |      -> gen_ai.application_name: Str(openlit-python-ollama)\r\notel-collector      |      -> gen_ai.request.model: Str(codegemma:2b-code)\r\notel-collector      |      -> gen_ai.request.top_p: Int(1)\r\notel-collector      |      -> gen_ai.request.max_tokens: Str()\r\notel-collector      |      -> gen_ai.request.user: Str()\r\notel-collector      |      -> gen_ai.request.temperature: Int(1)\r\notel-collector      |      -> gen_ai.request.presence_penalty: Int(0)\r\notel-collector      |      -> gen_ai.request.frequency_penalty: Int(0)\r\notel-collector      |      -> gen_ai.request.seed: Str()\r\notel-collector      |      -> gen_ai.request.is_stream: Bool(false)\r\notel-collector      |      -> gen_ai.prompt: Str(user: <|fim_prefix|>def hello_world():<|fim_suffix|><|fim_middle|>)\r\notel-collector      |      -> gen_ai.usage.prompt_tokens: Int(24)\r\notel-collector      |      -> gen_ai.usage.completion_tokens: Int(60)\r\notel-collector      |      -> gen_ai.usage.total_tokens: Int(84)\r\notel-collector      |      -> gen_ai.response.finish_reason: Str(stop)\r\notel-collector      |      -> gen_ai.usage.cost: Int(0)\r\notel-collector      |      -> gen_ai.completion: Str(  (0)  (1)  (2)  (3)  (4)  (5)  (6)  (7)  (8)  (9)> def hello_world():\r\notel-collector      | >>> print(\"Hello, world!\")\r\notel-collector      | \r\notel-collector      | )\r\notel-collector      |   {\"kind\": \"exporter\", \"data_type\": \"traces\", \"name\": \"debug\"}\r\notel-collector      | 2024-07-24T12:01:00.605Z  info    MetricsExporter {\"kind\": \"exporter\", \"data_type\": \"metrics\", \"name\": \"debug\", \"resource metrics\": 1, \"metrics\": 5, \"data points\": 5}\r\notel-collector      | 2024-07-24T12:01:00.605Z  info    ResourceMetrics #0\r\notel-collector      | Resource SchemaURL: \r\notel-collector      | Resource attributes:\r\notel-collector      |      -> service.name: Str(openlit-python-ollama)\r\notel-collector      |      -> deployment.environment: Str(default)\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      | ScopeMetrics #0\r\notel-collector      | ScopeMetrics SchemaURL: \r\notel-collector      | InstrumentationScope openlit.otel.metrics 0.1.0\r\notel-collector      | Metric #0\r\notel-collector      | Descriptor:\r\notel-collector      |      -> Name: gen_ai.total.requests\r\notel-collector      |      -> Description: Number of requests to GenAI\r\notel-collector      |      -> Unit: 1\r\notel-collector      |      -> DataType: Sum\r\notel-collector      |      -> IsMonotonic: true\r\notel-collector      |      -> AggregationTemporality: Cumulative\r\notel-collector      | NumberDataPoints #0\r\notel-collector      | Data point attributes:\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      |      -> gen_ai.application_name: Str(openlit-python-ollama)\r\notel-collector      |      -> gen_ai.system: Str(openai)\r\notel-collector      |      -> gen_ai.environment: Str(default)\r\notel-collector      |      -> gen_ai.operation.name: Str(chat)\r\notel-collector      |      -> gen_ai.request.model: Str(codegemma:2b-code)\r\notel-collector      | StartTimestamp: 2024-07-24 12:01:00.598679471 +0000 UTC\r\notel-collector      | Timestamp: 2024-07-24 12:01:00.603022168 +0000 UTC\r\notel-collector      | Value: 1\r\notel-collector      | Metric #1\r\notel-collector      | Descriptor:\r\notel-collector      |      -> Name: gen_ai.usage.total_tokens\r\notel-collector      |      -> Description: Number of total tokens processed.\r\notel-collector      |      -> Unit: 1\r\notel-collector      |      -> DataType: Sum\r\notel-collector      |      -> IsMonotonic: true\r\notel-collector      |      -> AggregationTemporality: Cumulative\r\notel-collector      | NumberDataPoints #0\r\notel-collector      | Data point attributes:\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      |      -> gen_ai.application_name: Str(openlit-python-ollama)\r\notel-collector      |      -> gen_ai.system: Str(openai)\r\notel-collector      |      -> gen_ai.environment: Str(default)\r\notel-collector      |      -> gen_ai.operation.name: Str(chat)\r\notel-collector      |      -> gen_ai.request.model: Str(codegemma:2b-code)\r\notel-collector      | StartTimestamp: 2024-07-24 12:01:00.59870018 +0000 UTC\r\notel-collector      | Timestamp: 2024-07-24 12:01:00.603022168 +0000 UTC\r\notel-collector      | Value: 84\r\notel-collector      | Metric #2\r\notel-collector      | Descriptor:\r\notel-collector      |      -> Name: gen_ai.usage.completion_tokens\r\notel-collector      |      -> Description: Number of completion tokens processed.\r\notel-collector      |      -> Unit: 1\r\notel-collector      |      -> DataType: Sum\r\notel-collector      |      -> IsMonotonic: true\r\notel-collector      |      -> AggregationTemporality: Cumulative\r\notel-collector      | NumberDataPoints #0\r\notel-collector      | Data point attributes:\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      |      -> gen_ai.application_name: Str(openlit-python-ollama)\r\notel-collector      |      -> gen_ai.system: Str(openai)\r\notel-collector      |      -> gen_ai.environment: Str(default)\r\notel-collector      |      -> gen_ai.operation.name: Str(chat)\r\notel-collector      |      -> gen_ai.request.model: Str(codegemma:2b-code)\r\notel-collector      | StartTimestamp: 2024-07-24 12:01:00.598710138 +0000 UTC\r\notel-collector      | Timestamp: 2024-07-24 12:01:00.603022168 +0000 UTC\r\notel-collector      | Value: 60\r\notel-collector      | Metric #3\r\notel-collector      | Descriptor:\r\notel-collector      |      -> Name: gen_ai.usage.prompt_tokens\r\notel-collector      |      -> Description: Number of prompt tokens processed.\r\notel-collector      |      -> Unit: 1\r\notel-collector      |      -> DataType: Sum\r\notel-collector      |      -> IsMonotonic: true\r\notel-collector      |      -> AggregationTemporality: Cumulative\r\notel-collector      | NumberDataPoints #0\r\notel-collector      | Data point attributes:\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      |      -> gen_ai.application_name: Str(openlit-python-ollama)\r\notel-collector      |      -> gen_ai.system: Str(openai)\r\notel-collector      |      -> gen_ai.environment: Str(default)\r\notel-collector      |      -> gen_ai.operation.name: Str(chat)\r\notel-collector      |      -> gen_ai.request.model: Str(codegemma:2b-code)\r\notel-collector      | StartTimestamp: 2024-07-24 12:01:00.598714888 +0000 UTC\r\notel-collector      | Timestamp: 2024-07-24 12:01:00.603022168 +0000 UTC\r\notel-collector      | Value: 24\r\notel-collector      | Metric #4\r\notel-collector      | Descriptor:\r\notel-collector      |      -> Name: gen_ai.usage.cost\r\notel-collector      |      -> Description: The distribution of GenAI request costs.\r\notel-collector      |      -> Unit: USD\r\notel-collector      |      -> DataType: Histogram\r\notel-collector      |      -> AggregationTemporality: Cumulative\r\notel-collector      | HistogramDataPoints #0\r\notel-collector      | Data point attributes:\r\notel-collector      |      -> telemetry.sdk.name: Str(openlit)\r\notel-collector      |      -> gen_ai.application_name: Str(openlit-python-ollama)\r\notel-collector      |      -> gen_ai.system: Str(openai)\r\notel-collector      |      -> gen_ai.environment: Str(default)\r\notel-collector      |      -> gen_ai.operation.name: Str(chat)\r\notel-collector      |      -> gen_ai.request.model: Str(codegemma:2b-code)\r\notel-collector      | StartTimestamp: 2024-07-24 12:01:00.598719347 +0000 UTC\r\notel-collector      | Timestamp: 2024-07-24 12:01:00.603022168 +0000 UTC\r\notel-collector      | Count: 1\r\notel-collector      | Sum: 0.000000\r\notel-collector      | Min: 0.000000\r\notel-collector      | Max: 0.000000\r\notel-collector      | ExplicitBounds #0: 0.000000\r\notel-collector      | ExplicitBounds #1: 5.000000\r\notel-collector      | ExplicitBounds #2: 10.000000\r\notel-collector      | ExplicitBounds #3: 25.000000\r\notel-collector      | ExplicitBounds #4: 50.000000\r\notel-collector      | ExplicitBounds #5: 75.000000\r\notel-collector      | ExplicitBounds #6: 100.000000\r\notel-collector      | ExplicitBounds #7: 250.000000\r\notel-collector      | ExplicitBounds #8: 500.000000\r\notel-collector      | ExplicitBounds #9: 750.000000\r\notel-collector      | ExplicitBounds #10: 1000.000000\r\notel-collector      | ExplicitBounds #11: 2500.000000\r\notel-collector      | ExplicitBounds #12: 5000.000000\r\notel-collector      | ExplicitBounds #13: 7500.000000\r\notel-collector      | ExplicitBounds #14: 10000.000000\r\notel-collector      | Buckets #0, Count: 1\r\notel-collector      | Buckets #1, Count: 0\r\notel-collector      | Buckets #2, Count: 0\r\notel-collector      | Buckets #3, Count: 0\r\notel-collector      | Buckets #4, Count: 0\r\notel-collector      | Buckets #5, Count: 0\r\notel-collector      | Buckets #6, Count: 0\r\notel-collector      | Buckets #7, Count: 0\r\notel-collector      | Buckets #8, Count: 0\r\notel-collector      | Buckets #9, Count: 0\r\notel-collector      | Buckets #10, Count: 0\r\notel-collector      | Buckets #11, Count: 0\r\notel-collector      | Buckets #12, Count: 0\r\notel-collector      | Buckets #13, Count: 0\r\notel-collector      | Buckets #14, Count: 0\r\notel-collector      | Buckets #15, Count: 0\r\notel-collector      |   {\"kind\": \"exporter\", \"data_type\": \"metrics\", \"name\": \"debug\"}\r\n```\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [e.g., 0.0.1]\r\n- OpenLIT SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Docker]\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "codefromthecrypt",
      "author_type": "User",
      "created_at": "2024-07-25T03:13:57Z",
      "updated_at": "2024-08-02T02:43:31Z",
      "closed_at": "2024-07-30T06:22:34Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/342/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/342",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/342",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:32.704881",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @codefromthecrypt \r\n\r\n> name='openai.chat.completions' (should be 'chat codegemma:2b-code')\r\n\r\nWont be changing this, It doesn't make sense to have the way we have in the conventions and it should be the endpoint that is being hit which is the general norm.\r\n\r\n> attributes['gen_ai.request.max_to",
          "created_at": "2024-07-25T07:09:32Z"
        },
        {
          "author": "codefromthecrypt",
          "body": "@patcher9 you mind putting a comment about the name stuff here? One of the good outcomes I was hoping from this work is folks approving specs to know the impact to implementors. Maybe this can lead to more careful consideration or active pinging in the future https://github.com/open-telemetry/semant",
          "created_at": "2024-07-25T07:58:35Z"
        },
        {
          "author": "codefromthecrypt",
          "body": "> It is an int, since its empty we pass an empty string as passing any other default integer would be wrong\r\n\r\nmaybe there is something to clear up on the semantics about this, too. For example, to omit a value if nil has significance vs whatever the data type's zero value is. This is the first time",
          "created_at": "2024-07-25T08:01:17Z"
        },
        {
          "author": "xrmx",
          "body": "> > It is an int, since its empty we pass an empty string as passing any other default integer would be wrong\r\n> \r\n> maybe there is something to clear up on the semantics about this, too. For example, to omit a value if nil has significance vs whatever the data type's zero value is. This is the firs",
          "created_at": "2024-07-25T08:28:03Z"
        },
        {
          "author": "patcher9",
          "body": "> Since this is recommended maybe it's simpler to don't send it if you don't have it?\r\n\r\nI can try to set a conditional if the value is not none but ideally wanna avoid adding conditionals as it adds to the latency. ",
          "created_at": "2024-07-25T13:57:19Z"
        }
      ]
    },
    {
      "issue_number": 351,
      "title": "[Feat]: Add GPU Monitoring SigNoz dashboard",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nThe current SigNoz GenAI/LLM Observability dashboard doesnt show the GPU Metrics. \r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nAdd a pre-built dashboard JSON for GPU Monitoring\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "dev-gabrielchaves",
      "author_type": "User",
      "created_at": "2024-07-31T17:26:12Z",
      "updated_at": "2024-08-01T04:46:36Z",
      "closed_at": "2024-08-01T04:46:36Z",
      "labels": [
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/351/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/351",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/351",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:32.934131",
      "comments": []
    },
    {
      "issue_number": 337,
      "title": "[Bug]:Version Conflict Between OpenLit 1.16 and Qdrant-Client 1.9.0",
      "body": "### üêõ What's Going Wrong?\r\n>  Explain the bug. Tell us what's happening that shouldn't be.\r\n\r\nI installed OpenLit 1.16 and Qdrant-Client 1.9.0 using Python 3. This caused a dependency conflict where grpcio-tools 1.65.1 requires protobuf <6.0dev, >=5.26.1, but I have protobuf 4.25.3, which is incompatible.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  To see the bug, what steps should we follow?\r\n1. Install Qdrant-Client 1.9.0 using pip install qdrant-client==1.9.0.\r\n2. Install OpenLit 1.16.0 using pip install openlit==1.16.0.\r\n\r\n\r\n### üéØ What Did You Expect?\r\n>  Describe what you thought would happen.\r\n\r\nI followed the versions specified in the documentation and expected that this issue would not occur.\r\n### üì∏ Any Screenshots?\r\n>  Pictures can say a thousand words and can be super helpful!\r\n\r\n![image](https://github.com/user-attachments/assets/ed580572-5e5d-452c-9857-9a8fee944e3e)\r\n![image](https://github.com/user-attachments/assets/5423a964-9cd8-4994-920c-2ffd1b9d902e)\r\n![image](https://github.com/user-attachments/assets/b9b70f32-bdf1-49f4-bb3b-9e4452f39352)\r\n![image](https://github.com/user-attachments/assets/2e875ea3-8c30-4f95-bb24-5159c2695bd5)\r\n\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [1.16.0]\r\n- Qdrant Python SDK client Version: [1.9.0]\r\n- Deployment Method: [Docker]\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "knut-bw",
      "author_type": "User",
      "created_at": "2024-07-23T06:16:12Z",
      "updated_at": "2024-07-31T15:45:50Z",
      "closed_at": "2024-07-31T15:45:50Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/337/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/337",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/337",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:32.934153",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @knut-bw \r\nThanks for raising this issue. \r\n\r\nYeah grpcio-tools is not a direct dependency of OpenLIT but rather a dependency I think from the one of the opentelemetry package. I'll see if we can do a version bump on either and see if this can be sorted out. \r\n\r\nQQ, Is this causing any issues wh",
          "created_at": "2024-07-24T06:30:53Z"
        },
        {
          "author": "knut-bw",
          "body": "Hello, sorry for the late reply. Actually, my deployment approach involves packaging the application into a container for execution. During the image build process, I encountered a package conflict issue when trying to install dependencies.\r\n\r\nI resolved the issue by first running pip install -r req",
          "created_at": "2024-07-30T05:41:17Z"
        },
        {
          "author": "patcher9",
          "body": "Gotcha, Thanks! That makes sense. Ill still look up if I am facing any issue. Also feel free to join our [Community Slack](https://join.slack.com/t/openlit/shared_invite/zt-2etnfttwg-TjP_7BZXfYg84oAukY8QRQ) if you wanna disuss or need help with anything.",
          "created_at": "2024-07-30T06:25:26Z"
        }
      ]
    },
    {
      "issue_number": 339,
      "title": "[Bug]:Can't use it with LiteLLM - litellm 1.41.27 requires tiktoken>=0.7.0, but you have tiktoken 0.6.0 which is incompatible.",
      "body": "### üêõ What's Going Wrong?\r\n>  Following the docs, https://docs.openlit.io/latest/integrations/litellm I'm trying to add openlit to a litellm project, but it has a dependency conflict:\r\n\r\n litellm 1.41.27 requires tiktoken>=0.7.0, but you have tiktoken 0.6.0 which is incompatible.\r\n openlit 1.16.0 depends on tiktoken<0.7.0 and >=0.6.0\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nCreate a litellm project\r\npip install openlit\r\n\r\n### üéØ What Did You Expect?\r\nInstallation to be completed.\r\n\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: 1.16.0\r\n- Deployment Method: Docker",
      "state": "closed",
      "author": "aluferraz",
      "author_type": "User",
      "created_at": "2024-07-24T11:50:37Z",
      "updated_at": "2024-07-31T15:45:39Z",
      "closed_at": "2024-07-31T15:45:39Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/339/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/339",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/339",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.114131",
      "comments": [
        {
          "author": "patcher9",
          "body": "> openlit 1.16.0 depends on tiktoken<0.7.0 and >=0.6.0\r\n\r\n\r\nHey so we don't have this dependency as such as you can see https://github.com/openlit/openlit/blob/87be86c925a0bd213d48a69e8f7f3a847a46b807/sdk/python/pyproject.toml#L15",
          "created_at": "2024-07-24T12:40:44Z"
        },
        {
          "author": "patcher9",
          "body": "https://github.com/openlit/openlit/pull/340\r\n\r\nI have created this PR to anycase use 0.0.7 as the version for tiktoken",
          "created_at": "2024-07-24T12:44:14Z"
        }
      ]
    },
    {
      "issue_number": 318,
      "title": "[Bug]: Rename Span attribute `gen_ai.response.finish_reason` to  `gen_ai.response.finish_reasons` and type array",
      "body": "### üêõ What's Going Wrong?\r\n>  Explain the bug. Tell us what's happening that shouldn't be.\r\nhttps://github.com/openlit/openlit/issues/316 - @codefromthecrypt makes a good point the fix to be made in `gen_ai.response.finish_reason` attribute\r\n\r\n**Changes needed:**\r\n- `gen_ai.response.finish_reason` to `gen_ai.response.finish_reasons`\r\n- `gen_ai.response.finish_reasons` type should be an array\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  To see the bug, what steps should we follow?\r\nNA\r\n\r\n### üéØ What Did You Expect?\r\n>  Describe what you thought would happen.\r\nNA\r\n\r\n### üì∏ Any Screenshots?\r\n>  Pictures can say a thousand words and can be super helpful!\r\nNA\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [e.g., 0.0.1]\r\n- OpenLIT SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Docker]\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-07-07T17:43:29Z",
      "updated_at": "2024-07-30T06:27:28Z",
      "closed_at": "2024-07-30T06:27:28Z",
      "labels": [
        ":bug: Bug",
        "openlit-python",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/318/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/318",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/318",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.305630",
      "comments": []
    },
    {
      "issue_number": 324,
      "title": "[Bug]:Empty panels on SigNoz dashboards",
      "body": "### üêõ What's Going Wrong?\r\n>  Explain the bug. Tell us what's happening that shouldn't be.\r\n\r\nProblem related on:\r\nhttps://openlit.slack.com/archives/C0712FM210U/p1720553279900999\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n>  To see the bug, what steps should we follow?\r\n\r\n### üéØ What Did You Expect?\r\n>  Describe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\n>  Pictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [e.g., 0.0.1]\r\n- OpenLIT SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Docker]\r\n\r\n### üìù Additional Notes\r\n>  Got more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "dev-gabrielchaves",
      "author_type": "User",
      "created_at": "2024-07-09T20:39:10Z",
      "updated_at": "2024-07-30T06:26:12Z",
      "closed_at": "2024-07-30T06:26:12Z",
      "labels": [
        ":bug: Bug",
        "good first issue",
        ":raised_hand: Up for Grabs",
        "Connections"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/324/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/324",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/324",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.305673",
      "comments": [
        {
          "author": "patcher9",
          "body": "As discussed, This doesn't seem to be an issue as of now so closing this. Thanks for raising this!",
          "created_at": "2024-07-30T06:26:12Z"
        }
      ]
    },
    {
      "issue_number": 338,
      "title": "[Feat]: Add Exceptions Monitoring ",
      "body": "### üöÄ What's the Problem?\r\nAdd a page to view all the exception traces that occurred.\r\n\r\n\r\n### üí° Your Dream Solution\r\nI should be able to see exceptions with pagination and sorting by timestamp\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-07-23T10:31:23Z",
      "updated_at": "2024-07-25T08:00:44Z",
      "closed_at": "2024-07-25T08:00:44Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/338/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/338",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/338",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.527142",
      "comments": []
    },
    {
      "issue_number": 323,
      "title": "[Feat]:Add OpenTelemetry Instrumentation for ElevenLabs",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\n\r\nProblem related on slack: \r\nhttps://openlit.slack.com/archives/C0712FM210U/p1720553279900999\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "dev-gabrielchaves",
      "author_type": "User",
      "created_at": "2024-07-09T20:37:39Z",
      "updated_at": "2024-07-10T14:48:38Z",
      "closed_at": "2024-07-10T14:48:38Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/323/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/323",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/323",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.527166",
      "comments": []
    },
    {
      "issue_number": 316,
      "title": "[Bug]: incompatibilities with LLM semantics",
      "body": "### üêõ What's Going Wrong?\r\n\r\nAs a first timer, I tried the ollama instrumentation, and sent a trace to a local collector. Then I compared the output with [llm semantics defined by otel](https://opentelemetry.io/docs/specs/semconv/gen-ai/llm-spans/). I noticed some incompatibilities and a lot of attributes not yet defined.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\n\r\nmake an ollama chat instrumented with openlit and look at the trace data sent to the collector\r\n\r\n### üéØ What Did You Expect?\r\n\r\notel specs should be a subset of openlit semantics, so no incompatible attributes. I would expect an issue tracking incompatabilities.\r\n\r\n### üì∏ Any Screenshots?\r\n\r\nSemantically, we can only discuss [spans](https://opentelemetry.io/docs/specs/semconv/gen-ai/llm-spans) as there are not yet semantics defined for metrics.\r\n\r\ncompatible:\r\n* kind=client\r\n* name=ollama.chat\r\n* attributes['telemetry.sdk.name']='openlit' (defined in resource semantics)\r\n* attributes['gen_ai.system']='ollama'\r\n* attributes['gen_ai.request.model']='llama3'\r\n* attributes['gen_ai.usage.prompt_tokens']=31\r\n* attributes['gen_ai.usage.completion_tokens']=11\r\n\r\nincompatible:\r\n* attributes['gen_ai.prompt']='[chat](user: <|fim_prefix|>def hello_world():<|fim_suffix|><|fim_middle|>)' (should be an event attribute, not a span attribute)\r\n* attributes['gen_ai.completion']='def hello_world():' (should be an event attribute, not a span attribute)\r\n* attributes['gen_ai.response.finish_reason']='stop' (should be finish_reasons and an array)\r\n\r\nnot yet defined in the standard:\r\n* attributes['gen_ai.operation.name']='chat'\r\n* attributes['gen_ai.endpoint']='ollama.chat'\r\n* attributes['gen_ai.environment']='default'\r\n* attributes['gen_ai.application_name']='openlit-python-ollama'\r\n* attributes['gen_ai.request.is_stream']=false\r\n* attributes['gen_ai.usage.total_tokens']=43\r\n* attributes['gen_ai.usage.cost']=0\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [e.g., 0.0.1]\r\n- OpenLIT SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Docker]\r\n\r\n### üìù Additional Notes\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "codefromthecrypt",
      "author_type": "User",
      "created_at": "2024-07-06T04:57:26Z",
      "updated_at": "2024-07-07T17:44:36Z",
      "closed_at": "2024-07-07T17:44:36Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/316/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/316",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/316",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.527175",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @codefromthecrypt , As we have already discussed, we will be making these changes. I have created to seperate issues to track this. Closing this in favour of #317 and #318 \r\n\r\nThanks!",
          "created_at": "2024-07-07T17:44:36Z"
        }
      ]
    },
    {
      "issue_number": 309,
      "title": "[Feat]: Add a feature to share database config with other users",
      "body": "### üöÄ What's the Problem?\r\nRight now even if the database config is same the same team users will have to add it in order to view the dashboard. Enable the feature to share the database config with the other users.\r\n\r\n\r\n### üí° Your Dream Solution\r\nThe database config can be shared with other users with different permission to edit, delete or share it.\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-06-29T08:34:41Z",
      "updated_at": "2024-07-03T03:26:00Z",
      "closed_at": "2024-07-03T03:26:00Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/309/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/309",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/309",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.783345",
      "comments": []
    },
    {
      "issue_number": 305,
      "title": "[Bug]: Failed to instrument langchain: type object 'BaseLoader' has no attribute 'aload'",
      "body": "### üêõ What's Going Wrong?\r\n> When starting the langchain based application, getting the following error,  - \r\n   \r\n\"Failed to instrument langchain: type object 'BaseLoader' has no attribute 'aload'\"\r\n\r\nPossibly Impacting: Traces \r\n\r\n\r\n\r\nLog: \r\n----\r\nINFO:     Will watch for changes in these directories: ['/app']\r\nINFO:     Uvicorn running on http://0.0.0.0:443 (Press CTRL+C to quit)\r\nINFO:     Started reloader process [8] using StatReload\r\nFailed to instrument langchain: type object 'BaseLoader' has no attribute 'aload'\r\nINFO:     Started server process [14]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\n\r\n\r\n\r\n### üì∏ Any Screenshots?\r\nNo traces are displayed.\r\n<img width=\"830\" alt=\"Screenshot 2024-06-28 at 4 02 34 PM\" src=\"https://github.com/openlit/openlit/assets/49898295/39393439-666b-46f0-b4bf-3fa47033c7af\">\r\n\r\n\r\n\r\n### üíª Your Setup\r\n\r\nlangchain                                0.1.4\r\nlangchain-community                      0.0.14\r\nlangchain-core                           0.1.23\r\nlangchain-openai                         0.0.5\r\nlanggraph                                0.0.15\r\nlangsmith                                0.0.87\r\nopenai                                   1.35.7\r\nopenlit                                  1.14.1\r\n\r\n",
      "state": "closed",
      "author": "nkkrishnakfk",
      "author_type": "User",
      "created_at": "2024-06-28T10:33:37Z",
      "updated_at": "2024-07-01T16:52:41Z",
      "closed_at": "2024-07-01T16:52:41Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/305/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/305",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/305",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.783364",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @nkkrishnakfk Thanks for raising the issue. I think there is some issue with the 0.1.4 version as 0.1.1 works well for me. I am trying to resolve another langchain [issue](https://github.com/openlit/openlit/issues/297). Ill include this as part of my fix ",
          "created_at": "2024-07-01T13:05:30Z"
        },
        {
          "author": "patcher9",
          "body": "Okay I was able to replicate this, If you use `langchain-community >= 0.1.20` it should work as intended. Apologies on my behalf for not mentioning the correct version in our docs, Ill get it fixed now",
          "created_at": "2024-07-01T15:24:37Z"
        },
        {
          "author": "patcher9",
          "body": "Closing this issues as fixed in OpenLIT sdk version `1.14.2`\r\n\r\nIf its still relevant, Feel free to reopen\r\n\r\n",
          "created_at": "2024-07-01T16:52:41Z"
        }
      ]
    },
    {
      "issue_number": 308,
      "title": "[Feat]: Use Parent ID and show full trace when the user clicks on a request card",
      "body": "### üöÄ What's the Problem?\r\nShow parent trace if the request has parent span id attached to it\r\n\r\n\r\n### üí° Your Dream Solution\r\nI should be able to view the parent span trace for a request if its present\r\n\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\n\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\n\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-06-29T08:29:19Z",
      "updated_at": "2024-06-29T11:03:49Z",
      "closed_at": "2024-06-29T11:03:49Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/308/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/308",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/308",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.957098",
      "comments": []
    },
    {
      "issue_number": 132,
      "title": "[Feat]: Add Evaluation and Tuning Playground",
      "body": "### üöÄ What's the Problem?\nCurrently we cant experiment quickly and compare different models side-by-side on their response and related metrics like token usage, cost and latency\n\n### üí° Your Dream Solution\nAdd a playground where users can compare between different models from different providers side by siude on the same set of prompts.\n\n### ü§î Seen anything similar?\nOpenAI Playground but think of it for multiple model providers and for testing a single prompt rather than the whole chat\n\n### üñºÔ∏è Pictures or Drawings\nNA\n\n### üëê Want to Help Make It Happen?\n- [x] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-21T17:02:34Z",
      "updated_at": "2024-06-16T19:34:18Z",
      "closed_at": "2024-06-16T19:34:18Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/132/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/132",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/132",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.957129",
      "comments": []
    },
    {
      "issue_number": 267,
      "title": "[Question]: maybe `reset_to_defaults()` do not call `fetch_pricing_info()`",
      "body": "\r\nIn the OpenlitConfig, should `reset_to_defaults()`  change `cls.pricing_info = fetch_pricing_info()` to `cls.pricing_info = {}` ?\r\nIt seems that `update_config()` will still be called `fetch_pricing_info()` again.\r\n\r\nhttps://github.com/openlit/openlit/blob/86dde16719bccc2bbbd56bf33d7005b58c086da6/sdk/python/src/openlit/__init__.py#L69\r\n\r\n",
      "state": "closed",
      "author": "CherishCai",
      "author_type": "User",
      "created_at": "2024-06-06T09:55:52Z",
      "updated_at": "2024-06-14T07:56:52Z",
      "closed_at": "2024-06-14T07:56:52Z",
      "labels": [
        ":question: Question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/267/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/267",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/267",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:33.957138",
      "comments": [
        {
          "author": "patcher9",
          "body": "Hey @CherishCai \r\n\r\n`fetch_pricing_info()` would still be called once and if empty it would return {}. We could reset_to_defaults() but that technically what `fetch_pricing_info()` covers considering its returns {} if the pricing cant be found",
          "created_at": "2024-06-10T05:26:28Z"
        },
        {
          "author": "CherishCai",
          "body": "@patcher9 Thanks for your reply.\r\nIt seem to be twice HTTP remote calls now, so look forward to reducing one.",
          "created_at": "2024-06-10T05:40:14Z"
        },
        {
          "author": "patcher9",
          "body": "Do you wanna prepare a PR for you idea?\r\n",
          "created_at": "2024-06-10T09:55:03Z"
        },
        {
          "author": "CherishCai",
          "body": "Oh, it's an honor to be a contributor when I get to know `openlit` better.",
          "created_at": "2024-06-11T03:03:58Z"
        },
        {
          "author": "CherishCai",
          "body": "I debugged it locally, added `print()`, and saw that there were indeed two calls to `fetch_pricing_info()`. \r\n\r\nso will make a PR for `reset_to_defaults()` change `cls.pricing_info = fetch_pricing_info()` to `cls.pricing_info = {}`.\r\n\r\n![image](https://github.com/openlit/openlit/assets/16837364/91c4",
          "created_at": "2024-06-13T08:12:23Z"
        }
      ]
    },
    {
      "issue_number": 259,
      "title": "[Feat]: Add Validation workflow for Pricing JSON",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nChanges made to pricing json need to be validated manually which is problematic considering the size of the default LLM Pricing file\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nAdd a workflow for validating the LLM Pricing JSON\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-27T09:49:00Z",
      "updated_at": "2024-06-13T06:44:32Z",
      "closed_at": "2024-06-13T06:44:25Z",
      "labels": [
        ":rocket: Feature",
        "good first issue",
        ":raised_hand: Up for Grabs",
        "github_actions"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/259/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/259",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/259",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:34.161441",
      "comments": [
        {
          "author": "patcher9",
          "body": "Closed by commit https://github.com/openlit/openlit/commit/8ac5a445fbc01dc978c9acedb85b845114476eb9",
          "created_at": "2024-06-13T06:44:19Z"
        }
      ]
    },
    {
      "issue_number": 276,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring NVIDIA GPUs",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nFor self-hosted LLMs, Its critical to also understand the GPU usage along with the traces and metrics collected from the Application side.\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nAdd one click GPU Monitoring with OpenTelemetry\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-12T05:44:00Z",
      "updated_at": "2024-06-12T06:22:05Z",
      "closed_at": "2024-06-12T06:22:04Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/276/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/276",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/276",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:34.385060",
      "comments": []
    },
    {
      "issue_number": 24,
      "title": "Build a dashboard/Mixin for Grafana Cloud",
      "body": "### üöÄ What's the Problem?\r\nWhile Doku, is capable of exporting metrics to Grafana Cloud, we currently do not provide any premade dashboards or alert configurations for our users. This absence of initial setup tools forces users to spend additional time and effort crafting their own dashboards to monitor their data effectively. This step poses a challenge for users who may not be familiar with best practices for dashboard creation in Grafana, potentially leading to suboptimal monitoring setups.\r\n\r\n### üí° Your Dream Solution\r\nThe ideal solution would be the development of a comprehensive monitoring mixin for Grafana Cloud tailored specifically for Doku's exported metrics. This mixin would include:\r\n\r\n- **Predefined Dashboards:** Prebuilt dashboard to display LLM Monitoring data processed by Doku' in a clear, insightful manner. These dashboards would highlight key performance indicators and provide users with an immediate overview of their system's health.\r\n\r\n### ü§î Seen anything similar?\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-02T07:34:47Z",
      "updated_at": "2024-06-05T10:08:31Z",
      "closed_at": "2024-02-09T06:00:32Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/24/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/24",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/24",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:34.385083",
      "comments": [
        {
          "author": "Aparna-Tyagi",
          "body": "Hi, do we have monitoring mixin available for grafana cloud?",
          "created_at": "2024-06-05T10:08:30Z"
        }
      ]
    },
    {
      "issue_number": 263,
      "title": "[Feat]: Add pre-built LLM Observability dashboard for Elastic",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nWe don't currently have a pre-built dashboard for users exporting LLM monitoring data collected by OpenLIT to Elastic which puts the burden on the user to see traces and metrics manually and build a dashboard\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nSimilar to pre-built dashboard for Grafana Cloud, Signoz etc, Built a Elastic OOTB dashboard for users to monitor their LLM Applications\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-06-02T09:31:50Z",
      "updated_at": "2024-06-03T05:15:17Z",
      "closed_at": "2024-06-03T05:15:17Z",
      "labels": [
        ":book: Documentation",
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/263/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/263",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/263",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:34.619111",
      "comments": []
    },
    {
      "issue_number": 254,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring LLM Applications using GPT4All",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCurrently we cant track and monitor LLM Application using GPT4All using OpenLIT\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nAdd OpenTelemetry auto instrumentation and provider LLM Observability experience similar to Ollama for GPT4All based application\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-26T05:54:15Z",
      "updated_at": "2024-05-27T09:11:51Z",
      "closed_at": "2024-05-27T09:11:51Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/254/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/254",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/254",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381412",
      "comments": []
    },
    {
      "issue_number": 255,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring LLM Applications using EmbedChain",
      "body": "### üöÄ What's the Problem?\r\n<!-- Tell us what's not working right or what's getting in your way -->\r\nCurrently we cant track and monitor LLM Application using EmbedChain using OpenLIT\r\n\r\n### üí° Your Dream Solution\r\n<!-- Tell us what you wish for, with as much detail as you like -->\r\nAdd OpenTelemetry auto instrumentation and provide LLM Observability experience similar to LangChain for EmbedChain based application\r\n\r\n### ü§î Seen anything similar?\r\n<!-- Share other things you've tried or seen that are close to what you're thinking -->\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n<!-- Share any picture or drawing that shows your idea if you have -->\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-26T05:55:22Z",
      "updated_at": "2024-05-27T07:18:01Z",
      "closed_at": "2024-05-27T07:18:01Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/255/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/255",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/255",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381438",
      "comments": []
    },
    {
      "issue_number": 237,
      "title": "[Feat]: Add support for filters (cost, model etc) and sorting for the request list page",
      "body": "### üöÄ What's the Problem?\r\nThe request page right now lists the traces ordered by request time and on the basis of just date range. Add the feature to filter on the basis of cost, model etc and sort on the basis of other parameters.\r\n\r\n### üí° Your Dream Solution\r\nAble to filter & sort the traces on the basis of different parameters.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-05-20T08:33:35Z",
      "updated_at": "2024-05-26T05:14:32Z",
      "closed_at": "2024-05-26T05:14:32Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/237/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/237",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/237",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381448",
      "comments": []
    },
    {
      "issue_number": 240,
      "title": "[Feat]: Added ability to pass a custom URL to load the pricing JSON",
      "body": "### üöÄ What's the Problem?\r\nAs the default pricing JSON cannot cover Pricing for fine-tuned models, The cost is always 0 for fine tuned models or custom models which in not correct\r\n\r\n### üí° Your Dream Solution\r\nAbility to pass a custom Pricing JSON URL which users can use to add custom model names that they use. This would also enable users to remove unused models from pricing JSON increasing evaluation speed.\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-21T16:32:19Z",
      "updated_at": "2024-05-22T16:22:48Z",
      "closed_at": "2024-05-22T16:22:48Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/240/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/240",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/240",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381454",
      "comments": []
    },
    {
      "issue_number": 235,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring LLM Applications using Milvus",
      "body": "### üöÄ What's the Problem?\r\nOpenTelemetry Instrumentation for Monitoring Milvus usage.  \r\n\r\n### üí° Your Dream Solution\r\nSimilar OpenTelemetry Auto instrumentation for Milvus as provided for monitoring other LLM stack like ChromaDB and Pinecone\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-17T10:50:30Z",
      "updated_at": "2024-05-17T11:21:01Z",
      "closed_at": "2024-05-17T11:21:01Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/235/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/235",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/235",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381460",
      "comments": []
    },
    {
      "issue_number": 138,
      "title": "[Feat]: Add export Connection for OpenObserve",
      "body": "### üöÄ What's the Problem?\r\nAdd Connection for exportng LLM Monitoring metrics and logs to OpenObserve\r\n\r\n### üí° Your Dream Solution\r\nSimilar to the current setup for DataDog etc, Auto export all LLM Observability data from Ingester\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-23T18:16:28Z",
      "updated_at": "2024-05-17T10:55:08Z",
      "closed_at": "2024-04-18T19:56:21Z",
      "labels": [
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/138/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/138",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/138",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381466",
      "comments": []
    },
    {
      "issue_number": 204,
      "title": "[Feat]: Add Monitoring Integration for Amazon Bedrock",
      "body": "### üöÄ What's the Problem?\r\nUsers using Amazon Bedrock to call their base models like Claude currently cannot access monitor their LLM Applications properly using OpenLIT\r\n\r\n### üí° Your Dream Solution\r\nAdd a OpenTelemetry native Auto instrumentation for Amazon Bedrock usage via boto3\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-01T06:37:55Z",
      "updated_at": "2024-05-17T10:54:51Z",
      "closed_at": "2024-05-01T06:42:19Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/204/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/204",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/204",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381473",
      "comments": []
    },
    {
      "issue_number": 220,
      "title": "[Feat]: Add Connection for Monitoring LLM Applications using Prometheus and Grafana Tempo",
      "body": "### üöÄ What's the Problem?\r\nAdds Documentation for monitoring LLM Application using popular Observability tools like Prometheus and Grafana Tempo\r\n\r\n### üí° Your Dream Solution\r\nSimple Doc that explains how LLM Observability can be achieved using OpenLIT, Prometheus and Grafana Tempo \r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-09T10:22:43Z",
      "updated_at": "2024-05-17T10:54:27Z",
      "closed_at": "2024-05-09T10:41:05Z",
      "labels": [
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/220/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/220",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/220",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381478",
      "comments": []
    },
    {
      "issue_number": 228,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring LLM Applications using Groq",
      "body": "### üöÄ What's the Problem?\r\nOpenTelemetry Instrumentation for Monitoring Groq usage.  \r\n\r\n### üí° Your Dream Solution\r\nSimilar OpenTelemetry Auto instrumentation for Groq as provided for monitoring other LLM stack like OpenAI and Pinecone\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-16T06:26:31Z",
      "updated_at": "2024-05-17T10:53:57Z",
      "closed_at": "2024-05-16T06:50:36Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/228/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/228",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/228",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381483",
      "comments": []
    },
    {
      "issue_number": 231,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring LLM Applications using Qdrant",
      "body": "### üöÄ What's the Problem?\r\nOpenTelemetry Instrumentation for Monitoring Qdrant usage.  \r\n\r\n### üí° Your Dream Solution\r\nSimilar OpenTelemetry Auto instrumentation for Qdrant as provided for monitoring other LLM stack like ChromaDB and Pinecone\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-16T12:21:51Z",
      "updated_at": "2024-05-17T10:53:47Z",
      "closed_at": "2024-05-16T12:51:28Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/231/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/231",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/231",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381489",
      "comments": []
    },
    {
      "issue_number": 233,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring LLM Applications using Ollama",
      "body": "### üöÄ What's the Problem?\r\nOpenTelemetry Instrumentation for Monitoring Ollama usage.  \r\n\r\n### üí° Your Dream Solution\r\nSimilar OpenTelemetry Auto instrumentation for Ollama as provided for monitoring other LLM stack like ChromaDB and Pinecone\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-17T08:43:17Z",
      "updated_at": "2024-05-17T10:53:41Z",
      "closed_at": "2024-05-17T08:59:07Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/233/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/233",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/233",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381494",
      "comments": []
    },
    {
      "issue_number": 226,
      "title": "[Feat]: Support for Monitoring Haystack and LlamaIndex Frameworks",
      "body": "### üí° Your Dream Solution\r\nHaystack and LlamaIndex are prominent frameworks in the field of GenAI. Requesting support for these frameworks in OpenLIT.\r\n\r\nHaystack: https://github.com/deepset-ai/haystack\r\n\r\nLlamaIndex: https://github.com/run-llama/llama_index\r\n",
      "state": "closed",
      "author": "kjoth",
      "author_type": "User",
      "created_at": "2024-05-14T10:04:40Z",
      "updated_at": "2024-05-15T10:20:39Z",
      "closed_at": "2024-05-15T10:13:03Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/226/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/226",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/226",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.381522",
      "comments": [
        {
          "author": "patcher9",
          "body": "Thanks @kjoth \r\nThe integration has been tested and now available in openlit python sdk `1.3.0`",
          "created_at": "2024-05-15T10:20:38Z"
        }
      ]
    },
    {
      "issue_number": 223,
      "title": "[Feat]: Add Connection for Monitoring LLM Applications using Prometheus and Jaeger",
      "body": "### üöÄ What's the Problem?\r\nAdds Documentation for monitoring LLM Application using popular Observability tools like Prometheus and Jaeger\r\n\r\n### üí° Your Dream Solution\r\nSimple Doc that explains how LLM Observability can be achieved using OpenLIT, Prometheus and Jaeger\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-10T08:10:11Z",
      "updated_at": "2024-05-10T08:18:14Z",
      "closed_at": "2024-05-10T08:18:14Z",
      "labels": [
        ":book: Documentation",
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/223/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/223",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/223",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579599",
      "comments": []
    },
    {
      "issue_number": 217,
      "title": "[Feat]: Custom date range selector to get data",
      "body": "### üöÄ What's the Problem?\r\nCurrently we have predefined ranges to get the data.\r\n\r\n### üí° Your Dream Solution\r\nI would be able to select data on the basis of custom range.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-05-06T22:29:26Z",
      "updated_at": "2024-05-10T08:12:30Z",
      "closed_at": "2024-05-10T08:12:30Z",
      "labels": [
        ":rocket: Feature",
        "client",
        "javascript"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/217/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/217",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/217",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579620",
      "comments": []
    },
    {
      "issue_number": 216,
      "title": "[Bug]: Vector db request not displayed properly",
      "body": "### üêõ What's Going Wrong?\r\nVector db requests are not displayed properly on the request table and on the detail popover for request.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Start at '...'\r\n2. Click '...'\r\n3. Look for '...'\r\n4. Oops, there's the issue!\r\n\r\n### üéØ What Did You Expect?\r\nDescribe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\nPictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- OpenLIT Version: [e.g., 0.0.1]\r\n- OpenLIT SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Docker]\r\n\r\n### üìù Additional Notes\r\nGot more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-05-06T22:28:23Z",
      "updated_at": "2024-05-10T08:12:29Z",
      "closed_at": "2024-05-10T08:12:29Z",
      "labels": [
        ":bug: Bug",
        "client",
        "javascript"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/216/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/216",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/216",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579628",
      "comments": []
    },
    {
      "issue_number": 215,
      "title": "[Feat]: Add OpenTelemetry instrumentation for Monitoring LLM Applications using Vertex AI",
      "body": "### üöÄ What's the Problem?\r\nOpenTelemetry Instrumentation for Monitoring Vertex AI usage.  \r\n\r\n### üí° Your Dream Solution\r\nSimilar OpenTelemetry Auto instrumentation for Vertex AI as provided for monitoring other LLM stack like OpenAI and Pinecone\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2024-05-06T11:14:03Z",
      "updated_at": "2024-05-07T06:17:55Z",
      "closed_at": "2024-05-07T06:17:55Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "Integration"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/215/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher9"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/215",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/215",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579635",
      "comments": []
    },
    {
      "issue_number": 142,
      "title": "[Feat]: Support Cost Monitoring for fine-tuned LLM Models",
      "body": "### üöÄ What's the Problem?\r\nFine tuned models cannot be calculated currently in Doku\r\n\r\n### üí° Your Dream Solution\r\nShow costs even for fine-tuned models\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-24T09:33:54Z",
      "updated_at": "2024-05-01T06:42:52Z",
      "closed_at": "2024-05-01T06:42:52Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/142/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/142",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/142",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579644",
      "comments": []
    },
    {
      "issue_number": 133,
      "title": "[Feat]: Support Tracing for RAG based Applications",
      "body": "### üöÄ What's the Problem?\nCurrently for direct use of OpenAI and similar models, We are able to gather metrics. But when using a RAG based approach in our LLM Applications, We don't get a full overview of the steps taken by the model\n\n### üí° Your Dream Solution\nDokuMetry SDK has a flag for collecting OpenTelemetry based Traces and spans\n\n### ü§î Seen anything similar?\nOpenTelemetry libraries already support tracing and these can be visualised easily in any of the observability tools.\n\n### üñºÔ∏è Pictures or Drawings\nNA\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-21T17:13:32Z",
      "updated_at": "2024-04-18T19:56:39Z",
      "closed_at": "2024-04-18T19:56:39Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/133/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041",
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/133",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/133",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579653",
      "comments": []
    },
    {
      "issue_number": 134,
      "title": "[Feat]: Add Monitoring Integration for Pinecone",
      "body": "### üöÄ What's the Problem?\nWith Doku, We can easily monitor our LLM Applications, but other parts of the LLM Stack like Vector DBs are currently not supported\n\n### üí° Your Dream Solution\nSupport monitoring of vector DBs like Pinecone to begin with\n\n### ü§î Seen anything similar?\nPinecone already emits standard prometheus style metrics which can be used as is by Doku Ingester\n\n### üñºÔ∏è Pictures or Drawings\nNA\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-21T17:16:13Z",
      "updated_at": "2024-04-18T19:56:33Z",
      "closed_at": "2024-04-18T19:56:33Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "Connections"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/134/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041",
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/134",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/134",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579659",
      "comments": []
    },
    {
      "issue_number": 140,
      "title": "[Feat]: Add Monitoring Integration for LiteLLM framework",
      "body": "### üöÄ What's the Problem?\r\nCurrently we support LLM Observability integration when directly using official SDKs. LiteLLM is a popular choice when to use giving a unified usage for any LLM Model\r\n\r\n### üí° Your Dream Solution\r\nSimilar setup as OpenAI Observability, Build a LLM Observability integration for LangChain\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-23T18:23:54Z",
      "updated_at": "2024-04-18T19:55:54Z",
      "closed_at": "2024-04-18T19:55:54Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js",
        ":raised_hand: Up for Grabs",
        "Integration"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/140/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041",
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/140",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/140",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579666",
      "comments": []
    },
    {
      "issue_number": 87,
      "title": "[Feat]: Track OpenAI Usage with No-code",
      "body": "### üöÄ What's the Problem?\r\nOpenAI has a hidden API endpoint, We can make a curl request to get the token usage, number of requests and some basic details that are also showed on OpenAI Usage page. This is a sample curl request we can make to get this information\r\n\r\n```\r\ncurl -X GET 'https://api.openai.com/v1/usage?date=2024-03-02' \\\r\n-H \"Authorization: Bearer sk-xxxxxxxx\" \\\r\n-H \"Openai-Organization: org-xxxxxx\"\r\n```\r\n\r\n### üí° Your Dream Solution\r\n- Users who are not looking to get the information on request, prompts and inner details, Essentially using LLM tools where they have to pass OpenAI API key. They can pass the keys and orgs that they want details for and Ingester runs a cronjob very 24hours to get usage for the day and add it into the database.\r\n\r\n### ü§î Seen anything similar?\r\nllm.report has a similar functionality\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-04T08:00:42Z",
      "updated_at": "2024-04-18T19:55:20Z",
      "closed_at": "2024-04-18T19:55:20Z",
      "labels": [
        ":rocket: Feature"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/87/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/87",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/87",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579672",
      "comments": []
    },
    {
      "issue_number": 169,
      "title": "[Bug]: Failed clickhouse connection blocks the page",
      "body": "### üêõ What's Going Wrong?\r\nIf the active database config for the clickhouse datasource cannot be reached then also the other queries are still going which blocks the page for the data queries and multiple pending pooling connection are being requested. Hence optimise the application for checking the ping status for the active database beforehand to run the data queries only when the connection is established.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Add a wrong database config\r\n2. Switch to dashboard page\r\n3. Observe failed queries\r\n\r\n### üéØ What Did You Expect?\r\nDescribe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\nPictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- Doku Version: [e.g., 0.0.1]\r\n- DokuMetry SDK: [Python or NodeJS]\r\n- DokuMetry SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Linux, Windows, Docker]\r\n\r\n### üìù Additional Notes\r\nGot more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-04-05T20:07:16Z",
      "updated_at": "2024-04-06T11:57:12Z",
      "closed_at": "2024-04-06T11:57:12Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/169/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/169",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/169",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.579681",
      "comments": [
        {
          "author": "AmanAgarwal041",
          "body": "Wrong pr merge led to reopening of this issue.",
          "created_at": "2024-04-06T06:21:00Z"
        }
      ]
    },
    {
      "issue_number": 79,
      "title": "[Feat]: Implement state management tool app wide",
      "body": "### üöÄ What's the Problem?\r\nRecommended tool : [zustand](https://github.com/pmndrs/zustand)\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-29T21:41:03Z",
      "updated_at": "2024-04-06T06:21:11Z",
      "closed_at": "2024-04-06T06:21:11Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/79/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/79",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/79",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:36.781385",
      "comments": []
    },
    {
      "issue_number": 136,
      "title": "[Feat]: Reduce client docker image size using alpine",
      "body": "### üöÄ What's the Problem?\r\nDoku client Image is currently a whopping 2.14 GB in size. Reduce the size for this image using alpine images\r\n\r\n### üí° Your Dream Solution\r\nTo reduce the image size <200 MB\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-03-22T22:23:08Z",
      "updated_at": "2024-03-23T08:07:38Z",
      "closed_at": "2024-03-23T08:07:38Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/136/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/136",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/136",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777271",
      "comments": []
    },
    {
      "issue_number": 131,
      "title": "[Feat]: Reduce Doku Ingester Image size using Multi Stage Docker build and alpine image",
      "body": "### üöÄ What's the Problem?\r\nDoku Ingester Image is currently a whopping 1.2Gb in size. Reduce the size for this image using multi stage builds and alpine images\r\n\r\n### üí° Your Dream Solution\r\nImage size <100mi\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-21T11:53:14Z",
      "updated_at": "2024-03-23T07:54:10Z",
      "closed_at": "2024-03-23T07:54:10Z",
      "labels": [
        ":rocket: Feature"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/131/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/131",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/131",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777298",
      "comments": []
    },
    {
      "issue_number": 77,
      "title": "[Feat]: Add connections page for exporting functionality through ingester",
      "body": "### üöÄ What's the Problem?\r\nThrough this page, user can add connections to export the llm data to their respective platforms mentioned below : \r\n- [x] grafana\r\n- [x] newrelic\r\n- [x] datadog\r\n- [x] dynatrace\r\n- [x] signoz\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-29T21:37:48Z",
      "updated_at": "2024-03-20T21:48:06Z",
      "closed_at": "2024-03-20T21:43:12Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client",
        "Connections"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/77/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/77",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/77",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777310",
      "comments": []
    },
    {
      "issue_number": 110,
      "title": "[FEAT]: Add `GET` for `/api/connections` endpoint in Ingester",
      "body": "### üöÄ What's the Problem?\r\nCurrently, there's no straightforward way for users to verify or view the configuration details of the observability platform they have set up to send LLM observability data to from Doku. This lack of transparency can lead to confusion or misconfiguration issues, as users cannot easily confirm the current setup.\r\n\r\n### üí° Your Dream Solution\r\nI envision a robust GET endpoint that users can call to retrieve the current configuration details of their observability platform. This endpoint would ideally return information such as the platform name (e.g., Grafana, NewRelic), API keys, the URLs for metrics and logs endpoints, any relevant usernames, and timestamps showing when the configuration was last updated.\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNo specific diagrams provided, but imagine a simple JSON structure depicting a clear, hierarchical organization of configuration details like\r\n\r\n```json\r\n{\r\n    \"connections\": {\r\n        \"apiKey\": \"xxx\",\r\n        \"created_at\": \"2024-03-17T18:12:54Z\",\r\n        \"logsUrl\": \"https://logxxx.com\",\r\n        \"logsUsername\": \"123\",\r\n        \"metricsUrl\": \"https://metrics.com\",\r\n        \"metricsUsername\": \"123\",\r\n        \"platform\": \"grafana\"\r\n    },\r\n    \"status\": 200\r\n}\r\n```\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-17T18:44:02Z",
      "updated_at": "2024-03-17T18:53:04Z",
      "closed_at": "2024-03-17T18:53:04Z",
      "labels": [
        ":rocket: Feature"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/110/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/110",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/110",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777317",
      "comments": []
    },
    {
      "issue_number": 106,
      "title": "Feat: Add support for Mistral AI monitoring",
      "body": "### üöÄ What's the Problem?\r\nDoku currently supports OpenAI Monitoring along with Cohere and Anthropic. Monitoring LLM applications using Mistral AI dont have any support for monitoring\r\n\r\n### üí° Your Dream Solution\r\nSame level of support for monitoring Mistral AI based LLM Applications\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-14T08:14:20Z",
      "updated_at": "2024-03-17T12:22:56Z",
      "closed_at": "2024-03-17T12:22:56Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js",
        "client"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/106/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/106",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/106",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777324",
      "comments": []
    },
    {
      "issue_number": 107,
      "title": "Feat: Add support for Azure OpenAI monitoring",
      "body": "### üöÄ What's the Problem?\r\nDoku currently supports OpenAI Monitoring along with Cohere and Anthropic. Monitoring LLM applications using Azure OpenAI don't have any support for monitoring\r\n\r\n### üí° Your Dream Solution\r\nSame level of support for monitoring Azure OpenAI based LLM Applications\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-16T06:05:37Z",
      "updated_at": "2024-03-17T12:22:39Z",
      "closed_at": "2024-03-17T12:22:39Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js",
        "client"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/107/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/107",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/107",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777331",
      "comments": []
    },
    {
      "issue_number": 103,
      "title": "[Bug]: Minor UI fixes",
      "body": "### üêõ What's Going Wrong?\r\n- [x]  Logo on the Home page  is not a PNG\r\n- [x]  At the bottom, Slack Invite link seems to have expired, Update that to use our slack workspace URL?\r\n- [x]  Auth pages make the corners a bit more rounded\r\n- [x]  The Doku logo on dashboard fix on sidebar\r\n- [x]  In Profile page, Maybe add email as that what the user used to signin?\r\n- [x]  In request page, When I open Audio speech request it has `00` in between cost and source lang\r\n   \r\n![image](https://github.com/dokulabs/doku/assets/7565635/48f9ad45-4563-47a2-8c2b-95a3d1438857)\r\n\r\n\r\n- [x]  Why does the request page show page as 1 of 0, 2 of 0. Probably fix that\r\n   \r\n![image](https://github.com/dokulabs/doku/assets/7565635/0cf0e7f6-f54b-4d55-8c18-00a91da7451b)\r\n\r\n    \r\n- [x]  In request page, When opening a image request, It doesnt show the image\r\n  ![image](https://github.com/dokulabs/doku/assets/7565635/8b1e01d9-c405-4da6-9792-e6f1a730a9c2)\r\n    \r\n\r\n### Doku Ingester\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Start at '...'\r\n2. Click '...'\r\n3. Look for '...'\r\n4. Oops, there's the issue!\r\n\r\n### üéØ What Did You Expect?\r\nDescribe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\nPictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- Doku Version: [e.g., 0.0.1]\r\n- DokuMetry SDK: [Python or NodeJS]\r\n- DokuMetry SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Linux, Windows, Docker]\r\n\r\n### üìù Additional Notes\r\nGot more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-03-11T21:20:51Z",
      "updated_at": "2024-03-12T06:10:41Z",
      "closed_at": "2024-03-11T21:25:31Z",
      "labels": [
        ":bug: Bug",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/103/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/103",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/103",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777336",
      "comments": []
    },
    {
      "issue_number": 93,
      "title": "Feat: Update Anthropic usage monitoring in `DokuMetry`",
      "body": "### üöÄ What's the Problem?\r\nAnthropic has released an update to their APIs and SDKs which needs corresponding updates in DokuMetry\r\n\r\n### üí° Your Dream Solution\r\nSupprt `messages` with sync and async in `DokuMetry`\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-03-08T10:47:33Z",
      "updated_at": "2024-03-11T18:38:25Z",
      "closed_at": "2024-03-11T18:38:25Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/93/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/93",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/93",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777343",
      "comments": []
    },
    {
      "issue_number": 1,
      "title": "Support for `Async` OpenAI Python usage",
      "body": "### üöÄ What's the Problem?\r\nThe current DokuMetry Python SDK doesnt support Async OpenAI\r\n\r\n### üí° Your Dream Solution\r\nSimilar setup for Async OpenAI as we have for normal OpenAI usage\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-01-24T07:15:14Z",
      "updated_at": "2024-03-10T10:38:12Z",
      "closed_at": "2024-03-10T10:38:12Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/1/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/1",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/1",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777349",
      "comments": []
    },
    {
      "issue_number": 90,
      "title": "[Feat]: Add docker image to build client UI",
      "body": "### üöÄ What's the Problem?\r\nTell us what's not working right or what's getting in your way.\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-03-06T21:22:33Z",
      "updated_at": "2024-03-08T22:40:17Z",
      "closed_at": "2024-03-08T22:40:17Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/90/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/90",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/90",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777355",
      "comments": []
    },
    {
      "issue_number": 76,
      "title": "[Feat]: Add init db seed in case of local clickhouse installation",
      "body": "### üöÄ What's the Problem?\r\nAdd an init db seed for creating a user with local clickhouse db config. Update client readme for the same\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-29T21:35:09Z",
      "updated_at": "2024-03-04T19:59:28Z",
      "closed_at": "2024-03-04T19:59:28Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/76/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/76",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/76",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777362",
      "comments": []
    },
    {
      "issue_number": 82,
      "title": "[Feat]: Move to sqlite for doku UI client ",
      "body": "### üöÄ What's the Problem?\r\nThe problem right now if that if a user installs doku, one would end up using 4 services i.e. ingester, ui client, clickhouse and postgres. We want to keep it to limited resources, so we would be moving our ui client db to use sqlite.\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-03-02T09:13:48Z",
      "updated_at": "2024-03-03T21:04:42Z",
      "closed_at": "2024-03-03T21:04:42Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/82/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/82",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/82",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777367",
      "comments": []
    },
    {
      "issue_number": 72,
      "title": "[Feat]: Create a page to add database configuration",
      "body": "### üöÄ What's the Problem?\r\nProvide a mechanism to let users add their databases for the metrics\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-23T14:51:04Z",
      "updated_at": "2024-02-29T21:32:16Z",
      "closed_at": "2024-02-29T21:32:16Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/72/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/72",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/72",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777374",
      "comments": []
    },
    {
      "issue_number": 18,
      "title": "Add Getting Started Steps in the UI",
      "body": "### üöÄ What's the Problem?\r\nWhen we login into the Doku UI for the first time (When we haven't created an API Key or sent any data) the UI should show some instructions. The instructions should be such that all values are already added in the provided snippet. \r\n\r\n### üí° Your Dream Solution\r\nNA\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-01T09:59:52Z",
      "updated_at": "2024-02-29T21:21:42Z",
      "closed_at": "2024-02-29T21:21:41Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/18/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/18",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/18",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:38.777380",
      "comments": [
        {
          "author": "AmanAgarwal041",
          "body": "Duplicate issue https://github.com/dokulabs/doku/issues/71 . Its already addressed in https://github.com/dokulabs/doku/pull/73",
          "created_at": "2024-02-29T21:21:41Z"
        }
      ]
    },
    {
      "issue_number": 71,
      "title": "[Feat]: Getting started page",
      "body": "### üöÄ What's the Problem?\r\nTo define the steps to introduce dokumetry in user's backend and to determine whether the database is connected or not\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-23T14:49:53Z",
      "updated_at": "2024-02-23T20:44:08Z",
      "closed_at": "2024-02-23T20:44:08Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/71/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/71",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/71",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.001688",
      "comments": []
    },
    {
      "issue_number": 42,
      "title": "Fix: Request Page load time, Date and Time for each request and sub-text for ",
      "body": "### üêõ What's Going Wrong?\r\n\r\n- The load time for the request page is slow. The SQL Query itself returns pretty fast so probably the rendering of the UI elements is causing Load time issues\r\n- Set default time as 24h for both pages(Dashboard and Requests) and ideally have the time interval be carried between the two pages (same on visiting)\r\n- Add time to along with Date on each LLM request log panel\r\n- Add sub-text like how we do for total token, cost etc in the LLM request log panel\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nGo to the UI and checkout requests Page\r\n\r\n### üéØ What Did You Expect?\r\nExplained above\r\n\r\n### üì∏ Any Screenshots?\r\nNA\r\n\r\n### üíª Your Setup\r\n- Doku Version: [e.g., 0.0.1]\r\n- DokuMetry SDK: [Python or NodeJS]\r\n- DokuMetry SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Linux, Windows, Doker]\r\n\r\n### üìù Additional Notes\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-09T12:20:41Z",
      "updated_at": "2024-02-22T09:56:52Z",
      "closed_at": "2024-02-22T09:56:52Z",
      "labels": [
        ":bug: Bug",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/42/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/42",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/42",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.001716",
      "comments": [
        {
          "author": "AmanAgarwal041",
          "body": "Distributed issues : \r\n- https://github.com/dokulabs/doku/issues/68\r\n- https://github.com/dokulabs/doku/issues/69\r\n\r\n\r\nOther issues are already been taken care of üëç \r\n- Add time to along with Date on each LLM request log panel\r\n- Add sub-text like how we do for total token, cost etc in the LLM reque",
          "created_at": "2024-02-22T09:56:52Z"
        }
      ]
    },
    {
      "issue_number": 44,
      "title": "Feat: Add more panels to the Dashboard page",
      "body": "### üöÄ What's the Problem?\r\nThe LLM Monitoring dashboard currently only has a few panels which don't give much information\r\n\r\n### üí° Your Dream Solution\r\nMore panels like seen in the Grafana Dashboard we've built for export functionality\r\nhttps://docs.dokulabs.com/0.0.1/integrations/grafanaoss\r\n\r\n### ü§î Seen anything similar?\r\n![1](https://github.com/dokulabs/doku/blob/main/docs/images/grafana-dashboard-1.jpg?raw=true)\r\n![2](https://github.com/dokulabs/doku/blob/main/docs/images/grafana-dashboard-3.jpg?raw=true)\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShared above\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-09T12:33:56Z",
      "updated_at": "2024-02-22T09:43:47Z",
      "closed_at": "2024-02-22T09:43:47Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/44/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/44",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/44",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298832",
      "comments": []
    },
    {
      "issue_number": 35,
      "title": "Feat: add theming colors to whole project",
      "body": "### üöÄ What's the Problem?\r\nCurrently, our project's user interface lacks a cohesive color scheme that aligns with our logo. \r\n\r\n### üí° Your Dream Solution\r\nThe ideal solution would involve integrating a theming system throughout our application that is both dynamic and versatile, allowing for our primary, secondary, and any additional branding colors to be consistently applied across all UI components. This theming system would:\r\n\r\n- **Reflect Our Branding:** Seamlessly integrate our logo's color palette(orange) throughout the application's interface, creating a visually cohesive experience.\r\n- **Support Dark/Light Modes:** Include both light and dark mode support, ensuring colors adapt well to both themes without compromising readability or visual appeal.\r\n- **Customizable Components:** Ensure that UI components such as buttons, headers, backgrounds, and text elements can dynamically adapt to the defined theme.\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-06T21:14:52Z",
      "updated_at": "2024-02-17T20:24:24Z",
      "closed_at": "2024-02-17T20:24:24Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/35/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/35",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/35",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298849",
      "comments": []
    },
    {
      "issue_number": 34,
      "title": "Fix: loading states for different pages",
      "body": "### üêõ What's Going Wrong?\r\nThere are no loading states for :\r\n- [ ]  Login/logout Form\r\n- [ ] Dashboard page\r\n- [ ] All Request page\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Start at '...'\r\n2. Click '...'\r\n3. Look for '...'\r\n4. Oops, there's the issue!\r\n\r\n### üéØ What Did You Expect?\r\nDescribe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\nPictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- Doku Version: [e.g., 0.0.1]\r\n- DokuMetry SDK: [Python or NodeJS]\r\n- DokuMetry SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Linux, Windows, Doker]\r\n\r\n### üìù Additional Notes\r\nGot more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-06T21:14:01Z",
      "updated_at": "2024-02-17T20:24:23Z",
      "closed_at": "2024-02-17T20:24:23Z",
      "labels": [
        ":bug: Bug",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/34/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/34",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/34",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298856",
      "comments": []
    },
    {
      "issue_number": 33,
      "title": "Feat: Add a view to display request details on click of any request in all request page",
      "body": "### üöÄ What's the Problem?\r\nThere is no way to display all the details of a request\r\n\r\n### üí° Your Dream Solution\r\nDisplay data in similar kind of popover\r\n<img width=\"1324\" alt=\"Screenshot 2024-02-07 at 2 42 14 AM\" src=\"https://github.com/dokulabs/doku/assets/7565635/a80149c0-50af-4ba6-9f20-2b63e5b41580\">\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-06T21:12:41Z",
      "updated_at": "2024-02-17T20:24:23Z",
      "closed_at": "2024-02-17T20:24:23Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs",
        "client"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/33/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/33",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/33",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298862",
      "comments": []
    },
    {
      "issue_number": 61,
      "title": "Feat: Build a LLM Monitoring dashboard for New Relic",
      "body": "### üöÄ What's the Problem?\r\nWhile Doku, is capable of exporting metrics to New Relic, we currently do not provide any premade dashboards. This absence of initial setup tools forces users to spend additional time and effort crafting their own dashboards to monitor their data effectively. This step poses a challenge for users who may not be familiar with best practices for dashboard creation in New Relic, potentially leading to suboptimal monitoring setups.\r\n\r\n### üí° Your Dream Solution\r\nThe ideal solution would be the development of a comprehensive LLM monitoring dashboard for New Relic tailored specifically for Doku's exported metrics. The goal is to allow users to monitor LLM Applications witj:\r\n\r\n- **Predefined Dashboard:** Prebuilt dashboard to display LLM Monitoring data processed by Doku' in a clear, insightful manner. These dashboards would highlight key performance indicators and provide users with an immediate overview of their system's health.\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-13T06:14:53Z",
      "updated_at": "2024-02-13T06:21:44Z",
      "closed_at": "2024-02-13T06:21:44Z",
      "labels": [
        ":book: Documentation",
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/61/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/61",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/61",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298868",
      "comments": []
    },
    {
      "issue_number": 60,
      "title": "Feat: Build a LLM Monitoring dashboard for DataDog",
      "body": "### üöÄ What's the Problem?\r\nWhile Doku, is capable of exporting metrics to DataDog, we currently do not provide any premade dashboards. This absence of initial setup tools forces users to spend additional time and effort crafting their own dashboards to monitor their data effectively. This step poses a challenge for users who may not be familiar with best practices for dashboard creation in DataDog, potentially leading to suboptimal monitoring setups.\r\n\r\n### üí° Your Dream Solution\r\nThe ideal solution would be the development of a comprehensive LLM monitoring dashboard for DataDog tailored specifically for Doku's exported metrics. The goal is to allow users to monitor LLM Applications witj:\r\n\r\n- **Predefined Dashboard:** Prebuilt dashboard to display LLM Monitoring data processed by Doku' in a clear, insightful manner. These dashboards would highlight key performance indicators and provide users with an immediate overview of their system's health.\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-13T06:13:54Z",
      "updated_at": "2024-02-13T06:21:44Z",
      "closed_at": "2024-02-13T06:21:44Z",
      "labels": [
        ":book: Documentation",
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/60/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/60",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/60",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298874",
      "comments": []
    },
    {
      "issue_number": 59,
      "title": "Feat: Add `platform`, `generation` and `job.Name` labels to LLM Observability data exported to DataDog and New Relic",
      "body": "### üêõ What's Going Wrong?\r\nWhen sending LLM Monitoring data to DataDog and New Relic, We don't extract the `platform` and `generation` type from the endpoint key which leads to difficulty when building and aggregating data by provider and generation in the dashboard\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Export monitoring data from LLM Applications to DataDog and New Relic via Doku Ingester\r\n\r\n### üéØ What Did You Expect?\r\nA single label to aggregate all metrics and also be able to drill down on metrics from `platform` and `generation`\r\n\r\n### üì∏ Any Screenshots?\r\nNA\r\n\r\n### üíª Your Setup\r\n- Doku Version: 0.0.3\r\n- DokuMetry SDK: Python/NodeJS\r\n- DokuMetry SDK Version: 0.0.4\r\n- Deployment Method: Helm\r\n\r\n### üìù Additional Notes\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-13T06:09:20Z",
      "updated_at": "2024-02-13T06:18:30Z",
      "closed_at": "2024-02-13T06:18:30Z",
      "labels": [
        ":rocket: Feature",
        "Connections"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/59/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/59",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/59",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298880",
      "comments": []
    },
    {
      "issue_number": 49,
      "title": "Track LLM Request ID as `llmReqId` ",
      "body": "### üöÄ What's the Problem?\r\n`dokumetry` SDKs currently do not track LLM Request ID and the ingester also does not store it in the `DOKU_LLM_DATA` table\r\n\r\n### üí° Your Dream Solution\r\nTrack LLM Request ID as `LLM_REQ_ID` \r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-10T08:52:19Z",
      "updated_at": "2024-02-11T04:53:14Z",
      "closed_at": "2024-02-11T04:53:14Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/49/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/49",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/49",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298888",
      "comments": []
    },
    {
      "issue_number": 47,
      "title": "Add LLM Monitoring Data retention policy to `DOKU_LLM_DATA` Table",
      "body": "### üöÄ What's the Problem?\r\nThe LLM Monitoring data stored in `DOKU_LLM_DATA` Table currently has no retention period which can lead to storage issue\r\n\r\n### üí° Your Dream Solution\r\nAdd data retention policy which can be user configured with a default of 6 months.\r\n\r\n### ü§î Seen anything similar?\r\nhttps://docs.timescale.com/use-timescale/latest/data-retention/create-a-retention-policy/\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-10T07:21:04Z",
      "updated_at": "2024-02-10T07:57:45Z",
      "closed_at": "2024-02-10T07:57:45Z",
      "labels": [
        ":rocket: Feature"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/47/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/47",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/47",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298895",
      "comments": []
    },
    {
      "issue_number": 40,
      "title": "Feat: Add temporary home page",
      "body": "### üöÄ What's the Problem?\r\nTell us what's not working right or what's getting in your way.\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-08T13:16:28Z",
      "updated_at": "2024-02-10T07:56:55Z",
      "closed_at": "2024-02-08T19:37:51Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/40/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/40",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/40",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298900",
      "comments": []
    },
    {
      "issue_number": 45,
      "title": "Feat: Add `created_on` fields to the `DOKU_APIKEYS` table",
      "body": "### üöÄ What's the Problem?\r\nThe `created_on` field is not stored when creating API Keys\r\n\r\n### üí° Your Dream Solution\r\n`created_on` field in the table\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-09T13:04:32Z",
      "updated_at": "2024-02-10T07:15:59Z",
      "closed_at": "2024-02-10T07:15:59Z",
      "labels": [
        ":rocket: Feature",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/45/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/45",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/45",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298905",
      "comments": []
    },
    {
      "issue_number": 43,
      "title": "Update: Fix `id` in `DOKU_DATA` table to be a UUID instead of `BIGSERIAL`",
      "body": "### üêõ What's Going Wrong?\r\nThe `id` field currently is a BIGSERIAL which can lead to security issues, Convert it to UUID\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nNA\r\n\r\n### üéØ What Did You Expect?\r\n`id` field to be UUID\r\n\r\n### üì∏ Any Screenshots?\r\nNA\r\n\r\n### üíª Your Setup\r\n- Doku Version: 0.0.1\r\n- DokuMetry SDK: Python\r\n- DokuMetry SDK Version: 0.0.3\r\n- Deployment Method: Helm\r\n\r\n### üìù Additional Notes\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-09T12:23:05Z",
      "updated_at": "2024-02-10T07:05:33Z",
      "closed_at": "2024-02-10T07:05:33Z",
      "labels": [
        ":bug: Bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/43/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/43",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/43",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.298911",
      "comments": [
        {
          "author": "patcher99",
          "body": "resolved in https://github.com/dokulabs/doku/commit/41fea603f786135fd1b273a40641e06551d5bea8",
          "created_at": "2024-02-10T07:05:24Z"
        }
      ]
    },
    {
      "issue_number": 39,
      "title": "Feat: Add `platform`, `generation` and `job` labels to LLM Observability data exported to Grafana Cloud",
      "body": "### üêõ What's Going Wrong?\r\nWhen sending LLM Monitoring data to Grafana Cloud, We don't extract the `platform` and `generation` type from the endpoint key which leads to difficulty when building and aggregating data by provider and generation in the dashboard\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Export monitoring data from LLM Applications to Grafana Cloud via Doku Ingester\r\n\r\n### üéØ What Did You Expect?\r\nA single label to aggregate all metrics and also be able to drill down on metrics from `platform` and `generation`\r\n\r\n### üì∏ Any Screenshots?\r\nNA\r\n\r\n### üíª Your Setup\r\n- Doku Version: 0.0.1\r\n- DokuMetry SDK: Python/NodeJS\r\n- DokuMetry SDK Version: 0.0.3\r\n- Deployment Method: Helm\r\n\r\n### üìù Additional Notes\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-08T05:50:35Z",
      "updated_at": "2024-02-08T10:46:10Z",
      "closed_at": "2024-02-08T10:46:10Z",
      "labels": [
        ":bug: Bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/39/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/39",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/39",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.523272",
      "comments": []
    },
    {
      "issue_number": 19,
      "title": "Add a page for API Keys in the UI",
      "body": "### üöÄ What's the Problem?\r\nAdd a page in the UI for creating and Deleting API Keys\r\n### üí° Your Dream Solution\r\nThe API Key should be visible only for the first time, and should be masked in the UI\r\n\r\n### ü§î Seen anything similar?\r\nOpenAI UI has a good page for managing API Keys\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-01T10:04:15Z",
      "updated_at": "2024-02-06T23:15:35Z",
      "closed_at": "2024-02-06T23:15:35Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/19/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/19",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/19",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.523298",
      "comments": []
    },
    {
      "issue_number": 20,
      "title": "Add a page for Tracking all requests",
      "body": "### üöÄ What's the Problem?\r\nAdd a page for tracking all Requests makde to OpenAI, Anthropic and Cohere\r\n\r\n### üí° Your Dream Solution\r\nUsers should be able to filter by Platform, Category, Application and Environment\r\n\r\n### ü§î Seen anything similar?\r\nNA\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-01T10:06:32Z",
      "updated_at": "2024-02-06T21:15:09Z",
      "closed_at": "2024-02-06T21:15:09Z",
      "labels": [
        ":rocket: Feature",
        "client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/20/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/20",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/20",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.523306",
      "comments": []
    },
    {
      "issue_number": 28,
      "title": "Build a dashboard/Mixin for Grafana OSS",
      "body": "### üöÄ What's the Problem?\r\nTell us what's not working right or what's getting in your way.\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-04T11:56:52Z",
      "updated_at": "2024-02-04T11:57:53Z",
      "closed_at": "2024-02-04T11:57:52Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/28/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/28",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/28",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.523315",
      "comments": []
    },
    {
      "issue_number": 17,
      "title": "Add `id` and `llm_api_id` to Database Schema",
      "body": null,
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-02-01T09:53:54Z",
      "updated_at": "2024-02-04T11:57:28Z",
      "closed_at": "2024-02-04T11:57:28Z",
      "labels": [
        ":rocket: Feature",
        "openlit-python",
        "openlit-js"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/17/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/17",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/17",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:39.523323",
      "comments": []
    },
    {
      "issue_number": 22,
      "title": "[Feature]: Integrate Metrics APIs for dashboard",
      "body": "### üöÄ What's the Problem?\r\nIntegrate the APIs mentioned in the issue : https://github.com/dokulabs/doku/issues/7 to display UI on the dashboard screen\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [ ] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-02-01T19:43:38Z",
      "updated_at": "2024-02-01T20:46:35Z",
      "closed_at": "2024-02-01T20:46:35Z",
      "labels": [
        ":rocket: Feature",
        ":raised_hand: Up for Grabs"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/22/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/22",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/22",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:40.991282",
      "comments": []
    },
    {
      "issue_number": 10,
      "title": "Add retries to Database connection in Ingester",
      "body": "### üêõ What's Going Wrong?\r\nWhen the database is not running or currently not available, The Initialization in Ingester fails and restarts the whole process\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Install Ingester without TimescaleDB access\r\n\r\n### üéØ What Did You Expect?\r\nIngester should have atleast 5 retries before exiting the whole process\r\n\r\n### üì∏ Any Screenshots?\r\nNA\r\n\r\n### üíª Your Setup\r\n- Doku Version: 0.0.1\r\n- DokuMetry SDK: NA\r\n- DokuMetry SDK Version: NA\r\n- Deployment Method: Helm\r\n\r\n### üìù Additional Notes\r\nNA\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "patcher99",
      "author_type": "User",
      "created_at": "2024-01-28T12:49:40Z",
      "updated_at": "2024-02-01T11:12:39Z",
      "closed_at": "2024-02-01T11:12:39Z",
      "labels": [
        ":bug: Bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/10/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "patcher99"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/10",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/10",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:40.991310",
      "comments": []
    },
    {
      "issue_number": 7,
      "title": "Create apis for different stats",
      "body": "### üöÄ What's the Problem?\n- [x]  Finding Total Requests\n- [x]  Finding Request per time\n- [x]  Finding Avg Request Duration\n- [x]  Finding Total Cost\n- [x]  Finding Avg Cost per request\n- [x]  Finding Top models\n- [x]  Finding Distribution by generation categories\n\n### üí° Your Dream Solution\nTell us what you wish for, with as much detail as you like.\n\n### ü§î Seen anything similar?\nShare other things you've tried or seen that are close to what you're thinking.\n\n### üñºÔ∏è Pictures or Drawings\nShare any picture or drawing that shows your idea if you have.\n\n### üëê Want to Help Make It Happen?\n- [ ] Yes, I'd like to volunteer and help out with this!\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-01-25T22:04:35Z",
      "updated_at": "2024-02-01T07:51:15Z",
      "closed_at": "2024-01-29T18:52:24Z",
      "labels": [
        ":rocket: Feature"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/7/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/7",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/7",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:40.991319",
      "comments": []
    },
    {
      "issue_number": 5,
      "title": "Fix redirection for auth and create common auth components",
      "body": "### üêõ What's Going Wrong?\r\nExplain the bug. Tell us what's happening that shouldn't be.\r\n\r\n### üïµÔ∏è Steps to Reproduce\r\nTo see the bug, what should we do?\r\n1. Start at '...'\r\n2. Click '...'\r\n3. Look for '...'\r\n4. Oops, there's the issue!\r\n\r\n### üéØ What Did You Expect?\r\nDescribe what you thought would happen.\r\n\r\n### üì∏ Any Screenshots?\r\nPictures can say a thousand words and can be super helpful!\r\n\r\n### üíª Your Setup\r\n- Doku Version: [e.g., 0.0.1]\r\n- DokuMetry SDK: [Python or NodeJS]\r\n- DokuMetry SDK Version: [e.g., 0.0.3]\r\n- Deployment Method: [Helm, Linux, Windows, Doker]\r\n\r\n### üìù Additional Notes\r\nGot more to say? Tell us here.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-01-25T21:45:11Z",
      "updated_at": "2024-01-28T07:02:45Z",
      "closed_at": "2024-01-28T07:02:45Z",
      "labels": [
        ":bug: Bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/5/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/5",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/5",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:40.991328",
      "comments": []
    },
    {
      "issue_number": 3,
      "title": "Create user login, register and logout",
      "body": "### üöÄ What's the Problem?\r\nTell us what's not working right or what's getting in your way.\r\n\r\n### üí° Your Dream Solution\r\nTell us what you wish for, with as much detail as you like.\r\n\r\n### ü§î Seen anything similar?\r\nShare other things you've tried or seen that are close to what you're thinking.\r\n\r\n### üñºÔ∏è Pictures or Drawings\r\nShare any picture or drawing that shows your idea if you have.\r\n\r\n### üëê Want to Help Make It Happen?\r\n- [x] Yes, I'd like to volunteer and help out with this!\r\n",
      "state": "closed",
      "author": "AmanAgarwal041",
      "author_type": "User",
      "created_at": "2024-01-24T15:34:17Z",
      "updated_at": "2024-01-25T21:25:42Z",
      "closed_at": "2024-01-25T21:25:42Z",
      "labels": [
        ":rocket: Feature"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/openlit/openlit/issues/3/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "AmanAgarwal041"
      ],
      "milestone": null,
      "html_url": "https://github.com/openlit/openlit/issues/3",
      "api_url": "https://api.github.com/repos/openlit/openlit/issues/3",
      "repository": "openlit/openlit",
      "extraction_date": "2025-06-22T00:51:40.991334",
      "comments": []
    }
  ]
}