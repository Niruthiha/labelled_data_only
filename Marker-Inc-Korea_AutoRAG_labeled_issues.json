{
  "repository": "Marker-Inc-Korea/AutoRAG",
  "repository_info": {
    "repo": "Marker-Inc-Korea/AutoRAG",
    "stars": 4046,
    "language": "Python",
    "description": "AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
    "url": "https://github.com/Marker-Inc-Korea/AutoRAG",
    "topics": [
      "analysis",
      "automl",
      "benchmarking",
      "document-parser",
      "embeddings",
      "evaluation",
      "llm",
      "llm-evaluation",
      "llm-ops",
      "open-source",
      "ops",
      "optimization",
      "pipeline",
      "python",
      "qa",
      "rag",
      "rag-evaluation",
      "retrieval-augmented-generation"
    ],
    "created_at": "2024-01-10T12:25:00Z",
    "updated_at": "2025-06-21T15:45:04Z",
    "search_query": "RAG retrieval augmented language:python stars:>3",
    "total_issues_estimate": 114,
    "labeled_issues_estimate": 79,
    "labeling_rate": 70.0,
    "sample_labeled": 21,
    "sample_total": 30,
    "has_issues": true,
    "repo_id": 741450201,
    "default_branch": "main",
    "size": 74300
  },
  "extraction_date": "2025-06-22T00:41:41.924646",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 399,
  "issues": [
    {
      "issue_number": 1115,
      "title": "[Feature Request]ollama ,huggingface",
      "body": "Add support for ollama and huggingface modeles",
      "state": "open",
      "author": "werruww",
      "author_type": "User",
      "created_at": "2025-06-08T22:36:36Z",
      "updated_at": "2025-06-21T00:30:19Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 37,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1115/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1115",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1115",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:40.205714",
      "comments": []
    },
    {
      "issue_number": 1117,
      "title": "[BUG] auto-rag.com site can’t be reached.",
      "body": "auto-rag.com site can’t be reached.\n\n안녕하세요.\nauto-rag.com 사이트가(도메인이) 잘못된 것 같습니다.\n접속이 안되네요. 주소 자체는 맞는 것 같은데 DNS_PROBE_FINISHED_NXDOMAIN 라고 나옵니다.\n확인 부탁드리겠습니다.\n```\nDNS records for auto-rag.com\n[Cloudflare](https://www.nslookup.io/domains/auto-rag.com/dns-records/#cloudflare)\n[Google DNS](https://www.nslookup.io/domains/auto-rag.com/dns-records/#google)\n[Authoritative](https://www.nslookup.io/domains/auto-rag.com/dns-records/#authoritative)\nControl D\n\nLocal DNS\n\nThe Google DNS server responded with these DNS records. Google will serve these records for as long as the time to live (TTL) has not expired. After this period, Google will update its cache by querying one of the authoritative name servers.\n\nA records\nNo A records found.\nAAAA records\nNo AAAA records found.\nCNAME record\nNo CNAME record found.\nTXT records\nNo TXT records found.\n```\n\n![Image](https://github.com/user-attachments/assets/ab712548-17cf-4c86-868d-51f09e4a06d1)\n\n![Image](https://github.com/user-attachments/assets/325cc431-63d6-4b89-9923-bdcedc28d191)\n\n수고하세요.",
      "state": "closed",
      "author": "thejjw",
      "author_type": "User",
      "created_at": "2025-06-20T06:23:13Z",
      "updated_at": "2025-06-20T06:33:00Z",
      "closed_at": "2025-06-20T06:26:52Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1117/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1117",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1117",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:40.205738",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@thejjw 안녕하세요. \nauto-rag.com 도메인이 만료되어, AutoRAG 문서는 https://marker-inc-korea.github.io/AutoRAG/ 해당 URL로 변경해 두었습니다.\n또한 메인페이지는 https://autorag.carrd.co/ 해당 URL에서 보실 수 있습니다.\n\n감사합니다.",
          "created_at": "2025-06-20T06:26:52Z"
        },
        {
          "author": "thejjw",
          "body": "빠른 코멘트 감사합니다.\n문서에서 해당 주소로 연결된 거라...PR 제출하였으니 참고하여 주시면 감사하겠습니다.\n수고하세요.",
          "created_at": "2025-06-20T06:33:00Z"
        }
      ]
    },
    {
      "issue_number": 636,
      "title": "Can I use certain tags from the ollama model in the generator node? (generator node에서 ollama 모델의 특정 태그를 사용 할 수 있나요?)",
      "body": "I want to use a specific ```model:tag``` provided by ollama.\r\n\r\nRef\r\n- https://ollama.com/library/qwen2:72b\r\n\r\nHowever, if i use the ollama specific tag as shown below, i got an error.\r\n```\r\n      modules:\r\n        - module_type: llama_index_llm\r\n          llm: ollama\r\n          model: [llama3, qwen2:72b]\r\n          temperature: 0.7\r\n          batch: 1\r\n```\r\n\r\n<details>\r\n<summary>error message</summary>\r\n\r\n```\r\n[08/22/24 12:35:30] ERROR    [__init__.py:73] >> Unexpected exception                                                         __init__.py:73\r\n                             ╭───────────────────────────── Traceback (most recent call last) \r\n\r\n.... \r\n                             ReadTimeout                                                                                                    \r\n                                                                                                                                            \r\n                             The above exception was the direct cause of the following exception:                                           \r\n                                                                                                                                            \r\n                             ╭───────────────────────────── Traceback (most recent call last) ──────────────────────────────╮               \r\n                             │ /root/workspace/corporate-llm/autorag/main.py:27 in <module>                                 │               \r\n                             │                                                                                              │               \r\n                             │   24                                                                                         │               \r\n                             │   25                                                                                         │               \r\n                             │   26 if __name__ == '__main__':                                                              │               \r\n                             │ ❱ 27 │   main()                                                                              │               \r\n                             │   28                                                                                         │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/click/core.py:1157 in __call__         │               \r\n                             │                                                                                              │               \r\n                             │   1154 │                                                                                     │               \r\n                             │   1155 │   def __call__(self, *args: t.Any, **kwargs: t.Any) -> t.Any:                       │               \r\n                             │   1156 │   │   \"\"\"Alias for :meth:`main`.\"\"\"                                                 │               \r\n                             │ ❱ 1157 │   │   return self.main(*args, **kwargs)                                             │               \r\n                             │   1158                                                                                       │               \r\n                             │   1159                                                                                       │               \r\n                             │   1160 class Command(BaseCommand):                                                           │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/click/core.py:1078 in main             │               \r\n                             │                                                                                              │               \r\n                             │   1075 │   │   try:                                                                          │               \r\n                             │   1076 │   │   │   try:                                                                      │               \r\n                             │   1077 │   │   │   │   with self.make_context(prog_name, args, **extra) as ctx:              │               \r\n                             │ ❱ 1078 │   │   │   │   │   rv = self.invoke(ctx)                                             │               \r\n                             │   1079 │   │   │   │   │   if not standalone_mode:                                           │               \r\n                             │   1080 │   │   │   │   │   │   return rv                                                     │               \r\n                             │   1081 │   │   │   │   │   # it's not safe to `ctx.exit(rv)` here!                           │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/click/core.py:1434 in invoke           │               \r\n                             │                                                                                              │               \r\n                             │   1431 │   │   │   echo(style(message, fg=\"red\"), err=True)                                  │               \r\n                             │   1432 │   │                                                                                 │               \r\n                             │   1433 │   │   if self.callback is not None:                                                 │               \r\n                             │ ❱ 1434 │   │   │   return ctx.invoke(self.callback, **ctx.params)                            │               \r\n                             │   1435 │                                                                                     │               \r\n                             │   1436 │   def shell_complete(self, ctx: Context, incomplete: str) -> t.List[\"CompletionItem │               \r\n                             │   1437 │   │   \"\"\"Return a list of completions for the incomplete value. Looks               │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/click/core.py:783 in invoke            │               \r\n                             │                                                                                              │               \r\n                             │    780 │   │                                                                                 │               \r\n                             │    781 │   │   with augment_usage_errors(__self):                                            │               \r\n                             │    782 │   │   │   with ctx:                                                                 │               \r\n                             │ ❱  783 │   │   │   │   return __callback(*args, **kwargs)                                    │               \r\n                             │    784 │                                                                                     │               \r\n                             │    785 │   def forward(                                                                      │               \r\n                             │    786 │   │   __self, __cmd: \"Command\", *args: t.Any, **kwargs: t.Any  # noqa: B902         │               \r\n                             │                                                                                              │               \r\n                             │ /root/workspace/corporate-llm/autorag/main.py:23 in main                                     │               \r\n                             │                                                                                              │               \r\n                             │   20 │   if not os.path.exists(project_dir):                                                 │               \r\n                             │   21 │   │   os.makedirs(project_dir)                                                        │               \r\n                             │   22 │   evaluator = Evaluator(qa_data_path, corpus_data_path, project_dir=project_dir)      │               \r\n                             │ ❱ 23 │   evaluator.start_trial(config)                                                       │               \r\n                             │   24                                                                                         │               \r\n                             │   25                                                                                         │               \r\n                             │   26 if __name__ == '__main__':                                                              │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/evaluator.py:98 in start_trial │               \r\n                             │                                                                                              │               \r\n                             │    95 │   │   │   if i == 0:                                                                 │               \r\n                             │    96 │   │   │   │   previous_result = self.qa_data                                         │               \r\n                             │    97 │   │   │   logger.info(f'Running node line {node_line_name}...')                      │               \r\n                             │ ❱  98 │   │   │   previous_result = run_node_line(node_line, node_line_dir, previous_result) │               \r\n                             │    99 │   │   │                                                                              │               \r\n                             │   100 │   │   │   trial_summary_df = self._append_node_line_summary(node_line_name,          │               \r\n                             │       node_line_dir, trial_summary_df)                                                       │               \r\n                             │   101                                                                                        │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/node_line.py:45 in             │               \r\n                             │ run_node_line                                                                                │               \r\n                             │                                                                                              │               \r\n                             │   42 │                                                                                       │               \r\n                             │   43 │   summary_lst = []                                                                    │               \r\n                             │   44 │   for node in nodes:                                                                  │               \r\n                             │ ❱ 45 │   │   previous_result = node.run(previous_result, node_line_dir)                      │               \r\n                             │   46 │   │   node_summary_df = load_summary_file(os.path.join(node_line_dir, node.node_type, │               \r\n                             │      'summary.csv'))                                                                         │               \r\n                             │   47 │   │   best_node_row = node_summary_df.loc[node_summary_df['is_best']]                 │               \r\n                             │   48 │   │   summary_lst.append({                                                            │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/schema/node.py:57 in run       │               \r\n                             │                                                                                              │               \r\n                             │    54 │   def run(self, previous_result: pd.DataFrame, node_line_dir: str) -> pd.DataFrame:  │               \r\n                             │    55 │   │   logger.info(f'Running node {self.node_type}...')                               │               \r\n                             │    56 │   │   input_modules, input_params = self.get_param_combinations()                    │               \r\n                             │ ❱  57 │   │   return self.run_node(modules=input_modules,                                    │               \r\n                             │    58 │   │   │   │   │   │   │    module_params=input_params,                               │               \r\n                             │    59 │   │   │   │   │   │   │    previous_result=previous_result,                          │               \r\n                             │    60 │   │   │   │   │   │   │    node_line_dir=node_line_dir,                              │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/nodes/generator/run.py:43 in   │               \r\n                             │ run_generator_node                                                                           │               \r\n                             │                                                                                              │               \r\n                             │   40 │   │   raise ValueError(\"You must have 'generation_gt' column in qa.parquet.\")         │               \r\n                             │   41 │   generation_gt = list(map(lambda x: x.tolist(), qa_data['generation_gt'].tolist()))  │               \r\n                             │   42 │                                                                                       │               \r\n                             │ ❱ 43 │   results, execution_times = zip(*map(lambda x: measure_speed(                        │               \r\n                             │   44 │   │   x[0], project_dir=project_dir, previous_result=previous_result, **x[1]),        │               \r\n                             │   45 │   │   │   │   │   │   │   │   │   │   zip(modules, module_params)))                   │               \r\n                             │   46 │   average_times = list(map(lambda x: x / len(results[0]), execution_times))           │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/nodes/generator/run.py:43 in   │               \r\n                             │ <lambda>                                                                                     │               \r\n                             │                                                                                              │               \r\n                             │   40 │   │   raise ValueError(\"You must have 'generation_gt' column in qa.parquet.\")         │               \r\n                             │   41 │   generation_gt = list(map(lambda x: x.tolist(), qa_data['generation_gt'].tolist()))  │               \r\n                             │   42 │                                                                                       │               \r\n                             │ ❱ 43 │   results, execution_times = zip(*map(lambda x: measure_speed(                        │               \r\n                             │   44 │   │   x[0], project_dir=project_dir, previous_result=previous_result, **x[1]),        │               \r\n                             │   45 │   │   │   │   │   │   │   │   │   │   zip(modules, module_params)))                   │               \r\n                             │   46 │   average_times = list(map(lambda x: x / len(results[0]), execution_times))           │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/strategy.py:14 in              │               \r\n                             │ measure_speed                                                                                │               \r\n                             │                                                                                              │               \r\n                             │    11 │   Method for measuring execution speed of the function.                              │               \r\n                             │    12 │   \"\"\"                                                                                │               \r\n                             │    13 │   start_time = time.time()                                                           │               \r\n                             │ ❱  14 │   result = func(*args, **kwargs)                                                     │               \r\n                             │    15 │   end_time = time.time()                                                             │               \r\n                             │    16 │   return result, end_time - start_time                                               │               \r\n                             │    17                                                                                        │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/utils/util.py:56 in wrapper    │               \r\n                             │                                                                                              │               \r\n                             │    53 │   def decorator_result_to_dataframe(func: Callable):                                 │               \r\n                             │    54 │   │   @functools.wraps(func)                                                         │               \r\n                             │    55 │   │   def wrapper(*args, **kwargs) -> pd.DataFrame:                                  │               \r\n                             │ ❱  56 │   │   │   results = func(*args, **kwargs)                                            │               \r\n                             │    57 │   │   │   if len(column_names) == 1:                                                 │               \r\n                             │    58 │   │   │   │   df_input = {column_names[0]: results}                                  │               \r\n                             │    59 │   │   │   else:                                                                      │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/nodes/generator/base.py:43 in  │               \r\n                             │ wrapper                                                                                      │               \r\n                             │                                                                                              │               \r\n                             │   40 │   │   │   │   │   │   │   │    \"You can check valid llm names from                    │               \r\n                             │      autorag.generator_models.\")                                                             │               \r\n                             │   41 │   │   │   batch = kwargs.pop('batch', 16)                                             │               \r\n                             │   42 │   │   │   llm_instance = generator_models[llm](**kwargs)                              │               \r\n                             │ ❱ 43 │   │   │   result = func(prompts=prompts, llm=llm_instance, batch=batch)               │               \r\n                             │   44 │   │   │   del llm_instance                                                            │               \r\n                             │   45 │   │   │   return result                                                               │               \r\n                             │   46 │   │   else:                                                                           │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/nodes/generator/llama_index_ll │               \r\n                             │ m.py:30 in llama_index_llm                                                                   │               \r\n                             │                                                                                              │               \r\n                             │   27 │   \"\"\"                                                                                 │               \r\n                             │   28 │   tasks = [llm.acomplete(prompt) for prompt in prompts]                               │               \r\n                             │   29 │   loop = asyncio.get_event_loop()                                                     │               \r\n                             │ ❱ 30 │   results = loop.run_until_complete(process_batch(tasks, batch_size=batch))           │               \r\n                             │   31 │                                                                                       │               \r\n                             │   32 │   generated_texts = list(map(lambda x: x.text, results))                              │               \r\n                             │   33 │   tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)                   │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/asyncio/base_events.py:649 in run_until_complete     │               \r\n                             │                                                                                              │               \r\n                             │    646 │   │   if not future.done():                                                         │               \r\n                             │    647 │   │   │   raise RuntimeError('Event loop stopped before Future completed.')         │               \r\n                             │    648 │   │                                                                                 │               \r\n                             │ ❱  649 │   │   return future.result()                                                        │               \r\n                             │    650 │                                                                                     │               \r\n                             │    651 │   def stop(self):                                                                   │               \r\n                             │    652 │   │   \"\"\"Stop running the event loop.                                               │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/autorag/utils/util.py:273 in           │               \r\n                             │ process_batch                                                                                │               \r\n                             │                                                                                              │               \r\n                             │   270 │                                                                                      │               \r\n                             │   271 │   for i in range(0, len(tasks), batch_size):                                         │               \r\n                             │   272 │   │   batch = tasks[i:i + batch_size]                                                │               \r\n                             │ ❱ 273 │   │   batch_results = await asyncio.gather(*batch)                                   │               \r\n                             │   274 │   │   results.extend(batch_results)                                                  │               \r\n                             │   275 │                                                                                      │               \r\n                             │   276 │   return results                                                                     │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/llama_index/core/instrumentation/dispa │               \r\n                             │ tcher.py:290 in async_wrapper                                                                │               \r\n                             │                                                                                              │               \r\n                             │   287 │   │   │   │   tags=tags,                                                             │               \r\n                             │   288 │   │   │   )                                                                          │               \r\n                             │   289 │   │   │   try:                                                                       │               \r\n                             │ ❱ 290 │   │   │   │   result = await func(*args, **kwargs)                                   │               \r\n                             │   291 │   │   │   except BaseException as e:                                                 │               \r\n                             │   292 │   │   │   │   self.event(SpanDropEvent(span_id=id_, err_str=str(e)))                 │               \r\n                             │   293 │   │   │   │   self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err= │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:334 │               \r\n                             │ in wrapped_async_llm_predict                                                                 │               \r\n                             │                                                                                              │               \r\n                             │   331 │   │   │   │   )                                                                      │               \r\n                             │   332 │   │   │   │                                                                          │               \r\n                             │   333 │   │   │   │   try:                                                                   │               \r\n                             │ ❱ 334 │   │   │   │   │   f_return_val = await f(_self, *args, **kwargs)                     │               \r\n                             │   335 │   │   │   │   except BaseException as e:                                             │               \r\n                             │   336 │   │   │   │   │   callback_manager.on_event_end(                                     │               \r\n                             │   337 │   │   │   │   │   │   CBEventType.LLM,                                               │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/llama_index/llms/ollama/base.py:399 in │               \r\n                             │ acomplete                                                                                    │               \r\n                             │                                                                                              │               \r\n                             │   396 │   async def acomplete(                                                               │               \r\n                             │   397 │   │   self, prompt: str, formatted: bool = False, **kwargs: Any                      │               \r\n                             │   398 │   ) -> CompletionResponse:                                                           │               \r\n                             │ ❱ 399 │   │   return await achat_to_completion_decorator(self.achat)(prompt, **kwargs)       │               \r\n                             │   400 │                                                                                      │               \r\n                             │   401 │   @llm_completion_callback()                                                         │               \r\n                             │   402 │   def stream_complete(                                                               │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/llama_index/core/base/llms/generic_uti │               \r\n                             │ ls.py:221 in wrapper                                                                         │               \r\n                             │                                                                                              │               \r\n                             │   218 │   async def wrapper(prompt: str, **kwargs: Any) -> CompletionResponse:               │               \r\n                             │   219 │   │   # normalize input                                                              │               \r\n                             │   220 │   │   messages = prompt_to_messages(prompt)                                          │               \r\n                             │ ❱ 221 │   │   chat_response = await func(messages, **kwargs)                                 │               \r\n                             │   222 │   │   # normalize output                                                             │               \r\n                             │   223 │   │   return chat_response_to_completion_response(chat_response)                     │               \r\n                             │   224                                                                                        │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/llama_index/core/instrumentation/dispa │               \r\n                             │ tcher.py:290 in async_wrapper                                                                │               \r\n                             │                                                                                              │               \r\n                             │   287 │   │   │   │   tags=tags,                                                             │               \r\n                             │   288 │   │   │   )                                                                          │               \r\n                             │   289 │   │   │   try:                                                                       │               \r\n                             │ ❱ 290 │   │   │   │   result = await func(*args, **kwargs)                                   │               \r\n                             │   291 │   │   │   except BaseException as e:                                                 │               \r\n                             │   292 │   │   │   │   self.event(SpanDropEvent(span_id=id_, err_str=str(e)))                 │               \r\n                             │   293 │   │   │   │   self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err= │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:76  │               \r\n                             │ in wrapped_async_llm_chat                                                                    │               \r\n                             │                                                                                              │               \r\n                             │    73 │   │   │   │   │   },                                                                 │               \r\n                             │    74 │   │   │   │   )                                                                      │               \r\n                             │    75 │   │   │   │   try:                                                                   │               \r\n                             │ ❱  76 │   │   │   │   │   f_return_val = await f(_self, messages, **kwargs)                  │               \r\n                             │    77 │   │   │   │   except BaseException as e:                                             │               \r\n                             │    78 │   │   │   │   │   callback_manager.on_event_end(                                     │               \r\n                             │    79 │   │   │   │   │   │   CBEventType.LLM,                                               │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/llama_index/llms/ollama/base.py:369 in │               \r\n                             │ achat                                                                                        │               \r\n                             │                                                                                              │               \r\n                             │   366 │   │                                                                                  │               \r\n                             │   367 │   │   tools = kwargs.pop(\"tools\", None)                                              │               \r\n                             │   368 │   │                                                                                  │               \r\n                             │ ❱ 369 │   │   response = await self.async_client.chat(                                       │               \r\n                             │   370 │   │   │   model=self.model,                                                          │               \r\n                             │   371 │   │   │   messages=ollama_messages,                                                  │               \r\n                             │   372 │   │   │   stream=False,                                                              │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/ollama/_client.py:653 in chat          │               \r\n                             │                                                                                              │               \r\n                             │    650 │     if images := message.get('images'):                                             │               \r\n                             │    651 │   │   message['images'] = [_encode_image(image) for image in images]                │               \r\n                             │    652 │                                                                                     │               \r\n                             │ ❱  653 │   return await self._request_stream(                                                │               \r\n                             │    654 │     'POST',                                                                         │               \r\n                             │    655 │     '/api/chat',                                                                    │               \r\n                             │    656 │     json={                                                                          │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/ollama/_client.py:517 in               │               \r\n                             │ _request_stream                                                                              │               \r\n                             │                                                                                              │               \r\n                             │    514 │   if stream:                                                                        │               \r\n                             │    515 │     return await self._stream(*args, **kwargs)                                      │               \r\n                             │    516 │                                                                                     │               \r\n                             │ ❱  517 │   response = await self._request(*args, **kwargs)                                   │               \r\n                             │    518 │   return response.json()                                                            │               \r\n                             │    519                                                                                       │               \r\n                             │    520   @overload                                                                           │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/ollama/_client.py:482 in _request      │               \r\n                             │                                                                                              │               \r\n                             │    479 │   super().__init__(httpx.AsyncClient, host, **kwargs)                               │               \r\n                             │    480                                                                                       │               \r\n                             │    481   async def _request(self, method: str, url: str, **kwargs) -> httpx.Response:        │               \r\n                             │ ❱  482 │   response = await self._client.request(method, url, **kwargs)                      │               \r\n                             │    483 │                                                                                     │               \r\n                             │    484 │   try:                                                                              │               \r\n                             │    485 │     response.raise_for_status()                                                     │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/httpx/_client.py:1574 in request       │               \r\n                             │                                                                                              │               \r\n                             │   1571 │   │   │   timeout=timeout,                                                          │               \r\n                             │   1572 │   │   │   extensions=extensions,                                                    │               \r\n                             │   1573 │   │   )                                                                             │               \r\n                             │ ❱ 1574 │   │   return await self.send(request, auth=auth, follow_redirects=follow_redirects) │               \r\n                             │   1575 │                                                                                     │               \r\n                             │   1576 │   @asynccontextmanager                                                              │               \r\n                             │   1577 │   async def stream(                                                                 │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/httpx/_client.py:1661 in send          │               \r\n                             │                                                                                              │               \r\n                             │   1658 │   │                                                                                 │               \r\n                             │   1659 │   │   auth = self._build_request_auth(request, auth)                                │               \r\n                             │   1660 │   │                                                                                 │               \r\n                             │ ❱ 1661 │   │   response = await self._send_handling_auth(                                    │               \r\n                             │   1662 │   │   │   request,                                                                  │               \r\n                             │   1663 │   │   │   auth=auth,                                                                │               \r\n                             │   1664 │   │   │   follow_redirects=follow_redirects,                                        │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/httpx/_client.py:1689 in               │               \r\n                             │ _send_handling_auth                                                                          │               \r\n                             │                                                                                              │               \r\n                             │   1686 │   │   │   request = await auth_flow.__anext__()                                     │               \r\n                             │   1687 │   │   │                                                                             │               \r\n                             │   1688 │   │   │   while True:                                                               │               \r\n                             │ ❱ 1689 │   │   │   │   response = await self._send_handling_redirects(                       │               \r\n                             │   1690 │   │   │   │   │   request,                                                          │               \r\n                             │   1691 │   │   │   │   │   follow_redirects=follow_redirects,                                │               \r\n                             │   1692 │   │   │   │   │   history=history,                                                  │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/httpx/_client.py:1726 in               │               \r\n                             │ _send_handling_redirects                                                                     │               \r\n                             │                                                                                              │               \r\n                             │   1723 │   │   │   for hook in self._event_hooks[\"request\"]:                                 │               \r\n                             │   1724 │   │   │   │   await hook(request)                                                   │               \r\n                             │   1725 │   │   │                                                                             │               \r\n                             │ ❱ 1726 │   │   │   response = await self._send_single_request(request)                       │               \r\n                             │   1727 │   │   │   try:                                                                      │               \r\n                             │   1728 │   │   │   │   for hook in self._event_hooks[\"response\"]:                            │               \r\n                             │   1729 │   │   │   │   │   await hook(response)                                              │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/httpx/_client.py:1763 in               │               \r\n                             │ _send_single_request                                                                         │               \r\n                             │                                                                                              │               \r\n                             │   1760 │   │   │   )                                                                         │               \r\n                             │   1761 │   │                                                                                 │               \r\n                             │   1762 │   │   with request_context(request=request):                                        │               \r\n                             │ ❱ 1763 │   │   │   response = await transport.handle_async_request(request)                  │               \r\n                             │   1764 │   │                                                                                 │               \r\n                             │   1765 │   │   assert isinstance(response.stream, AsyncByteStream)                           │               \r\n                             │   1766 │   │   response.request = request                                                    │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/httpx/_transports/default.py:372 in    │               \r\n                             │ handle_async_request                                                                         │               \r\n                             │                                                                                              │               \r\n                             │   369 │   │   │   content=request.stream,                                                    │               \r\n                             │   370 │   │   │   extensions=request.extensions,                                             │               \r\n                             │   371 │   │   )                                                                              │               \r\n                             │ ❱ 372 │   │   with map_httpcore_exceptions():                                                │               \r\n                             │   373 │   │   │   resp = await self._pool.handle_async_request(req)                          │               \r\n                             │   374 │   │                                                                                  │               \r\n                             │   375 │   │   assert isinstance(resp.stream, typing.AsyncIterable)                           │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/contextlib.py:153 in __exit__                        │               \r\n                             │                                                                                              │               \r\n                             │   150 │   │   │   │   # tell if we get the same exception back                               │               \r\n                             │   151 │   │   │   │   value = typ()                                                          │               \r\n                             │   152 │   │   │   try:                                                                       │               \r\n                             │ ❱ 153 │   │   │   │   self.gen.throw(typ, value, traceback)                                  │               \r\n                             │   154 │   │   │   except StopIteration as exc:                                               │               \r\n                             │   155 │   │   │   │   # Suppress StopIteration *unless* it's the same exception that         │               \r\n                             │   156 │   │   │   │   # was passed to throw().  This prevents a StopIteration                │               \r\n                             │                                                                                              │               \r\n                             │ /opt/conda/envs/llm_hugg/lib/python3.10/site-packages/httpx/_transports/default.py:86 in     │               \r\n                             │ map_httpcore_exceptions                                                                      │               \r\n                             │                                                                                              │               \r\n                             │    83 │   │   │   raise                                                                      │               \r\n                             │    84 │   │                                                                                  │               \r\n                             │    85 │   │   message = str(exc)                                                             │               \r\n                             │ ❱  86 │   │   raise mapped_exc(message) from exc                                             │               \r\n                             │    87                                                                                        │               \r\n                             │    88                                                                                        │               \r\n                             │    89 HTTPCORE_EXC_MAP = {                                                                   │               \r\n                             ╰──────────────────────────────────────────────────────────────────────────────────────────────╯               \r\n                             ReadTimeout                                                                                                    \r\nsys:1: RuntimeWarning: coroutine 'Dispatcher.span.<locals>.async_wrapper' was never awaited\r\n```\r\n\r\n</details>\r\n\r\nautorag works when i use untagged model\r\n```\r\n      modules:\r\n        - module_type: llama_index_llm\r\n          llm: ollama\r\n          model: llama3\r\n          temperature: 0.7\r\n          batch: 1\r\n```\r\n",
      "state": "closed",
      "author": "khlee369",
      "author_type": "User",
      "created_at": "2024-08-22T03:38:12Z",
      "updated_at": "2025-06-19T16:14:44Z",
      "closed_at": "2024-08-30T08:48:53Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/636/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/636",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/636",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:40.492763",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@khlee369 Thanks for the bug report. We will investigate why this isn't working",
          "created_at": "2024-08-22T05:15:45Z"
        },
        {
          "author": "khlee369",
          "body": "It seems that the issue is related to the default setting of ```DEFAULT_REQUEST_TIMEOUT = 30.0``` in ```llama_index.llms.ollama```. When loading a model with [ollama](https://ollama.com/) and performing inference, if the process takes longer than 30 seconds, the coroutine fails to await and the requ",
          "created_at": "2024-08-25T09:30:40Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Close this issue since this is not the code problem.\r\nYou have to set request_timeout more higher value.",
          "created_at": "2024-08-30T08:48:53Z"
        },
        {
          "author": "werruww",
          "body": "Can you write the codes that I used to run the library with Ulama, because I have been trying to run it for a while and it is not working?\n\n\n\nhttps://github.com/Marker-Inc-Korea/AutoRAG/issues/1115",
          "created_at": "2025-06-09T03:46:19Z"
        },
        {
          "author": "werruww",
          "body": "autorag evaluate ^\n  --qa_data_path ./qa_train.parquet ^\n  --corpus_data_path ./corpus.parquet ^\n  --project_dir AutoRAG ^\n  --config ./config.yaml\n\nImportError: cannot import name 'IncludeEnum' from 'chromadb.api.types'\n                             (C:\\ProgramData\\anaconda3\\envs\\2\\Lib\\site-packages",
          "created_at": "2025-06-09T03:47:20Z"
        }
      ]
    },
    {
      "issue_number": 1106,
      "title": "[Version Upgrade] Upgrade chroma version up to 1.0.0",
      "body": "Latest version looks not compatible with current AutoRAG system",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2025-04-24T02:58:12Z",
      "updated_at": "2025-05-04T05:49:06Z",
      "closed_at": "2025-05-04T05:49:06Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1106/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1106",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1106",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:40.717314",
      "comments": []
    },
    {
      "issue_number": 1096,
      "title": "[BUG] raise ValueError(\"The PDF parser must valorize the standard metadata.\") at table hybrid parse",
      "body": "**Describe the bug**\nAt the hybrid parse, the below error occured in the latest langchain-community package.\n\n```\nraise ValueError(\"The PDF parser must valorize the standard metadata.\")\n```\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Update `langchain-community` package to the latest.\n2. Run `test_table_hybrid_parse.py` test code.\n\n**Expected behavior**\nThe code will be run without error and success to the test.\n\n**Full Error log**\n```\n======================= 1 failed, 11 warnings in 10.83s ========================\n\n-------------------------------- live log call ---------------------------------\nINFO     AutoRAG:base.py:23 Running parser - table_hybrid_parse module...\nFAILED                                                                   [100%][03/03/25 15:17:51] INFO     [__init__.py:81] >> You are using    __init__.py:81\n                             API version of AutoRAG.To use local                \n                             version, run pip install                           \n                             'AutoRAG[gpu]'                                     \n[03/03/25 15:17:55] INFO     [__init__.py:81] >> You are using    __init__.py:81\n                             API version of AutoRAG.To use local                \n                             version, run pip install                           \n                             'AutoRAG[gpu]'                                     \n\ntests/autorag/data/parse/test_table_hybrid_parse.py:32 (test_table_hybrid_parse_node)\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/jeffrey/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/jeffrey/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/Users/jeffrey/PycharmProjects/AutoRAG/autorag/autorag/data/parse/langchain_parse.py\", line 58, in langchain_parse_pure\n    documents = parse_instance.load()\n  File \"/Users/jeffrey/PycharmProjects/AutoRAG/venv310/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 31, in load\n    return list(self.lazy_load())\n  File \"/Users/jeffrey/PycharmProjects/AutoRAG/venv310/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py\", line 682, in lazy_load\n    yield from self.parser.lazy_parse(blob)\n  File \"/Users/jeffrey/PycharmProjects/AutoRAG/venv310/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 793, in lazy_parse\n    metadata=_validate_metadata(doc_metadata),\n  File \"/Users/jeffrey/PycharmProjects/AutoRAG/venv310/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 140, in _validate_metadata\n    raise ValueError(\"The PDF parser must valorize the standard metadata.\")\nValueError: The PDF parser must valorize the standard metadata.\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\n    def test_table_hybrid_parse_node():\n>   \tresult_df = table_hybrid_parse(hybrid_glob, file_type=\"pdf\", **table_hybrid_params)\n\nautorag/data/parse/test_table_hybrid_parse.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../autorag/autorag/utils/util.py:72: in wrapper\n    results = func(*args, **kwargs)\n../autorag/autorag/data/parse/base.py:65: in wrapper\n    result = func(data_path_list=data_paths, **kwargs)\n../autorag/autorag/data/parse/table_hybrid_parse.py:54: in table_hybrid_parse\n    text_results, text_file_path = get_each_module_result(\n../autorag/autorag/data/parse/table_hybrid_parse.py:123: in get_each_module_result\n    texts, path, _ = module_original(data_path_list, **module_params)\n../autorag/autorag/data/parse/langchain_parse.py:30: in langchain_parse\n    results = pool.starmap(\n../../../.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/pool.py:375: in starmap\n    return self._map_async(func, iterable, starmapstar, chunksize).get()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <multiprocessing.pool.MapResult object at 0x1682b3b50>, timeout = None\n\n    def get(self, timeout=None):\n        self.wait(timeout)\n        if not self.ready():\n            raise TimeoutError\n        if self._success:\n            return self._value\n        else:\n>           raise self._value\nE           ValueError: The PDF parser must valorize the standard metadata.\n\n../../../.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/pool.py:774: ValueError\n```\n\n**Code that bug is happened**\nGo to `langchain_parse.py`\n\n**Desktop (please complete the following information):**\n - OS: MacOS\n - Python version 3.10\n\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2025-03-03T06:27:07Z",
      "updated_at": "2025-04-28T09:36:14Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1096/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1096",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1096",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:40.717338",
      "comments": [
        {
          "author": "lalalalini",
          "body": "i have the same problem,have u solved? In my case ,i use scanned pdf, so i think maybe the parser can not extract the context.",
          "created_at": "2025-03-25T06:22:19Z"
        },
        {
          "author": "Gwenn-LR",
          "body": "> i have the same problem,have u solved? In my case ,i use scanned pdf, so i think maybe the parser can not extract the context.\n\nUploading `langchain-community` to version `0.3.22` (last version as this time) seems to fix the issue for me.",
          "created_at": "2025-04-28T09:34:57Z"
        }
      ]
    },
    {
      "issue_number": 1102,
      "title": "[BUG] docker compose up -d failed",
      "body": "**Describe the bug**\nrun `docker compose up -d` got:\n\n```\n2.945   Preparing metadata (setup.py): started\n3.035   Preparing metadata (setup.py): finished with status 'error'\n3.037   error: subprocess-exited-with-error\n3.037   \n3.037   × python setup.py egg_info did not run successfully.\n3.037   │ exit code: 1\n3.037   ╰─> [6 lines of output]\n3.037       Traceback (most recent call last):\n3.037         File \"<string>\", line 2, in <module>\n3.037         File \"<pip-setuptools-caller>\", line 34, in <module>\n3.037         File \"/tmp/pip-install-ontmg_bf/kiwipiepy_0a72cfa866634e7483c609898f62aa58/setup.py\", line 15, in <module>\n3.037           import numpy as np\n3.037       ModuleNotFoundError: No module named 'numpy'\n3.037       [end of output]\n3.037   \n3.037   note: This error originates from a subprocess, and is likely not a problem with pip.\n3.095 error: metadata-generation-failed\n3.095 \n3.095 × Encountered error while generating package metadata.\n3.095 ╰─> See above for output.\n3.095 \n3.095 note: This is an issue with the package mentioned above, not pip.\n3.095 hint: See above for details.\n------\n```\n\n\n**Code that bug is happened**\n\nincomplete setup in dockerfile of AutoRAG/api\n\n**Desktop (please complete the following information):**\n - OS: MacOS\n - Python version 3.10\n\n\n",
      "state": "open",
      "author": "MoeBuTa",
      "author_type": "User",
      "created_at": "2025-03-19T08:28:09Z",
      "updated_at": "2025-04-04T06:02:26Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1102/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1102",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1102",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:40.931394",
      "comments": [
        {
          "author": "MoeBuTa",
          "body": "can be fixed by updating dockerfile:\n\n```\nFROM python:3.10-slim\n\nWORKDIR /app\n\n# Install system dependencies + parsing dependencies\nRUN apt-get update && apt-get install -y \\\n    python3-pip \\\n    build-essential \\\n    libmagic-dev \\\n    libgl1-mesa-dev \\\n    libglib2.0-0 \\\n    poppler-utils \\\n    t",
          "created_at": "2025-03-19T08:30:15Z"
        },
        {
          "author": "alexkreidler",
          "body": "Thanks @MoeBuTa your fix worked for me! ",
          "created_at": "2025-03-26T19:30:57Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I think I must streamline and fix the installation process. \nThanks for reporting the bug.",
          "created_at": "2025-04-04T06:02:25Z"
        }
      ]
    },
    {
      "issue_number": 1105,
      "title": "[Feature Request] Ability to create QA dataset that is larger than # of chunks",
      "body": "**Is your feature request related to a problem? Please describe.**\nCurrently, if you try to sample from a corpus, you cannot specify a larger sample than the number of chunks the corpus contains. You will get a `Cannot take a larger sample than population when 'replace=False'` error. However, there is no such option to specify `replace=False` in the sampling function (`random_single_hop`, for example, takes no such argument).\n\nI can work around this by making smaller chunks so that I have more chunks, but this is not ideal as I may have other reasons for preferring larger chunks.\n\n**Describe the solution you'd like**\nA way to sample with replacement from chunks so that you can have more Q/As than chunks.\n\n**Describe alternatives you've considered**\nI could also rerun the job multiple times to generate more questions",
      "state": "open",
      "author": "boltzmann-brain",
      "author_type": "User",
      "created_at": "2025-03-31T21:09:15Z",
      "updated_at": "2025-04-04T06:01:35Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1105/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1105",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1105",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:41.166823",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@boltzmann-brain Hello!\nIt seems like you want to generate multiple questions in one chunk.\nIt will be better we have this feature.\nThanks for suggestion!",
          "created_at": "2025-04-04T06:01:33Z"
        }
      ]
    },
    {
      "issue_number": 1089,
      "title": "[BUG] I can't open chroma db",
      "body": "**Bug Description**\nAfter running the evaluation, I attempted to retrieve the stored results from the vector database using Python, but no data was loaded.\n\n\n**To Reproduce**\n1. evaluate  \nexport OPENAI_API_KEY=\"..\"  \nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True  \ntrial(1) : autorag evaluate --config config.yaml --qa_data_path drl_qa.parquet --corpus_data_path drl_corpus.parquet --project_dir ./project\ntrial(2) : autorag evaluate --config config.yaml --qa_data_path drl_qa.parquet --corpus_data_path drl_corpus.parquet --project_dir ./project --skip_validation true\n```shell\nPydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` \ninstead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration \nGuide at https://errors.pydantic.dev/2.9/migration/\n  warnings.warn(\nGenerating embeddings:   0%|                                      | 0/7 [00:00<?, ?it/s]\n[02/15/25 20:31:10] INFO     [_client.py:1038] >> HTTP Request: POST     _client.py:1038\n                             https://api.openai.com/v1/embeddings                       \n                             \"HTTP/1.1 200 OK\"                                          \nGenerating embeddings: 100%|##############################| 7/7 [00:01<00:00,  5.34it/s]\nGenerating embeddings: 100%|##############################| 7/7 [00:01<00:00,  5.33it/s]\n\nGenerating embeddings:   0%|                                      | 0/7 [00:00<?, ?it/s]\n[02/15/25 20:31:11] INFO     [_client.py:1038] >> HTTP Request: POST     _client.py:1038\n                             https://api.openai.com/v1/embeddings                       \n                             \"HTTP/1.1 200 OK\"                                          \nGenerating embeddings: 100%|##############################| 7/7 [00:00<00:00, 14.41it/s]\nGenerating embeddings: 100%|##############################| 7/7 [00:00<00:00, 14.39it/s]\n\n[02/15/25 20:31:13] INFO     [evaluator.py:218] >> Evaluation complete. evaluator.py:218\nIngesting VectorDB... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:01\nEvaluating...         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 6/6 0:01:14\n```\n \n2. bring the db data with python\n3. there is no data**Full Error log**\n```Python\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nvectorstore = Chroma(persist_directory=\"./autorag/project/resources/chroma\",\n                     embedding_function=OpenAIEmbeddings(), \n                     )\n\nprint(vectorstore.get())\ndocs = vectorstore.similarity_search(\"question....\")\nprint(len(docs))\n\nif docs:\n    print(docs[0].page_content)\nelse:\n    print(\"검색된 문서가 없습니다.\")\n```\n{'ids': [], 'embeddings': None, 'documents': [], 'uris': None, 'data': None, 'metadatas': [], 'included': [<IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n0\n검색된 문서가 없습니다.\n\n**Code that bug is happened**\nconfig.yaml\n```yaml\nnode_lines:\n- node_line_name: retrieve_node_line\n  nodes:\n    - node_type: retrieval\n      strategy:\n        batch_size: 1\n        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision,\n                   retrieval_ndcg, retrieval_map, retrieval_mrr ]\n        speed_threshold: 10\n      top_k: 5\n      modules:\n        - module_type: bm25\n          bm25_tokenizer: [ porter_stemmer, space, gpt2 ]\n        - module_type: vectordb\n          vectordb: default\n        - module_type: hybrid_rrf\n          weight_range: (4,80)\n        - module_type: hybrid_cc\n          normalize_method: [ mm, tmm, z, dbsf ]\n          weight_range: (0.0, 1.0)\n          test_weight_size: 101\n    - node_type: passage_augmenter\n      strategy:\n        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]\n        speed_threshold: 5\n      top_k: 5\n      embedding_model: openai\n      modules:\n        - module_type: pass_passage_augmenter\n        - module_type: prev_next_augmenter\n          mode: next\n    - node_type: passage_reranker\n      strategy:\n        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]\n        speed_threshold: 10\n      top_k: 3\n      modules:\n        - module_type: pass_reranker\n        - module_type: upr\n        - module_type: rankgpt\n        - module_type: sentence_transformer_reranker\n        - module_type: flag_embedding_reranker\n        - module_type: openvino_reranker\n        - module_type: flashrank_reranker\n\n    - node_type: passage_filter\n      strategy:\n        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]\n        speed_threshold: 5\n      modules:\n        - module_type: pass_passage_filter\n        - module_type: similarity_threshold_cutoff\n          threshold: 0.85\n        - module_type: similarity_percentile_cutoff\n          percentile: 0.6\n        - module_type: threshold_cutoff\n          threshold: 0.85\n        - module_type: percentile_cutoff\n          percentile: 0.6\n\n- node_line_name: post_retrieve_node_line  # Arbitrary node line name\n  nodes:\n    - node_type: prompt_maker\n      strategy:\n        metrics:\n          - metric_name: bleu\n          - metric_name: meteor\n          - metric_name: rouge\n          - metric_name: sem_score\n            embedding_model: openai\n        speed_threshold: 10\n        generator_modules:\n          - module_type: llama_index_llm\n            llm: openai\n            model: [gpt-4o-mini]\n      modules:\n        - module_type: fstring\n          prompt:\n            - \"Answer to given questions with the following passage: {retrieved_contents} \\n\\n Question: {query} \\n\\n Answer:\"\n            - \"There is a passages related to user question. Please response carefully to the following question. \\n\\n Passage: {retrieved_contents} \\n\\n Question: {query} \\n\\n Answer the question. Think step by step.\" # Zero-shot CoT prompt\n            - \"{retrieved_contents} \\n\\n Read the passage carefully, and answer this question. \\n\\n Question: {query} \\n\\n Answer the question. Be concise.\" # concise prompt\n        - module_type: long_context_reorder\n          prompt:\n            - \"Answer to given questions with the following passage: {retrieved_contents} \\n\\n Question: {query} \\n\\n Answer:\"\n            - \"There is a passages related to user question. Please response carefully to the following question. \\n\\n Passage: {retrieved_contents} \\n\\n Question: {query} \\n\\n Answer the question. Think step by step.\" # Zero-shot CoT prompt\n            - \"{retrieved_contents} \\n\\n Read the passage carefully, and answer this question. \\n\\n Question: {query} \\n\\n Answer the question. Be concise.\" # concise prompt\n    - node_type: generator\n      strategy:\n        metrics:\n          - metric_name: rouge\n          - embedding_model: openai\n            metric_name: sem_score\n          - metric_name: bert_score\n        speed_threshold: 10\n      modules:\n        - module_type: llama_index_llm\n          llm: [openai]\n          model: [gpt-4o-mini]\n          temperature: [0.5, 1.0]\n\nquantization_config:\n  bits: 4\n  group_size: 128\n  dataset: \"c4\"\n  model_seqlen: 2048\n  desc_act: False\n  device: \"cpu\"\n\nmodel_load:\n  low_cpu_mem_usage: True\n  torch_dtype: \"auto\"\n  trust_remote_code: True\n\n```\n\n**Desktop (please complete the following information):**\n - OS: [ubuntu 22.04]\n - Python version [3.10.12]\n\n**Additional context**\nmy project tree\n📦project\n ┣ 📂0\n ┃ ┣ 📂post_retrieve_node_line\n ┃ ┃ ┣ 📂generator\n ┃ ┃ ┃ ┣ 📜0.parquet\n ┃ ┃ ┃ ┣ 📜1.parquet\n ┃ ┃ ┃ ┣ 📜best_0.parquet\n ┃ ┃ ┃ ┗ 📜summary.csv\n ┃ ┃ ┣ 📂prompt_maker\n ┃ ┃ ┃ ┣ 📜0.parquet\n ┃ ┃ ┃ ┣ 📜1.parquet\n ┃ ┃ ┃ ┣ 📜2.parquet\n ┃ ┃ ┃ ┣ 📜3.parquet\n ┃ ┃ ┃ ┣ 📜4.parquet\n ┃ ┃ ┃ ┣ 📜5.parquet\n ┃ ┃ ┃ ┣ 📜best_0.parquet\n ┃ ┃ ┃ ┗ 📜summary.csv\n ┃ ┃ ┗ 📜summary.csv\n ┃ ┣ 📂retrieve_node_line\n ┃ ┃ ┣ 📂passage_augmenter\n ┃ ┃ ┃ ┣ 📜0.parquet\n ┃ ┃ ┃ ┣ 📜1.parquet\n ┃ ┃ ┃ ┣ 📜best_0.parquet\n ┃ ┃ ┃ ┗ 📜summary.csv\n ┃ ┃ ┣ 📂passage_filter\n ┃ ┃ ┃ ┣ 📜0.parquet\n ┃ ┃ ┃ ┣ 📜1.parquet\n ┃ ┃ ┃ ┣ 📜2.parquet\n ┃ ┃ ┃ ┣ 📜3.parquet\n ┃ ┃ ┃ ┣ 📜4.parquet\n ┃ ┃ ┃ ┣ 📜best_3.parquet\n ┃ ┃ ┃ ┗ 📜summary.csv\n ┃ ┃ ┣ 📂passage_reranker\n ┃ ┃ ┃ ┣ 📜0.parquet\n ┃ ┃ ┃ ┣ 📜1.parquet\n ┃ ┃ ┃ ┣ 📜2.parquet\n ┃ ┃ ┃ ┣ 📜3.parquet\n ┃ ┃ ┃ ┣ 📜4.parquet\n ┃ ┃ ┃ ┣ 📜5.parquet\n ┃ ┃ ┃ ┣ 📜6.parquet\n ┃ ┃ ┃ ┣ 📜best_0.parquet\n ┃ ┃ ┃ ┗ 📜summary.csv\n ┃ ┃ ┣ 📂retrieval\n ┃ ┃ ┃ ┣ 📜0.parquet\n ┃ ┃ ┃ ┣ 📜1.parquet\n ┃ ┃ ┃ ┣ 📜2.parquet\n ┃ ┃ ┃ ┣ 📜3.parquet\n ┃ ┃ ┃ ┣ 📜4.parquet\n ┃ ┃ ┃ ┣ 📜5.parquet\n ┃ ┃ ┃ ┣ 📜6.parquet\n ┃ ┃ ┃ ┣ 📜7.parquet\n ┃ ┃ ┃ ┣ 📜8.parquet\n ┃ ┃ ┃ ┣ 📜best_5.parquet\n ┃ ┃ ┃ ┗ 📜summary.csv\n ┃ ┃ ┗ 📜summary.csv\n ┃ ┣ 📜config.yaml\n ┃ ┗ 📜summary.csv\n ┣ 📂data\n ┃ ┣ 📜corpus.parquet\n ┃ ┗ 📜qa.parquet\n ┣ 📂resources\n ┃ ┣ 📂chroma\n ┃ ┃ ┣ 📂0ec9cd05-0d96-4fc7-9a7a-a1abea6c5ce8\n ┃ ┃ ┣ 📂50f4b08f-74bf-4fdc-9097-edbe9649cda4\n ┃ ┃ ┃ ┣ 📜data_level0.bin\n ┃ ┃ ┃ ┣ 📜header.bin\n ┃ ┃ ┃ ┣ 📜length.bin\n ┃ ┃ ┃ ┗ 📜link_lists.bin\n ┃ ┃ ┗ 📜chroma.sqlite3\n ┃ ┣ 📜bm25_gpt2.pkl\n ┃ ┣ 📜bm25_porter_stemmer.pkl\n ┃ ┣ 📜bm25_space.pkl\n ┃ ┗ 📜vectordb.yaml\n ┗ 📜trial.json\nI don't know it is because of evaluate error, or config.yaml error, or python code error. can you find the cause of error?\ncf. I made dataset myself (by hand), is it possible to occur error?\n\nthank you for reading",
      "state": "closed",
      "author": "Sung-Jae-Seong",
      "author_type": "User",
      "created_at": "2025-02-15T11:51:14Z",
      "updated_at": "2025-03-31T06:11:22Z",
      "closed_at": "2025-03-31T06:11:22Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1089/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1089",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1089",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:41.362425",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@Sung-Jae-Seong Hello!\n\nSince `from langchain_community.vectorstores import Chroma` is deprecated, can you try to use [this](https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html#langchain_chroma.vectorstores.Chroma) instead when you try to load a c",
          "created_at": "2025-02-16T05:43:39Z"
        },
        {
          "author": "Sung-Jae-Seong",
          "body": "Thank you for your response.\nI tried new chroma vector db, I couldn't solve this.\n\nI checked my chroma.sqlite3, there was empty.\n\nand I found warning message during evaluating\n\n```shell\n/home/rokey4090/.local/lib/python3.10/site-packages/autorag/nodes/retrieval/hybrid_cc.py:16: RuntimeWarning: inval",
          "created_at": "2025-02-17T00:25:20Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@Sung-Jae-Seong \nCan you open the parquet file at the retrieval folder?\nYou can find in [your_project_dir] - [your_node_line_name] - retrieval - 0.parquet or any other parquet files.\nIf you can find the 'retrieved_content', 'retrieved_id', 'retrieve_score' with full list, that means the retrieval an",
          "created_at": "2025-02-17T03:16:32Z"
        }
      ]
    },
    {
      "issue_number": 806,
      "title": "[Feature Request] docker image push CI/CD",
      "body": "**Is your feature request related to a problem? Please describe.**\nThe current process of manually building and pushing Docker images is time-consuming and prone to errors. It's particularly challenging to maintain consistency when managing multiple environments or versions.\n\n**Describe the solution you'd like**\nWe need an automated CI/CD pipeline using GitHub Actions to automatically build Docker images and push them to Docker Hub. This pipeline should trigger whenever code changes are merged into the main branch.\nDescribe alternatives you've considered\n\n**Describe alternatives you've considered**\nSetting up a CI/CD pipeline using Jenkins\nUsing GitLab CI/CD (which would require migrating from GitHub to GitLab)\nCreating semi-automated deployment scripts\n\n**Additional context**\nhttps://hub.docker.com/repository/docker/autoraghq/autorag/general\n\ndocker pull autoraghq/autorag:0.3.3-all\ndocker pull autoraghq/autorag:0.3.3-ko\ndocker pull autoraghq/autorag:0.3.3-parse\ndocker pull autoraghq/autorag:0.3.3-dev\n\n'all' includes all features, 'ko' adds Korean support, 'parse' is a lightweight version with parsing only, and 'dev' is the latest development version with potential instabilities.\n\n",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-06T04:35:33Z",
      "updated_at": "2025-03-26T19:33:25Z",
      "closed_at": "2024-10-08T06:54:38Z",
      "labels": [
        "enhancement",
        "ci/cd"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/806/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/806",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/806",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:41.593509",
      "comments": [
        {
          "author": "alexkreidler",
          "body": "This is awesome!\n\nCould we add something that pushes the `autorag-frontend` image too? Also, it would be nice if the `docker-compose.yml` used the pushed images by default so we don't run into issues like #1102 from being on the bleeding edge. \n\nMaybe we can add a separate `docker-compose.dev.yml` t",
          "created_at": "2025-03-26T19:32:47Z"
        }
      ]
    },
    {
      "issue_number": 1099,
      "title": "[BUG] TypeError: 'NoneType' object is not callable Exception ignored in: <function BasePromptMaker.__del__ at 0x733030a85bd0>",
      "body": "run\n`from autorag.deploy import Runner\nfrom dotenv import load_dotenv\nload_dotenv()\nrunner = Runner.from_trial_folder('benchmark1/9'\nrunner.run('How many teams are there in South Korea\\'s 2024 baseball season?')\n`\nand then \n\nException ignored in: <function LlamaIndexLLM.__del__ at 0x733030a84160>\nTraceback (most recent call last):\n  File \"/data/disk1/anaconda3/envs/env_for_autorag/lib/python3.10/site-packages/autorag/nodes/generator/llama_index_llm.py\", line 55, in __del__\n  File \"/data/disk1/anaconda3/envs/env_for_autorag/lib/python3.10/site-packages/autorag/nodes/generator/base.py\", line 23, in __del__\n  File \"/data/disk1/anaconda3/envs/env_for_autorag/lib/python3.10/logging/__init__.py\", line 1477, in info\n  File \"/data/disk1/anaconda3/envs/env_for_autorag/lib/python3.10/logging/__init__.py\", line 1624, in _log\n  File \"/data/disk1/anaconda3/envs/env_for_autorag/lib/python3.10/logging/__init__.py\", line 1634, in handle\n  File \"/data/disk1/anaconda3/envs/env_for_autorag/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n  File \"/data/disk1/anaconda3/envs/env_for_autorag/lib/python3.10/logging/__init__.py\", line 968, in handle\n  File \"/home/ps/.local/lib/python3.10/site-packages/rich/logging.py\", line 159, in emit\n  File \"/home/ps/.local/lib/python3.10/site-packages/rich/logging.py\", line 185, in render_message\nTypeError: 'NoneType' object is not callable\nException ignored in: <function BasePromptMaker.__del__ at 0x733030a85bd0>\n",
      "state": "open",
      "author": "zhoujiamei-git",
      "author_type": "User",
      "created_at": "2025-03-05T02:37:17Z",
      "updated_at": "2025-03-06T08:18:37Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1099/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1099",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1099",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:41.796856",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "I don't know why but it looks like the `self.__class__` is not working. \nThe line that error occured looks like this.\n\n```\n\tdef __del__(self):\n\t\tlogger.info(f\"Deleting generator module - {self.__class__.__name__}\")\n```\n\nJust a logging for deletion of the class.\n\nCan you provide some script or colab ",
          "created_at": "2025-03-06T08:18:36Z"
        }
      ]
    },
    {
      "issue_number": 1100,
      "title": "[BUG]  The temperature hyperparameter is out of range. The valid range for the temperature hyperparameter should be between 0 and 1.",
      "body": "![Image](https://github.com/user-attachments/assets/0f50350c-c728-4be9-9471-8ee8e7a2a996)\nI suppose that the temperature parameter 1.5 should be removed ",
      "state": "closed",
      "author": "zhoujiamei-git",
      "author_type": "User",
      "created_at": "2025-03-05T02:48:45Z",
      "updated_at": "2025-03-06T08:15:36Z",
      "closed_at": "2025-03-06T08:15:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1100/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1100",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1100",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:42.038902",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "<img width=\"783\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d4af0741-da95-4908-9825-1741ec9b5153\" />\nhttps://platform.openai.com/docs/api-reference/chat/create\n\nAccording to the openai official APi documentation, the range is 0 to 2. \n\nPlus, in theory, you can increase the temperatu",
          "created_at": "2025-03-06T08:15:36Z"
        }
      ]
    },
    {
      "issue_number": 1094,
      "title": "[Feature Request] Add gpt-4.5-preview",
      "body": "Recently, OpenAI released the gpt-4.5-preview model. However, the openai_llm.py file in AutoRAG does not yet support this model.\n\nI made the following changes to add support for gpt-4.5-preview:\n\n1. Added the context length for gpt-4.5-preview to MAX_TOKEN_DICT.\n![Image](https://github.com/user-attachments/assets/6498f8e4-1764-4b30-8799-9785a855cfa4)\n\n2. Since gpt-4.5-preview does not support tiktoken.encoding_for_model(self.llm), I added a conditional case to handle this.\n![Image](https://github.com/user-attachments/assets/b2051438-e7f1-4b9c-a7ba-60db67495ae8)\n\n3.  gpt-4.5-preview also does not support the logprobs parameter. I added a conditional case for this as well, with comments explaining the limitation.\n\n\n",
      "state": "closed",
      "author": "minsing-jin",
      "author_type": "User",
      "created_at": "2025-03-02T14:56:03Z",
      "updated_at": "2025-03-03T06:28:04Z",
      "closed_at": "2025-03-03T06:28:03Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1094/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1094",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1094",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:42.263132",
      "comments": []
    },
    {
      "issue_number": 1085,
      "title": "[BUG] Error on building GUI installation",
      "body": "**Describe the bug**\nError encoutering \"ENOENT: no such file or directory, ...\" on ```npm run build```\n\n**To Reproduce**\nSteps to reproduce the behavior:\non step to building and running  GUI application, \n\n```npm run start```\n\nreturns following error,\nError: ENOENT: no such file or directory, open '~/AutoRAG/autorag-frontend/.next/server/app/projects/[project_id]/trials/[trial_id]/parquet_wasm_bg.wasm'\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Full Error log**\n```\n % npm run start\n\n> next-app-template@0.0.1 start\n> next start\n\n   ▲ Next.js 15.1.6\n   - Local:        http://localhost:3000\n   - Network:      http://172.16.21.27:3000\n\n ✓ Starting...\n ✓ Ready in 155ms\nError: ENOENT: no such file or directory, open '~/AutoRAG/autorag-frontend/.next/server/app/projects/[project_id]/trials/[trial_id]/parquet_wasm_bg.wasm'\n    at 52342 (.next/server/app/projects/[project_id]/trials/[trial_id]/page.js:4:678)\n    at t (.next/server/webpack-runtime.js:1:142)\n    at 84163 (.next/server/app/projects/[project_id]/trials/[trial_id]/page.js:3:18254)\n    at Function.t (.next/server/webpack-runtime.js:1:142) {\n  errno: -2,\n  code: 'ENOENT',\n  syscall: 'open',\n  path: '~/AutoRAG/autorag-frontend/.next/server/app/projects/[project_id]/trials/[trial_id]/parquet_wasm_bg.wasm'\n}\n ⨯ unhandledRejection:  Error: ENOENT: no such file or directory, open '~/AutoRAG/autorag-frontend/.next/server/app/projects/[project_id]/trials/[trial_id]/parquet_wasm_bg.wasm'\n    at 52342 (.next/server/app/projects/[project_id]/trials/[trial_id]/page.js:4:678)\n    at t (.next/server/webpack-runtime.js:1:142)\n    at 84163 (.next/server/app/projects/[project_id]/trials/[trial_id]/page.js:3:18254)\n    at Function.t (.next/server/webpack-runtime.js:1:142) {\n  errno: -2,\n  code: 'ENOENT',\n  syscall: 'open',\n  path: '~/AutoRAG/autorag-frontend/.next/server/app/projects/[project_id]/trials/[trial_id]/parquet_wasm_bg.wasm'\n}\n```\n\n**Desktop (please complete the following information):**\n - OS: MacOS\n - Python version [e.g. 3.12]\n\n",
      "state": "closed",
      "author": "caisarl76",
      "author_type": "User",
      "created_at": "2025-02-06T06:41:26Z",
      "updated_at": "2025-02-16T12:29:46Z",
      "closed_at": "2025-02-16T12:29:45Z",
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1085/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1085",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1085",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:42.263154",
      "comments": [
        {
          "author": "teddythinh",
          "body": "Have you tried reinstalling it again? Delete `.next` and `node_module` and try again. 😃 \n\n",
          "created_at": "2025-02-06T10:00:21Z"
        },
        {
          "author": "caisarl76",
          "body": "Still not working ;(",
          "created_at": "2025-02-07T06:08:49Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@caisarl76 \nHello!!\nApology to the late reply.\n\nDid you run `npm install` right?\n\nI will investigate the issue.",
          "created_at": "2025-02-16T05:45:17Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@caisarl76 \nSorry for confusion.\nYou can try to re-install modules using yarn.\n\n```\nyarn install\n```",
          "created_at": "2025-02-16T06:00:20Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@caisarl76 \nIn #1091 I added a GUI application at the docker compose also!\nJust run `docker compose up -d` and the GUI will be successfully build.\n\nThank you.\nI will close this issue.",
          "created_at": "2025-02-16T12:29:45Z"
        }
      ]
    },
    {
      "issue_number": 1091,
      "title": "[Feature Request] Integrate AutoRAG GUI to the docker compose.",
      "body": "**Is your feature request related to a problem? Please describe.**\nFrom now you have to build your own next.js app from local to use AutoRAG GUI.\nTo overcome this, make a new docker container that serves next.js app.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2025-02-16T06:09:38Z",
      "updated_at": "2025-02-16T12:28:57Z",
      "closed_at": "2025-02-16T12:28:57Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1091/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1091",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1091",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:42.469175",
      "comments": []
    },
    {
      "issue_number": 1084,
      "title": "[BUG]",
      "body": "Sorry for all the bug reports lately. I appreciate your help.\n\nI am confused why this exists in the validator. Sampling from corpus means the QA doc_ids likely won't be found in the corpus data.\n\n\t\twith (\n\t\t\ttempfile.NamedTemporaryFile(suffix=\".parquet\", delete=False) as qa_path,\n\t\t\ttempfile.NamedTemporaryFile(suffix=\".parquet\", delete=False) as corpus_path,\n\t\t\ttempfile.TemporaryDirectory(ignore_cleanup_errors=True) as temp_project_dir,\n\t\t):\n\t\t\tsample_qa_df.to_parquet(qa_path.name, index=False)\n\t\t\tsample_corpus_df.to_parquet(corpus_path.name, index=False)\n\n\t\t\tevaluator = Evaluator(\n\t\t\t\tqa_data_path=qa_path.name,\n\t\t\t\tcorpus_data_path=corpus_path.name,\n\t\t\t\tproject_dir=temp_project_dir,\n\t\t\t)",
      "state": "closed",
      "author": "AlbertoMQ",
      "author_type": "User",
      "created_at": "2025-02-05T20:41:18Z",
      "updated_at": "2025-02-16T06:08:02Z",
      "closed_at": "2025-02-16T06:08:01Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1084/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1084",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1084",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:42.469195",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@AlbertoMQ \nHello! In the validator, the `sample_corpus_df` is constructed from the `sample_qa_df` retrieval_gt ids. \nSo it will be matched perfectly unless you are using `passage augmenter` modules.\n\nThanks for the question",
          "created_at": "2025-02-16T06:08:01Z"
        }
      ]
    },
    {
      "issue_number": 1010,
      "title": "[Feature Request] Do not allow to use ‘top_k’ list in the config YAML file",
      "body": "**Is your feature request related to a problem? Please describe.**\nIt will cause error at the hybrid retrieval.\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-28T01:28:57Z",
      "updated_at": "2025-02-10T09:25:27Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1010/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1010",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1010",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:42.675873",
      "comments": [
        {
          "author": "cyx441984694",
          "body": "any plan to support the top_k as a list so that it can also be as one of the optimized parameters?\n",
          "created_at": "2025-02-10T09:25:25Z"
        }
      ]
    },
    {
      "issue_number": 1081,
      "title": "[BUG] Custom embedding models",
      "body": "Thank you for making this and for your help.\n\nI am having a near impossible time setting up custom embedding models. \nHere are things I've tried\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nembed_model_name = Config._default_embed_model_name.replace(\"/\", \"-\")\nembed_model = HuggingFaceEmbedding(\n                    model_name= \"BAAI/bge-large-en\",\n                    cache_folder=   Config.MODELS_DIR\n                )\nautorag.embedding_models[embed_model_name] = LazyInit(Config.default_embed_model, model_name=Config._default_embed_model_name)\nautorag.embedding_models[embed_model_name] = Config.default_embed_model\n```\n\n```yaml\nvectordb:\n  - name: bge-large\n    db_type: chroma\n    client_type: persistent\n    embedding_model: BAAI-bge-large-en\n    collection_name: BAAI-bge-large-en\n    path: ${PROJECT_DIR}/data/chroma\nnode_lines:\n- node_line_name: retrieve_node_line\n  nodes:\n    - node_type: retrieval\n      strategy:\n        metrics: [retrieval_f1, retrieval_recall, retrieval_precision,\n                  retrieval_map, retrieval_mrr, retrieval_ndcg]\n      top_k: [1, 3, 5, 10, 50]\n      modules:\n        - module_type: vectordb\n          vectordb: bge-large\n```\n\n///////////////////////////////////\n\n                             TypeError: missing a required argument: 'nodes'          \n\n//////////////////////////////////\n\nSwitching embeddings                              \n  embedding_model:\n  - type: huggingface\n    model_name: intfloat/multilingual-e5-large-instruct         \n\n///////////////////////////\nThis gives an error because it is called as embedding_models[] in the code, and this dict won't do.\n//////////////////////////\n\nI've tried this style as well.  (I have no idea which I'm supposed to use)\n\n      modules:\n        - module_type: vectordb\n          embedding_model:\n           - bge-large-en\n \n                                                        ",
      "state": "closed",
      "author": "AlbertoMQ",
      "author_type": "User",
      "created_at": "2025-01-31T20:14:45Z",
      "updated_at": "2025-02-05T18:44:14Z",
      "closed_at": "2025-02-05T18:44:14Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1081/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1081",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1081",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:42.917801",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "1. Upgrade AutoRAG version to latest (up to v0.3.13)\n2. Try like this.\n\n```yaml\nvectordb:\n- name: bge-large\n  db_type: chroma\n  client_type: persistent\n  path: ${PROJECT_DIR}/data/chroma\n  embedding_model:\n  - type: huggingface\n    model_name: BAAI/bge-large-en\n```\n\nAnd you can use the \"bge-large\" v",
          "created_at": "2025-02-02T06:08:18Z"
        }
      ]
    },
    {
      "issue_number": 1077,
      "title": "[BUG] OpenAILIKE additional params are not supported",
      "body": "**Describe the bug**\nOpenAILIKE additional params are not supported like is_chat_model param...\n\n**To Reproduce**\nConfigure the following llm module and use is_chat_model param like the following: \n```yaml\n    - node_type: generator\n      modules:\n        - batch: 2\n          module_type: llama_index_llm\n          llm: openailike\n          model: qwen-plus\n          is_chat_model: True\n          api_base: https://dashscope.aliyuncs.com/compatible-mode/v1\n          api_key: x\n```\nIt will have error like 404 error with the endpoint '/v1/completions'. The error is because is_chat_model param is not passed into the model. If the model itself does not support /v1/completions endpoint, it will have error as it doesn't call /v1/chat/completions'.\n\n**Additional context**\nThe root cause may be due to llm_instance params is using llm_class.__init__ instead of the llm_class...\n\n```\nautorag/nodes/generator/llama_index_llm.py line 52 __init__ method\n\nself.llm_instance: BaseLLM = llm_class(**pop_params(llm_class.__init__, kwargs))\n\n```\nWhen using openAILike, it is converting the OpenAI params with ignoring OpenAILike params....\n\n**Possible fix**\nStupid but quick fix...\n```\n\t\tllm_openaiLIke_params = None\n\t\tif llm_class.class_name() == \"OpenAILike\":\n\t\t\tllm_openaiLIke_params = pop_params(llm_class, kwargs)\n\t\toriginal_llm_params = pop_params(llm_class.__init__, kwargs)\n\t\tif llm_openaiLIke_params is not None:\n\t\t\tllm_params = {**llm_openaiLIke_params,**original_llm_params}\n\t\telse:\n\t\t\tllm_params = original_llm_params\n\t\tself.llm_instance: BaseLLM = llm_class(**llm_params)\n```\n\n\n\n",
      "state": "open",
      "author": "cyx441984694",
      "author_type": "User",
      "created_at": "2025-01-29T08:57:17Z",
      "updated_at": "2025-02-05T03:29:30Z",
      "closed_at": null,
      "labels": [
        "bug",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1077/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1077",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1077",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:43.236850",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Thank you for registering bug!\nIt looks like the `pop_params` is not well functioning to the pydantic class. I might change it to support it. ",
          "created_at": "2025-01-30T07:50:44Z"
        }
      ]
    },
    {
      "issue_number": 1069,
      "title": "[Feature Request] I hope the author adds support for the Chinese language. Thank you very much!",
      "body": "I hope the author adds support for the Chinese language. Thank you very much!",
      "state": "open",
      "author": "yahiko-l",
      "author_type": "User",
      "created_at": "2025-01-15T03:34:55Z",
      "updated_at": "2025-02-05T03:00:23Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1069/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1069",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1069",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:43.453827",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hello! @yahiko-l \nSince we are not native to Chinese, we need someone to provide Chinese translation for all the prompts. ",
          "created_at": "2025-01-21T08:45:48Z"
        },
        {
          "author": "cyx441984694",
          "body": "> Hello! [@yahiko-l](https://github.com/yahiko-l) Since we are not native to Chinese, we need someone to provide Chinese translation for all the prompts.\n\n@vkehfdl1  Hello! I am Chinese. Possibly I can help with that. Let me know if there is any needs. ",
          "created_at": "2025-02-05T02:59:52Z"
        }
      ]
    },
    {
      "issue_number": 1064,
      "title": "[BUG] Parsing module exports same parsed_result about all modules.",
      "body": "**Describe the bug**\n\nHello,\n\nCould you help me? It seems that there has been a change in the parsing module.\nThe parsing module now exports the same `parsed_result` for all modules.\nI have set up my config file for the parser as shown below:\n```yaml\n  - module_type: langchain_parse\n    parse_method: upstagedocumentparse\n    split: [ page ]\n    file_type: pdf\n    output_format: [ html, markdown ]\n  - module_type: llamaparse\n    result_type: markdown\n    file_type: pdf\n    language: ko\n```\nThe current parsed result is as follows:\n\n![Image](https://github.com/user-attachments/assets/ff7b9242-6a4c-41c4-ba39-c1bdd5108799)\n\nHowever, the previous results were different. For example, when I set the `output_format` to `html` in `upstagedocumentparse`, the parsed result used to contain HTML tags.\n\n![Image](https://github.com/user-attachments/assets/87fc55d7-eee1-4f32-8893-3837fad6e512)\n\nI would appreciate your assistance. Thank you.",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-12-20T01:33:46Z",
      "updated_at": "2025-02-03T08:17:13Z",
      "closed_at": "2025-02-03T08:17:13Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1064/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1064",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1064",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:43.666861",
      "comments": []
    },
    {
      "issue_number": 1073,
      "title": "[BUG] Unexpected keyword argument 'temperature' when using QA script with Gemini LlamaIndex",
      "body": "**Describe the bug**\nWith the provided QA creation script in the README, I tried to replace OpenAI model with Gemini. After modifying which model to use and my file paths, I ran the code and got this error: `TypeError: ChatSession.send_message_async() got an unexpected keyword argument 'temperature'`.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Import `Gemini` from LlamaIndex `llm`\n2. Instantiate a new LLM object for Gemini\n3. Use that object for llm arguments in the code\n\n**Expected behavior**\nNormal execution.\n\n**Full Error log**\nThis happens in `\\llama_index\\llms\\gemini\\base.py:233 in achat`\n\n**Code that bug is happened**\n```\nimport pandas as pd\nfrom llama_index.llms.gemini import Gemini\n\nfrom autorag.data.qa.filter.dontknow import dontknow_filter_rule_based\nfrom autorag.data.qa.generation_gt.llama_index_gen_gt import (\n    make_basic_gen_gt,\n    make_concise_gen_gt,\n)\nfrom autorag.data.qa.schema import Raw, Corpus\nfrom autorag.data.qa.query.llama_gen_query import factoid_query_gen\nfrom autorag.data.qa.sample import random_single_hop\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nllm = Gemini(model=\"models/gemini-1.5-flash\")\nraw_df = pd.read_parquet(\"data/raw.parquet\")\nraw_instance = Raw(raw_df)\n\ncorpus_df = pd.read_parquet(\"data/corpus-semantic.parquet\")\ncorpus_instance = Corpus(corpus_df, raw_instance)\n\ninitial_qa = (\n    corpus_instance.sample(random_single_hop, n=3)\n    .map(\n        lambda df: df.reset_index(drop=True),\n    )\n    .make_retrieval_gt_contents()\n    .batch_apply(\n        factoid_query_gen,  # query generation\n        llm=llm,\n    )\n    .batch_apply(\n        make_basic_gen_gt,  # answer generation (basic)\n        llm=llm,\n    )\n    .batch_apply(\n        make_concise_gen_gt,  # answer generation (concise)\n        llm=llm,\n    )\n    .filter(\n        dontknow_filter_rule_based,  # filter don't know\n        lang=\"vi\",\n    )\n)\n\ninitial_qa.to_parquet('data/qa.parquet', 'data/corpus.parquet')\n```\n\n**Desktop (please complete the following information):**\n - OS: Windows\n - Python version 3.12\n",
      "state": "open",
      "author": "keandk",
      "author_type": "User",
      "created_at": "2025-01-22T07:50:06Z",
      "updated_at": "2025-02-03T08:07:45Z",
      "closed_at": null,
      "labels": [
        "bug",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1073/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1073",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1073",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:43.666882",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@keandk \nThank you for reporting the bug.\nI will check it out.",
          "created_at": "2025-01-25T05:06:24Z"
        },
        {
          "author": "keandk",
          "body": "Hi, I have updated AutoRAG package and tried running the code again and i got this new error:\n```\n[02/03/25 15:06:12] ERROR    [__init__.py:53] >> Unexpected exception                                           __init__.py:53\n                             ╭────────────────────── Traceback (most recent",
          "created_at": "2025-02-03T08:07:43Z"
        }
      ]
    },
    {
      "issue_number": 1079,
      "title": "[HotFix] Import fix at cli.py",
      "body": "* As-is\n\n```\nfrom autorag.deploy import extract_best_config as original_extract_best_config\n```\n\n* To-be\n\n```\nfrom autorag.deploy.base import extract_best_config as original_extract_best_config\n```\n\nat cli.py",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2025-01-31T03:30:57Z",
      "updated_at": "2025-01-31T08:57:34Z",
      "closed_at": "2025-01-31T08:57:33Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1079/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1079",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1079",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:43.876623",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "This is not error",
          "created_at": "2025-01-31T08:57:33Z"
        }
      ]
    },
    {
      "issue_number": 1076,
      "title": "[Feature Request] Running locally",
      "body": "Finding no resources on how to run this all locally. \n\nGetting a timeout error, looks like from open_ai\n\nGuessing it's defaulting to open_ai if it doesn't find the llm? Not really sure\n\n\nHere is my YAML\n\n```yaml\nnode_lines:\n- node_line_name: retrieve_node_line\n  nodes:\n  llm_model:\n  type: huggingface\n  model_name: HuggingFaceH4/zephyr-7b-beta \n\n  \n    - node_type: retrieval\n      strategy:\n        metrics: [retrieval_f1, retrieval_recall, retrieval_ndcg, retrieval_mrr]  # Include multiple metrics for comparison\n      top_k: 5  # Adjust based on desired recall depth\n      modules:\n        - module_type: vectordb\n          vectordb: default  # Default vector database used for retrieval\n          embedding_model:\n            type: huggingface\n            model_name: sentence-transformers/all-MiniLM-L6-v2  # Model 1\n```\n\noutput\n\n[01/27/25 12:21:13] INFO     [posthog.py:22] >> Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry posthog.py:22\n                             for more information.                                                                                                      \n[01/27/25 12:21:14] INFO     [_base_client.py:1666] >> Retrying request to /embeddings in 0.479268 seconds                          _base_client.py:1666\n                    INFO     [_base_client.py:1666] >> Retrying request to /embeddings in 0.892558 seconds                          _base_client.py:1666\n[01/27/25 12:21:15] INFO     [_base_client.py:1666] >> Retrying request to /embeddings in 1.674951 seconds                          _base_client.py:1666\n[01/27/25 12:21:17] INFO     [_base_client.py:1666] >> Retrying request to /embeddings in 3.469390 seconds                          _base_client.py:1666\nIngesting VectorDB... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0% 0/1 0:00:49",
      "state": "closed",
      "author": "AlbertoMQ",
      "author_type": "User",
      "created_at": "2025-01-27T18:27:43Z",
      "updated_at": "2025-01-30T07:59:19Z",
      "closed_at": "2025-01-30T07:59:18Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1076/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1076",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1076",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:44.073896",
      "comments": [
        {
          "author": "AlbertoMQ",
          "body": "Anything? Why is openai being accessed??\n\nnode_lines:\n  - node_line_name: retrieve_node_line\n    nodes:\n      - node_type: retrieval\n        strategy:\n          metrics: [retrieval_f1, retrieval_recall, retrieval_ndcg, retrieval_mrr]  # Multiple metrics for comparison\n        top_k: 5  # Adjust base",
          "created_at": "2025-01-29T18:07:40Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@AlbertoMQ \n\nHello, \nhttps://docs.auto-rag.com/integration/vectordb/vectordb.html\nhttps://docs.auto-rag.com/nodes/retrieval/vectordb.html\n\nYou have to configure your embedding model at vectordb section.\nPlease read above documentations and follow the instruction. \n\nThank you.",
          "created_at": "2025-01-30T07:59:18Z"
        }
      ]
    },
    {
      "issue_number": 1072,
      "title": "ModuleNotFoundError: No module named 'autorag.data'; 'autorag' is not a package",
      "body": "This strangely happens with autorag.data but not autorag. \n\nimport autorag\nimport autorag.data\nfrom autorag.data.qa.schema import Raw, Corpus\n\nModuleNotFoundError: No module named 'autorag.data'; 'autorag' is not a package\n\nautorag/data has the __init__.py file as expected.\n",
      "state": "closed",
      "author": "AlbertoMQ",
      "author_type": "User",
      "created_at": "2025-01-21T18:46:11Z",
      "updated_at": "2025-01-21T19:19:51Z",
      "closed_at": "2025-01-21T19:19:51Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1072/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1072",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1072",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:44.279551",
      "comments": [
        {
          "author": "AlbertoMQ",
          "body": "am I missing some dependency? I think I downloaded everything in the install instructions.",
          "created_at": "2025-01-21T19:10:37Z"
        }
      ]
    },
    {
      "issue_number": 1070,
      "title": "[Feature Request] Add Ollama Embedding",
      "body": "**Is your feature request related to a problem? Please describe.**\nNot supporting ollama for embedding model.\nIt may related to #985 \n\n**Describe the solution you'd like**\nFix the bug and add ollama for embedding model\n\n**Describe alternatives you've considered**\n\n\n**Additional context**\n\n",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2025-01-18T12:06:29Z",
      "updated_at": "2025-01-21T08:40:59Z",
      "closed_at": "2025-01-21T08:40:59Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1070/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rjwharry"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1070",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1070",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:44.459945",
      "comments": [
        {
          "author": "rjwharry",
          "body": "If you are willing to apply ollama, then please assign to me.",
          "created_at": "2025-01-18T12:07:26Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry Thank you! I assigned to you.\nOne thing, we changed a little bit to use embedding model recently. Not much changed, but please check https://github.com/Marker-Inc-Korea/AutoRAG/pull/1063 #1063 ",
          "created_at": "2025-01-19T05:46:35Z"
        }
      ]
    },
    {
      "issue_number": 1060,
      "title": "[Feature Request] Proposal to Allow Dynamic Use of Additional Embedding Models",
      "body": "**Is your feature request related to a problem? Please describe.**\nHello!\n\nHow about improving the system to allow the use of other embedding models dynamically, in addition to the predefined models?\n\nhttps://github.com/Marker-Inc-Korea/AutoRAG/blob/499f54a2a012ee398850bd94ea10127a1860caf6/autorag/vectordb/base.py#L19\n\nhttps://github.com/Marker-Inc-Korea/AutoRAG/blob/499f54a2a012ee398850bd94ea10127a1860caf6/autorag/__init__.py#L65-L103",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-12-18T01:46:30Z",
      "updated_at": "2025-01-04T07:30:32Z",
      "closed_at": "2025-01-04T07:30:32Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1060/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1060",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1060",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:44.650903",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@e7217 Do you have any idea?\nIf you suggest any idea to config the embedding model, I think we can talk about how to make it.",
          "created_at": "2024-12-19T01:33:51Z"
        },
        {
          "author": "e7217",
          "body": "@vkehfdl1 Thank you for your reply.\nI will also consider a good solution for this.\n",
          "created_at": "2024-12-19T08:36:39Z"
        }
      ]
    },
    {
      "issue_number": 1065,
      "title": "[BUG] When `file_type` is set to `all_files`, only `pdfminer` is used",
      "body": "**Describe the bug**\nHello,\n\nIt seems that when the parser module is configured with `file_type: all_files`, only pdfminer is applied. I have tried using `langchain_parser/upstagedocumentparse` and `llamaparser`, and both appear to use pdfminer exclusively. Even when I set the `output_format` to `html`, it seems like pdfminer is still being used. Am I mistaken about something?\n\nBelow is the YAML file I configured:\n```yaml\n- module_type: langchain_parse\n  parse_method: upstagedocumentparse\n  split: page\n  file_type: all_files\n  output_format: html\n```\nor\n```yaml\n- module_type: llamaparse\n  result_type: markdown\n  file_type: all_files\n  language: ko\n```\n\nAnd here is the result:\n![Image](https://github.com/user-attachments/assets/6f2a893b-74ba-484b-b77d-40790c8adf98)\n\nI would appreciate your help. Thank you.",
      "state": "open",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-12-20T03:22:11Z",
      "updated_at": "2024-12-21T01:39:44Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1065/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1065",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1065",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:44.851950",
      "comments": [
        {
          "author": "e7217",
          "body": "I found the way to set all_files.\nTo use it, we should set `all_files` in `start_parsing()` to `True`\n```python\nparser.start_parsing(parse_config, all_files=True)\n```\n\nwhat is correct way to set all files between as follows?\n- `start_parsing(... , all_files=True)`\n- set `file_type` to `all_files` in",
          "created_at": "2024-12-20T06:06:31Z"
        },
        {
          "author": "bwook00",
          "body": "First, sorry for the late comment (I was on vacation this week🫣)\n\nYou're right, you need to do `start_parsing(... , all_files=True)` to use file_type as all_files!\nI forgot and didn't write it in Docs,, sorry for the confusion!\n\nI designed it so that if you don't put `all_files=True`, it will parse ",
          "created_at": "2024-12-21T01:39:43Z"
        }
      ]
    },
    {
      "issue_number": 1029,
      "title": "[Feature Request] Update Cohere rerank model",
      "body": "We have 3.5 rerank model in Cohere! Make it default",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-03T05:16:49Z",
      "updated_at": "2024-12-19T03:02:16Z",
      "closed_at": "2024-12-19T03:02:16Z",
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1029/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1029",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1029",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.071309",
      "comments": []
    },
    {
      "issue_number": 1052,
      "title": "[API] Fix file upload cannot receive csv and json files",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-13T11:20:53Z",
      "updated_at": "2024-12-17T12:19:03Z",
      "closed_at": "2024-12-17T12:19:01Z",
      "labels": [
        "enhancement",
        "API"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1052/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1052",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1052",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.071331",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #1053",
          "created_at": "2024-12-17T12:19:02Z"
        }
      ]
    },
    {
      "issue_number": 1057,
      "title": "[BUG] Unable to access the documentation page.",
      "body": "**Describe the bug**\n\nHi,\n\nThe documentation page is showing a 404 error. Could you please check it?\"\n\n![Image](https://github.com/user-attachments/assets/07eabde9-5278-4395-b01e-64fdbdc9393c)\n",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-12-16T13:40:10Z",
      "updated_at": "2024-12-16T23:15:41Z",
      "closed_at": "2024-12-16T23:15:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1057/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1057",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1057",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.388844",
      "comments": []
    },
    {
      "issue_number": 1054,
      "title": "[Feature Request] Add file page_num at /v1/retrieve",
      "body": "like /v1/stream\nIt needs to have page_num at the endpoint also. (file metadatas)",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-13T13:40:29Z",
      "updated_at": "2024-12-15T07:16:34Z",
      "closed_at": "2024-12-15T07:16:34Z",
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1054/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1054",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1054",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.388876",
      "comments": []
    },
    {
      "issue_number": 1033,
      "title": "[BUG] ID Mismatch Error in VectorDB During Evaluation",
      "body": "**Describe the bug**\nHello,\n\nI have a question regarding the error described below.\nIt appears that the error occurs because the id values in the corpus data located in the benchmark/data folder do not match the id values retrieved from the VectorDB.\n \n- corpus data :\n![Image](https://github.com/user-attachments/assets/4d9980b6-f991-467a-94ca-7e1cebc8b472)\n\n- vector db :\n![Image](https://github.com/user-attachments/assets/ab2fc2b4-43eb-403c-9c69-eea9d352a25a)\n\n- code :\n![Image](https://github.com/user-attachments/assets/2cf926ca-e79b-4a22-acf9-74e0251ecdd7)\n![Image](https://github.com/user-attachments/assets/e7cc7330-0685-4bf8-a09a-49722191e5b4)\n\n**Full logs**\n```bash\n[12/03/24 16:58:20] ERROR    [__init__.py:60] >> Unexpected exception                                              __init__.py:60\n                             ╭──────────────────────── Traceback (most recent call last) ────────────────────────╮               \n                             │ /root/.pyenv/versions/3.10.13/lib/python3.10/runpy.py:196 in _run_module_as_main  │               \n                             │                                                                                   │               \n                             │   193 │   main_globals = sys.modules[\"__main__\"].__dict__                         │               \n                             │   194 │   if alter_argv:                                                          │               \n                             │   195 │   │   sys.argv[0] = mod_spec.origin                                       │               \n                             │ ❱ 196 │   return _run_code(code, main_globals, None,                              │               \n                             │   197 │   │   │   │   │    \"__main__\", mod_spec)                                  │               \n                             │   198                                                                             │               \n                             │   199 def run_module(mod_name, init_globals=None,                                 │               \n                             │                                                                                   │               \n                             │ /root/.pyenv/versions/3.10.13/lib/python3.10/runpy.py:86 in _run_code             │               \n                             │                                                                                   │               \n                             │    83 │   │   │   │   │      __loader__ = loader,                                 │               \n                             │    84 │   │   │   │   │      __package__ = pkg_name,                              │               \n                             │    85 │   │   │   │   │      __spec__ = mod_spec)                                 │               \n                             │ ❱  86 │   exec(code, run_globals)                                                 │               \n                             │    87 │   return run_globals                                                      │               \n                             │    88                                                                             │               \n                             │    89 def _run_module_code(code, init_globals=None,                               │               \n                             │                                                                                   │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/lib │               \n                             │ s/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py:71 in <module> │               \n                             │                                                                                   │               \n                             │   68 │                                                                            │               \n                             │   69 │   from debugpy.server import cli                                           │               \n                             │   70 │                                                                            │               \n                             │ ❱ 71 │   cli.main()                                                               │               \n                             │   72                                                                              │               \n                             │                                                                                   │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/lib │               \n                             │ s/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py:5 │               \n                             │ 01 in main                                                                        │               \n                             │                                                                                   │               \n                             │   498 │   │   │   │   \"code\": run_code,                                           │               \n                             │   499 │   │   │   │   \"pid\": attach_to_pid,                                       │               \n                             │   500 │   │   │   }[options.target_kind]                                          │               \n                             │ ❱ 501 │   │   │   run()                                                           │               \n                             │   502 │   except SystemExit as exc:                                               │               \n                             │   503 │   │   log.reraise_exception(                                              │               \n                             │   504 │   │   │   \"Debuggee exited via SystemExit: {0!r}\", exc.code, level=\"debug │               \n                             │                                                                                   │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/lib │               \n                             │ s/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py:3 │               \n                             │ 51 in run_file                                                                    │               \n                             │                                                                                   │               \n                             │   348 │   log.describe_environment(\"Pre-launch environment:\")                     │               \n                             │   349 │                                                                           │               \n                             │   350 │   log.info(\"Running file {0!r}\", target)                                  │               \n                             │ ❱ 351 │   runpy.run_path(target, run_name=\"__main__\")                             │               \n                             │   352                                                                             │               \n                             │   353                                                                             │               \n                             │   354 def run_module():                                                           │               \n                             │                                                                                   │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/lib │               \n                             │ s/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py:310 in run_path         │               \n                             │                                                                                   │               \n                             │   307 │   │   # Not a valid sys.path entry, so run the code directly              │               \n                             │   308 │   │   # execfile() doesn't help as we want to allow compiled files        │               \n                             │   309 │   │   code, fname = _get_code_from_file(run_name, path_name)              │               \n                             │ ❱ 310 │   │   return _run_module_code(code, init_globals, run_name, pkg_name=pkg_ │               \n                             │       script_name=fname)                                                          │               \n                             │   311 │   else:                                                                   │               \n                             │   312 │   │   # Finder is defined for path, so add it to                          │               \n                             │   313 │   │   # the start of sys.path                                             │               \n                             │                                                                                   │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/lib │               \n                             │ s/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py:127 in _run_module_code │               \n                             │                                                                                   │               \n                             │   124 │   fname = script_name if mod_spec is None else mod_spec.origin            │               \n                             │   125 │   with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):       │               \n                             │   126 │   │   mod_globals = temp_module.module.__dict__                           │               \n                             │ ❱ 127 │   │   _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_ │               \n                             │       script_name)                                                                │               \n                             │   128 │   # Copy the globals of the temporary module, as they                     │               \n                             │   129 │   # may be cleared when the temporary module goes away                    │               \n                             │   130 │   return mod_globals.copy()                                               │               \n                             │                                                                                   │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/lib │               \n                             │ s/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py:118 in _run_code        │               \n                             │                                                                                   │               \n                             │   115 │   run_globals.update(                                                     │               \n                             │   116 │   │   __name__=mod_name, __file__=fname, __cached__=cached, __doc__=None, │               \n                             │       __loader__=loader, __package__=pkg_name, __spec__=mod_spec                  │               \n                             │   117 │   )                                                                       │               \n                             │ ❱ 118 │   exec(code, run_globals)                                                 │               \n                             │   119 │   return run_globals                                                      │               \n                             │   120                                                                             │               \n                             │   121                                                                             │               \n                             │                                                                                   │               \n                             │ /app/evaluate/main.py:186 in <module>                                             │               \n                             │                                                                                   │               \n                             │   183 │   │   │   opt[\"qa_data_path\"] = os.path.join(current_dir,                 │               \n                             │       \"qa\",\"parsed_{}_chunk_{}_qa.parquet\".format(i,j))                           │               \n                             │   184 │   │   │   opt[\"corpus_data_path\"] = os.path.join(current_dir,             │               \n                             │       \"qa\",\"parsed_{}_chunk_{}_corpus.parquet\".format(i,j))                       │               \n                             │   185 │   │   │   opt[\"project_dir\"] = os.path.join(current_dir, \"benchmark\")     │               \n                             │ ❱ 186 │   │   │   evaluate(**opt)                                                 │               \n                             │   187                                                                             │               \n                             │                                                                                   │               \n                             │ /app/evaluate/main.py:158 in evaluate                                             │               \n                             │                                                                                   │               \n                             │   155 │   │   os.makedirs(project_dir)                                            │               \n                             │   156 │                                                                           │               \n                             │   157 │   evaluator = Evaluator(qa_data_path, corpus_data_path, project_dir=proje │               \n                             │ ❱ 158 │   evaluator.start_trial(config, skip_validation=True)                     │               \n                             │   159                                                                             │               \n                             │   160                                                                             │               \n                             │   161 if __name__ == \"__main__\":                                                  │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/evaluator.py:206 in start_trial                              │               \n                             │                                                                                   │               \n                             │   203 │   │   │   │   if i == 0:                                                  │               \n                             │   204 │   │   │   │   │   previous_result = self.qa_data                          │               \n                             │   205 │   │   │   │   logger.info(f\"Running node line {node_line_name}...\")       │               \n                             │ ❱ 206 │   │   │   │   previous_result = run_node_line(                            │               \n                             │   207 │   │   │   │   │   node_line, node_line_dir, previous_result, progress, ta │               \n                             │   208 │   │   │   │   )                                                           │               \n                             │   209                                                                             │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/node_line.py:52 in run_node_line                             │               \n                             │                                                                                   │               \n                             │   49 │                                                                            │               \n                             │   50 │   summary_lst = []                                                         │               \n                             │   51 │   for node in nodes:                                                       │               \n                             │ ❱ 52 │   │   previous_result = node.run(previous_result, node_line_dir)           │               \n                             │   53 │   │   node_summary_df = load_summary_file(                                 │               \n                             │   54 │   │   │   os.path.join(node_line_dir, node.node_type, \"summary.csv\")       │               \n                             │   55 │   │   )                                                                    │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/schema/node.py:57 in run                                     │               \n                             │                                                                                   │               \n                             │    54 │   def run(self, previous_result: pd.DataFrame, node_line_dir: str) -> pd. │               \n                             │    55 │   │   logger.info(f\"Running node {self.node_type}...\")                    │               \n                             │    56 │   │   input_modules, input_params = self.get_param_combinations()         │               \n                             │ ❱  57 │   │   return self.run_node(                                               │               \n                             │    58 │   │   │   modules=input_modules,                                          │               \n                             │    59 │   │   │   module_params=input_params,                                     │               \n                             │    60 │   │   │   previous_result=previous_result,                                │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/nodes/retrieval/run.py:173 in run_retrieval_node             │               \n                             │                                                                                   │               \n                             │   170 │   │   │   │   zip(modules, module_params),                                │               \n                             │   171 │   │   │   )                                                               │               \n                             │   172 │   │   )                                                                   │               \n                             │ ❱ 173 │   │   semantic_results, semantic_times = run(semantic_modules, semantic_m │               \n                             │   174 │   │   semantic_summary_df = save_and_summary(                             │               \n                             │   175 │   │   │   semantic_modules,                                               │               \n                             │   176 │   │   │   semantic_module_params,                                         │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/nodes/retrieval/run.py:71 in run                             │               \n                             │                                                                                   │               \n                             │    68 │   │   :return: First, it returns list of result dataframe.                │               \n                             │    69 │   │   Second, it returns list of execution times.                         │               \n                             │    70 │   │   \"\"\"                                                                 │               \n                             │ ❱  71 │   │   result, execution_times = zip(                                      │               \n                             │    72 │   │   │   *map(                                                           │               \n                             │    73 │   │   │   │   lambda task: measure_speed(                                 │               \n                             │    74 │   │   │   │   │   task[0].run_evaluator,                                  │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/nodes/retrieval/run.py:73 in <lambda>                        │               \n                             │                                                                                   │               \n                             │    70 │   │   \"\"\"                                                                 │               \n                             │    71 │   │   result, execution_times = zip(                                      │               \n                             │    72 │   │   │   *map(                                                           │               \n                             │ ❱  73 │   │   │   │   lambda task: measure_speed(                                 │               \n                             │    74 │   │   │   │   │   task[0].run_evaluator,                                  │               \n                             │    75 │   │   │   │   │   project_dir=project_dir,                                │               \n                             │    76 │   │   │   │   │   previous_result=previous_result,                        │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/strategy.py:14 in measure_speed                              │               \n                             │                                                                                   │               \n                             │    11 │   Method for measuring execution speed of the function.                   │               \n                             │    12 │   \"\"\"                                                                     │               \n                             │    13 │   start_time = time.time()                                                │               \n                             │ ❱  14 │   result = func(*args, **kwargs)                                          │               \n                             │    15 │   end_time = time.time()                                                  │               \n                             │    16 │   return result, end_time - start_time                                    │               \n                             │    17                                                                             │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/schema/base.py:27 in run_evaluator                           │               \n                             │                                                                                   │               \n                             │   24 │   ):                                                                       │               \n                             │   25 │   │   log_to_file(content=f\"Running {cls.__name__} with {cls.run_evaluator │               \n                             │   26 │   │   instance = cls(project_dir, *args, **kwargs)                         │               \n                             │ ❱ 27 │   │   result = instance.pure(previous_result, *args, **kwargs)             │               \n                             │   28 │   │   del instance                                                         │               \n                             │   29 │   │   return result                                                        │               \n                             │   30                                                                              │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/utils/util.py:72 in wrapper                                  │               \n                             │                                                                                   │               \n                             │    69 │   def decorator_result_to_dataframe(func: Callable):                      │               \n                             │    70 │   │   @functools.wraps(func)                                              │               \n                             │    71 │   │   def wrapper(*args, **kwargs) -> pd.DataFrame:                       │               \n                             │ ❱  72 │   │   │   results = func(*args, **kwargs)                                 │               \n                             │    73 │   │   │   if len(column_names) == 1:                                      │               \n                             │    74 │   │   │   │   df_input = {column_names[0]: results}                       │               \n                             │    75 │   │   │   else:                                                           │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/nodes/retrieval/vectordb.py:73 in pure                       │               \n                             │                                                                                   │               \n                             │    70 │   │   queries = self.cast_to_run(previous_result)                         │               \n                             │    71 │   │   pure_params = pop_params(self._pure, kwargs)                        │               \n                             │    72 │   │   ids, scores = self._pure(queries, **pure_params)                    │               \n                             │ ❱  73 │   │   contents = fetch_contents(self.corpus_df, ids)                      │               \n                             │    74 │   │   return contents, ids, scores                                        │               \n                             │    75 │                                                                           │               \n                             │    76 │   def _pure(                                                              │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/utils/util.py:40 in fetch_contents                           │               \n                             │                                                                                   │               \n                             │    37 │   ):                                                                      │               \n                             │    38 │   │   return list(map(lambda x: fetch_one_content(corpus_data, x, column_ │               \n                             │    39 │                                                                           │               \n                             │ ❱  40 │   result = flatten_apply(                                                 │               \n                             │    41 │   │   fetch_contents_pure, ids, corpus_data=corpus_data, column_name=colu │               \n                             │    42 │   )                                                                       │               \n                             │    43 │   return result                                                           │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/utils/util.py:377 in flatten_apply                           │               \n                             │                                                                                   │               \n                             │   374 │   \"\"\"                                                                     │               \n                             │   375 │   df = pd.DataFrame({\"col1\": nested_list})                                │               \n                             │   376 │   df = df.explode(\"col1\")                                                 │               \n                             │ ❱ 377 │   df[\"result\"] = func(df[\"col1\"].tolist(), **kwargs)                      │               \n                             │   378 │   return df.groupby(level=0, sort=False)[\"result\"].apply(list).tolist()   │               \n                             │   379                                                                             │               \n                             │   380                                                                             │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/utils/util.py:38 in fetch_contents_pure                      │               \n                             │                                                                                   │               \n                             │    35 │   def fetch_contents_pure(                                                │               \n                             │    36 │   │   ids: List[str], corpus_data: pd.DataFrame, column_name: str         │               \n                             │    37 │   ):                                                                      │               \n                             │ ❱  38 │   │   return list(map(lambda x: fetch_one_content(corpus_data, x, column_ │               \n                             │    39 │                                                                           │               \n                             │    40 │   result = flatten_apply(                                                 │               \n                             │    41 │   │   fetch_contents_pure, ids, corpus_data=corpus_data, column_name=colu │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/utils/util.py:38 in <lambda>                                 │               \n                             │                                                                                   │               \n                             │    35 │   def fetch_contents_pure(                                                │               \n                             │    36 │   │   ids: List[str], corpus_data: pd.DataFrame, column_name: str         │               \n                             │    37 │   ):                                                                      │               \n                             │ ❱  38 │   │   return list(map(lambda x: fetch_one_content(corpus_data, x, column_ │               \n                             │    39 │                                                                           │               \n                             │    40 │   result = flatten_apply(                                                 │               \n                             │    41 │   │   fetch_contents_pure, ids, corpus_data=corpus_data, column_name=colu │               \n                             │                                                                                   │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/s │               \n                             │ ite-packages/autorag/utils/util.py:57 in fetch_one_content                        │               \n                             │                                                                                   │               \n                             │    54 │   │   │   return None                                                     │               \n                             │    55 │   │   fetch_result = corpus_data[corpus_data[id_column_name] == id_]      │               \n                             │    56 │   │   if fetch_result.empty:                                              │               \n                             │ ❱  57 │   │   │   raise ValueError(f\"doc_id: {id_} not found in corpus_data.\")    │               \n                             │    58 │   │   else:                                                               │               \n                             │    59 │   │   │   return fetch_result[column_name].iloc[0]                        │               \n                             │    60 │   else:                                                                   │               \n                             ╰───────────────────────────────────────────────────────────────────────────────────╯               \n                             ValueError: doc_id: 5713eedc-1e2e-4395-8efe-06433f40a094 not found in corpus_data.                  \nException ignored in: <function VectorDB.__del__ at 0x7f7096dde560>\nTraceback (most recent call last):\n  File \"/root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/nodes/retrieval/vectordb.py\", line 66, in __del__\n  File \"/root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/nodes/retrieval/base.py\", line 28, in __del__\n  File \"/root/.pyenv/versions/3.10.13/lib/python3.10/logging/__init__.py\", line 1477, in info\n  File \"/root/.pyenv/versions/3.10.13/lib/python3.10/logging/__init__.py\", line 1624, in _log\n  File \"/root/.pyenv/versions/3.10.13/lib/python3.10/logging/__init__.py\", line 1634, in handle\n  File \"/root/.pyenv/versions/3.10.13/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n  File \"/root/.pyenv/versions/3.10.13/lib/python3.10/logging/__init__.py\", line 968, in handle\n  File \"/root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/rich/logging.py\", line 168, in emit\n  File \"/root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/rich/logging.py\", line 229, in render\n  File \"/root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/rich/_log_render.py\", line 43, in __call__\nImportError: sys.meta_path is None, Python is likely shutting down\n```\n\nWhen performing the `evaluate` step repeatedly, items seem to accumulate in the VectorDB.  \n![Image](https://github.com/user-attachments/assets/634b714d-074a-4893-bb5a-aaaaf338a8ab)\n\nShould the number of items in the VectorDB collection always be reset to zero before performing the `evaluate` step?  \n\nIt would be very helpful to understand the intention behind implementing this flow, as it will assist me in using this package more effectively.\n\nThank you for your assistance!  \n",
      "state": "open",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-12-03T08:50:45Z",
      "updated_at": "2024-12-13T03:27:13Z",
      "closed_at": null,
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1033/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1033",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1033",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.388886",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@e7217 Did you use passage augmenter & validation?\nFrom now, the passage augmenter is not supporting the validation process, so you have to unable the validation process while evaluation.\nUsing `skip_validation` parameter",
          "created_at": "2024-12-03T09:20:50Z"
        },
        {
          "author": "e7217",
          "body": "@vkehfdl1 umm... This is my current configuration, and I do not include the passage augmenter in any of my stages.\n\n![Image](https://github.com/user-attachments/assets/cebf41ae-e02b-4bde-9c3d-0ba793ecf8ca)\n\n```yaml\nvectordb:\n- name: autorag_test\n  db_type: milvus\n  embedding_model: openai\n  collecti",
          "created_at": "2024-12-03T09:31:22Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@e7217 Thanks for providing the configuration sir! \nI will further investigate the issue. Thanks :)",
          "created_at": "2024-12-04T08:44:50Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Hi @e7217 Did you resolve the issue?\nBecause It was hard to reproduce the same error. Seems the id not in corpus data ingested earlier to the vector db. ",
          "created_at": "2024-12-12T05:34:02Z"
        },
        {
          "author": "e7217",
          "body": "@vkehfdl1 \nHello,\n\nThe issue has not been resolved yet. I’ve written some temporary code to work around the issue, but it’s not ideal for general use. \n\n1. Delete all trials in benchmark dir.\n2. Delete a collection in vectordb\n3. Try running the code for evaluation.\n\nI haven’t found a proper solutio",
          "created_at": "2024-12-13T00:23:01Z"
        }
      ]
    },
    {
      "issue_number": 1049,
      "title": "[Feature Request] Add docx support at parsing",
      "body": "![Image](https://github.com/user-attachments/assets/d5a6c7ca-62c7-49c6-9098-52b430ae61d5)\n\n\nAdd docx support!",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-12T06:22:34Z",
      "updated_at": "2024-12-12T06:25:50Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "AutoRAG Core",
        "frontend"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1049/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1049",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1049",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.622808",
      "comments": []
    },
    {
      "issue_number": 1042,
      "title": "[BUG] Unexpected keyword argument 'embedding_model' for passage filter module",
      "body": "**Describe the bug**\nunexpected keyword argument 'embedding_model' error occurred in passage filter module such as `SimilarityThresholdCutoff._pure()`\n\n**To Reproduce**\n1. set `embedding_model` of `similarity_threshold_cutoff` in config.yaml\n2. Run autorag\n\n**Expected behavior**\nSucceed\n\n**Full Error log**\nTypeError: SimilarityThresholdCutoff._pure() got an unexpected keyword argument 'embedding_model'\n\n**Code that bug is happened**\nAfter init `cls` instance, `embedding_model` key must be popped from `kwargs`. But `kwargs` in `BaseModule` and `SimilarityThresholdCutoff` have different id like deep copy.\n```python\nclass BaseModule(metaclass=ABCMeta):\n    @classmethod\n        def run_evaluator(\n            cls,\n            project_dir: Union[str, Path],\n            previous_result: pd.DataFrame,\n            *args,\n            **kwargs,\n        ):\n            instance = cls(project_dir, *args, **kwargs)\n            result = instance.pure(previous_result, *args, **kwargs)\n            del instance\n            return result\nclass SimilarityThresholdCutoff(BasePassageFilter):\n    def __init__(self, project_dir: str, *args, **kwargs):\n        super().__init__(project_dir, *args, **kwargs)\n        embedding_model_str = kwargs.pop(\"embedding_model\", \"openai\")\n        self.embedding_model = embedding_models[embedding_model_str]()\n```",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2024-12-08T14:25:34Z",
      "updated_at": "2024-12-12T05:20:28Z",
      "closed_at": "2024-12-12T05:20:28Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1042/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1042",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1042",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.622832",
      "comments": [
        {
          "author": "rjwharry",
          "body": "I'm figuring out how to pass kwargs as shallow copy",
          "created_at": "2024-12-08T14:25:58Z"
        }
      ]
    },
    {
      "issue_number": 1045,
      "title": "[Feature Request] change parse_result parquet file name at `all_files = True`",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-12-09T05:42:57Z",
      "updated_at": "2024-12-09T06:08:19Z",
      "closed_at": "2024-12-09T06:08:19Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1045/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1045",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1045",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.848283",
      "comments": []
    },
    {
      "issue_number": 1040,
      "title": "[HotFix] I think the logic at the file type reduction is wrong at the parser",
      "body": "**Describe the bug**\nparse/run.py 56:61\n\n```python\n# create a list of only those file_types that are in file_types but not in set_file_types\nmissing_file_types = list(file_types - set_file_types)\nif list(set_file_types - file_types):\n\traise ValueError(\n\t\tf\"File types {list(set_file_types - file_types)} are not in the data path.”\n\t)\n```\nI don’t know why the hell we raise an error when `set_file_types - file_types`. I bet people wants just ignore the error when this kind of error occured. \n\n\n**To Reproduce**\nWhen you put file_type to pdf, csv, html, md…\nOnly put pdf files in the folder.\nYou can get an `ValueError: File types ['csv', 'json', 'xml', 'html', 'md'] are not in the data path.`\n\n**Expected behavior**\nI think it supposed to act like this.\n\n### 1. If there are no file type, even user write an file_type in the YAML file.\n\nJust ignore it. Don’t load the module that doesn’t have any file is quite good idea.\n\n ### 2. If there is a new file type, even the user doesn’t write an file_type in the YAML file.\n\nRaise an warning, not error. I bet skip it is okay. But just raise an warning for user will notice it.\n\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-08T08:21:57Z",
      "updated_at": "2024-12-09T05:18:34Z",
      "closed_at": "2024-12-09T05:18:34Z",
      "labels": [
        "bug",
        "enhancement",
        "data creation",
        "AutoRAG Core"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1040/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1040",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1040",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:45.848304",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@bwook00 \n",
          "created_at": "2024-12-08T08:22:06Z"
        }
      ]
    },
    {
      "issue_number": 1039,
      "title": "[API] Add parsed file view endpoint",
      "body": "GET /projects/{project_id}/parse/${parsed_name}?filename={filename}&page=${pageNum}\n\nIf page not given, return -1.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-08T05:59:00Z",
      "updated_at": "2024-12-08T08:43:52Z",
      "closed_at": "2024-12-08T08:43:51Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1039/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1039",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1039",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:46.089702",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #1041",
          "created_at": "2024-12-08T08:43:51Z"
        }
      ]
    },
    {
      "issue_number": 1037,
      "title": "[Feature Request] Add prompt to exclude question like “What is the file name?\"",
      "body": "At the factoid question generation, there are some questions like “What is the file name?” When used add_file_name chunk. \nIt is not good sign to the quesiton generation.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-07T09:15:30Z",
      "updated_at": "2024-12-07T13:47:14Z",
      "closed_at": "2024-12-07T13:47:14Z",
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1037/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1037",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1037",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:46.293374",
      "comments": []
    },
    {
      "issue_number": 1034,
      "title": "[BUG] [API] Prevent double execution of optimization process in the API server from now",
      "body": "We have to manage the resources more efficiently, but now the duplicate optimization process can kill the whole machine.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-04T06:26:07Z",
      "updated_at": "2024-12-04T08:39:57Z",
      "closed_at": "2024-12-04T08:39:57Z",
      "labels": [
        "High Priority",
        "API"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1034/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1034",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1034",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:46.293400",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Just increase memory of instance",
          "created_at": "2024-12-04T07:28:49Z"
        }
      ]
    },
    {
      "issue_number": 989,
      "title": "[API] fix Korean (UTF-8) Project Title",
      "body": "<!-- 새 문제의 본문을 편집한 다음 편집기의 오른쪽 상단에 있는 ✓ \"문제 만들기\" 버튼을 클릭합니다. 첫 번째 줄은 문제 제목이 됩니다. 담당자와 레이블은 빈 줄 뒤에 옵니다. 문제의 본문을 시작하기 전에 빈 줄을 남겨 두세요. -->",
      "state": "open",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-23T14:26:09Z",
      "updated_at": "2024-12-03T09:19:32Z",
      "closed_at": null,
      "labels": [
        "API"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/989/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/989",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/989",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:46.483134",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Plus the API server cannot get the korean title of file names also",
          "created_at": "2024-11-24T02:27:14Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I fixed this ",
          "created_at": "2024-11-30T03:55:07Z"
        },
        {
          "author": "hongsw",
          "body": "@vkehfdl1 which branch or commit?\n",
          "created_at": "2024-12-03T06:30:01Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@hongsw Ah sorry I misunderstood the issue.\nIt is \"project title\", not \"file name\"",
          "created_at": "2024-12-03T09:19:30Z"
        }
      ]
    },
    {
      "issue_number": 997,
      "title": "[API] Move config folder data to SQLite trial schema",
      "body": "- [x] \"Move config folder data to SQLite trial schema\"\n\n_Originally posted by @hongsw in https://github.com/Marker-Inc-Korea/AutoRAG/issues/987#issuecomment-2496531965_\n            ",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-25T02:10:17Z",
      "updated_at": "2024-12-03T06:38:06Z",
      "closed_at": "2024-12-03T06:38:05Z",
      "labels": [
        "API"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/997/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/997",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/997",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:46.703024",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "I did iot",
          "created_at": "2024-12-03T06:38:05Z"
        }
      ]
    },
    {
      "issue_number": 1031,
      "title": "[BUG] API request is not working at the corpus with start_end_idx",
      "body": "\nAt `extract_retrieve_passage`\nThere is an error.\n\nMake a proper test for it. Thx",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-03T05:35:51Z",
      "updated_at": "2024-12-03T06:30:20Z",
      "closed_at": "2024-12-03T06:30:20Z",
      "labels": [
        "bug",
        "High Priority",
        "AutoRAG Core"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1031/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1031",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1031",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:46.949983",
      "comments": []
    },
    {
      "issue_number": 1019,
      "title": "[Feature Request] AutoRAG RaaS Clinet",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\n\n\n# AutoRAGClient: RAG Pipeline Management Client\n\n## Purpose\nAutoRAGClient is a client library designed to simplify the creation and management of RAG (Retrieval-Augmented Generation) pipelines through a REST API interface.\n\n## Key Features\n1. **Project Management**\n   - Create new RAG projects\n   - Manage documents within projects\n   - Project-level configuration\n\n2. **Document Processing**\n   - Support for multiple file formats (PDF, TXT, CSV, MD)\n   - Batch file uploads with pattern matching\n   - File tracking and management\n\n3. **Embedding Management**\n   - Generate vector embeddings for documents\n   - Flexible vector storage options\n   - Automatic embedding model selection\n\n4. **RAG Pipeline Operations**\n   - Create and manage RAG pipelines\n   - Question-answering capabilities\n   - Context retrieval and results management\n\n## Technical Details\n- Asynchronous HTTP client (using aiohttp)\n- Context manager support (`async with` syntax)\n- Environment-based configuration\n- Integrated logging system\n- Error handling and API error abstractions\n\n## Usage Example\n\n\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "open",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-30T20:03:25Z",
      "updated_at": "2024-12-03T06:28:53Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Client"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1019/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1019",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1019",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:46.950003",
      "comments": []
    },
    {
      "issue_number": 1027,
      "title": "[Feature Request] Control remote true or false at the run_api cli",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-12-03T01:57:46Z",
      "updated_at": "2024-12-03T04:05:25Z",
      "closed_at": "2024-12-03T04:05:25Z",
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1027/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1027",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1027",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:48.654007",
      "comments": []
    },
    {
      "issue_number": 1024,
      "title": "[Feature Request] A feature to view comprehensive statistics and optimal settings in AutoRAG",
      "body": "**Is your feature request related to a problem? Please describe.**\nI have been using AutoRAG and performing the parse, chunk, and evaluate steps separately, and then reviewing the data stored in the benchmark one by one. However, it is time-consuming to click through each step and difficult to compare the results. Is there a feature that would allow me to view the optimal settings and detailed statistics considering all the configuration values collectively?\n\n```python\n    for parsed_raw in parsed_raw_files:\n        for chunk in chunked_file_list:\n            parsed_raw_index = parsed_raw.split(\"/\")[-1].split(\".\")[0]\n            chunk_index = str(chunk).split(\"/\")[-1].split(\".\")[0]\n            \n            if not parsed_raw_index == chunk.parent.name:\n                continue\n            initial_raw = Raw(pd.read_parquet(parsed_raw, engine=\"pyarrow\"))\n            initial_corpus = Corpus(pd.read_parquet(chunk, engine=\"pyarrow\"), initial_raw)\n            qa = initial_corpus.sample(random_single_hop, n=len(initial_corpus.data), random_state=random.randint(1,100)).map(\n                    lambda df: df.reset_index(drop=True),\n                ).make_retrieval_gt_contents().batch_apply(\n                    multiple_queries_gen,  # query generation\n                    llm=llm,\n                    lang=\"ko\",\n                    n=10,\n                ).batch_apply(\n                    make_basic_gen_gt,  # answer generation (basic)\n                    llm=llm,\n                    lang=\"ko\",\n                ).batch_apply(\n                    make_concise_gen_gt,  # answer generation (concise)\n                    llm=llm,\n                    lang=\"ko\",\n                ).filter(\n                    dontknow_filter_rule_based,  # filter unanswerable questions\n                    lang=\"ko\",\n                )\n                \n            qa_dir_name = \"qa\"\n            \n            if not os.path.exists(os.path.join(current_dir, qa_dir_name)):\n                os.makedirs(os.path.join(current_dir, qa_dir_name))\n\n            output_path = os.path.join(current_dir, qa_dir_name, f\"parsed_{parsed_raw_index}_chunk_{chunk_index}_qa.parquet\")\n            corpus_output_path = os.path.join(current_dir, qa_dir_name, f\"parsed_{parsed_raw_index}_chunk_{chunk_index}_corpus.parquet\")\n            qa.to_parquet(output_path, corpus_output_path)\n```\n\n```python\n    for i in range(10):\n        for j in range(4):\n            opt[\"config\"] = os.path.join(current_dir, \"config\", \"evaluate_config.yaml\")\n            opt[\"qa_data_path\"] = os.path.join(current_dir, \"qa\",\"parsed_{}_chunk_{}_qa.parquet\".format(i,j))\n            opt[\"corpus_data_path\"] = os.path.join(current_dir, \"qa\",\"parsed_{}_chunk_{}_corpus.parquet\".format(i,j))\n            opt[\"project_dir\"] = os.path.join(current_dir, \"benchmark\")\n            evaluate(**opt)\n```\n\n![Image](https://github.com/user-attachments/assets/2e1779e8-265f-496a-bf58-275b81ef7c33)\n",
      "state": "open",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-12-02T07:55:20Z",
      "updated_at": "2024-12-02T12:15:45Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1024/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1024",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1024",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:48.654029",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@e7217 That is a really good idea! \nWe have dashboard, but it doesn't show the parsing and chunking views.\n",
          "created_at": "2024-12-02T10:11:18Z"
        }
      ]
    },
    {
      "issue_number": 1022,
      "title": "[Feature Request] How to control a large number of requests",
      "body": "**Is your feature request related to a problem? Please describe.**\nIt seems that an error occurs when there are too many requests. Would it be possible to have a parameter that allows us to control this?\n\n**logs**\n```\n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/llama_index/llms/openai/base.py:627 in achat                │               \n                             │                                                                                                                                                           │               \n                             │   624 │   │   │   achat_fn = self._achat                                                                                                                  │               \n                             │   625 │   │   else:                                                                                                                                       │               \n                             │   626 │   │   │   achat_fn = acompletion_to_chat_decorator(self._acomplete)                                                                               │               \n                             │ ❱ 627 │   │   return await achat_fn(messages, **kwargs)                                                                                                   │               \n                             │   628 │                                                                                                                                                   │               \n                             │   629 │   @llm_chat_callback()                                                                                                                            │               \n                             │   630 │   async def astream_chat(                                                                                                                         │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:189 in async_wrapped           │               \n                             │                                                                                                                                                           │               \n                             │   186 │   │   │   # calling the same wrapped functions multiple times in the same stack                                                                   │               \n                             │   187 │   │   │   copy = self.copy()                                                                                                                      │               \n                             │   188 │   │   │   async_wrapped.statistics = copy.statistics  # type: ignore[attr-defined]                                                                │               \n                             │ ❱ 189 │   │   │   return await copy(fn, *args, **kwargs)                                                                                                  │               \n                             │   190 │   │                                                                                                                                               │               \n                             │   191 │   │   # Preserve attributes                                                                                                                       │               \n                             │   192 │   │   async_wrapped.retry = self  # type: ignore[attr-defined]                                                                                    │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:111 in __call__                │               \n                             │                                                                                                                                                           │               \n                             │   108 │   │                                                                                                                                               │               \n                             │   109 │   │   retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)                                                            │               \n                             │   110 │   │   while True:                                                                                                                                 │               \n                             │ ❱ 111 │   │   │   do = await self.iter(retry_state=retry_state)                                                                                           │               \n                             │   112 │   │   │   if isinstance(do, DoAttempt):                                                                                                           │               \n                             │   113 │   │   │   │   try:                                                                                                                                │               \n                             │   114 │   │   │   │   │   result = await fn(*args, **kwargs)                                                                                              │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:153 in iter                    │               \n                             │                                                                                                                                                           │               \n                             │   150 │   │   self._begin_iter(retry_state)                                                                                                               │               \n                             │   151 │   │   result = None                                                                                                                               │               \n                             │   152 │   │   for action in self.iter_state.actions:                                                                                                      │               \n                             │ ❱ 153 │   │   │   result = await action(retry_state)                                                                                                      │               \n                             │   154 │   │   return result                                                                                                                               │               \n                             │   155 │                                                                                                                                                   │               \n                             │   156 │   def __iter__(self) -> t.Generator[AttemptManager, None, None]:                                                                                  │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/tenacity/_utils.py:99 in inner                              │               \n                             │                                                                                                                                                           │               \n                             │    96 │   │   return call                                                                                                                                 │               \n                             │    97 │                                                                                                                                                   │               \n                             │    98 │   async def inner(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:                                                                         │               \n                             │ ❱  99 │   │   return call(*args, **kwargs)                                                                                                                │               \n                             │   100 │                                                                                                                                                   │               \n                             │   101 │   return inner                                                                                                                                    │               \n                             │   102                                                                                                                                                     │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/tenacity/__init__.py:418 in exc_check                       │               \n                             │                                                                                                                                                           │               \n                             │   415 │   │   │   │   fut = t.cast(Future, rs.outcome)                                                                                                    │               \n                             │   416 │   │   │   │   retry_exc = self.retry_error_cls(fut)                                                                                               │               \n                             │   417 │   │   │   │   if self.reraise:                                                                                                                    │               \n                             │ ❱ 418 │   │   │   │   │   raise retry_exc.reraise()                                                                                                       │               \n                             │   419 │   │   │   │   raise retry_exc from fut.exception()                                                                                                │               \n                             │   420 │   │   │                                                                                                                                           │               \n                             │   421 │   │   │   self._add_action_func(exc_check)                                                                                                        │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/tenacity/__init__.py:185 in reraise                         │               \n                             │                                                                                                                                                           │               \n                             │   182 │                                                                                                                                                   │               \n                             │   183 │   def reraise(self) -> t.NoReturn:                                                                                                                │               \n                             │   184 │   │   if self.last_attempt.failed:                                                                                                                │               \n                             │ ❱ 185 │   │   │   raise self.last_attempt.result()                                                                                                        │               \n                             │   186 │   │   raise self                                                                                                                                  │               \n                             │   187 │                                                                                                                                                   │               \n                             │   188 │   def __str__(self) -> str:                                                                                                                       │               \n                             │                                                                                                                                                           │               \n                             │ /root/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/_base.py:451 in result                                                                    │               \n                             │                                                                                                                                                           │               \n                             │   448 │   │   │   │   if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:                                                                              │               \n                             │   449 │   │   │   │   │   raise CancelledError()                                                                                                          │               \n                             │   450 │   │   │   │   elif self._state == FINISHED:                                                                                                       │               \n                             │ ❱ 451 │   │   │   │   │   return self.__get_result()                                                                                                      │               \n                             │   452 │   │   │   │                                                                                                                                       │               \n                             │   453 │   │   │   │   self._condition.wait(timeout)                                                                                                       │               \n                             │   454                                                                                                                                                     │               \n                             │                                                                                                                                                           │               \n                             │ /root/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/_base.py:403 in __get_result                                                              │               \n                             │                                                                                                                                                           │               \n                             │   400 │   def __get_result(self):                                                                                                                         │               \n                             │   401 │   │   if self._exception:                                                                                                                         │               \n                             │   402 │   │   │   try:                                                                                                                                    │               \n                             │ ❱ 403 │   │   │   │   raise self._exception                                                                                                               │               \n                             │   404 │   │   │   finally:                                                                                                                                │               \n                             │   405 │   │   │   │   # Break a reference cycle with the exception in self._exception                                                                     │               \n                             │   406 │   │   │   │   self = None                                                                                                                         │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/tenacity/asyncio/__init__.py:114 in __call__                │               \n                             │                                                                                                                                                           │               \n                             │   111 │   │   │   do = await self.iter(retry_state=retry_state)                                                                                           │               \n                             │   112 │   │   │   if isinstance(do, DoAttempt):                                                                                                           │               \n                             │   113 │   │   │   │   try:                                                                                                                                │               \n                             │ ❱ 114 │   │   │   │   │   result = await fn(*args, **kwargs)                                                                                              │               \n                             │   115 │   │   │   │   except BaseException:  # noqa: B902                                                                                                 │               \n                             │   116 │   │   │   │   │   retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]                                                             │               \n                             │   117 │   │   │   │   else:                                                                                                                               │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/llama_index/llms/openai/base.py:674 in _achat               │               \n                             │                                                                                                                                                           │               \n                             │   671 │   │   message_dicts = to_openai_message_dicts(messages, model=self.model)                                                                         │               \n                             │   672 │   │                                                                                                                                               │               \n                             │   673 │   │   if self.reuse_client:                                                                                                                       │               \n                             │ ❱ 674 │   │   │   response = await aclient.chat.completions.create(                                                                                       │               \n                             │   675 │   │   │   │   messages=message_dicts, stream=False, **self._get_model_kwargs(**kwargs)                                                            │               \n                             │   676 │   │   │   )                                                                                                                                       │               \n                             │   677 │   │   else:                                                                                                                                       │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/resources/chat/completions.py:1661 in create         │               \n                             │                                                                                                                                                           │               \n                             │   1658 │   │   timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,                                                                              │               \n                             │   1659 │   ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:                                                                                        │               \n                             │   1660 │   │   validate_response_format(response_format)                                                                                                  │               \n                             │ ❱ 1661 │   │   return await self._post(                                                                                                                   │               \n                             │   1662 │   │   │   \"/chat/completions\",                                                                                                                   │               \n                             │   1663 │   │   │   body=await async_maybe_transform(                                                                                                      │               \n                             │   1664 │   │   │   │   {                                                                                                                                  │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1840 in post                         │               \n                             │                                                                                                                                                           │               \n                             │   1837 │   │   opts = FinalRequestOptions.construct(                                                                                                      │               \n                             │   1838 │   │   │   method=\"post\", url=path, json_data=body, files=await                                                                                   │               \n                             │        async_to_httpx_files(files), **options                                                                                                             │               \n                             │   1839 │   │   )                                                                                                                                          │               \n                             │ ❱ 1840 │   │   return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)                                                             │               \n                             │   1841 │                                                                                                                                                  │               \n                             │   1842 │   async def patch(                                                                                                                               │               \n                             │   1843 │   │   self,                                                                                                                                      │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1534 in request                      │               \n                             │                                                                                                                                                           │               \n                             │   1531 │   │   else:                                                                                                                                      │               \n                             │   1532 │   │   │   retries_taken = 0                                                                                                                      │               \n                             │   1533 │   │                                                                                                                                              │               \n                             │ ❱ 1534 │   │   return await self._request(                                                                                                                │               \n                             │   1535 │   │   │   cast_to=cast_to,                                                                                                                       │               \n                             │   1536 │   │   │   options=options,                                                                                                                       │               \n                             │   1537 │   │   │   stream=stream,                                                                                                                         │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1620 in _request                     │               \n                             │                                                                                                                                                           │               \n                             │   1617 │   │   │                                                                                                                                          │               \n                             │   1618 │   │   │   if remaining_retries > 0 and self._should_retry(err.response):                                                                         │               \n                             │   1619 │   │   │   │   await err.response.aclose()                                                                                                        │               \n                             │ ❱ 1620 │   │   │   │   return await self._retry_request(                                                                                                  │               \n                             │   1621 │   │   │   │   │   input_options,                                                                                                                 │               \n                             │   1622 │   │   │   │   │   cast_to,                                                                                                                       │               \n                             │   1623 │   │   │   │   │   retries_taken=retries_taken,                                                                                                   │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1667 in _retry_request               │               \n                             │                                                                                                                                                           │               \n                             │   1664 │   │                                                                                                                                              │               \n                             │   1665 │   │   await anyio.sleep(timeout)                                                                                                                 │               \n                             │   1666 │   │                                                                                                                                              │               \n                             │ ❱ 1667 │   │   return await self._request(                                                                                                                │               \n                             │   1668 │   │   │   options=options,                                                                                                                       │               \n                             │   1669 │   │   │   cast_to=cast_to,                                                                                                                       │               \n                             │   1670 │   │   │   retries_taken=retries_taken + 1,                                                                                                       │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1620 in _request                     │               \n                             │                                                                                                                                                           │               \n                             │   1617 │   │   │                                                                                                                                          │               \n                             │   1618 │   │   │   if remaining_retries > 0 and self._should_retry(err.response):                                                                         │               \n                             │   1619 │   │   │   │   await err.response.aclose()                                                                                                        │               \n                             │ ❱ 1620 │   │   │   │   return await self._retry_request(                                                                                                  │               \n                             │   1621 │   │   │   │   │   input_options,                                                                                                                 │               \n                             │   1622 │   │   │   │   │   cast_to,                                                                                                                       │               \n                             │   1623 │   │   │   │   │   retries_taken=retries_taken,                                                                                                   │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1667 in _retry_request               │               \n                             │                                                                                                                                                           │               \n                             │   1664 │   │                                                                                                                                              │               \n                             │   1665 │   │   await anyio.sleep(timeout)                                                                                                                 │               \n                             │   1666 │   │                                                                                                                                              │               \n                             │ ❱ 1667 │   │   return await self._request(                                                                                                                │               \n                             │   1668 │   │   │   options=options,                                                                                                                       │               \n                             │   1669 │   │   │   cast_to=cast_to,                                                                                                                       │               \n                             │   1670 │   │   │   retries_taken=retries_taken + 1,                                                                                                       │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1620 in _request                     │               \n                             │                                                                                                                                                           │               \n                             │   1617 │   │   │                                                                                                                                          │               \n                             │   1618 │   │   │   if remaining_retries > 0 and self._should_retry(err.response):                                                                         │               \n                             │   1619 │   │   │   │   await err.response.aclose()                                                                                                        │               \n                             │ ❱ 1620 │   │   │   │   return await self._retry_request(                                                                                                  │               \n                             │   1621 │   │   │   │   │   input_options,                                                                                                                 │               \n                             │   1622 │   │   │   │   │   cast_to,                                                                                                                       │               \n                             │   1623 │   │   │   │   │   retries_taken=retries_taken,                                                                                                   │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1667 in _retry_request               │               \n                             │                                                                                                                                                           │               \n                             │   1664 │   │                                                                                                                                              │               \n                             │   1665 │   │   await anyio.sleep(timeout)                                                                                                                 │               \n                             │   1666 │   │                                                                                                                                              │               \n                             │ ❱ 1667 │   │   return await self._request(                                                                                                                │               \n                             │   1668 │   │   │   options=options,                                                                                                                       │               \n                             │   1669 │   │   │   cast_to=cast_to,                                                                                                                       │               \n                             │   1670 │   │   │   retries_taken=retries_taken + 1,                                                                                                       │               \n                             │                                                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1635 in _request                     │               \n                             │                                                                                                                                                           │               \n                             │   1632 │   │   │   │   await err.response.aread()                                                                                                         │               \n                             │   1633 │   │   │                                                                                                                                          │               \n                             │   1634 │   │   │   log.debug(\"Re-raising status error\")                                                                                                   │               \n                             │ ❱ 1635 │   │   │   raise self._make_status_error_from_response(err.response) from None                                                                    │               \n                             │   1636 │   │                                                                                                                                              │               \n                             │   1637 │   │   return await self._process_response(                                                                                                       │               \n                             │   1638 │   │   │   cast_to=cast_to,                                                                                                                       │               \n                             ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯               \n                             RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-Y9VgTSCB4k6OcvPF42h5yAYO on tokens per min                  \n                             (TPM): Limit 200000, Used 199980, Requested 2213. Please try again in 657ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type':                 \n                             'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}  \n```",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-12-02T02:13:47Z",
      "updated_at": "2024-12-02T11:38:35Z",
      "closed_at": "2024-12-02T11:38:34Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1022/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1022",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1022",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:48.894094",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@e7217 \n\nYou can decrease the batch_size at the YAML file! You can find 'batch_size' setting at the almost every modules that are using API calls. \nIt will be slower, but it will prevent rate limit error.",
          "created_at": "2024-12-02T10:12:21Z"
        },
        {
          "author": "e7217",
          "body": "> [@e7217](https://github.com/e7217)\n> \n> You can decrease the batch_size at the YAML file! You can find 'batch_size' setting at the almost every modules that are using API calls. It will be slower, but it will prevent rate limit error.\n\n@vkehfdl1 Oh! I completely forgot about that! Thank you!",
          "created_at": "2024-12-02T11:38:32Z"
        }
      ]
    },
    {
      "issue_number": 1003,
      "title": "[BUG] AttributeError: vllm_model",
      "body": "**Describe the bug**\nThe following error occurred when using the vllm module. It did not occur when using the openai_llm module. This error may have occurred when the `vllm_model` was deleted. I am unsure of the exact cause, so I would appreciate it if you could investigate.\n\n**configuration**\n```yaml\nvectordb:\n  ...\nnode_lines:\n- node_line_name: retrieve_node_line\n  ...\n- node_line_name: post_retrieve_node_line\n  nodes:\n    - node_type: prompt_maker\n      ...\n    - node_type: generator\n      strategy:\n        metrics: # bert_score 및 g_eval 사용 역시 추천합니다. 빠른 실행을 위해 여기서는 제외하고 하겠습니다.\n          - metric_name: rouge\n          - metric_name: sem_score\n            embedding_model: openai\n#          - metric_name: bert_score\n#            lang: ko\n      modules:\n        - module_type: openai_llm\n          llm: gpt-4o-mini\n          temperature: [ 0.1, 1.0 ]\n          batch: 16\n        - module_type: vllm\n          llm: meta-llama/Llama-3.2-1B-instruct\n          temperature: [ 0.1, 1.0 ]\n          # tensor_parallel_size: 4 # If the gpu is two.\n          max_tokens: 128\n          max_model_len: 200\n          gpu_memory_utilization: 0.5\n```\n\n**Full Error log**\n```\nIngesting VectorDB... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:00\nEvaluating...         ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━  67% 2/3 0:03:08\n[11/25/24 18:06:53] ERROR    [__init__.py:60] >> Unexpected exception                                                                                      __init__.py:60\n                             ╭──────────────────────────────────────────── Traceback (most recent call last) ────────────────────────────────────────────╮               \n                             │ /root/.pyenv/versions/3.10.13/lib/python3.10/runpy.py:196 in _run_module_as_main                                          │               \n                             │                                                                                                                           │               \n                             │   193 │   main_globals = sys.modules[\"__main__\"].__dict__                                                                 │               \n                             │   194 │   if alter_argv:                                                                                                  │               \n                             │   195 │   │   sys.argv[0] = mod_spec.origin                                                                               │               \n                             │ ❱ 196 │   return _run_code(code, main_globals, None,                                                                      │               \n                             │   197 │   │   │   │   │    \"__main__\", mod_spec)                                                                          │               \n                             │   198                                                                                                                     │               \n                             │   199 def run_module(mod_name, init_globals=None,                                                                         │               \n                             │                                                                                                                           │               \n                             │ /root/.pyenv/versions/3.10.13/lib/python3.10/runpy.py:86 in _run_code                                                     │               \n                             │                                                                                                                           │               \n                             │    83 │   │   │   │   │      __loader__ = loader,                                                                         │               \n                             │    84 │   │   │   │   │      __package__ = pkg_name,                                                                      │               \n                             │    85 │   │   │   │   │      __spec__ = mod_spec)                                                                         │               \n                             │ ❱  86 │   exec(code, run_globals)                                                                                         │               \n                             │    87 │   return run_globals                                                                                              │               \n                             │    88                                                                                                                     │               \n                             │    89 def _run_module_code(code, init_globals=None,                                                                       │               \n                             │                                                                                                                           │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher │               \n                             │ /../../debugpy/__main__.py:71 in <module>                                                                                 │               \n                             │                                                                                                                           │               \n                             │   68 │                                                                                                                    │               \n                             │   69 │   from debugpy.server import cli                                                                                   │               \n                             │   70 │                                                                                                                    │               \n                             │ ❱ 71 │   cli.main()                                                                                                       │               \n                             │   72                                                                                                                      │               \n                             │                                                                                                                           │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher │               \n                             │ /../../debugpy/../debugpy/server/cli.py:501 in main                                                                       │               \n                             │                                                                                                                           │               \n                             │   498 │   │   │   │   \"code\": run_code,                                                                                   │               \n                             │   499 │   │   │   │   \"pid\": attach_to_pid,                                                                               │               \n                             │   500 │   │   │   }[options.target_kind]                                                                                  │               \n                             │ ❱ 501 │   │   │   run()                                                                                                   │               \n                             │   502 │   except SystemExit as exc:                                                                                       │               \n                             │   503 │   │   log.reraise_exception(                                                                                      │               \n                             │   504 │   │   │   \"Debuggee exited via SystemExit: {0!r}\", exc.code, level=\"debug\"                                        │               \n                             │                                                                                                                           │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher │               \n                             │ /../../debugpy/../debugpy/server/cli.py:351 in run_file                                                                   │               \n                             │                                                                                                                           │               \n                             │   348 │   log.describe_environment(\"Pre-launch environment:\")                                                             │               \n                             │   349 │                                                                                                                   │               \n                             │   350 │   log.info(\"Running file {0!r}\", target)                                                                          │               \n                             │ ❱ 351 │   runpy.run_path(target, run_name=\"__main__\")                                                                     │               \n                             │   352                                                                                                                     │               \n                             │   353                                                                                                                     │               \n                             │   354 def run_module():                                                                                                   │               \n                             │                                                                                                                           │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundl │               \n                             │ e/pydevd_runpy.py:310 in run_path                                                                                         │               \n                             │                                                                                                                           │               \n                             │   307 │   │   # Not a valid sys.path entry, so run the code directly                                                      │               \n                             │   308 │   │   # execfile() doesn't help as we want to allow compiled files                                                │               \n                             │   309 │   │   code, fname = _get_code_from_file(run_name, path_name)                                                      │               \n                             │ ❱ 310 │   │   return _run_module_code(code, init_globals, run_name, pkg_name=pkg_name,                                    │               \n                             │       script_name=fname)                                                                                                  │               \n                             │   311 │   else:                                                                                                           │               \n                             │   312 │   │   # Finder is defined for path, so add it to                                                                  │               \n                             │   313 │   │   # the start of sys.path                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundl │               \n                             │ e/pydevd_runpy.py:127 in _run_module_code                                                                                 │               \n                             │                                                                                                                           │               \n                             │   124 │   fname = script_name if mod_spec is None else mod_spec.origin                                                    │               \n                             │   125 │   with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):                                               │               \n                             │   126 │   │   mod_globals = temp_module.module.__dict__                                                                   │               \n                             │ ❱ 127 │   │   _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_name,                                    │               \n                             │       script_name)                                                                                                        │               \n                             │   128 │   # Copy the globals of the temporary module, as they                                                             │               \n                             │   129 │   # may be cleared when the temporary module goes away                                                            │               \n                             │   130 │   return mod_globals.copy()                                                                                       │               \n                             │                                                                                                                           │               \n                             │ /root/.vscode-server/extensions/ms-python.debugpy-2024.12.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundl │               \n                             │ e/pydevd_runpy.py:118 in _run_code                                                                                        │               \n                             │                                                                                                                           │               \n                             │   115 │   run_globals.update(                                                                                             │               \n                             │   116 │   │   __name__=mod_name, __file__=fname, __cached__=cached, __doc__=None,                                         │               \n                             │       __loader__=loader, __package__=pkg_name, __spec__=mod_spec                                                          │               \n                             │   117 │   )                                                                                                               │               \n                             │ ❱ 118 │   exec(code, run_globals)                                                                                         │               \n                             │   119 │   return run_globals                                                                                              │               \n                             │   120                                                                                                                     │               \n                             │   121                                                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /app/evaluate/main.py:138 in <module>                                                                                     │               \n                             │                                                                                                                           │               \n                             │   135 │   # parse()                                                                                                       │               \n                             │   136 │   # chunk()                                                                                                       │               \n                             │   137 │   # make_qa()                                                                                                     │               \n                             │ ❱ 138 │   evaluate()                                                                                                      │               \n                             │   139                                                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /app/evaluate/main.py:127 in evaluate                                                                                     │               \n                             │                                                                                                                           │               \n                             │   124 │   │   os.makedirs(project_dir)                                                                                    │               \n                             │   125 │                                                                                                                   │               \n                             │   126 │   evaluator = Evaluator(qa_data_path, corpus_data_path, project_dir=project_dir)                                  │               \n                             │ ❱ 127 │   evaluator.start_trial(config)                                                                                   │               \n                             │   128                                                                                                                     │               \n                             │   129                                                                                                                     │               \n                             │   130 if __name__ == \"__main__\":                                                                                          │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/evaluator.py:138 in │               \n                             │ start_trial                                                                                                               │               \n                             │                                                                                                                           │               \n                             │   135 │   │   │   validator = Validator(                                                                                  │               \n                             │   136 │   │   │   │   qa_data_path=self.qa_data_path, corpus_data_path=self.corpus_data_path                              │               \n                             │   137 │   │   │   )                                                                                                       │               \n                             │ ❱ 138 │   │   │   validator.validate(yaml_path)                                                                           │               \n                             │   139 │   │                                                                                                               │               \n                             │   140 │   │   os.environ[\"PROJECT_DIR\"] = self.project_dir                                                                │               \n                             │   141                                                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/validator.py:92 in  │               \n                             │ validate                                                                                                                  │               \n                             │                                                                                                                           │               \n                             │   89 │   │   │   │   corpus_data_path=corpus_path.name,                                                                   │               \n                             │   90 │   │   │   │   project_dir=temp_project_dir,                                                                        │               \n                             │   91 │   │   │   )                                                                                                        │               \n                             │ ❱ 92 │   │   │   evaluator.start_trial(yaml_path, skip_validation=True)                                                   │               \n                             │   93 │   │   │   qa_path.close()                                                                                          │               \n                             │   94 │   │   │   corpus_path.close()                                                                                      │               \n                             │   95 │   │   │   os.unlink(qa_path.name)                                                                                  │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/evaluator.py:206 in │               \n                             │ start_trial                                                                                                               │               \n                             │                                                                                                                           │               \n                             │   203 │   │   │   │   if i == 0:                                                                                          │               \n                             │   204 │   │   │   │   │   previous_result = self.qa_data                                                                  │               \n                             │   205 │   │   │   │   logger.info(f\"Running node line {node_line_name}...\")                                               │               \n                             │ ❱ 206 │   │   │   │   previous_result = run_node_line(                                                                    │               \n                             │   207 │   │   │   │   │   node_line, node_line_dir, previous_result, progress, task_eval                                  │               \n                             │   208 │   │   │   │   )                                                                                                   │               \n                             │   209                                                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/node_line.py:52 in  │               \n                             │ run_node_line                                                                                                             │               \n                             │                                                                                                                           │               \n                             │   49 │                                                                                                                    │               \n                             │   50 │   summary_lst = []                                                                                                 │               \n                             │   51 │   for node in nodes:                                                                                               │               \n                             │ ❱ 52 │   │   previous_result = node.run(previous_result, node_line_dir)                                                   │               \n                             │   53 │   │   node_summary_df = load_summary_file(                                                                         │               \n                             │   54 │   │   │   os.path.join(node_line_dir, node.node_type, \"summary.csv\")                                               │               \n                             │   55 │   │   )                                                                                                            │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/schema/node.py:57   │               \n                             │ in run                                                                                                                    │               \n                             │                                                                                                                           │               \n                             │    54 │   def run(self, previous_result: pd.DataFrame, node_line_dir: str) -> pd.DataFrame:                               │               \n                             │    55 │   │   logger.info(f\"Running node {self.node_type}...\")                                                            │               \n                             │    56 │   │   input_modules, input_params = self.get_param_combinations()                                                 │               \n                             │ ❱  57 │   │   return self.run_node(                                                                                       │               \n                             │    58 │   │   │   modules=input_modules,                                                                                  │               \n                             │    59 │   │   │   module_params=input_params,                                                                             │               \n                             │    60 │   │   │   previous_result=previous_result,                                                                        │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/nodes/generator/run │               \n                             │ .py:47 in run_generator_node                                                                                              │               \n                             │                                                                                                                           │               \n                             │    44 │   if \"generation_gt\" not in qa_data.columns:                                                                      │               \n                             │    45 │   │   raise ValueError(\"You must have 'generation_gt' column in qa.parquet.\")                                     │               \n                             │    46 │                                                                                                                   │               \n                             │ ❱  47 │   results, execution_times = zip(                                                                                 │               \n                             │    48 │   │   *map(                                                                                                       │               \n                             │    49 │   │   │   lambda x: measure_speed(                                                                                │               \n                             │    50 │   │   │   │   x[0].run_evaluator,                                                                                 │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/nodes/generator/run │               \n                             │ .py:49 in <lambda>                                                                                                        │               \n                             │                                                                                                                           │               \n                             │    46 │                                                                                                                   │               \n                             │    47 │   results, execution_times = zip(                                                                                 │               \n                             │    48 │   │   *map(                                                                                                       │               \n                             │ ❱  49 │   │   │   lambda x: measure_speed(                                                                                │               \n                             │    50 │   │   │   │   x[0].run_evaluator,                                                                                 │               \n                             │    51 │   │   │   │   project_dir=project_dir,                                                                            │               \n                             │    52 │   │   │   │   previous_result=previous_result,                                                                    │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/strategy.py:14 in   │               \n                             │ measure_speed                                                                                                             │               \n                             │                                                                                                                           │               \n                             │    11 │   Method for measuring execution speed of the function.                                                           │               \n                             │    12 │   \"\"\"                                                                                                             │               \n                             │    13 │   start_time = time.time()                                                                                        │               \n                             │ ❱  14 │   result = func(*args, **kwargs)                                                                                  │               \n                             │    15 │   end_time = time.time()                                                                                          │               \n                             │    16 │   return result, end_time - start_time                                                                            │               \n                             │    17                                                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/schema/base.py:25   │               \n                             │ in run_evaluator                                                                                                          │               \n                             │                                                                                                                           │               \n                             │   22 │   │   *args,                                                                                                       │               \n                             │   23 │   │   **kwargs,                                                                                                    │               \n                             │   24 │   ):                                                                                                               │               \n                             │ ❱ 25 │   │   instance = cls(project_dir, *args, **kwargs)                                                                 │               \n                             │   26 │   │   result = instance.pure(previous_result, *args, **kwargs)                                                     │               \n                             │   27 │   │   del instance                                                                                                 │               \n                             │   28 │   │   return result                                                                                                │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/nodes/generator/vll │               \n                             │ m.py:29 in __init__                                                                                                       │               \n                             │                                                                                                                           │               \n                             │    26 │   │   sampling_params_init_params = pop_params(                                                                   │               \n                             │    27 │   │   │   SamplingParams.from_optional, input_kwargs                                                              │               \n                             │    28 │   │   )                                                                                                           │               \n                             │ ❱  29 │   │   self.vllm_model = LLM(model, **input_kwargs)                                                                │               \n                             │    30 │   │                                                                                                               │               \n                             │    31 │   │   # delete not sampling param keys in the kwargs                                                              │               \n                             │    32 │   │   kwargs_keys = list(kwargs.keys())                                                                           │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/utils.py:1028 in inner │               \n                             │                                                                                                                           │               \n                             │   1025 │   │   │   │   │   │   stacklevel=3,  # The inner function takes up one level                                     │               \n                             │   1026 │   │   │   │   │   )                                                                                              │               \n                             │   1027 │   │   │                                                                                                          │               \n                             │ ❱ 1028 │   │   │   return fn(*args, **kwargs)                                                                             │               \n                             │   1029 │   │                                                                                                              │               \n                             │   1030 │   │   return inner  # type: ignore                                                                               │               \n                             │   1031                                                                                                                    │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/entrypoints/llm.py:210 │               \n                             │ in __init__                                                                                                               │               \n                             │                                                                                                                           │               \n                             │   207 │   │   self.engine_class = self.get_engine_class()                                                                 │               \n                             │   208 │   │                                                                                                               │               \n                             │   209 │   │   # TODO(rob): enable mp by default (issue with fork vs spawn)                                                │               \n                             │ ❱ 210 │   │   self.llm_engine = self.engine_class.from_engine_args(                                                       │               \n                             │   211 │   │   │   engine_args, usage_context=UsageContext.LLM_CLASS)                                                      │               \n                             │   212 │   │                                                                                                               │               \n                             │   213 │   │   self.request_counter = Counter()                                                                            │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/engine/llm_engine.py:5 │               \n                             │ 85 in from_engine_args                                                                                                    │               \n                             │                                                                                                                           │               \n                             │    582 │   │   engine_config = engine_args.create_engine_config()                                                         │               \n                             │    583 │   │   executor_class = cls._get_executor_cls(engine_config)                                                      │               \n                             │    584 │   │   # Create the LLM engine.                                                                                   │               \n                             │ ❱  585 │   │   engine = cls(                                                                                              │               \n                             │    586 │   │   │   vllm_config=engine_config,                                                                             │               \n                             │    587 │   │   │   executor_class=executor_class,                                                                         │               \n                             │    588 │   │   │   log_stats=not engine_args.disable_log_stats,                                                           │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/engine/llm_engine.py:3 │               \n                             │ 47 in __init__                                                                                                            │               \n                             │                                                                                                                           │               \n                             │    344 │   │   self.input_processor = input_registry.create_input_processor(                                              │               \n                             │    345 │   │   │   model_config)                                                                                          │               \n                             │    346 │   │                                                                                                              │               \n                             │ ❱  347 │   │   self.model_executor = executor_class(vllm_config=vllm_config, )                                            │               \n                             │    348 │   │                                                                                                              │               \n                             │    349 │   │   if self.model_config.task != \"embedding\":                                                                  │               \n                             │    350 │   │   │   self._initialize_kv_caches()                                                                           │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/executor/executor_base │               \n                             │ .py:36 in __init__                                                                                                        │               \n                             │                                                                                                                           │               \n                             │    33 │   │   self.speculative_config = vllm_config.speculative_config                                                    │               \n                             │    34 │   │   self.prompt_adapter_config = vllm_config.prompt_adapter_config                                              │               \n                             │    35 │   │   self.observability_config = vllm_config.observability_config                                                │               \n                             │ ❱  36 │   │   self._init_executor()                                                                                       │               \n                             │    37 │                                                                                                                   │               \n                             │    38 │   @abstractmethod                                                                                                 │               \n                             │    39 │   def _init_executor(self) -> None:                                                                               │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/executor/gpu_executor. │               \n                             │ py:39 in _init_executor                                                                                                   │               \n                             │                                                                                                                           │               \n                             │    36 │   │   │   \"GPUExecutor only supports single GPU.\")                                                                │               \n                             │    37 │   │                                                                                                               │               \n                             │    38 │   │   self.driver_worker = self._create_worker()                                                                  │               \n                             │ ❱  39 │   │   self.driver_worker.init_device()                                                                            │               \n                             │    40 │   │   self.driver_worker.load_model()                                                                             │               \n                             │    41 │                                                                                                                   │               \n                             │    42 │   def _get_worker_kwargs(                                                                                         │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/worker/worker.py:145   │               \n                             │ in init_device                                                                                                            │               \n                             │                                                                                                                           │               \n                             │   142 │   │   │   raise RuntimeError(                                                                                     │               \n                             │   143 │   │   │   │   f\"Not support device type: {self.device_config.device}\")                                            │               \n                             │   144 │   │   # Initialize the distributed environment.                                                                   │               \n                             │ ❱ 145 │   │   init_worker_distributed_environment(self.parallel_config, self.rank,                                        │               \n                             │   146 │   │   │   │   │   │   │   │   │   │   │   self.distributed_init_method,                                           │               \n                             │   147 │   │   │   │   │   │   │   │   │   │   │   self.local_rank)                                                        │               \n                             │   148 │   │   # Set random seed.                                                                                          │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/worker/worker.py:462   │               \n                             │ in init_worker_distributed_environment                                                                                    │               \n                             │                                                                                                                           │               \n                             │   459 │   init_distributed_environment(parallel_config.world_size, rank,                                                  │               \n                             │   460 │   │   │   │   │   │   │   │    distributed_init_method, local_rank)                                               │               \n                             │   461 │                                                                                                                   │               \n                             │ ❱ 462 │   ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,                                         │               \n                             │   463 │   │   │   │   │   │   │   │   │     parallel_config.pipeline_parallel_size)                                       │               \n                             │   464                                                                                                                     │               \n                             │   465                                                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/vllm/distributed/parallel_s │               \n                             │ tate.py:1102 in ensure_model_parallel_initialized                                                                         │               \n                             │                                                                                                                           │               \n                             │   1099 │   or ensure tensor-parallel and pipeline-parallel sizes are equal to expected                                    │               \n                             │   1100 │   values if the model parallel groups are initialized.                                                           │               \n                             │   1101 │   \"\"\"                                                                                                            │               \n                             │ ❱ 1102 │   backend = backend or torch.distributed.get_backend(                                                            │               \n                             │   1103 │   │   get_world_group().device_group)                                                                            │               \n                             │   1104 │   if not model_parallel_is_initialized():                                                                        │               \n                             │   1105 │   │   initialize_model_parallel(tensor_model_parallel_size,                                                      │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/torch/distributed/distribut │               \n                             │ ed_c10d.py:1215 in get_backend                                                                                            │               \n                             │                                                                                                                           │               \n                             │   1212 │   if _rank_not_in_group(pg):                                                                                     │               \n                             │   1213 │   │   raise ValueError(\"Invalid process group specified\")                                                        │               \n                             │   1214 │   pg_store = _world.pg_map[pg] if pg in _world.pg_map else None                                                  │               \n                             │ ❱ 1215 │   return Backend(not_none(pg_store)[0])                                                                          │               \n                             │   1216                                                                                                                    │               \n                             │   1217                                                                                                                    │               \n                             │   1218 def _get_process_group_uid(pg: ProcessGroup) -> int:                                                               │               \n                             │                                                                                                                           │               \n                             │ /root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/torch/utils/_typing_utils.p │               \n                             │ y:13 in not_none                                                                                                          │               \n                             │                                                                                                                           │               \n                             │   10                                                                                                                      │               \n                             │   11 def not_none(obj: Optional[T]) -> T:                                                                                 │               \n                             │   12 │   if obj is None:                                                                                                  │               \n                             │ ❱ 13 │   │   raise TypeError(\"Invariant encountered: value was None when it should not be\")                               │               \n                             │   14 │   return obj                                                                                                       │               \n                             │   15                                                                                                                      │               \n                             ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯               \n                             TypeError: Invariant encountered: value was None when it should not be                                                                      \nException ignored in: <function Vllm.__del__ at 0x7f35f96b6dd0>\nTraceback (most recent call last):\n  File \"/root/.cache/pypoetry/virtualenvs/app-rag-sample-9TtSrW0h-py3.10/lib/python3.10/site-packages/autorag/nodes/generator/vllm.py\", line 53, in __del__\nAttributeError: vllm_model\n[rank0]:[W1125 18:06:59.980650822 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n```\n\n**Desktop (please complete the following information):**\n - OS: Ubuntu 20.04\n - Python version: 3.10.13\n",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-11-25T09:18:42Z",
      "updated_at": "2024-12-02T01:50:14Z",
      "closed_at": "2024-12-02T01:50:14Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1003/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1003",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1003",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:49.129657",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://github.com/pytorch/pytorch/issues/132003\nIt can be a pytorch issue!",
          "created_at": "2024-11-27T12:11:46Z"
        },
        {
          "author": "e7217",
          "body": "> [pytorch/pytorch#132003](https://github.com/pytorch/pytorch/issues/132003) It can be a pytorch issue!\n\nThank you for providing the link as well. I will take a look at it. 😃 ",
          "created_at": "2024-11-27T12:34:40Z"
        },
        {
          "author": "e7217",
          "body": "This issue appears to occur when the PyTorch environment is not completely removed. As a result, the issue arises when the vllm module is called twice. To avoid this, it is recommended to set `skip_validation=True` and call the vllm module only once.\n\n```python\nevaluator = Evaluator(qa_data_path, co",
          "created_at": "2024-11-29T00:41:44Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@e7217 Hi! I understand the issue:)\nIf the validation run and pytorch seems not properly not clear its cache.\nI added many codes to clear up the cache of pytorch, but it is hard to cover all hardware setups 🤯\n\nFirst, I think we need to add documentation that skip_validation is recommended to the vll",
          "created_at": "2024-11-29T03:45:33Z"
        },
        {
          "author": "e7217",
          "body": "@vkehfdl1 \nI will make sure to incorporate this into the document. Thank you!",
          "created_at": "2024-11-29T07:38:51Z"
        }
      ]
    },
    {
      "issue_number": 984,
      "title": "[API] fix validate and evaluation api config, set_trial_config",
      "body": "https://github.com/Auto-RAG/autorag-frontend/issues/3\n\n`/projects/<string:project_id>/trials/<string:trial_id>/validate`\n`/projects/<string:project_id>/trials/<string:trial_id>/evaluate`\n\n\nIssue\nManaging nested data with CSVs is inefficient, and frequent dumps are cumbersome.\n\nSolution\nSwitch to SQLite with asynchronous execution managed by Celery for safe task handling.\n\nPlan\nTest project creation, trials (1, 2, 3), validation, and execution in the new setup.\n\n",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-23T01:32:41Z",
      "updated_at": "2024-12-01T09:03:50Z",
      "closed_at": "2024-12-01T09:03:48Z",
      "labels": [
        "enhancement",
        "API"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/984/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/984",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/984",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:49.323391",
      "comments": [
        {
          "author": "hongsw",
          "body": "draft",
          "created_at": "2024-11-24T00:31:26Z"
        },
        {
          "author": "vkehfdl1",
          "body": "close at #1016",
          "created_at": "2024-12-01T09:03:48Z"
        }
      ]
    },
    {
      "issue_number": 1006,
      "title": "[Feature Request] Create multiple questions",
      "body": "**Is your feature request related to a problem? Please describe.**\nI find the QA generation feature very useful and am eager to make full use of it. I have a question I'd like to ask.\n\nCurrently, it seems that only one question is generated per row in the corpus. Is there a feature that allows multiple questions to be generated for a single row? I couldn’t find this feature in the official documentation, so if I’ve missed something, I would appreciate your guidance.\n\nThank you for developing such a great library!\n",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-11-26T03:41:40Z",
      "updated_at": "2024-11-29T17:12:54Z",
      "closed_at": "2024-11-29T17:12:54Z",
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1006/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1006",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1006",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:49.549974",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://github.com/Marker-Inc-Korea/AutoRAG/pull/1009#issuecomment-2503759822",
          "created_at": "2024-11-29T13:30:21Z"
        }
      ]
    },
    {
      "issue_number": 1002,
      "title": "[Feature Request] support parsing multiple types of documents at once",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-25T08:11:26Z",
      "updated_at": "2024-11-29T12:20:10Z",
      "closed_at": "2024-11-29T12:20:10Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1002/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1002",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1002",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:49.724247",
      "comments": []
    },
    {
      "issue_number": 996,
      "title": "[API] The filepath configuration schema needs to be updated from (raw_data, parse, chunk, qa) to (files, knowledges, qa_docs).",
      "body": "- [x] Celery integration completed.  \n- [x] SQLiteProjectDB migration completed.  \n- [x] `trial_task.py` is to be determined (TBD); assistance needed from @vkehfdl1.  \n- [x] Frontend migration completed, including changes to the `waitForTask` function.\n- [ ] The filepath configuration schema needs to be updated from `(raw_data, parse, chunk, qa)` to `(files, knowledges, qa_docs)`.\n<img width=\"842\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1c96f6f1-0d05-45fb-ab98-cd126e8dc9db\">\nFigure 1: Files are initially managed in the files folder. In the knowledges folder, selected files are processed, parsed, and chunked for further use. QA documents are generated and stored in the qa_docs folder. Lastly, in the trials folder, specific knowledge and QA documents are selected to perform evaluations and generate reports.\n\n_Originally posted by @hongsw in https://github.com/Marker-Inc-Korea/AutoRAG/issues/987#issuecomment-2496144117_\n            ",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-25T02:08:37Z",
      "updated_at": "2024-11-29T11:44:21Z",
      "closed_at": "2024-11-29T11:44:20Z",
      "labels": [
        "API"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/996/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/996",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/996",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:49.724265",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #1011",
          "created_at": "2024-11-29T11:44:20Z"
        }
      ]
    },
    {
      "issue_number": 1013,
      "title": "[Feature Request] It seems that the validation logic is not only performing validation.",
      "body": "**Is your feature request related to a problem? Please describe.**\nWhen running AutoRAG, I expected the evaluation logic to be triggered once, but it seems to occur twice. \n![Image](https://github.com/user-attachments/assets/5aa2461b-8ead-4811-a823-5fb960ab5207)\nThe code related to this is in the validate function of the Validator class.\n```python\n# start Evaluate at temp project directory\nwith (\n\ttempfile.NamedTemporaryFile(suffix=\".parquet\", delete=False) as qa_path,\n\ttempfile.NamedTemporaryFile(suffix=\".parquet\", delete=False) as corpus_path,\n\ttempfile.TemporaryDirectory(ignore_cleanup_errors=True) as temp_project_dir,\n):\n\tsample_qa_df.to_parquet(qa_path.name, index=False)\n\tsample_corpus_df.to_parquet(corpus_path.name, index=False)\n\n\tevaluator = Evaluator(\n\t\tqa_data_path=qa_path.name,\n\t\tcorpus_data_path=corpus_path.name,\n\t\tproject_dir=temp_project_dir,\n\t)\n\tevaluator.start_trial(yaml_path, skip_validation=True)\n\tqa_path.close()\n\tcorpus_path.close()\n\tos.unlink(qa_path.name)\n\tos.unlink(corpus_path.name)\n```\n\nCalling the evaluation twice results in two API requests, causing the user to be charged twice. \nHow about performing the validation only for the form?\n",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-11-29T00:21:42Z",
      "updated_at": "2024-11-29T08:45:00Z",
      "closed_at": "2024-11-29T08:45:00Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1013/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1013",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/1013",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:49.946911",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@e7217 \nWhen we execute validation, we slice the data to the small pieces. Just 5 QAs. So it does not call many APIs. But, still loading and experimenting all features so can check the system quite well.\nSo it is intended feature.\n",
          "created_at": "2024-11-29T03:43:41Z"
        }
      ]
    },
    {
      "issue_number": 970,
      "title": "[BUG] azure AOAI hang to the final state",
      "body": "![Image](https://github.com/user-attachments/assets/7087ed36-da6a-4937-b557-0c212e72656a)\n\nnot only AOAI but also for ollama",
      "state": "open",
      "author": "hellangleZ",
      "author_type": "User",
      "created_at": "2024-11-21T04:52:56Z",
      "updated_at": "2024-11-28T03:34:05Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/970/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/970",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/970",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:50.160955",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hi @hellangleZ Is there any bug or log?\nPlus, what environment are you using?",
          "created_at": "2024-11-21T05:00:43Z"
        },
        {
          "author": "nllzz",
          "body": "@hellangleZ  hi guys could u share ur config.yaml and demo.py？\nthanks!",
          "created_at": "2024-11-28T03:34:04Z"
        }
      ]
    },
    {
      "issue_number": 982,
      "title": "[API] retrieve files from the raw_data for viewing in the frontend",
      "body": "`/projects/{{projectId}/artifacts/content?filename={{filename}}`",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-22T11:23:33Z",
      "updated_at": "2024-11-26T09:07:55Z",
      "closed_at": "2024-11-26T09:07:52Z",
      "labels": [
        "enhancement",
        "API"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/982/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/982",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/982",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:50.354894",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #1008",
          "created_at": "2024-11-26T09:07:52Z"
        }
      ]
    },
    {
      "issue_number": 999,
      "title": "[API] change api port from 5000 to 8000",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-25T04:04:02Z",
      "updated_at": "2024-11-26T07:59:10Z",
      "closed_at": "2024-11-26T07:59:08Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/999/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/999",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/999",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:50.543485",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #1007",
          "created_at": "2024-11-26T07:59:09Z"
        }
      ]
    },
    {
      "issue_number": 998,
      "title": "[BUG] [milvus] RPC error: [create_index], <MilvusException: (code=0, message=nlist out of range: [1, 65536])>",
      "body": "**Describe the bug**\nWhen trying to use Milvus as a vectordb, the following error occurs. Should I add any additional configuration values in the YAML settings?\n\n**logs**\n```bash\n[11/25/24 10:59:39] INFO     [evaluator.py:127] >>                                                                                                    evaluator.py:127\n                                             _        _____            _____                                                                                          \n                                  /\\        | |      |  __ \\     /\\   / ____|                                                                                         \n                                 /  \\  _   _| |_ ___ | |__) |   /  \\ | |  __                                                                                          \n                                / /\\ \\| | | | __/ _ \\|  _  /   / /\\ \\| | |_ |                                                                                         \n                               / ____ \\ |_| | || (_) | | \\ \\  / ____ \\ |__| |                                                                                         \n                              /_/    \\_\\__,_|\\__\\___/|_|  \\_\\/_/    \\_\\_____|                                                                                         \n                                                                                                                                                                      \n                                                                                                                                                                      \n                    INFO     [evaluator.py:128] >> Start Validation input data and config YAML file first. If you want to skip this, put the          evaluator.py:128\n                             --skip_validation flag or `skip_validation` at the start_trial function.                                                                 \n                    WARNING  [validator.py:50] >> Minimal Requested sample size (5) is larger than available records (1). Sampling will be limited to  validator.py:50\n                             1 records.                                                                                                                               \n                    INFO     [evaluator.py:228] >> Embedding BM25 corpus...                                                                           evaluator.py:228\n[11/25/24 10:59:53] INFO     [evaluator.py:248] >> BM25 corpus embedding complete.                                                                    evaluator.py:248\n                    ERROR    [decorators.py:147] >> RPC error: [load_collection], <MilvusException: (code=0, message=index doesn't exist,            decorators.py:147\n                             collectionID 453301748613080480)>, <Time:{'RPC start': '2024-11-25 10:59:53.542740', 'RPC error': '2024-11-25                            \n                             10:59:53.545308'}>                                                                                                                       \n[11/25/24 10:59:54] INFO     [_client.py:1787] >> HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"                            _client.py:1787\n[11/25/24 10:59:58] ERROR    [decorators.py:147] >> RPC error: [create_index], <MilvusException: (code=0, message=nlist out of range: [1, 65536])>,  decorators.py:147\n                             <Time:{'RPC start': '2024-11-25 10:59:58.492197', 'RPC error': '2024-11-25 10:59:58.498422'}>                                            \nIngesting VectorDB... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0% 0/1 0:00:05\n```\n\n**Desktop (please complete the following information):**\n - OS: Ubuntu 20.04\n - Python version : 3.10.13\n\n**configuration**\n```yaml\nvectordb:\n- name: autorag_2024_11_21\n  db_type: milvus\n  embedding_model: openai\n  collection_name: autorag_2024_11_21\n  uri: http://192.xxx.xxx.xxx:19530\n  # embedding_batch: 50\n  similarity_metric: cosine\nnode_lines:\n- node_line_name: retrieve_node_line\n  nodes:\n    - node_type: retrieval\n      strategy:\n        metrics: [retrieval_f1, retrieval_ndcg, retrieval_map]\n      top_k: 3\n      modules:\n        - module_type: bm25\n          bm25_tokenizer: [ ko_kiwi, ko_okt, ko_kkma ]\n        - module_type: vectordb\n          vectordb: autorag_2024_11_21\n        - module_type: hybrid_rrf\n        - module_type: hybrid_cc\n          normalize_method: [ mm, tmm ]\n- node_line_name: post_retrieve_node_line\n  nodes:\n    - node_type: prompt_maker\n      strategy:\n        metrics:\n          - metric_name: rouge\n          - metric_name: sem_score\n            embedding_model: openai\n#          - metric_name: bert_score\n#            lang: ko\n        generator_modules:\n          - module_type: openai_llm\n            llm: gpt-4o-mini\n      modules:\n        - module_type: fstring\n          prompt:\n          - | \n            단락을 읽고 질문에 답하세요. \\n 질문 : {query} \\n 단락: {retrieved_contents} \\n 답변 :\n          - |\n            단락을 읽고 질문에 답하세요. 답할때 단계별로 천천히 고심하여 답변하세요. 반드시 단락 내용을 기반으로 말하고 거짓을 말하지 마세요. \\n 질문: {query} \\n 단락: {retrieved_contents} \\n 답변 :\n    - node_type: generator\n      strategy:\n        metrics: # bert_score 및 g_eval 사용 역시 추천합니다. 빠른 실행을 위해 여기서는 제외하고 하겠습니다.\n          - metric_name: rouge\n          - metric_name: sem_score\n            embedding_model: openai\n#          - metric_name: bert_score\n#            lang: ko\n      modules:\n        - module_type: openai_llm\n          llm: gpt-4o-mini\n          temperature: [ 0.1, 1.0 ]\n          batch: 16\n```\n\n**Desktop (please complete the following information):**\n - OS: Ubuntu 20.04\n - Python version : 3.10.13",
      "state": "closed",
      "author": "e7217",
      "author_type": "User",
      "created_at": "2024-11-25T02:12:45Z",
      "updated_at": "2024-11-25T13:43:00Z",
      "closed_at": "2024-11-25T13:43:00Z",
      "labels": [
        "bug",
        "High Priority",
        "AutoRAG Core"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/998/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/998",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/998",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:50.781714",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@bwook00 \nI think we have problem to ingest large data into the Milvus. \nhttps://github.com/milvus-io/milvus/issues/16343\nCan you investigate this bug?\n\n@e7217 Looks like we have to change Milvus ingest code in `milvus.py` \nThank you for reporting us bug.",
          "created_at": "2024-11-25T03:28:48Z"
        }
      ]
    },
    {
      "issue_number": 882,
      "title": "[Feature Request] [Milvus] Configure index type",
      "body": "**Is your feature request related to a problem? Please describe.**\nNow, we use only IVF_FLAT.\nMaybe we can use other index type.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-23T17:10:26Z",
      "updated_at": "2024-11-25T13:43:00Z",
      "closed_at": "2024-11-25T13:43:00Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/882/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/882",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/882",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:50.971027",
      "comments": []
    },
    {
      "issue_number": 969,
      "title": "🐛 Fix WORK_DIR path resolution in development environment (api/)",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\nhttps://github.com/Marker-Inc-Korea/AutoRAG/blob/81a2f63e26f1ed7c4d292c340f4a82fa216a6263/api/app.py#L91\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Full Error log**\nIf applicable, add full error log to help explain your problem.\n\n**Code that bug is happened**\nIf applicable, add the code that bug is happened.\n(Especially, your AutoRAG YAML file or python codes that you wrote)\n\n**Desktop (please complete the following information):**\n - OS: [e.g. Windows, Linux, MacOS]\n - Python version [e.g. 3.10]\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-20T13:16:14Z",
      "updated_at": "2024-11-25T05:49:03Z",
      "closed_at": "2024-11-25T05:49:02Z",
      "labels": [
        "API"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/969/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/969",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/969",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:50.971049",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "I am on it!",
          "created_at": "2024-11-20T13:22:40Z"
        },
        {
          "author": "vkehfdl1",
          "body": "close at #971 and #968",
          "created_at": "2024-11-25T05:49:02Z"
        }
      ]
    },
    {
      "issue_number": 974,
      "title": "[API] Add endpoint for env variable management",
      "body": "GET /env => return all env variable key\n\nDELETE /env/<string:key> => delete the certain key as a env variable.\n\n\nWe will manage with '.env' file comes with app.py file. \nLoad it when the api-key needed endpoint starting. (parse, chunk, qa creation, validation, evaluate, chat)",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-21T08:33:49Z",
      "updated_at": "2024-11-25T05:48:36Z",
      "closed_at": "2024-11-25T05:48:35Z",
      "labels": [
        "enhancement",
        "API"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/974/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/974",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/974",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:51.163072",
      "comments": [
        {
          "author": "hongsw",
          "body": "Is It work with .env.local file?",
          "created_at": "2024-11-21T09:11:51Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@hongsw It works with `.env` file at the api folder. API key only for running AutoRAG.",
          "created_at": "2024-11-21T09:19:27Z"
        },
        {
          "author": "vkehfdl1",
          "body": "close at #975",
          "created_at": "2024-11-25T05:48:35Z"
        }
      ]
    },
    {
      "issue_number": 980,
      "title": "[API] Upload multiple files using /upload API endpoint",
      "body": "Now it gets only one file at a time.\nIt can get multiple files at one time. ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-22T08:32:26Z",
      "updated_at": "2024-11-25T05:48:16Z",
      "closed_at": "2024-11-25T05:48:15Z",
      "labels": [
        "enhancement",
        "API"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/980/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/980",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/980",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:51.349726",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #981",
          "created_at": "2024-11-25T05:48:15Z"
        }
      ]
    },
    {
      "issue_number": 995,
      "title": "[API] Start chunk in trial_task.py",
      "body": "- [x] Celery integration completed.  \n- [x] SQLiteProjectDB migration completed.  \n- [x] `trial_task.py` is to be determined (TBD); assistance needed from @vkehfdl1.  \n- [x] Frontend migration completed, including changes to the `waitForTask` function.\n- [ ] The filepath configuration schema needs to be updated from `(raw_data, parse, chunk, qa)` to `(files, knowledges, qa_docs)`.\n<img width=\"842\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1c96f6f1-0d05-45fb-ab98-cd126e8dc9db\">\nFigure 1: Files are initially managed in the files folder. In the knowledges folder, selected files are processed, parsed, and chunked for further use. QA documents are generated and stored in the qa_docs folder. Lastly, in the trials folder, specific knowledge and QA documents are selected to perform evaluations and generate reports.\n\n_Originally posted by @hongsw in https://github.com/Marker-Inc-Korea/AutoRAG/issues/987#issuecomment-2496144117_\n            ",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-25T02:08:05Z",
      "updated_at": "2024-11-25T05:48:01Z",
      "closed_at": "2024-11-25T05:47:59Z",
      "labels": [
        "API"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/995/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/995",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/995",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:51.557607",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "resolved by me",
          "created_at": "2024-11-25T05:47:59Z"
        }
      ]
    },
    {
      "issue_number": 991,
      "title": "[API] Set server time as UTC",
      "body": "The default server time must be UTC, and the client can change it to the user's timezone. \nSo, it will be great to fix every `datetime.now()` to UTC in the API server. ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-24T06:21:19Z",
      "updated_at": "2024-11-25T05:45:14Z",
      "closed_at": "2024-11-25T05:45:13Z",
      "labels": [
        "API"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/991/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/991",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/991",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:51.768617",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #992",
          "created_at": "2024-11-25T05:45:13Z"
        }
      ]
    },
    {
      "issue_number": 993,
      "title": "[Feature Request] Add full YAML at vectorDB integration docs",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-24T12:29:13Z",
      "updated_at": "2024-11-25T02:02:48Z",
      "closed_at": "2024-11-25T02:02:48Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/993/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/993",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/993",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:51.954063",
      "comments": []
    },
    {
      "issue_number": 986,
      "title": "[Sample Config Fix] Update non_gpu YAML files to use local embeddings.",
      "body": "After v0.3.8 we have to configure embeddings at seperated vecotrdb settings.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-23T13:33:02Z",
      "updated_at": "2024-11-24T06:20:51Z",
      "closed_at": "2024-11-24T06:20:51Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/986/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/986",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/986",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:51.954077",
      "comments": []
    },
    {
      "issue_number": 957,
      "title": "[BUG] Milvus error",
      "body": "MilvusException: <MilvusException: (code=1100, message=nlist out of range: [1, 65536]: invalid parameter[expected=valid index params][actual=invalid index params])>",
      "state": "closed",
      "author": "Aaryaveerkrishna23",
      "author_type": "User",
      "created_at": "2024-11-18T12:32:10Z",
      "updated_at": "2024-11-24T05:46:44Z",
      "closed_at": "2024-11-24T02:55:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/957/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/957",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/957",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:51.954081",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@Aaryaveerkrishna23 Hello. Can you provide a full trace of the bug?",
          "created_at": "2024-11-19T03:15:59Z"
        },
        {
          "author": "Aaryaveerkrishna23",
          "body": "> [@Aaryaveerkrishna23](https://github.com/Aaryaveerkrishna23) Hello. Can you provide a full trace of the bug?\n\nHi , I resolved it thanks :)\nI would like to know how we can add a layer of observability in auto RAG, such as using Langfuse, for example, to monitor the system in development, pre-produc",
          "created_at": "2024-11-21T14:21:58Z"
        },
        {
          "author": "Aaryaveerkrishna23",
          "body": "> [@Aaryaveerkrishna23](https://github.com/Aaryaveerkrishna23) Hello. Can you provide a full trace of the bug?\n\nJust catching up , any updates on the observability part ?",
          "created_at": "2024-11-23T14:24:03Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@Aaryaveerkrishna23 Hi sorry for the late reply! \nIt will be great to implement some great observability tool. \nFrom now, you can check the whole results in the each .parquet files in the result of AutoRAG.\nBut yes, it is quite hard to check it out manually 👎 \n\nSo we are happy to implement observabi",
          "created_at": "2024-11-24T02:55:22Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Since we resolve the error, I am closing the issue. ",
          "created_at": "2024-11-24T02:55:31Z"
        }
      ]
    },
    {
      "issue_number": 990,
      "title": "[Feature Request] Add Observability tool like Langfuse",
      "body": "@Aaryaveerkrishna23\n\nI would like to know how we can add a layer of observability in auto RAG, such as using Langfuse, for example, to monitor the system in development, pre-production, and production environments , to check what chunks are being retrieved , what was the final prompt after data augmentation etc .\n\n---\nGreat idea! ",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-24T02:54:58Z",
      "updated_at": "2024-11-24T02:54:58Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/990/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/990",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/990",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.172358",
      "comments": []
    },
    {
      "issue_number": 985,
      "title": "[BUG] Ollama Embeddings do not working",
      "body": "**Describe the bug**\nThe Ollama Embeddings is not working. \n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Use Ollama Embedding as embedding model\n2. Try to use it at vectordb retrieval module\n\nThe `aget_text_embedding_batch` has `asyncio.gather` itself, and it has conflict with `process_batch` function. It likely try to use different event loop.\n\n**Full Error log**\n[main_log.txt](https://github.com/user-attachments/files/17879756/main_log.txt)",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-23T13:07:36Z",
      "updated_at": "2024-11-23T13:07:36Z",
      "closed_at": null,
      "labels": [
        "bug",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/985/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/985",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/985",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.172377",
      "comments": []
    },
    {
      "issue_number": 979,
      "title": "[Feature Request] Add MongoDB Atlas VectorDB",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-22T08:10:04Z",
      "updated_at": "2024-11-22T08:10:04Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/979/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/979",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/979",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.172383",
      "comments": []
    },
    {
      "issue_number": 978,
      "title": "[Feature Request] Add pgvector DB",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-22T04:16:52Z",
      "updated_at": "2024-11-22T04:16:52Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/978/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/978",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/978",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.172388",
      "comments": []
    },
    {
      "issue_number": 963,
      "title": "[Feature Request] Add Qdrant VectorDB",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-19T13:25:24Z",
      "updated_at": "2024-11-21T14:22:49Z",
      "closed_at": "2024-11-21T14:22:49Z",
      "labels": [
        "enhancement",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/963/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/963",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/963",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.172395",
      "comments": []
    },
    {
      "issue_number": 972,
      "title": "[BUG] Pydantic Validation Error at MockEmbedding",
      "body": "**Describe the bug**\nThe Pydantic 2.10.0 has a error at the latest LlamaIndex version.\n\n**Desktop (please complete the following information):**\nPython 3.10\nPydantic 2.10.0\nLlamaIndex 0.12.1\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-21T08:21:50Z",
      "updated_at": "2024-11-21T13:00:48Z",
      "closed_at": "2024-11-21T13:00:48Z",
      "labels": [
        "bug",
        "AutoRAG Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/972/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/972",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/972",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.172400",
      "comments": []
    },
    {
      "issue_number": 968,
      "title": "🐛 Fix WORK_DIR path resolution in development environment (API)",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Full Error log**\nIf applicable, add full error log to help explain your problem.\n\n**Code that bug is happened**\nIf applicable, add the code that bug is happened.\n(Especially, your AutoRAG YAML file or python codes that you wrote)\n\n**Desktop (please complete the following information):**\n - OS: [e.g. Windows, Linux, MacOS]\n - Python version [e.g. 3.10]\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-20T13:14:26Z",
      "updated_at": "2024-11-21T05:25:41Z",
      "closed_at": "2024-11-21T05:25:31Z",
      "labels": [
        "duplicate",
        "API"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/968/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/968",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/968",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.172405",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "duplicate with #969 ",
          "created_at": "2024-11-21T05:25:31Z"
        }
      ]
    },
    {
      "issue_number": 965,
      "title": "[API] [BUG] daemonic processes are not allowed to have children",
      "body": "When try to run parsing, this error occured. \n\n```\ndaemonic processes are not allowed to have children\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-20T02:08:27Z",
      "updated_at": "2024-11-21T04:59:09Z",
      "closed_at": "2024-11-21T04:59:08Z",
      "labels": [
        "API"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/965/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/965",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/965",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.397210",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://github.com/pgjones/hypercorn/issues/191\n\nTried \n- Hypercorn : Daemon cannot have children issue\n- Daphne : Task is not finished\n- Uvicorn : nest_asyncio error (will not work with AutoRAG)\n\nWow. Just... wow",
          "created_at": "2024-11-20T02:56:07Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Just quart.run worked well to me. ",
          "created_at": "2024-11-20T02:56:26Z"
        },
        {
          "author": "vkehfdl1",
          "body": "uvicorn app:app --host 0.0.0.0 --port 5001 --reload --loop asyncio\n\n\nThis worked!! YEAHHHH \nuvicorn is the GOD",
          "created_at": "2024-11-20T03:13:01Z"
        },
        {
          "author": "vkehfdl1",
          "body": "close at #966 ",
          "created_at": "2024-11-21T04:59:08Z"
        }
      ]
    },
    {
      "issue_number": 938,
      "title": "[Feature Request] Implement DeasyLabs metadata retrieval and metadata rerankers.",
      "body": "**Is your feature request related to a problem? Please describe.**\nDeasyLabs supports great metadata retrieval and metadata reranker system. Plus auto-generating metadata for great search. It can help us to get a better RAG performance. \n\n**Describe the solution you'd like**\nNew modules required.\n\n- DeasyLabs Ingestion => At the dataset creation.\n- DeasyMetadataRetrieval\n- DeasyMetadataReranker\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-11T06:45:49Z",
      "updated_at": "2024-11-20T04:44:41Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/938/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/938",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/938",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.615556",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "First need to implement #943 ",
          "created_at": "2024-11-14T09:33:22Z"
        }
      ]
    },
    {
      "issue_number": 956,
      "title": "[Feature Request] Add Couchbase vector DB",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-18T11:55:55Z",
      "updated_at": "2024-11-20T01:20:34Z",
      "closed_at": "2024-11-20T01:20:34Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/956/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/956",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/956",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872250",
      "comments": []
    },
    {
      "issue_number": 962,
      "title": "[Feature Request] Add faiss \bVectorDB",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-19T13:17:33Z",
      "updated_at": "2024-11-19T13:17:33Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/962/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/962",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/962",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872269",
      "comments": []
    },
    {
      "issue_number": 958,
      "title": "[Feature Request] Display AutoRAG Newsletter (New Features, Security Updates, Changelog) on CLI",
      "body": "**Is your feature request related to a problem? Please describe.**\nCurrently, users must access a separate platform or website to view the latest updates on AutoRAG, including new features, security alerts, and changelogs. This extra step can be inconvenient and may result in missed updates.\n\n\n**Describe the solution you'd like**\nImplement a command in the CLI that allows users to view the latest AutoRAG newsletter, covering new features, security updates, and changelogs. This would provide users with quick, in-terminal access to important updates and eliminate the need to check external sources.\n\n**Describe alternatives you've considered**\nSending newsletters via email to subscribed users.\nCreating a dashboard widget that displays updates within the GUI.\nDisplaying critical updates only on the CLI (e.g., security alerts) and reserving feature updates for other platforms.\n\n**Additional context**\nThis feature would streamline user experience by consolidating all relevant updates into the CLI, making it easy for users to stay informed without leaving their workflow.\n\n\n",
      "state": "open",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-19T04:04:54Z",
      "updated_at": "2024-11-19T04:04:54Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/958/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/958",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/958",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872274",
      "comments": []
    },
    {
      "issue_number": 950,
      "title": "[Feature Request] Add pinecone vectorDB",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-16T12:52:23Z",
      "updated_at": "2024-11-18T14:28:01Z",
      "closed_at": "2024-11-18T14:28:01Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/950/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/950",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/950",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872280",
      "comments": []
    },
    {
      "issue_number": 947,
      "title": "[Feature Request] Retrieval Endpoint to use AutoRAG only for retrieval purpose",
      "body": "**Is your feature request related to a problem? Please describe.**\nIt is easy to replace other retrieval to AutoRAG retrieval, because of its optimized performance.\nMake an endpoint to use it more convenient. \n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-14T04:25:38Z",
      "updated_at": "2024-11-18T13:21:21Z",
      "closed_at": "2024-11-18T13:21:21Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/947/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/947",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/947",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872286",
      "comments": []
    },
    {
      "issue_number": 952,
      "title": "[Documentation] Add document about measuring your custom RAG method.",
      "body": "Did you ever want to compare your RAG method and AutoRAG optimized pipeline?\nThere is a good feature for you.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-16T13:29:36Z",
      "updated_at": "2024-11-17T14:37:24Z",
      "closed_at": "2024-11-17T14:37:24Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/952/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/952",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/952",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872292",
      "comments": []
    },
    {
      "issue_number": 951,
      "title": "[Documentation] Add custom module, custom metric guide",
      "body": "AutoRAG can easily add custom module and use it while optimization process.\nAdd the guide how to add custom module and custom metric.",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-16T13:02:50Z",
      "updated_at": "2024-11-16T13:02:50Z",
      "closed_at": null,
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/951/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/951",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/951",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872298",
      "comments": []
    },
    {
      "issue_number": 943,
      "title": "[Feature Request] Add Weaviate vector db",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-12T13:22:46Z",
      "updated_at": "2024-11-16T05:29:33Z",
      "closed_at": "2024-11-16T05:29:33Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/943/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/943",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/943",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872303",
      "comments": []
    },
    {
      "issue_number": 948,
      "title": "[BUG] Unexpected Open API Requests Triggered Under Inactive Conditions",
      "body": "**Describe the bug**\nAPI calls to OpenAI occur despite having multiple modules configured to operate locally without OpenAI access.\n\nThis occurs in `post_retrieve_node_line` within `prompt_maker` when there are multiple instances of `module_type`, such as `fstrings`, during the Validation phase.\n\n**To Reproduce**\nPart of the config.yml\n```\n(...)\n- node_line_name: post_retrieve_node_line  # Arbitrary node line name\n  nodes:\n    - node_type: prompt_maker\n      strategy:\n        metrics: [ meteor, rouge, bert_score ]\n      modules:\n          # - module_type: fstring\n          #   prompt: \"Read the passages and answer the given question. \\n Question: {query} \\n Passage: {retrieved_contents} \\n Answer : \"\n         - module_type: fstring\n           prompt: [\"Tell me something about the question: {query} \\n\\n {retrieved_contents}\",\n                    \"Question: {query} \\n Something to read: {retrieved_contents} \\n What's your answer?\"]\n        \n      generator_modules:\n          - module_type: llama_index_llm\n            llm: ollama\n            model: gemma2:27b\n(...)\n````\n\nI get the logs:\n```\nINFO     [_client.py:1786] >> HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"                                _client.py:1786\n\t                    INFO     [_base_client.py:1661] >> Retrying request to /chat/completions in 0.421052 seconds                                                           _base_client.py:1661\n\t                    INFO     [_client.py:1786] >> HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"  \n```\n\nSwapping the comments between the module_types allows the validation to pass successfully.\n\nThanks\n",
      "state": "closed",
      "author": "wisaaco",
      "author_type": "User",
      "created_at": "2024-11-14T12:47:02Z",
      "updated_at": "2024-11-14T14:39:03Z",
      "closed_at": "2024-11-14T14:39:01Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/948/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/948",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/948",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:52.872308",
      "comments": [
        {
          "author": "wisaaco",
          "body": "My mistake. I need to put the `generator_module` inside the strategy:\n```yaml\n      strategy:\n        metrics: [ meteor, rouge ]\n        generator_modules:\n          - module_type: llama_index_llm\n            llm: ollama\n            model: gemma2:27b\n            batch: 1\n            temperature: 1.0",
          "created_at": "2024-11-14T14:39:02Z"
        }
      ]
    },
    {
      "issue_number": 944,
      "title": "[BUG] Having trouble using LiteLLM Embedding",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Full Error log**\nIf applicable, add full error log to help explain your problem.\n\n**Code that bug is happened**\nIf applicable, add the code that bug is happened.\n(Especially, your AutoRAG YAML file or python codes that you wrote)\n\n**Desktop (please complete the following information):**\n - OS: [e.g. Windows, Linux, MacOS]\n - Python version [e.g. 3.10]\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-12T15:28:14Z",
      "updated_at": "2024-11-14T09:33:03Z",
      "closed_at": "2024-11-14T09:33:02Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/944/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/944",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/944",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:53.044168",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "The LiteLLm embedding works well.\nSince it is not bug, I am closing this issue.",
          "created_at": "2024-11-14T09:33:02Z"
        }
      ]
    },
    {
      "issue_number": 946,
      "title": "[BUG] Invalid temperature values in full.yaml.",
      "body": "# Description of  the Bug\nThe temperature parameter was set as a list with values [0.5, 1.0, 1.5], which includes an invalid value (1.5). Since valid temperature values must be within the range of 0–1, this caused a validation error when running the code.\n\n# To Reproduce:\n\n# Steps to reproduce the behavior:\n\n- The temperature parameter in (https://github.com/Marker-Inc-Korea/AutoRAG/blob/main/sample_config/rag/english/gpu/full.yaml) is set to [0.5, 1.0, 1.5] in the configuration file.\n\n- Run the code.\n   Observe the validation error due to the invalid temperature value.\n\n# Expected Behavior\nThe temperature parameter should only contain values within the range of 0–1.\n\n# Full Error Log\nValidationError: 1 validation error for OpenAI\n                             temperature\n                               Input should be less than or equal to 1 [type=less_than_equal, input_value=1.5, input_type=float]\n\n**Code that bug is happened**\n- full.yaml where the temperature is set as a list\ntemperature: [0.5, 1.0, 1.5]  # Change to valid values, e.g., [0.5, 1.0]\n\n\n# Desktop:\n - OS: Windows\n - Python version: 3.10.9\n\n# Additional Context\nTo fix the issue, consider limiting the list to values within the 0–1 range, e.g., [0.5, 1.0], or use a single value if a range is not required.",
      "state": "closed",
      "author": "ImaadHasan2002",
      "author_type": "User",
      "created_at": "2024-11-13T21:27:35Z",
      "updated_at": "2024-11-14T04:52:34Z",
      "closed_at": "2024-11-14T03:47:01Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/946/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/946",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/946",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:53.264828",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hi, @ImaadHasan2002 \nAt OpenAI temperature setting, 0 to 2 is valid settings for temperature.\n\nhttps://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature\n```\ntemperature\nnumber or null\n\nOptional\nDefaults to 1\nWhat sampling temperature to use, between 0 and 2. Higher values li",
          "created_at": "2024-11-14T03:47:01Z"
        }
      ]
    },
    {
      "issue_number": 942,
      "title": "[BUG] The vectordb setting is always applied as default when validating",
      "body": "**Describe the bug**\nEvent I configure vectordb, always set as default when validating\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Configure yaml with custom vectordb configuration\n2. Run validation\n3. See error\n\n**Expected behavior**\nIngest vectordb with my configuration\n\n**Full Error log**\nnot error\n\n**Code that bug is happened**\n`config_dict` is full dict of config.yaml, so it only has **node_lines** as key. \nvectordb_list always becomes empty list because `config_dict` has to \"vectordb\" key.  \nIf that's not what you intended, it need to be fixed it.\n```python\ndef load_all_vectordb_from_yaml(\n    yaml_path: str, project_dir: str\n) -> List[BaseVectorStore]:\n    config_dict = load_yaml_config(yaml_path)\n    vectordb_list = config_dict.get(\"vectordb\", [])\n    if len(vectordb_list) == 0:\n        chroma_path = os.path.join(project_dir, \"resources\", \"chroma\")\n        return [\n            load_vectordb(\n                \"chroma\",\n                client_type=\"persistent\",\n                embedding_model=\"openai\",\n                collection_name=\"openai\",\n                path=chroma_path,\n            )\n        ]\n...\n```\n**Suggestion**\nWe need to ingest vectordb configured in retrieval node\n\n",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2024-11-12T11:24:46Z",
      "updated_at": "2024-11-13T11:20:45Z",
      "closed_at": "2024-11-13T11:20:43Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/942/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/942",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/942",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:53.435983",
      "comments": [
        {
          "author": "rjwharry",
          "body": "I'll get vectordb configuration from retrieval node",
          "created_at": "2024-11-12T11:28:35Z"
        },
        {
          "author": "rjwharry",
          "body": "Or maybe ingest all vectordb in config.yaml like bm25 did",
          "created_at": "2024-11-12T11:31:24Z"
        },
        {
          "author": "rjwharry",
          "body": "I need to be fixed right now! This is not only for validation but also Evaluator.\nI think we should ingest all vectordb in config.yaml like bm25",
          "created_at": "2024-11-12T11:47:21Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry \nActually I think it is intended. We changed the VectorDB configuration setting since v0.3.8\n\nhttps://docs.auto-rag.com/integration/vectordb/vectordb.html\nHere is the full documentation how to set different vectorDB, also embedding models.\n\nYou must configure the vectorDB at the `vectordb`",
          "created_at": "2024-11-13T11:06:29Z"
        },
        {
          "author": "rjwharry",
          "body": "I've figured out how to configure VectorDB. This is my mistake, let me close this issue",
          "created_at": "2024-11-13T11:20:43Z"
        }
      ]
    },
    {
      "issue_number": 937,
      "title": "[BUG] ImportError: cannot import name 'HTMLBox' from 'bokeh.models.layouts'`",
      "body": "**Describe the bug**\nWhile running the following command locally,\n\n`!autorag dashboard --trial_dir /content/project_dir/0`\n\nin [AutoRAG evaluator - Tutorial - Step 1 (deploy).ipynb](https://colab.research.google.com/drive/19OEQXO_pHN6gnn2WdfPd4hjnS-4GurVd), \n\nI'm getting\n\n`ImportError: cannot import name 'HTMLBox' from 'bokeh.models.layouts'`\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Run the specified step in colab notebook in a local envrionment with following dependencies\n\npip3 install blinker==1.8.2\npip3 install ipykernel==5.5.6 ipywidgets-bokeh==1.0.2\npip3 install AutoRAG==0.3.7\npip3 install datasets\npip3 install python-dotenv\npip3 install jupyter\n\n**Full Error log**\n```\n[11/10/24 19:38:31] ERROR    [__init__.py:60] >> Unexpected exception                                                                                                                               __init__.py:60\n                             ╭──────────────────────────────────────────────────────────────── Traceback (most recent call last) ─────────────────────────────────────────────────────────────────╮\n                             │ /home/autorag/autorag-env/bin/autorag:5 in <module>                                                                             │\n                             │                                                                                                                                                                    │\n                             │   2 # -*- coding: utf-8 -*-                                                                                                                                        │\n                             │   3 import re                                                                                                                                                      │\n                             │   4 import sys                                                                                                                                                     │\n                             │ ❱ 5 from autorag.cli import cli                                                                                                                                    │\n                             │   6 if __name__ == '__main__':                                                                                                                                     │\n                             │   7 │   sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])                                                                                           │\n                             │   8 │   sys.exit(cli())                                                                                                                                            │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/autorag/cli.py:12 in <module>                                            │\n                             │                                                                                                                                                                    │\n                             │     9 import click                                                                                                                                                 │\n                             │    10 import nest_asyncio                                                                                                                                          │\n                             │    11                                                                                                                                                              │\n                             │ ❱  12 from autorag import dashboard                                                                                                                                │\n                             │    13 from autorag.deploy import extract_best_config as original_extract_best_config                                                                               │\n                             │    14 from autorag.deploy.api import ApiRunner                                                                                                                     │\n                             │    15 from autorag.evaluator import Evaluator                                                                                                                      │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/autorag/dashboard.py:15 in <module>                                      │\n                             │                                                                                                                                                                    │\n                             │    12                                                                                                                                                              │\n                             │    13 from autorag.utils.util import dict_to_markdown, dict_to_markdown_table                                                                                      │\n                             │    14                                                                                                                                                              │\n                             │ ❱  15 pn.extension(                                                                                                                                                │\n                             │    16 │   \"terminal\",                                                                                                                                              │\n                             │    17 │   \"tabulator\",                                                                                                                                             │\n                             │    18 │   \"mathjax\",                                                                                                                                               │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/pyviz_comms/__init__.py:64 in __new__                                    │\n                             │                                                                                                                                                                    │\n                             │    61 │   │   │   extension._last_execution_count = exec_count                                                                                                     │\n                             │    62 │   │   except Exception:                                                                                                                                    │\n                             │    63 │   │   │   pass                                                                                                                                             │\n                             │ ❱  64 │   │   return param.ParameterizedFunction.__new__(cls, *args, **kwargs)                                                                                     │\n                             │    65 │                                                                                                                                                            │\n                             │    66 │   @classmethod                                                                                                                                             │\n                             │    67 │   def add_delete_action(cls, action):                                                                                                                      │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/param/parameterized.py:4468 in __new__                                   │\n                             │                                                                                                                                                                    │\n                             │   4465 │   │   # Create and __call__() an instance of this class.                                                                                                  │\n                             │   4466 │   │   inst = class_.instance()                                                                                                                            │\n                             │   4467 │   │   inst.param._set_name(class_.__name__)                                                                                                               │\n                             │ ❱ 4468 │   │   return inst.__call__(*args,**params)                                                                                                                │\n                             │   4469 │                                                                                                                                                           │\n                             │   4470 │   def __call__(self,*args,**kw):                                                                                                                          │\n                             │   4471 │   │   raise NotImplementedError(\"Subclasses must implement __call__.\")                                                                                    │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/panel/config.py:742 in __call__                                          │\n                             │                                                                                                                                                                    │\n                             │   739 │   │   │   │   │   │   │   if qual and qual not in _default_resolver.known_models:                                                                          │\n                             │   740 │   │   │   │   │   │   │   │   _default_resolver.add(model)                                                                                                 │\n                             │   741 │   │   │   │   else:                                                                                                                                        │\n                             │ ❱ 742 │   │   │   │   │   __import__(module)                                                                                                                       │\n                             │   743 │   │   │   │   self._loaded_extensions.append(arg)                                                                                                          │\n                             │   744 │   │   │   │                                                                                                                                                │\n                             │   745 │   │   │   │   if state.curdoc:                                                                                                                             │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/panel/io/ipywidget.py:22 in <module>                                     │\n                             │                                                                                                                                                                    │\n                             │    19 # Stop ipywidgets_bokeh from patching the kernel                                                                                                             │\n                             │    20 ipykernel.kernelbase.Kernel._instance = ''                                                                                                                   │\n                             │    21                                                                                                                                                              │\n                             │ ❱  22 from ipywidgets_bokeh.kernel import (                                                                                                                        │\n                             │    23 │   BokehKernel, SessionWebsocket, WebsocketStream,                                                                                                          │\n                             │    24 )                                                                                                                                                            │\n                             │    25 from ipywidgets_bokeh.widget import IPyWidget                                                                                                                │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/ipywidgets_bokeh/__init__.py:1 in <module>                               │\n                             │                                                                                                                                                                    │\n                             │ ❱ 1 from .widget import IPyWidget                                                                                                                                  │\n                             │   2                                                                                                                                                                │\n                             │   3 __version__ = \"1.0.2\"                                                                                                                                          │\n                             │   4                                                                                                                                                                │\n                             │                                                                                                                                                                    │\n                             │ /home/autorag/autorag-env/lib/python3.11/site-packages/ipywidgets_bokeh/widget.py:9 in <module>                                 │\n                             │                                                                                                                                                                    │\n                             │    6 #-----------------------------------------------------------------------------                                                                                │\n                             │    7                                                                                                                                                               │\n                             │    8 from bokeh.core.properties import Any                                                                                                                         │\n                             │ ❱  9 from bokeh.models.layouts import HTMLBox                                                                                                                      │\n                             │   10                                                                                                                                                               │\n                             │   11 from ipywidgets import embed, Widget                                                                                                                          │\n                             │   12                                                                                                                                                               │\n                             ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n                             ImportError: cannot import name 'HTMLBox' from 'bokeh.models.layouts'\n```\n\n\n**Desktop (please complete the following information):**\n - Windows with WSL 2\n\n",
      "state": "open",
      "author": "mattquestions",
      "author_type": "User",
      "created_at": "2024-11-10T18:43:36Z",
      "updated_at": "2024-11-13T11:00:57Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/937/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/937",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/937",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:53.649600",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@mattquestions Thanks for reporting a bug. I think in colab it is hard to see the dashboard. \nI will find out another way to see it. \nColab is just for tutorial, and use dashboard and web feature, recommend to install in your local environment. Thank you",
          "created_at": "2024-11-12T03:36:54Z"
        },
        {
          "author": "mattquestions",
          "body": "Thank you @vkehfdl1, this is happening in my local environment as well.",
          "created_at": "2024-11-12T12:16:19Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Hi @mattquestions \nCan you try to update the bokeh and panel version?\nAnd can you tell me the version of bokeh and panel library?",
          "created_at": "2024-11-13T11:00:55Z"
        }
      ]
    },
    {
      "issue_number": 928,
      "title": "[Feature Request] Update README.md",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-09T14:35:43Z",
      "updated_at": "2024-11-12T08:31:47Z",
      "closed_at": "2024-11-12T08:31:47Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/928/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/928",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/928",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:53.852717",
      "comments": []
    },
    {
      "issue_number": 940,
      "title": "[Feature Request] Extract Panel Dashboard as html file.",
      "body": "https://panel.holoviz.org/how_to/export/saving.html\n\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-12T03:51:41Z",
      "updated_at": "2024-11-12T07:08:18Z",
      "closed_at": "2024-11-12T07:08:16Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/940/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/940",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/940",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:53.852738",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Not working at panel FastTemplate ",
          "created_at": "2024-11-12T07:08:16Z"
        }
      ]
    },
    {
      "issue_number": 714,
      "title": "[Feature Request] Can you add azure open ai implementation for autoRAG?",
      "body": "\r\nI want to add azure open ai credentials for embedding and llm model for performing AutoRAG.\r\n\r\nCurrent embedding only support openai credentials.\r\n\r\nand if possible add documentation for Azure Open AI, quick start\r\n",
      "state": "closed",
      "author": "wanjeakshay",
      "author_type": "User",
      "created_at": "2024-09-16T10:21:44Z",
      "updated_at": "2024-11-12T06:28:44Z",
      "closed_at": "2024-09-23T07:49:41Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/714/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/714",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/714",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:54.028588",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@wanjeakshay \r\nHello you can use azure openai and embedding with llama_index in AutoRAG.\r\n\r\nHere is the documentation how to implement it.\r\nhttps://docs.auto-rag.com/local_model.html\r\n\r\nIf you are in trouble to do it, please ask me again (mention please)",
          "created_at": "2024-09-16T11:14:04Z"
        },
        {
          "author": "wanjeakshay",
          "body": "Hi @vkehfdl1, thank you for the doc link, I updated my code to include azure AI embedding like this\r\n\r\nfrom llama_index.llms.azure_openai import AzureOpenAI\r\nfrom llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\r\nimport autorag\r\nfrom autorag import LazyInit\r\n\r\n# You need to deploy you",
          "created_at": "2024-09-16T11:58:58Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@wanjeakshay \r\nHello, it looks like you missed the api key and azure_endpoint at the embedding model. \r\n\r\n```python\r\nimport os\r\nimport autorag\r\n\r\nautorag.embedding_models['azureAI'] = LazyInit(AzureOpenAIEmbedding, model_name=\"text-embedding-ada-002”,\r\napi_key=os.getenv(“AZURE_API_KEY”), azure_endpo",
          "created_at": "2024-09-17T02:11:00Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Since it looks you managed, and works well in the AutoRAG, closing this issue.",
          "created_at": "2024-09-23T07:49:41Z"
        },
        {
          "author": "hellangleZ",
          "body": "> [@wanjeakshay](https://github.com/wanjeakshay) Hello you can use azure openai and embedding with llama_index in AutoRAG.\n> \n> Here is the documentation how to implement it. https://docs.auto-rag.com/local_model.html\n> \n> If you are in trouble to do it, please ask me again (mention please)\n\nHi @vke",
          "created_at": "2024-10-30T15:32:36Z"
        }
      ]
    },
    {
      "issue_number": 929,
      "title": "[Feature Request] Update Documentation",
      "body": "- Added descriptions for Ollama, Bedrock, Nvidia Nim, etc. available in the our Llama Index LLM\n- Sufficient explanation about our sample YAML file",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-09T14:41:55Z",
      "updated_at": "2024-11-12T03:31:25Z",
      "closed_at": "2024-11-12T03:31:25Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/929/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/929",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/929",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:54.251736",
      "comments": []
    },
    {
      "issue_number": 921,
      "title": "[BUG] RecursionError when Using BAAI/bge-m3 with HuggingFaceEmbedding in AutoRAG Tutorial",
      "body": "**Describe the bug**  \nWhen executing the `Evaluator.start_trial` method in the autorag library, a `RecursionError: maximum recursion depth exceeded` occurs. This issue arises specifically when using `HuggingfaceEmbedding` with the `BAAI/bge-m3` model for embedding, following the instructions in the README's \"Step 3: Use Custom LLM & Embedding Model | Use Custom Model.\"\n\n**To Reproduce**  \nSteps to reproduce the issue:\n1. Create an `Evaluator` object, specifying `qa_data_path`, `corpus_data_path`, and `project_dir`.\n2. Call the `evaluator.start_trial` method with a configuration file (`config.yaml`).\n3. In the configuration file, set the embedding model to `HuggingfaceEmbedding` with `BAAI/bge-m3`, as described in the README's \"Step 3: Use Custom LLM & Embedding Model | Use Custom Model.\"\n\n**Expected behavior**  \nThe `Evaluator` should successfully start the QA evaluation using the configuration provided, utilizing `HuggingfaceEmbedding` with the `BAAI/bge-m3` model, and return the evaluation results.\n\n**Full Error Log**  \n```\nIngesting VectorDB... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:03:47\nEvaluating...         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0% 0/3 0:00:03\nmodules.json: 100%\n 349/349 [00:00<00:00, 11.9kB/s]\nconfig_sentence_transformers.json: 100%\n 123/123 [00:00<00:00, 6.83kB/s]\nREADME.md: 100%\n 15.8k/15.8k [00:00<00:00, 1.01MB/s]\nsentence_bert_config.json: 100%\n 54.0/54.0 [00:00<00:00, 5.09kB/s]\nconfig.json: 100%\n 687/687 [00:00<00:00, 25.7kB/s]\ntokenizer_config.json: 100%\n 444/444 [00:00<00:00, 19.7kB/s]\nsentencepiece.bpe.model: 100%\n 5.07M/5.07M [00:00<00:00, 48.7MB/s]\ntokenizer.json: 100%\n 17.1M/17.1M [00:00<00:00, 44.4MB/s]\nspecial_tokens_map.json: 100%\n 964/964 [00:00<00:00, 99.2kB/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]Exception in thread Thread-12:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\", line 224, in catch_format_error\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n  0%|          | 0/77 [00:00<?, ?it/s]\n    r = method(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\", line 909, in __call__\n    printer = self.lookup(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\", line 392, in lookup\n    if obj_id in self.singleton_printers:\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\", line 700, in __get__\n    return self.get(obj, cls)\nRecursionError: maximum recursion depth exceeded\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/live.py\", line 32, in run\n    self.live.refresh()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/live.py\", line 237, in refresh\n    with self.ipy_widget:\n  File \"/usr/local/lib/python3.10/dist-packages/ipywidgets/widgets/widget_output.py\", line 109, in __enter__\n    self._flush()\n  File \"/usr/local/lib/python3.10/dist-packages/ipywidgets/widgets/widget_output.py\", line 132, in _flush\n    sys.stderr.flush()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/file_proxy.py\", line 53, in flush\n    self.__console.print(output)\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 1678, in print\n    with self:\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 864, in __exit__\n    self._exit_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 822, in _exit_buffer\n    self._check_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2019, in _check_buffer\n    self._write_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2035, in _write_buffer\n    display(self._buffer, self._render_buffer(self._buffer[:]))\n  File \"/usr/local/lib/python3.10/dist-packages/rich/jupyter.py\", line 91, in display\n    ipython_display(jupyter_renderable)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 327, in display\n    publish_display_data(data=format_dict, metadata=md_dict, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 119, in publish_display_data\n    display_pub.publish(\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 115, in publish\n    self._flush_streams()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 83, in _flush_streams\n    sys.stderr.flush()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/file_proxy.py\", line 53, in flush\n    self.__console.print(output)\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 1678, in print\n    with self:\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 864, in __exit__\n    self._exit_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 822, in _exit_buffer\n    self._check_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2019, in _check_buffer\n    self._write_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2035, in _write_buffer\n    display(self._buffer, self._render_buffer(self._buffer[:]))\n  File \"/usr/local/lib/python3.10/dist-packages/rich/jupyter.py\", line 91, in display\n    ipython_display(jupyter_renderable)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 327, in display\n    publish_display_data(data=format_dict, metadata=md_dict, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 119, in publish_display_data\n    display_pub.publish(\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 115, in publish\n    self._flush_streams()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 83, in _flush_streams\n    sys.stderr.flush()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/file_proxy.py\", line 53, in flush\n    self.__console.print(output)\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 1678, in print\n    with self:\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 864, in __exit__\n    self._exit_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 822, in _exit_buffer\n    self._check_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2019, in _check_buffer\n    self._write_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2035, in _write_buffer\n    display(self._buffer, self._render_buffer(self._buffer[:]))\n  File \"/usr/local/lib/python3.10/dist-packages/rich/jupyter.py\", line 91, in display\n    ipython_display(jupyter_renderable)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 327, in display\n    publish_display_data(data=format_dict, metadata=md_dict, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 119, in publish_display_data\n    display_pub.publish(\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 115, in publish\n    self._flush_streams()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 83, in _flush_streams\n    sys.stderr.flush()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/file_proxy.py\", line 53, in flush\n    self.__console.print(output)\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 1678, in print\n    with self:\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 864, in __exit__\n    self._exit_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 822, in _exit_buffer\n    self._check_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2019, in _check_buffer\n    self._write_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2035, in _write_buffer\n    display(self._buffer, self._render_buffer(self._buffer[:]))\n  File \"/usr/local/lib/python3.10/dist-packages/rich/jupyter.py\", line 91, in display\n    ipython_display(jupyter_renderable)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 327, in display\n    publish_display_data(data=format_dict, metadata=md_dict, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 119, in publish_display_data\n    display_pub.publish(\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 115, in publish\n    self._flush_streams()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 83, in _flush_streams\n    sys.stderr.flush()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/file_proxy.py\", line 53, in flush\n    self.__console.print(output)\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 1678, in print\n    with self:\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 864, in __exit__\n    self._exit_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 822, in _exit_buffer\n    self._check_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2019, in _check_buffer\n    self._write_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2035, in _write_buffer\n    display(self._buffer, self._render_buffer(self._buffer[:]))\n  File \"/usr/local/lib/python3.10/dist-packages/rich/jupyter.py\", line 91, in display\n    ipython_display(jupyter_renderable)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 327, in display\n    publish_display_data(data=format_dict, metadata=md_dict, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\", line 119, in publish_display_data\n    display_pub.publish(\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 115, in publish\n    self._flush_streams()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 83, in _flush_streams\n    sys.stderr.flush()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/file_proxy.py\", line 53, in flush\n    self.__console.print(output)\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 1678, in print\n    with self:\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 864, in __exit__\n    self._exit_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 822, in _exit_buffer\n---------------------------------------------------------------------------\nRecursionError                            Traceback (most recent call last)\n[/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py](https://localhost:8080/#) in catch_format_error(method, self, *args, **kwargs)\n    223     try:\n--> 224         r = method(self, *args, **kwargs)\n    225     except NotImplementedError:\n\n28 frames\nRecursionError: maximum recursion depth exceeded\n\nDuring handling of the above exception, another exception occurred:\n\nRecursionError                            Traceback (most recent call last)\n... last 11 frames repeated, from the frame below ...\n\n[/usr/local/lib/python3.10/dist-packages/rich/file_proxy.py](https://localhost:8080/#) in flush(self)\n     51         output = \"\".join(self.__buffer)\n     52         if output:\n---> 53             self.__console.print(output)\n     54         del self.__buffer[:]\n     55 \n\nRecursionError: maximum recursion depth exceeded in __instancecheck__\n    self._check_buffer()\n  File \"/usr/local/lib/python3.10/dist-packages/rich/console.py\", line 2019, in _check_buffer\n    self._write_buffer()\n```\n\n**Code where the bug occurred**  \n\n```python\nimport autorag\nfrom autorag import LazyInit\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nautorag.embedding_models['upstage'] = LazyInit(HuggingFaceEmbedding, model_name=\"BAAI/bge-m3\")\n```\n\n```python\nfrom autorag.evaluator import Evaluator\n\nevaluator = Evaluator(\n    qa_data_path='/content/eli5_data/qa_sample.parquet', \n    corpus_data_path='/content/eli5_data/corpus_sample.parquet',\n    project_dir='/content/project_dir'\n)\n\nevaluator.start_trial('/content/config.yaml')\n```\n\n**AutoRAG YAML file (config.yaml):**  \n\n```yaml\n%%writefile config.yaml\nvectordb:\n  - name: chroma_upstage\n    db_type: chroma\n    client_type: persistent\n    collection_name: upstage\n    embedding_model: upstage\n    path: ${PROJECT_DIR}/resources/chroma\nnode_lines:\n- node_line_name: retrieve_node_line\n  nodes:\n    - node_type: retrieval\n      strategy:\n        metrics: [retrieval_f1, retrieval_recall, retrieval_ndcg, retrieval_mrr]\n      top_k: 3\n      modules:\n        - module_type: vectordb\n          vectordb: chroma_upstage\n        - module_type: bm25\n        - module_type: hybrid_rrf\n          weight_range: (4,80)\n- node_line_name: post_retrieve_node_line\n  nodes:\n    - node_type: prompt_maker\n      strategy:\n        metrics:\n          - metric_name: meteor\n          - metric_name: rouge\n          - metric_name: sem_score\n            embedding_model: upstage # Use upstage embedding model\n      modules:\n        - module_type: fstring\n          prompt: \"Read the passages and answer the given question. \\n Question: {query} \\n Passage: {retrieved_contents} \\n Answer : \"\n    - node_type: generator\n      strategy:\n        metrics:\n          - metric_name: meteor\n          - metric_name: rouge\n          - metric_name: sem_score\n            embedding_model: upstage # Use upstage embedding model\n      modules:\n        - module_type: llama_index_llm\n          llm: upstage  # Use upstage LLM\n          batch: 4\n```\n\n**Desktop (please complete the following information):**\n- OS: Linux\n- Python version: 3.10\n\n**Additional context**  \nI used **Step 3: Use Custom LLM & Embedding Model | Use Custom Model** from the Colab Tutorial provided in your README.md, and in this step, I only modified the following code:\n\n```python\nimport autorag\nfrom autorag import LazyInit\nfrom llama_index.embeddings.upstage import UpstageEmbedding\n\nautorag.embedding_models['upstage'] = LazyInit(UpstageEmbedding, model_name=\"solar-embedding-1-large\")\n```\n\nto\n\n```python\nimport autorag\nfrom autorag import LazyInit\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nautorag.embedding_models['upstage'] = LazyInit(HuggingFaceEmbedding, model_name=\"BAAI/bge-m3\")\n```",
      "state": "closed",
      "author": "jjaegii",
      "author_type": "User",
      "created_at": "2024-11-04T06:44:17Z",
      "updated_at": "2024-11-10T16:02:43Z",
      "closed_at": "2024-11-10T16:02:42Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/921/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/921",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/921",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:54.251754",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@jjaegii Hello! I will look into this issue. Thanks for reporting the bug.",
          "created_at": "2024-11-05T01:20:52Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@jjaegii \nHi, it looks like you tried to use BAAI-bge embedding model but the config YAML file and code in the colab demo is upstage API model. \nCan you check the [docs](https://docs.auto-rag.com/local_model.html#configure-the-embedding-model) and tutorial to change this to BAAI bge model?\n ",
          "created_at": "2024-11-05T02:26:02Z"
        },
        {
          "author": "jjaegii",
          "body": "I reviewed the documentation and modified the code and YAML as follows, but the same error still occurs.\n\n```python\nimport autorag\nfrom autorag import LazyInit\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nautorag.embedding_models['huggingface_bge_m3'] = LazyInit(HuggingFaceEm",
          "created_at": "2024-11-05T04:22:12Z"
        },
        {
          "author": "jjaegii",
          "body": "Additionally, using the provided **AutoRAG Tutorial 3 - Use Custom LLM & Embedding Model.ipynb** file exactly as-is with the default `upstage llm` and `upstage embedding model` also results in the same error.\n\nAny additional guidance or advice on resolving this issue would be greatly appreciated. Th",
          "created_at": "2024-11-05T04:23:19Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@jjaegii Thank you! We will fix that ASAP. ",
          "created_at": "2024-11-08T14:03:43Z"
        }
      ]
    },
    {
      "issue_number": 930,
      "title": "[BUG] Kkma raises UnicodeDecodeError when text includes emoji",
      "body": "**Describe the bug**\nWhen emoji is included in text, Kkma raises `UnicodeDecodeError` in `morphs(text)` function.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Use `ko_kkma` as tokenizer in retrieval node\n2. Include text with emoji in corpus.parquet\n3. Run validator\n4. See error\n\n**Expected behavior**\nPass validation\n\n**Full Error log**\n`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte`\n\n**Desktop (please complete the following information):**\n - OS: Linux\n - Python version 3.10\n\n",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2024-11-10T12:12:18Z",
      "updated_at": "2024-11-10T15:40:35Z",
      "closed_at": "2024-11-10T15:40:35Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/930/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [
        "rjwharry"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/930",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/930",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:54.493682",
      "comments": [
        {
          "author": "rjwharry",
          "body": "I think removing emoji can be good solution. Mostly emoji does not have much information",
          "created_at": "2024-11-10T12:15:04Z"
        },
        {
          "author": "rjwharry",
          "body": "Please assign to me, I'll figure it out",
          "created_at": "2024-11-10T12:22:32Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry I assigned to you, but it is possible that this error is not occured because of ko_kkma.\nI think it will be possible that `normalize_unicode` function occured the error in the `utils/util.py`\n\n\n",
          "created_at": "2024-11-10T12:41:09Z"
        },
        {
          "author": "rjwharry",
          "body": "It occurred in kkma. Here is the full error log\n![Image](https://github.com/user-attachments/assets/853157dc-9548-4e36-9e54-09b153db2bf4)\n",
          "created_at": "2024-11-10T13:50:07Z"
        }
      ]
    },
    {
      "issue_number": 933,
      "title": "[BUG] RecursionError: maximum recursion depth exceeded in __instancecheck__",
      "body": "**Describe the bug**\nWhile running [AutoRAG evaluator - Tutorial - Step 1 (deploy).ipynb](https://colab.research.google.com/drive/19OEQXO_pHN6gnn2WdfPd4hjnS-4GurVd), I'm getting\n\n`RecursionError: maximum recursion depth exceeded in __instancecheck__`\n\nSame thing also happening in my local environment.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Just run the tutorial notebook in colab.",
      "state": "closed",
      "author": "mattquestions",
      "author_type": "User",
      "created_at": "2024-11-10T14:21:58Z",
      "updated_at": "2024-11-10T15:05:01Z",
      "closed_at": "2024-11-10T15:05:01Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/933/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/933",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/933",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:54.761740",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hello @mattquestions \nI am fixing this error now. \nThank you. \n\n@bwook00 rich error. Possible conflict with tqdm?",
          "created_at": "2024-11-10T14:23:07Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@bwook00 \nWhen I have change `from tqdm import tqdm` to `from tqdm.rich import tqdm`, it occurs error like this.\n\n```\nIngesting VectorDB... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:04\nEvaluating...         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0% 0/3 0:00:04\n/usr/local/lib/python3",
          "created_at": "2024-11-10T14:28:57Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@mattquestions \nRecommended to downgrade AutoRAG version to v0.3.7 before we resolve the error.\nThanks!",
          "created_at": "2024-11-10T14:30:48Z"
        }
      ]
    },
    {
      "issue_number": 700,
      "title": "[Feature Request] Add citation paragraph at README.md ",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nSome researchers want to add AutoRAG to their own papers.\r\nIt will be great we add citation link for the researchers. ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-11T02:35:41Z",
      "updated_at": "2024-11-10T14:57:55Z",
      "closed_at": "2024-11-10T14:57:55Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/700/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/700",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/700",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:55.004566",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://arxiv.org/pdf/2406.19251\r\n\r\nIn this paper there is citation for AutoRAG repo",
          "created_at": "2024-09-11T02:36:23Z"
        }
      ]
    },
    {
      "issue_number": 924,
      "title": "[BUG] Upstage Embedding not working",
      "body": "At vectordb.py, llamaindex UpstageEmbedding class is instance of OpenAIEmbedding.\nSo it tires to truncate token to 8000, but tiktoken cannot tokenize it because solar embedding model is not capable of tokenize in tiktoken. \n\nAdd model_name condition at truncate openai token also.\n\n\n\n+ \nJust change colab demo to BAAI",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-05T04:39:48Z",
      "updated_at": "2024-11-10T12:56:40Z",
      "closed_at": "2024-11-10T12:56:40Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/924/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/924",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/924",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:55.283381",
      "comments": []
    },
    {
      "issue_number": 493,
      "title": "Making retrieved contents visible in Run_web",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-06-14T16:09:29Z",
      "updated_at": "2024-11-10T12:55:15Z",
      "closed_at": "2024-11-10T12:55:13Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/493/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/493",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/493",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:55.283403",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "This is supported by #801",
          "created_at": "2024-11-10T12:55:13Z"
        }
      ]
    },
    {
      "issue_number": 927,
      "title": "[Feature Request] Add fsspec at Parsing part",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-08T05:40:12Z",
      "updated_at": "2024-11-08T05:40:12Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/927/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/927",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/927",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:55.502588",
      "comments": []
    },
    {
      "issue_number": 926,
      "title": "[Feature Request] Add Nvidia Nim LLM",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-08T01:22:56Z",
      "updated_at": "2024-11-08T05:39:37Z",
      "closed_at": "2024-11-08T05:39:35Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/926/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/926",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/926",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:55.502609",
      "comments": [
        {
          "author": "bwook00",
          "body": "We can use Nvidia Nim LLM to use LlamaIndex LLM's OpenAI LIke !\n\nHere is the example YAML file to use `nvidia/llama-3.1-nemotron-70b-instruct` model.\n\n```\nnodes:\n  - node_line_name: node_line_1\n    nodes:\n      - node_type: generator\n        modules:\n          - module_type: llama_index_llm\n        ",
          "created_at": "2024-11-08T05:39:36Z"
        }
      ]
    },
    {
      "issue_number": 925,
      "title": "[Feature Request] Automated Integration of LlamaIndex with Langfuse for Enhanced Observability",
      "body": "**Is your feature request related to a problem? Please describe.**\n\nIntegrating LlamaIndex with Langfuse for observability currently requires manual code implementation, which can be time-consuming and prone to errors.\n\n**Describe the solution you'd like**\n\nAn automated integration between LlamaIndex and Langfuse that simplifies the setup process, allowing developers to achieve observability with minimal configuration. Utilizing Langfuse's `LlamaIndexInstrumentor` for straightforward initialization would be ideal.\n\n**Describe alternatives you've considered**\n\nManually coding the integration, which increases the risk of inconsistencies and maintenance challenges. Exploring other observability tools was considered, but Langfuse's features align better with our requirements.\n\n**Additional context**\n\nLangfuse's official documentation provides examples for integrating with LlamaIndex.  Implementing an automated integration based on these guidelines would enhance developer productivity. \n\nhttps://langfuse.com/docs/integrations/llama-index/example-python-instrumentation-module ",
      "state": "open",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-11-07T07:18:56Z",
      "updated_at": "2024-11-07T07:19:09Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/925/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/925",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/925",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:55.725154",
      "comments": []
    },
    {
      "issue_number": 802,
      "title": "[Feature Request] Confuse when I create yaml file",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\nI'm newbie of RAG and using LLM. When I use autorag in my Kice slayer leader board project, There is a confusing issue when creating a yaml file and using the huggingface model in the Generator node.\n\n![Image](https://github.com/user-attachments/assets/578084fc-c9d9-401f-8b2c-bcf2780a9775)\n\nI misunderstood that I could use a HuggingFace model by setting `module_type` parameter as HuggingFaceLLM. \n\nFor those who don't know that they can use the huggingface model from llamma index llm, it would be nice to add a usecase in the doc to add the huggingface model to the yaml file.\n\n![Image](https://github.com/user-attachments/assets/6f43ec49-c960-403e-9b65-99726dccea4f)\n\n",
      "state": "closed",
      "author": "minsing-jin",
      "author_type": "User",
      "created_at": "2024-10-05T05:30:11Z",
      "updated_at": "2024-11-05T05:38:17Z",
      "closed_at": "2024-11-05T05:38:17Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/802/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/802",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/802",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.567464",
      "comments": []
    },
    {
      "issue_number": 679,
      "title": "[Structure Refactoring] Refactor for efficient deploying",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nThe current deployment is so inefficient, because it loads each model and delete it at inference. \r\nIt is good at optimization process, but not good at inference stage.\r\n\r\n**Describe the solution you'd like**\r\n![image](https://github.com/user-attachments/assets/e5954183-e4d8-4153-9112-3dcc6584e140)\r\n\r\n\r\n## Refactor Modules - each including run.py\r\n- [x] [Structure Refactoring] Retrieval\r\n- [x] [Structure Refactoring] Query Expansion\r\n- [x] [Structure Refactoring] Passage Reranker\r\n- [x] [Structure Refactoring] Passage Augmenter\r\n- [x] [Structure Refactoring] Passage Filter\r\n- [x] [Structure Refactoring] Generator\r\n- [x] [Structure Refactoring] Prompt Maker\r\n- [x] [Structure Refactoring] Passage Compressor\r\n- [x] #728\r\n- [x] #729\r\n\r\n## Extra\r\n- [x] Refactoring Evaluator \r\n- [x] Refactoring Runner\r\n\r\n## Test\r\n- [x] Testing and resolve errors\r\n- [x] Full run and real version test\r\n- [x] Documentation\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-05T10:54:05Z",
      "updated_at": "2024-11-05T02:43:25Z",
      "closed_at": "2024-09-23T14:35:59Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/679/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/679",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/679",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.567502",
      "comments": []
    },
    {
      "issue_number": 922,
      "title": "[Feature Request]Add RAG prompt",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-04T11:27:00Z",
      "updated_at": "2024-11-04T11:27:00Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/922/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/922",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/922",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.567514",
      "comments": []
    },
    {
      "issue_number": 915,
      "title": "[BUG] Unable to Route WatsonxLLM Requests to /ml/v1/text/generation — Defaults to Incorrect Endpoint",
      "body": "**Describe the bug**  \nIn AutoRAG's QA creation process using `WatsonxLLM`, setting the `url` parameter as `f\"{os.getenv('RUNTIME_ENV_APSX_URL')}\"` does not correctly route requests to the intended `/ml/v1/text/generation` endpoint. Instead, it defaults to `/m1/v1/text/chat`, resulting in a 404 error.\n\n**To Reproduce**  \nSteps to reproduce the behavior:\n1. Set up `WatsonxLLM` as shown below, ensuring `url=f\"{os.getenv('RUNTIME_ENV_APSX_URL')}\"` without appending `/ml/v1/text/generation`.\n2. Run the QA creation in AutoRAG.\n3. Note the error, as `WatsonxLLM` still attempts to reach `/m1/v1/text/chat` instead of the correct endpoint.\n\n**Expected behavior**  \nThe `WatsonxLLM` instance should route the request to `/ml/v1/text/generation` as intended when `url=f\"{os.getenv('RUNTIME_ENV_APSX_URL')}\"` is set.\n\n**Full Error log**  \n```\n---------------------------------------------------------------------------\nApiRequestFailure                         Traceback (most recent call last)\nCell In[107], line 24\n     15 corpus_df = pd.read_parquet(\"chunk.parquet\")\n     16 corpus_instance = Corpus(corpus_df, raw_instance)\n     18 initial_qa = (\n     19     corpus_instance.sample(random_single_hop, n=3)\n     20     .map(\n     21         lambda df: df.reset_index(drop=True),\n     22     )\n     23     .make_retrieval_gt_contents()\n---> 24     .batch_apply(\n     25         factoid_query_gen,  # query generation\n     26         llm=llm,\n     27     )\n     28     .batch_apply(\n     29         make_basic_gen_gt,  # answer generation (basic)\n     30         llm=llm,\n     31     )\n     32     .batch_apply(\n     33         make_concise_gen_gt,  # answer generation (concise)\n     34         llm=llm,\n     35     )\n     36     .filter(\n     37         dontknow_filter_rule_based,  # filter don't know\n     38         lang=\"ko\",\n     39     )\n     40 )\n     42 initial_qa.to_parquet('./qa.parquet', './corpus.parquet')\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/autorag/data/qa/schema.py:139, in QA.batch_apply(self, fn, batch_size, **kwargs)\n    137 loop = get_event_loop()\n    138 tasks = [fn(qa_dict, **kwargs) for qa_dict in qa_dicts]\n--> 139 results = loop.run_until_complete(process_batch(tasks, batch_size))\n    140 return QA(pd.DataFrame(results), self.linked_corpus)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/nest_asyncio.py:98, in _patch_loop.<locals>.run_until_complete(self, future)\n     95 if not f.done():\n     96     raise RuntimeError(\n     97         'Event loop stopped before Future completed.')\n---> 98 return f.result()\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/asyncio/futures.py:203, in Future.result(self)\n    201 self.__log_traceback = False\n    202 if self._exception is not None:\n--> 203     raise self._exception.with_traceback(self._exception_tb)\n    204 return self._result\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/asyncio/tasks.py:279, in Task.__step(***failed resolving arguments***)\n    277         result = coro.send(None)\n    278     else:\n--> 279         result = coro.throw(exc)\n    280 except StopIteration as exc:\n    281     if self._must_cancel:\n    282         # Task is cancelled right before coro stops.\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/autorag/utils/util.py:304, in process_batch(tasks, batch_size)\n    302 for i in range(0, len(tasks), batch_size):\n    303 \tbatch = tasks[i : i + batch_size]\n--> 304 \tbatch_results = await asyncio.gather(*batch)\n    305 \tresults.extend(batch_results)\n    307 return results\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/asyncio/tasks.py:349, in Task.__wakeup(self, future)\n    347 def __wakeup(self, future):\n    348     try:\n--> 349         future.result()\n    350     except BaseException as exc:\n    351         # This may also be a cancellation.\n    352         self.__step(exc)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/asyncio/tasks.py:277, in Task.__step(***failed resolving arguments***)\n    273 try:\n    274     if exc is None:\n    275         # We use the `send` method directly, because coroutines\n    276         # don't have `__iter__` and `__next__` methods.\n--> 277         result = coro.send(None)\n    278     else:\n    279         result = coro.throw(exc)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/autorag/data/qa/query/llama_gen_query.py:30, in factoid_query_gen(row, llm, lang)\n     25 async def factoid_query_gen(\n     26 \trow: Dict,\n     27 \tllm: BaseLLM,\n     28 \tlang: str = \"en\",\n     29 ) -> Dict:\n---> 30 \treturn await llama_index_generate_base(\n     31 \t\trow, llm, QUERY_GEN_PROMPT[\"factoid_single_hop\"][lang]\n     32 \t)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/autorag/data/qa/query/llama_gen_query.py:20, in llama_index_generate_base(row, llm, messages)\n     18 user_message = ChatMessage(role=MessageRole.USER, content=user_prompt)\n     19 new_messages = [*messages, user_message]\n---> 20 chat_response: ChatResponse = await llm.achat(messages=new_messages)\n     21 row[\"query\"] = chat_response.message.content\n     22 return row\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:357, in Dispatcher.span.<locals>.async_wrapper(func, instance, args, kwargs)\n    349 self.span_enter(\n    350     id_=id_,\n    351     bound_args=bound_args,\n   (...)\n    354     tags=tags,\n    355 )\n    356 try:\n--> 357     result = await func(*args, **kwargs)\n    358 except BaseException as e:\n    359     self.event(SpanDropEvent(span_id=id_, err_str=str(e)))\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:75, in llm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat(_self, messages, **kwargs)\n     66 event_id = callback_manager.on_event_start(\n     67     CBEventType.LLM,\n     68     payload={\n   (...)\n     72     },\n     73 )\n     74 try:\n---> 75     f_return_val = await f(_self, messages, **kwargs)\n     76 except BaseException as e:\n     77     callback_manager.on_event_end(\n     78         CBEventType.LLM,\n     79         payload={EventPayload.EXCEPTION: e},\n     80         event_id=event_id,\n     81     )\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/llama_index/llms/ibm/base.py:450, in WatsonxLLM.achat(self, messages, **kwargs)\n    444 @llm_chat_callback()\n    445 async def achat(\n    446     self,\n    447     messages: Sequence[ChatMessage],\n    448     **kwargs: Any,\n    449 ) -> ChatResponse:\n--> 450     return self.chat(messages, **kwargs)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:311, in Dispatcher.span.<locals>.wrapper(func, instance, args, kwargs)\n    308             _logger.debug(f\"Failed to reset active_span_id: {e}\")\n    310 try:\n--> 311     result = func(*args, **kwargs)\n    312     if isinstance(result, asyncio.Future):\n    313         # If the result is a Future, wrap it\n    314         new_future = asyncio.ensure_future(result)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:173, in llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat(_self, messages, **kwargs)\n    164 event_id = callback_manager.on_event_start(\n    165     CBEventType.LLM,\n    166     payload={\n   (...)\n    170     },\n    171 )\n    172 try:\n--> 173     f_return_val = f(_self, messages, **kwargs)\n    174 except BaseException as e:\n    175     callback_manager.on_event_end(\n    176         CBEventType.LLM,\n    177         payload={EventPayload.EXCEPTION: e},\n    178         event_id=event_id,\n    179     )\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/llama_index/llms/ibm/base.py:442, in WatsonxLLM.chat(self, messages, **kwargs)\n    439 else:\n    440     chat_fn = self._chat\n--> 442 return chat_fn(messages, **kwargs)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/llama_index/llms/ibm/base.py:419, in WatsonxLLM._chat(self, messages, **kwargs)\n    416 message_dicts = [to_watsonx_message_dict(message) for message in messages]\n    418 params, generation_kwargs = self._split_chat_generation_params(kwargs)\n--> 419 response = self._model.chat(\n    420     messages=message_dicts,\n    421     params=params,\n    422     tools=generation_kwargs.get(\"tools\"),\n    423     tool_choice=generation_kwargs.get(\"tool_choice\"),\n    424     tool_choice_option=generation_kwargs.get(\"tool_choice_option\"),\n    425 )\n    427 wx_message = response[\"choices\"][0][\"message\"]\n    428 message = from_watsonx_message(wx_message)\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/model_inference.py:271, in ModelInference.chat(self, messages, params, tools, tool_choice, tool_choice_option)\n    266 if self.model_id is None:\n    267     raise WMLClientError(\n    268         Messages.get_message(message_id=\"chat_deployment_scenario\")\n    269     )\n--> 271 return self._inference.chat(\n    272     messages=messages,\n    273     params=params,\n    274     tools=tools,\n    275     tool_choice=tool_choice,\n    276     tool_choice_option=tool_choice_option,\n    277 )\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py:129, in FMModelInference.chat(self, messages, params, tools, tool_choice, tool_choice_option)\n    116 def chat(\n    117     self,\n    118     messages: list[dict] | None = None,\n   (...)\n    122     tool_choice_option: Literal[\"none\", \"auto\"] | None = None,\n    123 ) -> dict:\n    125     text_chat_url = (\n    126         self._client.service_instance._href_definitions.get_fm_chat_href(\"chat\")\n    127     )\n--> 129     return self._send_chat_payload(\n    130         messages=messages,\n    131         params=params,\n    132         generate_url=text_chat_url,\n    133         tools=tools,\n    134         tool_choice=tool_choice,\n    135         tool_choice_option=tool_choice_option,\n    136     )\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py:273, in BaseModelInference._send_chat_payload(self, messages, params, generate_url, tools, tool_choice, tool_choice_option)\n    269     post_params[\"_retry_status_codes\"] = _RETRY_STATUS_CODES\n    271 response_scoring = self._http_client.post(**post_params)\n--> 273 return self._handle_response(\n    274     200,\n    275     \"generate\",\n    276     response_scoring,\n    277     _field_to_hide=\"generated_chat\",\n    278 )\n\nFile /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/ibm_watsonx_ai/wml_resource.py:155, in WMLResource._handle_response(self, expected_status_code, operationName, response, json_response, _silent_response_logging, _field_to_hide)\n    153         return response.text\n    154 else:\n--> 155     raise ApiRequestFailure(\n    156         \"Failure during {}.\".format(operationName),\n    157         response,\n    158     )\n\nApiRequestFailure: Failure during generate. (POST [Internal URL]\nStatus code: 404, body: {\"errors\":[{\"code\":\"path_not_found_error\",\"message\":\"URI path '/ml/v1/text/chat' does not exist\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai-cp\"}],\"trace\":\"83e0d568-eec6-4a83-80aa-9bb1c5acec70\",\"status_code\":404}\n```\n\n**Code that bug is happened**  \n```python\nfrom llama_index.llms.ibm import WatsonxLLM\nimport os\n\nllm = WatsonxLLM(\n    model_id=\"meta-llama/llama-3-1-8b-instruct\",\n    url=f\"{os.getenv('RUNTIME_ENV_APSX_URL')}\",  # Only base URL is set here\n    username=\"<username>\",\n    password=\"<password>\",\n    instance_id=\"openshift\",\n    version=\"5.0\",\n    space_id=\"<space_id>\",\n    temperature=0.1,\n    max_new_tokens=200,\n)\n```\n\n```python\nimport pandas as pd\n\nfrom autorag.data.qa.filter.dontknow import dontknow_filter_rule_based\nfrom autorag.data.qa.generation_gt.llama_index_gen_gt import (\n    make_basic_gen_gt,\n    make_concise_gen_gt,\n)\nfrom autorag.data.qa.schema import Raw, Corpus\nfrom autorag.data.qa.query.llama_gen_query import factoid_query_gen\nfrom autorag.data.qa.sample import random_single_hop\n\nraw_df = pd.read_parquet(\"parse_done.parquet\")\nraw_instance = Raw(raw_df)\n\ncorpus_df = pd.read_parquet(\"chunk.parquet\")\ncorpus_instance = Corpus(corpus_df, raw_instance)\n\ninitial_qa = (\n    corpus_instance.sample(random_single_hop, n=3)\n    .map(\n        lambda df: df.reset_index(drop=True),\n    )\n    .make_retrieval_gt_contents()\n    .batch_apply(\n        factoid_query_gen,  # query generation\n        llm=llm,\n    )\n    .batch_apply(\n        make_basic_gen_gt,  # answer generation (basic)\n        llm=llm,\n    )\n    .batch_apply(\n        make_concise_gen_gt,  # answer generation (concise)\n        llm=llm,\n    )\n    .filter(\n        dontknow_filter_rule_based,  # filter don't know\n        lang=\"ko\",\n    )\n)\n\ninitial_qa.to_parquet('./qa.parquet', './corpus.parquet')\n```\n\n**Desktop (please complete the following information):**\n- OS: Linux\n- Python version: 3.11.9\n\n**Additional Context**  \nThe packages were installed as follows:\n```bash\n!pip install \"AutoRAG[gpu,parse]\"\n!pip install -qU llama-index-llms-ibm\n```\n\nEven with `url=f\"{os.getenv('RUNTIME_ENV_APSX_URL')}\"`, `WatsonxLLM` defaults to the `/m1/v1/text/chat` endpoint, which causes the issue. An update is needed to ensure that requests are correctly routed to the intended endpoint.",
      "state": "closed",
      "author": "jjaegii",
      "author_type": "User",
      "created_at": "2024-11-01T05:58:08Z",
      "updated_at": "2024-11-04T05:01:13Z",
      "closed_at": "2024-11-04T05:01:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/915/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/915",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/915",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.567523",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@jjaegii Hello!\nI think it is because of the LlamaIndex. \nWe use `llm.achat` function, so maybe it will use chat endpoint. \n\nOne option is you can change the function of the each step. \nFor example, you can find the prompt of `factoid_query_gen` and making the similar function that uses `llm.acomple",
          "created_at": "2024-11-01T10:39:56Z"
        },
        {
          "author": "jjaegii",
          "body": "Hi! Thanks for the suggestions.\n\nIt seems that the issue indeed stems from `llm.achat` calling the chat endpoint, which isn’t available in my environment. I’ll try creating custom versions of each function step (like `factoid_query_gen`) to use `llm.acomplete` with a prompt-based approach. If needed",
          "created_at": "2024-11-04T05:01:12Z"
        }
      ]
    },
    {
      "issue_number": 918,
      "title": "[BUG] Missing Ollama and huggingface initialization",
      "body": "**Describe the bug**\n@bwook00 \nYou deleted this when you add Bedrock model.\nI feel ashamed that I missed this in the review process. Damn!!!\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-11-01T10:26:10Z",
      "updated_at": "2024-11-01T10:35:20Z",
      "closed_at": "2024-11-01T10:35:20Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/918/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/918",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/918",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.770671",
      "comments": []
    },
    {
      "issue_number": 908,
      "title": "[BUG]",
      "body": "**Describe the bug**\nAn error occurs during validation when trying to sample data from qa_data. The application throws a ValueError: Cannot take a larger sample than population when 'replace=False', indicating an attempt to sample more records than are available.\n\n**To Reproduce**\n\nSteps to reproduce the behavior:\n\nRun the command evaluator.start_trial('/content/config.yaml').\nObserve the sampling attempt in validator.validate(yaml_path) within the evaluator.py file.\nError is thrown due to the sampling request exceeding available data.\n\n**Expected behavior**\nThe validator should handle the sampling request correctly, sampling either the specified number of records or the available number if it’s smaller.\n\n**Full Error log**\n```\nevaluator.start_trial('/content/config.yaml')\n[10/31/24 03:48:58] INFO     [evaluator.py:127] >>                                        evaluator.py:127\n                                             _        _____            _____                              \n                                  /\\        | |      |  __ \\     /\\   / ____|                             \n                                 /  \\  _   _| |_ ___ | |__) |   /  \\ | |  __                              \n                                / /\\ \\| | | | __/ _ \\|  _  /   / /\\ \\| | |_ |                             \n                               / ____ \\ |_| | || (_) | | \\ \\  / ____ \\ |__| |                             \n                              /_/    \\_\\__,_|\\__\\___/|_|  \\_\\/_/    \\_\\_____|                             \n                                                                                                          \n                                                                                                          \n                    INFO     [evaluator.py:128] >> Start Validation input data and config evaluator.py:128\n                             YAML file first. If you want to skip this, put the                           \n                             --skip_validation flag or `skip_validation` at the                           \n                             start_trial function.                                                        \n[10/31/24 03:48:59] ERROR    [__init__.py:60] >> Unexpected exception                       __init__.py:60\n                             ╭──────────── Traceback (most recent call last) ─────────────╮               \n                             │ in <module>:1                                              │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/autorag/evaluator.py:138 in      │               \n                             │ start_trial                                                │               \n                             │                                                            │               \n                             │   135 │   │   │   validator = Validator(                   │               \n                             │   136 │   │   │   │   qa_data_path=self.qa_data_path, corp │               \n                             │   137 │   │   │   )                                        │               \n                             │ ❱ 138 │   │   │   validator.validate(yaml_path)            │               \n                             │   139 │   │                                                │               \n                             │   140 │   │   os.environ[\"PROJECT_DIR\"] = self.project_dir │               \n                             │   141                                                      │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/autorag/validator.py:46 in       │               \n                             │ validate                                                   │               \n                             │                                                            │               \n                             │   43 │                                                     │               \n                             │   44 │   def validate(self, yaml_path: str, qa_cnt: int =  │               \n                             │   45 │   │   # sample QA data                              │               \n                             │ ❱ 46 │   │   sample_qa_df = self.qa_data.sample(qa_cnt, ra │               \n                             │   47 │   │   sample_qa_df.reset_index(drop=True, inplace=T │               \n                             │   48 │   │                                                 │               \n                             │   49 │   │   # get doc_id                                  │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/pandas/core/generic.py:6118 in   │               \n                             │ sample                                                     │               \n                             │                                                            │               \n                             │    6115 │   │   if weights is not None:                    │               \n                             │    6116 │   │   │   weights = sample.preprocess_weights(se │               \n                             │    6117 │   │                                              │               \n                             │ ❱  6118 │   │   sampled_indices = sample.sample(obj_len, s │               \n                             │    6119 │   │   result = self.take(sampled_indices, axis=a │               \n                             │    6120 │   │                                              │               \n                             │    6121 │   │   if ignore_index:                           │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/pandas/core/sample.py:152 in     │               \n                             │ sample                                                     │               \n                             │                                                            │               \n                             │   149 │   │   else:                                        │               \n                             │   150 │   │   │   raise ValueError(\"Invalid weights: weigh │               \n                             │   151 │                                                    │               \n                             │ ❱ 152 │   return random_state.choice(obj_len, size=size, r │               \n                             │   153 │   │   np.intp, copy=False                          │               \n                             │   154 │   )                                                │               \n                             │   155                                                      │               \n                             │                                                            │               \n                             │ in numpy.random.mtrand.RandomState.choice:1001             │               \n                             ╰────────────────────────────────────────────────────────────╯               \n                             ValueError: Cannot take a larger sample than population when                 \n                             'replace=False'                                                              \n>>> exit()\n(venv) (base) ➜  autorag autorag evaluate --config config.yaml --qa_data_path ./content/initial_qa.parquet --corpus_data_path ./content/initial_corpus.parquet\n\n\nNone of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n[10/31/24 03:53:45] INFO     [__init__.py:100] >> You are using API version of AutoRAG.To  __init__.py:100\n                             use local version, run pip install 'AutoRAG[gpu]'                            \n                    INFO     [__init__.py:125] >> You are using API version of AutoRAG.To  __init__.py:125\n                             use local version, run pip install 'AutoRAG[gpu]'                            \n[10/31/24 03:53:53] INFO     [_client.py:1038] >> HTTP Request: GET                        _client.py:1038\n                             https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"                 \n                    INFO     [evaluator.py:127] >>                                        evaluator.py:127\n                                             _        _____            _____                              \n                                  /\\        | |      |  __ \\     /\\   / ____|                             \n                                 /  \\  _   _| |_ ___ | |__) |   /  \\ | |  __                              \n                                / /\\ \\| | | | __/ _ \\|  _  /   / /\\ \\| | |_ |                             \n                               / ____ \\ |_| | || (_) | | \\ \\  / ____ \\ |__| |                             \n                              /_/    \\_\\__,_|\\__\\___/|_|  \\_\\/_/    \\_\\_____|                             \n                                                                                                          \n                                                                                                          \n                    INFO     [evaluator.py:128] >> Start Validation input data and config evaluator.py:128\n                             YAML file first. If you want to skip this, put the                           \n                             --skip_validation flag or `skip_validation` at the                           \n                             start_trial function.                                                        \n[10/31/24 03:53:54] ERROR    [__init__.py:60] >> Unexpected exception                       __init__.py:60\n                             ╭──────────── Traceback (most recent call last) ─────────────╮               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/bin │               \n                             │ /autorag:8 in <module>                                     │               \n                             │                                                            │               \n                             │   5 from autorag.cli import cli                            │               \n                             │   6 if __name__ == '__main__':                             │               \n                             │   7 │   sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '' │               \n                             │ ❱ 8 │   sys.exit(cli())                                    │               \n                             │   9                                                        │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/click/core.py:1157 in __call__   │               \n                             │                                                            │               \n                             │   1154 │                                                   │               \n                             │   1155 │   def __call__(self, *args: t.Any, **kwargs: t.An │               \n                             │   1156 │   │   \"\"\"Alias for :meth:`main`.\"\"\"               │               \n                             │ ❱ 1157 │   │   return self.main(*args, **kwargs)           │               \n                             │   1158                                                     │               \n                             │   1159                                                     │               \n                             │   1160 class Command(BaseCommand):                         │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/click/core.py:1078 in main       │               \n                             │                                                            │               \n                             │   1075 │   │   try:                                        │               \n                             │   1076 │   │   │   try:                                    │               \n                             │   1077 │   │   │   │   with self.make_context(prog_name, a │               \n                             │ ❱ 1078 │   │   │   │   │   rv = self.invoke(ctx)           │               \n                             │   1079 │   │   │   │   │   if not standalone_mode:         │               \n                             │   1080 │   │   │   │   │   │   return rv                   │               \n                             │   1081 │   │   │   │   │   # it's not safe to `ctx.exit(rv │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/click/core.py:1688 in invoke     │               \n                             │                                                            │               \n                             │   1685 │   │   │   │   super().invoke(ctx)                 │               \n                             │   1686 │   │   │   │   sub_ctx = cmd.make_context(cmd_name │               \n                             │   1687 │   │   │   │   with sub_ctx:                       │               \n                             │ ❱ 1688 │   │   │   │   │   return _process_result(sub_ctx. │               \n                             │   1689 │   │                                               │               \n                             │   1690 │   │   # In chain mode we create the contexts step │               \n                             │   1691 │   │   # base command has been invoked.  Because a │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/click/core.py:1434 in invoke     │               \n                             │                                                            │               \n                             │   1431 │   │   │   echo(style(message, fg=\"red\"), err=True │               \n                             │   1432 │   │                                               │               \n                             │   1433 │   │   if self.callback is not None:               │               \n                             │ ❱ 1434 │   │   │   return ctx.invoke(self.callback, **ctx. │               \n                             │   1435 │                                                   │               \n                             │   1436 │   def shell_complete(self, ctx: Context, incomple │               \n                             │   1437 │   │   \"\"\"Return a list of completions for the inc │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/click/core.py:783 in invoke      │               \n                             │                                                            │               \n                             │    780 │   │                                               │               \n                             │    781 │   │   with augment_usage_errors(__self):          │               \n                             │    782 │   │   │   with ctx:                               │               \n                             │ ❱  783 │   │   │   │   return __callback(*args, **kwargs)  │               \n                             │    784 │                                                   │               \n                             │    785 │   def forward(                                    │               \n                             │    786 │   │   __self, __cmd: \"Command\", *args: t.Any, **k │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/autorag/cli.py:54 in evaluate    │               \n                             │                                                            │               \n                             │    51 │   if not os.path.exists(config):                   │               \n                             │    52 │   │   raise ValueError(f\"Config file {config} does │               \n                             │    53 │   evaluator = Evaluator(qa_data_path, corpus_data_ │               \n                             │ ❱  54 │   evaluator.start_trial(config, skip_validation=sk │               \n                             │    55                                                      │               \n                             │    56                                                      │               \n                             │    57 @click.command()                                     │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/autorag/evaluator.py:138 in      │               \n                             │ start_trial                                                │               \n                             │                                                            │               \n                             │   135 │   │   │   validator = Validator(                   │               \n                             │   136 │   │   │   │   qa_data_path=self.qa_data_path, corp │               \n                             │   137 │   │   │   )                                        │               \n                             │ ❱ 138 │   │   │   validator.validate(yaml_path)            │               \n                             │   139 │   │                                                │               \n                             │   140 │   │   os.environ[\"PROJECT_DIR\"] = self.project_dir │               \n                             │   141                                                      │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/autorag/validator.py:46 in       │               \n                             │ validate                                                   │               \n                             │                                                            │               \n                             │   43 │                                                     │               \n                             │   44 │   def validate(self, yaml_path: str, qa_cnt: int =  │               \n                             │   45 │   │   # sample QA data                              │               \n                             │ ❱ 46 │   │   sample_qa_df = self.qa_data.sample(qa_cnt, ra │               \n                             │   47 │   │   sample_qa_df.reset_index(drop=True, inplace=T │               \n                             │   48 │   │                                                 │               \n                             │   49 │   │   # get doc_id                                  │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/pandas/core/generic.py:6118 in   │               \n                             │ sample                                                     │               \n                             │                                                            │               \n                             │    6115 │   │   if weights is not None:                    │               \n                             │    6116 │   │   │   weights = sample.preprocess_weights(se │               \n                             │    6117 │   │                                              │               \n                             │ ❱  6118 │   │   sampled_indices = sample.sample(obj_len, s │               \n                             │    6119 │   │   result = self.take(sampled_indices, axis=a │               \n                             │    6120 │   │                                              │               \n                             │    6121 │   │   if ignore_index:                           │               \n                             │                                                            │               \n                             │ /Users/jayanthkumar/Documents/development/autorag/venv/lib │               \n                             │ /python3.10/site-packages/pandas/core/sample.py:152 in     │               \n                             │ sample                                                     │               \n                             │                                                            │               \n                             │   149 │   │   else:                                        │               \n                             │   150 │   │   │   raise ValueError(\"Invalid weights: weigh │               \n                             │   151 │                                                    │               \n                             │ ❱ 152 │   return random_state.choice(obj_len, size=size, r │               \n                             │   153 │   │   np.intp, copy=False                          │               \n                             │   154 │   )                                                │               \n                             │   155                                                      │               \n                             │                                                            │               \n                             │ in numpy.random.mtrand.RandomState.choice:1001             │               \n                             ╰────────────────────────────────────────────────────────────╯               \n                             ValueError: Cannot take a larger sample than population when                 \n                             'replace=False'         \n```\n\n**Code that bug is happened**\nevaluator.start_trial('/content/config.yaml')\n\n**Desktop (please complete the following information):**\n- OS: MacOS\n- Python version: 3.10\n\n**Additional context**\nIt appears that the number of records requested in the sampling (qa_cnt) exceeds the available records in qa_data.\nConsider adding a check to compare qa_cnt with the actual count of records before sampling, or using min(qa_cnt, len(self.qa_data)) in the sampling function to prevent this error.\n",
      "state": "closed",
      "author": "jayanthkmr",
      "author_type": "User",
      "created_at": "2024-10-30T22:28:22Z",
      "updated_at": "2024-11-01T10:08:54Z",
      "closed_at": "2024-11-01T10:08:54Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/908/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/908",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/908",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.770696",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@jayanthkmr Hello!\nIt looks like your `qa.parquet` rows are less than 5. \nFor validation, you have to prepare more than 5 QAs.\nPlus, for accurate result, we recommend to use more than 30 QAs at least.",
          "created_at": "2024-10-31T03:03:06Z"
        }
      ]
    },
    {
      "issue_number": 916,
      "title": "[Hotfix] change hugging face url at README",
      "body": "RAG pipeline optimization space url is wrong",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-11-01T09:19:17Z",
      "updated_at": "2024-11-01T10:00:55Z",
      "closed_at": "2024-11-01T10:00:55Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/916/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/916",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/916",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.959288",
      "comments": []
    },
    {
      "issue_number": 914,
      "title": "[Feature Request] Add Data Creation Tutorial .ipynb file and tutorial folder",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-31T11:32:04Z",
      "updated_at": "2024-10-31T11:32:04Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/914/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/914",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/914",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.959307",
      "comments": []
    },
    {
      "issue_number": 910,
      "title": "[Feature Request] Add CLI Version Check",
      "body": "**Is your feature request related to a problem? Please describe.**\nIt’s difficult to verify the CLI version quickly, leading to compatibility issues.\n\n**Describe the solution you'd like**\nAdd a version check command (e.g., --version or -v) to display the current CLI version.\n\n**Describe alternatives you've considered**\nManually checking version details in files.\n/python3.10/site-packages/AutoRAG-0.3.8.dist-info/\n\nor \n```sh \n$ pip show autorag\nName: AutoRAG\nVersion: 0.3.8\n...\n```\n**Additional context**\nN/A",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-31T01:41:24Z",
      "updated_at": "2024-10-31T08:52:30Z",
      "closed_at": "2024-10-31T08:52:30Z",
      "labels": [
        "enhancement",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/910/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/910",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/910",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:57.959314",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@hongsw \nYou can check `VERSION` file.\n\n```python\nwith open(‘VERSION’, ‘w’) as f:\n    version = f.read()\n```\n\nSomething like this?",
          "created_at": "2024-10-31T03:04:09Z"
        },
        {
          "author": "hongsw",
          "body": "$ autorag -V\n\nAutoRAG v0.3.8\n\nHere's what I expect it to do.\n@vkehfdl1 ",
          "created_at": "2024-10-31T05:29:48Z"
        }
      ]
    },
    {
      "issue_number": 854,
      "title": "[Qustion] QA-Corpus mapping",
      "body": "I'm confused with that step2 notebook and tutorial document are a little different.\n\nIn step2 notebook, QA is generated from these 2 instances:\n\n```python\nraw_df = pd.read_parquet(\"/content/chunk_project_dir/0/0.parquet\")\nraw_instance = Raw(raw_df)\n\ncorpus_df = pd.read_parquet(\"/content/chunk_project_dir/0/0.parquet\")\ncorpus_instance = Corpus(corpus_df, raw_instance)\n\ninitial_qa = (\n    corpus_instance.sample(random_single_hop, n=3)\n    (snip)\n)\n\ninitial_qa.to_parquet('/content/initial_qa.parquet', '/content/initial_corpus.parquet')\n\nnew_corpus_df = pd.read_parquet(\"/content/chunk_project_dir/0/1.parquet\")\nnew_corpus_instance = Corpus(new_corpus_df, raw_instance)\n\nnew_qa = initial_qa.update_corpus(new_corpus_instance)\nnew_qa.to_parquet(\"/content/new_qa.parquet\", \"/content/new_corpus.parquet\")\n```\n\nseems initial_qa and initial corpus are created from one of \"chunked\" parquet data, and another optimized qa data and corpus are created from the combination of another \"chunked\" parquet and initial corpus and qa.\n\nOTOH, in tutorial documents(https://docs.auto-rag.com/data_creation/tutorial.html) seems:\n\n```python\n# initial_raw seems \"parsed\" data\ninitial_raw = Raw(initial_raw_df)\n\n# initial chunk\ninitial_corpus = initial_raw.chunk(\n    \"llama_index_chunk\", chunk_method=\"token\", chunk_size=128, chunk_overlap=5\n)\nllm = OpenAI()\ninitial_qa = (\n    initial_corpus.sample(random_single_hop, n=3)\n   (snip)\n)\ninitial_qa.to_parquet(\"./initial_qa.parquet\", \"./initial_corpus.parquet\")\n\n# chunk optimization, now we have other variations of \"chunk\"s\nchunker = Chunker.from_parquet(\"./initial_raw.parquet\", \"./chunk_project_dir\")\nchunker.start_chunking(\"./chunking.yaml\")\n\n# corpus-qa mapping\nraw = Raw(initial_raw_df)   # <-- \"parsed\" data, right?\ncorpus = Corpus(initial_corpus_df, raw)\nqa = QA(initial_qa_df, corpus)\n\nnew_qa = qa.update_corpus(Corpus(new_corpus_df, raw))\n```\n\nin this tutorial, optimized qa data and corpus are created from the combination of  \"initial_raw_df\", which is the same as \"parsed\" data, and initial corpus and qa.\n\nseems \"different data\" are used and I'm confused.\n\nso my basic question: is this code in step2 notebook correct?\n\n```\nraw_df = pd.read_parquet(\"/content/chunk_project_dir/0/0.parquet\")\nraw_instance = Raw(raw_df)\n```",
      "state": "closed",
      "author": "kun432",
      "author_type": "User",
      "created_at": "2024-10-17T01:59:15Z",
      "updated_at": "2024-10-31T08:45:48Z",
      "closed_at": "2024-10-31T08:45:48Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/854/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "thap2331"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/854",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/854",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:58.164070",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hello @kun432 \n\n```python\nraw_df = pd.read_parquet(\"/content/chunk_project_dir/0/0.parquet\")\nraw_instance = Raw(raw_df)\n```\nThis code is nothing wrong. So `Raw` instance is basically a parsed data. You can make it using `Parser` and `parse.yaml`. https://docs.auto-rag.com/data_creation/parse/parse.h",
          "created_at": "2024-10-18T06:51:20Z"
        },
        {
          "author": "thap2331",
          "body": "Wohh...\nThis needs to be somewhere in the doc...Please...\n\n`Raw` : parsed data\n`Corpus` : chunked data\n`QA` : Question & Answer dataset based on the corpus",
          "created_at": "2024-10-20T00:32:46Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@thap2331 Okay we will.\nI will make this as documentation issue",
          "created_at": "2024-10-20T08:42:58Z"
        },
        {
          "author": "kun432",
          "body": "@vkehfdl1 Thank you.\n\nWell, I think I got it. It's like `Raw` make its input parquet/df to \"parsed\" instance regardless of whether its input parquet/df is chunked or not, right? \n\nLet's say with step2 notebook, as an example.\n\nFirst, parse a pdf with Parser.\n\n```\nfrom autorag.parser import Parser\n\np",
          "created_at": "2024-10-20T14:11:38Z"
        },
        {
          "author": "thap2331",
          "body": "> [@thap2331](https://github.com/thap2331) Okay we will. I will make this as documentation issue\n\n@vkehfdl1 you can assign this to me. Or, I can get this going. If I do, I will assign this to you.",
          "created_at": "2024-10-20T17:17:19Z"
        }
      ]
    },
    {
      "issue_number": 911,
      "title": "[Guide] AttributeError: '_config' object has no attribute 'initialized",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n```\nAttributeError: '_config' object has no attribute 'initialized\n```\n![Image](https://github.com/user-attachments/assets/4feda015-14e8-4e50-8d46-a4cd64908f70)\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Full Error log**\nIf applicable, add full error log to help explain your problem.\n\n**Code that bug is happened**\nIf applicable, add the code that bug is happened.\n(Especially, your AutoRAG YAML file or python codes that you wrote)\n\n**Desktop (please complete the following information):**\n - OS: MacOS\n - Python version 3.10.15\n - AutoRAG version 0.3.8\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-31T02:41:19Z",
      "updated_at": "2024-10-31T02:45:39Z",
      "closed_at": "2024-10-31T02:44:02Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/911/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/911",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/911",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:58.367722",
      "comments": [
        {
          "author": "hongsw",
          "body": "```sh\npip freeze > temp_requirements-main.venv.txt    # backup current versions\npip freeze | xargs pip uninstall -y\npip install autorag\n```\nor\n\nMake sure the Langchain version is '0.3.6'\n![Image](https://github.com/user-attachments/assets/baf62883-d84c-44c7-8d21-be518ce2be05)\n",
          "created_at": "2024-10-31T02:43:57Z"
        }
      ]
    },
    {
      "issue_number": 909,
      "title": "[Feature Request] GitHub Action Causes Large Docker Image Size",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n\nhttps://hub.docker.com/layers/autoraghq/autorag/api-0.3.7/images/sha256-d3b8569b1e2e1a2649e9f1461cab159830a23eb287dbf38f1891bf2a80c91562?context=explore\n\nhttps://hub.docker.com/layers/autoraghq/autorag/api-0.3.8/images/sha256-9f08bfc5f178f6c5e57f77925055ee98f7d80ec5b4712a8a7952bb926c06f2c8?context=explore",
      "state": "open",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-31T00:43:43Z",
      "updated_at": "2024-10-31T00:46:17Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/909/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/909",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/909",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:58.557685",
      "comments": []
    },
    {
      "issue_number": 906,
      "title": "[Feature Request] Add API server for running each features like validation, starting trial, chunking or parsing.",
      "body": "**Is your feature request related to a problem? Please describe.**\nUsing AutoRAG at remote service.\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-30T07:32:44Z",
      "updated_at": "2024-10-30T14:10:48Z",
      "closed_at": "2024-10-30T14:10:46Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/906/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/906",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/906",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:58.557709",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Work this as private",
          "created_at": "2024-10-30T14:10:46Z"
        }
      ]
    },
    {
      "issue_number": 907,
      "title": "[Feature Request]Need supporting Azure OpenAI",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "closed",
      "author": "hellangleZ",
      "author_type": "User",
      "created_at": "2024-10-30T12:34:41Z",
      "updated_at": "2024-10-30T14:10:27Z",
      "closed_at": "2024-10-30T14:10:26Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/907/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/907",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/907",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:59.092742",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@hellangleZ Hello!\nYou can use Azure OpenAI using LlamaIndex LLM.\nDuplicate with #714 \n\nhttps://docs.auto-rag.com/local_model.html\nSee this docs how to add it.\n\nWe support all LLMs in LlamaIndex, so you do not worried about LLM integrations.\n\nSince this is duplicate I am closing this issue.",
          "created_at": "2024-10-30T14:10:26Z"
        }
      ]
    },
    {
      "issue_number": 866,
      "title": "[Feature Request] Add MultiModal Parsing at Llama Parse Module",
      "body": "https://docs.cloud.llamaindex.ai/llamaparse/features/multimodal",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-20T06:27:54Z",
      "updated_at": "2024-10-30T06:34:52Z",
      "closed_at": "2024-10-30T06:34:51Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/866/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/866",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/866",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:59.314097",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "clost at #868 ",
          "created_at": "2024-10-30T06:34:51Z"
        }
      ]
    },
    {
      "issue_number": 900,
      "title": "[Feature Request] Add progress bar",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-30T03:55:33Z",
      "updated_at": "2024-10-30T05:38:47Z",
      "closed_at": "2024-10-30T05:38:47Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/900/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/900",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/900",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:59.536231",
      "comments": []
    },
    {
      "issue_number": 901,
      "title": "[BUG] VectorDB not working at the query expansion",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Full Error log**\n```\n │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/evaluator.py:188 in start_trial                               │               \n                             │                                                                                                                           │               \n                             │   185 │   │   │   if i == 0:                                                                                              │               \n                             │   186 │   │   │   │   previous_result = self.qa_data                                                                      │               \n                             │   187 │   │   │   logger.info(f\"Running node line {node_line_name}...\")                                                   │               \n                             │ ❱ 188 │   │   │   previous_result = run_node_line(node_line, node_line_dir, previous_result)                              │               \n                             │   189 │   │   │                                                                                                           │               \n                             │   190 │   │   │   trial_summary_df = self._append_node_line_summary(                                                      │               \n                             │   191 │   │   │   │   node_line_name, node_line_dir, trial_summary_df                                                     │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/node_line.py:47 in run_node_line                              │               \n                             │                                                                                                                           │               \n                             │   44 │                                                                                                                    │               \n                             │   45 │   summary_lst = []                                                                                                 │               \n                             │   46 │   for node in nodes:                                                                                               │               \n                             │ ❱ 47 │   │   previous_result = node.run(previous_result, node_line_dir)                                                   │               \n                             │   48 │   │   node_summary_df = load_summary_file(                                                                         │               \n                             │   49 │   │   │   os.path.join(node_line_dir, node.node_type, \"summary.csv\")                                               │               \n                             │   50 │   │   )                                                                                                            │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/schema/node.py:57 in run                                      │               \n                             │                                                                                                                           │               \n                             │    54 │   def run(self, previous_result: pd.DataFrame, node_line_dir: str) -> pd.DataFrame:                               │               \n                             │    55 │   │   logger.info(f\"Running node {self.node_type}...\")                                                            │               \n                             │    56 │   │   input_modules, input_params = self.get_param_combinations()                                                 │               \n                             │ ❱  57 │   │   return self.run_node(                                                                                       │               \n                             │    58 │   │   │   modules=input_modules,                                                                                  │               \n                             │    59 │   │   │   module_params=input_params,                                                                             │               \n                             │    60 │   │   │   previous_result=previous_result,                                                                        │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/nodes/queryexpansion/run.py:132 in run_query_expansion_node   │               \n                             │                                                                                                                           │               \n                             │   129 │   │   ]                                                                                                           │               \n                             │   130 │   │                                                                                                               │               \n                             │   131 │   │   # run evaluation                                                                                            │               \n                             │ ❱ 132 │   │   evaluation_results = list(                                                                                  │               \n                             │   133 │   │   │   map(                                                                                                    │               \n                             │   134 │   │   │   │   lambda result: evaluate_one_query_expansion_node(                                                   │               \n                             │   135 │   │   │   │   │   retrieval_callables,                                                                            │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/nodes/queryexpansion/run.py:134 in <lambda>                   │               \n                             │                                                                                                                           │               \n                             │   131 │   │   # run evaluation                                                                                            │               \n                             │   132 │   │   evaluation_results = list(                                                                                  │               \n                             │   133 │   │   │   map(                                                                                                    │               \n                             │ ❱ 134 │   │   │   │   lambda result: evaluate_one_query_expansion_node(                                                   │               \n                             │   135 │   │   │   │   │   retrieval_callables,                                                                            │               \n                             │   136 │   │   │   │   │   retrieval_params,                                                                               │               \n                             │   137 │   │   │   │   │   [                                                                                               │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/nodes/queryexpansion/run.py:212 in                            │               \n                             │ evaluate_one_query_expansion_node                                                                                         │               \n                             │                                                                                                                           │               \n                             │   209 │   previous_result[\"queries\"] = [                                                                                  │               \n                             │   210 │   │   metric_input.queries for metric_input in metric_inputs                                                      │               \n                             │   211 │   ]                                                                                                               │               \n                             │ ❱ 212 │   retrieval_results = list(                                                                                       │               \n                             │   213 │   │   map(                                                                                                        │               \n                             │   214 │   │   │   lambda x: x[0].run_evaluator(                                                                           │               \n                             │   215 │   │   │   │   project_dir=project_dir, previous_result=previous_result, **x[1]                                    │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/nodes/queryexpansion/run.py:214 in <lambda>                   │               \n                             │                                                                                                                           │               \n                             │   211 │   ]                                                                                                               │               \n                             │   212 │   retrieval_results = list(                                                                                       │               \n                             │   213 │   │   map(                                                                                                        │               \n                             │ ❱ 214 │   │   │   lambda x: x[0].run_evaluator(                                                                           │               \n                             │   215 │   │   │   │   project_dir=project_dir, previous_result=previous_result, **x[1]                                    │               \n                             │   216 │   │   │   ),                                                                                                      │               \n                             │   217 │   │   │   zip(retrieval_funcs, retrieval_params),                                                                 │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/schema/base.py:26 in run_evaluator                            │               \n                             │                                                                                                                           │               \n                             │   23 │   │   **kwargs,                                                                                                    │               \n                             │   24 │   ):                                                                                                               │               \n                             │   25 │   │   instance = cls(project_dir, *args, **kwargs)                                                                 │               \n                             │ ❱ 26 │   │   result = instance.pure(previous_result, *args, **kwargs)                                                     │               \n                             │   27 │   │   del instance                                                                                                 │               \n                             │   28 │   │   return result                                                                                                │               \n                             │   29                                                                                                                      │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/utils/util.py:69 in wrapper                                   │               \n                             │                                                                                                                           │               \n                             │    66 │   def decorator_result_to_dataframe(func: Callable):                                                              │               \n                             │    67 │   │   @functools.wraps(func)                                                                                      │               \n                             │    68 │   │   def wrapper(*args, **kwargs) -> pd.DataFrame:                                                               │               \n                             │ ❱  69 │   │   │   results = func(*args, **kwargs)                                                                         │               \n                             │    70 │   │   │   if len(column_names) == 1:                                                                              │               \n                             │    71 │   │   │   │   df_input = {column_names[0]: results}                                                               │               \n                             │    72 │   │   │   else:                                                                                                   │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/nodes/retrieval/vectordb.py:72 in pure                        │               \n                             │                                                                                                                           │               \n                             │    69 │   │   queries = self.cast_to_run(previous_result)                                                                 │               \n                             │    70 │   │   pure_params = pop_params(self._pure, kwargs)                                                                │               \n                             │    71 │   │   # import pdb; pdb.set_trace()                                                                               │               \n                             │ ❱  72 │   │   ids, scores = self._pure(queries, **pure_params)                                                            │               \n                             │    73 │   │   contents = fetch_contents(self.corpus_df, ids)                                                              │               \n                             │    74 │   │   return contents, ids, scores                                                                                │               \n                             │    75                                                                                                                     │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/nodes/retrieval/vectordb.py:111 in _pure                      │               \n                             │                                                                                                                           │               \n                             │   108 │   │   │   for query_list in queries                                                                               │               \n                             │   109 │   │   ]                                                                                                           │               \n                             │   110 │   │   loop = get_event_loop()                                                                                     │               \n                             │ ❱ 111 │   │   results = loop.run_until_complete(                                                                          │               \n                             │   112 │   │   │   process_batch(tasks, batch_size=embedding_batch)                                                        │               \n                             │   113 │   │   )                                                                                                           │               \n                             │   114 │   │   id_result = list(map(lambda x: x[0], results))                                                              │               \n                             │                                                                                                                           │               \n                             │ /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py │               \n                             │ :687 in run_until_complete                                                                                                │               \n                             │                                                                                                                           │               \n                             │    684 │   │   if not future.done():                                                                                      │               \n                             │    685 │   │   │   raise RuntimeError('Event loop stopped before Future completed.')                                      │               \n                             │    686 │   │                                                                                                              │               \n                             │ ❱  687 │   │   return future.result()                                                                                     │               \n                             │    688 │                                                                                                                  │               \n                             │    689 │   def stop(self):                                                                                                │               \n                             │    690 │   │   \"\"\"Stop running the event loop.                                                                            │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/utils/util.py:302 in process_batch                            │               \n                             │                                                                                                                           │               \n                             │   299 │                                                                                                                   │               \n                             │   300 │   for i in range(0, len(tasks), batch_size):                                                                      │               \n                             │   301 │   │   batch = tasks[i : i + batch_size]                                                                           │               \n                             │ ❱ 302 │   │   batch_results = await asyncio.gather(*batch)                                                                │               \n                             │   303 │   │   results.extend(batch_results)                                                                               │               \n                             │   304 │                                                                                                                   │               \n                             │   305 │   return results                                                                                                  │               \n                             │                                                                                                                           │               \n                             │ /Users/rfagundes/Documents/Projects/AutoRAG_Braxted/autorag/nodes/retrieval/vectordb.py:192 in vectordb_pure              │               \n                             │                                                                                                                           │               \n                             │   189 │   │   )                                                                                                           │               \n                             │   190 │   ]                                                                                                               │               \n                             │   191 │   # import pdb; pdb.set_trace()                                                                                   │               \n                             │ ❱ 192 │   id_result, score_result = zip(*result)                                                                          │               \n                             │   193 │   return list(id_result), list(score_result)                                                                      │               \n                             │   194                                                                                                                     │               \n                             │   195 async def filter_exist_ids(                                                                                         │               \n                             ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯               \n                             ValueError: not enough values to unpack (expected 2, got 0)  \n```\n\n**Code that bug is happened**\nIf applicable, add the code that bug is happened.\n(Especially, your AutoRAG YAML file or python codes that you wrote)\n\n**Desktop (please complete the following information):**\n - OS: [e.g. Windows, Linux, MacOS]\n - Python version [e.g. 3.10]\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-30T04:11:44Z",
      "updated_at": "2024-10-30T05:32:16Z",
      "closed_at": "2024-10-30T05:32:16Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/901/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/901",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/901",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:59.536254",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It looks like you are using vectordb name \"default\" at your YAML file, is that correct? I do not recommend you use the name \"default\". I think it causes error because the name \"default\" is directed to default vectordb. Which is using openai embedding model and collection_name openai. It will be caus",
          "created_at": "2024-10-30T04:59:36Z"
        }
      ]
    },
    {
      "issue_number": 886,
      "title": "[BUG] pip install -U AutoRAG fails: Cannot uninstall distutils-installed blinker 1.4",
      "body": "![Image](https://github.com/user-attachments/assets/c67e56e6-4886-4d45-aeec-287dfcbd3934)\n",
      "state": "closed",
      "author": "abdshomad",
      "author_type": "User",
      "created_at": "2024-10-25T11:26:17Z",
      "updated_at": "2024-10-30T05:29:03Z",
      "closed_at": "2024-10-30T05:29:02Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/886/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/886",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/886",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:59.723737",
      "comments": [
        {
          "author": "abdshomad",
          "body": "**Describe the bug**\n\nEnvironment: Google Colab running AutoRAG Tutorial 2 - Evaluation Dataset Creation.ipynb\n\n`pip` fails to install -Uq AutoRAG[parse]>=0.3.0 datasets. It required to uninstall the `blinker` package (version 1.4) when attempting to upgrade `AutoRAG`, preventing the upgrade process",
          "created_at": "2024-10-25T11:27:03Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@abdshomad Hello!\nTry this before installing AutoRAG in colab.\n\n```\n%pip install blinker==1.4 ipykernel==5.5.6\n```\n",
          "created_at": "2024-10-25T13:10:31Z"
        },
        {
          "author": "abdshomad",
          "body": "> [@abdshomad](https://github.com/abdshomad) Hello! Try this before installing AutoRAG in colab.\n> \n> ```\n> %pip install blinker==1.4 ipykernel==5.5.6\n> ```\n\nI tried but still failed. Here's the screenshot. NOTE: I am running it in Google Colab link provided in this repo. \n\n![Image](https://github.c",
          "created_at": "2024-10-26T15:20:32Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@abdshomad \nWe are working on this issue on the #888 PR...",
          "created_at": "2024-10-27T10:39:36Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@abdshomad \n\n```\n%%shell\napt-get remove python3-blinker\npip install blinker==1.8.2\n%pip install -Uq ipykernel==5.5.6 ipywidgets-bokeh==1.0.2\n```\n\nUse this on the colab before installing AutoRAG\n\nWe updated colab tutorial! ",
          "created_at": "2024-10-30T05:29:02Z"
        }
      ]
    },
    {
      "issue_number": 878,
      "title": "[Feature Request] Add AutoRAG at prometheus",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-22T15:06:37Z",
      "updated_at": "2024-10-30T03:54:51Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/878/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/878",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/878",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:59.954379",
      "comments": []
    },
    {
      "issue_number": 898,
      "title": "[Feature Request] SGlang, the fast serving framework",
      "body": "**Is your feature request related to a problem? Please describe.**\n\n\nThis is kind of an 'FYI' for @vkehfdl1 from our previous brief coffee chat. You may simply close this issue if you are already fully aware of it. \n\n\n\n\n\n\n**Additional context**\n\n\nYou may refer the sources below:\n1. the official [GitHub repo]( https://github.com/sgl-project/sglang)\n2. reactions, reviews from [the Korean community](https://arca.live/b/alpaca/119757762?p=1)\n",
      "state": "open",
      "author": "skmanzg",
      "author_type": "User",
      "created_at": "2024-10-28T01:44:57Z",
      "updated_at": "2024-10-28T01:52:00Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/898/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/898",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/898",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:40:59.954401",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@skmanzg \nThanks:)\nIt is weird I cannot find sglang llama index integration.\nWe will implement it soon.\n\nThanks:)",
          "created_at": "2024-10-28T01:51:59Z"
        }
      ]
    },
    {
      "issue_number": 844,
      "title": "[Question] How can I retrieve image or files from AutoRAG? / 이미지나 파일 등을 어떻게 RAG에 저장할 수 있을까요?",
      "body": "Hi, I'm building a kind of modular RAG system. But my data includes some images, tables ( I used html to save tables), files.\nTo embed images and files, they are translated into texts by OCR. And original sources are saved as url.\n\nI found AutoRAG is the system I need, but I can't figure out how I should use those html texts and file links.\n\nCan you suggest any hint or idea how I can utilize image and files in modular RAG?\n\n\nAnother Question, I didn't understand the mechanism of PASS module yet. How do you judge which answer is the best answer?\n\nThanks for reading.",
      "state": "closed",
      "author": "TheClevers",
      "author_type": "User",
      "created_at": "2024-10-14T09:56:11Z",
      "updated_at": "2024-10-28T01:50:45Z",
      "closed_at": "2024-10-26T06:21:28Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/844/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/844",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/844",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:00.143234",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@TheClevers \n안녕하세요! 먼저 AutoRAG를 사용해주셔서 감사합니다.\n차트나 그래프 등과 같은 이미지를 포함한 multi-modal RAG를 구성하고 싶으신 것인가요? \n일단, 이미지 같은 경우에는 아직 AutoRAG에서 지원하지는 않지만 @bwook00 님께서 빠른 시일 안에 추가 feature로 추가할 예정입니다. \nhtml text와 같은 경우에는 LlamaIndex의 [가이드](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/)를",
          "created_at": "2024-10-14T13:46:25Z"
        },
        {
          "author": "bwook00",
          "body": "@TheClevers \n안녕하세요, 이미지를 파싱하는 기능이 AutoRAG 0.3.7 버전에 추가되었습니다 :)\n\n자세하게는, AutoRAG의 Llama Parse 모듈에서 Multi Modal Parsing 기능을 지원하게 되었습니다.\nMulti Modal을 이용한 Parsing이 가능해지면서 문서 속에 있는 이미지를 함께 파싱할 수 있게 되었습니다.\n자세한 정보는 [AutoRAG Llama Parse Documentation](https://docs.auto-rag.com/data_creation/parse/llama_parse",
          "created_at": "2024-10-26T06:21:28Z"
        },
        {
          "author": "TheClevers",
          "body": "알려주셔서 감사합니다!\n",
          "created_at": "2024-10-28T01:50:44Z"
        }
      ]
    },
    {
      "issue_number": 892,
      "title": "[Feature Request] Add custom_query_gen function",
      "body": "**Is your feature request related to a problem? Please describe.**\nDifferent LLM models result in different prompts, especially for local LLMs. Why not allow users to try their own queries?\n\n**Describe the solution you'd like**\nImplement custom_query_gen function receiving user's custom query generation prompt.\n",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2024-10-26T08:58:55Z",
      "updated_at": "2024-10-27T12:38:41Z",
      "closed_at": "2024-10-27T12:38:41Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/892/reactions",
        "total_count": 3,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "rjwharry"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/892",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/892",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:00.332975",
      "comments": [
        {
          "author": "rjwharry",
          "body": "If you think it is good, please assign to me",
          "created_at": "2024-10-26T09:10:42Z"
        },
        {
          "author": "bwook00",
          "body": " @rjwharry \nThanks for making this issue!\nI think this feature will make it easier to create synthetic queries.\n\nI just assigned it!",
          "created_at": "2024-10-26T12:28:18Z"
        }
      ]
    },
    {
      "issue_number": 849,
      "title": "[Feature Request] Config YAML schema validator",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\nYes, users can edit YAML freely, but they don't know if it's valid before running the program.\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\nUsing JSON Schema, create our YAML and JSON specific config files.\nYAML files should be convertible to JSON.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\nAjv (Ajv JSON schema validator)\n\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "open",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-15T23:39:05Z",
      "updated_at": "2024-10-27T10:40:28Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/849/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 1
      },
      "assignees": [
        "hongsw",
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/849",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/849",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:00.846737",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "I think I will be implement this on the library itself.",
          "created_at": "2024-10-27T10:40:27Z"
        }
      ]
    },
    {
      "issue_number": 893,
      "title": "[Feature Request] \bDelete Trial Path at Parse and Chunk",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-27T05:07:32Z",
      "updated_at": "2024-10-27T10:38:42Z",
      "closed_at": "2024-10-27T10:38:42Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/893/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/893",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/893",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.049370",
      "comments": []
    },
    {
      "issue_number": 801,
      "title": "[Feature Request] Implement kotaemon to the AutoRAG run_web",
      "body": "**Is your feature request related to a problem? Please describe.**\nkotaemon have a nice UI made with gradio for RAG\nWhy don't we implement it? \n\n**Describe the solution you'd like**\nLet's make kotaemon Runner or something like that.\n\nhttps://github.com/Cinnamon/kotaemon/",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-05T02:34:48Z",
      "updated_at": "2024-10-27T02:39:45Z",
      "closed_at": "2024-10-27T02:39:45Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/801/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/801",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/801",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.049390",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "This will resolve issue #493 and #635 as well",
          "created_at": "2024-10-05T02:35:08Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I already implement some of it as prototype at my repo.\nhttps://github.com/vkehfdl1/AutoRAG-web-kotaemon",
          "created_at": "2024-10-10T07:12:55Z"
        }
      ]
    },
    {
      "issue_number": 630,
      "title": "[BUG] Running evaluate on generated qa set gives error that retrieval_gt  doc_id not in corpus, but it is in the corpus",
      "body": "**Describe the bug**\r\nI generated a QA set ..\r\n\r\n```\r\nqa_df = pd.read_parquet(QA_FILE)\r\ndisplay(qa_df)\r\n```\r\n\r\n```\r\nqid | query | retrieval_gt | generation_gt\r\n-- | -- | -- | --\r\n601d49ed-df3a-4b4b-a889-c4daaf4fc589 | What key insights and lessons are available to improve school-based programming for enhanced school health and nutrition outcomes??? | [[**e18c6855-f1c8-40ff-b9c5-ea1a0ab2f639**, e9208f70-c4b2-4d18-b942-58b59cb3831f, 78fa60b3-fe56-4d74-9cc2-c1fedd5fd4f5]]\r\n```\r\nNote the retrieval_gt doc_id e18c6855-f1c8-40ff-b9c5-ea1a0ab2f639.\r\n\r\nChecking the corpus ...\r\n\r\n```\r\ncorpus_df = pd.read_parquet(CORPUS_FILE)\r\ndisplay(corpus_df[corpus_df['doc_id'] == 'e18c6855-f1c8-40ff-b9c5-ea1a0ab2f639'])\r\n```\r\n\r\n```\r\ndoc_id | contents | metadata\r\n-- | -- | --\r\n**e18c6855-f1c8-40ff-b9c5-ea1a0ab2f639** | and healthy practices. There is also an increase in (a) demonstration of new teaching techniques \\nand tools by the teachers (b) attentiveness and attendance of the students (c) discourse on \\nimproving quality of education. With regard to health and dietary practices, there is an increase \\nin the dietary diversity score in intervention schools from baseline (4.15) to midterm (5.49) due \\nto an increased demand from st udents for nutritious food as a result of awareness generation \\ninitiatives of the SFP. \\n19 However, an unintended negative impact was found as a result of the programme wherein, the \\nabsence of a proper waste disposal mechanism for the plastic wrappers of biscuits, resulted in \\nschools burning the wrappers in open instead of disposin g them responsibly. This poses a major \\nenvironmental concern and requires immediate correction . \\n20 Consi dering that there is a growing realization on the importance of education,\r\n```\r\n\r\nIt exists. However, when I run the evaluate with ...\r\n\r\n```\r\nPROJECT_DIR = f\"{DATA_DIR}/autorag\"\r\nAUTORAG_CONFIG='./autorag_openai.yaml'\r\n\r\nif not os.path.exists(PROJECT_DIR):\r\n    os.makedirs(PROJECT_DIR)\r\n\r\nevaluator = Evaluator(qa_data_path=QA_FILE, corpus_data_path=CORPUS_FILE,\r\n                      project_dir=PROJECT_DIR)\r\n\r\nevaluator.start_trial(AUTORAG_CONFIG)\r\n```\r\n\r\nI get an error saying this doc_id doesn't exist ...\r\n\r\n`ValueError: doc_id: e18c6855-f1c8-40ff-b9c5-ea1a0ab2f639 not found in corpus_data.`\r\n\r\nI have restarted the kernel, so that it's reading everything from the file system, as shown above, same error.\r\n\r\n**To Reproduce**\r\nSee above.\r\n\r\n**Expected behavior**\r\nShould work. :)\r\n\r\n**Full Error log**\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[10], [line 10](vscode-notebook-cell:?execution_count=10&line=10)\r\n      [5](vscode-notebook-cell:?execution_count=10&line=5)     os.makedirs(PROJECT_DIR)\r\n      [7](vscode-notebook-cell:?execution_count=10&line=7) evaluator = Evaluator(qa_data_path=QA_FILE, corpus_data_path=CORPUS_FILE,\r\n      [8](vscode-notebook-cell:?execution_count=10&line=8)                       project_dir=PROJECT_DIR)\r\n---> [10](vscode-notebook-cell:?execution_count=10&line=10) evaluator.start_trial(AUTORAG_CONFIG)\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/evaluator.py:98, in Evaluator.start_trial(self, yaml_path)\r\n     [96](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/evaluator.py:96)         previous_result = self.qa_data\r\n     [97](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/evaluator.py:97)     logger.info(f'Running node line {node_line_name}...')\r\n---> [98](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/evaluator.py:98)     previous_result = run_node_line(node_line, node_line_dir, previous_result)\r\n    [100](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/evaluator.py:100)     trial_summary_df = self._append_node_line_summary(node_line_name, node_line_dir, trial_summary_df)\r\n    [102](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/evaluator.py:102) trial_summary_df.to_csv(os.path.join(self.project_dir, trial_name, 'summary.csv'), index=False)\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/node_line.py:45, in run_node_line(nodes, node_line_dir, previous_result)\r\n     [43](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/node_line.py:43) summary_lst = []\r\n     [44](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/node_line.py:44) for node in nodes:\r\n---> [45](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/node_line.py:45)     previous_result = node.run(previous_result, node_line_dir)\r\n     [46](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/node_line.py:46)     node_summary_df = load_summary_file(os.path.join(node_line_dir, node.node_type, 'summary.csv'))\r\n     [47](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/node_line.py:47)     best_node_row = node_summary_df.loc[node_summary_df['is_best']]\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:57, in Node.run(self, previous_result, node_line_dir)\r\n     [55](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:55) logger.info(f'Running node {self.node_type}...')\r\n     [56](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:56) input_modules, input_params = self.get_param_combinations()\r\n---> [57](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:57) return self.run_node(modules=input_modules,\r\n     [58](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:58)                      module_params=input_params,\r\n     [59](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:59)                      previous_result=previous_result,\r\n     [60](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:60)                      node_line_dir=node_line_dir,\r\n     [61](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/schema/node.py:61)                      strategies=self.strategy)\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:121, in run_retrieval_node(modules, module_params, previous_result, node_line_dir, strategies)\r\n    [118](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:118) if any([module.__name__ in semantic_module_names for module in modules]):\r\n    [119](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:119)     semantic_modules, semantic_module_params = zip(*filter(lambda x: x[0].__name__ in semantic_module_names,\r\n    [120](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:120)                                                            zip(modules, module_params)))\r\n--> [121](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:121)     semantic_results, semantic_times = run(semantic_modules, semantic_module_params)\r\n    [122](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:122)     semantic_summary_df = save_and_summary(semantic_modules, semantic_module_params,\r\n    [123](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:123)                                            semantic_results, semantic_times, filename_first)\r\n    [124](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:124)     semantic_selected_result, semantic_selected_filename = find_best(semantic_results, semantic_times,\r\n    [125](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:125)                                                                      semantic_summary_df['filename'].tolist())\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:62, in run_retrieval_node.<locals>.run(input_modules, input_module_params)\r\n     [53](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:53) def run(input_modules, input_module_params) -> Tuple[List[pd.DataFrame], List]:\r\n     [54](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:54)     \"\"\"\r\n     [55](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:55)         Run input modules and parameters.\r\n     [56](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:56) \r\n   (...)\r\n     [60](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:60)         Second, it returns list of execution times.\r\n     [61](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:61)     \"\"\"\r\n---> [62](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:62)     result, execution_times = zip(*map(lambda task: measure_speed(\r\n     [63](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:63)         task[0], project_dir=project_dir, previous_result=previous_result, **task[1]),\r\n     [64](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:64)                                        zip(input_modules, input_module_params)))\r\n     [65](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:65)     average_times = list(map(lambda x: x / len(result[0]), execution_times))\r\n     [67](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:67)     # run metrics before filtering\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:62, in run_retrieval_node.<locals>.run.<locals>.<lambda>(task)\r\n     [53](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:53) def run(input_modules, input_module_params) -> Tuple[List[pd.DataFrame], List]:\r\n     [54](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:54)     \"\"\"\r\n     [55](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:55)         Run input modules and parameters.\r\n     [56](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:56) \r\n   (...)\r\n     [60](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:60)         Second, it returns list of execution times.\r\n     [61](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:61)     \"\"\"\r\n---> [62](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:62)     result, execution_times = zip(*map(lambda task: measure_speed(\r\n     [63](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:63)         task[0], project_dir=project_dir, previous_result=previous_result, **task[1]),\r\n     [64](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:64)                                        zip(input_modules, input_module_params)))\r\n     [65](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:65)     average_times = list(map(lambda x: x / len(result[0]), execution_times))\r\n     [67](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/run.py:67)     # run metrics before filtering\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:14, in measure_speed(func, *args, **kwargs)\r\n     [10](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:10) \"\"\"\r\n     [11](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:11) Method for measuring execution speed of the function.\r\n     [12](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:12) \"\"\"\r\n     [13](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:13) start_time = time.time()\r\n---> [14](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:14) result = func(*args, **kwargs)\r\n     [15](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:15) end_time = time.time()\r\n     [16](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/strategy.py:16) return result, end_time - start_time\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:56, in result_to_dataframe.<locals>.decorator_result_to_dataframe.<locals>.wrapper(*args, **kwargs)\r\n     [54](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:54) @functools.wraps(func)\r\n     [55](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:55) def wrapper(*args, **kwargs) -> pd.DataFrame:\r\n---> [56](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:56)     results = func(*args, **kwargs)\r\n     [57](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:57)     if len(column_names) == 1:\r\n     [58](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:58)         df_input = {column_names[0]: results}\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/base.py:97, in retrieval_node.<locals>.wrapper(project_dir, previous_result, **kwargs)\r\n     [95](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/base.py:95) # fetch data from corpus_data\r\n     [96](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/base.py:96) corpus_data = pd.read_parquet(os.path.join(data_dir, \"corpus.parquet\"), engine='pyarrow')\r\n---> [97](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/base.py:97) contents = fetch_contents(corpus_data, ids)\r\n     [99](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/base.py:99) return contents, ids, scores\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:30, in fetch_contents(corpus_data, ids, column_name)\r\n     [27](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:27) def fetch_contents_pure(ids: List[str], corpus_data: pd.DataFrame, column_name: str):\r\n     [28](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:28)     return list(map(lambda x: fetch_one_content(corpus_data, x, column_name), ids))\r\n---> [30](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:30) result = flatten_apply(fetch_contents_pure, ids, corpus_data=corpus_data, column_name=column_name)\r\n     [31](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:31) return result\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:335, in flatten_apply(func, nested_list, **kwargs)\r\n    [333](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:333) df = pd.DataFrame({'col1': nested_list})\r\n    [334](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:334) df = df.explode('col1')\r\n--> [335](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:335) df['result'] = func(df['col1'].tolist(), **kwargs)\r\n    [336](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:336) return df.groupby(level=0, sort=False)['result'].apply(list).tolist()\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:28, in fetch_contents.<locals>.fetch_contents_pure(ids, corpus_data, column_name)\r\n     [27](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:27) def fetch_contents_pure(ids: List[str], corpus_data: pd.DataFrame, column_name: str):\r\n---> [28](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:28)     return list(map(lambda x: fetch_one_content(corpus_data, x, column_name), ids))\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:28, in fetch_contents.<locals>.fetch_contents_pure.<locals>.<lambda>(x)\r\n     [27](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:27) def fetch_contents_pure(ids: List[str], corpus_data: pd.DataFrame, column_name: str):\r\n---> [28](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:28)     return list(map(lambda x: fetch_one_content(corpus_data, x, column_name), ids))\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:41, in fetch_one_content(corpus_data, id_, column_name)\r\n     [39](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:39) fetch_result = corpus_data[corpus_data['doc_id'] == id_]\r\n     [40](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:40) if fetch_result.empty:\r\n---> [41](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:41)     raise ValueError(f\"doc_id: {id_} not found in corpus_data.\")\r\n     [42](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:42) else:\r\n     [43](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/utils/util.py:43)     return fetch_result[column_name].iloc[0]\r\n\r\nValueError: doc_id: e18c6855-f1c8-40ff-b9c5-ea1a0ab2f639 not found in corpus_data.\r\n\r\n```\r\n**Code that bug is happened**\r\nSee above\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: MacOS running miniconda\r\n - Python version Python 3.11.4\r\n - AutoRAG                                  0.2.14\r\n\r\n**Additional context**\r\nThanks!\r\n",
      "state": "closed",
      "author": "dividor",
      "author_type": "User",
      "created_at": "2024-08-21T05:39:36Z",
      "updated_at": "2024-10-26T03:39:20Z",
      "closed_at": "2024-08-21T10:17:50Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/630/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/630",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/630",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.284810",
      "comments": [
        {
          "author": "dividor",
          "body": "Ahh. It seems if the evaluate doesn't complete, I needed to remove the project directory *and* restart the kernel. Seems to be working now.",
          "created_at": "2024-08-21T05:50:09Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@dividor \r\nThat's right. You **have to** make the new folder when the data has changed.\r\n\r\nClose this issue since this is not a bug. ",
          "created_at": "2024-08-21T10:17:50Z"
        },
        {
          "author": "tha23rd",
          "body": "Hi - happy to create a new issue, but I'm running into the same issue. I've deleted the autorag folder and have tried regenerating things multiple times, and I still get the:\n```\nValueError: doc_id: 1240b6d7-c82b-4367-bbff-753fa6e50cbc not found in corpus_data.\n```\n\nI have triple checked that this d",
          "created_at": "2024-10-25T11:47:38Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Hello! @tha23rd \nI think you delete whole project directory, and double checked the doc_id is existed in qa and corpus df. \nHmm.... It is weird.\nCan you send me a full bug trace to me? Love to help you.\n\n(Plus, be sure to delete \"all project directory\". If you run project directory without setting i",
          "created_at": "2024-10-25T15:02:29Z"
        },
        {
          "author": "tha23rd",
          "body": "> Hello! [@tha23rd](https://github.com/tha23rd) I think you delete whole project directory, and double checked the doc_id is existed in qa and corpus df. Hmm.... It is weird. Can you send me a full bug trace to me? Love to help you.\n> \n> (Plus, be sure to delete \"all project directory\". If you run p",
          "created_at": "2024-10-25T15:42:59Z"
        }
      ]
    },
    {
      "issue_number": 889,
      "title": "[Feature Request] Add More Vector DB",
      "body": "**Is your feature request related to a problem? Please describe.**\nWe can support more Vector DBs like Weaviate, Pinecone, FAISS, Vectara, pgvector, mongo search and more. \n\n**Describe the solution you'd like**\nWe want to use Langchain implementation, but it has few limitations so we have to implement it on our own. \n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-25T13:32:06Z",
      "updated_at": "2024-10-25T13:32:06Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/889/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/889",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/889",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.493618",
      "comments": []
    },
    {
      "issue_number": 885,
      "title": "[README] Add comparison table with other services",
      "body": "- Langchain + Langsmith (Framework)\n- LlamaIndex (Framework)\n- RAGAS (Evaluation Framework)\n- Langfuse (observe)\n- Dify (No-code LLM app builder)",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-25T09:41:43Z",
      "updated_at": "2024-10-25T09:41:43Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/885/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/885",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/885",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.493638",
      "comments": []
    },
    {
      "issue_number": 861,
      "title": "[BUG] Error at docker push",
      "body": "**Describe the bug**\nDocker CD failed\nBecause.... hahahaha\n\nERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n\n**To Reproduce**\nPush to main with VERSION changed.\n\n**Desktop (please complete the following information):**\nOn Github Action\n\n\n**Additional context**\nDo we have to pay for github action?\nOr make it smaller... Ah I have a headache for this damn",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-18T06:45:01Z",
      "updated_at": "2024-10-24T06:15:02Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/861/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/861",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/861",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.493644",
      "comments": [
        {
          "author": "hongsw",
          "body": "https://github.com/Marker-Inc-Korea/AutoRAG/actions/runs/11387904505/job/31683629917#step:7:1612",
          "created_at": "2024-10-18T07:56:51Z"
        },
        {
          "author": "hongsw",
          "body": "I recommend using the dynamic pip module installation approach https://github.com/Marker-Inc-Korea/AutoRAG/issues/824 to reduce docker image size.",
          "created_at": "2024-10-18T08:05:15Z"
        },
        {
          "author": "vkehfdl1",
          "body": "https://flavono123.oopy.io/posts/resolve-no-space-left-on-device-in-github-action-ubuntu-runner",
          "created_at": "2024-10-24T06:15:00Z"
        }
      ]
    },
    {
      "issue_number": 830,
      "title": "[Feature Request] Connect different Vector Stores as vectordb.",
      "body": "**Is your feature request related to a problem? Please describe.**\n\bConnect vector databases via llamaindex. \n\n**Describe the solution you'd like**\nMaybe LlamaIndex is okay? We need integrated interface for this...\n\n**Describe alternatives you've considered**\nI think we need to manage #824 first for managing complex dependencies.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-11T13:08:21Z",
      "updated_at": "2024-10-23T15:59:21Z",
      "closed_at": "2024-10-23T15:59:21Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/830/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/830",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/830",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.682048",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "But how connect existing vector DB to AutoRAG?\nIt doesn't have corpus.parquet and how to make it???? corpus.parquet is crucial to use it.\nConsider corpus.parquet as one of the Vector Database is viable? Hmmmm",
          "created_at": "2024-10-11T13:09:38Z"
        }
      ]
    },
    {
      "issue_number": 869,
      "title": "[Feature Request] Add Milvus Vector DB as external vector database",
      "body": "Let’s add Milvus when we have the proper format for adding new vector DBs.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-20T16:22:07Z",
      "updated_at": "2024-10-23T08:01:51Z",
      "closed_at": "2024-10-23T08:01:49Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/869/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/869",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/869",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:01.865132",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #879",
          "created_at": "2024-10-23T08:01:49Z"
        }
      ]
    },
    {
      "issue_number": 858,
      "title": "[Feature Request] Available Modules List Show content directly on the Docs page",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-18T06:06:53Z",
      "updated_at": "2024-10-20T16:26:36Z",
      "closed_at": "2024-10-20T16:26:35Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/858/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/858",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/858",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.056069",
      "comments": []
    },
    {
      "issue_number": 867,
      "title": "[Feature Request] Add Omni OCR Parse Module",
      "body": "https://getomni.ai/ocr-demo\n\nopensource OCR",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-20T06:29:44Z",
      "updated_at": "2024-10-20T06:29:51Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/867/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/867",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/867",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.056089",
      "comments": []
    },
    {
      "issue_number": 863,
      "title": "[BUG] Prev Next Augmeneter is impossible to validate",
      "body": "**Describe the bug**\nWhen you use prev-next augmenter, it is impossible to pass validation stage.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Make YAML file with prev-next augmenter\n2. Run validation (the qa must be larger than 5)\n\n**Expected behavior**\nThis is happening because we cut some part of corpus.parquet. Because for validation we don’t need full corpus. However, at prev_next_augmenter, we have to retrieve contents from the prev id or next id at the corpus. \nThen, at the corpus, the corresponding id is not existed. So the validation failed. \n\n**Code that bug is happened**\nAt the prev next augmenter\n\n**Desktop (please complete the following information):**\nAutoRAG 0.3.6\npython 3.10\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-19T09:32:30Z",
      "updated_at": "2024-10-19T09:32:30Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/863/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/863",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/863",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.056094",
      "comments": []
    },
    {
      "issue_number": 857,
      "title": "[Feature Request]Deploy with AWS ECS",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-17T10:11:26Z",
      "updated_at": "2024-10-18T13:59:17Z",
      "closed_at": "2024-10-18T13:59:17Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/857/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/857",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/857",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.056101",
      "comments": [
        {
          "author": "hongsw",
          "body": "https://github.com/hongsw/autorag-ecs",
          "created_at": "2024-10-18T13:58:54Z"
        }
      ]
    },
    {
      "issue_number": 860,
      "title": "[Feature Request] Add Roadmap link at README.md",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-18T06:42:42Z",
      "updated_at": "2024-10-18T08:05:52Z",
      "closed_at": "2024-10-18T08:05:52Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/860/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/860",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/860",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.321011",
      "comments": []
    },
    {
      "issue_number": 609,
      "title": "ERROR with Event Loop",
      "body": "Hello. I wrote a small RAG system for ma project and I decided to push it on localhost\r\n\r\n```\r\nhost = \"localhost\"\r\nrunner = Runner.from_yaml('RAG_system/data_original/test_1/pipline.yaml', project_dir = \"RAG_system/data_original/test_1\")\r\nrunner.run_api_server(host)\r\n\r\n```\r\nMy code for posting \r\n\r\n```\r\nimport requests \r\nurl = \"http://localhost:8000/run\"\r\ndata = {\r\n    \"query\": \"query\",\r\n    \"result_column\": \"Answer\"\r\n}\r\nimport json```\r\npayload = json.dumps(data)\r\nheaders = {\r\n    'accept': \"application/json\",\r\n    'Content-Type': \"application/json\"\r\n    }\r\nresponse = requests.request(\"POST\", url, data=payload, headers = headers)\r\nprint(response.text)\r\n```\r\n\r\nAnd every time I get this error  \r\n### RuntimeError: This event loop is already running\r\nHow do I solve it?",
      "state": "closed",
      "author": "warjohn",
      "author_type": "User",
      "created_at": "2024-08-12T08:23:11Z",
      "updated_at": "2024-10-18T07:36:00Z",
      "closed_at": "2024-08-13T05:13:30Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/609/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/609",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/609",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.321032",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@warjohn Hi, did you run this code in jupyter?\r\n\r\nTry this\r\n\r\n```python\r\nimport nest_asyncio\r\nnest_asyncio.apply()\r\n```",
          "created_at": "2024-08-12T11:38:26Z"
        },
        {
          "author": "warjohn",
          "body": "it doen't work on linux, idk",
          "created_at": "2024-08-12T13:53:26Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@warjohn \r\nI am investigating this problem. \r\nThanks for reporting the issue. ",
          "created_at": "2024-08-13T02:12:43Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@warjohn \r\nI made PR to resolve this error. #611 \r\nThanks for reporting a but for me again.\r\nThis fix will be update on AutoRAG v0.2.14",
          "created_at": "2024-08-13T02:41:19Z"
        },
        {
          "author": "hongsw",
          "body": "> [@warjohn](https://github.com/warjohn) Hi, did you run this code in jupyter?\n> \n> Try this\n> \n> import nest_asyncio\n> nest_asyncio.apply()\n\nThanks ![Image](https://github.com/user-attachments/assets/4147137f-01ec-4c8a-9290-4845bb4f40ac)\n",
          "created_at": "2024-10-18T07:35:59Z"
        }
      ]
    },
    {
      "issue_number": 855,
      "title": "[Feature Request] Add AWS Bedrock generator",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-17T05:46:14Z",
      "updated_at": "2024-10-17T15:15:05Z",
      "closed_at": "2024-10-17T15:15:05Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/855/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/855",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/855",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.523119",
      "comments": []
    },
    {
      "issue_number": 765,
      "title": "[Feature Request] Add external chunk information at the front of the chunk (Anthropic method)",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nAt the antropic, they extract core information for lexical search from the each chunk and put into the front side of the chunk.\r\n\r\n**Describe the solution you'd like**\r\nI think it is super expensive if we generate new information at the each chunk while running the `Chunker` \r\nIt is better to use it on the only ‘one chunk’ or the `Corpus` instance.\r\nOr compare between with metadata and without one.\r\n\r\n**Describe alternatives you've considered**\r\nIs there any method to create metadata from the raw document?\r\n\r\n**Additional context**\r\nhttps://www.linkedin.com/feed/update/urn:li:activity:7245896802107826176/\r\nhttps://lnkd.in/gaHby5ZT\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-29T05:00:22Z",
      "updated_at": "2024-10-16T17:31:41Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/765/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/765",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/765",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.523140",
      "comments": [
        {
          "author": "Aryazaky",
          "body": "I think this will be useful if a new chunk module for contextual chunking is created. Here is the [supporting blog post](https://www.anthropic.com/news/contextual-retrieval)",
          "created_at": "2024-10-16T17:31:40Z"
        }
      ]
    },
    {
      "issue_number": 852,
      "title": "[Feature Request] New Pinecone Reranker module",
      "body": "https://docs.pinecone.io/guides/inference/rerank",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-16T16:10:54Z",
      "updated_at": "2024-10-16T16:10:54Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/852/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/852",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/852",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.710642",
      "comments": []
    },
    {
      "issue_number": 623,
      "title": "[Feature Request] Add new logo",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nNow, the AutoRAG logo image generated by Dall-e 2 of OpenAI. It has no meaning, lame, not good, not catchy at all. No sense of design. It is bad. \r\n\r\n**Describe the solution you'd like**\r\nSome designer or who can write great midjourney prompt can help us. \r\n\r\n**Additional context**\r\nSomeday we can make sticker of AutoRAG? For your macbook 👍 \r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-20T06:08:24Z",
      "updated_at": "2024-10-15T08:27:39Z",
      "closed_at": "2024-10-14T14:06:58Z",
      "labels": [
        "enhancement",
        "good first issue",
        "help wanted"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/623/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/623",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/623",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.710665",
      "comments": [
        {
          "author": "skmanzg",
          "body": "@vkehfdl1 \r\n_Am I the only one who can not find the current AutoRAG logo? I cannot find it anywhere lol._ \r\n\r\nI might try this via FLUX but, could you specify the logo design you have imagined? You may require specifically like... the logo should be ~~~   with reference images attached if possible. ",
          "created_at": "2024-08-22T23:19:18Z"
        },
        {
          "author": "vkehfdl1",
          "body": "![image](https://github.com/user-attachments/assets/db76bc89-0ba8-416d-89b3-fda475795613)\r\n\r\n@skmanzg \r\nThis is current AutoRAG logo. \r\nSince it is not good, we don’t use it very often.",
          "created_at": "2024-08-23T06:31:59Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@skmanzg Plus thank you for your kind reply for this issue (+ instreset as well)\r\n\r\nI think the logo should be simple and show the name of the product is “AutoRAG” \r\nActually I like logo of Tensorflow, intuitive just have T & F.\r\nIt doesn’t need to be fancy. \r\nI prefer OBS or pytorch ones. \r\nBut sin",
          "created_at": "2024-08-23T06:35:40Z"
        },
        {
          "author": "vkehfdl1",
          "body": "We find our logo LOL :) \r\n![image](https://github.com/user-attachments/assets/0dcc7952-4a46-415d-8659-7313c08ced7d)\r\n",
          "created_at": "2024-09-23T07:56:49Z"
        },
        {
          "author": "skmanzg",
          "body": "@vkehfdl1 you remind me of this thing when I was working on my things. These are neither professional results nor creations from FLUX but I have tried. You can use or refer to those results if they are ok.\r\n![스크린샷 2024-09-25 103310](https://github.com/user-attachments/assets/7a3b9d39-055b-44d5-83e5-",
          "created_at": "2024-09-25T01:48:18Z"
        }
      ]
    },
    {
      "issue_number": 843,
      "title": "[Feature Request] Add telemetry feature",
      "body": "**Is your feature request related to a problem? Please describe.**\nTo collect user's action anonymously.\n\n**Describe the solution you'd like**\nOpentelemetry?\nLet's search what is a good service with non-legal restriction and non-privacy matters.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-14T04:51:38Z",
      "updated_at": "2024-10-14T13:39:08Z",
      "closed_at": "2024-10-14T13:39:06Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/843/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/843",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/843",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:02.928781",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "We will not implement this feature. ",
          "created_at": "2024-10-14T13:39:06Z"
        }
      ]
    },
    {
      "issue_number": 841,
      "title": "[BUG] Import error when don’t install kiwipiepy",
      "body": "**Describe the bug**\n\nAt autorag/data/__init__.py\n\n```python\ntry:\n\tfrom kiwipiepy import Kiwi\n\n\tdef split_by_sentence_kiwi() -> Callable[[str], List[str]]:\n\t\tkiwi = Kiwi()\n\n\t\tdef split(text: str) -> List[str]:\n\t\t\tkiwi_result = kiwi.split_into_sents(text)\n\t\t\tsentences = list(map(lambda x: x.text, kiwi_result))\n\n\t\t\treturn sentences\n\n\t\treturn split\n\n\tsentence_splitter_modules = {\"kiwi\": LazyInit(split_by_sentence_kiwi)}\n\nexcept ImportError:\n\tlogger.info(\n\t\t\"You did not install korean version of AutoRAG.\"\n\t\t\"To use korean version, run pip install 'AutoRAG[ko]'\"\n\t)\n```\n\nIn here, if we do not install ko version of AutoRAG, we have import error at here.\n\ndata/chunk/base.py\n\n```python\nfrom autorag.data import chunk_modules, sentence_splitter_modules\n```\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-13T11:50:00Z",
      "updated_at": "2024-10-14T12:39:26Z",
      "closed_at": "2024-10-14T12:39:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/841/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/841",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/841",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:03.096495",
      "comments": []
    },
    {
      "issue_number": 822,
      "title": "[Bug] Restrict 'threshold' config to numbers only for module_type",
      "body": "> Restrict 'threshold' config to numbers only for module_type \n\n _Originally posted by @hongsw in [#821](https://github.com/Marker-Inc-Korea/AutoRAG/issues/821#issuecomment-2403716359)_\n\n\n## Is your feature request related to a problem? Please describe.\nCurrently, the `threshold` configuration for `module_type` accepts any type of value, which can lead to validation errors and potential runtime issues. Specifically, we've observed instances where date strings (e.g., \"2015-01-01T00:00:00.000Z\") are being used instead of numerical values. This inconsistency can cause confusion for users and may result in unexpected behavior in our application.\n\nFor example, we've seen the following error:\n\n```\nTYPE must be number\n\n  216 |             {\n  217 |               \"module_type\": \"recency_filter\",\n> 218 |               \"threshold\": \"2015-01-01T00:00:00.000Z\"\n      |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^ 👈🏽  type must be number\n```\n\nThis indicates that a string is being used where a number is expected.\n\n## Describe the solution you'd like\nWe propose to modify the JSON schema to explicitly restrict the `threshold` configuration for `module_type` to accept only numerical values. This change should:\n\n1. Update the JSON schema to specify that `threshold` must be a number.\n2. Provide clear error messages when non-numerical values are used for `threshold`.\n3. Update any relevant documentation to clearly state that `threshold` should be a number.\n\n## Describe alternatives you've considered\n1. Allow both numbers and date strings, and convert date strings to numerical timestamps internally:\n   - Pros: More flexible for users who prefer working with dates\n   - Cons: Increases complexity and may lead to inconsistent usage\n\n2. Use a separate configuration for date-based thresholds:\n   - Pros: Clear separation between numerical and date-based thresholds\n   - Cons: Requires more configuration options, potentially complicating the schema\n\n## Additional context\nThis change will improve the consistency of our configuration schema and reduce errors caused by incorrect data types. It's part of our ongoing effort to make the configuration process more robust and user-friendly.\n\nExample of the desired schema change:\n\n```json\n{\n  \"properties\": {\n    \"module_type\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"threshold\": {\n          \"type\": \"number\"\n        }\n      }\n    }\n  }\n}\n```\n\nThis change will ensure that only numerical values are accepted for the `threshold` configuration.",
      "state": "closed",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-10T01:28:13Z",
      "updated_at": "2024-10-13T07:11:51Z",
      "closed_at": "2024-10-13T07:11:50Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/822/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/822",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/822",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:03.096511",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@hongsw \nBut since it is \"recency filter\", that filtered old passages, the `threshold` parameter must get the exact datetime. ",
          "created_at": "2024-10-10T02:58:16Z"
        }
      ]
    },
    {
      "issue_number": 833,
      "title": "[BUG] Sphinx build CD failed",
      "body": "**Describe the bug**\nSphinx build for docs deploy is not working.\n\n**To Reproduce**\nSee main github actions history.\n\nWe have to checkout python for it.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-12T05:33:18Z",
      "updated_at": "2024-10-13T05:29:12Z",
      "closed_at": "2024-10-13T05:29:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/833/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/833",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/833",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:03.302398",
      "comments": []
    },
    {
      "issue_number": 770,
      "title": "[Feature Request] Fixed port at dashboard",
      "body": "**Is your feature request related to a problem? Please describe.**\r\npanel Dashboard port is changing at every execution. We have to fix to one port.\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-30T06:19:10Z",
      "updated_at": "2024-10-12T05:49:28Z",
      "closed_at": "2024-10-12T05:49:28Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/770/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/770",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/770",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.072836",
      "comments": []
    },
    {
      "issue_number": 829,
      "title": "[Feature Request] Connect S3 as file loader",
      "body": "**Is your feature request related to a problem? Please describe.**\nI want to connect Amazon S3 as the file loader to parse to the VectorDB.\n\n**Describe the solution you'd like**\nUse Langchain or LlamaIndex (or something better) one to connect many document source to parsing. \n\n**Describe alternatives you've considered**\nWe can use other library, like liteLLM for getting documents. \n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-11T12:50:11Z",
      "updated_at": "2024-10-11T15:43:29Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/829/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/829",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/829",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.072859",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "As alternative, we can build example jupyter notebook.",
          "created_at": "2024-10-11T15:12:39Z"
        },
        {
          "author": "vkehfdl1",
          "body": "To support AWS well, I think it is better to use fsspec. Unified interface for loading files! \nWe are now only support pdf, so loading pdf files from all kinds of file system.\n\nBelow is the full fsspec supported protocol.\n\nIt contains dropbox, google drive, S3, even jupyter & github!\n\n```\n['abfs',\n ",
          "created_at": "2024-10-11T15:43:27Z"
        }
      ]
    },
    {
      "issue_number": 832,
      "title": "[Feature Request] Add new nlm-ingestor pdf parsing method",
      "body": "**Is your feature request related to a problem? Please describe.**\nI found this \n\n\n**Additional context**\nhttps://github.com/nlmatics/nlm-ingestor\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-11T15:32:02Z",
      "updated_at": "2024-10-11T15:32:02Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/832/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/832",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/832",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.322132",
      "comments": []
    },
    {
      "issue_number": 775,
      "title": "[Feature Request] New FlashRank Reranker Module",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nNew reranker module!!! Yeah~~~\r\n\r\n**Describe the solution you'd like**\r\nGo to FlashRank Github page\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-01T01:16:08Z",
      "updated_at": "2024-10-11T15:11:15Z",
      "closed_at": "2024-10-11T15:11:14Z",
      "labels": [
        "enhancement",
        "good first issue",
        "New Module"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/775/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/775",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/775",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.322156",
      "comments": []
    },
    {
      "issue_number": 831,
      "title": "[BUG] `vllm._C` ModuleNotFoundError",
      "body": "**Describe the bug**\n\n**To Reproduce**\nuse our `simple_local.yaml` i got this error.\n(And I used sample qa & corpus file in our resources folder)\n\nevaluator = Evaluator(qa_data_path=qa_parquet, corpus_data_path=corpus_parquet)\nevaluator.start_trial(example_yaml)\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Full Error log**\n```\nINFO     [node.py:55] >> Running node             node.py:55\n                             generator...                                       \n                    INFO     [base.py:19] >> Initialize generator     base.py:19\n                             node - Vllm                                        \nWARNING 10-11 22:51:03 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\nINFO 10-11 22:51:03 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTraceback (most recent call last):\n\n\n...\n\n\n    raise RuntimeError(\"Failed to infer device type\")\nRuntimeError: Failed to infer device type\n```\n\n",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-11T13:57:13Z",
      "updated_at": "2024-10-11T14:49:53Z",
      "closed_at": "2024-10-11T14:49:53Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/831/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/831",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/831",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.322333",
      "comments": []
    },
    {
      "issue_number": 819,
      "title": "[Feature Request] AutoRAG only-API-call version",
      "body": "**Is your feature request related to a problem? Please describe.**\nTry to make a lightweight Docker container that only contains API-connected modules. (Without local reranker models, longllmlingua, etc.)\n\n**Describe the solution you'd like**\nFirst, use other requirements file at pyproject.toml. \nAnd build requirements.txt separately.\nMaybe we will have torch issue or others. (import issue at __init__.py especially)\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-09T13:28:47Z",
      "updated_at": "2024-10-11T13:05:19Z",
      "closed_at": "2024-10-11T13:05:19Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/819/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/819",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/819",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.322344",
      "comments": []
    },
    {
      "issue_number": 820,
      "title": "[Feature Request] Always run validation at the start_trial",
      "body": "**Is your feature request related to a problem? Please describe.**\nRun a whole trial can be time consuming and cost-intensive.\nSmall error can lead big irritation. \nSo, why don’t we run validation at every start_trial??\nIt will add a minute, but it will help lots of devs.\n\n**Many people will not read docs super carefully before execution**\n\n**Describe the solution you'd like**\nUse `Validator` at the start_trial.\nGet the option skipping validation, but set it default.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-09T15:46:37Z",
      "updated_at": "2024-10-11T03:35:06Z",
      "closed_at": "2024-10-11T03:35:06Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/820/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/820",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/820",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.322351",
      "comments": []
    },
    {
      "issue_number": 825,
      "title": "[Feature Request] Web UI about start_trial",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-10T07:58:54Z",
      "updated_at": "2024-10-10T07:58:54Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/825/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/825",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/825",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.322359",
      "comments": []
    },
    {
      "issue_number": 821,
      "title": "[Feature Request] Migrate to Node.js for YAML Config Validator",
      "body": "\n## Is your feature request related to a problem? Please describe.\nThe current Python implementation of our YAML Config Validator has limitations that significantly impact its effectiveness and user experience:\n\n1. It can only report one error at a time, requiring multiple runs to identify all issues in a configuration file.\n2. The error messages lack detailed context and suggestions for fixing issues.\n3. The validator's performance is suboptimal for large configuration files.\n4. The Python implementation has limited support for complex JSON Schema features.\n\nThese limitations result in a time-consuming and frustrating process for users trying to validate and correct their YAML configurations.\n\n## Describe the solution you'd like\nWe propose migrating the YAML Config Validator to Node.js, utilizing the following libraries:\n\n- Ajv for JSON Schema validation\n- js-yaml for YAML parsing\n- better-ajv-errors for detailed error reporting\n\nThis migration would provide the following improvements:\n\n1. Multiple error reporting in a single run\n2. More comprehensive and context-rich error messages\n3. Better performance for large configuration files\n4. More extensive JSON Schema support for complex validation rules\n5. Improved integration with web-based tools and services\n\n## Describe alternatives you've considered\nWe considered the following alternatives:\n\n1. Enhancing the current Python implementation:\n   - Pros: Keeps existing codebase, familiar to current maintainers\n   - Cons: Limited libraries for advanced JSON Schema validation, may not fully address all current limitations\n\n2. Using a different language (e.g., Go, Rust):\n   - Pros: Potential performance improvements\n   - Cons: Smaller ecosystem for JSON Schema validation, steeper learning curve for the team\n\n3. Creating a custom validation engine from scratch:\n   - Pros: Full control over features and implementation\n   - Cons: Time-consuming, risk of introducing new bugs, maintenance burden\n\n## Additional context\nThe Node.js ecosystem has mature and well-maintained libraries for JSON Schema validation and YAML parsing. This migration aligns with our goal of providing a more robust and user-friendly tool for configuration validation.\n\nExample of the improved error output with the Node.js implementation:\n\n[full.yaml](https://github.com/Marker-Inc-Korea/AutoRAG/blob/main/sample_config/rag/full.yaml)\n```sh\nValidation errors:\nTYPE must be number\n\n  216 |             {\n  217 |               \"module_type\": \"recency_filter\",\n> 218 |               \"threshold\": \"2015-01-01T00:00:00.000Z\"\n      |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^ 👈🏽  type must be number\n```\n\nThis migration will significantly improve the user experience and effectiveness of our YAML Config Validator.",
      "state": "open",
      "author": "hongsw",
      "author_type": "User",
      "created_at": "2024-10-10T01:22:54Z",
      "updated_at": "2024-10-10T01:31:40Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/821/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/821",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/821",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.322366",
      "comments": [
        {
          "author": "hongsw",
          "body": "https://github.com/hongsw/AutoRAG-YAML-Config-Validator\n\n",
          "created_at": "2024-10-10T01:24:33Z"
        },
        {
          "author": "hongsw",
          "body": "@vkehfdl1 we need to change the [full.yaml](https://github.com/Marker-Inc-Korea/AutoRAG/blob/main/sample_config/rag/full.yaml) file\n```yaml\n- module_type: recency_filter\n            threshold: 2015-01-01\n```\n\n",
          "created_at": "2024-10-10T01:26:56Z"
        },
        {
          "author": "hongsw",
          "body": "Restrict 'threshold' config to numbers only for module_type",
          "created_at": "2024-10-10T01:27:53Z"
        },
        {
          "author": "hongsw",
          "body": "https://json-schema-everywhere.github.io/yaml",
          "created_at": "2024-10-10T01:31:39Z"
        }
      ]
    },
    {
      "issue_number": 817,
      "title": "[Feature Request] Add OpenAI compatible Server",
      "body": "**Is your feature request related to a problem? Please describe.**\nAs new runner, add OpenAI compatible server.\n\n**Describe the solution you'd like**\nIt will be useful to use it if we have OpenAI compatible server! \n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-09T11:08:41Z",
      "updated_at": "2024-10-09T11:57:52Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/817/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/817",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/817",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.545172",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "I cannot a API spec that can send retrieved passages as well as LLM generated answers.\nI don’t know how it will be effective until there is not standard API that retrieves passages.",
          "created_at": "2024-10-09T11:57:52Z"
        }
      ]
    },
    {
      "issue_number": 789,
      "title": "[Feature Request] New MixedbreadAI_Rerank module",
      "body": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/postprocessor/llama-index-postprocessor-mixedbreadai-rerank",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:20:59Z",
      "updated_at": "2024-10-09T11:18:41Z",
      "closed_at": "2024-10-09T11:18:40Z",
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/789/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/789",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/789",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.807069",
      "comments": []
    },
    {
      "issue_number": 815,
      "title": "[BUG] Chroma distance score bug when do hybrid retrieval.",
      "body": "**Describe the bug**\n```\nid_results: GetResult = collection.get(ids, include=[\"embeddings\"])\n\ttemp_collection = temp_client.create_collection(\n\t\tname=\"temp\", metadata={\"hnsw:space\": \"cosine\"}\n\t)\n\ttemp_collection.add(ids=id_results[\"ids\"], embeddings=id_results[\"embeddings\"])\n\n\tquery_result: QueryResult = temp_collection.query(\n\t\tquery_embeddings=query_embeddings, n_results=len(ids)\n\t)\n\tassert len(query_result[\"ids\"]) == len(query_result[\"distances\"])\n\tid_scores_dict = {id_: [] for id_ in ids}\n\tfor id_list, score_list in zip(query_result[\"ids\"], query_result[\"distances\"]):\n\t\tfor id_ in list(id_scores_dict.keys()):\n\t\t\tid_idx = id_list.index(id_)\n\t\t\tid_scores_dict[id_].append(score_list[id_idx])\n\tid_scores_pd = pd.DataFrame(id_scores_dict)\n\ttemp_client.delete_collection(\"temp\")\n\treturn id_scores_pd.max(axis=0).tolist()\n```\n\nNeed to fix here because our distance score.\n\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-08T14:41:26Z",
      "updated_at": "2024-10-09T06:16:15Z",
      "closed_at": "2024-10-09T06:16:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/815/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/815",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/815",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.807090",
      "comments": []
    },
    {
      "issue_number": 793,
      "title": "[Feature Request] New Voyageai-Reranker",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:24:22Z",
      "updated_at": "2024-10-08T12:05:16Z",
      "closed_at": "2024-10-08T12:05:16Z",
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/793/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/793",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/793",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.807098",
      "comments": []
    },
    {
      "issue_number": 804,
      "title": "[Feature Request] Refactor API server for processing whole YAML file.",
      "body": "**Is your feature request related to a problem? Please describe.**\nWe need this features.\n\n- Post query and get generated texts\n- The text can be streamed\n- Return used passages and its raw document path and passages\n\n**Describe the solution you'd like**\nI want to make an OpenAPI sepcifiction for it\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-05T09:05:17Z",
      "updated_at": "2024-10-08T01:36:41Z",
      "closed_at": "2024-10-08T01:36:41Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/804/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/804",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/804",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.807104",
      "comments": []
    },
    {
      "issue_number": 811,
      "title": "[BUG] OpenAIEmbedding did not truncate as openai_truncate_by_token",
      "body": "**Describe the bug**\nisinstance(embedding_model, OpenAIEmbedding) => this will be False\n\nYou can use this\n\nisinstance(embedding_model._instance, OpenAIEmbedding)\n\n**Code that bug is happened**\nIf applicable, add the code that bug is happened.\n(Especially, your AutoRAG YAML file or python codes that you wrote)\n\n**Desktop (please complete the following information):**\n - OS: [e.g. Windows, Linux, MacOS]\n - Python version [e.g. 3.10]\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-07T15:34:26Z",
      "updated_at": "2024-10-08T01:06:11Z",
      "closed_at": "2024-10-08T01:06:11Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/811/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/811",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/811",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.807111",
      "comments": []
    },
    {
      "issue_number": 639,
      "title": "[BUG] ValueError:",
      "body": "\r\n![Screenshot 2024-08-24 172724](https://github.com/user-attachments/assets/73438c24-6db8-4384-a823-39164170f2c1)\r\nwhat is the issue?\r\n\r\n",
      "state": "closed",
      "author": "arunkumarja",
      "author_type": "User",
      "created_at": "2024-08-24T11:59:59Z",
      "updated_at": "2024-10-07T13:58:33Z",
      "closed_at": "2024-10-07T13:58:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/639/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/639",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/639",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.807116",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@arunkumarja \r\nCan you give us the full bug trace (bug log) and what is the situation that the bug happens?",
          "created_at": "2024-08-25T02:55:58Z"
        },
        {
          "author": "arunkumarja",
          "body": "```\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\Users\\arunk\\Envs\\auto\\lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to ",
          "created_at": "2024-08-25T04:23:11Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@arunkumarja \r\nIt looks like there is something wrong at `panel`. \r\nRecommend to reinstall following packages.\r\n\r\n```\r\npanel\r\nseaborn\r\nipykernel\r\nipywidgets\r\nipywidgets_bokeh\r\n```",
          "created_at": "2024-08-25T07:37:53Z"
        },
        {
          "author": "arunkumarja",
          "body": "same error again through creating a new environment, install AutoRAG above package also install but same error return.",
          "created_at": "2024-08-25T09:29:22Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@arunkumarja \r\nCan you try python 3.10 environment?",
          "created_at": "2024-09-04T09:13:21Z"
        }
      ]
    },
    {
      "issue_number": 602,
      "title": "\bI got semantic retrieval module running log when I don't use it.",
      "body": "I used only bm25 at retrieval node, but i got semantic retrieval module running log.\r\n\r\n![image](https://github.com/user-attachments/assets/b47e4bef-9533-4fe8-86df-e05d42e19ee8)\r\n\r\nused yaml file (upload only retrieve_node_line!)\r\n\r\n```\r\nnode_lines:\r\n  - node_line_name: retrieve_node_line\r\n    nodes:\r\n      - node_type: retrieval\r\n        strategy:\r\n          metrics: [ retrieval_f1 ]\r\n        top_k: 5\r\n        modules:\r\n          - module_type: bm25\r\n            bm25_tokenizer: [ ko_kiwi ]\r\n      - node_type: passage_reranker\r\n        strategy:\r\n          metrics: [ retrieval_f1 ]\r\n        top_k: 3\r\n        modules:\r\n          - module_type: koreranker\r\n```",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-08-07T06:09:08Z",
      "updated_at": "2024-10-07T13:57:39Z",
      "closed_at": "2024-10-07T13:57:37Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/602/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/602",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/602",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:05.992138",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "I think this is not happening more because we refactored whole structure since v0.3",
          "created_at": "2024-10-07T13:57:37Z"
        }
      ]
    },
    {
      "issue_number": 538,
      "title": "Allow for variations of metrics",
      "body": "currently, AutoRAG system not allow variations of each metric in once experiments.\r\nexamples:\r\nRouge - need to choice between Rouge-1, Rouge-2, Rouge-L, Rouge-sum (supported from RougeScorer)\r\nBLEU - each \"n\"  gram\r\nsem score - if user want to compare between many model embedding?\r\n\r\nHow should we do?",
      "state": "open",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-06-26T08:56:08Z",
      "updated_at": "2024-10-07T13:55:42Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Low-Level"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/538/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/538",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/538",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:06.175376",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Let's break down to the smaller issues. \r\nSince we can specify about metric kwargs at YAML, we can make it all work in AutoRAG",
          "created_at": "2024-08-28T07:08:46Z"
        },
        {
          "author": "tenseisoham",
          "body": "Hey, i would like to contribute to this, how do I get started?",
          "created_at": "2024-09-29T06:09:44Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@tenseisoham Hello! \r\nFirst, check out our `CONTRIBUTING.md` file. It explains the dev setups. like pre-commit or testing.\r\nThen, you can find the generation metric at `autorag/evaluation/metric/generation.py`.\r\n\r\nYou can see, the Rouge already supports each Rouge-1,  Rouge-2, Rouge-L, Rouge-sum.\r\nW",
          "created_at": "2024-09-29T06:14:41Z"
        },
        {
          "author": "vkehfdl1",
          "body": "We just need to resolve #264 to resolve this issue.",
          "created_at": "2024-10-07T13:55:35Z"
        }
      ]
    },
    {
      "issue_number": 449,
      "title": "bug in `restart_evaluate` not getting all node_lines in `summary.csv`",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-05-20T06:45:23Z",
      "updated_at": "2024-10-07T13:54:13Z",
      "closed_at": "2024-10-07T13:54:11Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/449/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/449",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/449",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:06.364906",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "We couldn't reproduce this bug, so we are closing this issue.",
          "created_at": "2024-10-07T13:54:12Z"
        }
      ]
    },
    {
      "issue_number": 264,
      "title": "Can’t calculate more than two sem score with different model in one config YAML file",
      "body": null,
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-04T06:52:10Z",
      "updated_at": "2024-10-07T13:51:22Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/264/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/264",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/264",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:06.555494",
      "comments": []
    },
    {
      "issue_number": 43,
      "title": "Add generation evaluation decorator to provide yield return",
      "body": "from PR #41\r\n\r\nsometimes, generation evaluation need to provide evaluation result in progress with token scale(like log_prob etc,)\r\nfor more advanced generation.",
      "state": "open",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-01-23T11:07:03Z",
      "updated_at": "2024-10-07T12:56:59Z",
      "closed_at": null,
      "labels": [
        "wontfix"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/43/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/43",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/43",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:06.555517",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It looks like 'stream' support.",
          "created_at": "2024-03-12T22:28:42Z"
        }
      ]
    },
    {
      "issue_number": 792,
      "title": "[Feature Request] New Sbert Reranker",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:23:29Z",
      "updated_at": "2024-10-07T06:32:05Z",
      "closed_at": "2024-10-07T06:32:04Z",
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/792/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/792",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/792",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:06.794421",
      "comments": [
        {
          "author": "bwook00",
          "body": "It is Sentence Transformer Reranker. But we already have it. So i close this issue",
          "created_at": "2024-10-07T06:32:04Z"
        }
      ]
    },
    {
      "issue_number": 786,
      "title": "[BUG]  windows Installation is difficult on Windows. Can you make it easier to use on Windows as well?",
      "body": "Installation is difficult on Windows. Can you make it easier to use on Windows as well?",
      "state": "closed",
      "author": "Yoontaewoong",
      "author_type": "User",
      "created_at": "2024-10-04T08:40:26Z",
      "updated_at": "2024-10-07T05:53:34Z",
      "closed_at": "2024-10-07T05:53:33Z",
      "labels": [
        "bug",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/786/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/786",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/786",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.006404",
      "comments": [
        {
          "author": "Yoontaewoong",
          "body": "  Building wheel for chroma-hnswlib (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  × Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [9 lines of output]\n      running bdist_wheel\n      running build\n      running build_ext\n    ",
          "created_at": "2024-10-04T08:52:46Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Hello @Yoontaewoong \n\nFirst, about your error.\nIt looks like you are get trouble installing chroma in Windows. And you are missing Micorsoft C++ exectuable (2022 version? i think)\nYou must install it. For more information, we highly recommend you to check out chroma docs\n\n---\n\nBtw, you must check fe",
          "created_at": "2024-10-04T12:01:05Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Close this issue because of inactivity. \nWindows users! Recommend to use Docker container.",
          "created_at": "2024-10-07T05:53:33Z"
        }
      ]
    },
    {
      "issue_number": 790,
      "title": "[Feature Request] New OpenVINO Rerank module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:22:09Z",
      "updated_at": "2024-10-06T14:59:22Z",
      "closed_at": "2024-10-06T14:59:21Z",
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/790/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/790",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/790",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.223216",
      "comments": []
    },
    {
      "issue_number": 648,
      "title": "add metric and reform style",
      "body": "**Insufficient metric**\r\nThere are various metrics available for RAG.\r\nThese include projects such as deepeval, or Galileo.\r\nWith the addition of that metric, more diverse metrics should be accommodated in AutoRAG.\r\n\r\n**Describe the solution you'd like**\r\n- make rag subject schema for input of metric\r\n- every metric function use this schema as input.\r\n\r\n**Describe alternatives you've considered**\r\n- [x] #649\r\n- [x] #655\r\n## gallileo metrics\r\n- [ ] #650\r\n- [ ] add gallileo_sexism\r\n- [ ] add gallileo_tone\r\n- [ ] add gallileo_toxicity\r\n- [ ] add gallileo_private_identifiable_information\r\n- [ ] add gallileo_context_relevance\r\n- [ ] add gallileo_prompt_perplexity\r\n- [ ] add gallileo_uncertainty\r\n- [ ] add gallileo_chunk_utilization\r\n- [ ] add gallileo_chunk_attribution\r\n- [ ] add gallileo_completeness\r\n- [ ] add gallileo_correctness\r\n- [ ] add gallileo_context_adherence\r\n- [ ] add gallileo_instruction_adherence\r\n- [ ] add gallileo_chunk_relevance\r\n## MLflow\r\n- [ ] add mlflow_relevance\r\n- [ ] add mlflow_faithfulness\r\n- [ ] add mlflow_answer_relevance\r\n- [ ] add mlflow_answer_similarity\r\n- [ ] add mlflow_anwer_correctness\r\n## deepeval metrics\r\n- [ ] #654\r\n- [ ] #752\r\n- [x] #753\r\n- [ ] #754\r\n- [ ] #755\r\n- [ ] #756\r\n- [ ] #757\r\n- [ ] #758\r\n- [ ] #759\r\n## trulens\r\n- [ ] add trulens_answer relevancy\r\n- [ ] add trulens_context_relevancy\r\n- [ ] add trulens_groundedness\r\n- [ ] add trulens_coherence\r\n- [ ] add trulens_comprehensiveness\r\n- [ ] add trulens_conciseness\r\n- [ ] add trulens_controversialty\r\n- [ ] add trulens_correctness\r\n- [ ] add trulens_criminality\r\n- [ ] add trulens_generate_score\r\n- [ ] add trulens_groundedness\r\n- [ ] add trulens_harmfulness\r\n- [ ] add trulens_helpfulness\r\n- [ ] add trulens_insensitivity\r\n- [ ] add trulens_maliciousness\r\n- [ ] add trulens_misogyny\r\n- [ ] add trulens_relevance\r\n- [ ] add trulens_sentiment\r\n- [ ] add trulens_strereotypes\r\n- [ ] add trulens_summarization\r\n## openai\r\n- [ ] add openai_hate\r\n- [ ] add openai_harassment\r\n- [ ] add openai_self-harm\r\n- [ ] add openai_sexual\r\n- [ ] add openai_violence\r\n\r\n",
      "state": "open",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-08-26T06:25:03Z",
      "updated_at": "2024-10-05T05:03:00Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/648/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/648",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/648",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.223282",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Wow there's a lot of metric 👍 ",
          "created_at": "2024-08-26T06:26:04Z"
        }
      ]
    },
    {
      "issue_number": 635,
      "title": "[Feature Request] Retrieval results in deployment results",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nI have generated an evluation set, run Autorag to find best pipeline, great stuff. I would like to use it, and can run the CLI, API and web app. However, these all only seem to reply with the generated text, in many cases it's good to see the retrieval results also to eyeball how well generation is going, but many RAG systems present these to users.\r\n\r\n**Describe the solution you'd like**\r\nIt would be great if the CLI and API also returned retrieval results.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n\r\n**Additional context**\r\nThanks!\r\n",
      "state": "open",
      "author": "dividor",
      "author_type": "User",
      "created_at": "2024-08-21T15:11:25Z",
      "updated_at": "2024-10-05T02:36:17Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/635/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/635",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/635",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.438509",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Little bit duplicate with #493",
          "created_at": "2024-08-23T06:37:25Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Anyway @dividor Thanks for your interest and making an issue at AutoRAG.\r\n\r\nI have a question. Do you want `retrieved_contents` at both API server and web???\r\nIf so, I think it will be better to change this issue for API server, and for web use #493 issue. Is it okay?",
          "created_at": "2024-08-23T06:38:39Z"
        },
        {
          "author": "dividor",
          "body": "Hi! Sorry for the delay. \r\n\r\nAPI would be fine, thanks.",
          "created_at": "2024-08-29T23:19:50Z"
        }
      ]
    },
    {
      "issue_number": 800,
      "title": "[Feature Request] New Accumulated Compressor module",
      "body": "https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/response_synthesizers/accumulate.py",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-05T00:41:26Z",
      "updated_at": "2024-10-05T00:41:26Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/800/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/800",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/800",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660514",
      "comments": []
    },
    {
      "issue_number": 799,
      "title": "[Feature Request] New Compact and Accumulate Compressor module",
      "body": "https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/response_synthesizers/compact_and_accumulate.py\n",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-05T00:40:04Z",
      "updated_at": "2024-10-05T00:41:01Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/799/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/799",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/799",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660534",
      "comments": []
    },
    {
      "issue_number": 798,
      "title": "[Feature Request] New Compact and Refine Compressor module",
      "body": "https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/response_synthesizers/compact_and_refine.py",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-05T00:38:30Z",
      "updated_at": "2024-10-05T00:40:53Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/798/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/798",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/798",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660558",
      "comments": []
    },
    {
      "issue_number": 796,
      "title": "[Feature Request] New Keyword Filter module",
      "body": "https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-05T00:27:52Z",
      "updated_at": "2024-10-05T00:34:56Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/796/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/796",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/796",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660565",
      "comments": []
    },
    {
      "issue_number": 797,
      "title": "[Feature Request] New Step Decompose Query Expansion module",
      "body": "https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/query/query_transform/base.py#L269",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-05T00:33:40Z",
      "updated_at": "2024-10-05T00:33:40Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/797/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/797",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/797",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660570",
      "comments": []
    },
    {
      "issue_number": 795,
      "title": "[BUG] Flag Embedding Reranker has error at the batch one.",
      "body": "**Describe the bug**\ntest_flag_embedding_reranker_batch_one test function do not passaed.\n\n**Full Error log**\n```\ntests/autorag/nodes/passagereranker/test_flag_embedding.py:30 (test_flag_embedding_reranker_batch_one)\nflag_embedding_reranker_instance = <autorag.nodes.passagereranker.flag_embedding.FlagEmbeddingReranker object at 0x138707a90>\n\n    @pytest.mark.skipif(is_github_action(), reason=\"Skipping this test on GitHub Actions\")\n    def test_flag_embedding_reranker_batch_one(flag_embedding_reranker_instance):\n    \ttop_k = 3\n    \tbatch = 1\n    \tcontents_result, id_result, score_result = flag_embedding_reranker_instance._pure(\n    \t\tqueries_example, contents_example, ids_example, top_k, batch\n    \t)\n>   \tbase_reranker_test(contents_result, id_result, score_result, top_k)\n\ntest_flag_embedding.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncontents = [['Paris is the capital of France.', 'havertz is suck at soccer', 'NomaDamas is Great Team'], ['Newjeans has 5 members.', 'LA is a country in the United States.', 'i am hungry']]\nids = [['6649ae5c-6f98-4e9a-8a2e-d94f2ecd928b', '8ba2dacb-59f4-4142-b00a-c11b205a5f03', '26e27df6-a3ab-4974-ad2c-aa1abe906dc...fdfbf058-390b-4395-a480-318a90b87b4c', '853982fc-37df-4fb6-b909-eada007d8c1a', '2a8ad16b-f14d-47a5-a7de-5d171a64a49b']]\nscores = [[[7.978793621063232], [-8.296481132507324], [-9.318552017211914]], [[3.5015883445739746], [-8.99830150604248], [-9.481008529663086]]]\ntop_k = 3, use_ko = False, descending = True\n\n    def base_reranker_test(contents, ids, scores, top_k, use_ko=False, descending=True):\n    \tassert len(contents) == len(ids) == len(scores) == 2\n    \tassert len(contents[0]) == len(ids[0]) == len(scores[0]) == top_k\n    \tfor content_list, id_list, score_list in zip(contents, ids, scores):\n    \t\tassert isinstance(content_list, list)\n    \t\tassert isinstance(id_list, list)\n    \t\tassert isinstance(score_list, list)\n    \t\tfor content, _id, score in zip(content_list, id_list, score_list):\n    \t\t\tassert isinstance(content, str)\n    \t\t\tassert isinstance(_id, str)\n>   \t\t\tassert isinstance(score, float)\nE      assert False\nE       +  where False = isinstance([7.978793621063232], float)\n\ntest_passage_reranker_base.py:73: AssertionError\n```\n\n**Code that bug is happened**\nIf applicable, add the code that bug is happened.\n(Especially, your AutoRAG YAML file or python codes that you wrote)\n\n**Desktop (please complete the following information):**\n - OS: Mac, Windows\n - Python version 3.10.14\n-FlagEmbedding==1.2.11\n\n\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-04T17:22:39Z",
      "updated_at": "2024-10-04T17:22:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/795/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/795",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/795",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660576",
      "comments": []
    },
    {
      "issue_number": 794,
      "title": "[Feature Request] New Xinference Reranker",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:25:09Z",
      "updated_at": "2024-10-04T13:25:09Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/794/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/794",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/794",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660581",
      "comments": []
    },
    {
      "issue_number": 791,
      "title": "[Feature Request] New Presidio reranker module",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:22:57Z",
      "updated_at": "2024-10-04T13:22:57Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/791/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/791",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/791",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660586",
      "comments": []
    },
    {
      "issue_number": 788,
      "title": "[Feature Request] New DashScope-Rerank module",
      "body": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/postprocessor/llama-index-postprocessor-dashscope-rerank",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:20:02Z",
      "updated_at": "2024-10-04T13:20:09Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/788/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/788",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/788",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660591",
      "comments": []
    },
    {
      "issue_number": 787,
      "title": "[Feature Request] New Alibabacloud_Aisearch_Rerank module",
      "body": "[Alibabacloud_Aisearch_Rerank](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/postprocessor/llama-index-postprocessor-alibabacloud-aisearch-rerank)",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-10-04T13:18:55Z",
      "updated_at": "2024-10-04T13:19:29Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/787/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/787",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/787",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660598",
      "comments": []
    },
    {
      "issue_number": 577,
      "title": "Implement srrf hybrid retrieval",
      "body": null,
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-17T10:45:18Z",
      "updated_at": "2024-10-04T07:28:59Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/577/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/577",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/577",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660603",
      "comments": []
    },
    {
      "issue_number": 587,
      "title": "Add TILDE for new passage reranker module",
      "body": "https://github.com/ielab/TILDE",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-25T02:56:59Z",
      "updated_at": "2024-10-04T07:28:51Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/587/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/587",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/587",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660608",
      "comments": []
    },
    {
      "issue_number": 614,
      "title": "Add LeanContext as PassageCompressor",
      "body": "[Paper](https://arxiv.org/pdf/2309.00841)\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-14T03:54:09Z",
      "updated_at": "2024-10-04T07:28:44Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/614/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/614",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/614",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660613",
      "comments": []
    },
    {
      "issue_number": 477,
      "title": "Add MMR retrieval",
      "body": "https://wikidocs.net/231585",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-05T16:28:58Z",
      "updated_at": "2024-10-04T07:28:10Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/477/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/477",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/477",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.660618",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "When you try to make this issue, you must consider the hybrid retrieval!!",
          "created_at": "2024-08-15T08:10:44Z"
        }
      ]
    },
    {
      "issue_number": 782,
      "title": "[BUG] Data creation pipeline event loop error when executing openai api",
      "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nTry to use this code.\r\n\r\n```python\r\n\tclient = AsyncOpenAI()\r\n\r\n\tinitial_raw = Raw(pd.read_parquet(raw_path, engine=\"pyarrow\"))\r\n\tinitial_corpus = Corpus(pd.read_parquet(corpus_path, engine=\"pyarrow\"), initial_raw)\r\n\tqa = initial_corpus.sample(random_single_hop, n=qa_size).map(\r\n\t\t\tlambda df: df.reset_index(drop=True),\r\n\t\t).make_retrieval_gt_contents().batch_apply(\r\n\t\t\tfactoid_query_gen,  # query generation\r\n\t\t\tclient=client,\r\n\t\t\tmodel_name=\"gpt-4o-2024-08-06\",\r\n\t\t\tlang=\"ko\",\r\n\t\t)\r\n\tqa = qa.batch_apply(\r\n\t\t\tmake_basic_gen_gt,  # answer generation (basic)\r\n\t\t\tclient=client,\r\n\t\t\tmodel_name=\"gpt-4o-2024-08-06\",\r\n\t\t\tlang=\"ko\",\r\n\t\t)\r\n\tqa = qa.batch_apply(\r\n\t\t\tmake_concise_gen_gt,  # answer generation (concise)\r\n\t\t\tclient=client,\r\n\t\t\tmodel_name=\"gpt-4o-2024-08-06\",\r\n\t\t\tlang=\"ko\",\r\n\t\t).filter(\r\n\t\t\tdontknow_filter_rule_based,  # filter unanswerable questions\r\n\t\t\tlang=\"ko\",\r\n\t\t\tclient=client,\r\n\t\t\tmodel_name=\"gpt-4o-2024-08-06\",\r\n\t\t)\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Full Error log**\r\n```\r\n                             ╭──────────────────────────────────── Traceback (most recent call last) ────────────────────────────────────╮               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/openai/_base_client. │               \r\n                             │ py:1562 in _request                                                                                       │               \r\n                             │                                                                                                           │               \r\n                             │   1559 │   │   │   kwargs[\"auth\"] = self.custom_auth                                                      │               \r\n                             │   1560 │   │                                                                                              │               \r\n                             │   1561 │   │   try:                                                                                       │               \r\n                             │ ❱ 1562 │   │   │   response = await self._client.send(                                                    │               \r\n                             │   1563 │   │   │   │   request,                                                                           │               \r\n                             │   1564 │   │   │   │   stream=stream or self._should_stream_response_body(request=request),               │               \r\n                             │   1565 │   │   │   │   **kwargs,                                                                          │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpx/_client.py:167 │               \r\n                             │ 4 in send                                                                                                 │               \r\n                             │                                                                                                           │               \r\n                             │   1671 │   │                                                                                              │               \r\n                             │   1672 │   │   auth = self._build_request_auth(request, auth)                                             │               \r\n                             │   1673 │   │                                                                                              │               \r\n                             │ ❱ 1674 │   │   response = await self._send_handling_auth(                                                 │               \r\n                             │   1675 │   │   │   request,                                                                               │               \r\n                             │   1676 │   │   │   auth=auth,                                                                             │               \r\n                             │   1677 │   │   │   follow_redirects=follow_redirects,                                                     │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpx/_client.py:170 │               \r\n                             │ 2 in _send_handling_auth                                                                                  │               \r\n                             │                                                                                                           │               \r\n                             │   1699 │   │   │   request = await auth_flow.__anext__()                                                  │               \r\n                             │   1700 │   │   │                                                                                          │               \r\n                             │   1701 │   │   │   while True:                                                                            │               \r\n                             │ ❱ 1702 │   │   │   │   response = await self._send_handling_redirects(                                    │               \r\n                             │   1703 │   │   │   │   │   request,                                                                       │               \r\n                             │   1704 │   │   │   │   │   follow_redirects=follow_redirects,                                             │               \r\n                             │   1705 │   │   │   │   │   history=history,                                                               │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpx/_client.py:173 │               \r\n                             │ 9 in _send_handling_redirects                                                                             │               \r\n                             │                                                                                                           │               \r\n                             │   1736 │   │   │   for hook in self._event_hooks[\"request\"]:                                              │               \r\n                             │   1737 │   │   │   │   await hook(request)                                                                │               \r\n                             │   1738 │   │   │                                                                                          │               \r\n                             │ ❱ 1739 │   │   │   response = await self._send_single_request(request)                                    │               \r\n                             │   1740 │   │   │   try:                                                                                   │               \r\n                             │   1741 │   │   │   │   for hook in self._event_hooks[\"response\"]:                                         │               \r\n                             │   1742 │   │   │   │   │   await hook(response)                                                           │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpx/_client.py:177 │               \r\n                             │ 6 in _send_single_request                                                                                 │               \r\n                             │                                                                                                           │               \r\n                             │   1773 │   │   │   )                                                                                      │               \r\n                             │   1774 │   │                                                                                              │               \r\n                             │   1775 │   │   with request_context(request=request):                                                     │               \r\n                             │ ❱ 1776 │   │   │   response = await transport.handle_async_request(request)                               │               \r\n                             │   1777 │   │                                                                                              │               \r\n                             │   1778 │   │   assert isinstance(response.stream, AsyncByteStream)                                        │               \r\n                             │   1779 │   │   response.request = request                                                                 │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpx/_transports/de │               \r\n                             │ fault.py:377 in handle_async_request                                                                      │               \r\n                             │                                                                                                           │               \r\n                             │   374 │   │   │   extensions=request.extensions,                                                          │               \r\n                             │   375 │   │   )                                                                                           │               \r\n                             │   376 │   │   with map_httpcore_exceptions():                                                             │               \r\n                             │ ❱ 377 │   │   │   resp = await self._pool.handle_async_request(req)                                       │               \r\n                             │   378 │   │                                                                                               │               \r\n                             │   379 │   │   assert isinstance(resp.stream, typing.AsyncIterable)                                        │               \r\n                             │   380                                                                                                     │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_async/conn │               \r\n                             │ ection_pool.py:216 in handle_async_request                                                                │               \r\n                             │                                                                                                           │               \r\n                             │   213 │   │   │   │   closing = self._assign_requests_to_connections()                                    │               \r\n                             │   214 │   │   │                                                                                           │               \r\n                             │   215 │   │   │   await self._close_connections(closing)                                                  │               \r\n                             │ ❱ 216 │   │   │   raise exc from None                                                                     │               \r\n                             │   217 │   │                                                                                               │               \r\n                             │   218 │   │   # Return the response. Note that in this case we still have to manage                       │               \r\n                             │   219 │   │   # the point at which the response is closed.                                                │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_async/conn │               \r\n                             │ ection_pool.py:196 in handle_async_request                                                                │               \r\n                             │                                                                                                           │               \r\n                             │   193 │   │   │   │                                                                                       │               \r\n                             │   194 │   │   │   │   try:                                                                                │               \r\n                             │   195 │   │   │   │   │   # Send the request on the assigned connection.                                  │               \r\n                             │ ❱ 196 │   │   │   │   │   response = await connection.handle_async_request(                               │               \r\n                             │   197 │   │   │   │   │   │   pool_request.request                                                        │               \r\n                             │   198 │   │   │   │   │   )                                                                               │               \r\n                             │   199 │   │   │   │   except ConnectionNotAvailable:                                                      │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_async/conn │               \r\n                             │ ection.py:101 in handle_async_request                                                                     │               \r\n                             │                                                                                                           │               \r\n                             │    98 │   │   │   self._connect_failed = True                                                             │               \r\n                             │    99 │   │   │   raise exc                                                                               │               \r\n                             │   100 │   │                                                                                               │               \r\n                             │ ❱ 101 │   │   return await self._connection.handle_async_request(request)                                 │               \r\n                             │   102 │                                                                                                   │               \r\n                             │   103 │   async def _connect(self, request: Request) -> AsyncNetworkStream:                               │               \r\n                             │   104 │   │   timeouts = request.extensions.get(\"timeout\", {})                                            │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_async/http │               \r\n                             │ 11.py:143 in handle_async_request                                                                         │               \r\n                             │                                                                                                           │               \r\n                             │   140 │   │   │   with AsyncShieldCancellation():                                                         │               \r\n                             │   141 │   │   │   │   async with Trace(\"response_closed\", logger, request) as trace:                      │               \r\n                             │   142 │   │   │   │   │   await self._response_closed()                                                   │               \r\n                             │ ❱ 143 │   │   │   raise exc                                                                               │               \r\n                             │   144 │                                                                                                   │               \r\n                             │   145 │   # Sending the request...                                                                        │               \r\n                             │   146                                                                                                     │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_async/http │               \r\n                             │ 11.py:113 in handle_async_request                                                                         │               \r\n                             │                                                                                                           │               \r\n                             │   110 │   │   │   │   │   reason_phrase,                                                                  │               \r\n                             │   111 │   │   │   │   │   headers,                                                                        │               \r\n                             │   112 │   │   │   │   │   trailing_data,                                                                  │               \r\n                             │ ❱ 113 │   │   │   │   ) = await self._receive_response_headers(**kwargs)                                  │               \r\n                             │   114 │   │   │   │   trace.return_value = (                                                              │               \r\n                             │   115 │   │   │   │   │   http_version,                                                                   │               \r\n                             │   116 │   │   │   │   │   status,                                                                         │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_async/http │               \r\n                             │ 11.py:186 in _receive_response_headers                                                                    │               \r\n                             │                                                                                                           │               \r\n                             │   183 │   │   timeout = timeouts.get(\"read\", None)                                                        │               \r\n                             │   184 │   │                                                                                               │               \r\n                             │   185 │   │   while True:                                                                                 │               \r\n                             │ ❱ 186 │   │   │   event = await self._receive_event(timeout=timeout)                                      │               \r\n                             │   187 │   │   │   if isinstance(event, h11.Response):                                                     │               \r\n                             │   188 │   │   │   │   break                                                                               │               \r\n                             │   189 │   │   │   if (                                                                                    │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_async/http │               \r\n                             │ 11.py:224 in _receive_event                                                                               │               \r\n                             │                                                                                                           │               \r\n                             │   221 │   │   │   │   event = self._h11_state.next_event()                                                │               \r\n                             │   222 │   │   │                                                                                           │               \r\n                             │   223 │   │   │   if event is h11.NEED_DATA:                                                              │               \r\n                             │ ❱ 224 │   │   │   │   data = await self._network_stream.read(                                             │               \r\n                             │   225 │   │   │   │   │   self.READ_NUM_BYTES, timeout=timeout                                            │               \r\n                             │   226 │   │   │   │   )                                                                                   │               \r\n                             │   227                                                                                                     │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/httpcore/_backends/a │               \r\n                             │ nyio.py:35 in read                                                                                        │               \r\n                             │                                                                                                           │               \r\n                             │    32 │   │   with map_exceptions(exc_map):                                                               │               \r\n                             │    33 │   │   │   with anyio.fail_after(timeout):                                                         │               \r\n                             │    34 │   │   │   │   try:                                                                                │               \r\n                             │ ❱  35 │   │   │   │   │   return await self._stream.receive(max_bytes=max_bytes)                          │               \r\n                             │    36 │   │   │   │   except anyio.EndOfStream:  # pragma: nocover                                        │               \r\n                             │    37 │   │   │   │   │   return b\"\"                                                                      │               \r\n                             │    38                                                                                                     │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/anyio/streams/tls.py │               \r\n                             │ :205 in receive                                                                                           │               \r\n                             │                                                                                                           │               \r\n                             │   202 │   │   await self.transport_stream.aclose()                                                        │               \r\n                             │   203 │                                                                                                   │               \r\n                             │   204 │   async def receive(self, max_bytes: int = 65536) -> bytes:                                       │               \r\n                             │ ❱ 205 │   │   data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)                  │               \r\n                             │   206 │   │   if not data:                                                                                │               \r\n                             │   207 │   │   │   raise EndOfStream                                                                       │               \r\n                             │   208                                                                                                     │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/anyio/streams/tls.py │               \r\n                             │ :147 in _call_sslobject_method                                                                            │               \r\n                             │                                                                                                           │               \r\n                             │   144 │   │   │   │   │   if self._write_bio.pending:                                                     │               \r\n                             │   145 │   │   │   │   │   │   await self.transport_stream.send(self._write_bio.read())                    │               \r\n                             │   146 │   │   │   │   │                                                                                   │               \r\n                             │ ❱ 147 │   │   │   │   │   data = await self.transport_stream.receive()                                    │               \r\n                             │   148 │   │   │   │   except EndOfStream:                                                                 │               \r\n                             │   149 │   │   │   │   │   self._read_bio.write_eof()                                                      │               \r\n                             │   150 │   │   │   │   except OSError as exc:                                                              │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/anyio/_backends/_asy │               \r\n                             │ ncio.py:1198 in receive                                                                                   │               \r\n                             │                                                                                                           │               \r\n                             │   1195 │   │   │   │   and not self._protocol.is_at_eof                                                   │               \r\n                             │   1196 │   │   │   ):                                                                                     │               \r\n                             │   1197 │   │   │   │   self._transport.resume_reading()                                                   │               \r\n                             │ ❱ 1198 │   │   │   │   await self._protocol.read_event.wait()                                             │               \r\n                             │   1199 │   │   │   │   self._transport.pause_reading()                                                    │               \r\n                             │   1200 │   │   │   else:                                                                                  │               \r\n                             │   1201 │   │   │   │   await AsyncIOBackend.checkpoint()                                                  │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/.pyenv/versions/3.10.14/lib/python3.10/asyncio/locks.py:211 in wait                        │               \r\n                             │                                                                                                           │               \r\n                             │   208 │   │   if self._value:                                                                             │               \r\n                             │   209 │   │   │   return True                                                                             │               \r\n                             │   210 │   │                                                                                               │               \r\n                             │ ❱ 211 │   │   fut = self._get_loop().create_future()                                                      │               \r\n                             │   212 │   │   self._waiters.append(fut)                                                                   │               \r\n                             │   213 │   │   try:                                                                                        │               \r\n                             │   214 │   │   │   await fut                                                                               │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/.pyenv/versions/3.10.14/lib/python3.10/asyncio/mixins.py:30 in _get_loop                   │               \r\n                             │                                                                                                           │               \r\n                             │   27 │   │   │   │   if self._loop is None:                                                               │               \r\n                             │   28 │   │   │   │   │   self._loop = loop                                                                │               \r\n                             │   29 │   │   if loop is not self._loop:                                                                   │               \r\n                             │ ❱ 30 │   │   │   raise RuntimeError(f'{self!r} is bound to a different event loop')                       │               \r\n                             │   31 │   │   return loop                                                                                  │               \r\n                             │   32                                                                                                      │               \r\n                             ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯               \r\n                             RuntimeError: <asyncio.locks.Event object at 0x1391e42e0 [unset]> is bound to a different event loop                        \r\n                                                                                                                                                         \r\n                             The above exception was the direct cause of the following exception:                                                        \r\n                                                                                                                                                         \r\n                             ╭──────────────────────────────────── Traceback (most recent call last) ────────────────────────────────────╮               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/./make_qa.py:72 in <module>                            │               \r\n                             │                                                                                                           │               \r\n                             │   69                                                                                                      │               \r\n                             │   70                                                                                                      │               \r\n                             │   71 if __name__ == \"__main__\":                                                                           │               \r\n                             │ ❱ 72 │   main()                                                                                           │               \r\n                             │   73                                                                                                      │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/click/core.py:1157   │               \r\n                             │ in __call__                                                                                               │               \r\n                             │                                                                                                           │               \r\n                             │   1154 │                                                                                                  │               \r\n                             │   1155 │   def __call__(self, *args: t.Any, **kwargs: t.Any) -> t.Any:                                    │               \r\n                             │   1156 │   │   \"\"\"Alias for :meth:`main`.\"\"\"                                                              │               \r\n                             │ ❱ 1157 │   │   return self.main(*args, **kwargs)                                                          │               \r\n                             │   1158                                                                                                    │               \r\n                             │   1159                                                                                                    │               \r\n                             │   1160 class Command(BaseCommand):                                                                        │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/click/core.py:1078   │               \r\n                             │ in main                                                                                                   │               \r\n                             │                                                                                                           │               \r\n                             │   1075 │   │   try:                                                                                       │               \r\n                             │   1076 │   │   │   try:                                                                                   │               \r\n                             │   1077 │   │   │   │   with self.make_context(prog_name, args, **extra) as ctx:                           │               \r\n                             │ ❱ 1078 │   │   │   │   │   rv = self.invoke(ctx)                                                          │               \r\n                             │   1079 │   │   │   │   │   if not standalone_mode:                                                        │               \r\n                             │   1080 │   │   │   │   │   │   return rv                                                                  │               \r\n                             │   1081 │   │   │   │   │   # it's not safe to `ctx.exit(rv)` here!                                        │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/click/core.py:1434   │               \r\n                             │ in invoke                                                                                                 │               \r\n                             │                                                                                                           │               \r\n                             │   1431 │   │   │   echo(style(message, fg=\"red\"), err=True)                                               │               \r\n                             │   1432 │   │                                                                                              │               \r\n                             │   1433 │   │   if self.callback is not None:                                                              │               \r\n                             │ ❱ 1434 │   │   │   return ctx.invoke(self.callback, **ctx.params)                                         │               \r\n                             │   1435 │                                                                                                  │               \r\n                             │   1436 │   def shell_complete(self, ctx: Context, incomplete: str) -> t.List[\"CompletionItem\"]:           │               \r\n                             │   1437 │   │   \"\"\"Return a list of completions for the incomplete value. Looks                            │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/click/core.py:783 in │               \r\n                             │ invoke                                                                                                    │               \r\n                             │                                                                                                           │               \r\n                             │    780 │   │                                                                                              │               \r\n                             │    781 │   │   with augment_usage_errors(__self):                                                         │               \r\n                             │    782 │   │   │   with ctx:                                                                              │               \r\n                             │ ❱  783 │   │   │   │   return __callback(*args, **kwargs)                                                 │               \r\n                             │    784 │                                                                                                  │               \r\n                             │    785 │   def forward(                                                                                   │               \r\n                             │    786 │   │   __self, __cmd: \"Command\", *args: t.Any, **kwargs: t.Any  # noqa: B902                      │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/./make_qa.py:50 in main                                │               \r\n                             │                                                                                                           │               \r\n                             │   47 │   │   │   model_name=\"gpt-4o-2024-08-06\",                                                          │               \r\n                             │   48 │   │   │   lang=\"ko\",                                                                               │               \r\n                             │   49 │   │   )                                                                                            │               \r\n                             │ ❱ 50 │   qa = qa.batch_apply(                                                                             │               \r\n                             │   51 │   │   │   make_basic_gen_gt,  # answer generation (basic)                                          │               \r\n                             │   52 │   │   │   client=client,                                                                           │               \r\n                             │   53 │   │   │   model_name=\"gpt-4o-2024-08-06\",                                                          │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/autorag/data/qa/sche │               \r\n                             │ ma.py:139 in batch_apply                                                                                  │               \r\n                             │                                                                                                           │               \r\n                             │   136 │   │   qa_dicts = self.data.to_dict(orient=\"records\")                                              │               \r\n                             │   137 │   │   loop = get_event_loop()                                                                     │               \r\n                             │   138 │   │   tasks = [fn(qa_dict, **kwargs) for qa_dict in qa_dicts]                                     │               \r\n                             │ ❱ 139 │   │   results = loop.run_until_complete(process_batch(tasks, batch_size))                         │               \r\n                             │   140 │   │   return QA(pd.DataFrame(results), self.linked_corpus)                                        │               \r\n                             │   141 │                                                                                                   │               \r\n                             │   142 │   def batch_filter(                                                                               │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/.pyenv/versions/3.10.14/lib/python3.10/asyncio/base_events.py:649 in run_until_complete    │               \r\n                             │                                                                                                           │               \r\n                             │    646 │   │   if not future.done():                                                                      │               \r\n                             │    647 │   │   │   raise RuntimeError('Event loop stopped before Future completed.')                      │               \r\n                             │    648 │   │                                                                                              │               \r\n                             │ ❱  649 │   │   return future.result()                                                                     │               \r\n                             │    650 │                                                                                                  │               \r\n                             │    651 │   def stop(self):                                                                                │               \r\n                             │    652 │   │   \"\"\"Stop running the event loop.                                                            │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/autorag/utils/util.p │               \r\n                             │ y:301 in process_batch                                                                                    │               \r\n                             │                                                                                                           │               \r\n                             │   298 │                                                                                                   │               \r\n                             │   299 │   for i in range(0, len(tasks), batch_size):                                                      │               \r\n                             │   300 │   │   batch = tasks[i : i + batch_size]                                                           │               \r\n                             │ ❱ 301 │   │   batch_results = await asyncio.gather(*batch)                                                │               \r\n                             │   302 │   │   results.extend(batch_results)                                                               │               \r\n                             │   303 │                                                                                                   │               \r\n                             │   304 │   return results                                                                                  │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/autorag/data/qa/gene │               \r\n                             │ ration_gt/openai_gen_gt.py:82 in make_basic_gen_gt                                                        │               \r\n                             │                                                                                                           │               \r\n                             │   79 │   │   Default is \"en\".                                                                             │               \r\n                             │   80 │   :return: The output row of the qa dataframe with added \"generation_gt\" in it.                    │               \r\n                             │   81 │   \"\"\"                                                                                              │               \r\n                             │ ❱ 82 │   return await make_gen_gt_openai(                                                                 │               \r\n                             │   83 │   │   row, client, GEN_GT_SYSTEM_PROMPT[\"basic\"][lang], model_name                                 │               \r\n                             │   84 │   )                                                                                                │               \r\n                             │   85                                                                                                      │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/autorag/data/qa/gene │               \r\n                             │ ration_gt/openai_gen_gt.py:28 in make_gen_gt_openai                                                       │               \r\n                             │                                                                                                           │               \r\n                             │   25 │   passage_str = \"\\n\".join(retrieval_gt_contents)                                                   │               \r\n                             │   26 │   user_prompt =                                                                                    │               \r\n                             │      f\"Text:\\n<|text_start|>\\n{passage_str}\\n<|text_end|>\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"               │               \r\n                             │   27 │                                                                                                    │               \r\n                             │ ❱ 28 │   completion = await client.beta.chat.completions.parse(                                           │               \r\n                             │   29 │   │   model=model_name,                                                                            │               \r\n                             │   30 │   │   messages=[                                                                                   │               \r\n                             │   31 │   │   │   {\"role\": \"system\", \"content\": system_prompt},                                            │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/openai/resources/bet │               \r\n                             │ a/chat/completions.py:332 in parse                                                                        │               \r\n                             │                                                                                                           │               \r\n                             │   329 │   │   │   **(extra_headers or {}),                                                                │               \r\n                             │   330 │   │   }                                                                                           │               \r\n                             │   331 │   │                                                                                               │               \r\n                             │ ❱ 332 │   │   raw_completion = await self._client.chat.completions.create(                                │               \r\n                             │   333 │   │   │   messages=messages,                                                                      │               \r\n                             │   334 │   │   │   model=model,                                                                            │               \r\n                             │   335 │   │   │   response_format=_type_to_response_format(response_format),                              │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/openai/resources/cha │               \r\n                             │ t/completions.py:1412 in create                                                                           │               \r\n                             │                                                                                                           │               \r\n                             │   1409 │   │   timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,                              │               \r\n                             │   1410 │   ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:                                        │               \r\n                             │   1411 │   │   validate_response_format(response_format)                                                  │               \r\n                             │ ❱ 1412 │   │   return await self._post(                                                                   │               \r\n                             │   1413 │   │   │   \"/chat/completions\",                                                                   │               \r\n                             │   1414 │   │   │   body=await async_maybe_transform(                                                      │               \r\n                             │   1415 │   │   │   │   {                                                                                  │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/openai/_base_client. │               \r\n                             │ py:1829 in post                                                                                           │               \r\n                             │                                                                                                           │               \r\n                             │   1826 │   │   opts = FinalRequestOptions.construct(                                                      │               \r\n                             │   1827 │   │   │   method=\"post\", url=path, json_data=body, files=await                                   │               \r\n                             │        async_to_httpx_files(files), **options                                                             │               \r\n                             │   1828 │   │   )                                                                                          │               \r\n                             │ ❱ 1829 │   │   return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)             │               \r\n                             │   1830 │                                                                                                  │               \r\n                             │   1831 │   async def patch(                                                                               │               \r\n                             │   1832 │   │   self,                                                                                      │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/openai/_base_client. │               \r\n                             │ py:1523 in request                                                                                        │               \r\n                             │                                                                                                           │               \r\n                             │   1520 │   │   else:                                                                                      │               \r\n                             │   1521 │   │   │   retries_taken = 0                                                                      │               \r\n                             │   1522 │   │                                                                                              │               \r\n                             │ ❱ 1523 │   │   return await self._request(                                                                │               \r\n                             │   1524 │   │   │   cast_to=cast_to,                                                                       │               \r\n                             │   1525 │   │   │   options=options,                                                                       │               \r\n                             │   1526 │   │   │   stream=stream,                                                                         │               \r\n                             │                                                                                                           │               \r\n                             │ /Users/jeffrey/PycharmProjects/AutoRAG-tutorial-ko/venv/lib/python3.10/site-packages/openai/_base_client. │               \r\n                             │ py:1596 in _request                                                                                       │               \r\n                             │                                                                                                           │               \r\n                             │   1593 │   │   │   │   )                                                                                  │               \r\n                             │   1594 │   │   │                                                                                          │               \r\n                             │   1595 │   │   │   log.debug(\"Raising connection error\")                                                  │               \r\n                             │ ❱ 1596 │   │   │   raise APIConnectionError(request=request) from err                                     │               \r\n                             │   1597 │   │                                                                                              │               \r\n                             │   1598 │   │   log.debug(                                                                                 │               \r\n                             │   1599 │   │   │   'HTTP Request: %s %s \"%i %s\"', request.method, request.url,                            │               \r\n                             │        response.status_code, response.reason_phrase                                                       │               \r\n                             ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯               \r\n                             APIConnectionError: Connection error. \r\n```\r\n\r\n**Code that bug is happened**\r\nIf applicable, add the code that bug is happened.\r\n(Especially, your AutoRAG YAML file or python codes that you wrote)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. Windows, Linux, MacOS]\r\n - Python version [e.g. 3.10]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-03T05:01:15Z",
      "updated_at": "2024-10-04T07:19:30Z",
      "closed_at": "2024-10-04T07:19:30Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/782/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/782",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/782",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:07.846296",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "This is happening on the python 3.10 version, but not in python 3.9 version.\nIn python 3.9, it retries the request and do not occur event loop error\n",
          "created_at": "2024-10-03T17:05:24Z"
        }
      ]
    },
    {
      "issue_number": 737,
      "title": "[Parse] Making Parse runnable from the CLI",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-25T13:38:41Z",
      "updated_at": "2024-10-03T17:01:57Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/737/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/737",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/737",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.060535",
      "comments": []
    },
    {
      "issue_number": 781,
      "title": "[BUG] Cannot get start_end_idx from the langchain_chunk konlpy splitter",
      "body": "**Describe the bug**\r\nI used chunk.yaml file like below\r\n\r\n```yaml\r\n  - module_type: langchain_chunk\r\n    chunk_method: konlpy\r\n    chunk_size: 512\r\n    chunk_overlap: 48\r\n```\r\n\r\nAnd I got no start_end_idx on the result corpus.parquet file.\r\n\r\n![image](https://github.com/user-attachments/assets/61573de5-6ae0-4cf3-a927-c7ac700c50bd)\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse this 5.parquet. \r\nI can’t upload parquet file in the github issue OMG\r\n\r\n**Expected behavior**\r\nIt is missing start_end_idx… It is crucial.\r\n\r\n**Full Error log**\r\nIf applicable, add full error log to help explain your problem.\r\n\r\n**Code that bug is happened**\r\nIf applicable, add the code that bug is happened.\r\n(Especially, your AutoRAG YAML file or python codes that you wrote)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. Windows, Linux, MacOS]\r\n - Python version [e.g. 3.10]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-03T04:11:22Z",
      "updated_at": "2024-10-03T06:45:07Z",
      "closed_at": "2024-10-03T06:45:07Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/781/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/781",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/781",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.060560",
      "comments": [
        {
          "author": "bwook00",
          "body": "![image](https://github.com/user-attachments/assets/9bfc4887-b3fe-46e9-beb7-2b0ffdb595ac)\r\nI added Support Parsing Modules page",
          "created_at": "2024-10-03T06:45:07Z"
        }
      ]
    },
    {
      "issue_number": 779,
      "title": "[BUG] When there are no table in the page in clova, it occurs KeyError",
      "body": "**Describe the bug**\r\n\r\nGo to `autorag/data/parse/clova.py` in line 103\r\n\r\n```python\r\n\t\ttable_html = json_to_html_table(resp_json[\"images\"][0][\"tables\"][0][\"cells\"])\r\n\t\tpage_text = extract_text_from_fields(resp_json[\"images\"][0][\"fields\"])\r\n```\r\nWhen there are no key “tables”, it occurs key error.\r\nIt can be possible to put the pdf page that do not include table at all. Then it returns no table. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Use non-table exists page to use clova.py\r\n\r\n**Expected behavior**\r\nIt must pass the table processing procedure.\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-03T03:17:34Z",
      "updated_at": "2024-10-03T06:27:03Z",
      "closed_at": "2024-10-03T06:27:03Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/779/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/779",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/779",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308575",
      "comments": []
    },
    {
      "issue_number": 780,
      "title": "[BUG] Certain parsing modules only parse the first page of the whole pdf files, but got page_num -1",
      "body": "**Describe the bug**\r\nIt have to be contain whole pages of the pdf files, but got only first page on the certain modules. \r\n\r\n<The buggy modules>\r\n\r\n1. Llamaparse\r\n2. pymupdf (langchain_parse)\r\n3. pdfplumber (langchain_parse)\r\n4. pypdf (langchain_parse)\r\n5. pypdfium2 (langchain_parse)\r\n\r\n**To Reproduce**\r\nUse this file . \r\n[[현장] 데뷔 23개월 만에 도쿄돔 달군 뉴진스, 월드투어 기대감 키웠다.pdf](https://github.com/user-attachments/files/17238758/23.pdf)\r\n\r\n**Expected behavior**\r\nIt have to be contain whole pages of the pdf files. \r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-10-03T03:38:57Z",
      "updated_at": "2024-10-03T06:04:41Z",
      "closed_at": "2024-10-03T06:04:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/780/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/780",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/780",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308598",
      "comments": []
    },
    {
      "issue_number": 430,
      "title": "Add NVIDA reranker module",
      "body": "https://build.nvidia.com/nvidia/rerank-qa-mistral-4b",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-05-13T08:26:23Z",
      "updated_at": "2024-10-02T11:59:42Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/430/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/430",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/430",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308609",
      "comments": []
    },
    {
      "issue_number": 555,
      "title": "Add ClapNQ as RAG evaluation dataset sample",
      "body": "https://arxiv.org/pdf/2404.02103",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-03T09:58:30Z",
      "updated_at": "2024-10-02T11:59:10Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/555/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/555",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/555",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308615",
      "comments": []
    },
    {
      "issue_number": 749,
      "title": "[Feature Request] Add azure api docs and tutorial (+ sample config files)",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nI see many people using Azure OpenAI, because whole company using Azure only. \r\n\r\n**Describe the solution you'd like**\r\nTest Azure openai system in AutoRAG, and find out what will be needed.\r\n\r\n**Describe alternatives you've considered**\r\nI think there are some modules that can only use OpenAI (like g_eval..?)\r\nIt can be an issue;;\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-26T14:48:08Z",
      "updated_at": "2024-10-02T11:58:20Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/749/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/749",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/749",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308621",
      "comments": []
    },
    {
      "issue_number": 771,
      "title": "[Feature Request] Make short version data of Eli5 dataset for quick testing and evaluation",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrent version of Eli5 dataset is 400 rows (as test data). This is too many for beginners who want to test AutoRAG.\r\n\r\n**Describe the solution you'd like**\r\nCut them to 100 rows and also corpus as well. Corpus maybe about 500 rows?\r\n\r\n**Additional context**\r\nAfter doing this, we will use this dataset at Docker container tutorial. Or maybe use it in colab tutorial also.\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-30T06:30:19Z",
      "updated_at": "2024-10-02T11:57:58Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/771/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/771",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/771",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308627",
      "comments": []
    },
    {
      "issue_number": 773,
      "title": "[BUG] Bug at casting of retrieva_gt in retrieval run.py",
      "body": "**Describe the bug**\r\nThere are strange code at the retrieval run.py\r\n\r\n```python\r\nretrieval_gt = qa_df[\"retrieval_gt\"].tolist()\r\nretrieval_gt = [\r\n\t\t[\r\n\t\t\t[str(uuid) for uuid in sub_array] if sub_array.size > 0 else []\r\n\t\t\tfor sub_array in inner_array\r\n\t\t]\r\n\t\tfor inner_array in retrieval_gt\r\n\t]\r\n```\r\n\r\nIn here, the `sub_array.size` will be problem because of the `sub_array` can be string or something else. Especially, it is not working for list.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nMaybe use list of list retrieval_gt as retrieval_gt columns.\r\n\r\n**Expected behavior**\r\nIt will not work when sub_array is string or list.\r\n\r\n**Full Error log**\r\n![image](https://github.com/user-attachments/assets/aabfe789-ba3e-447b-ad35-413270b48a8b)\r\n\r\n\r\n**Code that bug is happened**\r\n\r\n```yaml\r\nnode_lines:\r\n- node_line_name: retrieve_node_line  # Set Node Line (Arbitrary Name)\r\n  nodes:\r\n    - node_type: retrieval  # Set Retrieval Node\r\n      strategy:\r\n        metrics: [retrieval_f1, retrieval_recall, retrieval_ndcg, retrieval_mrr]  # Set Retrieval Metrics\r\n      top_k: 3\r\n      modules:\r\n        - module_type: vectordb\r\n          embedding_model: openai\r\n        - module_type: bm25\r\n        - module_type: hybrid_rrf\r\n          weight_range: (4,80)\r\n- node_line_name: post_retrieve_node_line  # Set Node Line (Arbitrary Name)\r\n  nodes:\r\n    - node_type: prompt_maker  # Set Prompt Maker Node\r\n      strategy:\r\n        metrics:   # Set Generation Metrics\r\n          - metric_name: meteor\r\n          - metric_name: rouge\r\n          - metric_name: sem_score\r\n            embedding_model: openai\r\n      modules:\r\n        - module_type: fstring\r\n          prompt: \"Read the passages and answer the given question. \\n Question: {query} \\n Passage: {retrieved_contents} \\n Answer : \"\r\n    - node_type: generator  # Set Generator Node\r\n      strategy:\r\n        metrics:  # Set Generation Metrics\r\n          - metric_name: meteor\r\n          - metric_name: rouge\r\n          - metric_name: sem_score\r\n            embedding_model: openai\r\n      modules:\r\n        - module_type: openai_llm\r\n          llm: gpt-4o-mini\r\n          batch: 16\r\n```\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. Windows, Linux, MacOS]\r\n - Python version [e.g. 3.10]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-30T14:12:27Z",
      "updated_at": "2024-10-01T12:30:56Z",
      "closed_at": "2024-10-01T12:30:56Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/773/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/773",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/773",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308636",
      "comments": []
    },
    {
      "issue_number": 709,
      "title": "[Data Creation] Evolving Question",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nEvolving question is necessary to develop more advanced questions.\r\n\r\n**Describe the solution you'd like**\r\nUse LlamaIndex or OpenAI for evolving questions.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-15T08:58:41Z",
      "updated_at": "2024-10-01T12:21:41Z",
      "closed_at": "2024-10-01T12:21:40Z",
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/709/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/709",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/709",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308643",
      "comments": []
    },
    {
      "issue_number": 671,
      "title": "[Feature Request] Add Dockerfile for running AutoRAG",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nBecause of the parsing modules, it is become much harder to install everything right in the environment.\r\nEspecially, because of the `pySSL`, `poppler-utils`, and `tesseract-ocr`. \r\nIt looks much better to use different container to work on it smoothly. \r\n\r\n**Describe the solution you'd like**\r\n- Write a Dockerfile for this.\r\n- **MUST RUN WHOLE TEST IN THE DOCKERFILE**\r\n- Use Docker image at github actions also. In this scenario, we have to find out how automatically build, how deploy to github registry, and pull it to test actions. => This better be extra issue.\r\n\r\n**Describe alternatives you've considered**\r\nMaybe make .sh file for installation. But we have virtual env actually.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-03T02:39:09Z",
      "updated_at": "2024-10-01T01:39:58Z",
      "closed_at": "2024-10-01T01:39:58Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/671/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 1
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/671",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/671",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.308651",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Possible deploy to the ECS",
          "created_at": "2024-09-13T07:24:44Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@hongsw will assign this issue",
          "created_at": "2024-09-27T04:26:08Z"
        },
        {
          "author": "hongsw",
          "body": "https://github.com/Marker-Inc-Korea/AutoRAG/pull/763",
          "created_at": "2024-09-29T01:11:31Z"
        }
      ]
    },
    {
      "issue_number": 762,
      "title": "[Feature Request] Add Windows Support",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nSome people still using windows. We need to test AutoRAG on the Windows machine.\r\n\r\n**Describe the solution you'd like**\r\nTest the pytest first. And then if something’s wrong, we will fix it.\r\n\r\n**Describe alternatives you've considered**\r\nI think we need to analyze how many people downloading AutoRAG in Windows.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-28T02:41:43Z",
      "updated_at": "2024-09-30T13:39:08Z",
      "closed_at": "2024-09-30T13:39:07Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/762/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/762",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/762",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.527904",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://docs.python.org/ko/3/library/tempfile.html\r\nhttps://www.scivision.dev/python-tempfile-permission-error-windows/\r\n\r\nMost of the error occured because of the tempfile library!!",
          "created_at": "2024-09-28T04:23:18Z"
        },
        {
          "author": "vkehfdl1",
          "body": "TART will not supported on the Windows.\r\n(tokenizer byte sequence problem)\r\n\r\nPlus, you have to install this.\r\n```python\r\nimport nltk\r\nnltk.download('averaged_perceptron_tagger_eng')\r\n```\r\n\r\n\r\nPlus, I got trouble to install libmagic. Install libmagic at Windows is tricky....\r\n\r\nWe need to change lib",
          "created_at": "2024-09-29T10:33:58Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Also UPR will not supported on the Windows, with the same reason.\r\nMonoT5 as well.\r\n\r\n+ You cannot use Flag Embedding Reranker with 'batch one'. You have to use more than batch 2. The default batch is 64.",
          "created_at": "2024-09-29T10:34:47Z"
        },
        {
          "author": "vkehfdl1",
          "body": "It looks like it is hard to support konlpy korean tokenizers on Windows. Hard to install it.",
          "created_at": "2024-09-29T12:03:05Z"
        }
      ]
    },
    {
      "issue_number": 768,
      "title": "[HotFix] Missing content about `extract_best_config` or running `Runner` from the trial folder.",
      "body": "**Describe the bug**\r\nThere are no explain about extract best config to YAML file. \r\n\r\n**To Reproduce**\r\nGo to README.md\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-30T02:24:25Z",
      "updated_at": "2024-09-30T13:32:09Z",
      "closed_at": "2024-09-30T13:32:09Z",
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/768/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/768",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/768",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.743738",
      "comments": []
    },
    {
      "issue_number": 585,
      "title": "\"Access denied\" when opening Colab Tutorial 2",
      "body": "The following link is not public : [Step 2: Create evaluation dataset](https://colab.research.google.com/drive/1HXjVHCLTaX7mkmZp3IKlEPt0B3jVeHvP#scrollTo=cgFUCuaUZvTr)",
      "state": "closed",
      "author": "baptiste-pasquier",
      "author_type": "User",
      "created_at": "2024-07-24T09:26:51Z",
      "updated_at": "2024-09-29T06:19:15Z",
      "closed_at": "2024-09-29T06:19:15Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/585/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/585",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/585",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.743758",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@baptiste-pasquier Thanks for the report. We'll check",
          "created_at": "2024-07-25T02:34:13Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@bwook00 please check this",
          "created_at": "2024-07-25T10:15:48Z"
        },
        {
          "author": "vkehfdl1",
          "body": "We added tutorial three and two also",
          "created_at": "2024-09-29T06:19:15Z"
        }
      ]
    },
    {
      "issue_number": 764,
      "title": "[Feature Request] Dockerfile for Korean version",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nTo use korean version, we need JVM not only python. Because of the konlpy. \r\nThe JVM version for konlpy is tricky, so we have to set it right.\r\n\r\n**Describe the solution you'd like**\r\nMake autorag:latest-korean docker container. \r\n\r\n**Describe alternatives you've considered**\r\n\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-29T01:41:45Z",
      "updated_at": "2024-09-29T01:41:46Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/764/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hongsw"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/764",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/764",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.928809",
      "comments": []
    },
    {
      "issue_number": 687,
      "title": "[Data Creation] [Feature Request] Add passage dependency filter",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nAdd passage dependency filter for filtering passage-dependent questions\r\n\r\n**Describe the solution you'd like**\r\nUse LLM. Both openai and LlamaIndex\r\n\r\n**Describe alternatives you've considered**\r\nActually it is hard to get high accuracy at this task. But we have to do something.\r\nSomeday we can make custom classification model for this.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-09T04:41:45Z",
      "updated_at": "2024-09-27T03:40:53Z",
      "closed_at": "2024-09-27T03:40:53Z",
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/687/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/687",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/687",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.928851",
      "comments": []
    },
    {
      "issue_number": 732,
      "title": "[Feature Request] WARNING  sacrebleu:bleu.py:418 It is recommended to enable `effective_order` for sentence-level BLEU.",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nI got this warning message when I use BLEU\r\n\r\n```\r\nWARNING  sacrebleu:bleu.py:418 It is recommended to enable `effective_order` for sentence-level BLEU.\r\n```\r\n\r\n**Describe the solution you'd like**\r\nI think we can enable effective_order, and investigate what is it\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\npip freeze | grep -i sacrebleu\r\nsacrebleu==2.4.2\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-23T03:56:13Z",
      "updated_at": "2024-09-27T03:35:05Z",
      "closed_at": "2024-09-27T03:35:05Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/732/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/732",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/732",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:08.928857",
      "comments": []
    },
    {
      "issue_number": 746,
      "title": "[BUG] YAML file is wrong at chunk_full.yaml and README.md",
      "body": "**Describe the bug**\r\n\r\nAt the chunk_full.yaml and README.md, add_file_name is `ko` or `en`\r\nRight? @bwook00 ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-26T07:29:33Z",
      "updated_at": "2024-09-26T11:42:10Z",
      "closed_at": "2024-09-26T11:42:10Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/746/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/746",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/746",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:10.724547",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Plus, the import is different (do not use beta) at README.md in data creation\r\n\r\n",
          "created_at": "2024-09-26T07:39:52Z"
        }
      ]
    },
    {
      "issue_number": 733,
      "title": "[Structure Refactoring] Change beta data creation to default, and makes legacy version to legacy for v0.3",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-23T07:51:22Z",
      "updated_at": "2024-09-25T15:14:13Z",
      "closed_at": "2024-09-25T15:14:13Z",
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/733/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/733",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/733",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:10.965116",
      "comments": []
    },
    {
      "issue_number": 738,
      "title": "[Chunk] Making Chunk runable from the CLI",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-25T13:39:05Z",
      "updated_at": "2024-09-25T13:39:06Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/738/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/738",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/738",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:10.965135",
      "comments": []
    },
    {
      "issue_number": 734,
      "title": "[BUG] [HotFix] Fix vllm.py bug ",
      "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Full Error log**\r\nIf applicable, add full error log to help explain your problem.\r\n\r\n**Code that bug is happened**\r\nIf applicable, add the code that bug is happened.\r\n(Especially, your AutoRAG YAML file or python codes that you wrote)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. Windows, Linux, MacOS]\r\n - Python version [e.g. 3.10]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-24T01:08:31Z",
      "updated_at": "2024-09-24T13:52:07Z",
      "closed_at": "2024-09-24T13:52:07Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/734/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/734",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/734",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:10.965144",
      "comments": []
    },
    {
      "issue_number": 722,
      "title": "[Feature Request] \bAutoRAG install takes a long time because of botocore",
      "body": "![image](https://github.com/user-attachments/assets/e1ec3bcd-7c2e-4c7d-a860-d28073e0819a)\r\n",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-17T05:54:47Z",
      "updated_at": "2024-09-23T14:35:59Z",
      "closed_at": "2024-09-23T14:35:59Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/722/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/722",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/722",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:10.965150",
      "comments": [
        {
          "author": "bwook00",
          "body": "Python version == 3.9",
          "created_at": "2024-09-17T06:48:40Z"
        },
        {
          "author": "vkehfdl1",
          "body": "There was no error like this up to python 3.10",
          "created_at": "2024-09-18T00:37:18Z"
        }
      ]
    },
    {
      "issue_number": 460,
      "title": "Parallel process at longllmlingua",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-05-22T04:51:20Z",
      "updated_at": "2024-09-20T02:27:45Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/460/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/460",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/460",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.197758",
      "comments": []
    },
    {
      "issue_number": 730,
      "title": "[QA Creation]If QA Data already exists, handle cases where answer(generation_gt) is Unanswerable in Corpus",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nIf QA Data already exists, handle cases where answer(generation_gt) is Unanswerable in Corpus\r\n\r\n**Describe the solution you'd like**\r\nIf QA Data already exists, implement a feature to check if it is answerable with data in the corpus\r\n",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-19T14:34:39Z",
      "updated_at": "2024-09-19T14:58:40Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/730/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/730",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/730",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.197780",
      "comments": [
        {
          "author": "bwook00",
          "body": "Since unanswerable questions don't have retrieval_gt, I think we should delete them until we deal with unanswerable questions.",
          "created_at": "2024-09-19T14:52:11Z"
        },
        {
          "author": "bwook00",
          "body": "I think we should check if it is answerable based on the \"Question\", whether it has only Q, Q-A, or both.",
          "created_at": "2024-09-19T14:58:05Z"
        }
      ]
    },
    {
      "issue_number": 731,
      "title": "[\bChunk] Implement a feature to parse on a per-document basis when doing semantic chunking, even if you're using a per-page parsing module",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nSemantic chunking cuts into sentences and then merges those with close semantic scores (cosine similarity) to form passages.\r\nHowever, when parsing on a page-by-page basis, semantic score comparisons are only made between sentences on a page, which may not perform as well as comparing sentences across the document.\r\n\r\n**Describe the solution you'd like**\r\nImplement a feature to parse on a per-document basis when doing semantic chunking, even if you're using a per-page parsing module (e.g. clova, table_hybrid_parse)\r\n\r\n\r\n",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-19T14:42:46Z",
      "updated_at": "2024-09-19T14:47:26Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/731/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/731",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/731",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.472776",
      "comments": []
    },
    {
      "issue_number": 725,
      "title": "[Data Creation] [Feature Request] Extract Evidence from initial QA data retrieval gt",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nWhen mapping new corpus, it is better to extract exact evidence paragraph from the passage.\r\nSo, it needs extra extraction from the retrieval gt passage\r\n\r\n**Describe the solution you'd like**\r\nWe will use LLM to extract exact evidence sentence from the retrieval gt passage and mapping it to the `evidence_start_end_idx`\r\n\r\n**Describe alternatives you've considered**\r\nFrom now, you can use `update_corpus` without `extract_evidence` method\r\n\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-18T00:45:01Z",
      "updated_at": "2024-09-18T00:45:02Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/725/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/725",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/725",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.472800",
      "comments": []
    },
    {
      "issue_number": 718,
      "title": "[Feature Request] Compatibility with latest OpenAI gpt-4 o1 model.",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nWe have new gpt-4 o1 model and have to use it.\r\n\r\n**Describe the solution you'd like**\r\nWe need to check compatability at llama_index_llm and openai_llm modules.\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-16T13:24:46Z",
      "updated_at": "2024-09-17T04:12:36Z",
      "closed_at": "2024-09-17T04:12:36Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/718/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/718",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/718",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.472808",
      "comments": []
    },
    {
      "issue_number": 672,
      "title": "[Bug] Add guide for setup bm25_tokenizer in strategy of query_expansion",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nThe user must set the value of bm25_tokenizer in startegy on the query_extend node to be the same as the bm25_tokenizer in bm25 on the retrieval node.    \r\nIf the bm25_tokenizer in retrieval node is missing any of the values in bm25_tokenizer in the query_extension node, an error is raised with: `bm25_path {bm25_path} does not exist. Please ingest first.`.    \r\nHere is an example\r\n```yaml\r\n- node_line_name: pre_retrieve_node_line \r\n  nodes:\r\n    - node_type: query_expansion\r\n      strategy:\r\n        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]\r\n        top_k: 5\r\n        retrieval_modules:\r\n          - module_type: bm25\r\n            bm25_tokenizer: [ porter_stemmer, huggingface/new_model ]\r\n\r\n- node_line_name: retrieve_node_line \r\n  nodes:\r\n    - node_type: retrieval\r\n      modules:\r\n        - module_type: bm25\r\n          bm25_tokenizer: [ porter_stemmer ]\r\n```\r\nThere is no guidance or notice about this in the documentation, so I recommend adding a cautionary note about this situation.\r\n\r\n**Describe the solution you'd like**\r\nThere is no guide or notice about this in document. So it would be better to add guide the caution for this situation.\r\n",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2024-09-04T14:13:21Z",
      "updated_at": "2024-09-16T12:59:14Z",
      "closed_at": "2024-09-16T12:59:14Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/672/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "vkehfdl1",
        "rjwharry"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/672",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/672",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.472815",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It seems not intended. \r\nI will look what is wrong.",
          "created_at": "2024-09-06T04:07:58Z"
        },
        {
          "author": "rjwharry",
          "body": "If it is not intended, then I think we should ingest even in strategy of query_expansion too.",
          "created_at": "2024-09-06T04:14:56Z"
        },
        {
          "author": "rjwharry",
          "body": "If we evaluate query expansion based on retriever, then I think it has to be same as modules of retrieval node_type",
          "created_at": "2024-09-08T23:47:15Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry \r\nYes we need to check what tokenizer or embedding model used in query expansion at the start of the trial.\r\nSorry for late bug fix.\r\nWe are developing AutoRAG 0.3 version, so it might be delayed.",
          "created_at": "2024-09-09T01:51:12Z"
        },
        {
          "author": "rjwharry",
          "body": "Okay, then let me work on it.",
          "created_at": "2024-09-09T01:58:54Z"
        }
      ]
    },
    {
      "issue_number": 576,
      "title": "bm25_path .\\resources\\bm25_gpt2.pkl does not exist. Please ingest first.",
      "body": "ran through the tutorial using the simple_openai.yaml and it worked. I am trying to use the full.yaml and I am getting the following error\r\n\r\n`bm25_path .\\resources\\bm25_gpt2.pkl does not exist. Please ingest first.`\r\n\r\nI checked the resources folder and it does not in fact have the bm25_gpt2.pkl \r\n![image](https://github.com/user-attachments/assets/4973735a-4c9c-4c60-8846-feac2536be7d)\r\n\r\n![image](https://github.com/user-attachments/assets/98db556d-ae00-4d52-a280-26fbcec8f9d6)\r\n\r\n\r\nI have looked for resources in the github to manually download, but have not found any.\r\n\r\nAny help on this error is greatly appreciated.",
      "state": "closed",
      "author": "maglore9900",
      "author_type": "User",
      "created_at": "2024-07-16T16:06:05Z",
      "updated_at": "2024-09-16T12:59:14Z",
      "closed_at": "2024-09-16T12:59:14Z",
      "labels": [
        "bug",
        "question",
        "Windows"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/576/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/576",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/576",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.771684",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hello @maglore9900 \r\nYou must create new project directory and specify it when run the evaluation.\r\n\r\nuse `--project_dir` parameter at cli. Or use `project_dir` parameter when run the trial(evaluation)",
          "created_at": "2024-07-17T03:13:43Z"
        },
        {
          "author": "maglore9900",
          "body": "Appreciate the quick feedback. I am using the python script you provided, which you can see below\r\n\r\n```\r\nfrom autorag.evaluator import Evaluator\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\nevaluator = Evaluator(qa_data_path='corpus/qa.parquet', corpus_data_path='corpus/corpus.parquet')\r\nevalu",
          "created_at": "2024-07-17T03:17:18Z"
        },
        {
          "author": "maglore9900",
          "body": "if that is the suggestion, it still failed in the same way.\r\n\r\n![image](https://github.com/user-attachments/assets/8ef11153-fac1-4add-b615-ae7fa79ebe17)\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/687513bd-cad4-4239-bfdc-6d88f3096d6e)\r\n",
          "created_at": "2024-07-17T03:23:14Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I think you need to make a **new directory** as new project directory. And set it to the `project_dir` parameter. Can you try that again?",
          "created_at": "2024-07-17T03:46:04Z"
        },
        {
          "author": "maglore9900",
          "body": "The issue is that the current code was not recognizing the bm25_porter_stemmer.pkl, even though its supposed to. The solution on my side was basically a hack where I commented out the detection code in the module and explicitly put in bm25_porter_stemmer.pkl. \r\n\r\nI was running autorag with python 3.",
          "created_at": "2024-07-18T14:28:14Z"
        }
      ]
    },
    {
      "issue_number": 658,
      "title": "\bAdd AutoParse",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-08-28T08:35:23Z",
      "updated_at": "2024-09-16T12:29:48Z",
      "closed_at": "2024-09-16T12:29:47Z",
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/658/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/658",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/658",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:11.995245",
      "comments": [
        {
          "author": "bwook00",
          "body": "merged #668 ",
          "created_at": "2024-09-16T12:29:47Z"
        }
      ]
    },
    {
      "issue_number": 656,
      "title": "\bAdd AutoChunk",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-08-27T14:18:51Z",
      "updated_at": "2024-09-16T12:29:31Z",
      "closed_at": "2024-09-16T12:29:31Z",
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/656/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/656",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/656",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:12.192800",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "merged #681",
          "created_at": "2024-09-08T08:57:16Z"
        },
        {
          "author": "bwook00",
          "body": "merged #693",
          "created_at": "2024-09-16T12:29:31Z"
        }
      ]
    },
    {
      "issue_number": 696,
      "title": "[BUG] ValueError: XFormers does not support attention logits soft capping.",
      "body": "**Describe the bug**\r\nValueError: XFormers does not support attention logits soft capping.\r\n\r\n\r\n\r\n\r\n\r\n**Full Error log**\r\n{\r\n\t\"name\": \"ValueError\",\r\n\t\"message\": \"XFormers does not support attention logits soft capping.\",\r\n\t\"stack\": \"---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[7], line 3\r\n      1 import nest_asyncio\r\n      2 nest_asyncio.apply()\r\n----> 3 evaluator.start_trial(yaml_path)\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/evaluator.py:126, in Evaluator.start_trial(self, yaml_path)\r\n    124 \\t\\tprevious_result = self.qa_data\r\n    125 \\tlogger.info(f\\\"Running node line {node_line_name}...\\\")\r\n--> 126 \\tprevious_result = run_node_line(node_line, node_line_dir, previous_result)\r\n    128 \\ttrial_summary_df = self._append_node_line_summary(\r\n    129 \\t\\tnode_line_name, node_line_dir, trial_summary_df\r\n    130 \\t)\r\n    132 trial_summary_df.to_csv(\r\n    133 \\tos.path.join(self.project_dir, trial_name, \\\"summary.csv\\\"), index=False\r\n    134 )\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/node_line.py:47, in run_node_line(nodes, node_line_dir, previous_result)\r\n     45 summary_lst = []\r\n     46 for node in nodes:\r\n---> 47 \\tprevious_result = node.run(previous_result, node_line_dir)\r\n     48 \\tnode_summary_df = load_summary_file(\r\n     49 \\t\\tos.path.join(node_line_dir, node.node_type, \\\"summary.csv\\\")\r\n     50 \\t)\r\n     51 \\tbest_node_row = node_summary_df.loc[node_summary_df[\\\"is_best\\\"]]\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/schema/node.py:57, in Node.run(self, previous_result, node_line_dir)\r\n     55 logger.info(f\\\"Running node {self.node_type}...\\\")\r\n     56 input_modules, input_params = self.get_param_combinations()\r\n---> 57 return self.run_node(\r\n     58 \\tmodules=input_modules,\r\n     59 \\tmodule_params=input_params,\r\n     60 \\tprevious_result=previous_result,\r\n     61 \\tnode_line_dir=node_line_dir,\r\n     62 \\tstrategies=self.strategy,\r\n     63 )\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/nodes/generator/run.py:46, in run_generator_node(modules, module_params, previous_result, node_line_dir, strategies)\r\n     43 \\traise ValueError(\\\"You must have 'generation_gt' column in qa.parquet.\\\")\r\n     44 generation_gt = list(map(lambda x: x.tolist(), qa_data[\\\"generation_gt\\\"].tolist()))\r\n---> 46 results, execution_times = zip(\r\n     47 \\t*map(\r\n     48 \\t\\tlambda x: measure_speed(\r\n     49 \\t\\t\\tx[0], project_dir=project_dir, previous_result=previous_result, **x[1]\r\n     50 \\t\\t),\r\n     51 \\t\\tzip(modules, module_params),\r\n     52 \\t)\r\n     53 )\r\n     54 average_times = list(map(lambda x: x / len(results[0]), execution_times))\r\n     56 # get average token usage\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/nodes/generator/run.py:48, in run_generator_node.<locals>.<lambda>(x)\r\n     43 \\traise ValueError(\\\"You must have 'generation_gt' column in qa.parquet.\\\")\r\n     44 generation_gt = list(map(lambda x: x.tolist(), qa_data[\\\"generation_gt\\\"].tolist()))\r\n     46 results, execution_times = zip(\r\n     47 \\t*map(\r\n---> 48 \\t\\tlambda x: measure_speed(\r\n     49 \\t\\t\\tx[0], project_dir=project_dir, previous_result=previous_result, **x[1]\r\n     50 \\t\\t),\r\n     51 \\t\\tzip(modules, module_params),\r\n     52 \\t)\r\n     53 )\r\n     54 average_times = list(map(lambda x: x / len(results[0]), execution_times))\r\n     56 # get average token usage\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/strategy.py:14, in measure_speed(func, *args, **kwargs)\r\n     10 \\\"\\\"\\\"\r\n     11 Method for measuring execution speed of the function.\r\n     12 \\\"\\\"\\\"\r\n     13 start_time = time.time()\r\n---> 14 result = func(*args, **kwargs)\r\n     15 end_time = time.time()\r\n     16 return result, end_time - start_time\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/utils/util.py:67, in result_to_dataframe.<locals>.decorator_result_to_dataframe.<locals>.wrapper(*args, **kwargs)\r\n     65 @functools.wraps(func)\r\n     66 def wrapper(*args, **kwargs) -> pd.DataFrame:\r\n---> 67 \\tresults = func(*args, **kwargs)\r\n     68 \\tif len(column_names) == 1:\r\n     69 \\t\\tdf_input = {column_names[0]: results}\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/nodes/generator/base.py:49, in generator_node.<locals>.wrapper(project_dir, previous_result, llm, **kwargs)\r\n     47 \\treturn result\r\n     48 else:\r\n---> 49 \\treturn func(prompts=prompts, llm=llm, **kwargs)\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/nodes/generator/vllm.py:38, in vllm(prompts, llm, **kwargs)\r\n     33 \\traise ImportError(\r\n     34 \\t\\t\\\"Please install vllm library. You can install it by running `pip install vllm`.\\\"\r\n     35 \\t)\r\n     37 input_kwargs = deepcopy(kwargs)\r\n---> 38 vllm_model = make_vllm_instance(llm, input_kwargs)\r\n     40 if \\\"logprobs\\\" not in input_kwargs:\r\n     41 \\tinput_kwargs[\\\"logprobs\\\"] = 1\r\n\r\nFile /home3/dgon/NLP/gits/AutoRAG/autorag/nodes/generator/vllm.py:74, in make_vllm_instance(llm, input_args)\r\n     72 \\tif v is not None:\r\n     73 \\t\\tinput_kwargs[param] = v\r\n---> 74 return LLM(model, **input_kwargs)\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/entrypoints/llm.py:177, in LLM.__init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, **kwargs)\r\n    153     raise TypeError(\r\n    154         \\\"There is no need to pass vision-related arguments anymore.\\\")\r\n    155 engine_args = EngineArgs(\r\n    156     model=model,\r\n    157     tokenizer=tokenizer,\r\n   (...)\r\n    175     **kwargs,\r\n    176 )\r\n--> 177 self.llm_engine = LLMEngine.from_engine_args(\r\n    178     engine_args, usage_context=UsageContext.LLM_CLASS)\r\n    179 self.request_counter = Counter()\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/engine/llm_engine.py:538, in LLMEngine.from_engine_args(cls, engine_args, usage_context, stat_loggers)\r\n    536 executor_class = cls._get_executor_cls(engine_config)\r\n    537 # Create the LLM engine.\r\n--> 538 engine = cls(\r\n    539     **engine_config.to_dict(),\r\n    540     executor_class=executor_class,\r\n    541     log_stats=not engine_args.disable_log_stats,\r\n    542     usage_context=usage_context,\r\n    543     stat_loggers=stat_loggers,\r\n    544 )\r\n    546 return engine\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/engine/llm_engine.py:305, in LLMEngine.__init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, step_return_finished_only)\r\n    301 self.input_registry = input_registry\r\n    302 self.input_processor = input_registry.create_input_processor(\r\n    303     model_config)\r\n--> 305 self.model_executor = executor_class(\r\n    306     model_config=model_config,\r\n    307     cache_config=cache_config,\r\n    308     parallel_config=parallel_config,\r\n    309     scheduler_config=scheduler_config,\r\n    310     device_config=device_config,\r\n    311     lora_config=lora_config,\r\n    312     speculative_config=speculative_config,\r\n    313     load_config=load_config,\r\n    314     prompt_adapter_config=prompt_adapter_config,\r\n    315     observability_config=self.observability_config,\r\n    316 )\r\n    318 if not self.model_config.embedding_mode:\r\n    319     self._initialize_kv_caches()\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/executor/executor_base.py:47, in ExecutorBase.__init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\r\n     45 self.prompt_adapter_config = prompt_adapter_config\r\n     46 self.observability_config = observability_config\r\n---> 47 self._init_executor()\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:40, in GPUExecutor._init_executor(self)\r\n     38 self.driver_worker = self._create_worker()\r\n     39 self.driver_worker.init_device()\r\n---> 40 self.driver_worker.load_model()\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/worker/worker.py:182, in Worker.load_model(self)\r\n    181 def load_model(self):\r\n--> 182     self.model_runner.load_model()\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/worker/model_runner.py:917, in GPUModelRunnerBase.load_model(self)\r\n    915 logger.info(\\\"Starting to load model %s...\\\", self.model_config.model)\r\n    916 with CudaMemoryProfiler() as m:\r\n--> 917     self.model = get_model(model_config=self.model_config,\r\n    918                            device_config=self.device_config,\r\n    919                            load_config=self.load_config,\r\n    920                            lora_config=self.lora_config,\r\n    921                            parallel_config=self.parallel_config,\r\n    922                            scheduler_config=self.scheduler_config,\r\n    923                            cache_config=self.cache_config)\r\n    925 self.model_memory_usage = m.consumed_memory\r\n    926 logger.info(\\\"Loading model weights took %.4f GB\\\",\r\n    927             self.model_memory_usage / float(2**30))\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py:19, in get_model(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, cache_config)\r\n     13 def get_model(*, model_config: ModelConfig, load_config: LoadConfig,\r\n     14               device_config: DeviceConfig, parallel_config: ParallelConfig,\r\n     15               scheduler_config: SchedulerConfig,\r\n     16               lora_config: Optional[LoRAConfig],\r\n     17               cache_config: CacheConfig) -> nn.Module:\r\n     18     loader = get_model_loader(load_config)\r\n---> 19     return loader.load_model(model_config=model_config,\r\n     20                              device_config=device_config,\r\n     21                              lora_config=lora_config,\r\n     22                              parallel_config=parallel_config,\r\n     23                              scheduler_config=scheduler_config,\r\n     24                              cache_config=cache_config)\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:341, in DefaultModelLoader.load_model(self, model_config, device_config, lora_config, parallel_config, scheduler_config, cache_config)\r\n    339 with set_default_torch_dtype(model_config.dtype):\r\n    340     with target_device:\r\n--> 341         model = _initialize_model(model_config, self.load_config,\r\n    342                                   lora_config, cache_config,\r\n    343                                   scheduler_config)\r\n    344     model.load_weights(\r\n    345         self._get_weights_iterator(model_config.model,\r\n    346                                    model_config.revision,\r\n   (...)\r\n    349                                        \\\"fall_back_to_pt_during_load\\\",\r\n    350                                        True)), )\r\n    352     for _, module in model.named_modules():\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:170, in _initialize_model(model_config, load_config, lora_config, cache_config, scheduler_config)\r\n    167 \\\"\\\"\\\"Initialize a model with the given configurations.\\\"\\\"\\\"\r\n    168 model_class, _ = get_model_architecture(model_config)\r\n--> 170 return build_model(\r\n    171     model_class,\r\n    172     model_config.hf_config,\r\n    173     cache_config=cache_config,\r\n    174     quant_config=_get_quantization_config(model_config, load_config),\r\n    175     lora_config=lora_config,\r\n    176     multimodal_config=model_config.multimodal_config,\r\n    177     scheduler_config=scheduler_config,\r\n    178 )\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:155, in build_model(model_class, hf_config, cache_config, quant_config, lora_config, multimodal_config, scheduler_config)\r\n    145 def build_model(model_class: Type[nn.Module], hf_config: PretrainedConfig,\r\n    146                 cache_config: Optional[CacheConfig],\r\n    147                 quant_config: Optional[QuantizationConfig], *,\r\n    148                 lora_config: Optional[LoRAConfig],\r\n    149                 multimodal_config: Optional[MultiModalConfig],\r\n    150                 scheduler_config: Optional[SchedulerConfig]) -> nn.Module:\r\n    151     extra_kwargs = _get_model_initialization_kwargs(model_class, lora_config,\r\n    152                                                     multimodal_config,\r\n    153                                                     scheduler_config)\r\n--> 155     return model_class(config=hf_config,\r\n    156                        cache_config=cache_config,\r\n    157                        quant_config=quant_config,\r\n    158                        **extra_kwargs)\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:329, in Gemma2ForCausalLM.__init__(***failed resolving arguments***)\r\n    327 assert config.tie_word_embeddings\r\n    328 self.quant_config = quant_config\r\n--> 329 self.model = Gemma2Model(config, cache_config, quant_config)\r\n    330 self.logits_processor = LogitsProcessor(\r\n    331     config.vocab_size, soft_cap=config.final_logit_softcapping)\r\n    332 self.sampler = Sampler()\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:255, in Gemma2Model.__init__(self, config, cache_config, quant_config)\r\n    249 self.config = config\r\n    251 self.embed_tokens = VocabParallelEmbedding(\r\n    252     config.vocab_size,\r\n    253     config.hidden_size,\r\n    254 )\r\n--> 255 self.layers = nn.ModuleList([\r\n    256     Gemma2DecoderLayer(layer_idx, config, cache_config, quant_config)\r\n    257     for layer_idx in range(config.num_hidden_layers)\r\n    258 ])\r\n    259 self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\r\n    261 # Normalize the embedding by sqrt(hidden_size)\r\n    262 # The normalizer's data type should be downcasted to the model's\r\n    263 # data type such as bfloat16, not float32.\r\n    264 # See https://github.com/huggingface/transformers/pull/29402\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:256, in <listcomp>(.0)\r\n    249 self.config = config\r\n    251 self.embed_tokens = VocabParallelEmbedding(\r\n    252     config.vocab_size,\r\n    253     config.hidden_size,\r\n    254 )\r\n    255 self.layers = nn.ModuleList([\r\n--> 256     Gemma2DecoderLayer(layer_idx, config, cache_config, quant_config)\r\n    257     for layer_idx in range(config.num_hidden_layers)\r\n    258 ])\r\n    259 self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\r\n    261 # Normalize the embedding by sqrt(hidden_size)\r\n    262 # The normalizer's data type should be downcasted to the model's\r\n    263 # data type such as bfloat16, not float32.\r\n    264 # See https://github.com/huggingface/transformers/pull/29402\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:181, in Gemma2DecoderLayer.__init__(self, layer_idx, config, cache_config, quant_config)\r\n    179 super().__init__()\r\n    180 self.hidden_size = config.hidden_size\r\n--> 181 self.self_attn = Gemma2Attention(\r\n    182     layer_idx=layer_idx,\r\n    183     config=config,\r\n    184     hidden_size=self.hidden_size,\r\n    185     num_heads=config.num_attention_heads,\r\n    186     num_kv_heads=config.num_key_value_heads,\r\n    187     head_dim=config.head_dim,\r\n    188     max_position_embeddings=config.max_position_embeddings,\r\n    189     rope_theta=config.rope_theta,\r\n    190     cache_config=cache_config,\r\n    191     quant_config=quant_config,\r\n    192     attn_logits_soft_cap=config.attn_logit_softcapping,\r\n    193 )\r\n    194 self.hidden_size = config.hidden_size\r\n    195 self.mlp = Gemma2MLP(\r\n    196     hidden_size=self.hidden_size,\r\n    197     intermediate_size=config.intermediate_size,\r\n   (...)\r\n    200     quant_config=quant_config,\r\n    201 )\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/model_executor/models/gemma2.py:147, in Gemma2Attention.__init__(self, layer_idx, config, hidden_size, num_heads, num_kv_heads, head_dim, max_position_embeddings, rope_theta, cache_config, quant_config, attn_logits_soft_cap)\r\n    144 use_sliding_window = (layer_idx % 2 == 1\r\n    145                       and config.sliding_window is not None)\r\n    146 del use_sliding_window  # Unused.\r\n--> 147 self.attn = Attention(self.num_heads,\r\n    148                       self.head_dim,\r\n    149                       self.scaling,\r\n    150                       num_kv_heads=self.num_kv_heads,\r\n    151                       cache_config=cache_config,\r\n    152                       quant_config=quant_config,\r\n    153                       logits_soft_cap=attn_logits_soft_cap)\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/attention/layer.py:84, in Attention.__init__(self, num_heads, head_size, scale, num_kv_heads, alibi_slopes, cache_config, quant_config, blocksparse_params, logits_soft_cap, prefix)\r\n     79 attn_backend = get_attn_backend(num_heads, head_size, num_kv_heads,\r\n     80                                 sliding_window, dtype, kv_cache_dtype,\r\n     81                                 block_size, blocksparse_params\r\n     82                                 is not None)\r\n     83 impl_cls = attn_backend.get_impl_cls()\r\n---> 84 self.impl = impl_cls(num_heads, head_size, scale, num_kv_heads,\r\n     85                      alibi_slopes, sliding_window, kv_cache_dtype,\r\n     86                      blocksparse_params, logits_soft_cap)\r\n\r\nFile ~/anaconda3/envs/autorag/lib/python3.10/site-packages/vllm/attention/backends/xformers.py:422, in XFormersImpl.__init__(self, num_heads, head_size, scale, num_kv_heads, alibi_slopes, sliding_window, kv_cache_dtype, blocksparse_params, logits_soft_cap)\r\n    419     raise ValueError(\r\n    420         \\\"XFormers does not support block-sparse attention.\\\")\r\n    421 if logits_soft_cap is not None:\r\n--> 422     raise ValueError(\r\n    423         \\\"XFormers does not support attention logits soft capping.\\\")\r\n    424 self.num_heads = num_heads\r\n    425 self.head_size = head_size\r\n\r\nValueError: XFormers does not support attention logits soft capping.\"\r\n}\r\n\r\n**Code that bug is happened**\r\n\r\n```py\r\nfrom autorag.evaluator import Evaluator\r\n\r\nevaluator = Evaluator(qa_data_path='qd.parquet', corpus_data_path='corpus.parquet',\r\n                      project_dir='local_result')\r\n\r\nimport nest_asyncio\r\nnest_asyncio.apply()\r\nevaluator.start_trial(yaml_path)\r\n\r\n```\r\n\r\n```yaml\r\n# This config YAML file does not contain any optimization.\r\nnode_lines:\r\n- node_line_name: retrieve_node_line  # Arbitrary node line name\r\n  nodes:\r\n    - node_type: retrieval\r\n      strategy:\r\n        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]\r\n      top_k: 3\r\n      modules:\r\n        - module_type: vectordb\r\n          embedding_model: my_bge_model\r\n- node_line_name: post_retrieve_node_line  # Arbitrary node line name\r\n  nodes:\r\n    - node_type: prompt_maker\r\n      strategy:\r\n        metrics: [ meteor, rouge, bert_score ]\r\n      modules:\r\n        - module_type: fstring\r\n          prompt: \"주어진 문서만을 이용하여 question에 따라 답하시오. 표와 텍스트 전부 확인하여 문장형태로 답변해줘. \\n\\n {retrieved_contents} \\n\\n Question: {query} \\n\\n Answer: \"\r\n    - node_type: generator\r\n      strategy:\r\n        metrics: [ meteor, rouge, bert_score ]\r\n      modules:\r\n        - module_type: vllm\r\n          llm: google/gemma-2-9b-it\r\n          dtype : bfloat16\r\n          temperature: [ 0.1 ]\r\n          max_tokens: 400\r\n\r\n```\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Linux\r\n - Python version 3.10.14\r\n\r\nAutoRAG                                  0.2.15\r\ntorch                                    2.4.0+cu118\r\n\r\n",
      "state": "closed",
      "author": "daegonYu",
      "author_type": "User",
      "created_at": "2024-09-09T08:28:21Z",
      "updated_at": "2024-09-16T12:26:13Z",
      "closed_at": "2024-09-16T12:26:13Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/696/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/696",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/696",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:12.426521",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@daegonYu It looks like vllm error. \r\nSomethings wrong at xformers version or feature?",
          "created_at": "2024-09-09T08:32:27Z"
        },
        {
          "author": "daegonYu",
          "body": "My xformers version is 0.0.27.post2+cu118\r\nI installed it according to the installation guide on xformers GitHub.\r\n\r\n![image](https://github.com/user-attachments/assets/23ec6b6c-38db-480f-8900-ff3e476275c1)\r\n\r\nThis error isn't on the xformers github, should I ask there?\r\n",
          "created_at": "2024-09-10T00:18:00Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@daegonYu \r\nIt will be great you asked about this error at there.\r\nBecause I have never seen this error and the reason this happened is in the vllm and xformers.\r\nIf I face this issue, I will investigate it\r\n",
          "created_at": "2024-09-10T01:11:50Z"
        },
        {
          "author": "effortprogrammer",
          "body": "Yes, this issue is related with vllm. When you try to serve gemma-2 model using vllm without using Flashinfer backend, it automatically uses xformers backend. Unfortunately, xformers backend does not support attention logits soft capping. \r\n\r\nOne way that you can serve in xformers backend is removin",
          "created_at": "2024-09-12T00:38:10Z"
        },
        {
          "author": "daegonYu",
          "body": "@effortprogrammer \r\nThank you for your reply. If I want to use the Flashinfer backend, can I do it with pip install?",
          "created_at": "2024-09-12T04:38:56Z"
        }
      ]
    },
    {
      "issue_number": 699,
      "title": "[BUG] Possible bug at retrieval module when top-k 1",
      "body": "**Describe the bug**\r\nWhen benchmarking korean embedding model, there are error that top-k 1 and top-k 3 result is inconsistent.\r\n![image](https://github.com/user-attachments/assets/b4cd1ad3-c8af-4d40-8cc2-98ed70e114c3)\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nhttps://github.com/Marker-Inc-Korea/AutoRAG-example-korean-embedding-benchmark\r\nGo here and follow the instructions\r\nUsed main branch(commit=78966cc198a5b502abacaf1a3ea1e606a0209539)\r\n\r\n**Expected behavior**\r\nThe top-k 1 result must be same with the top-k 3 results top retrieval_id\r\n\r\n**Full Error log**\r\nIf applicable, add full error log to help explain your problem.\r\n\r\n**Code that bug is happened**\r\nIf applicable, add the code that bug is happened.\r\n(Especially, your AutoRAG YAML file or python codes that you wrote)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. Windows, Linux, MacOS]\r\n - Python version [e.g. 3.10]\r\n\r\n**Additional context**\r\nThis issue posted in AutoRAG Discord\r\n\r\n```\r\n안녕하세요~! 유튜브와 세미나를 참고해서 간단하게 지표를 뽑아볼 수 있었습니다. 감사합니다. 결과 페이지에서 MRR 및 MAP 지표와 같이 순서와 연관된 지표에 대해 여쭈어보고싶은 점이 있는데요.\r\nK=1 인 경우, retrieved_ids 배열의 길이는 1이고,\r\nK=3 인 경우, retrieved_ids 배열의 길이는 3이였습니다.\r\n벡터 공간에서 현재 Query와 매칭되는 vector들 중 가장 유사한 Top K개의 결과를 검색해오는 것으로 이해했습니다.\r\n\r\n그런데, K=3일 때, 검색 결과 리스트에 나열 된 순서가 정렬이 되지 않은 것을 확인했습니다.\r\n동일 임베딩 모델에 대해 K=1일 때 가져온 Top 1 문서 'A'를 확인했는데, K=3일 때 가져온 Top 3 문서는 ['C', 'B', 'A'] 와 같이 기존에 가장 최상위에 위치한 Top 1 문서 'A'가 리스트의 가장 처음(index = 0)이 아니라 끝(index = -1)에 위치해있는 것을 확인할 수 있었습니다.\r\n\r\n이러한 경우, 순서에 따른 가중치를 부여하는 MRR 및 MAP 지표에서 gt에 해당하는 문서 'A'가 리스트의 가장 맨 끝에 있기 때문에 가중치를 제대로 못받으며 계산이 되는 것 같은데요.\r\n이렇게 리스트 내의 검색 된 문서가 유사도 순으로 정렬이 되지 않은 경우, MRR 및 MAP 처럼 순서를 고려하는 지표는 의미가 없어진다고 이해해도 괜찮을까요?\r\nJeffrey Kim — 어제 오후 9:53\r\n혹시 어떤 모듈 사용하셨을때 정렬이 안되어 있었을까요? 확인 부탁드려요.\r\n또한 retrieve_scores 컬럼도 결과에서 확인이 가능하신가요? 해당 점수도 정렬이 되지 않았는지 확인 부탁드립니다\r\n김기훈_PC\r\n작성자\r\n — 어제 오후 10:10\r\nhttps://github.com/Marker-Inc-Korea/AutoRAG-example-korean-embedding-benchmark\r\n샘플 프로젝트 그대로(config도) 사용하면서 데이터만 제가 가진 데이터로 바꿔서 해봤습니다!\r\n\r\n첨부해드린 그림에서 확인하실 수 있듯이, K=3일 때 retrieve_scores 기준으로 정렬은 내림차순으로 잘 되어있습니다.\r\n흠.. 그렇다면 K=1일 때의 retrieve_scores 컬럼의 값이 최대인 문서가 조회 되지 않은 것인가요?\r\n```\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-11T02:33:23Z",
      "updated_at": "2024-09-16T11:40:38Z",
      "closed_at": "2024-09-16T11:40:37Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/699/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/699",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/699",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:12.683582",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "I confirm that there was an error.\r\nNon-ranking method does not effected, but rank aware methods and passage filter like threshold passage filter.\r\n",
          "created_at": "2024-09-16T10:03:36Z"
        },
        {
          "author": "vkehfdl1",
          "body": "close at #713",
          "created_at": "2024-09-16T11:40:37Z"
        }
      ]
    },
    {
      "issue_number": 695,
      "title": "[Parse] Write parse docs",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-09T07:41:41Z",
      "updated_at": "2024-09-16T08:55:03Z",
      "closed_at": "2024-09-16T08:55:03Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/695/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/695",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/695",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:12.926087",
      "comments": []
    },
    {
      "issue_number": 694,
      "title": "[\bChunk] Write chunk docs",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-09T07:41:03Z",
      "updated_at": "2024-09-16T08:55:03Z",
      "closed_at": "2024-09-16T08:55:03Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/694/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/694",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/694",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:12.926108",
      "comments": []
    },
    {
      "issue_number": 613,
      "title": "[Data Creation Refactoring] Re-design QA generation process",
      "body": "Current library is little bit complicated if you want to make a custom QA generation process. \r\nMaking custom QA creation process must be easy. \r\n\r\n\"The process will change a lot.”\r\n\r\nI think the legacy codes will be deprecated and delete at AutoRAG 0.3.0.\r\n\r\n\r\n- [x] #661\r\n- [x] #662\r\n- [x] #673\r\n- [x] #674\r\n- [x] #675\r\n- [x] #676\r\n- [x] #677 \r\n- [x] #683",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-13T09:02:23Z",
      "updated_at": "2024-09-16T03:49:27Z",
      "closed_at": "2024-09-16T03:49:27Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/613/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/613",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/613",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:12.926117",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It can be a kind of monad(????? but is it monad???? I don’t know).\r\nThere is a dataclass. Let's name it.... `Dataset` \r\n`Dataset` contains `qa` and `corpus`.\r\nEvery function in `Dataset` returns `Dataset` instance, but modified `Dataset` instance. \r\nEvery function must be `pure function`. Its input ",
          "created_at": "2024-08-14T03:36:05Z"
        },
        {
          "author": "vkehfdl1",
          "body": "## The basic workflow\r\n\r\n1. Parsing the raw document and make `Raw`\r\n2. Initial chunk the corpus from `Raw`\r\n3. Sample some chunks to generate questions\r\n4. Generate questions from the selected chunks\r\n5. Generate answer and finish the initial `QA` set\r\n6. Add more `Corpus` to use different chunking",
          "created_at": "2024-09-05T08:59:58Z"
        },
        {
          "author": "vkehfdl1",
          "body": "All task is done\r\nI will do the documentation at this branch for the last time\r\n@bwook00 ",
          "created_at": "2024-09-14T12:47:28Z"
        }
      ]
    },
    {
      "issue_number": 707,
      "title": "[Documentation] Add func annotation about parse module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-13T05:51:03Z",
      "updated_at": "2024-09-14T12:44:25Z",
      "closed_at": "2024-09-14T12:44:25Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/707/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/707",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/707",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.135280",
      "comments": []
    },
    {
      "issue_number": 675,
      "title": "[Data Creation Refactoring] Add `update_corpus` function at QA",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-05T09:07:25Z",
      "updated_at": "2024-09-13T12:22:10Z",
      "closed_at": "2024-09-13T12:22:10Z",
      "labels": [
        "data creation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/675/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/675",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/675",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.135301",
      "comments": []
    },
    {
      "issue_number": 655,
      "title": "refactor existing metric python files with input schema",
      "body": null,
      "state": "closed",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-08-26T10:45:03Z",
      "updated_at": "2024-09-11T10:46:45Z",
      "closed_at": "2024-09-11T10:46:45Z",
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/655/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/655",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/655",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.135309",
      "comments": []
    },
    {
      "issue_number": 703,
      "title": "[BUG] If kiwipiepy version is lower than 0.18.0, test_llama_index_chunk_sentence_node result is different",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-11T08:05:15Z",
      "updated_at": "2024-09-11T09:58:54Z",
      "closed_at": "2024-09-11T09:58:53Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/703/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/703",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/703",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.135314",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #704",
          "created_at": "2024-09-11T09:58:53Z"
        }
      ]
    },
    {
      "issue_number": 701,
      "title": "[BUG] If kiwipiepy version is lower than 0.18.0, `test_llama_index_chunk_sentence_node` result is different",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-11T07:45:06Z",
      "updated_at": "2024-09-11T08:04:40Z",
      "closed_at": "2024-09-11T08:04:40Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/701/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/701",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/701",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.335803",
      "comments": []
    },
    {
      "issue_number": 653,
      "title": "[BUG] HuggingfaceLLM is not loaded",
      "body": "## Issue\r\nWhen using a model from Huggingface, the repo ID is not correctly recognized by AutoRAG, so the model fails to load.\r\n\r\n## reproduce\r\nRun `https://github.com/Marker-Inc-Korea/AutoRAG-tutorial-ko` with any Huggingface model.\r\n```\r\n                             │ /opt/titan/conda/envs/autorag/lib/python3.10/site-packages/llama_index/llms/huggingface/base. │               \r\n                             │ py:253 in __init__                                                                            │               \r\n                             │                                                                                               │               \r\n                             │    250 │   │   print(\"self._model\", self._model)                                              │               \r\n                             │    251 │   │                                                                                  │               \r\n                             │    252 │   │   # check context_window                                                         │               \r\n                             │ ❱  253 │   │   config_dict = self._model.config.to_dict()                                     │               \r\n                             │    254 │   │   model_context_window = int(                                                    │               \r\n                             │    255 │   │   │   config_dict.get(\"max_position_embeddings\", context_window)                 │               \r\n                             │    256 │   │   )                                                                              │               \r\n                             ╰───────────────────────────────────────────────────────────────────────────────────────────────╯               \r\n                             AttributeError: 'str' object has no attribute 'config'           \r\n```\r\n\r\n\r\n## Cause analysis\r\n- `HuggingFaceLLM` class from `llama_index` requires either `model` or `model_name`, and the repo ID (a string) must be passed to HuggingFaceLLM via `model_name`, not `model`.\r\n- The current code fails to load the model because it uses `model` as the repo ID (it works okay for openai as it uses `model` for the same purpose.)\r\n\r\n\r\n## pr\r\nhttps://github.com/Marker-Inc-Korea/AutoRAG/pull/652\r\n\r\n\r\n",
      "state": "closed",
      "author": "jis478",
      "author_type": "User",
      "created_at": "2024-08-26T08:51:24Z",
      "updated_at": "2024-09-11T05:24:15Z",
      "closed_at": "2024-09-11T05:24:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/653/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/653",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/653",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.335823",
      "comments": []
    },
    {
      "issue_number": 674,
      "title": "[Data Creation Refactoring] Link chunk from Chunker",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-05T09:07:23Z",
      "updated_at": "2024-09-11T05:08:50Z",
      "closed_at": "2024-09-11T05:08:50Z",
      "labels": [
        "data creation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/674/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/674",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/674",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.489106",
      "comments": []
    },
    {
      "issue_number": 512,
      "title": "vLLM terminated unexpectedly at the multi-gpu environment",
      "body": "Hello,\r\n\r\nWhile using the ELI5 and TriviaQA datasets from the Hugging Face library, I encountered errors related to missing documents that are not present in the corpus. I experienced a similar issue with the HotpotQA dataset but managed to resolve it by cleaning the mismatched documents.\r\n\r\nHowever, when I switched to using the HotpotQA dataset, I observed some strange behavior with the VLLM module. Specifically, the process is terminated without any error messages.\r\n\r\nThank you in advance.",
      "state": "closed",
      "author": "gnekt",
      "author_type": "User",
      "created_at": "2024-06-20T08:23:58Z",
      "updated_at": "2024-09-11T04:58:59Z",
      "closed_at": "2024-09-11T04:58:59Z",
      "labels": [
        "bug",
        "help wanted",
        "High Priority"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/512/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/512",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/512",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.489128",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hello @gnekt First of all, Thanks for the report.\r\nWe'll check Eli5 and triviaQA dataset first. @bwook00 @Eastsidegunn will help this also.\r\n\r\nAnd about the termination of vLLM module. It can be OOM error. Do you check your VRAM status? \r\nAnd what file did you use, and what system you used for runni",
          "created_at": "2024-06-20T08:26:31Z"
        },
        {
          "author": "CristianCosci",
          "body": "> And what file did you use, and what system you used for running each dataset?\r\n\r\nCPU: AMD EPYC 7402P (48) @ 2.800GHz \r\nGPU0: NVIDIA GeForce RTX 3090 \r\nGPU1: NVIDIA GeForce RTX 3090 \r\nMemory: 3042MiB / 128667MiB\r\n\r\n\r\n\r\n\r\n\r\n> And about the termination of vLLM module. It can be OOM error. Do you chec",
          "created_at": "2024-06-20T08:34:56Z"
        },
        {
          "author": "CristianCosci",
          "body": "This is the last logger output when executing autorag:\r\n```bash\r\nUserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\r\n  warnings.warn(\r\n[06/20/24 08:39:54] INFO     [evaluator.py:9",
          "created_at": "2024-06-20T08:43:07Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Hi @gnekt I checked Eli5 and triviaQA dataset and the dataset had no missing doc_id in the corpus dataset. Unfortunately, I can't find the error you mentioned.\r\n\r\n@gnekt @CristianCosci Plus, about the vLLM termination, actually I can't find this error also. I think it is because the vLLM install env",
          "created_at": "2024-06-22T05:50:45Z"
        },
        {
          "author": "vkehfdl1",
          "body": "It looks like there is an error at the multi-gpu environment...",
          "created_at": "2024-08-05T06:15:26Z"
        }
      ]
    },
    {
      "issue_number": 691,
      "title": "[BUG] Does it work in a multi-GPU environment?",
      "body": "**Describe the bug**\r\nWhen I try to infer llm locally, only the 0th GPU is used. Is there a guide to using multi GPU?\r\n",
      "state": "closed",
      "author": "daegonYu",
      "author_type": "User",
      "created_at": "2024-09-09T06:47:54Z",
      "updated_at": "2024-09-11T04:58:58Z",
      "closed_at": "2024-09-11T04:58:58Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/691/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/691",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/691",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.676769",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@daegonYu \r\nHello.\r\nWe do not have multi-GPU environment yet, so we couldn't experiment multi-GPU environment yet.",
          "created_at": "2024-09-09T06:49:40Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@daegonYu \r\nHi\r\nFortunately we get multi-gpu server and investigating this issue",
          "created_at": "2024-09-09T07:01:17Z"
        },
        {
          "author": "daegonYu",
          "body": "Okay, thank you.",
          "created_at": "2024-09-09T08:09:19Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Not resolve this issue yet",
          "created_at": "2024-09-09T08:27:32Z"
        }
      ]
    },
    {
      "issue_number": 682,
      "title": "[Chunk] \bAdd langchain chunk module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-07T03:56:49Z",
      "updated_at": "2024-09-09T13:14:14Z",
      "closed_at": "2024-09-09T13:14:14Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/682/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/682",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/682",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.858279",
      "comments": []
    },
    {
      "issue_number": 683,
      "title": "[Data Creation Refactoring] Merge multiple parsing results to one Raw. Load Raw instance to Chunker.",
      "body": "1. Raw needs some kind of merge with another Raw. Because sometimes there will be a multiple parsed result and wants to use at the same RAG pipeline. \r\nSo make something like `Raw` plus override.\r\n2. Use `Raw` instance at `Chunker` `__init__()` and load from file with new classmethod `from_parquet` ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-08T00:28:08Z",
      "updated_at": "2024-09-09T07:14:05Z",
      "closed_at": "2024-09-09T07:14:05Z",
      "labels": [
        "enhancement",
        "data creation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/683/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/683",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/683",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.858298",
      "comments": []
    },
    {
      "issue_number": 684,
      "title": "[Chunk] return the chunk filepath in `parsed_result` and what index that the chunk start and end",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-08T06:56:21Z",
      "updated_at": "2024-09-09T06:59:21Z",
      "closed_at": "2024-09-09T06:59:21Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/684/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/684",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/684",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.858325",
      "comments": []
    },
    {
      "issue_number": 690,
      "title": "[Chunk] Make semantic chunks also get_start_end_idx",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-09T06:26:17Z",
      "updated_at": "2024-09-09T06:26:18Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/690/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/690",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/690",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.858330",
      "comments": []
    },
    {
      "issue_number": 689,
      "title": "[Chunk] Handling when there are multiple serch texts in the source text in `get_start_end_idx`",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-09T06:25:29Z",
      "updated_at": "2024-09-09T06:25:30Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/689/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/689",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/689",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.858335",
      "comments": []
    },
    {
      "issue_number": 688,
      "title": "[Chunk] add langchain_chunk module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-09-09T05:17:37Z",
      "updated_at": "2024-09-09T05:28:35Z",
      "closed_at": "2024-09-09T05:28:35Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/688/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/688",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/688",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:13.858341",
      "comments": [
        {
          "author": "bwook00",
          "body": "duplicate issue #682 ",
          "created_at": "2024-09-09T05:28:35Z"
        }
      ]
    },
    {
      "issue_number": 677,
      "title": "[Data Creation Refactoring] Add filters that delete unwanted QA or corpus",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-05T09:07:31Z",
      "updated_at": "2024-09-09T05:05:57Z",
      "closed_at": "2024-09-09T05:05:57Z",
      "labels": [
        "data creation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/677/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/677",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/677",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.037018",
      "comments": []
    },
    {
      "issue_number": 664,
      "title": "\bAdd llama parse module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-08-30T06:10:52Z",
      "updated_at": "2024-09-08T09:15:51Z",
      "closed_at": "2024-09-08T09:15:51Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/664/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/664",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/664",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.037035",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #666",
          "created_at": "2024-09-08T09:15:51Z"
        }
      ]
    },
    {
      "issue_number": 676,
      "title": "[Data Creation Refactoring] Add more qa generation functions",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-05T09:07:27Z",
      "updated_at": "2024-09-06T12:32:57Z",
      "closed_at": "2024-09-06T12:32:57Z",
      "labels": [
        "data creation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/676/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/676",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/676",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.265296",
      "comments": []
    },
    {
      "issue_number": 665,
      "title": "Add Hybrid table parse module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-08-30T07:24:24Z",
      "updated_at": "2024-09-06T04:06:06Z",
      "closed_at": "2024-09-06T04:06:06Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/665/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/665",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/665",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.265312",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #668 ",
          "created_at": "2024-09-06T04:06:06Z"
        }
      ]
    },
    {
      "issue_number": 673,
      "title": "[Data Creation Refactoring] Add sample functions from corpus to QA",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-09-05T09:06:54Z",
      "updated_at": "2024-09-06T02:40:45Z",
      "closed_at": "2024-09-06T02:40:45Z",
      "labels": [
        "data creation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/673/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/673",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/673",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.485354",
      "comments": []
    },
    {
      "issue_number": 662,
      "title": "[Data Creation Refactoring] Add generate qa set.",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-30T01:44:05Z",
      "updated_at": "2024-09-06T02:40:45Z",
      "closed_at": "2024-09-06T02:40:44Z",
      "labels": [
        "data creation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/662/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/662",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/662",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.485377",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Add `sample_corpus` and `make_qa` at `Corpus` schema.",
          "created_at": "2024-08-30T01:44:17Z"
        }
      ]
    },
    {
      "issue_number": 647,
      "title": "[BUG] Failed to parse yaml when tag exist",
      "body": "**Describe the bug**\r\nFailed to parse yaml when `!!python/tuple` tag exist\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Prepare pipeline.yaml including `!!python/tuple`\r\n2. Run `Runner.from_yaml()`\r\n\r\n**Expected behavior**\r\nMake runner instance successfully\r\n\r\n**Full Error log**\r\nIf applicable, add full error log to help explain your problem.\r\n```bash\r\nConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/tuple'\r\n```\r\n**Code that bug is happened**\r\nIf applicable, add the code that bug is happened.\r\n```yaml\r\nnode_lines:\r\n- node_line_name: retrieve_node_line\r\n  nodes:\r\n  - modules:\r\n    - module_type: hybrid_cc\r\n      normalize_method: mm\r\n      target_module_params: !!python/tuple\r\n      - embedding_batch: 256\r\n        embedding_model: openai\r\n        top_k: 3\r\n      - bm25_tokenizer: porter_stemmer\r\n        top_k: 3\r\n      target_modules: !!python/tuple\r\n      - vectordb\r\n      - bm25\r\n      top_k: 3\r\n      weight: 0.47000000000000003\r\n      ...\r\n```\r\n```python\r\nrunner = Runner.from_yaml(\"./1/pipeline.yaml\")\r\n```\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Linux (Ubuntu 20.04)\r\n - Python version: 3.11\r\n",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2024-08-26T01:55:04Z",
      "updated_at": "2024-09-02T14:09:48Z",
      "closed_at": "2024-09-02T14:09:48Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/647/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/647",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/647",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.660382",
      "comments": [
        {
          "author": "rjwharry",
          "body": "According to this [link](https://death.andgravity.com/yaml-unknown-tag), we need to change `yaml.safe_load` to `yaml.full_load`. It works on my local. \r\nI'll make PR for this.",
          "created_at": "2024-08-26T01:56:27Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry \r\nI have one question.\r\nWhy do you need use `!!python/tuple` at `target_module_params`?? ",
          "created_at": "2024-08-26T03:02:20Z"
        },
        {
          "author": "rjwharry",
          "body": "@vkehfdl1 \r\nIt was automatically generated when extracting pipeline.yaml, so I thought that AutoRAG was adding it.",
          "created_at": "2024-08-26T03:17:04Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry Oh really? That can be an issue. We'll check about it.",
          "created_at": "2024-08-26T04:18:32Z"
        },
        {
          "author": "rjwharry",
          "body": "I check that pyyaml add tag automatically.\r\n```python\r\nimport yaml\r\n\r\ntest = {\r\n    \"test\": (1, 2)\r\n}\r\nprint(yaml.dump(test))\r\n```\r\n```bash\r\n# Result\r\ntest: !!python/tuple\r\n- 1\r\n- 2\r\n```\r\nMy pyyaml version is 6.0.1",
          "created_at": "2024-08-29T03:20:57Z"
        }
      ]
    },
    {
      "issue_number": 377,
      "title": "Mock all OpenAI embeddings at test code",
      "body": "OpenAI embeddings can cost some money to auto-testing.\r\nMocking it for cost optimization and faster process.\r\nPlus, it will be great at new commiter who are using forked repository.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-26T06:19:06Z",
      "updated_at": "2024-08-28T14:49:36Z",
      "closed_at": "2024-08-28T14:49:36Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/377/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/377",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/377",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.866336",
      "comments": []
    },
    {
      "issue_number": 417,
      "title": "Batch API Support and Incremental Data Addition/Subtraction?",
      "body": "Hey AutoRAG devwlopers,\n\nThanks for the great work on AutoRAG. It's been a valuable tool for optimizing RAG pipelines. I thought there are a couple of feature requests that could make it even more powerful and efficient:\n\n1. Batch API Support:\nWith OpenAI's recent release of the Batch API, there's an opportunity to reduce the cost of generating QA data from corpus data by up to 50%. Integrating Batch API support into AutoRAG would be good for users working with large datasets and limited budgets. \n\n2. Incremental Data Addition/Subtraction:\nThe current AutoRAG pipeline requires creating corpus data and generating QA data from it sequentially, as I understood. However, there are scenarios where users may have additional data to incorporate into an existing RAG pipeline, or need to remove or replace it(like renewed docs) It would be great to know if there are any plans or ezisting methods to support incremental data addition in the future or how this could be implemented.\n\nThese feature requests may require significant development efforts, but they would greatly enhance the usability and efficiency of AutoRAG. If there's anything that can be done to help or provide further clarification, please let me know.\n\nThanks for considering these feature requests. Looking forward to hearing your thoughts.",
      "state": "open",
      "author": "kirillocha",
      "author_type": "User",
      "created_at": "2024-05-03T03:31:17Z",
      "updated_at": "2024-08-28T07:11:25Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/417/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/417",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/417",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:14.866351",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hi @kirillocha Thanks for making new issue.\r\n\r\nFirst, batch API looks great. We will consider to implement it to data creation process.\r\n\r\nSecond, there are some cases that users may want to use AutoRAG with their pre-ingested corpus.\r\nLike if there are millions of embeddings on their own vector DB ",
          "created_at": "2024-05-03T04:25:25Z"
        },
        {
          "author": "kirillocha",
          "body": "Thank you for addressing my suggestions and feedback in detail. I appreciate your openness to enhancing AutoRAG and your dedication to the project, as shown by your frequent commits. Your hard work is appreciated, and I'm grateful for your commitment to making AutoRAG better. Thank you for consideri",
          "created_at": "2024-05-03T07:56:46Z"
        },
        {
          "author": "vkehfdl1",
          "body": "It has beed a long time to make this issue, but we are considering to do this at refactored data creation. \r\nMaybe not AutoRAG version 0.2\r\nWill be AutoRAG version 0.3 we are working on",
          "created_at": "2024-08-28T07:11:24Z"
        }
      ]
    },
    {
      "issue_number": 640,
      "title": "[Feature Request] Delete nest_asyncio dependency at library (FastAPI server execution)",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nUsing `nest_asyncio` actually breaks the design of the `asyncio`. But we had to use it because of the FastAPI server.\r\nIn this issue, we have to figure out how to use FastAPI server and AutoRAG without using `nest_asyncio` since it is not intended usage of `asyncio` [Relevant Link](https://stackoverflow.com/questions/59740704/correct-use-constraints-of-use-of-nest-asyncio)\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nWe can just use it. Actually this issue is not that serious yet.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-25T07:27:15Z",
      "updated_at": "2024-08-28T04:00:21Z",
      "closed_at": "2024-08-28T04:00:21Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/640/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rjwharry"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/640",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/640",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:15.071326",
      "comments": [
        {
          "author": "rjwharry",
          "body": "@vkehfdl1  How about using Flask instead of FastAPI?  \r\nWe have to use ASGI(uvicorn) to run FastAPI, but cannot avoid running event loop.  \r\nSo my suggestion is using Flask instead. As you know, Flask is WSGI Application and it will not cause conflict of event loop.  \r\nI tested on my local, and it w",
          "created_at": "2024-08-27T15:50:52Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry \r\nGreat idea! It will be great you can make PR for it.\r\nI will assign to you in this issue. \r\nThank you very much\r\n",
          "created_at": "2024-08-28T02:12:33Z"
        }
      ]
    },
    {
      "issue_number": 645,
      "title": "[Document Update] Update explain about request timeout at ollama generator.",
      "body": "Please see #636 and #638\r\n\r\nWe have to update the documentation because when user run big model, it takes long time to receive answer from ollama. \r\nWe can increase request timeout at docs.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-25T11:29:03Z",
      "updated_at": "2024-08-27T13:01:13Z",
      "closed_at": "2024-08-27T13:01:13Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/645/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/645",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/645",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:15.285602",
      "comments": []
    },
    {
      "issue_number": 637,
      "title": "[BUG] Fix incorrect usage of TokenTextSplitter at data_creation/tutorial.md docs",
      "body": "**Describe the bug**\r\nIncorrect usage of TokenTextSplitter at data_creation/tutorial.md docs.\r\n```\r\nnodes = TokenTextSplitter().get_nodes_from_documents(documents=documents, chunk_size=512, chunk_overlap=128)\r\n```\r\nThe LlamaIndex docs says the chunk_size and chunk_overlap must be in init of instance.\r\n\r\n\r\n[LlamaIndex Docs](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/token_text_splitter/)\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-23T06:23:02Z",
      "updated_at": "2024-08-27T11:04:01Z",
      "closed_at": "2024-08-27T11:04:01Z",
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/637/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/637",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/637",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:15.285617",
      "comments": []
    },
    {
      "issue_number": 643,
      "title": "[Feature Request] Update LlamaIndex version up to 0.11",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nThere was major update of LlamaIndex up to 0.11 [Link](https://www.llamaindex.ai/blog/introducing-llamaindex-0-11)\r\n\r\n**Describe the solution you'd like**\r\nFirst update the LlamaIndex version at `requirements.txt` and fix all errors that occured.\r\n\r\n**Describe alternatives you've considered**\r\nConstraint the version less than 0.11\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-25T08:45:17Z",
      "updated_at": "2024-08-26T10:13:02Z",
      "closed_at": "2024-08-26T10:13:02Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/643/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/643",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/643",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:17.199288",
      "comments": []
    },
    {
      "issue_number": 654,
      "title": "add deepeval_summarization",
      "body": null,
      "state": "open",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-08-26T09:12:41Z",
      "updated_at": "2024-08-26T09:22:03Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/654/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/654",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/654",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:17.199306",
      "comments": []
    },
    {
      "issue_number": 650,
      "title": "add gallileo_prompt_injection",
      "body": null,
      "state": "open",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-08-26T08:24:09Z",
      "updated_at": "2024-08-26T09:21:54Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/650/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/650",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/650",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:17.199310",
      "comments": []
    },
    {
      "issue_number": 608,
      "title": "add deep_eval metric",
      "body": null,
      "state": "closed",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-08-12T07:00:42Z",
      "updated_at": "2024-08-26T06:27:43Z",
      "closed_at": "2024-08-26T06:27:43Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/608/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/608",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/608",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:17.199313",
      "comments": [
        {
          "author": "Eastsidegunn",
          "body": "check #648 ",
          "created_at": "2024-08-26T06:27:43Z"
        }
      ]
    },
    {
      "issue_number": 616,
      "title": "How can I add custom embedding model when running web",
      "body": "I have custom huggingface embedding model in my pipeline.yaml.  \r\nWhen running as api server, I can add my custom model like below\r\n```python\r\nautorag.embedding_models[\"custom model\"] = LazyInit(\r\n    HuggingFaceEmbedding, model_name=\"custom_model\"\r\n)\r\nrunner = Runner.from_yaml(\"./6/pipeline.yaml\")\r\nrunner.run_api_server()\r\n```\r\nBut run_web support cli only, so I can't add custom model like above.  \r\nHow can I add custom model?\r\nIs there any reason why `run_web` is not supported by the Python SDK, like `run_api_server()`?",
      "state": "closed",
      "author": "rjwharry",
      "author_type": "User",
      "created_at": "2024-08-15T05:23:04Z",
      "updated_at": "2024-08-25T08:27:13Z",
      "closed_at": "2024-08-25T08:27:13Z",
      "labels": [
        "enhancement",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/616/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rjwharry"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/616",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/616",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:17.474883",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@bwook00 You might can help this issue.\r\n\r\n@rjwharry Great point! I think there is no docs for this situation. \r\n\r\nFirst, we might do experiment and leave a comment here again. Thanks. \r\n(We missed this situation)",
          "created_at": "2024-08-15T05:29:16Z"
        },
        {
          "author": "rjwharry",
          "body": "If you're willing to work on this issue once you've decided, can I contribute?  \r\nSeems like a good fit for a “good first issue”.",
          "created_at": "2024-08-15T05:38:55Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@rjwharry That will be lovely. \r\nI will assign you to assignee.",
          "created_at": "2024-08-15T07:09:12Z"
        },
        {
          "author": "vkehfdl1",
          "body": "There is a streamlit file at `autorag/web.py` \r\nAt `cli.py` we execute the streamlit with `streamlit run web.py` command. \r\nBecause the only thing to execute streamlit was cli, we made just cli method.\r\nYou might find a way to start streamlit without using cli. ",
          "created_at": "2024-08-15T07:11:34Z"
        },
        {
          "author": "rjwharry",
          "body": "I have some suggestions and requests\r\n### Request for style guide\r\nIf you have any documentation for the style guide or the '.pylintrc' file, please let me know. My preferences are quite different from yours.\r\n### Suggestions\r\nI'm working on this issue and I noticed that there is a limitation in imp",
          "created_at": "2024-08-17T11:31:42Z"
        }
      ]
    },
    {
      "issue_number": 631,
      "title": "[trivial issue] typo in the document",
      "body": "> > @vkehfdl1 \r\n> > There is another very trivial typo in https://docs.auto-rag.com/data_creation/tutorial.html.\r\n> > \r\n> > it says `# It have to contain 'query' and 'generation_gt' columns.` but `it has to contain` instead of `it have to contain...` ...\r\n> > Idk. tbh, it is very remarkable typo to me. 😅\r\n> > \r\n> \r\n> \r\n> \r\n> \r\n> \r\n> \r\n> _Originally posted by @skmanzg in https://github.com/Marker-Inc-Korea/AutoRAG/issues/629#issuecomment-2301198157_\r\n\r\n            \r\nI thought it was too trivial so I left the comment in the other's PR but I made a issue by the request from @vkehfdl1 .\r\nSince I have no right to fix the typo in the document (ofc!), I leave the issue here only.\r\n            ",
      "state": "closed",
      "author": "skmanzg",
      "author_type": "User",
      "created_at": "2024-08-21T07:47:31Z",
      "updated_at": "2024-08-22T08:19:29Z",
      "closed_at": "2024-08-22T08:19:29Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/631/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/631",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/631",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:17.707460",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@skmanzg Thanks 👍 ",
          "created_at": "2024-08-21T07:56:58Z"
        }
      ]
    },
    {
      "issue_number": 627,
      "title": "[BUG] Making new test set from existing queries fails ",
      "body": "**Describe the bug**\r\nFollowing the [documentation](https://docs.auto-rag.com/data_creation/tutorial.html#when-you-have-existing-queries), I am trying to create a test set from existing queries. It fails though on embedding (see below), but my input dataframe has good queries as far as I can tell. \r\n\r\n**To Reproduce**\r\n\r\nSee code below.\r\n\r\n\r\n**Expected behavior**\r\nShould work as documented.\r\n\r\n**Full Error log**\r\n```\r\n---------------------------------------------------------------------------\r\nBadRequestError                           Traceback (most recent call last)\r\nCell In[43], [line 58](vscode-notebook-cell:?execution_count=43&line=58)\r\n     [55](vscode-notebook-cell:?execution_count=43&line=55) print(existing_qa_df[\"query\"])\r\n     [57](vscode-notebook-cell:?execution_count=43&line=57) llm = OpenAI(model=CHAT_MODEL, temperature=1.0)\r\n---> [58](vscode-notebook-cell:?execution_count=43&line=58) qa_df = make_qa_with_existing_queries(corpus_df, existing_qa_df, content_size=5,\r\n     [59](vscode-notebook-cell:?execution_count=43&line=59)                                       answer_creation_func=generate_answers,\r\n     [60](vscode-notebook-cell:?execution_count=43&line=60)                                       llm=llm, output_filepath=QA_FILE, cache_batch=64,\r\n     [61](vscode-notebook-cell:?execution_count=43&line=61)                                       embedding_model='openai_embed_3_large', top_k=1)\r\n     [63](vscode-notebook-cell:?execution_count=43&line=63) # Prevent truncation of cell when using display\r\n     [64](vscode-notebook-cell:?execution_count=43&line=64) pd.set_option('display.max_colwidth', None)\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/data/qacreation/base.py:140, in make_qa_with_existing_queries(corpus_df, existing_query_df, content_size, answer_creation_func, output_filepath, embedding_model, collection, upsert, random_state, cache_batch, top_k, **kwargs)\r\n    [137](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/data/qacreation/base.py:137)     collection = chroma_client.get_or_create_collection(collection_name)\r\n    [139](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/data/qacreation/base.py:139) # embed corpus_df\r\n--> [140](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/data/qacreation/base.py:140) vectordb_ingest(collection, corpus_df, embeddings)\r\n    [141](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/data/qacreation/base.py:141) vectordb_func = vectordb.__wrapped__\r\n    [142](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/data/qacreation/base.py:142) retrieved_ids, retrieve_scores = vectordb_func(existing_query_df['query'].tolist(), top_k, collection, embeddings)\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/vectordb.py:116, in vectordb_ingest(collection, corpus_data, embedding_model, embedding_batch)\r\n    [113](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/vectordb.py:113)     new_contents = openai_truncate_by_token(new_contents, openai_embedding_limit, embedding_model.model_name)\r\n    [115](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/vectordb.py:115) new_ids = new_passage['doc_id'].tolist()\r\n--> [116](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/vectordb.py:116) embedded_contents = embedding_model.get_text_embedding_batch(new_contents, show_progress=True)\r\n    [117](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/vectordb.py:117) input_batches = create_batches(api=collection._client, ids=new_ids, embeddings=embedded_contents)\r\n    [118](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/autorag/nodes/retrieval/vectordb.py:118) for batch in input_batches:\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:260, in Dispatcher.span.<locals>.wrapper(func, instance, args, kwargs)\r\n    [252](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:252) self.span_enter(\r\n    [253](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:253)     id_=id_,\r\n    [254](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:254)     bound_args=bound_args,\r\n   (...)\r\n    [257](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:257)     tags=tags,\r\n    [258](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:258) )\r\n    [259](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:259) try:\r\n--> [260](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:260)     result = func(*args, **kwargs)\r\n    [261](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:261) except BaseException as e:\r\n    [262](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:262)     self.event(SpanDropEvent(span_id=id_, err_str=str(e)))\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:332, in BaseEmbedding.get_text_embedding_batch(self, texts, show_progress, **kwargs)\r\n    [323](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:323) dispatcher.event(\r\n    [324](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:324)     EmbeddingStartEvent(\r\n    [325](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:325)         model_dict=model_dict,\r\n    [326](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:326)     )\r\n    [327](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:327) )\r\n    [328](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:328) with self.callback_manager.event(\r\n    [329](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:329)     CBEventType.EMBEDDING,\r\n    [330](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:330)     payload={EventPayload.SERIALIZED: self.to_dict()},\r\n    [331](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:331) ) as event:\r\n--> [332](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:332)     embeddings = self._get_text_embeddings(cur_batch)\r\n    [333](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:333)     result_embeddings.extend(embeddings)\r\n    [334](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:334)     event.on_end(\r\n    [335](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:335)         payload={\r\n    [336](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:336)             EventPayload.CHUNKS: cur_batch,\r\n    [337](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:337)             EventPayload.EMBEDDINGS: embeddings,\r\n    [338](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:338)         },\r\n    [339](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:339)     )\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:432, in OpenAIEmbedding._get_text_embeddings(self, texts)\r\n    [425](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:425) \"\"\"Get text embeddings.\r\n    [426](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:426) \r\n    [427](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:427) By default, this is a wrapper around _get_text_embedding.\r\n    [428](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:428) Can be overridden for batch queries.\r\n    [429](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:429) \r\n    [430](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:430) \"\"\"\r\n    [431](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:431) client = self._get_client()\r\n--> [432](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:432) return get_embeddings(\r\n    [433](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:433)     client,\r\n    [434](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:434)     texts,\r\n    [435](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:435)     engine=self._text_engine,\r\n    [436](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:436)     **self.additional_kwargs,\r\n    [437](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:437) )\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:336, in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)\r\n    [334](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:334) copy = self.copy()\r\n    [335](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:335) wrapped_f.statistics = copy.statistics  # type: ignore[attr-defined]\r\n--> [336](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:336) return copy(f, *args, **kw)\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:475, in Retrying.__call__(self, fn, *args, **kwargs)\r\n    [473](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:473) retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\r\n    [474](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:474) while True:\r\n--> [475](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:475)     do = self.iter(retry_state=retry_state)\r\n    [476](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:476)     if isinstance(do, DoAttempt):\r\n    [477](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:477)         try:\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:376, in BaseRetrying.iter(self, retry_state)\r\n    [374](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:374) result = None\r\n    [375](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:375) for action in self.iter_state.actions:\r\n--> [376](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:376)     result = action(retry_state)\r\n    [377](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:377) return result\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:398, in BaseRetrying._post_retry_check_actions.<locals>.<lambda>(rs)\r\n    [396](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:396) def _post_retry_check_actions(self, retry_state: \"RetryCallState\") -> None:\r\n    [397](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:397)     if not (self.iter_state.is_explicit_retry or self.iter_state.retry_run_result):\r\n--> [398](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:398)         self._add_action_func(lambda rs: rs.outcome.result())\r\n    [399](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:399)         return\r\n    [401](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:401)     if self.after is not None:\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:449, in Future.result(self, timeout)\r\n    [447](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:447)     raise CancelledError()\r\n    [448](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:448) elif self._state == FINISHED:\r\n--> [449](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:449)     return self.__get_result()\r\n    [451](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:451) self._condition.wait(timeout)\r\n    [453](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:453) if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:401, in Future.__get_result(self)\r\n    [399](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:399) if self._exception:\r\n    [400](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:400)     try:\r\n--> [401](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:401)         raise self._exception\r\n    [402](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:402)     finally:\r\n    [403](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:403)         # Break a reference cycle with the exception in self._exception\r\n    [404](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/concurrent/futures/_base.py:404)         self = None\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:478, in Retrying.__call__(self, fn, *args, **kwargs)\r\n    [476](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:476) if isinstance(do, DoAttempt):\r\n    [477](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:477)     try:\r\n--> [478](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:478)         result = fn(*args, **kwargs)\r\n    [479](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:479)     except BaseException:  # noqa: B902\r\n    [480](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/tenacity/__init__.py:480)         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:180, in get_embeddings(client, list_of_text, engine, **kwargs)\r\n    [176](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:176) assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\r\n    [178](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:178) list_of_text = [text.replace(\"\\n\", \" \") for text in list_of_text]\r\n--> [180](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:180) data = client.embeddings.create(input=list_of_text, model=engine, **kwargs).data\r\n    [181](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:181) return [d.embedding for d in data]\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:114, in Embeddings.create(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\r\n    [108](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:108)         embedding.embedding = np.frombuffer(  # type: ignore[no-untyped-call]\r\n    [109](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:109)             base64.b64decode(data), dtype=\"float32\"\r\n    [110](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:110)         ).tolist()\r\n    [112](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:112)     return obj\r\n--> [114](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:114) return self._post(\r\n    [115](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:115)     \"/embeddings\",\r\n    [116](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:116)     body=maybe_transform(params, embedding_create_params.EmbeddingCreateParams),\r\n    [117](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:117)     options=make_request_options(\r\n    [118](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:118)         extra_headers=extra_headers,\r\n    [119](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:119)         extra_query=extra_query,\r\n    [120](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:120)         extra_body=extra_body,\r\n    [121](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:121)         timeout=timeout,\r\n    [122](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:122)         post_parser=parser,\r\n    [123](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:123)     ),\r\n    [124](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:124)     cast_to=CreateEmbeddingResponse,\r\n    [125](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/resources/embeddings.py:125) )\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1259, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\r\n   [1245](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1245) def post(\r\n   [1246](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1246)     self,\r\n   [1247](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1247)     path: str,\r\n   (...)\r\n   [1254](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1254)     stream_cls: type[_StreamT] | None = None,\r\n   [1255](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1255) ) -> ResponseT | _StreamT:\r\n   [1256](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1256)     opts = FinalRequestOptions.construct(\r\n   [1257](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1257)         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\r\n   [1258](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1258)     )\r\n-> [1259](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1259)     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:936, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    [927](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:927) def request(\r\n    [928](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:928)     self,\r\n    [929](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:929)     cast_to: Type[ResponseT],\r\n   (...)\r\n    [934](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:934)     stream_cls: type[_StreamT] | None = None,\r\n    [935](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:935) ) -> ResponseT | _StreamT:\r\n--> [936](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:936)     return self._request(\r\n    [937](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:937)         cast_to=cast_to,\r\n    [938](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:938)         options=options,\r\n    [939](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:939)         stream=stream,\r\n    [940](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:940)         stream_cls=stream_cls,\r\n    [941](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:941)         remaining_retries=remaining_retries,\r\n    [942](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:942)     )\r\n\r\nFile ~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1040, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n   [1037](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1037)         err.response.read()\r\n   [1039](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1039)     log.debug(\"Re-raising status error\")\r\n-> [1040](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1040)     raise self._make_status_error_from_response(err.response) from None\r\n   [1042](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1042) return self._process_response(\r\n   [1043](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1043)     cast_to=cast_to,\r\n   [1044](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1044)     options=options,\r\n   (...)\r\n   [1048](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1048)     retries_taken=options.get_max_retries(self.max_retries) - retries,\r\n   [1049](https://file+.vscode-resource.vscode-cdn.net/Users/matthewharris/Desktop/git/llm_eval_research/~/opt/miniconda3/envs/eval_demo/lib/python3.11/site-packages/openai/_base_client.py:1049) )\r\n\r\nBadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\r\n\r\n```\r\n**Code that bug is happened**\r\n\r\n```\r\nTEST_SET = f\"{DATA_DIR}/Evaluation Test Set.csv\"\r\n\r\n# Fix encoding when reading csv file\r\nexisting_qa_df = pd.read_csv(TEST_SET, encoding='ascii', encoding_errors='ignore')\r\n\r\n# Promote top row to column index\r\nexisting_qa_df.columns = existing_qa_df.iloc[1]\r\n\r\n# Remove top two rows, then make top row column names\r\nexisting_qa_df = existing_qa_df[2:]\r\n\r\n# It have to contain 'query' column\r\nexisting_qa_df = existing_qa_df.rename(columns={'Question': 'query'})\r\nexisting_qa_df = existing_qa_df.iloc[0:5]\r\nprint(existing_qa_df[\"query\"])\r\n\r\nllm = OpenAI(model=CHAT_MODEL, temperature=1.0)\r\nqa_df = make_qa_with_existing_queries(corpus_df, existing_qa_df, content_size=5,\r\n                                      answer_creation_func=generate_answers,\r\n                                      llm=llm, output_filepath=QA_FILE, cache_batch=64,\r\n                                      embedding_model='openai_embed_3_large', top_k=1)\r\n```\r\n\r\nThis outputs ...\r\n\r\n```\r\n2                                         What were the effects of the pandemic on WFP activities in the different countries?\r\n3    Was WFP able to maintaing its strategic positioning  during the pandemic  vis--vis the Government and other UN agencies?\r\n4                                                  What are the common findings on WFP's adaptation in response to COVID-19? \r\n5                                 To what extent were interventions effective in helping to mitigate the effects of COVID-19?\r\n6                                                            Was WFP timely in responding to needs under COVID-19 conditions?\r\nName: query, dtype: object\r\n```\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: MacOS, running  miniconda\r\n - Python version 3.11.4\r\n - AutoRAG                                  0.2.14\r\n\r\n**Additional context**\r\nThanks!\r\n",
      "state": "closed",
      "author": "dividor",
      "author_type": "User",
      "created_at": "2024-08-20T22:32:29Z",
      "updated_at": "2024-08-21T13:59:06Z",
      "closed_at": "2024-08-21T13:59:06Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/627/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/627",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/627",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:17.906472",
      "comments": [
        {
          "author": "dividor",
          "body": "This was because I had a text node with whitespace, I think it then failed on calling OpenAI embedding model. I removed the offending node, worked.\r\n\r\nWould be cool to perhaps add some validation, so will leave open, but feel free to close otherwise.",
          "created_at": "2024-08-21T02:44:57Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Yes it happens often. \r\nIt seems good idea to add 'delete whitespace' or 'drop the row that have whitespace'. \r\nWe will work on it at `vectordb_ingest` ",
          "created_at": "2024-08-21T03:02:17Z"
        },
        {
          "author": "dividor",
          "body": "Thanks!",
          "created_at": "2024-08-21T04:12:15Z"
        }
      ]
    },
    {
      "issue_number": 624,
      "title": "[Feature Request] Refactor CONTRIBUTING guide",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nThere is no contributing guide. Need to make one of it.\r\n\r\n**Describe the solution you'd like**\r\nI will write it\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-20T06:31:52Z",
      "updated_at": "2024-08-21T10:19:54Z",
      "closed_at": "2024-08-21T10:19:53Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/624/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/624",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/624",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.103004",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #625 ",
          "created_at": "2024-08-21T10:19:53Z"
        }
      ]
    },
    {
      "issue_number": 621,
      "title": "[Feature Request] Add pylint or styling guide for consistent code format (with contributors)",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nThe maintainers have to reformat again when there are some contributors because we use Pycharm code reformatting. \r\n\r\n**Describe the solution you'd like**\r\nConsider to use [ruff](https://github.com/astral-sh/ruff).\r\nSuper fast rust-based python linter. \r\n\r\n**Describe alternatives you've considered**\r\nWe can use `black` as well.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-19T02:05:08Z",
      "updated_at": "2024-08-21T04:04:29Z",
      "closed_at": "2024-08-21T04:04:29Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/621/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/621",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/621",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.316002",
      "comments": []
    },
    {
      "issue_number": 618,
      "title": "Add optional parameter 'exist_generation_gt' at `make_qa_with_existing_queries` function",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-08-16T04:37:55Z",
      "updated_at": "2024-08-21T03:00:17Z",
      "closed_at": "2024-08-21T03:00:17Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/618/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/618",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/618",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.316022",
      "comments": []
    },
    {
      "issue_number": 498,
      "title": "Experiment on OpenAI free tier ",
      "body": "Can OpenAI free tier run tutorial YAML file?? \r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-17T12:58:35Z",
      "updated_at": "2024-08-14T03:55:02Z",
      "closed_at": null,
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/498/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/498",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/498",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.316030",
      "comments": []
    },
    {
      "issue_number": 586,
      "title": "Update README.md pictures for new version v0.2.11",
      "body": "- Hybrid module changes\r\n- Colab link issue #585 ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-25T02:47:47Z",
      "updated_at": "2024-08-05T13:38:07Z",
      "closed_at": "2024-08-05T13:38:07Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/586/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/586",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/586",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.316039",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@bwook00 please change the excalidraw image. I don’t have the original one.",
          "created_at": "2024-08-04T08:10:01Z"
        }
      ]
    },
    {
      "issue_number": 495,
      "title": "Add other bm25 tokenizer for Korean",
      "body": "There are other Korean tokenizer for BM25 like Kkma or Oct",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-16T03:46:08Z",
      "updated_at": "2024-08-05T11:57:34Z",
      "closed_at": "2024-08-05T11:57:34Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/495/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/495",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/495",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.529712",
      "comments": []
    },
    {
      "issue_number": 560,
      "title": "Refactor documentation for better understanding",
      "body": "Thanks for Filipp who leave kind comment \r\n\r\n---\r\n\r\nFilipp Trigub — 2024.07.05. 오전 1:32\r\nHi @Jeffrey Kim, thanks for the quick response.\r\n\r\nIn a nutshell I would like a more complete docu. As a dev, I understand that this is not high priority at all (and annoying). \r\n\r\nAs a user however, I am truly perplex, why this project is not much more widely known. Difficulties in setting it up and running it might be one explanation. Never underestimate the laziness of tech people 😄 \r\n\r\nSome small examples I've encountered:\r\nthe prompt maker node https://docs.auto-rag.com/nodes/prompt_maker/prompt_maker.html lists generation modules as a setting. I believe it is optional, but that is not clear at all. Would wish for clear distinction between optional and mandatory settings.\r\nin https://docs.auto-rag.com/data_creation/data_format.html it is not clearly indicated that the doc_id in corpus must be one of the ids in retriaval_gt. Is that not obvious? On it's own, probably, but when you are already dealing with all the other info, it might get lost. Happened to me.\r\nin https://docs.auto-rag.com/optimization/optimization.html is is not clear, which nodes are mandatory and which ones are not (or maybe they all are and I am mistaken?).\r\n\r\nAdditionally, but not of any importance:\r\nlink at llama_index is outdated here https://docs.auto-rag.com/nodes/generator/llama_index_llm.html and here https://docs.auto-rag.com/local_model.html#supporting-llm-models\r\n\r\nHope that helps. I really think, people should use AutoRAG much more and the good thing is that it is actually easy to use, once you've understood the nuances! \r\n\r\n---\r\n\r\nLet's fix this! ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-08T00:40:31Z",
      "updated_at": "2024-08-04T14:11:31Z",
      "closed_at": "2024-08-04T14:11:31Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/560/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/560",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/560",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.529726",
      "comments": []
    },
    {
      "issue_number": 522,
      "title": "Compatible with latest numpy 2.0.0",
      "body": null,
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-24T02:57:07Z",
      "updated_at": "2024-08-04T08:56:25Z",
      "closed_at": null,
      "labels": [
        "dependencies"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/522/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/522",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/522",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.529731",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "LlamaIndex ,Langchain, and chromadb not supporting numpy2.\r\nSo it might be a long wait for supporting this new version",
          "created_at": "2024-08-04T08:56:25Z"
        }
      ]
    },
    {
      "issue_number": 592,
      "title": "Resolve new version error of cohere reranker",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-08-03T03:04:15Z",
      "updated_at": "2024-08-03T03:19:51Z",
      "closed_at": "2024-08-03T03:19:51Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/592/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/592",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/592",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.773414",
      "comments": []
    },
    {
      "issue_number": 589,
      "title": "Clear cache after evaluating LM-based generation metric",
      "body": "At small VRAM gpu like titan xp or gtx 1080 ti (12GB, 11GB of VRAM). The OOM error occured because of bert_score.\r\n\r\nBLEU, ROUGE, and METEOR is n-gram based, so it doesn’t matter.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-26T04:17:13Z",
      "updated_at": "2024-07-29T05:33:39Z",
      "closed_at": "2024-07-29T05:33:39Z",
      "labels": [
        "High Priority",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/589/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/589",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/589",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.773435",
      "comments": []
    },
    {
      "issue_number": 573,
      "title": "Get relevance score of all passages when execute hybrid retrieval ",
      "body": "When we run hybrid retrieval, the non-duplicated passage that do not have relevance score turns to be zero.\r\nBut that is not precise way to calculate hybrid retrieval score.\r\n\r\nSo, getting relevance score when the passage is not the top-k passage is essential for more precise hybrid retrieval.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-15T03:26:33Z",
      "updated_at": "2024-07-23T02:58:01Z",
      "closed_at": "2024-07-23T02:58:01Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/573/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/573",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/573",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.773443",
      "comments": []
    },
    {
      "issue_number": 557,
      "title": "Redesign hybrid-cc related retrievalmodule",
      "body": "The current hybrid module have some problems. It is all hybrid cc (convex combination) related modules except RRF. \r\nI think we need to change and re-design a whole module. \r\nHere is my proposed design.\r\n\r\n=> [related paper](https://arxiv.org/pdf/2210.11934)\r\n\r\n## 1. Make hybrid_cc module to one and get an 'normalize' parameter.\r\n\r\nAll hybrid_cc related module have different only at the normalization process. If you select different normalization method, the result will be slightly different. Keep this as parameter. \r\n\r\nWe must support below normalization method.\r\n\r\n- mm : Min-Max normalization\r\n- tmm : Theoretical min-max normalization\r\n- z : z-score normalization\r\n- dbsf : 3-sigma normalization\r\n\r\n## 2. Test **many** weights at once\r\n\r\nWe get semantic and lexical scores from previous run. So, fusion takes so little time to compute. But, fusion quality differs so much , up to 0.2 in NDCG.\r\nSo, it if crucial to do experiment on the many weights. \r\nIt will be great we set the weights quantity and do experiment on that. \r\nLike, setting it as 100, we will experiment each weights from 0.0 to 1.0 a hundred times. \r\n![image](https://github.com/Marker-Inc-Korea/AutoRAG/assets/28955029/4dfed8c4-6f92-4693-8a01-7421c159e690)\r\n\r\nThe actual weight optimization will be in the module execution. As result, you will only see the result of the optimization. Remain as you set the weight hyperparameter. Let me think how to save the great weight parameter at the process.\r\n\r\n## 3. Specify which module is lexical and which module is semantic.\r\n\r\nWe have to specify it since we will implement tmm normalization. \r\nWe have to test the name of the retrieval module, and set the theoretical minimum. \r\nThis way, it will much clearer.\r\n\r\n## 4. Set min and max value to the both \r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-04T13:38:29Z",
      "updated_at": "2024-07-15T08:01:47Z",
      "closed_at": "2024-07-15T08:01:47Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/557/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/557",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/557",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.773452",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "5. Do not let each retrieval have missing document score.",
          "created_at": "2024-07-11T08:32:52Z"
        }
      ]
    },
    {
      "issue_number": 569,
      "title": "Implement AutoRAG-HP method",
      "body": "No more greedy search.\r\n\r\nWhy don’t we use reward model for AutoRAG?\r\n\r\nhttps://arxiv.org/pdf/2406.19251\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-09T04:47:34Z",
      "updated_at": "2024-07-15T03:27:43Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/569/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/569",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/569",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:18.984709",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It can be a long-term project for AutoRAG.\r\nNeed to fix or add run.py and Evaluator",
          "created_at": "2024-07-15T03:27:41Z"
        }
      ]
    },
    {
      "issue_number": 563,
      "title": "Save as cache file while the data generation",
      "body": "If there is an error, the whole data generation process is gone. It is super annoying, especially you want to make big dataset.\r\n\r\nSo, the feature that creates cache file while data generation is crucial. To save some data while the generation when there is a crucial error.\r\n\r\n\r\n---\r\n\r\nThe initial feature request (by 호호) as Korean\r\n\r\n```\r\n그리고 커스텀하게 만들다가 중간에 에러가 나는 경우가 있어서 살펴보니, LLM (gpt-4o를 사용해도)에서 Q&A pair를 생성할 때\r\n[Q]\r\n[A]의 형태가 아니고\r\n[Q1]\r\n[A1]\r\n의 형태로 답하는 경우가 가끔씩 있더라구요. (few-shot으로 프롬프트를 구성하면 경우가 거의 없긴합니다.)\r\n그래서 답변을 파싱할 때 에러가 나서.. 생성 중이던 데이터가 모두 날아가는 경험을 한 적이 있습니다 ㅠ (1만여개 넘는 데이터 생성 중이었는데 다 없어졌습니다)\r\n\r\n데이터를 로컬에 계속 저장해가면서 진행할 수 있으면, 어떠한 오류로 실패되더라도 데이터는 좀 살릴 수 있을 것 같아서 의견드립니다..!\r\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-08T05:28:50Z",
      "updated_at": "2024-07-09T01:13:20Z",
      "closed_at": "2024-07-09T01:13:20Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/563/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/563",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/563",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:19.201219",
      "comments": []
    },
    {
      "issue_number": 561,
      "title": "[HotFix] Duplicate qid at make_single_content_qa",
      "body": "```\r\ncorpus_df = pd.read_parquet('./corpus_custom_splitter.parquet')\r\nllm = OpenAI(model='gpt-4o', temperature=1.0)\r\nqa_df = make_single_content_qa(corpus_df, content_size=500, qa_creation_func=generate_qa_llama_index,\r\n                               llm=llm, prompt=prompt, question_num_per_content=2)\r\nqa_df.to_parquet('./qa.parquet’)\r\n```\r\n\r\nIn here, it makes duplicate qid.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-07-08T05:14:16Z",
      "updated_at": "2024-07-08T06:00:15Z",
      "closed_at": "2024-07-08T06:00:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/561/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/561",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/561",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:19.201234",
      "comments": []
    },
    {
      "issue_number": 553,
      "title": "[HotFix] build failed mecab-ko",
      "body": "build failed mecab-ko\r\nsacrebleu for Korean need mecab-ko in the library(sacrebleu[ko]).\r\n\r\n",
      "state": "closed",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-07-03T07:08:48Z",
      "updated_at": "2024-07-04T00:54:03Z",
      "closed_at": "2024-07-04T00:54:02Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/553/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/553",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/553",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:19.201241",
      "comments": [
        {
          "author": "Eastsidegunn",
          "body": "```\r\nRequirement already satisfied: lxml in ./.venv/lib/python3.10/site-packages (from sacrebleu[ko]) (5.2.2)\r\nCollecting mecab-ko<=1.0.1,>=1.0.0 (from sacrebleu[ko])\r\n  Using cached mecab-ko-1.0.1.tar.gz (75 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting mecab-ko-dic<2.0,>=1.0 (from sacr",
          "created_at": "2024-07-03T07:48:57Z"
        }
      ]
    },
    {
      "issue_number": 418,
      "title": "Separate BLEU and SacreBLEU score",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-05-03T08:52:13Z",
      "updated_at": "2024-07-03T09:56:03Z",
      "closed_at": null,
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/418/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/418",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/418",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:19.415454",
      "comments": [
        {
          "author": "Eastsidegunn",
          "body": "in sacrebleu precision calculation, multiply it by 100.",
          "created_at": "2024-07-01T08:18:40Z"
        },
        {
          "author": "Eastsidegunn",
          "body": "The currently implemented bleu will be renamed to sacrebleu and another bleu will be implemented.",
          "created_at": "2024-07-01T08:19:28Z"
        }
      ]
    },
    {
      "issue_number": 544,
      "title": "Add batch parameter at tart.md",
      "body": "No explain about it OTL",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-28T07:28:30Z",
      "updated_at": "2024-07-02T12:12:16Z",
      "closed_at": "2024-07-02T12:12:16Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/544/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/544",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/544",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:19.676436",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Also monot5.md",
          "created_at": "2024-06-28T07:29:03Z"
        },
        {
          "author": "vkehfdl1",
          "body": "And any other all reranker methods (except UPR)",
          "created_at": "2024-06-28T07:30:32Z"
        }
      ]
    },
    {
      "issue_number": 548,
      "title": "In dashboard some module section did not show up",
      "body": "I can't upload whole result data because of the NDA. But it was my experience, so AutoRAG team can reproduce.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-29T11:08:37Z",
      "updated_at": "2024-07-02T06:41:56Z",
      "closed_at": "2024-07-02T06:41:56Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/548/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/548",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/548",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:19.869000",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "The dashboard have no problem,\r\nThere was no retrieval node line at trial summary df.\r\nWhy? I don't know. I can't reproduce it.",
          "created_at": "2024-07-02T06:41:56Z"
        }
      ]
    },
    {
      "issue_number": 517,
      "title": "Add option for BLEU score",
      "body": "Has there been thought into implementing cumulative n-gram comparison like BLEU-4? (across 1-4 gram?)\r\nI know the corpus_bleu and sentence_bleu implementations in nltk are quite popular",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-23T04:09:46Z",
      "updated_at": "2024-07-01T12:31:37Z",
      "closed_at": "2024-07-01T12:31:37Z",
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/517/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/517",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/517",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:20.110919",
      "comments": []
    },
    {
      "issue_number": 550,
      "title": "Add ollama samle YAML file",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-30T07:30:49Z",
      "updated_at": "2024-06-30T11:53:44Z",
      "closed_at": "2024-06-30T11:53:44Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/550/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/550",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/550",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:20.110940",
      "comments": []
    },
    {
      "issue_number": 545,
      "title": "[HotFIx] Resolve error at flag_embedding_run_model",
      "body": "```\r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/passagereranker/fl │               \r\n                             │ ag_embedding.py:38 in flag_embedding_reranker                            │               \r\n                             │                                                                          │               \r\n                             │   35 │   │   model_name_or_path=model_name, use_fp16=use_fp16            │               \r\n                             │   36 │   )                                                               │               \r\n                             │   37 │   nested_list = [list(map(lambda x: [query, x], content_list)) fo │               \r\n                             │      in zip(queries, contents_list)]                                     │               \r\n                             │ ❱ 38 │   rerank_scores = flatten_apply(flag_embedding_run_model, nested_ │               \r\n                             │      batch_size=batch)                                                   │               \r\n                             │   39 │                                                                   │               \r\n                             │   40 │   df = pd.DataFrame({                                             │               \r\n                             │   41 │   │   'contents': contents_list,                                  │               \r\n                             │                                                                          │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/utils/util.py:335 in     │               \r\n                             │ flatten_apply                                                            │               \r\n                             │                                                                          │               \r\n                             │   332 │   \"\"\"                                                            │               \r\n                             │   333 │   df = pd.DataFrame({'col1': nested_list})                       │               \r\n                             │   334 │   df = df.explode('col1')                                        │               \r\n                             │ ❱ 335 │   df['result'] = func(df['col1'].tolist(), **kwargs)             │               \r\n                             │   336 │   return df.groupby(level=0, sort=False)['result'].apply(list).t │               \r\n                             │   337                                                                    │               \r\n                             │   338                                                                    │               \r\n                             │                                                                          │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/passagereranker/fl │               \r\n                             │ ag_embedding.py:64 in flag_embedding_run_model                           │               \r\n                             │                                                                          │               \r\n                             │   61 │   │   if batch_size == 1:                                         │               \r\n                             │   62 │   │   │   results.append(pred_scores)                             │               \r\n                             │   63 │   │   else:                                                       │               \r\n                             │ ❱ 64 │   │   │   results.extend(pred_scores)                             │               \r\n                             │   65 │   return results                                                  │               \r\n                             │   66                                                                     │               \r\n                             ╰──────────────────────────────────────────────────────────────────────────╯               \r\n                             TypeError: 'float' object is not iterable \r\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-28T09:17:04Z",
      "updated_at": "2024-06-28T09:21:53Z",
      "closed_at": "2024-06-28T09:21:53Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/545/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/545",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/545",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:20.110948",
      "comments": []
    },
    {
      "issue_number": 438,
      "title": "Decorator that convert pd.Series or np.ndarray to list recursively.",
      "body": "In autorag, we use python default list when data processing.\r\nWhen we use pandas or numpy, it is really easy that lists convert to np.array, tuple, or pd.Series, and it occurs unexpect error easily.\r\n\r\nDevelop decorator that converts all Iterable types to list, even the 2-d or 3-d list recursively. \r\nAnd using it on the functions for preventing that kind of errors.\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-15T07:08:21Z",
      "updated_at": "2024-06-28T06:30:14Z",
      "closed_at": "2024-06-28T06:30:14Z",
      "labels": [
        "High Priority"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/438/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/438",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/438",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:20.110955",
      "comments": []
    },
    {
      "issue_number": 539,
      "title": "Error when specify strategy on query expansion",
      "body": "YAML file\r\n\r\n```\r\n    - node_type: query_expansion\r\n      strategy:\r\n        metrics: [ retrieval_recall, retrieval_f1, retrieval_precision, retrieval_ndcg, retrieval_map, retrieval_mrr ]\r\n        strategy: rank\r\n        top_k: 20\r\n        retrieval_modules:\r\n          - module_type: bm25\r\n            bm25_tokenizer: ko_kiwi\r\n          - module_type: vectordb\r\n            embedding_model: openai_embed_3_large\r\n      modules:\r\n        - module_type: pass_query_expansion\r\n        - module_type: hyde\r\n```\r\n\r\n—\r\n\r\nError code\r\n\r\n```\r\n                    ERROR    [__init__.py:75] >> Unexpected exception                                                                                                  __init__.py:75\r\n                             ╭────────────────────────────────────────────────── Traceback (most recent call last) ──────────────────────────────────────────────────╮               \r\n                             │ /usr/local/bin/autorag:8 in <module>                                                                                                  │               \r\n                             │                                                                                                                                       │               \r\n                             │   5 from autorag.cli import cli                                                                                                       │               \r\n                             │   6 if __name__ == '__main__':                                                                                                        │               \r\n                             │   7 │   sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])                                                              │               \r\n                             │ ❱ 8 │   sys.exit(cli())                                                                                                               │               \r\n                             │   9                                                                                                                                   │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1157 in __call__                                                                │               \r\n                             │                                                                                                                                       │               \r\n                             │   1154 │                                                                                                                              │               \r\n                             │   1155 │   def __call__(self, *args: t.Any, **kwargs: t.Any) -> t.Any:                                                                │               \r\n                             │   1156 │   │   \"\"\"Alias for :meth:`main`.\"\"\"                                                                                          │               \r\n                             │ ❱ 1157 │   │   return self.main(*args, **kwargs)                                                                                      │               \r\n                             │   1158                                                                                                                                │               \r\n                             │   1159                                                                                                                                │               \r\n                             │   1160 class Command(BaseCommand):                                                                                                    │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1078 in main                                                                    │               \r\n                             │                                                                                                                                       │               \r\n                             │   1075 │   │   try:                                                                                                                   │               \r\n                             │   1076 │   │   │   try:                                                                                                               │               \r\n                             │   1077 │   │   │   │   with self.make_context(prog_name, args, **extra) as ctx:                                                       │               \r\n                             │ ❱ 1078 │   │   │   │   │   rv = self.invoke(ctx)                                                                                      │               \r\n                             │   1079 │   │   │   │   │   if not standalone_mode:                                                                                    │               \r\n                             │   1080 │   │   │   │   │   │   return rv                                                                                              │               \r\n                             │   1081 │   │   │   │   │   # it's not safe to `ctx.exit(rv)` here!                                                                    │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1688 in invoke                                                                  │               \r\n                             │                                                                                                                                       │               \r\n                             │   1685 │   │   │   │   super().invoke(ctx)                                                                                            │               \r\n                             │   1686 │   │   │   │   sub_ctx = cmd.make_context(cmd_name, args, parent=ctx)                                                         │               \r\n                             │   1687 │   │   │   │   with sub_ctx:                                                                                                  │               \r\n                             │ ❱ 1688 │   │   │   │   │   return _process_result(sub_ctx.command.invoke(sub_ctx))                                                    │               \r\n                             │   1689 │   │                                                                                                                          │               \r\n                             │   1690 │   │   # In chain mode we create the contexts step by step, but after the                                                     │               \r\n                             │   1691 │   │   # base command has been invoked.  Because at that point we do not                                                      │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1434 in invoke                                                                  │               \r\n                             │                                                                                                                                       │               \r\n                             │   1431 │   │   │   echo(style(message, fg=\"red\"), err=True)                                                                           │               \r\n                             │   1432 │   │                                                                                                                          │               \r\n                             │   1433 │   │   if self.callback is not None:                                                                                          │               \r\n                             │ ❱ 1434 │   │   │   return ctx.invoke(self.callback, **ctx.params)                                                                     │               \r\n                             │   1435 │                                                                                                                              │               \r\n                             │   1436 │   def shell_complete(self, ctx: Context, incomplete: str) -> t.List[\"CompletionItem\"]:                                       │               \r\n                             │   1437 │   │   \"\"\"Return a list of completions for the incomplete value. Looks                                                        │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:783 in invoke                                                                   │               \r\n                             │                                                                                                                                       │               \r\n                             │    780 │   │                                                                                                                          │               \r\n                             │    781 │   │   with augment_usage_errors(__self):                                                                                     │               \r\n                             │    782 │   │   │   with ctx:                                                                                                          │               \r\n                             │ ❱  783 │   │   │   │   return __callback(*args, **kwargs)                                                                             │               \r\n                             │    784 │                                                                                                                              │               \r\n                             │    785 │   def forward(                                                                                                               │               \r\n                             │    786 │   │   __self, __cmd: \"Command\", *args: t.Any, **kwargs: t.Any  # noqa: B902                                                  │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/cli.py:35 in evaluate                                                                 │               \r\n                             │                                                                                                                                       │               \r\n                             │    32 │   if not os.path.exists(config):                                                                                              │               \r\n                             │    33 │   │   raise ValueError(f\"Config file {config} does not exist.\")                                                               │               \r\n                             │    34 │   evaluator = Evaluator(qa_data_path, corpus_data_path, project_dir=project_dir)                                              │               \r\n                             │ ❱  35 │   evaluator.start_trial(config)                                                                                               │               \r\n                             │    36 │   logger.info('Evaluation complete.')                                                                                         │               \r\n                             │    37                                                                                                                                 │               \r\n                             │    38                                                                                                                                 │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/evaluator.py:98 in start_trial                                                        │               \r\n                             │                                                                                                                                       │               \r\n                             │    95 │   │   │   if i == 0:                                                                                                          │               \r\n                             │    96 │   │   │   │   previous_result = self.qa_data                                                                                  │               \r\n                             │    97 │   │   │   logger.info(f'Running node line {node_line_name}...')                                                               │               \r\n                             │ ❱  98 │   │   │   previous_result = run_node_line(node_line, node_line_dir, previous_result)                                          │               \r\n                             │    99 │   │   │                                                                                                                       │               \r\n                             │   100 │   │   │   trial_summary_df = self._append_node_line_summary(node_line_name,                                                   │               \r\n                             │       node_line_dir, trial_summary_df)                                                                                                │               \r\n                             │   101                                                                                                                                 │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/node_line.py:45 in run_node_line                                                      │               \r\n                             │                                                                                                                                       │               \r\n                             │   42 │                                                                                                                                │               \r\n                             │   43 │   summary_lst = []                                                                                                             │               \r\n                             │   44 │   for node in nodes:                                                                                                           │               \r\n                             │ ❱ 45 │   │   previous_result = node.run(previous_result, node_line_dir)                                                               │               \r\n                             │   46 │   │   node_summary_df = load_summary_file(os.path.join(node_line_dir, node.node_type,                                          │               \r\n                             │      'summary.csv'))                                                                                                                  │               \r\n                             │   47 │   │   best_node_row = node_summary_df.loc[node_summary_df['is_best']]                                                          │               \r\n                             │   48 │   │   summary_lst.append({                                                                                                     │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/schema/node.py:57 in run                                                              │               \r\n                             │                                                                                                                                       │               \r\n                             │    54 │   def run(self, previous_result: pd.DataFrame, node_line_dir: str) -> pd.DataFrame:                                           │               \r\n                             │    55 │   │   logger.info(f'Running node {self.node_type}...')                                                                        │               \r\n                             │    56 │   │   input_modules, input_params = self.get_param_combinations()                                                             │               \r\n                             │ ❱  57 │   │   return self.run_node(modules=input_modules,                                                                             │               \r\n                             │    58 │   │   │   │   │   │   │    module_params=input_params,                                                                        │               \r\n                             │    59 │   │   │   │   │   │   │    previous_result=previous_result,                                                                   │               \r\n                             │    60 │   │   │   │   │   │   │    node_line_dir=node_line_dir,                                                                       │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/queryexpansion/run.py:94 in run_query_expansion_node                            │               \r\n                             │                                                                                                                                       │               \r\n                             │    91 │   │   retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"),                                         │               \r\n                             │       engine='pyarrow')['retrieval_gt'].tolist()                                                                                      │               \r\n                             │    92 │   │                                                                                                                           │               \r\n                             │    93 │   │   # run evaluation                                                                                                        │               \r\n                             │ ❱  94 │   │   evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(                                         │               \r\n                             │    95 │   │   │   retrieval_callables, retrieval_params, result['queries'].tolist(),                                                  │               \r\n                             │       retrieval_gt,                                                                                                                   │               \r\n                             │    96 │   │   │   general_strategy['metrics'], project_dir, previous_result,                                                          │               \r\n                             │       strategies.get('strategy', 'mean')), results))                                                                                  │               \r\n                             │    97                                                                                                                                 │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/queryexpansion/run.py:94 in <lambda>                                            │               \r\n                             │                                                                                                                                       │               \r\n                             │    91 │   │   retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"),                                         │               \r\n                             │       engine='pyarrow')['retrieval_gt'].tolist()                                                                                      │               \r\n                             │    92 │   │                                                                                                                           │               \r\n                             │    93 │   │   # run evaluation                                                                                                        │               \r\n                             │ ❱  94 │   │   evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(                                         │               \r\n                             │    95 │   │   │   retrieval_callables, retrieval_params, result['queries'].tolist(),                                                  │               \r\n                             │       retrieval_gt,                                                                                                                   │               \r\n                             │    96 │   │   │   general_strategy['metrics'], project_dir, previous_result,                                                          │               \r\n                             │       strategies.get('strategy', 'mean')), results))                                                                                  │               \r\n                             │    97                                                                                                                                 │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/queryexpansion/run.py:134 in evaluate_one_query_expansion_node                  │               \r\n                             │                                                                                                                                       │               \r\n                             │   131 │   │   │   │   │   │   │   │   │     previous_result: pd.DataFrame,                                                            │               \r\n                             │   132 │   │   │   │   │   │   │   │   │     strategy_name: str) -> pd.DataFrame:                                                      │               \r\n                             │   133 │   previous_result['queries'] = expanded_queries                                                                               │               \r\n                             │ ❱ 134 │   retrieval_results = list(map(lambda x: x[0](project_dir=project_dir,                                                        │               \r\n                             │       previous_result=previous_result, **x[1]),                                                                                       │               \r\n                             │   135 │   │   │   │   │   │   │   │    zip(retrieval_funcs, retrieval_params)))                                                       │               \r\n                             │   136 │   evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, retrieval_gt,                                            │               \r\n                             │       metrics,                                                                                                                        │               \r\n                             │   137 │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │                                                               │               \r\n                             │       previous_result['query'].tolist(),                                                                                              │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/queryexpansion/run.py:134 in <lambda>                                           │               \r\n                             │                                                                                                                                       │               \r\n                             │   131 │   │   │   │   │   │   │   │   │     previous_result: pd.DataFrame,                                                            │               \r\n                             │   132 │   │   │   │   │   │   │   │   │     strategy_name: str) -> pd.DataFrame:                                                      │               \r\n                             │   133 │   previous_result['queries'] = expanded_queries                                                                               │               \r\n                             │ ❱ 134 │   retrieval_results = list(map(lambda x: x[0](project_dir=project_dir,                                                        │               \r\n                             │       previous_result=previous_result, **x[1]),                                                                                       │               \r\n                             │   135 │   │   │   │   │   │   │   │    zip(retrieval_funcs, retrieval_params)))                                                       │               \r\n                             │   136 │   evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, retrieval_gt,                                            │               \r\n                             │       metrics,                                                                                                                        │               \r\n                             │   137 │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │                                                               │               \r\n                             │       previous_result['query'].tolist(),                                                                                              │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/utils/util.py:53 in wrapper                                                           │               \r\n                             │                                                                                                                                       │               \r\n                             │    50 │   def decorator_result_to_dataframe(func: Callable):                                                                          │               \r\n                             │    51 │   │   @functools.wraps(func)                                                                                                  │               \r\n                             │    52 │   │   def wrapper(*args, **kwargs) -> pd.DataFrame:                                                                           │               \r\n                             │ ❱  53 │   │   │   results = func(*args, **kwargs)                                                                                     │               \r\n                             │    54 │   │   │   if len(column_names) == 1:                                                                                          │               \r\n                             │    55 │   │   │   │   df_input = {column_names[0]: results}                                                                           │               \r\n                             │    56 │   │   │   else:                                                                                                               │               \r\n                             │                                                                                                                                       │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/retrieval/base.py:65 in wrapper                                                 │               \r\n                             │                                                                                                                                       │               \r\n                             │    62 │   │   # run retrieval function                                                                                                │               \r\n                             │    63 │   │   if func.__name__ == \"bm25\":                                                                                             │               \r\n                             │    64 │   │   │   bm25_corpus = load_bm25_corpus(bm25_path)                                                                           │               \r\n                             │ ❱  65 │   │   │   ids, scores = func(queries=queries, bm25_corpus=bm25_corpus, **kwargs)                                              │               \r\n                             │    66 │   │   elif func.__name__ == \"vectordb\":                                                                                       │               \r\n                             │    67 │   │   │   chroma_collection = load_chroma_collection(db_path=chroma_path,                                                     │               \r\n                             │       collection_name=embedding_model_str)                                                                                            │               \r\n                             │    68 │   │   │   if embedding_model_str in embedding_models:                                                                         │               \r\n                             ╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯               \r\n                             TypeError: bm25() got an unexpected keyword argument 'strategy'\r\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-27T03:36:51Z",
      "updated_at": "2024-06-28T05:53:36Z",
      "closed_at": "2024-06-28T05:53:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/539/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/539",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/539",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:20.110961",
      "comments": []
    },
    {
      "issue_number": 535,
      "title": "Add default prompt template at Query Decompose documentation",
      "body": "Query Decompose prompt is quite complex as default, but it is hard to edit if you don't look into our code.\r\nWrite a default prompt at docs for easier edit.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-26T06:07:17Z",
      "updated_at": "2024-06-26T12:42:36Z",
      "closed_at": "2024-06-26T12:42:36Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/535/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/535",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/535",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:20.110968",
      "comments": []
    },
    {
      "issue_number": 536,
      "title": "Typo at query decomposition default prompt.",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-26T06:14:00Z",
      "updated_at": "2024-06-26T12:42:35Z",
      "closed_at": "2024-06-26T12:42:35Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/536/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/536",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/536",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:20.110974",
      "comments": []
    },
    {
      "issue_number": 508,
      "title": "Delete ray dependency at UPR reranker",
      "body": "ray have conflict when installing, so delete it. Because I think it is not that important.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-20T07:14:03Z",
      "updated_at": "2024-06-24T13:46:15Z",
      "closed_at": "2024-06-24T13:46:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/508/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/508",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/508",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:21.891455",
      "comments": []
    },
    {
      "issue_number": 521,
      "title": "Change openai embedding token limit while using openai-embed-large model",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-24T02:55:11Z",
      "updated_at": "2024-06-24T13:35:18Z",
      "closed_at": "2024-06-24T13:35:18Z",
      "labels": [
        "bug",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/521/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/521",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/521",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:21.891482",
      "comments": []
    },
    {
      "issue_number": 520,
      "title": "Modify embedding model list at docs",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-24T02:54:19Z",
      "updated_at": "2024-06-24T05:31:19Z",
      "closed_at": "2024-06-24T05:31:19Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/520/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/520",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/520",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:21.891492",
      "comments": []
    },
    {
      "issue_number": 518,
      "title": "Add implementation of new pytorch acceleration ",
      "body": "Also, may be a good add to the docs - I believe the new pytorch implementations with M-series chipsets have made the LLM steps wildly faster than originally experienced on these machines. Didn't bother to try it at first given one of the notes I read about CUDA optimized runs (especially for flag passage reranking, etc.), and remoted into a Windows WSL2 instance to run it. Turns out, x3 times faster on my M3 Macbook (no memory pinning to contend with there too).\r\n\r\nMight be a nice s/o to that userbase if they're still using workarounds for their eval",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-23T04:31:09Z",
      "updated_at": "2024-06-24T02:53:23Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/518/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/518",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/518",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:21.891498",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It is okay to leave a comment at docs.",
          "created_at": "2024-06-24T02:53:22Z"
        }
      ]
    },
    {
      "issue_number": 515,
      "title": "Circular import on calculate_cosine_similarity",
      "body": "![image](https://github.com/Marker-Inc-Korea/AutoRAG/assets/28955029/6cacaa90-c88b-44cc-a1b7-af87ecf25388)\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-22T05:53:38Z",
      "updated_at": "2024-06-22T07:15:28Z",
      "closed_at": "2024-06-22T07:15:28Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/515/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/515",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/515",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:22.942702",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "This happens duplicate package name of huggingface evaluate and autorag.evaluate.\r\nIt occurs confusion between two packages.\r\nNeed to change our name to other.",
          "created_at": "2024-06-22T06:03:50Z"
        }
      ]
    },
    {
      "issue_number": 494,
      "title": "read_parquet error at Windows environment",
      "body": "Need to use engine=‘pyarrow’ at Windows env.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-16T02:27:55Z",
      "updated_at": "2024-06-21T11:19:42Z",
      "closed_at": "2024-06-21T11:19:42Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/494/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Eastsidegunn"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/494",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/494",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.163209",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #513",
          "created_at": "2024-06-21T11:19:42Z"
        }
      ]
    },
    {
      "issue_number": 476,
      "title": "[Bug] `AssertionError: tokens and logprobs size is different.` error at generator node",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-06-05T12:52:41Z",
      "updated_at": "2024-06-08T09:18:06Z",
      "closed_at": "2024-06-08T09:18:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/476/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/476",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/476",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.338553",
      "comments": [
        {
          "author": "bwook00",
          "body": "modules:\r\n          - module_type: openai_llm\r\n            llm: gpt-4-turbo-2024-04-09\r\n            temperature: [ 0.1 ]",
          "created_at": "2024-06-05T12:54:52Z"
        }
      ]
    },
    {
      "issue_number": 478,
      "title": "Adjust embedding batch size at vectordb ingest",
      "body": "Some embddings like `UpstageEmbeddings` have only 100 inputs at once.\r\nUser can adjust this on YAML file will be great.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-06T05:34:54Z",
      "updated_at": "2024-06-07T07:15:47Z",
      "closed_at": "2024-06-07T07:15:47Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/478/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/478",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/478",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.528359",
      "comments": []
    },
    {
      "issue_number": 249,
      "title": "Use generator modules at `hyde` and `question_decompose` module.",
      "body": "Now, it uses LLM to decompose or making hypothetical queries.\r\nIt often really slow to process it since we use default LlamaIndex only.\r\nSo, it can be great we can use generator modules like `vllm` at query_expansion modules.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-27T06:43:43Z",
      "updated_at": "2024-06-06T08:21:02Z",
      "closed_at": "2024-06-06T08:21:01Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/249/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/249",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/249",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.528376",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "We did this!",
          "created_at": "2024-06-06T08:21:01Z"
        }
      ]
    },
    {
      "issue_number": 472,
      "title": "[HotFix] Change vllm `destroy_model_parallel` file location",
      "body": "At the latest vllm version, the function moves to [here](https://github.com/vllm-project/vllm/blob/main/vllm/distributed/parallel_state.py)",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-06-03T06:44:47Z",
      "updated_at": "2024-06-03T11:18:34Z",
      "closed_at": "2024-06-03T11:18:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/472/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/472",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/472",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.698587",
      "comments": []
    },
    {
      "issue_number": 227,
      "title": "Add 'custom multi' query expansion module",
      "body": "There is a [MultiQueryRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever) in Langchain.\r\nIt is kind of 'query expansion', which can generate several queries from given user query.\r\n\r\nThere is a default prompt, but user can write their own 'query expansion' prompt. \r\n\r\nFrom now, we only have 'hyde' and 'query_decompose' module and do not have corresponding module from `MutliQueryRetriever`.\r\n\r\nTo use this, user can adapt any LLM using query expansion method easily, just change prompt in config YAML file.\r\n\r\n[How to Suggestion]\r\nIt can be great just slice LLM generation to line by line (using \\n)\r\nBecause this module must generate more than one question, it is important to parse LLM generation.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-16T21:16:13Z",
      "updated_at": "2024-05-28T04:53:25Z",
      "closed_at": "2024-05-28T04:53:25Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/227/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/227",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/227",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.698610",
      "comments": []
    },
    {
      "issue_number": 413,
      "title": "Add select best strategy - Normalize mean",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-01T13:49:22Z",
      "updated_at": "2024-05-28T04:42:55Z",
      "closed_at": "2024-05-28T04:42:55Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/413/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/413",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/413",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.698617",
      "comments": []
    },
    {
      "issue_number": 462,
      "title": "Add tonic validate metric with local model.",
      "body": "no_gt retrieval metrics needs large amount of LLM processing.\r\nSo, use local LLM model to compute it.\r\n\r\n+ ragas context precision need so much LLM calls. So, try to use tonic validate instead.",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-25T04:30:09Z",
      "updated_at": "2024-05-25T05:36:57Z",
      "closed_at": null,
      "labels": [
        "wontfix"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/462/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/462",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/462",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.698625",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Do this on 'aragog' branch.\r\nNo plan to integrate this to a main branch.",
          "created_at": "2024-05-25T05:36:49Z"
        }
      ]
    },
    {
      "issue_number": 361,
      "title": "Implement LLMLingua ",
      "body": "https://github.com/microsoft/LLMLingua\r\n\r\nIt will be new passage compressor module",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-22T13:22:00Z",
      "updated_at": "2024-05-23T06:40:59Z",
      "closed_at": "2024-05-23T06:40:59Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/361/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/361",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/361",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.967735",
      "comments": []
    },
    {
      "issue_number": 279,
      "title": "Visualize trial result to web",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-09T06:53:33Z",
      "updated_at": "2024-05-21T06:42:41Z",
      "closed_at": "2024-05-21T06:42:41Z",
      "labels": [
        "help wanted"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/279/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/279",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/279",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.967755",
      "comments": []
    },
    {
      "issue_number": 337,
      "title": "Move refine module Passage Compressor to Generator",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-16T07:12:48Z",
      "updated_at": "2024-05-21T05:58:02Z",
      "closed_at": "2024-05-21T05:58:02Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/337/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/337",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/337",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:23.967761",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "refine is passage compressor. Works great? Why generator? No! \r\nYes compressor!",
          "created_at": "2024-05-21T05:58:02Z"
        }
      ]
    },
    {
      "issue_number": 450,
      "title": "bug in passage_compressor node metrics get Nan",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-05-20T06:46:37Z",
      "updated_at": "2024-05-21T02:41:22Z",
      "closed_at": "2024-05-21T02:41:22Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/450/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/450",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/450",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:24.159080",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "You can't use passage compressor if there is no retrieval gt in QA dataset.",
          "created_at": "2024-05-20T08:11:04Z"
        }
      ]
    },
    {
      "issue_number": 412,
      "title": "Add select best strategy - Rank",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-01T13:49:01Z",
      "updated_at": "2024-05-20T15:29:28Z",
      "closed_at": "2024-05-20T15:29:28Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/412/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/412",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/412",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:24.338985",
      "comments": []
    },
    {
      "issue_number": 445,
      "title": "This error happens frequently on auto-testing CI",
      "body": "```\r\nFAILED tests/autorag/test_cli.py::test_restart_evaluate - AssertionError: assert False\r\n +  where False = <function exists at 0x7f4843d71f30>('/tmp/tmpq4xuk7vn/1/summary.csv')\r\n +    where <function exists at 0x7f4843d71f30> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.exists\r\n +      where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\r\n +    and   '/tmp/tmpq4xuk7vn/1/summary.csv' = <function join at 0x7f4843d727a0>('/tmp/tmpq4xuk7vn/1', 'summary.csv')\r\n +      where <function join at 0x7f4843d727a0> = <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'>.join\r\n +        where <module 'posixpath' from '/usr/lib/python3.10/posixpath.py'> = os.path\r\n===== 1 failed, 182 passed, 41 skipped, 326 warnings in 123.71s (0:02:03) ======\r\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-16T09:14:23Z",
      "updated_at": "2024-05-20T15:13:34Z",
      "closed_at": "2024-05-20T15:13:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/445/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/445",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/445",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:24.339010",
      "comments": []
    },
    {
      "issue_number": 443,
      "title": "Problem with Korean tokenize",
      "body": "https://jonsyou.tistory.com/26\r\n\r\nThis happens a lot when we processing Korean documents.\r\nNeed to fix (especially on the mac)",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-15T16:25:45Z",
      "updated_at": "2024-05-16T11:19:41Z",
      "closed_at": "2024-05-16T11:19:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/443/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/443",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/443",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:24.339018",
      "comments": []
    },
    {
      "issue_number": 439,
      "title": "[HotFix] [threshold cutoff] Error when the input 'scores_list' is np.ndarray.",
      "body": "In the running of AutoRAG, the input of `threshold_cutoff_pure` function's parameter `scores_list` is np.ndarray, it occurs error.\r\n\r\n```python\r\ndef threshold_cutoff_pure(scores_list: List[float],\r\n                          threshold: float,\r\n                          reverse: bool = False) -> List[int]:\r\n    if reverse:\r\n        remain_indices = [i for i, score in enumerate(scores_list) if score <= threshold]\r\n        default_index = scores_list.index(min(scores_list)) # error here\r\n    else:\r\n        remain_indices = [i for i, score in enumerate(scores_list) if score >= threshold]\r\n        default_index = scores_list.index(max(scores_list)) # error here\r\n\r\n    return remain_indices if remain_indices else [default_index]\r\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-15T07:10:45Z",
      "updated_at": "2024-05-16T05:50:27Z",
      "closed_at": "2024-05-16T05:50:27Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/439/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/439",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/439",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:24.339025",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "We will fix this kind of error with #438 later.",
          "created_at": "2024-05-15T07:11:07Z"
        }
      ]
    },
    {
      "issue_number": 411,
      "title": "Add percentile cutoff module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-05-01T13:39:13Z",
      "updated_at": "2024-05-14T05:39:14Z",
      "closed_at": "2024-05-14T05:39:14Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/411/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/411",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/411",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348349",
      "comments": []
    },
    {
      "issue_number": 410,
      "title": "Add threshold cutoff module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-05-01T13:38:08Z",
      "updated_at": "2024-05-13T14:17:17Z",
      "closed_at": "2024-05-13T14:17:17Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/410/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/410",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/410",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348373",
      "comments": []
    },
    {
      "issue_number": 422,
      "title": "Add Tonic validate retrieval precision metric as retrieval metric",
      "body": "With this, we can support non-gt metrics for evaluation now!\r\n\r\nhttps://docs.tonic.ai/validate/about-rag-metrics/tonic-validate-rag-metrics-reference#rag-metric-ref-retrieval-precision\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-08T15:39:11Z",
      "updated_at": "2024-05-13T13:35:42Z",
      "closed_at": "2024-05-13T13:35:42Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/422/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/422",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/422",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348381",
      "comments": []
    },
    {
      "issue_number": 416,
      "title": "Implement RSE module",
      "body": "https://github.com/SuperpoweredAI/spRAG",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-02T23:03:08Z",
      "updated_at": "2024-05-13T06:47:05Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/416/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/416",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/416",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348387",
      "comments": []
    },
    {
      "issue_number": 425,
      "title": "[HotFix] Error at vllm.py",
      "body": "1. The output logprob is not float, so it can't save as parquet file\r\n2. It does not properly empty GPU memory when destroyed.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-11T05:19:57Z",
      "updated_at": "2024-05-11T07:42:12Z",
      "closed_at": "2024-05-11T07:42:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/425/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/425",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/425",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348392",
      "comments": []
    },
    {
      "issue_number": 407,
      "title": "Colbert Reranker too many VRAM on H100 instance",
      "body": "H100 has 80gb vram. Colbert Reranker occurs OOM on the machine. It is really weird. ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-01T01:16:28Z",
      "updated_at": "2024-05-11T02:22:28Z",
      "closed_at": "2024-05-11T02:22:28Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/407/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/407",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/407",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348398",
      "comments": []
    },
    {
      "issue_number": 421,
      "title": "Any great method to keep update sample YAML file for the latest modules and nodes?",
      "body": "There are several obstacles to keep on update sample YAML file to the latest. \r\nPlus, it is quite little bit hard to user find the sample files easily.\r\n\r\n@bwook00 @Eastsidegunn  Any idea?\r\n\r\nI want below things.\r\n\r\nThe role model is certainly 'Github Actions' YAML marketplace. It is truly amazing for just sharing YAML file across the platform.\r\n\r\n1. Easily keep on update. Do not need complicate edit (or commit&push&review).\r\n2. Always check before version release. \r\n3. Versioning of each YAML files.\r\n\r\nWhat is the simplest yet effective way?",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-07T12:39:24Z",
      "updated_at": "2024-05-07T12:39:29Z",
      "closed_at": null,
      "labels": [
        "help wanted"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/421/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/421",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/421",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348438",
      "comments": []
    },
    {
      "issue_number": 399,
      "title": "Add tokenizer option at bm25",
      "body": "set proper tokenizer at bm25 looks important.\r\nSome robust method get tokenizer at ingesting and using it.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-29T13:14:16Z",
      "updated_at": "2024-05-01T11:36:57Z",
      "closed_at": "2024-05-01T11:36:57Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/399/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/399",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/399",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348443",
      "comments": []
    },
    {
      "issue_number": 405,
      "title": "[HotFix] Fix mocking at test_evaluate_generation",
      "body": "It occurs test CI failed.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-05-01T00:45:12Z",
      "updated_at": "2024-05-01T04:44:59Z",
      "closed_at": "2024-05-01T04:44:59Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/405/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/405",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/405",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348448",
      "comments": []
    },
    {
      "issue_number": 221,
      "title": "Add openai output token support at generator module",
      "body": "part of the #63 \r\n\r\nLlamaIndex openai model does not support logprob and output tokens. \r\nWe might need to support it for using various log-prob based modules.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-12T22:21:35Z",
      "updated_at": "2024-05-01T00:14:48Z",
      "closed_at": "2024-05-01T00:14:48Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/221/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/221",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/221",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.348453",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "You can see `g_eval` method how to get logprob from openai API call.",
          "created_at": "2024-03-12T22:28:14Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Don't have to get log prob from now.\r\n\r\n- First, Token support.\r\n- Second, Input token truncate by model's context length.",
          "created_at": "2024-04-29T09:02:21Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Third, no error because of the side-effect.\r\nDependency only in `opneai-python` library.",
          "created_at": "2024-04-30T05:54:53Z"
        }
      ]
    },
    {
      "issue_number": 359,
      "title": "Add Hit Rate retrieval metric",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-22T12:01:33Z",
      "updated_at": "2024-04-30T07:27:41Z",
      "closed_at": "2024-04-30T07:27:20Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/359/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/359",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/359",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.561981",
      "comments": [
        {
          "author": "bwook00",
          "body": "We decided not to support `Hit Rate` as a metric because it is exactly the same as `recall` except for the learning process.\r\nSo I close this issue",
          "created_at": "2024-04-30T07:27:20Z"
        }
      ]
    },
    {
      "issue_number": 358,
      "title": "Add MAP retrieval metric",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-22T12:01:15Z",
      "updated_at": "2024-04-30T06:57:52Z",
      "closed_at": "2024-04-30T06:57:52Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/358/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/358",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/358",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.800748",
      "comments": []
    },
    {
      "issue_number": 357,
      "title": "Add MRR retrieval metric",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-22T12:00:57Z",
      "updated_at": "2024-04-30T05:53:37Z",
      "closed_at": "2024-04-30T05:53:37Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/357/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/357",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/357",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.800767",
      "comments": []
    },
    {
      "issue_number": 360,
      "title": "Add nDCG retrieval metric",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-22T12:02:00Z",
      "updated_at": "2024-04-30T05:28:34Z",
      "closed_at": "2024-04-30T05:28:34Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/360/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/360",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/360",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.800773",
      "comments": []
    },
    {
      "issue_number": 339,
      "title": "Add pass augmenter module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-16T07:57:21Z",
      "updated_at": "2024-04-30T04:17:29Z",
      "closed_at": "2024-04-30T04:17:29Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/339/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/339",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/339",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:26.800779",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close at #352",
          "created_at": "2024-04-30T04:17:29Z"
        }
      ]
    },
    {
      "issue_number": 303,
      "title": "Refactor for parallel processing that using CUDA at model-based Rerankers",
      "body": "From now, all rerankers put a query to reranker model ‘one by one’ \r\nIt uses ‘async’, but it is not effective since the key to the parallize using model is putting model itself.\r\n\r\nSo, we have to refactor all rerankers that supports ‘real parralell’ processing. Put enough batch size ‘query and context’ set to model, and gather the results at once.\r\n\r\n- [x] #304\r\n- [x] #305\r\n- [x] #306\r\n- [x] #307\r\n- [x] #308\r\n- [x] #309",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-11T12:53:34Z",
      "updated_at": "2024-04-28T12:16:25Z",
      "closed_at": "2024-04-28T12:16:25Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/303/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/303",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/303",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.025969",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Close this since we parallel processing all rerankers",
          "created_at": "2024-04-28T12:16:25Z"
        }
      ]
    },
    {
      "issue_number": 301,
      "title": "Error when using Colbert Reranker at CUDA enabled device",
      "body": "```\r\nRuntimeError: Expected all tensors to be on the same                   \r\n                             device, but found at least two devices, cuda:0 and cpu!                \r\n                             (when checking argument for argument index in method                   \r\n                             wrapper_CUDA__index_select)  \r\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-11T06:56:50Z",
      "updated_at": "2024-04-27T06:20:34Z",
      "closed_at": "2024-04-27T06:20:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/301/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/301",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/301",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.268825",
      "comments": []
    },
    {
      "issue_number": 308,
      "title": "Parallel processing at Colbert Reranker",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-11T12:55:06Z",
      "updated_at": "2024-04-27T05:13:15Z",
      "closed_at": "2024-04-27T05:13:15Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/308/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/308",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/308",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.268853",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It looks like the real problem is not `batch` in here. The parallel code itself looks wrong. \r\n\r\nLook at #326.\r\n\r\nYou can see the original code, and the new code that you pushed.\r\n\r\nThe original code is like below.\r\n\r\n```python\r\ndef get_colbert_score(query: str, content_list: List[str],\r\n           ",
          "created_at": "2024-04-24T14:13:19Z"
        }
      ]
    },
    {
      "issue_number": 372,
      "title": "ChromaDB: Batch size exceeds maximum batch size using collection.add function",
      "body": "I cannot submit more than 41,666 embeddings at once with chromadb. All embeddings are lost (made me waste money). It would be useful to cache embeddings somewhere.\r\n\r\nchromadb issue was discussed [here](https://github.com/chroma-core/chroma/issues/1049)",
      "state": "closed",
      "author": "JulianLopezB",
      "author_type": "User",
      "created_at": "2024-04-25T14:54:26Z",
      "updated_at": "2024-04-26T07:27:06Z",
      "closed_at": "2024-04-26T07:27:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/372/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/372",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/372",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.519072",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@JulianLopezB Thank you for reporting chromadb error. I didn't know about this kind of error.\r\nI will fix ingest code for resolve this. Thanks.",
          "created_at": "2024-04-26T02:57:23Z"
        }
      ]
    },
    {
      "issue_number": 367,
      "title": "[Hotfix] recency filter doesn't get threshold as string",
      "body": "`TypeError: strptime() argument 1 must be str, not datetime.date `",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-23T11:19:17Z",
      "updated_at": "2024-04-24T16:49:52Z",
      "closed_at": "2024-04-24T16:49:52Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/367/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/367",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/367",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.792292",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "YAML supports datetime..? Then it is even better~~ Don't use strptime, get datetime or date directly!",
          "created_at": "2024-04-23T11:57:40Z"
        }
      ]
    },
    {
      "issue_number": 362,
      "title": "[HotFix] Error at tart.py",
      "body": "```\r\n[04/23/24 04:19:32] ERROR    [__init__.py:71] >> Unexpected exception                                                                                                         __init__.py:71\r\n                             ╭───────────────────────────────────────────────────── Traceback (most recent call last) ──────────────────────────────────────────────────────╮               \r\n                             │ /usr/local/bin/autorag:8 in <module>                                                                                                         │               \r\n                             │                                                                                                                                              │               \r\n                             │   5 from autorag.cli import cli                                                                                                              │               \r\n                             │   6 if __name__ == '__main__':                                                                                                               │               \r\n                             │   7 │   sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])                                                                     │               \r\n                             │ ❱ 8 │   sys.exit(cli())                                                                                                                      │               \r\n                             │   9                                                                                                                                          │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1157 in __call__                                                                       │               \r\n                             │                                                                                                                                              │               \r\n                             │   1154 │                                                                                                                                     │               \r\n                             │   1155 │   def __call__(self, *args: t.Any, **kwargs: t.Any) -> t.Any:                                                                       │               \r\n                             │   1156 │   │   \"\"\"Alias for :meth:`main`.\"\"\"                                                                                                 │               \r\n                             │ ❱ 1157 │   │   return self.main(*args, **kwargs)                                                                                             │               \r\n                             │   1158                                                                                                                                       │               \r\n                             │   1159                                                                                                                                       │               \r\n                             │   1160 class Command(BaseCommand):                                                                                                           │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1078 in main                                                                           │               \r\n                             │                                                                                                                                              │               \r\n                             │   1075 │   │   try:                                                                                                                          │               \r\n                             │   1076 │   │   │   try:                                                                                                                      │               \r\n                             │   1077 │   │   │   │   with self.make_context(prog_name, args, **extra) as ctx:                                                              │               \r\n                             │ ❱ 1078 │   │   │   │   │   rv = self.invoke(ctx)                                                                                             │               \r\n                             │   1079 │   │   │   │   │   if not standalone_mode:                                                                                           │               \r\n                             │   1080 │   │   │   │   │   │   return rv                                                                                                     │               \r\n                             │   1081 │   │   │   │   │   # it's not safe to `ctx.exit(rv)` here!                                                                           │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1688 in invoke                                                                         │               \r\n                             │                                                                                                                                              │               \r\n                             │   1685 │   │   │   │   super().invoke(ctx)                                                                                                   │               \r\n                             │   1686 │   │   │   │   sub_ctx = cmd.make_context(cmd_name, args, parent=ctx)                                                                │               \r\n                             │   1687 │   │   │   │   with sub_ctx:                                                                                                         │               \r\n                             │ ❱ 1688 │   │   │   │   │   return _process_result(sub_ctx.command.invoke(sub_ctx))                                                           │               \r\n                             │   1689 │   │                                                                                                                                 │               \r\n                             │   1690 │   │   # In chain mode we create the contexts step by step, but after the                                                            │               \r\n                             │   1691 │   │   # base command has been invoked.  Because at that point we do not                                                             │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:1434 in invoke                                                                         │               \r\n                             │                                                                                                                                              │               \r\n                             │   1431 │   │   │   echo(style(message, fg=\"red\"), err=True)                                                                                  │               \r\n                             │   1432 │   │                                                                                                                                 │               \r\n                             │   1433 │   │   if self.callback is not None:                                                                                                 │               \r\n                             │ ❱ 1434 │   │   │   return ctx.invoke(self.callback, **ctx.params)                                                                            │               \r\n                             │   1435 │                                                                                                                                     │               \r\n                             │   1436 │   def shell_complete(self, ctx: Context, incomplete: str) -> t.List[\"CompletionItem\"]:                                              │               \r\n                             │   1437 │   │   \"\"\"Return a list of completions for the incomplete value. Looks                                                               │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/click/core.py:783 in invoke                                                                          │               \r\n                             │                                                                                                                                              │               \r\n                             │    780 │   │                                                                                                                                 │               \r\n                             │    781 │   │   with augment_usage_errors(__self):                                                                                            │               \r\n                             │    782 │   │   │   with ctx:                                                                                                                 │               \r\n                             │ ❱  783 │   │   │   │   return __callback(*args, **kwargs)                                                                                    │               \r\n                             │    784 │                                                                                                                                     │               \r\n                             │    785 │   def forward(                                                                                                                      │               \r\n                             │    786 │   │   __self, __cmd: \"Command\", *args: t.Any, **kwargs: t.Any  # noqa: B902                                                         │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/cli.py:34 in evaluate                                                                        │               \r\n                             │                                                                                                                                              │               \r\n                             │    31 │   if not os.path.exists(config):                                                                                                     │               \r\n                             │    32 │   │   raise ValueError(f\"Config file {config} does not exist.\")                                                                      │               \r\n                             │    33 │   evaluator = Evaluator(qa_data_path, corpus_data_path, project_dir=project_dir)                                                     │               \r\n                             │ ❱  34 │   evaluator.start_trial(config)                                                                                                      │               \r\n                             │    35 │   logger.info('Evaluation complete.')                                                                                                │               \r\n                             │    36                                                                                                                                        │               \r\n                             │    37                                                                                                                                        │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/evaluator.py:95 in start_trial                                                               │               \r\n                             │                                                                                                                                              │               \r\n                             │    92 │   │   │   if i == 0:                                                                                                                 │               \r\n                             │    93 │   │   │   │   previous_result = self.qa_data                                                                                         │               \r\n                             │    94 │   │   │   logger.info(f'Running node line {node_line_name}...')                                                                      │               \r\n                             │ ❱  95 │   │   │   previous_result = run_node_line(node_line, node_line_dir, previous_result)                                                 │               \r\n                             │    96 │   │   │                                                                                                                              │               \r\n                             │    97 │   │   │   trial_summary_df = self._append_node_line_summary(node_line_name,                                                          │               \r\n                             │       node_line_dir, trial_summary_df)                                                                                                       │               \r\n                             │    98                                                                                                                                        │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/node_line.py:45 in run_node_line                                                             │               \r\n                             │                                                                                                                                              │               \r\n                             │   42 │                                                                                                                                       │               \r\n                             │   43 │   summary_lst = []                                                                                                                    │               \r\n                             │   44 │   for node in nodes:                                                                                                                  │               \r\n                             │ ❱ 45 │   │   previous_result = node.run(previous_result, node_line_dir)                                                                      │               \r\n                             │   46 │   │   node_summary_df = load_summary_file(os.path.join(node_line_dir, node.node_type,                                                 │               \r\n                             │      'summary.csv'))                                                                                                                         │               \r\n                             │   47 │   │   best_node_row = node_summary_df.loc[node_summary_df['is_best']]                                                                 │               \r\n                             │   48 │   │   summary_lst.append({                                                                                                            │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/schema/node.py:57 in run                                                                     │               \r\n                             │                                                                                                                                              │               \r\n                             │    54 │   def run(self, previous_result: pd.DataFrame, node_line_dir: str) -> pd.DataFrame:                                                  │               \r\n                             │    55 │   │   logger.info(f'Running node {self.node_type}...')                                                                               │               \r\n                             │    56 │   │   input_modules, input_params = self.get_param_combinations()                                                                    │               \r\n                             │ ❱  57 │   │   return self.run_node(modules=input_modules,                                                                                    │               \r\n                             │    58 │   │   │   │   │   │   │    module_params=input_params,                                                                               │               \r\n                             │    59 │   │   │   │   │   │   │    previous_result=previous_result,                                                                          │               \r\n                             │    60 │   │   │   │   │   │   │    node_line_dir=node_line_dir,                                                                              │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/passagereranker/run.py:39 in run_passage_reranker_node                                 │               \r\n                             │                                                                                                                                              │               \r\n                             │   36 │   project_dir = pathlib.PurePath(node_line_dir).parent.parent                                                                         │               \r\n                             │   37 │   retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\",                                                                    │               \r\n                             │      \"qa.parquet\"))['retrieval_gt'].tolist()                                                                                                 │               \r\n                             │   38 │                                                                                                                                       │               \r\n                             │ ❱ 39 │   results, execution_times = zip(*map(lambda task: measure_speed(                                                                     │               \r\n                             │   40 │   │   task[0], project_dir=project_dir, previous_result=previous_result, **task[1]),                                                  │               \r\n                             │      zip(modules, module_params)))                                                                                                           │               \r\n                             │   41 │   average_times = list(map(lambda x: x / len(results[0]), execution_times))                                                           │               \r\n                             │   42                                                                                                                                         │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/passagereranker/run.py:39 in <lambda>                                                  │               \r\n                             │                                                                                                                                              │               \r\n                             │   36 │   project_dir = pathlib.PurePath(node_line_dir).parent.parent                                                                         │               \r\n                             │   37 │   retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\",                                                                    │               \r\n                             │      \"qa.parquet\"))['retrieval_gt'].tolist()                                                                                                 │               \r\n                             │   38 │                                                                                                                                       │               \r\n                             │ ❱ 39 │   results, execution_times = zip(*map(lambda task: measure_speed(                                                                     │               \r\n                             │   40 │   │   task[0], project_dir=project_dir, previous_result=previous_result, **task[1]),                                                  │               \r\n                             │      zip(modules, module_params)))                                                                                                           │               \r\n                             │   41 │   average_times = list(map(lambda x: x / len(results[0]), execution_times))                                                           │               \r\n                             │   42                                                                                                                                         │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/strategy.py:13 in measure_speed                                                              │               \r\n                             │                                                                                                                                              │               \r\n                             │    10 │   Method for measuring execution speed of the function.                                                                              │               \r\n                             │    11 │   \"\"\"                                                                                                                                │               \r\n                             │    12 │   start_time = time.time()                                                                                                           │               \r\n                             │ ❱  13 │   result = func(*args, **kwargs)                                                                                                     │               \r\n                             │    14 │   end_time = time.time()                                                                                                             │               \r\n                             │    15 │   return result, end_time - start_time                                                                                               │               \r\n                             │    16                                                                                                                                        │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/utils/util.py:40 in wrapper                                                                  │               \r\n                             │                                                                                                                                              │               \r\n                             │    37 │   def decorator_result_to_dataframe(func: Callable):                                                                                 │               \r\n                             │    38 │   │   @functools.wraps(func)                                                                                                         │               \r\n                             │    39 │   │   def wrapper(*args, **kwargs) -> pd.DataFrame:                                                                                  │               \r\n                             │ ❱  40 │   │   │   results = func(*args, **kwargs)                                                                                            │               \r\n                             │    41 │   │   │   if len(column_names) == 1:                                                                                                 │               \r\n                             │    42 │   │   │   │   df_input = {column_names[0]: results}                                                                                  │               \r\n                             │    43 │   │   │   else:                                                                                                                      │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/passagereranker/base.py:48 in wrapper                                                  │               \r\n                             │                                                                                                                                              │               \r\n                             │   45 │   │   │   │   = func(contents_list=contents, scores_list=scores, ids_list=ids,                                                        │               \r\n                             │      time_list=times, *args, **kwargs)                                                                                                       │               \r\n                             │   46 │   │   else:                                                                                                                           │               \r\n                             │   47 │   │   │   reranked_contents, reranked_ids, reranked_scores \\                                                                          │               \r\n                             │ ❱ 48 │   │   │   │   = func(queries=queries, contents_list=contents, scores_list=scores,                                                     │               \r\n                             │      ids_list=ids, *args, **kwargs)                                                                                                          │               \r\n                             │   49 │   │                                                                                                                                   │               \r\n                             │   50 │   │   return reranked_contents, reranked_ids, reranked_scores                                                                         │               \r\n                             │   51                                                                                                                                         │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/passagereranker/tart/tart.py:46 in tart                                                │               \r\n                             │                                                                                                                                              │               \r\n                             │   43 │   nested_list = [[['{} [SEP] {}'.format(instruction, query)] for _ in contents] for                                                   │               \r\n                             │      query, contents in                                                                                                                      │               \r\n                             │   44 │   │   │   │      zip(queries, contents_list)]                                                                                         │               \r\n                             │   45 │                                                                                                                                       │               \r\n                             │ ❱ 46 │   rerank_scores = flatten_apply(tart_run_model, nested_list, model=model,                                                             │               \r\n                             │      batch_size=batch,                                                                                                                       │               \r\n                             │   47 │   │   │   │   │   │   │   │     tokenizer=tokenizer, device=device,                                                                   │               \r\n                             │      contents_list=contents_list)                                                                                                            │               \r\n                             │   48 │                                                                                                                                       │               \r\n                             │   49 │   df = pd.DataFrame({                                                                                                                 │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/utils/util.py:300 in flatten_apply                                                           │               \r\n                             │                                                                                                                                              │               \r\n                             │   297 │   \"\"\"                                                                                                                                │               \r\n                             │   298 │   df = pd.DataFrame({'col1': nested_list})                                                                                           │               \r\n                             │   299 │   df = df.explode('col1')                                                                                                            │               \r\n                             │ ❱ 300 │   df['result'] = func(df['col1'].tolist(), **kwargs)                                                                                 │               \r\n                             │   301 │   return df.groupby(level=0, sort=False)['result'].apply(list).tolist()                                                              │               \r\n                             │   302                                                                                                                                        │               \r\n                             │   303                                                                                                                                        │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/autorag/nodes/passagereranker/tart/tart.py:72 in tart_run_model                                      │               \r\n                             │                                                                                                                                              │               \r\n                             │   69 │   for batch_texts, batch_contents in zip(batch_input_texts, batch_contents_list):                                                     │               \r\n                             │   70 │   │   flattened_batch_texts = list(chain.from_iterable(batch_texts))                                                                  │               \r\n                             │   71 │   │   flattened_batch_contents = list(chain.from_iterable(batch_contents))                                                            │               \r\n                             │ ❱ 72 │   │   feature = tokenizer(flattened_batch_texts, flattened_batch_contents,                                                            │               \r\n                             │      padding=True, truncation=True,                                                                                                          │               \r\n                             │   73 │   │   │   │   │   │   │   return_tensors=\"pt\").to(device)                                                                             │               \r\n                             │   74 │   │   with torch.no_grad():                                                                                                           │               \r\n                             │   75 │   │   │   pred_scores = model(**feature).logits                                                                                       │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2872 in __call__                                             │               \r\n                             │                                                                                                                                              │               \r\n                             │   2869 │   │   │   # input mode in this case.                                                                                                │               \r\n                             │   2870 │   │   │   if not self._in_target_context_manager:                                                                                   │               \r\n                             │   2871 │   │   │   │   self._switch_to_input_mode()                                                                                          │               \r\n                             │ ❱ 2872 │   │   │   encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)                                                  │               \r\n                             │   2873 │   │   if text_target is not None:                                                                                                   │               \r\n                             │   2874 │   │   │   self._switch_to_target_mode()                                                                                             │               \r\n                             │   2875 │   │   │   target_encodings = self._call_one(text=text_target,                                                                       │               \r\n                             │        text_pair=text_pair_target, **all_kwargs)                                                                                             │               \r\n                             │                                                                                                                                              │               \r\n                             │ /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2953 in _call_one                                            │               \r\n                             │                                                                                                                                              │               \r\n                             │   2950 │   │   │   │   │   \" `text`.\"                                                                                                        │               \r\n                             │   2951 │   │   │   │   )                                                                                                                     │               \r\n                             │   2952 │   │   │   if text_pair is not None and len(text) != len(text_pair):                                                                 │               \r\n                             │ ❱ 2953 │   │   │   │   raise ValueError(                                                                                                     │               \r\n                             │   2954 │   │   │   │   │   f\"batch length of `text`: {len(text)} does not match batch length of                                              │               \r\n                             │        `text_pair`:\"                                                                                                                         │               \r\n                             │   2955 │   │   │   │   │   f\" {len(text_pair)}.\"                                                                                             │               \r\n                             │   2956 │   │   │   │   )                                                                                                                     │               \r\n                             ╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯               \r\n                             ValueError: batch length of `text`: 64 does not match batch length of `text_pair`: 150. \r\n```",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-23T04:24:13Z",
      "updated_at": "2024-04-23T11:24:35Z",
      "closed_at": "2024-04-23T11:24:35Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/362/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/362",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/362",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.970577",
      "comments": []
    },
    {
      "issue_number": 338,
      "title": "Create 'Passage Augmenter' and its first module 'prev_next_augmenter'",
      "body": "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/PrevNextPostprocessorDemo/",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-16T07:49:03Z",
      "updated_at": "2024-04-23T05:36:29Z",
      "closed_at": "2024-04-23T05:36:29Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/338/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/338",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/338",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.970598",
      "comments": []
    },
    {
      "issue_number": 336,
      "title": "Add recursive_metadata retrieval module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-16T07:09:25Z",
      "updated_at": "2024-04-22T12:10:28Z",
      "closed_at": "2024-04-22T12:02:17Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/336/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/336",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/336",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:27.970606",
      "comments": [
        {
          "author": "bwook00",
          "body": "We decided not to support this as a module in `Passage Augmenter` and closed the issue for now.",
          "created_at": "2024-04-22T12:10:26Z"
        }
      ]
    },
    {
      "issue_number": 317,
      "title": "Add Presidio PII-Masking module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-12T06:55:22Z",
      "updated_at": "2024-04-22T12:09:36Z",
      "closed_at": "2024-04-22T12:02:36Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/317/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/317",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/317",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:28.156222",
      "comments": [
        {
          "author": "bwook00",
          "body": "We decided not to support PII Masking as a module in `Passage Filter` and closed the issue for now.",
          "created_at": "2024-04-22T12:09:34Z"
        }
      ]
    },
    {
      "issue_number": 316,
      "title": "Add LLM PII-Masking module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-12T06:54:45Z",
      "updated_at": "2024-04-22T12:09:15Z",
      "closed_at": "2024-04-22T12:02:30Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/316/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/316",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/316",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:28.320055",
      "comments": [
        {
          "author": "bwook00",
          "body": "We decided not to support PII Masking as a module in `Passage Filter` and closed the issue for now.",
          "created_at": "2024-04-22T12:09:14Z"
        }
      ]
    },
    {
      "issue_number": 101,
      "title": "Add Contrbuting md file",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-03T10:20:42Z",
      "updated_at": "2024-04-21T06:12:25Z",
      "closed_at": "2024-04-21T06:12:25Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/101/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/101",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/101",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:28.538879",
      "comments": []
    },
    {
      "issue_number": 335,
      "title": "Add recursive_chunk passage_augmenter module",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-16T07:06:02Z",
      "updated_at": "2024-04-19T06:22:57Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/335/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/335",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/335",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:28.538907",
      "comments": []
    },
    {
      "issue_number": 255,
      "title": "Add sphinx-sitemap for docs SEO",
      "body": "https://sphinx-sitemap.readthedocs.io/en/latest/index.html\r\n\r\nSphinx has plugin for SEO, making sitemap.xml",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-29T05:13:36Z",
      "updated_at": "2024-04-18T07:46:28Z",
      "closed_at": "2024-04-18T07:46:28Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/255/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/255",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/255",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.332990",
      "comments": []
    },
    {
      "issue_number": 342,
      "title": "Add OpenAI batch feature ",
      "body": "Might be a new module for openai, optimized batch processing only",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-17T03:38:08Z",
      "updated_at": "2024-04-18T06:13:47Z",
      "closed_at": "2024-04-18T06:13:47Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/342/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/342",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/342",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.333016",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "We want the result ASAP. So, it doesn't have to implement this feature at AutoRAG.\r\n\r\n\"Batch feature needs to wait max 24 hour\"",
          "created_at": "2024-04-18T06:13:47Z"
        }
      ]
    },
    {
      "issue_number": 340,
      "title": "Add pass answer filter module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-16T07:57:54Z",
      "updated_at": "2024-04-17T09:16:30Z",
      "closed_at": "2024-04-17T09:16:30Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/340/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/340",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/340",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.558510",
      "comments": [
        {
          "author": "bwook00",
          "body": "We haven't created the Answer filter Node yet, so we're closing this issue as well.\r\nFor more information, please see the link below.\r\nhttps://github.com/Marker-Inc-Korea/AutoRAG/pull/345",
          "created_at": "2024-04-17T09:16:30Z"
        }
      ]
    },
    {
      "issue_number": 343,
      "title": "[HotFix] Fix error at qa_gen_by_ratio",
      "body": "The output result order got mixed because we shuffled content inside the function.\r\nWe have to fix this because its order is super important for making retrieval_gt. ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-17T05:52:30Z",
      "updated_at": "2024-04-17T06:31:18Z",
      "closed_at": "2024-04-17T06:31:18Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/343/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/343",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/343",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.836867",
      "comments": []
    },
    {
      "issue_number": 321,
      "title": "Add Refine summarize module",
      "body": "https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/refine/",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-12T11:35:56Z",
      "updated_at": "2024-04-15T05:47:41Z",
      "closed_at": "2024-04-15T05:47:41Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/321/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/321",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/321",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.836887",
      "comments": []
    },
    {
      "issue_number": 320,
      "title": "Error when sampling more than corpus size.",
      "body": "At `qa_generation`, you can't sample more than corpus_df. It occurs error.\r\nAdd exception or warning when user try to sample more than length of corpus dataframe.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-12T09:38:53Z",
      "updated_at": "2024-04-14T13:40:23Z",
      "closed_at": "2024-04-14T13:40:23Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/320/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/320",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/320",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.836896",
      "comments": []
    },
    {
      "issue_number": 293,
      "title": "Add recency_filter module",
      "body": "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/RecencyPostprocessorDemo/",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-10T08:11:16Z",
      "updated_at": "2024-04-14T10:57:16Z",
      "closed_at": "2024-04-14T10:57:16Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/293/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/293",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/293",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.836904",
      "comments": []
    },
    {
      "issue_number": 318,
      "title": "Add ASCII art at the begging of trial",
      "body": "like this!\r\n\r\n```\r\n                _        _____            _____ \r\n     /\\        | |      |  __ \\     /\\   / ____|\r\n    /  \\  _   _| |_ ___ | |__) |   /  \\ | |  __ \r\n   / /\\ \\| | | | __/ _ \\|  _  /   / /\\ \\| | |_ |\r\n  / ____ \\ |_| | || (_) | | \\ \\  / ____ \\ |__| |\r\n /_/    \\_\\__,_|\\__\\___/|_|  \\_\\/_/    \\_\\_____|\r\n                                                \r\n                                               \r\n```\r\n\r\nhttps://patorjk.com/software/taag/#p=display&f=Big&t=AutoRAG\r\n\r\nIt will be awesome haha",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-12T06:55:56Z",
      "updated_at": "2024-04-12T13:23:06Z",
      "closed_at": "2024-04-12T13:23:06Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/318/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/318",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/318",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:30.836911",
      "comments": [
        {
          "author": "bwook00",
          "body": "That's really great. I like it",
          "created_at": "2024-04-12T06:56:30Z"
        }
      ]
    },
    {
      "issue_number": 253,
      "title": "Autorag run_web cli relative path. Hard to execute it.",
      "body": "```\r\nautorag run_web --trial_path ./project/0 --project_dir ./project\r\n```\r\nThis works fine, but.\r\n\r\n```\r\nautorag run_web --trial_path ./project/0/ --project_dir ./project\r\n```\r\nThis is not working. It can't find bm25 path.\r\n\r\nThis is awful for usability.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-28T16:31:23Z",
      "updated_at": "2024-04-12T12:51:26Z",
      "closed_at": "2024-04-12T12:51:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/253/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/253",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/253",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.061573",
      "comments": []
    },
    {
      "issue_number": 296,
      "title": "Add similarity percentile cutoff module",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-10T12:34:31Z",
      "updated_at": "2024-04-12T07:05:03Z",
      "closed_at": "2024-04-12T07:05:03Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/296/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/296",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/296",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.061595",
      "comments": []
    },
    {
      "issue_number": 314,
      "title": "Add NER PII-Masking module",
      "body": null,
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-12T06:11:41Z",
      "updated_at": "2024-04-12T06:54:07Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/314/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/314",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/314",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.061602",
      "comments": []
    },
    {
      "issue_number": 273,
      "title": "Implement RAGAS test data generation to AutoRAG (or bridge for convert)",
      "body": "https://docs.ragas.io/en/stable/concepts/testset_generation.html\r\n\r\nRAGAS already supports great ‘synthetic evaluation data generation’\r\nSome modification will be great for AutoRAG evaluation data creation. \r\n\r\nTheir code is Apache 2.0 license like ours, so modification is totally okay! \r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-08T05:51:01Z",
      "updated_at": "2024-04-12T06:12:41Z",
      "closed_at": "2024-04-12T06:12:41Z",
      "labels": [
        "High Priority"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/273/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/273",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/273",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.061609",
      "comments": []
    },
    {
      "issue_number": 313,
      "title": "Add Multi-Step Query Engine module",
      "body": "https://docs.llamaindex.ai/en/stable/examples/query_transformations/SimpleIndexDemo-multistep/",
      "state": "open",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-12T06:10:00Z",
      "updated_at": "2024-04-12T06:10:00Z",
      "closed_at": null,
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/313/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/313",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/313",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.061615",
      "comments": []
    },
    {
      "issue_number": 284,
      "title": "[Question]: OpenAILike with custom OPENAI_API_BASE",
      "body": "Hi, \r\n\r\nis it possible to use custom LLMs hosted privately through the `OpenAILike` object of LlamaIndex with a custom `OPENAI_API_BASE` ? \r\n\r\nI saw the possibility of defining `openailike` in [the docs](https://marker-inc-korea.github.io/AutoRAG/local_model.html#supporting-llm-models) but with no further information regarding the private host.\r\n\r\nThanks! :)",
      "state": "closed",
      "author": "omar-araboghli",
      "author_type": "User",
      "created_at": "2024-04-09T17:06:39Z",
      "updated_at": "2024-04-12T05:47:48Z",
      "closed_at": "2024-04-12T05:47:48Z",
      "labels": [
        "documentation",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/284/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/284",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/284",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.061621",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@omar-araboghli \r\nOf course you can. You can put any parameters from LlamaIndex `OpenAILike` LLM class. [LlamaIndex Docs](https://docs.llamaindex.ai/en/v0.9.48/api_reference/llms/openai_like.html)\r\n\r\n```\r\nnodes:\r\n  - node_line_name: node_line_1\r\n    nodes:\r\n      - node_type: generator\r\n        modu",
          "created_at": "2024-04-10T02:36:25Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I think we need update docs for clarifying this. @bwook00 ",
          "created_at": "2024-04-10T02:37:03Z"
        },
        {
          "author": "omar-araboghli",
          "body": "Thanks @vkehfdl1, I'll give it a try!",
          "created_at": "2024-04-11T07:08:48Z"
        }
      ]
    },
    {
      "issue_number": 299,
      "title": "Add 'Time Reranker' module",
      "body": "For keeping recency information, this `time_reranker` module simply reranks a retrieved passage with datetime. \r\nIt uses a last_modified_datetime that in the corpus.parquet. \r\nUser can specify which metadata column they want to use for reranks by time. \r\nIt have to be an datetime object.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-10T15:39:59Z",
      "updated_at": "2024-04-12T05:37:05Z",
      "closed_at": "2024-04-12T05:37:04Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/299/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/299",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/299",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.262067",
      "comments": []
    },
    {
      "issue_number": 291,
      "title": "Add pass_passage_filter module",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-10T06:38:40Z",
      "updated_at": "2024-04-11T14:32:29Z",
      "closed_at": "2024-04-11T14:32:29Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/291/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/291",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/291",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.262090",
      "comments": []
    },
    {
      "issue_number": 295,
      "title": "Add Flag LLM reranker",
      "body": "While looking for Flag Embedding code, I found the 'Flag LLM reranker' separately, So i am going to make it a module.",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-10T11:02:22Z",
      "updated_at": "2024-04-11T14:06:44Z",
      "closed_at": "2024-04-11T14:06:44Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/295/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/295",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/295",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.262097",
      "comments": []
    },
    {
      "issue_number": 250,
      "title": "Are the sample parquets in huggingface working?",
      "body": "When I run it with triviaqa data, I get a chroma batch size error, so I split it up and send it to fetch_contents and get a size of 0. What's the problem? \r\n\r\nmy test code:\r\nevaluator = Evaluator(qa_data_path='./sample_dataset/dataset/triviaqa/qa_test.parquet', corpus_data_path='./sample_dataset/dataset/triviaqa/corpus.parquet')\r\nevaluator.start_trial('sample_config/compact_local.yaml')\r\n\r\nindex 0 is out of bounds for axis 0 with size 0  ",
      "state": "closed",
      "author": "hofe7",
      "author_type": "User",
      "created_at": "2024-03-28T03:58:21Z",
      "updated_at": "2024-04-10T15:41:07Z",
      "closed_at": "2024-04-10T15:41:07Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/250/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/250",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/250",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.262106",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Hello @hofe7 \r\nThank you for reporting your error.\r\n\r\nCan you tell me where you get the error? The full error track history will be great, which line you get the error. \r\nIt can help us fix this error.\r\n\r\nSince then, I'll try to reproduce this error, too. Thanks.",
          "created_at": "2024-03-28T04:33:58Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@hofe7  In my case, it works fine with triviaQA sample parquet file on embedding whole corpus file.\r\n\r\nWhat did you use for running AutoRAG? \r\nYou use 'compact_local.yaml' which use local embedding 'baai-small'. It can be occur OOM error on your CUDA machine possibly. \r\nIf your local machine is bott",
          "created_at": "2024-03-28T04:55:02Z"
        },
        {
          "author": "hofe7",
          "body": "@vkehfdl1 The same error occurs when using compact_openai.yaml. \r\nOn line 39 of utils/util.py file, index 0 is out of bounds for axis 0 with size 0 ",
          "created_at": "2024-03-28T05:30:14Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Can you give me full error record?  @hofe7 \r\nIt is hard to what is the problem with limited information. \r\nBecause `line 39 of utils/util.py file` used by a lot of features in AutoRAG...",
          "created_at": "2024-03-28T05:44:47Z"
        },
        {
          "author": "vkehfdl1",
          "body": "@hofe7 Are you using Windows? ",
          "created_at": "2024-03-28T06:03:47Z"
        }
      ]
    },
    {
      "issue_number": 283,
      "title": "[Bug] Error at testing flag_embedding_reranker",
      "body": "```python\r\n    @passage_reranker_node\r\n    def flag_embedding_reranker(queries: List[str], contents_list: List[List[str]],\r\n                                scores_list: List[List[float]], ids_list: List[List[str]],\r\n                                top_k: int, batch: int = 64, use_fp16: bool = False,\r\n                                model_name: str = \"BAAI/bge-reranker-large\",\r\n                                ) -> Tuple[List[List[str]], List[List[str]], List[List[float]]]:\r\n        \"\"\"\r\n        Rerank a list of contents based on their relevance to a query using BAAI Reranker model.\r\n    \r\n        :param queries: The list of queries to use for reranking\r\n        :param contents_list: The list of lists of contents to rerank\r\n        :param scores_list: The list of lists of scores retrieved from the initial ranking\r\n        :param ids_list: The list of lists of ids retrieved from the initial ranking\r\n        :param top_k: The number of passages to be retrieved\r\n        :param batch: The number of queries to be processed in a batch\r\n        :param use_fp16: Whether to use fp16 for inference\r\n        :param model_name: The name of the BAAI Reranker model name.\r\n            Default is \"BAAI/bge-reranker-large\"\r\n        :return: tuple of lists containing the reranked contents, ids, and scores\r\n        \"\"\"\r\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n>       model = FlagReranker(\r\n            model_name, use_fp16=use_fp16, device=device\r\n        )\r\nE       TypeError: __init__() got an unexpected keyword argument 'device'\r\n\r\n../../../../autorag/nodes/passagereranker/flag_embedding.py:32: TypeError\r\n```\r\n\r\nI am using FlagEmbedding==1.2.8 and m2 pro\r\nPlease fix this error ASAP. After that, I can release 0.1.2 version.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-09T15:39:55Z",
      "updated_at": "2024-04-10T04:35:26Z",
      "closed_at": "2024-04-10T04:35:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/283/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/283",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/283",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.543764",
      "comments": []
    },
    {
      "issue_number": 228,
      "title": "'Lost in the Middle' reorder module (or possible new node) (or possible new config option for reranker & retrieval)",
      "body": "[Long-context Reorder](https://python.langchain.com/docs/modules/data_connection/retrievers/long_context_reorder) from Langchain.\r\n\r\nIn the `Lost in the Middle` paper, it is well-known that reordering high-relevance context to be first and the last order in the context prompt. \r\n\r\n[How to?]\r\nIt can be four way.\r\n\r\n1. Add as Reranker module. => not suggest\r\nIts feature is simillar with Reranker because it reorder passage order.\r\nBut, in the real world scenario, it is so common to use both reranker and this 'reorder module'.\r\n\r\n2. Add as new node => maybe? But I don't like it.\r\nIt is reordering passages, but it uses only scores from retrieval or reranking. \r\nIt can be whole new node.\r\nHowever, the cons is that it can be enforce so much complexity to choose node in config YAML file.\r\n\r\n3. Add as 'retrieval' and 'rerank' option (or strategy)\r\nIt is possible to add as optional parameter at retrieval and reranker nodes.\r\nIt can be a new 'node parameter'\r\nBut my question is, how to test 'use lost in the middle' or not use? If we want to test this, it can be super complex at `run.py` code, because we have to test it with generator node and its metrics.\r\n\r\n4. Add as 'prompt maker' module.\r\nIt can resolve above issue about 'evaluation' problem.\r\nHowever, from now we don't have `scores` as input of prompt maker.\r\nSo we have to add `scores` as input parameter of prompt maker for making this module as prompt maker module.\r\n\r\n\r\nI think the best option is 'prompt maker' module... What do you think?\r\nNeed to talk about it more.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-16T21:30:07Z",
      "updated_at": "2024-04-09T15:28:33Z",
      "closed_at": "2024-04-09T15:28:33Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/228/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/228",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/228",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.543785",
      "comments": []
    },
    {
      "issue_number": 278,
      "title": "Add Long Context Reorder",
      "body": "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongContextReorder/",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-08T11:30:10Z",
      "updated_at": "2024-04-08T16:13:41Z",
      "closed_at": "2024-04-08T16:13:41Z",
      "labels": [
        "duplicate"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/278/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/278",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/278",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.543793",
      "comments": [
        {
          "author": "bwook00",
          "body": "duplicate isuue #228",
          "created_at": "2024-04-08T16:13:41Z"
        }
      ]
    },
    {
      "issue_number": 276,
      "title": "Add LLM Reranker",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-08T06:58:45Z",
      "updated_at": "2024-04-08T07:57:39Z",
      "closed_at": "2024-04-08T07:57:38Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/276/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/276",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/276",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:31.913896",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "This LLM reranker from LlamaIndex not working well.\r\nThe LLM return only ‘one’ passage, not full passages. \r\n\r\nIt means, if I put three passages it selects only the best one, and ignore the rest ones.\r\n\r\nSince it is crucial to reranking, I decided to not implement this module.\r\n\r\nClosing this issue.",
          "created_at": "2024-04-08T07:57:38Z"
        }
      ]
    },
    {
      "issue_number": 262,
      "title": "Add deep memory retireval",
      "body": "Add Active loop’s Deep Memory to use llama index",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-04-04T04:05:33Z",
      "updated_at": "2024-04-08T04:44:30Z",
      "closed_at": "2024-04-08T04:44:30Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/262/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/262",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/262",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:32.180655",
      "comments": [
        {
          "author": "bwook00",
          "body": "Decided not to support deep memory as a module because it is expensive and not universal",
          "created_at": "2024-04-08T04:44:30Z"
        }
      ]
    },
    {
      "issue_number": 268,
      "title": "Add Jina Reranker",
      "body": "https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/JinaRerank/",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-05T06:20:02Z",
      "updated_at": "2024-04-06T11:26:35Z",
      "closed_at": "2024-04-06T11:26:35Z",
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/268/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/268",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/268",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:32.444205",
      "comments": []
    },
    {
      "issue_number": 266,
      "title": "Add BERT score metric",
      "body": "In the sem score [paper](https://arxiv.org/pdf/2401.17072.pdf), BERT score has high correlation against human evaluation. \r\nIt can be helpful to add this to our metric.\r\n\r\n+ Plus, it can be great for multilingual setups.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-04-04T13:00:31Z",
      "updated_at": "2024-04-05T09:06:19Z",
      "closed_at": "2024-04-05T09:06:19Z",
      "labels": [
        "Strategy"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/266/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/266",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/266",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:32.444227",
      "comments": []
    },
    {
      "issue_number": 217,
      "title": "Add \"AutoRetrieval\" as retrieval module.",
      "body": "https://docs.llamaindex.ai/en/stable/examples/vector_stores/chroma_auto_retriever.html\r\n\r\n`AutoRetrieval` is a retrieval module that LLM make 'metadata' search query with given user query and metadata descriptions.\r\nUser must prepare detailed metadata at corpus.parquet. \r\nThe core part of llm that selected and make search query, follow llama index code.\r\nFor searching it, making custom function for filtering from corpus.parquet needed.\r\n",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-08T18:31:03Z",
      "updated_at": "2024-04-01T06:52:03Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/217/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/217",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/217",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:32.444235",
      "comments": []
    },
    {
      "issue_number": 92,
      "title": "llm token management + strategy",
      "body": "Need to manage input token on node using LLM. Will be supported in the next version.",
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-02-01T16:42:58Z",
      "updated_at": "2024-03-31T15:44:42Z",
      "closed_at": "2024-03-31T15:44:42Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/92/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/92",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/92",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:32.444244",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "[What to do?]\r\n\r\nAt first, this is a 'token strategy'. It means, you can set token limitation when optimizing each process. Some process consume many token to llm, and it effects overall pricing and speed.\r\nPlus, it matters because the devs sometimes wants to monitor their token usage. \r\n\r\n[Suggesti",
          "created_at": "2024-03-12T22:18:50Z"
        }
      ]
    },
    {
      "issue_number": 229,
      "title": "Add gif description for easy-to understand this project",
      "body": "Consider making awesome gif that explain or executing `AutoRAG` and add it to README.md.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-17T04:04:00Z",
      "updated_at": "2024-03-29T08:20:35Z",
      "closed_at": "2024-03-29T08:20:35Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/229/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/229",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/229",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:32.667253",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "+ add vllm\r\n+ edit config YAML file in README",
          "created_at": "2024-03-26T06:04:38Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Make it simple.\r\nEasy to understand the concept.",
          "created_at": "2024-03-26T11:22:36Z"
        }
      ]
    },
    {
      "issue_number": 251,
      "title": "Can’t calculate retrieval scores on multiple OR ground truths",
      "body": "While benchmarking triviaQA, the weird thing happend. \r\n\r\n![image](https://github.com/Marker-Inc-Korea/AutoRAG/assets/28955029/86112de8-9c0a-4135-83ee-6e86445b9582)\r\n\r\nAs you see, there are duplicated id with ground truth and the retrieved_ids. But all metrics got 0. This is not just one row, for the all row.\r\nSomething wrong at the retrieval metrics. Need to inspect carefully.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-28T05:46:43Z",
      "updated_at": "2024-03-29T05:08:27Z",
      "closed_at": "2024-03-29T05:08:27Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/251/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/251",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/251",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:32.868991",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "At first, the metric itself has no problem. We cover this case in our test code. And nothing matters. Something happened in `run.py`",
          "created_at": "2024-03-28T05:51:35Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I find the reason.\r\n\r\nBecause the `retrieval_gt` on trivia qa is 1-d list. And there is nothing to cast qa retrieval gt to 2-d list at the start of evaluator. \r\nThe `run.py` get retrieval_gt from data folder. It always occur error. \r\n\r\nAt conclusion, I must add 'cast_qa' data function to start of da",
          "created_at": "2024-03-28T06:30:01Z"
        }
      ]
    },
    {
      "issue_number": 241,
      "title": "Implement run as cli command 'restart_trial'",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-03-25T05:52:49Z",
      "updated_at": "2024-03-26T11:34:14Z",
      "closed_at": "2024-03-26T11:34:14Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/241/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/241",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/241",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.129672",
      "comments": []
    },
    {
      "issue_number": 201,
      "title": "Implement run as cli command 'extract_best config'",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-03-05T16:20:18Z",
      "updated_at": "2024-03-26T11:34:14Z",
      "closed_at": "2024-03-26T11:34:14Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/201/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/201",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/201",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.129695",
      "comments": [
        {
          "author": "bwook00",
          "body": "In the CLI, at the end of evaluate, it should ask \"do you want to extract best config? [yes/no]\" and if yes, it would be better if extract_best_config is executed immediately.",
          "created_at": "2024-03-17T10:38:17Z"
        },
        {
          "author": "bwook00",
          "body": "What I commented above will be done in issue#248 ",
          "created_at": "2024-03-26T08:37:56Z"
        }
      ]
    },
    {
      "issue_number": 93,
      "title": "\bImplement restart point where the error occurred.",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-02-01T16:54:23Z",
      "updated_at": "2024-03-25T14:02:29Z",
      "closed_at": "2024-03-25T14:02:29Z",
      "labels": [
        "enhancement",
        "help wanted",
        "High Priority"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/93/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/93",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/93",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.339174",
      "comments": [
        {
          "author": "bwook00",
          "body": "0. Save the restart result value in the `trial_path` where the error occurred.\r\n1. get \"trial_path\" as an input parameter to the Restart function\r\n2. base the restart on the presence of a `summary.csv` file\r\n    - Does node_line have a summary.csv?\r\n        - Does Node's summary.csv exist?\r\n\t=> exec",
          "created_at": "2024-03-08T16:15:11Z"
        }
      ]
    },
    {
      "issue_number": 243,
      "title": "Upgrade github actions to v4",
      "body": "Upgrade github actions checkout to v4. \r\nCompatible issue with Node.js 20.\r\nhttps://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-25T06:31:35Z",
      "updated_at": "2024-03-25T10:02:54Z",
      "closed_at": "2024-03-25T10:02:54Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/243/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/243",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/243",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.552714",
      "comments": []
    },
    {
      "issue_number": 233,
      "title": "add llama index qa generation => qa.parquet",
      "body": "It can be a new feature or just documentation update.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-21T14:24:29Z",
      "updated_at": "2024-03-25T07:31:00Z",
      "closed_at": "2024-03-25T07:31:00Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/233/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/233",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/233",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.552736",
      "comments": []
    },
    {
      "issue_number": 237,
      "title": "OpenAI embedding surpass token limit error",
      "body": "When you try to embed over 8192 tokens, there is an error at openai embedding. \nHuggingface embedding manage itself automatically its token size, but maybe openai embedding does not do that.\n\nIt can resolve in our codebase temporaly, but I think we can contribute to llama index. \n\n![1711161029285.png](https://github.com/Marker-Inc-Korea/AutoRAG/assets/28955029/d23c427a-1558-48dc-a013-4fa7ca4a5d69)\n\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-23T02:30:42Z",
      "updated_at": "2024-03-25T07:10:48Z",
      "closed_at": "2024-03-25T07:10:48Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/237/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/237",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/237",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.552743",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "There is no such feature that calculate each token size and do not execute embedding in LlamaIndex.\r\n\"Why.....?\"",
          "created_at": "2024-03-25T04:03:42Z"
        }
      ]
    },
    {
      "issue_number": 238,
      "title": "Add colab link to README",
      "body": "There are tutorial colab thanks to @hongsw \r\n\r\nAdd colab link for beginners.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-25T03:34:30Z",
      "updated_at": "2024-03-25T06:40:26Z",
      "closed_at": "2024-03-25T06:40:26Z",
      "labels": [
        "documentation",
        "enhancement"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/238/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/238",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/238",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.729974",
      "comments": [
        {
          "author": "bwook00",
          "body": "Thank you very much @hongsw",
          "created_at": "2024-03-25T04:08:52Z"
        }
      ]
    },
    {
      "issue_number": 232,
      "title": "add corpus.parquet convert method from langchain & llamaindex",
      "body": "User can generate corpus.parquet dataframe from llamaindex document or textnode, or langchain documents.\r\nIn this way, user can easily use pre-existed load and chunking method from langchain and llamaindex.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-21T12:13:43Z",
      "updated_at": "2024-03-22T16:56:29Z",
      "closed_at": "2024-03-22T16:56:29Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/232/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/232",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/232",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.912949",
      "comments": []
    },
    {
      "issue_number": 231,
      "title": "add llama_index qa generation function",
      "body": "guidance is quite limited to generate QA set.\r\nWe can use prompt and llama index to generate QA set at various model setups.\r\n\r\nMigrate llama index generation setups to AutoRAG for easy-to-use",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-21T12:10:47Z",
      "updated_at": "2024-03-22T09:14:26Z",
      "closed_at": "2024-03-22T09:14:26Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/231/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/231",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/231",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.912963",
      "comments": []
    },
    {
      "issue_number": 44,
      "title": "Add n-gram metric options",
      "body": "From PR #40 \r\n\r\nI think these n-gram based metrics can open some options.\r\n- N : int  minimum unit for n-gram\r\n- tokenizer : select tokenizer. (according to language, method etc include in tokenizer, tokenizer can influence value of each metric)\r\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \r\n- etc\r\n\r\n\r\n---\r\n\r\n- [x] #222\r\n- [x] #223",
      "state": "closed",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-01-23T11:27:43Z",
      "updated_at": "2024-03-16T21:17:08Z",
      "closed_at": "2024-03-16T21:17:08Z",
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/44/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/44",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/44",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:33.912967",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "How can get and write option about metrics at yaml file? It can be tricky, but if it is necessary, we'll find a way.",
          "created_at": "2024-01-23T17:53:50Z"
        },
        {
          "author": "vkehfdl1",
          "body": "close this issue because all generation metric has options. \r\nDONE!",
          "created_at": "2024-03-16T21:17:08Z"
        }
      ]
    },
    {
      "issue_number": 222,
      "title": "Add additional options for meteor metric.",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-12T22:29:47Z",
      "updated_at": "2024-03-16T21:16:15Z",
      "closed_at": "2024-03-16T21:16:15Z",
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/222/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/222",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/222",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:34.113633",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://huggingface.co/spaces/evaluate-metric/meteor/blame/d33847fd9d688beb98d7577c2960b006d361336a/meteor.py",
          "created_at": "2024-03-15T10:35:52Z"
        }
      ]
    },
    {
      "issue_number": 223,
      "title": "Add additional options for bleu metric.",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-12T22:29:53Z",
      "updated_at": "2024-03-16T21:16:14Z",
      "closed_at": "2024-03-16T21:16:14Z",
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/223/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/223",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/223",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:36.329831",
      "comments": []
    },
    {
      "issue_number": 159,
      "title": "Add Cohere Reranker",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-10T21:38:39Z",
      "updated_at": "2024-03-15T15:43:18Z",
      "closed_at": "2024-03-15T15:43:18Z",
      "labels": [
        "New Module"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/159/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/159",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/159",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:36.329859",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://txt.cohere.com/rerank/\r\nhttps://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html",
          "created_at": "2024-03-05T15:30:36Z"
        }
      ]
    },
    {
      "issue_number": 36,
      "title": "[Hotfix] Window OS error",
      "body": "I had run python -m pytest in terminal\r\n\r\nand I got this Error messege\r\n\r\n\r\n~~~\r\n======================================================================================================= short test summary info =======================================================================================================\r\nFAILED tests/autorag/test_evaluator.py::test_start_trial - OSError: [Errno 22] Invalid argument: 'C:\\\\Users\\\\hanpa\\\\PycharmProjects\\\\AutoRAG\\\\0\\\\retrieve_node_line\\\\retrieval\\\\bm25=>top_k_50.parquet'\r\nFAILED tests/autorag/nodes/retrieval/test_run_retrieval_node.py::test_run_retrieval_node - OSError: [Errno 22] Invalid argument: 'C:\\\\Users\\\\hanpa\\\\PycharmProjects\\\\AutoRAG\\\\tests\\\\resources\\\\test_project\\\\test_trial\\\\test_node_line\\\\retrieval\\\\bm25=>top_k_4.parquet'\r\n============================================================================================= 2 failed, 26 passed, 23 warnings in 24.79s ==============================================================================================\r\n(venv) PS C:\\Users\\hanpa\\PycharmProjects\\AutoRAG>\r\n",
      "state": "closed",
      "author": "Eastsidegunn",
      "author_type": "User",
      "created_at": "2024-01-22T10:40:56Z",
      "updated_at": "2024-03-15T10:31:33Z",
      "closed_at": "2024-03-15T10:31:32Z",
      "labels": [
        "bug",
        "wontfix"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/36/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/36",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/36",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:36.535260",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "@Eastsidegunn Is it fix at the latest version of v0.0.11?\r\nIf not, how can I reproduce this bug?",
          "created_at": "2024-03-07T17:06:39Z"
        },
        {
          "author": "Eastsidegunn",
          "body": "@vkehfdl1 sad news...\r\n\r\n```\r\n===================================================================================================== short test summary info ===================================================================================================== \r\nFAILED tests/autorag/evaluate/metric/tes",
          "created_at": "2024-03-13T04:44:29Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I looked all error on the windows os. \r\n**I won’t fix this issue**\r\n\r\n1. `test_g_eval_full` failed is sometimes happend, because it depends LLM output.\r\n2. `FileNotFoundError` is occured because `tempfile.NamedTemporaryFile()` in Windows.\r\nhttps://docs.python.org/ko/3/library/tempfile.html\r\n```\r\nOn ",
          "created_at": "2024-03-14T12:53:27Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Closing this issue since we will not fix this issue.",
          "created_at": "2024-03-15T10:31:32Z"
        }
      ]
    },
    {
      "issue_number": 215,
      "title": "Surpass context length when calculating sem_score.",
      "body": "[What is the problem?]\r\nWhile using sem_score with default mp_net or other local model, you can face ‘AssertionError’ when running in CUDA.\r\nIt is because input sentence surpass its context length limitation. \r\nWe have to prevent this kind of error.\r\n\r\n[How to?]\r\nI think there is a few method to prevent this. \r\n1. Max length option?\r\n2. Auto-retry\r\n3. 1+2\r\nOr any other solution will be welcome. Choose wisely.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-07T21:47:02Z",
      "updated_at": "2024-03-14T06:51:24Z",
      "closed_at": "2024-03-14T06:51:24Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/215/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/215",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/215",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:36.808063",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "It looks like LlamaIndex supports this kind of feature as default in `HuggingFaceEmbedding`.\r\n\r\n```python\r\nencoded_input = self._tokenizer(\r\n            sentences,\r\n            padding=True,\r\n            max_length=self.max_length,\r\n            truncation=True,\r\n            return_tensors=\"pt\",\r\n   ",
          "created_at": "2024-03-12T21:19:40Z"
        },
        {
          "author": "vkehfdl1",
          "body": "It was simple. The mp-net model config.json itself was the problem...",
          "created_at": "2024-03-12T21:42:31Z"
        }
      ]
    },
    {
      "issue_number": 218,
      "title": "Add 'vllm' module documentation",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-09T14:50:22Z",
      "updated_at": "2024-03-14T06:40:13Z",
      "closed_at": "2024-03-14T06:40:13Z",
      "labels": [
        "documentation",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/218/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/218",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/218",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.061430",
      "comments": []
    },
    {
      "issue_number": 198,
      "title": "Multi-turn chat support",
      "body": "It is hard and really vague issue name.\r\nBut, the real RAG must have multi-turn chat. \r\nHave conversation history and use it. \r\n\r\nHow can we cover this?\r\nThe creation of dataset.\r\nThe deployment. \r\nThe optimization.\r\n\r\nEverything can be obstacles to implement multi-turn chat.\r\n\r\nThink it can be a major update.",
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-01T23:02:08Z",
      "updated_at": "2024-03-12T22:30:49Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/198/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/198",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/198",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.061455",
      "comments": []
    },
    {
      "issue_number": 214,
      "title": "Add vllm 'llm module' for faster generation performance",
      "body": "LlamaIndex supports `vllm` model. However, it is too slow at batch processing. It literally do not support async inference.\r\nLook at my issue at [here](https://github.com/run-llama/llama_index/issues/10496).\r\nSo, it does not support any `async` call and it is super slow. It is basically linear process.\r\n\r\nI looked at both `llama_index` and `vllm` code, and I found this at vllm code.\r\n```python\r\ndef generate(\r\n        self,\r\n        prompts: Optional[Union[str, List[str]]] = None,\r\n        sampling_params: Optional[SamplingParams] = None,\r\n        prompt_token_ids: Optional[List[List[int]]] = None,\r\n        use_tqdm: bool = True,\r\n        lora_request: Optional[LoRARequest] = None,\r\n    ) -> List[RequestOutput]:\r\n```\r\nYou can see that you can put multiple prompts at once. \r\nBut look at `llama_index.complete()` method.\r\n```python\r\n    def complete(\r\n        self, prompt: str, formatted: bool = False, **kwargs: Any\r\n    ) -> CompletionResponse:\r\n        kwargs = kwargs if kwargs else {}\r\n        params = {**self._model_kwargs, **kwargs}\r\n\r\n        from vllm import SamplingParams\r\n\r\n        # build sampling parameters\r\n        sampling_params = SamplingParams(**params)\r\n        outputs = self._client.generate([prompt], sampling_params)\r\n        return CompletionResponse(text=outputs[0].outputs[0].text)\r\n```\r\n\r\nJust using \"only first\" index of input list.\r\n\r\nThink about any ML model. You can multiple inputs as batch, and it process async at \"GPU\" level, until it is not occur OOM error.\r\nI will set proper batch and put to vllm instance directly, since there is **no way** to support real async operation of Vllm model. \r\nBecause their abstraction methods only supports \"single prompt\" as input.\r\n\r\nIn my opinion, the optimization of processing speed itself is crucial to AutoRAG. Because it takes so....long time;; I had to optimize it. ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-07T21:24:33Z",
      "updated_at": "2024-03-08T23:59:10Z",
      "closed_at": "2024-03-08T23:59:09Z",
      "labels": [
        "enhancement",
        "New Module",
        "Low-Level"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/214/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/214",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/214",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.061463",
      "comments": []
    },
    {
      "issue_number": 207,
      "title": "late init embedding model",
      "body": "When embedding model initialization, it is likely to be downloaded all embedding models. \r\nIt is super inefficient since someone don't want to use local embedding, or only one embedding.\r\n\r\n[What to fix]\r\n- All embedding model instances is initializing at autorag/__init__.py. Late initialization all embedding models.\r\n\r\n[How to]\r\n- Suggest to search about `python late init`\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-06T22:09:49Z",
      "updated_at": "2024-03-08T16:24:48Z",
      "closed_at": "2024-03-08T16:24:48Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/207/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/207",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/207",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.061472",
      "comments": []
    },
    {
      "issue_number": 205,
      "title": "unallocate gpu CUDA memory after processing each reranker, metric model, embedding model",
      "body": "At intense optimization, lots of models allocate to gpu memory.\r\nThis happens in single process, so it is hard to orchestrate each models. \r\nSo it must unallocate gpu cuda memory when each node(or module) is done.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-03-05T23:34:47Z",
      "updated_at": "2024-03-07T10:52:01Z",
      "closed_at": "2024-03-07T10:52:01Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/205/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/205",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/205",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.061484",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Embedding model must implement #207 first.\r\nSo, in this issue, I will not implement unallocating embedding models.",
          "created_at": "2024-03-06T22:13:34Z"
        }
      ]
    },
    {
      "issue_number": 172,
      "title": "Add gradio or streamlit demo for running optimized pipeline",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-15T10:24:37Z",
      "updated_at": "2024-03-05T21:20:46Z",
      "closed_at": "2024-03-05T21:20:46Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/172/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/172",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/172",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.273448",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://github.com/jonfairbanks/local-rag\r\n\r\nThe great example of GUI for RAG.",
          "created_at": "2024-02-27T18:43:33Z"
        }
      ]
    },
    {
      "issue_number": 183,
      "title": "Runner with vectordb ingest all again",
      "body": "When I use Runner with vectordb, it seems like vectordb ingest all thing.\r\nWhy is that? \r\nI get numerous warning about add of existing id from hnsw.\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-17T09:41:22Z",
      "updated_at": "2024-02-24T08:12:33Z",
      "closed_at": "2024-02-24T08:12:33Z",
      "labels": [
        "bug",
        "wontfix"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/183/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1",
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/183",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/183",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.457678",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://github.com/chroma-core/chroma/issues/1733\r\n\r\nIt actually did not ingest anything. ",
          "created_at": "2024-02-18T17:51:52Z"
        },
        {
          "author": "vkehfdl1",
          "body": "## How to Fix this?\r\n\r\nGo to your lib folder and go to here `chromadb/segment/impl/vector/local_persistent_hnsw.py`\r\n\r\nAt the line of 271, change to this\r\n\r\n```\r\nelse:\r\n    pass\r\n    # logger.warning(f\"Add of existing embedding ID: {id}\")\r\n```\r\n\r\nThen the awful logging will vanish.",
          "created_at": "2024-02-18T17:53:08Z"
        },
        {
          "author": "vkehfdl1",
          "body": "I think I will submit this at troubleshooting docs.",
          "created_at": "2024-02-19T14:12:55Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Closing this issue since this is not a AutoRAG bug.",
          "created_at": "2024-02-24T08:12:33Z"
        }
      ]
    },
    {
      "issue_number": 160,
      "title": "Refactor all test codes for fast testing and auto testing",
      "body": "We disabled our test CI because of the slow speed plus storage limitation.\r\nWe have to optimize this.\r\nFor example, LlamaIndex is large and complex framework with hundreds of unit test, but it takes only 19 seconds to test all things.\r\nHowever, ours was more than 5 minutes and it is now not working at default ubunutu runner in github.\r\n\r\nThe suggest workflow is like below.\r\n\r\n1. Do not use real LLM. Just use `MockLLM` for mocking the functionality of LLM. \r\n2. Use MonkeyPatch to using LLM or local Model. Testing and donwloading whole model like TART or MonoT5 tooks long time and large download size.\r\n3. Do not use real embedding model. Use Mock embedding model or monkey patch.\r\n\r\nAfter making test faster and lighter, we will enable auto-testing with CI again.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-13T11:23:00Z",
      "updated_at": "2024-02-22T20:08:30Z",
      "closed_at": "2024-02-22T20:08:30Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/160/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/160",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/160",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.690898",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "https://moons08.github.io/programming/pytest_2/",
          "created_at": "2024-02-14T20:15:50Z"
        },
        {
          "author": "vkehfdl1",
          "body": "Plus, from LlamaIndex v0.10.0, `MockLLM` turns to deprecated. (Legacy code)\r\nConsider making OpenAI MonkeyPatch function for testing.",
          "created_at": "2024-02-14T20:16:53Z"
        }
      ]
    },
    {
      "issue_number": 190,
      "title": "Add Ko-Reranker",
      "body": "https://huggingface.co/Dongjin-kr/ko-reranker\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-22T09:54:47Z",
      "updated_at": "2024-02-22T15:55:45Z",
      "closed_at": "2024-02-22T15:55:45Z",
      "labels": [
        "enhancement",
        "New Module"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/190/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/190",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/190",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:37.879872",
      "comments": [
        {
          "author": "hongsw",
          "body": "I like this issue",
          "created_at": "2024-02-22T10:03:01Z"
        }
      ]
    },
    {
      "issue_number": 120,
      "title": "Do not embedding corpus again when id already existed in chroma vectordb collection",
      "body": "From now, there are no such feature.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-04T15:29:36Z",
      "updated_at": "2024-02-18T17:54:28Z",
      "closed_at": "2024-02-18T17:54:28Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/120/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/120",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/120",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.139969",
      "comments": []
    },
    {
      "issue_number": 184,
      "title": "[HotFix] Can’t run `Runner` more than two times",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-18T10:34:01Z",
      "updated_at": "2024-02-18T17:27:40Z",
      "closed_at": "2024-02-18T17:27:40Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/184/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/184",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/184",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.139990",
      "comments": []
    },
    {
      "issue_number": 168,
      "title": "Divide cli options to cli.py",
      "body": "There are cli options at `evaluator.py` from now.\r\n\r\n- Make cli.py at autorag root folder\r\n- Move all cli to `cli.py` file.\r\n\r\nThis needs to perform before you start to make easy-to-start CLI form for newbies.\r\nBecause it uses cli, and needs to be separate because we have more than three cli commands now.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-14T20:08:34Z",
      "updated_at": "2024-02-18T10:09:00Z",
      "closed_at": "2024-02-18T10:09:00Z",
      "labels": [
        "enhancement",
        "High Priority",
        "High-Level"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/168/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/168",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/168",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140001",
      "comments": []
    },
    {
      "issue_number": 158,
      "title": "Add warning about index of pd.DataFrame",
      "body": "When you input qa dataset, the index must be reset.\r\n\r\nWe can add `ignore_index` option at every concat of best_result and previous_result in `run.py`",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-10T16:32:51Z",
      "updated_at": "2024-02-16T08:08:50Z",
      "closed_at": "2024-02-16T08:08:50Z",
      "labels": [
        "bug",
        "documentation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/158/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/158",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/158",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140010",
      "comments": []
    },
    {
      "issue_number": 170,
      "title": "Use pytest-xdist for faster pytest",
      "body": "https://velog.io/@sangyeon217/pytest-plugin-pytest-xdist\r\n\r\nRecommend tempfile to all fixtures, because it is concurrent and the result affected by I/O ",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-15T09:13:34Z",
      "updated_at": "2024-02-15T09:59:33Z",
      "closed_at": "2024-02-15T09:59:33Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/170/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/170",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/170",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140017",
      "comments": []
    },
    {
      "issue_number": 131,
      "title": "Add env variable for easy-to-use ",
      "body": "When user needs to write their api key or api base directly to yaml, it is much better to use env varaible.\r\nBut, yaml doesn't support env variable as default.\r\nSo, it will be great we support env variable at config yaml file.\r\n\r\nBelow is the phind answer for making env varaible in yaml file.\r\n\r\n---\r\n\r\nTo use environment variables inside a YAML file when loading it with PyYAML's `safe_load()` function, you need to add custom logic to find and resolve these environment variables. This involves defining a pattern to identify environment variables, creating a custom constructor to replace placeholders with actual environment variable values, and registering this constructor with PyYAML.\r\n\r\nHere's a step-by-step guide based on the provided sources:\r\n\r\n1. **Define a Pattern to Identify Environment Variables**: Use a regular expression to match your environment variable placeholders. For example, if your placeholders look like `${ENV_VAR}`, you can use the following pattern:\r\n   ```python\r\n   import re\r\n   env_pattern = re.compile(r\".*?\\${(.*?)}.*?\")\r\n   ```\r\n\r\n2. **Create a Custom Constructor**: This function will be called by PyYAML when it encounters a node that matches the pattern defined in the previous step. It should replace the placeholder with the actual value of the environment variable:\r\n   ```python\r\n   import os\r\n   def env_constructor(loader, node):\r\n       value = loader.construct_scalar(node)\r\n       matches = env_pattern.findall(value)\r\n       for match in matches:\r\n           value = value.replace(f\"${{{match}}}\", os.environ.get(match, \"\"))\r\n       return value\r\n   ```\r\n\r\n3. **Register the Pattern and Constructor with PyYAML**: Before calling `safe_load()`, you need to tell PyYAML about your custom pattern and constructor. You also need to specify a custom tag (e.g., `!env`) to trigger your constructor:\r\n   ```python\r\n   import yaml\r\n   yaml.add_implicit_resolver(\"!env\", env_pattern)\r\n   yaml.add_constructor(\"!env\", env_constructor)\r\n   ```\r\n\r\n4. **Load Your YAML File**: Now, you can load your YAML file. Any occurrences of `${ENV_VAR}` will be replaced with the corresponding environment variable's value:\r\n   ```python\r\n   with open('your_file.yaml', 'r') as file:\r\n       config = yaml.safe_load(file)\r\n       print(config)\r\n   ```\r\n\r\nHere's an example of what your YAML file might look like:\r\n```yaml\r\nconfig:\r\n  username: admin\r\n  password: ${SERVICE_PASSWORD}\r\n  service: https://${SERVICE_HOST}/service\r\n```\r\n\r\nAnd here's how you might set the environment variables in a Unix-like shell before running your Python script:\r\n```shell\r\nexport SERVICE_PASSWORD=yourpassword\r\nexport SERVICE_HOST=yourhost.com\r\n```\r\n\r\nThis approach allows you to dynamically configure your application based on the environment it's running in, making it easier to manage configurations across different deployment environments without hardcoding sensitive information into your codebase or YAML files.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-06T08:50:43Z",
      "updated_at": "2024-02-15T09:58:16Z",
      "closed_at": "2024-02-15T09:58:16Z",
      "labels": [
        "enhancement",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/131/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/131",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/131",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140024",
      "comments": []
    },
    {
      "issue_number": 162,
      "title": "LlamaIndex Version Upgrade to v0.10.1",
      "body": "There was a really huge version upgrade at LlamaIndex.\r\nSince we are using lots of utils from LlamaIndex, it is inevitable to migrate to v0.10.1\r\n\r\n- [Migration Guide](https://github.com/run-llama/llama_index/pull/10537)\r\n\r\nBefore this, the new installed user might have problems.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-13T18:11:19Z",
      "updated_at": "2024-02-14T20:47:57Z",
      "closed_at": "2024-02-14T20:47:57Z",
      "labels": [
        "enhancement",
        "dependencies",
        "High Priority"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/162/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/162",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/162",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140033",
      "comments": []
    },
    {
      "issue_number": 154,
      "title": "Add new metrics for generator evaluation ‘SEMSCORE'",
      "body": "[paper](https://arxiv.org/pdf/2401.17072.pdf)\r\n\r\n### Overview\r\nSemScore is a simple metric. Just embeds generate ground truth and model’s generation, and calculates cosine similarity.\r\n\r\nEven it is quite simple, correlation with human evaluation and SemScore evaluation is quite high comparing between other evaluation scores. \r\nWe knew that traditional generation evaluation metrics are quite bad. This can be the great solution for us.  \r\n\r\n## To-do list\r\n- [x] add [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) embedding to support embedding models\r\n- [x] Get metrics via dictionary with their options. Enabling this will solve the issue #44 either.\r\n- [x] Make SemScore metrics. Default embedding model will be ‘all-mpnet-base-v2’, but I’m sure other embdding model can be effective.\r\n- [x] Make documentation for this metric. (We don’t have documentation for selecting rigth strategy for each module)\r\n\r\n",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-09T11:35:11Z",
      "updated_at": "2024-02-10T14:23:15Z",
      "closed_at": "2024-02-10T14:23:15Z",
      "labels": [
        "enhancement",
        "Strategy"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/154/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/154",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/154",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140041",
      "comments": []
    },
    {
      "issue_number": 80,
      "title": "Add Answer filter node",
      "body": null,
      "state": "open",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-01-31T15:46:07Z",
      "updated_at": "2024-02-09T10:53:46Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "New Node"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/80/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/80",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/80",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140048",
      "comments": []
    },
    {
      "issue_number": 100,
      "title": "Add samples of yaml file for launching",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-03T10:20:19Z",
      "updated_at": "2024-02-08T12:43:11Z",
      "closed_at": "2024-02-08T12:43:11Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/100/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/100",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/100",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.140057",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "close this as completed",
          "created_at": "2024-02-08T12:43:11Z"
        }
      ]
    },
    {
      "issue_number": 130,
      "title": "[HotFix] Extract strategy at deploy-test yaml file for running test.",
      "body": "You can't run test evaluation without strategy.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-06T08:46:34Z",
      "updated_at": "2024-02-06T14:12:39Z",
      "closed_at": "2024-02-06T14:12:39Z",
      "labels": [
        "bug",
        "High Priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/130/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/130",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/130",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:38.369275",
      "comments": []
    },
    {
      "issue_number": 106,
      "title": "Edit brand-new READMD.md",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-03T17:14:48Z",
      "updated_at": "2024-02-03T21:13:11Z",
      "closed_at": "2024-02-03T21:13:10Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/106/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/106",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/106",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:40.042801",
      "comments": []
    },
    {
      "issue_number": 91,
      "title": "Add Hybrid Retrieval \"Deploy\" Function.",
      "body": "It is so confusing and difficult to make deploy function at retrieval_node for hybrid retrieval.\r\nSo, I have to make deploy function for hybrid retrieval.\r\nPlus, I have to make hybrid retrieval yaml file for deployment.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-01T16:04:23Z",
      "updated_at": "2024-02-03T20:08:35Z",
      "closed_at": "2024-02-03T20:08:35Z",
      "labels": [
        "High Priority"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/91/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/91",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/91",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:40.042831",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "This issue must be include full Runner test.",
          "created_at": "2024-02-03T11:14:51Z"
        }
      ]
    },
    {
      "issue_number": 83,
      "title": "potentially Runner.run function will not work.",
      "body": "It override previous result, so delete all columns except result column.\r\nSo, it will can't preserve older results, it can be cause malfunction at certain RAG workflows.",
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-01-31T17:35:11Z",
      "updated_at": "2024-02-03T19:19:28Z",
      "closed_at": "2024-02-03T19:19:28Z",
      "labels": [
        "High Priority"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/83/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "vkehfdl1"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/83",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/83",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:40.258981",
      "comments": [
        {
          "author": "vkehfdl1",
          "body": "Need to make full run.yaml for testing.",
          "created_at": "2024-02-02T08:25:00Z"
        }
      ]
    },
    {
      "issue_number": 104,
      "title": "Add LLM Models",
      "body": null,
      "state": "closed",
      "author": "vkehfdl1",
      "author_type": "User",
      "created_at": "2024-02-03T14:13:38Z",
      "updated_at": "2024-02-03T19:10:14Z",
      "closed_at": "2024-02-03T19:10:14Z",
      "labels": [
        "High Priority"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/104/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/104",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/104",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:40.527056",
      "comments": []
    },
    {
      "issue_number": 60,
      "title": "Add Embedding models",
      "body": null,
      "state": "closed",
      "author": "bwook00",
      "author_type": "User",
      "created_at": "2024-01-25T18:07:25Z",
      "updated_at": "2024-02-03T19:10:13Z",
      "closed_at": "2024-02-03T19:10:13Z",
      "labels": [
        "High Priority"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/60/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "bwook00"
      ],
      "milestone": null,
      "html_url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/60",
      "api_url": "https://api.github.com/repos/Marker-Inc-Korea/AutoRAG/issues/60",
      "repository": "Marker-Inc-Korea/AutoRAG",
      "extraction_date": "2025-06-22T00:41:40.527082",
      "comments": []
    }
  ]
}