{
  "repository": "pydantic/pydantic-ai",
  "repository_info": {
    "repo": "pydantic/pydantic-ai",
    "stars": 10368,
    "language": "Python",
    "description": "Agent Framework / shim to use Pydantic with LLMs",
    "url": "https://github.com/pydantic/pydantic-ai",
    "topics": [
      "agent-framework",
      "llms",
      "pydantic",
      "python"
    ],
    "created_at": "2024-06-21T15:55:04Z",
    "updated_at": "2025-06-22T01:39:37Z",
    "search_query": "ai agent language:python stars:>3 -framework",
    "total_issues_estimate": 120,
    "labeled_issues_estimate": 105,
    "labeling_rate": 87.9,
    "sample_labeled": 29,
    "sample_total": 33,
    "has_issues": true,
    "repo_id": 818331198,
    "default_branch": "main",
    "size": 27150
  },
  "extraction_date": "2025-06-22T00:28:48.430403",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 500,
  "issues": [
    {
      "issue_number": 631,
      "title": "MALFORMED_FUNCTION finishReason in Gemini candidate",
      "body": "I've been getting these sporadically when using Gemini:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspaces/cardcraft/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 249, in run\r\n    model_response, request_usage = await agent_model.request(messages, model_settings)\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspaces/cardcraft/.venv/lib/python3.12/site-packages/pydantic_ai/models/gemini.py\", line 176, in request\r\n    response = _gemini_response_ta.validate_json(await http_response.aread())\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspaces/cardcraft/.venv/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 446, in validate_json\r\n    return self.validator.validate_json(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\npydantic_core._pydantic_core.ValidationError: 2 validation errors for typed-dict\r\ncandidates.0.content\r\n  Field required [type=missing, input_value={'finishReason': 'MALFORMED_FUNCTION_CALL'}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\r\ncandidates.0.finishReason\r\n  Input should be 'STOP' or 'MAX_TOKENS' [type=literal_error, input_value='MALFORMED_FUNCTION_CALL', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error\r\n```\r\n\r\nhttps://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/gemini.py#L611 only allows for `STOP`or `MAX_TOKENS`.\r\nIn addition, when `MALFORMED_FUNCTION` is the finish reason, no `content` is returned, so just adding the extra value to the literal definition isn't enough.\r\n\r\n**Expected behaviour**: `MALFORMED_FUNCTION` should be caught and trigger a retry (if there's still retry budget) - it often behaves correctly on another attempt.",
      "state": "open",
      "author": "intellectronica",
      "author_type": "User",
      "created_at": "2025-01-07T14:52:58Z",
      "updated_at": "2025-06-21T22:53:42Z",
      "closed_at": null,
      "labels": [
        "bug",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 27,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/631/reactions",
        "total_count": 4,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 3
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/631",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/631",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.218925",
      "comments": []
    },
    {
      "issue_number": 2050,
      "title": "Messages passed to history_processor functions include current run messages",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n## Issue\nFollowing the example in the doc [summarize-old-messages](https://ai.pydantic.dev/message-history/#summarize-old-messages), I run into an issue when using also tools with the Agent and OpenAI as provider.\n\nThe passed list of `messages` to the **history_processor** functions are including also generated messages (both **ModelRequest** and **ModelResponse**) during the run. If we follow the example which keeps only the latest message, the tool related Response is included but not the toll related Request which caused the OpenAI API to return HTTP error 400: \n\n> ModelHTTPError: status_code: 400, model_name: gpt-4o-mini, body: {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}\n\n## Discussion\nIn general, we do not want to summarize messages produced during the run, at least, I was not expecting this behavior and I do not see reasons why we should do it. What I was expecting with this feature is to process only messages passed in the `message_history` of the _run_ Agent method.\n\n### Doc example should be reconsidered\nIn addition, the example provided in the documentation is logically incorrect to me. It is making a summary of the first 10 messages in the list (which are the oldest), then it adds the most recent message. What about the messages between the index 10 and the last message ? \n\nI think the example considered a very simple case where a history is passed and the first user prompt is evaluated by the agent (the user prompt is `message[-1:]` and the entire history is summarized, considering exactly 10 messaged in the history). \n\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.messages import ModelMessage\n\nfrom pydantic_ai.models import ModelRequest, ModelResponse\nfrom pydantic_ai.messages import UserPromptPart, TextPart\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MyContext:\n    run_count: int = 0\n\n\nhistory = []\n# create 10 qa messages replicating the last one\nfor qa in range(20):\n    history.append(ModelRequest(parts=[UserPromptPart(content=f\"Question {qa + 1}\")]))\n    history.append(ModelResponse(parts=[TextPart(content=f\"Answer {qa + 1}\")]))\n\n# Use a cheaper model to summarize old messages.\nsummarize_agent = Agent(\n    \"openai:gpt-4o-mini\",\n    instructions=\"\"\"\nSummarize this conversation, omitting small talk and unrelated topics.\nFocus on the technical discussion and next steps.\n\"\"\",\n)\n\n\nasync def summarize_old_messages(\n    ctx: RunContext[MyContext], messages: list[ModelMessage]\n) -> list[ModelMessage]:\n    # Summarize the oldest 10 messages\n    if len(messages) > 10:\n        ctx.deps.run_count += 1\n        oldest_messages = messages[:10]\n        summary = await summarize_agent.run(message_history=oldest_messages)\n        # Return the last message and the summary\n\n        ############# DEBUG #############\n        print(f\"Run count: {ctx.deps.run_count}\")\n        for msg in messages[-5:]:\n            print(msg)\n        #################################\n\n        return summary.new_messages() + messages[-1:]\n\n    return messages\n\n\nagent = Agent(\"openai:gpt-4o-mini\", history_processors=[summarize_old_messages])\n\n\n# add a simple tool\n@agent.tool\nasync def get_answer(ctx: RunContext[MyContext], question_number: int) -> str:\n    return \"There is no answer!\"\n\n\nagent.run_sync(\n    \"What is the answer to question 5?\", message_history=history, deps=MyContext()\n)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13\npydantic-ai-slim[openai]>=0.3.2\n```",
      "state": "open",
      "author": "MrAngius",
      "author_type": "User",
      "created_at": "2025-06-21T15:57:47Z",
      "updated_at": "2025-06-21T15:57:47Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2050/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2050",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2050",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.218971",
      "comments": []
    },
    {
      "issue_number": 2048,
      "title": "The reasoning content from deepseek-reasoner is not included in streamed runs but included in non-streamed runs",
      "body": "### Initial Checks\n\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI'm trying to get the reasoning content from `deepseek-reasoner` in streamed runs.\n\nWhile using `agent.run_sync()`, the ThinkingPart from `deepseek-reasoner` is included correctly,\nbut for `agent.run_stream()`, the ThinkingPart is not included.\n\nThe output after running the example code:\n```plaintext\n'----- Non-streamed -----'\n[\n│   ModelRequest(parts=[UserPromptPart(content='Hello', timestamp=datetime.datetime(2025, 6, 21, 14, 27, 10, 219792, tzinfo=datetime.timezone.utc))]),\n│   ModelResponse(parts=[ThinkingPart(content='Okay, the user just said \"Hello\". Hmm, such a simple greeting but it\\'s the start of everything. \\n\\nFirst thought: This could be anyone - a first-time user testing the waters, or someone returning with more questions. The tone feels neutral, maybe slightly cautious? No context given yet, so I\\'ll assume they want a warm but open-ended response. \\n\\nI notice they didn\\'t use emojis or exclamation marks - probably prefers straightforward communication. Should match that energy but add a touch of enthusiasm. The \"How can I help?\" leaves room for them to steer the conversation. \\n\\n...Wait, is this too generic? No, better keep it simple for now. If they\\'re just being polite, they\\'ll disappear. If they need something, they\\'ll ask. The smiley feels necessary though - text needs warmth. \\n\\nAlso mentally noting: If their next message is still vague, might gently probe about their interests. But for now - minimalist reply it is.'), TextPart(content=\"Hello! 😊 How can I help you today? Whether you have a question, need assistance, or just want to chat—I'm here for you!\")], usage=Usage(requests=1, request_tokens=6, response_tokens=234, total_tokens=240, details={'reasoning_tokens': 200, 'cached_tokens': 0}), model_name='deepseek-reasoner', timestamp=datetime.datetime(2025, 6, 21, 14, 27, 10, tzinfo=TzInfo(UTC)), vendor_id='f0188eea-e4a1-43b4-aaf1-440f4f6b9fbd')\n]\n'----- Streamed -----'\n[\n│   ModelRequest(parts=[UserPromptPart(content='Hello', timestamp=datetime.datetime(2025, 6, 21, 14, 27, 23, 324377, tzinfo=datetime.timezone.utc))]),\n│   ModelResponse(parts=[TextPart(content=\"Hello! 😊 How can I assist you today? Whether you have a question, need help with something, or just want to chat—I'm here for you!\")], usage=Usage(requests=1, request_tokens=6, response_tokens=163, total_tokens=169, details={'reasoning_tokens': 127, 'cached_tokens': 0}), model_name='deepseek-reasoner', timestamp=datetime.datetime(2025, 6, 21, 14, 27, 23, tzinfo=TzInfo(UTC)))\n]\n```\n\n### Example Code\n\n```Python\nimport asyncio\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom rich.pretty import pprint\n\nDEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\nDEEPSEEK_BASE_URL = \"https://api.deepseek.com\"\n\nprovider = OpenAIProvider(base_url=DEEPSEEK_BASE_URL, api_key=DEEPSEEK_API_KEY)\nmodel = OpenAIModel(\"deepseek-reasoner\", provider=provider)\nagent = Agent(model)\n\npprint(\"----- Non-streamed -----\")\npprint(agent.run_sync(\"Hello\").all_messages())\n\npprint(\"----- Streamed -----\")\nasync def streamed():\n    async with agent.run_stream(\"Hello\") as result:\n        await result.get_output()\n    pprint(result.all_messages())\nasyncio.run(streamed())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.13\nPydanticAI: 0.3.2\n```",
      "state": "open",
      "author": "userwljs",
      "author_type": "User",
      "created_at": "2025-06-21T14:43:36Z",
      "updated_at": "2025-06-21T14:46:34Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2048/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2048",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2048",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.218984",
      "comments": []
    },
    {
      "issue_number": 774,
      "title": "(openai.APITimeoutError) OpenAI requests time out, but Gemini/Anthropic requests work fine",
      "body": "## OpenAI API Timeout Error in pydantic-ai Agent\n\n### Problem Description\nWhen using the OpenAI model with the pydantic-ai Agent, a persistent `APITimeoutError` is encountered during API requests, despite having a valid and current API key.\n\n### Environment\n- Library: pydantic-ai\n- Model Attempted: OpenAI (gpt-4o)\n- API Key Status: Valid and current\n\n### Code\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nGEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n\nagent = Agent(\n    # model = OpenAIModel('gpt-4o', api_key=OPENAI_API_KEY)\n    # model = GeminiModel('gemini-1.5-flash', api_key=GEMINI_API_KEY)\n    model = AnthropicModel('claude-3-5-sonnet-latest', api_key=ANTHROPIC_API_KEY)\n)\n\nresult = agent.run_sync(\"What color is an apple?\")\nprint(result.data)\n```\n### Error Traceback\nopenai.APITimeoutError: Request timed out.\n\n### Observations\n- This error occurs specifically with the OpenAI model\n- Requests using Gemini and Anthropic models work without issues\n\n### Additional Context\nI've verified that a direct OpenAI API call using the `openai` library works correctly, suggesting the issue may be specific to the pydantic-ai library's implementation of the OpenAI model. The following code works fine:\n```\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass MyModel(BaseModel):\n    place_of_interest: str = Field(description = \"The most popular museum in the city\")\n    nearest_water_body: str = Field(description = \"The nearest lake, or a large natural freshwater source\")\n    country: str = Field(description = \"The country to which the city belongs\")\n    situated: Literal[\"Northern Hemisphere\", \"Southern Hemisphere\"]\n    famous_person_from_city: str = Field(description = \"The most popular political figure from the place, ensuring they were born there\")\n    why_famous: str = Field(description = \"Why the most popular political figure\")\n\nload_dotenv()\nclient = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY1\"))\n\ndef get_response(query: str):\n    response = client.beta.chat.completions.parse(\n      # query=query,\n      model=\"gpt-4o-mini\",\n      messages=[{\"role\": \"system\",\"content\": \"You know all about world geography, and especially about cities! You respond in the tone of a pirate!\"},\n                {\"role\": \"user\",\"content\": query}],\n        temperature=0.2,\n        response_format=MyModel,\n        top_p=1\n    )\n    return response\n```\n### Reproducibility\nConsistently reproducible locally",
      "state": "open",
      "author": "A512A",
      "author_type": "User",
      "created_at": "2025-01-25T10:57:55Z",
      "updated_at": "2025-06-21T14:00:31Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 18,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/774/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/774",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/774",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.218995",
      "comments": [
        {
          "author": "A512A",
          "body": "```\nPS C:\\Users\\USER\\pydanticai> & C:/Users/USER/pydanticai/.venv/Scripts/python.exe c:/Users/USER/pydanticai/test1.py\nTraceback (most recent call last):\n  File \"C:\\Users\\USER\\pydanticai\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"C:\\",
          "created_at": "2025-01-25T10:58:48Z"
        },
        {
          "author": "samuelcolvin",
          "body": "I haven't seen this, and afaik, no one else has reported it, so I suspect it's a temporary or intermittent issue with OpenAI. \n\nHow long are requests taking directly to OpenAI? What timeout are you using in PydanticAI? ",
          "created_at": "2025-01-25T16:46:57Z"
        },
        {
          "author": "A512A",
          "body": "Thank you for your response. I was surprised not to come across any reports of this issue, and I’ve even tried reinstalling PydanticAI in multiple virtual environments to rule out environment-specific problems, but the peculiar issue remains. Requests sent directly to OpenAI are completing in just a",
          "created_at": "2025-01-25T17:18:21Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-02-02T14:01:36Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-02-05T14:07:08Z"
        }
      ]
    },
    {
      "issue_number": 1646,
      "title": "Dynamic System prompts not being included on fresh runs with message_history",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAs mentioned in the title, the documentation implies that dynamic prompts are always evaluated even if message_history is present. So far, logging all_messages() and putting a bunch of print statemetns all over the place seems to imply that they don't get included\n\nThe LLM model responses also suggest that the content from dynamic system prompts is not being included\n(I use Gemini models)\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai 0.1.9\nGemini 2.0\n```",
      "state": "open",
      "author": "strunov",
      "author_type": "User",
      "created_at": "2025-05-05T16:07:37Z",
      "updated_at": "2025-06-21T07:18:20Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1646/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1646",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1646",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.499450",
      "comments": [
        {
          "author": "Kludex",
          "body": "Did you try `instructions`?",
          "created_at": "2025-05-06T09:07:12Z"
        },
        {
          "author": "strunov",
          "body": "Yes, instructions work, though my understanding was that system prompts were the main mechanism to provide initial LLM role steering and dynamic context, particularly if this is the kind of context you cannot emit to the client side app as part of the message history",
          "created_at": "2025-05-07T11:02:05Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-14T14:00:36Z"
        },
        {
          "author": "amiyapatanaik",
          "body": "@strunov can you update the example code? When you say instructions worked, do you mean that when you provided instructions instead of system_prompt in the agent initialization, it worked?\n\nI'm having same issue and it seems even if you use instructions (or not provide anything), if a message_histor",
          "created_at": "2025-05-16T17:38:37Z"
        },
        {
          "author": "amiyapatanaik",
          "body": "Here is a minimal code to reproduce the error. Seems the dynamic parameter in the system_prompt decorator is not working at all the moment you include a message_history. \n```python\nfrom datetime import date\nfrom pydantic_ai.usage import Usage\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_a",
          "created_at": "2025-05-16T18:00:10Z"
        }
      ]
    },
    {
      "issue_number": 2046,
      "title": "Can't reproduce Annotated mermaid figure",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAs per title, I am unable to reproduce the first figure in the section [Mermaid Diagrams](https://ai.pydantic.dev/graph/#mermaid-diagrams) of the docs.\n\nI checked the docs and saw that the image is hard coded since the example is incomplete. The error I get is that `Edge` is not hashable.\n\nI am currently using the following naive subclass to fix this issue locally, but maybe you could just make Edge itself hashable? I really don't know if this would have unintended consequences tho..\n\n```python\nclass MyEdge(Edge):\n    def __init__(self, label: str | None):\n        super().__init__(label)\n\n    def __hash__(self) -> int:\n        return hash(self.label)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.10.11\nPydantic AI 0.3.1\n```",
      "state": "open",
      "author": "meutudo-jose-sartori",
      "author_type": "User",
      "created_at": "2025-06-21T02:03:21Z",
      "updated_at": "2025-06-21T02:29:08Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2046/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2046",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2046",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.746733",
      "comments": []
    },
    {
      "issue_number": 2040,
      "title": "Anthropic extended thinking: invalid request error when using tools",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using anthropic models with thinking enabled and tools available then an invalid request error is thrown after the first tool call. The traceback is shown below. The error can be reproduced by running the example code.\n\n#### Traceback\nException has occurred: ModelHTTPError\nstatus_code: 400, model_name: claude-opus-4-20250514, body: {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.1.content.0.type: Expected `thinking` or `redacted_thinking`, but found `text`. When `thinking` is enabled, a final `assistant` message must start with a thinking block (preceeding the lastmost set of `tool_use` and `tool_result` blocks). We recommend you include thinking blocks from previous turns. To avoid this requirement, disable `thinking`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking'}}\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.anthropic.com/v1/messages?beta=true'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nanthropic.BadRequestError: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.1.content.0.type: Expected `thinking` or `redacted_thinking`, but found `text`. When `thinking` is enabled, a final `assistant` message must start with a thinking block (preceeding the lastmost set of `tool_use` and `tool_result` blocks). We recommend you include thinking blocks from previous turns. To avoid this requirement, disable `thinking`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking'}}\n\nThe above exception was the direct cause of the following exception:\n\n  File \"/home/jonathan/titan/backend/mojo/agent/assistant.py\", line 519, in main\n    async with node.stream(run.ctx) as stream:\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jonathan/titan/backend/mojo/agent/assistant.py\", line 537, in <module>\n    asyncio.run(main(\"Hvordan har PFA's afkast været?\"))\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: claude-opus-4-20250514, body: {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.1.content.0.type: Expected `thinking` or `redacted_thinking`, but found `text`. When `thinking` is enabled, a final `assistant` message must start with a thinking block (preceeding the lastmost set of `tool_use` and `tool_result` blocks). We recommend you include thinking blocks from previous turns. To avoid this requirement, disable `thinking`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking'}}\n\n### Example Code\n\n```Python\nfrom pydantic_ai.models.anthropic import AnthropicModel, AnthropicModelSettings\nfrom pydantic_ai import Agent\nimport asyncio\n\nopus_model = AnthropicModel(\"claude-opus-4-20250514\")\nopus_settings = AnthropicModelSettings(\n    anthropic_thinking={\"type\": \"enabled\", \"budget_tokens\": 3000}\n    # parallel_tool_calls=True,\n)\n\n# Create the assistant agent\nassistant = Agent(\n    model=opus_model,\n    system_prompt=\"You are a helpful assistant\",\n    model_settings=opus_settings,\n)\n\n\n@assistant.tool_plain(docstring_format=\"numpy\")\ndef tool(name: str) -> str:\n    \"\"\"\n    Returns a string indicating the tool name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the tool.\n\n    Returns\n    -------\n    str\n        A string describing the tool with the given name.\n    \"\"\"\n    return f\"This is a tool {name}\"\n\n\nasync def main(msg: str):\n    async with assistant.iter(msg) as run:\n        async for event in run:\n            print(event)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main(\"Come up with a funny name for a tool and call the tool\"))\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nLatest pydantic AI version, where thinking has just been added.\n```",
      "state": "open",
      "author": "josca42",
      "author_type": "User",
      "created_at": "2025-06-20T11:02:19Z",
      "updated_at": "2025-06-21T00:14:48Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2040/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2040",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2040",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.746769",
      "comments": [
        {
          "author": "pedroallenrevez",
          "body": "Have exactly the same problem.\n\nFrom their docs:\nhttps://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#example-passing-thinking-blocks-with-tool-results\n\n> Preserving thinking blocks: During tool use, you must pass thinking blocks back to the API for the last assistant message. Incl",
          "created_at": "2025-06-20T12:08:19Z"
        },
        {
          "author": "nathan-gage",
          "body": "can confirm. using the iter() + tools and repro'd",
          "created_at": "2025-06-21T00:14:48Z"
        }
      ]
    },
    {
      "issue_number": 1496,
      "title": "Adding LiteLLM as model wrap just like how google-adk does it.",
      "body": "### Description\n\nLiteLLM seems to have all big providers and even things like Ollama and LMstudio. If it is possible it would probably be best for everybody if it is possible to use LiteLLM as a providor for the tool calls. This way Pydantic-ai can stop focussing on adding new AI's to the list and just focus on being an agentic framework.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-04-16T06:21:02Z",
      "updated_at": "2025-06-20T22:55:14Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1496/reactions",
        "total_count": 24,
        "+1": 24,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1496",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1496",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:44.987996",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "I think we can add LiteLLM as a model, but it's worth noting that we already support all big provider, Ollama and LMStudio, see https://ai.pydantic.dev/models/openai/#openai-compatible-models.",
          "created_at": "2025-05-27T06:07:25Z"
        },
        {
          "author": "DouweM",
          "body": "LiteLLM has an OpenAI-compatible API, so it should be easy enough to add a `LiteLLMProvider` that can be used with `OpenAIModel`. That'd be similar to OpenRouter, Together, Fireworks etc.\n\nTogether with https://github.com/pydantic/pydantic-ai/pull/1835, we can also have automatically pick the right ",
          "created_at": "2025-05-27T14:23:42Z"
        },
        {
          "author": "NorthIsUp",
          "body": "+1 to litellm support.\n\nLitellm's value is beyond its model abstraction layer. It also offers budget aware routing between providers (e.g. spreading budget across vertex/azure/bedrock). It offers virtual api keys so you can track usage across users/services. And a ton of other useful features that m",
          "created_at": "2025-06-13T22:57:12Z"
        },
        {
          "author": "tajkirkpatrick",
          "body": "Support for LiteLLM would be really appreciated!",
          "created_at": "2025-06-20T22:55:14Z"
        }
      ]
    },
    {
      "issue_number": 1071,
      "title": "Issue with LiteLLM and Agent Tool",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI'm creating an agent using an OpenAIModel with Litellm & Bedrock. \n\nWIthout tools, it works fine.  \n\n\nIf I include any tools, I get the following error.  \n\n\"\"\"\n\n  raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: claude-3-7-sonnet, body: {'message': 'litellm.BadRequestError: BedrockException - {\"message\":\"1 validation error detected: Value \\'\\' at \\'toolConfig.tools.1.member.toolSpec.description\\' failed to satisfy constraint: Member must have length greater than or equal to 1\"}. Received Model Group=claude-3-7-sonnet\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}\n\n\n\"\"\"\n\n### Example Code\n\n```Python\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.openai import OpenAIModel\n\n@dataclass\nclass Dependencies:\n    x: int\n\n\nmodel = OpenAIModel(\n    \"claude-3-7-sonnet\",\n    base_url=\"xxx\",\n    api_key=\"xxx\",\n)\n\nagent = Agent(\n    model,\n    deps_type=Dependencies,\n)\n\n\n@agent.tool\ndef get_current_date(ctx: RunContext[Dependencies]) -> datetime:\n    return datetime.now()\n\n\ndef run_ops_tool_agent() -> str:\n\n    deps = Dependencies(x=1)\n    agent.run_sync(\"test\", deps=deps)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython = 3.12.8\nPydantic AI version = 0.0.35\n```",
      "state": "open",
      "author": "mikeprince4",
      "author_type": "User",
      "created_at": "2025-03-06T17:34:14Z",
      "updated_at": "2025-06-20T22:33:19Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1071/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1071",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1071",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:45.215474",
      "comments": [
        {
          "author": "tajkirkpatrick",
          "body": "Having the same issue, with LiteLLM the Agents run fine without tools, but with tools I hit validation errors due to what seems like additional parameters getting injected into the tool that were not originally defined. Although using **kwargs as parameter to the tool definition, leaves me in an inf",
          "created_at": "2025-06-20T22:33:19Z"
        }
      ]
    },
    {
      "issue_number": 2045,
      "title": "How to see a full LLM request body?",
      "body": "### Question\n\nI need to investigate what request body Pydantic send to LLM provider. \n\nThe most critical part is tool description including typed response and tool-as-response. \n\nI need it because my agent unexpectedly call wrong tool. I can't understand why and maybe the reason is in tool description sended to LLM provider.\n\n\nThanks\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "alexey2baranov",
      "author_type": "User",
      "created_at": "2025-06-20T21:01:13Z",
      "updated_at": "2025-06-20T21:01:33Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2045/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2045",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2045",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:45.448226",
      "comments": []
    },
    {
      "issue_number": 2034,
      "title": "How to store 'artifacts' with tool responses?",
      "body": "### Question\n\nI understand tool call results are sent as [`ToolReturnPart`](https://ai.pydantic.dev/api/messages/#pydantic_ai.messages.ToolReturnPart) objects.\n\nIn the docs, under [Function Tool Output](https://ai.pydantic.dev/tools/#function-tool-output):\n> Some models (e.g. Gemini) natively support semi-structured return values, while some expect text (OpenAI) but seem to be just as good at extracting meaning from the data. If a Python object is returned and the model expects a string, the value will be serialized to JSON.\n\nHowever I want to have some `metadata` associated with the tool response that I can reference. An example use-case is sending back `str` formatted tool response to the model but retaining structured output in the messages.\n\nLangchain handles this like [so](https://python.langchain.com/docs/how_to/tool_artifacts/#defining-the-tool).\n\nIs there a way to achieve this currently?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "thisisarko",
      "author_type": "User",
      "created_at": "2025-06-19T21:36:38Z",
      "updated_at": "2025-06-20T18:39:23Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2034/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2034",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2034",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:45.448252",
      "comments": [
        {
          "author": "DouweM",
          "body": "@thisisarko So if I understand correctly, the idea of artifacts in LangChain is that they are not sent to the model but can be accessed from downstream chain steps or tools. In the case of Pydantic AI, how/when would you like to be able to access the artifacts? A concrete use case and example would ",
          "created_at": "2025-06-20T16:44:02Z"
        },
        {
          "author": "thisisarko",
          "body": "@DouweM The use-case is the following:\n\nAlthough some models like Gemini can parse structured output from tool responses, there are other models which are still limited to text-only output from tools for optimal performance. It would be good to have an option to send a `str` form of the output to th",
          "created_at": "2025-06-20T17:59:13Z"
        },
        {
          "author": "thisisarko",
          "body": "I am happy to author this feature if required.",
          "created_at": "2025-06-20T18:39:23Z"
        }
      ]
    },
    {
      "issue_number": 2035,
      "title": "Patter for using a provided httpx client instead of the default cached one",
      "body": "### Question\n\nI looked at the codebase and saw this in the documentation\n\n```python\n\"\"\"\n    There are good reasons why in production you should use a `httpx.AsyncClient` as an async context manager as\n    described in [encode/httpx#2026](https://github.com/encode/httpx/pull/2026), but when experimenting or showing\n    examples, it's very useful not to.\n\"\"\"\n```\n\nBut it's not very clear how you would really use it as a context manager when the API expects you to provide the client to the Provider during init\n\nI typically initialize my agents in the top level of the module like this\n\n```python\nmy_agent = Agent(\n    OpenAIModel(\n        'model-router',\n        provider=AzureProvider(\n            azure_endpoint='https://myendpoint.com',\n            api_version='2025-01-01-preview',\n            api_key=\"my_api_key\"\n        )\n    ),\n)\n```\n\nBut you can't really use a context manager here though the only way to pass a httpx client to an agent created this way is if you create a module level httpx client as well and pass it\n```python\nclient = httpx.AsyncClient()\n\nmy_agent = Agent(\n    OpenAIModel(\n        'model-router',\n        provider=AzureProvider(\n            azure_endpoint='https://myendpoint.com',\n            api_version='2025-01-01-preview',\n            api_key=\"my_api_key\",\n            http_client=client,\n        )\n    ),\n)\n```\nIs this fine? What are the right patterns to use here? Should we able to provide a client during the agent.run for example?\n\n### Additional Context\n\npydantic_ai 0.3.1\nPython 3.12",
      "state": "closed",
      "author": "vikigenius",
      "author_type": "User",
      "created_at": "2025-06-20T04:22:39Z",
      "updated_at": "2025-06-20T17:24:37Z",
      "closed_at": "2025-06-20T17:24:37Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2035/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2035",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2035",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:45.666876",
      "comments": [
        {
          "author": "DouweM",
          "body": "@vikigenius You can't specifically override the model's provider's `http_client` at agent-run time, and I don't think we'd be likely to add it as not all models/providers support it, but you _can_ override the entire model using `agent.override` or as an argument to `agent.run/run_sync/iter`, which ",
          "created_at": "2025-06-20T16:54:54Z"
        },
        {
          "author": "vikigenius",
          "body": "Yep that works.",
          "created_at": "2025-06-20T17:24:37Z"
        }
      ]
    },
    {
      "issue_number": 2037,
      "title": "Tool Injection for mcp-run-python: Enable Python code to call back to agent tools with request/response flow",
      "body": "### Description\n\n### **Feature Request: Tool Injection for mcp-run-python**\n\nCurrently, `mcp-run-python` executes Python code in complete isolation. This feature request proposes enabling **tool injection** through MCP notifications, allowing sandboxed Python code to call back to the parent agent's tools while maintaining complete security isolation.\n\n### **Demo of how code might look when using the feature**\n\n```python\n# Parent agent setup\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nagent = Agent('claude-3-5-haiku-latest', \n              mcp_servers=[mcp_run_python_server],\n              tools=[web_search_tool, database_query_tool, send_email_tool])\n\n# User query\nresult = await agent.run(\"\"\"\nFind recent AI breakthroughs relevant to our enterprise customers \nand send them personalized email updates.\n\"\"\")\n\n# LLM generates Python code with tool calls (parameters decided by LLM):\n\"\"\"\n# This runs in isolated Pyodide sandbox but can call back to agent tools\nrecent_news = call_tool(\"web_search\", \n                       query=\"AI breakthroughs enterprise 2025\",\n                       max_results=20,\n                       time_filter=\"1month\")\n\n# Python execution blocks here until web_search completes and returns results\n\n# LLM determines appropriate SQL query based on user intent\ncustomers = call_tool(\"database_query\",\n                     sql=\"SELECT email, company, industry FROM customers WHERE tier='enterprise'\")\n\n# Python logic processes results\nrelevant_articles = []\nfor article in recent_news:\n    if any(keyword in article['title'].lower() \n           for keyword in ['enterprise', 'business', 'scalability']):\n        relevant_articles.append(article)\n\n# Generate personalized emails with error handling\nfor customer in customers:\n    try:\n        industry_articles = [a for a in relevant_articles \n                            if customer['industry'].lower() in a['content'].lower()]\n        \n        if industry_articles:\n            email_body = f\"Hi {customer['company']}, here are {len(industry_articles)} AI developments...\"\n            \n            call_tool(\"send_email\",\n                     to=customer['email'],\n                     subject=f\"AI Updates for {customer['industry']}\",\n                     body=email_body)\n    except ToolCallError as e:\n        print(f\"Failed to send email to {customer['email']}: {e}\")\n\nprint(f\"Processing complete\")\n\"\"\"\n```\n\n### **Technical Implementation**\n\n#### **1. Complete Request/Response Flow**\n\n```mermaid\nsequenceDiagram\n    participant Python as Python (Pyodide)\n    participant MCP as MCP Server (deno)\n    participant Agent as Agent Client\n\n    Python->>MCP: call_tool(\"web_search\", query=\"...\")\n    Note over MCP: Generate request_id, store pending call\n    MCP->>Agent: tool_call_request(id=\"req_123\", tool=\"web_search\", args={...})\n    Note over Agent: Execute web_search tool\n    Agent->>MCP: tool_call_response(id=\"req_123\", result=[...])\n    Note over MCP: Resume Python execution with result\n    MCP->>Python: return result\n    Note over Python: Continue execution with tool result\n```\n\n#### **2. Enhanced MCP Tool Schema**\n```json\n{\n  \"name\": \"run_python_code\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"python_code\": {\"type\": \"string\"},\n      \"available_tools\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"description\": {\"type\": \"string\"}, \n            \"parameters\": {\"type\": \"object\"}\n          }\n        },\n        \"default\": []\n      }\n    }\n  }\n}\n```\n\n#### **3. MCP Notification Messages**\n```typescript\n// Tool call request (MCP Server → Agent)\ninterface ToolCallRequest {\n  jsonrpc: \"2.0\";\n  method: \"notifications/tool_call_request\";\n  params: {\n    requestId: string;\n    toolName: string;\n    arguments: Record<string, any>;\n  };\n}\n\n// Tool call response (Agent → MCP Server)\ninterface ToolCallResponse {\n  jsonrpc: \"2.0\";\n  method: \"notifications/tool_call_response\"; \n  params: {\n    requestId: string;\n    result?: any;\n    error?: string;\n  };\n}\n```\n\n#### **4. Implementation in MCP Server (deno)**\n```typescript\n// Handle pending tool calls with request/response pairing\nconst pendingToolCalls = new Map<string, {resolve: Function, reject: Function}>();\n\nasync function callTool(toolName: string, args: any): Promise<any> {\n  const requestId = generateId();\n  \n  // Send request to agent\n  const request = {\n    jsonrpc: \"2.0\",\n    method: \"notifications/tool_call_request\",\n    params: { requestId, toolName, arguments: args }\n  };\n  \n  // Create promise that resolves when response arrives\n  const responsePromise = new Promise((resolve, reject) => {\n    pendingToolCalls.set(requestId, { resolve, reject });\n    \n    // 30 second timeout\n    setTimeout(() => {\n      pendingToolCalls.delete(requestId);\n      reject(new Error(`Tool call timeout: ${toolName}`));\n    }, 30000);\n  });\n  \n  await sendMessage(request);\n  return responsePromise; // Python execution blocks here\n}\n\n// Handle responses from agent\nfunction handleToolResponse(response: ToolCallResponse) {\n  const pending = pendingToolCalls.get(response.params.requestId);\n  if (pending) {\n    pendingToolCalls.delete(response.params.requestId);\n    if (response.params.error) {\n      pending.reject(new Error(response.params.error));\n    } else {\n      pending.resolve(response.params.result);\n    }\n  }\n}\n```\n\n#### **5. Injected Python Functions**\n```python\n# Available in Pyodide globals during execution\ndef call_tool(tool_name: str, **kwargs):\n    \"\"\"Call an available agent tool and wait for response\"\"\"\n    try:\n        # This calls the deno layer which handles MCP communication\n        result = _internal_call_tool(tool_name, kwargs)\n        return result\n    except Exception as e:\n        raise ToolCallError(f\"Tool call '{tool_name}' failed: {str(e)}\")\n\ndef call_mcp_tool(server_name: str, tool_name: str, **kwargs):\n    \"\"\"Call a tool from an MCP server\"\"\"\n    return _internal_call_mcp_tool(server_name, tool_name, kwargs)\n\nclass ToolCallError(Exception):\n    \"\"\"Raised when a tool call fails\"\"\"\n    pass\n```\n\n### References\n\n- HuggingFace's `smolagents` ([link](https://github.com/huggingface/smolagents/blob/1da0527d0b0661dff31e34300d37207366ef7e32/src/smolagents/agents.py#L1475)) implements this pattern with their CodeAgent, where tools are exposed as Python functions during code execution.\n\n\nNote:\n- Samuel did mention this during his PyCon talk, but I couldn't locate an existing issue or PR for this feature. If this has already been discussed elsewhere, please feel free to link to the relevant discussion.",
      "state": "open",
      "author": "yamanahlawat",
      "author_type": "User",
      "created_at": "2025-06-20T04:47:59Z",
      "updated_at": "2025-06-20T17:15:41Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2037/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2037",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2037",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:45.879706",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Existing work is in #1550. I \"just\" need to finish it.\n\nI agree that my \"trick\" of hijacking sampling is probably not ideal, but I'd suggest we should use [Elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation) (recently added to the Python SDK in https://github.com/mod",
          "created_at": "2025-06-20T10:49:18Z"
        },
        {
          "author": "yamanahlawat",
          "body": "Thanks @samuelcolvin! You're right that elicitation is way better approach than custom notifications or sampling hijack. It seems elicitation is purpose-built for exactly this use case.\n\nHappy to help with a PR.",
          "created_at": "2025-06-20T17:15:41Z"
        }
      ]
    },
    {
      "issue_number": 2033,
      "title": "Missing Support for Llama 3.3/4 Pythonic Tool Calling Format",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nPydantic-ai currently does NOT support the pythonic tool calling format used by Meta's Llama 3.3 and Llama 4 models (observed when using them on AWS Bedrock) - making it impossible to use these models with tool calling.\n\nExpected Behavior\nWhen using Llama 3.3/4 models (via BedrockConverseModel), tool calls shall be detected and parsed correctly.\n\nActual Behavior\nTool calls in pythonic format are treated as regular text content and are not executed as tools - causing the agent to fail.\n\nTool Call Format\nLlama 3.3/4 models output tool calls in this format:\n\nExample\nInput Query: \"Search for information about architecture\"\n\nModel Output:\n    \n    <|python_start|>{\"name\": \"confluence_search_tool\", \"parameters\": {\"query\": \"architecture documentation\"}}. <|python_end|>\n    \n\nReproduction\n\n- Use BedrockConverseModel with a Llama 3.3 or Llama 4 model ID (e.g., meta.llama3-3-70b-instruct-v1:0)\n- Define tools on the agent\n- Send a query that should trigger tool usage\n- Observe that tool calls in pythonic format are not recognized\n- \n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai-slim[bedrock]>=0.2.18\n```",
      "state": "open",
      "author": "okigan-crunchyroll",
      "author_type": "User",
      "created_at": "2025-06-19T20:19:06Z",
      "updated_at": "2025-06-20T17:02:49Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2033/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2033",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2033",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:46.145817",
      "comments": [
        {
          "author": "DouweM",
          "body": "@okigan-crunchyroll Thanks for the feature request.\n\nThis would be similar to what we've done in https://github.com/pydantic/pydantic-ai/pull/1142 to parse `<think>...</think>` blocks in text messages as `ThinkingPart`s. Here we'd parse out `ToolCallPart`s.\n\nCan you confirm that when we send the too",
          "created_at": "2025-06-20T16:32:07Z"
        },
        {
          "author": "okigan",
          "body": "The llama4 models seems to flip between JSON based tool calling and pythonic based tool calling (I don't see a parameter to control it and prompting also does not seem not seem to control it).\n\n\nHere is my current [workaround](https://gist.github.com/okigan/0013f07be74262297acb22da51916564): overrid",
          "created_at": "2025-06-20T17:02:49Z"
        }
      ]
    },
    {
      "issue_number": 2001,
      "title": "Auto-fix incomplete JSON due to max_tokens through a prefilled followup LLM request",
      "body": "### Description\n\n`EOF while parsing an object ...` When tool response exceeds maximum tokens\n\nMy agent sometimes writes some very large files, which causes failure, even if I set max_tokens to the maximum.\n\nThe way I've come up with is to continue the output via [completions api](https://platform.openai.com/docs/api-reference/completions/create) and parse it manually, and I'm hoping that pydantic-ai will have a way for us to deal with these errors , like some hooks.\n\nThanks!\n\n### References\n\n_No response_",
      "state": "open",
      "author": "Wh1isper",
      "author_type": "User",
      "created_at": "2025-06-17T02:45:26Z",
      "updated_at": "2025-06-20T16:58:00Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2001/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2001",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2001",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:46.340086",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Wh1isper Can you please share the stack trace of the error? I'm also curious what your workaround with the completions API looks like, and what kinds of hooks you're thinking of.",
          "created_at": "2025-06-17T14:58:02Z"
        },
        {
          "author": "Wh1isper",
          "body": "Sorry for the delay, you can reproduce the problem with the following code.\n\n```python\nimport asyncio\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai.agent import Agent\nfrom pydantic_ai.result import ToolOutput\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.open",
          "created_at": "2025-06-20T07:17:37Z"
        },
        {
          "author": "Wh1isper",
          "body": "We can complement the parameters in the following way:\n\n```python\nfrom openai import OpenAI\nfrom json_repair import repair_json\n\nclient = OpenAI()\n\nto_continue = '[{\"type\": \"function\", \"function\": {\"name\": \"final_answer\", \"description\": \"A person with first and last name\", \"parameters\": {\"type\": \"ob",
          "created_at": "2025-06-20T07:43:44Z"
        },
        {
          "author": "Wh1isper",
          "body": "@DouweM Do you have any ideas? Add an `on_parse_exception` method to the tool? Or we can do this in another way?",
          "created_at": "2025-06-20T07:50:37Z"
        },
        {
          "author": "Wh1isper",
          "body": "Another option is that the model itself supports prefill: https://github.com/pydantic/pydantic-ai/issues/836\n\nBut I don't know if tool calls will be supported, as well as the fact that there doesn't seem to be much vendor support for prefill anymore.",
          "created_at": "2025-06-20T08:11:23Z"
        }
      ]
    },
    {
      "issue_number": 2036,
      "title": "Modifying AgentDeps using tools",
      "body": "### Question\n\nDo you recommend doing the following ? \n```\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic import BaseModel\nfrom rich.pretty import pprint\n\nclass AgentDeps(BaseModel):\n    test: int\n\nagent = Agent(\n    \"anthropic:claude-4-opus-20250514\",\n    deps_type=AgentDeps\n)\n\n@agent.tool\ndef test_tool(ctx: RunContext[AgentDeps]) -> str:\n    ctx.deps.test += 1\n    return f\"test_tool: {ctx.deps.test}\"\n\n@agent.output_validator\ndef test_validator(ctx: RunContext[AgentDeps], model_response: str) -> str:\n    ctx.deps.test += 1\n    return model_response\n\ndeps = AgentDeps(test=1)\nresult = agent.run_sync(\"Hey call test_tool at least once\", deps=deps)\npprint(result.all_messages())\nprint(deps.test)\n# 3\n```\nwhere I manipulate `deps` inside a validator or a tool ? where I can use this as a way to save the tools outputs in the context ? I am asking mainly for safety and if this is an expected functionality.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "essamgouda97",
      "author_type": "User",
      "created_at": "2025-06-20T04:40:13Z",
      "updated_at": "2025-06-20T15:51:50Z",
      "closed_at": "2025-06-20T14:36:23Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2036/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2036",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2036",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:46.597561",
      "comments": [
        {
          "author": "LongTCH",
          "body": "You can run agent call using iteration way to manually control each step of execution.\nhttps://ai.pydantic.dev/agents/#iterating-over-an-agents-graph\n\nAlso avoid asking something like call this tool at least one time when you provide the message_histories in agent run, because LLM will see you've ca",
          "created_at": "2025-06-20T06:35:54Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "@essamgouda97 Yes, this is an expected functionality.",
          "created_at": "2025-06-20T14:35:02Z"
        },
        {
          "author": "essamgouda97",
          "body": "> [@essamgouda97](https://github.com/essamgouda97) Yes, this is an expected functionality.\n\n@HamzaFarhan thank you appreciate the fast response !\n",
          "created_at": "2025-06-20T15:51:42Z"
        }
      ]
    },
    {
      "issue_number": 2032,
      "title": "GoogleModel generates empty text parts that Vertex AI rejects with 400 error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using `GoogleModel` with Vertex AI, requests fail with a 400 error because `pydantic_ai` generates empty text parts `{'text': ''}` that Google's API rejects.\n\n### Error Message\n\n```json\n400 Bad Request. {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unable to submit request because it must have a text parameter. Add a text parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n```\n\n### Root Cause\n\nIn `pydantic_ai/models/google.py`, the `_map_messages()` method at lines 362-364 adds empty text parts as a defensive measure:\n\n```python\n# Google GenAI requires at least one part in the message.\nif not message_parts:\n    message_parts = [{'text': ''}]\n```\n\nHowever, Google's Vertex AI API strictly validates the `text` parameter and rejects empty strings, requiring non-empty text content.\n\n### Reproduction\n\nThis occurs when:\n- Messages have no content after processing\n- Message parts arrays become empty during conversion\n- Certain edge cases in message handling\n\n### Impact\n\n- Affects all users of `GoogleModel` with Vertex AI\n- Causes complete request failures with 400 errors\n- Blocks streaming and non-streaming requests\n\n### Suggested Fix\n\nReplace the empty string with a minimal valid text:\n\n```python\n# Instead of:\nif not message_parts:\n    message_parts = [{'text': ''}]\n\n# Use:\nif not message_parts:\n    message_parts = [{'text': ' '}]  # Single space\n```\n\n**_Or add provider-specific handling for Google's stricter validation requirements._**\n\n### Temporary Workaround\n\nWe've implemented a subclass that filters empty text parts:\n\n```python\nclass FixedGoogleModel(GoogleModel):\n    async def _map_messages(self, messages):\n        system_instruction, contents = await super()._map_messages(messages)\n\n        # Filter out empty text parts and ensure valid structure\n        cleaned_contents = []\n        for content_item in contents:\n            if \"parts\" not in content_item:\n                cleaned_contents.append(content_item)\n                continue\n\n            original_parts = content_item.get(\"parts\", [])\n            cleaned_parts = [\n                part for part in original_parts\n                if not (isinstance(part, dict) and part.get(\"text\") == \"\" and len(part) == 1)\n            ]\n\n            if not cleaned_parts:\n                cleaned_parts = [{\"text\": \" \"}]\n\n            content_item[\"parts\"] = cleaned_parts\n            cleaned_contents.append(content_item)\n\n        if not cleaned_contents:\n            cleaned_contents = [{\"role\": \"user\", \"parts\": [{\"text\": \" \"}]}]\n\n        return system_instruction, cleaned_contents\n```\n\nThis workaround successfully resolves the 400 errors while maintaining API compatibility.\n\n### Example Code\n\n```Python\nimport asyncio\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nasync def reproduce_bug():\n    # Set up GoogleModel with Vertex AI\n    provider = GoogleProvider(\n        vertexai=True,\n        location=\"us-central1\"  # or your preferred location\n    )\n    model = GoogleModel(\"gemini-pro-2.5\", provider=provider)\n    \n    # Create an agent\n    agent = Agent(model=model)\n    \n    # This scenario can trigger empty message parts\n    # Case 1: Empty string message\n    try:\n        result = await agent.run(\"\")\n        print(\"Empty string succeeded:\", result)\n    except Exception as e:\n        print(\"Empty string failed:\", str(e))\n    \n    # Case 2: Message that becomes empty after processing\n    try:\n        result = await agent.run(\"   \")  # Only whitespace\n        print(\"Whitespace succeeded:\", result)\n    except Exception as e:\n        print(\"Whitespace failed:\", str(e))\n\n# Run the reproduction\nif __name__ == \"__main__\":\n    asyncio.run(reproduce_bug())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPydantic AI version: v0.3.1\nPython version: 3.12\nProvider: Google Vertex AI\nModel: gemini-pro-2.5 (via GoogleModel/Vertex)\n```",
      "state": "open",
      "author": "Infinnerty",
      "author_type": "User",
      "created_at": "2025-06-19T20:09:41Z",
      "updated_at": "2025-06-20T15:03:40Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2032/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2032",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2032",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:46.979483",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex Please have a look here as you did https://github.com/pydantic/pydantic-ai/pull/1922",
          "created_at": "2025-06-20T15:03:40Z"
        }
      ]
    },
    {
      "issue_number": 2042,
      "title": "Make output_type be conditionally available",
      "body": "### Description\n\nMake `run_sql` only available once the plan has been presented to the user.\n\n\n```python\nfrom dataclasses import dataclass\n\nfrom dotenv import load_dotenv\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.tools import Tool, ToolDefinition\n\nload_dotenv()\n\n\n@dataclass\nclass AgentDeps:\n    plan_presented: bool = False\n\n\nasync def present_plan(ctx: RunContext[AgentDeps], plan: str) -> str:\n    \"\"\"\n    Present the plan to the user.\n    \"\"\"\n    ctx.deps.plan_presented = True\n    return plan\n\n\nasync def run_sql(ctx: RunContext[AgentDeps], purpose: str, query: str) -> str:\n    \"\"\"\n    Run an SQL query.\n    \"\"\"\n    ...\n\n\nasync def only_if_plan_presented(ctx: RunContext[AgentDeps], tool_def: ToolDefinition) -> ToolDefinition | None:\n    if ctx.deps.plan_presented:\n        return tool_def\n\n\nagent = Agent(\n    model=\"google-gla:gemini-2.5-flash\",\n    deps_type=AgentDeps,\n    output_type=[present_plan, Tool(run_sql, prepare=only_if_plan_presented)],\n)\n\n\nasync def run_agent(\n    user_prompt: str, agent_deps: AgentDeps, message_history: list[ModelMessage] | None = None\n) -> str:\n    while True:\n        res = await agent.run(user_prompt=user_prompt, deps=agent_deps, message_history=message_history)\n        message_history = res.all_messages()\n        user_prompt = input(f\"{res.output} > \")\n        if user_prompt.lower() in [\"q\", \"quit\", \"exit\"]:\n            break\n    return res.output\n\n```\n\n### References\n\nBased on discussion with @DouweM here: https://pydanticlogfire.slack.com/archives/C083V7PMHHA/p1750354877664179\n\n",
      "state": "open",
      "author": "HamzaFarhan",
      "author_type": "User",
      "created_at": "2025-06-20T12:13:04Z",
      "updated_at": "2025-06-20T14:58:44Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2042/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2042",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2042",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:47.191375",
      "comments": [
        {
          "author": "DouweM",
          "body": "@HamzaFarhan Thank you, adding a `prepare` on the existing `ToolOutput` marker is a good idea. I may be able to make that work in https://github.com/pydantic/pydantic-ai/pull/2024 pretty easily.",
          "created_at": "2025-06-20T14:58:44Z"
        }
      ]
    },
    {
      "issue_number": 1904,
      "title": "Claude Sonnet 3.5 & 4.0 fail to call tools in streaming mode (v0.2.14)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n## Description\n\nClaude models (both 3.5 Sonnet and 4.0 Sonnet) do not call tools when using `run_stream()`, while the same code works correctly with `run()`. This issue affects only Anthropic Claude models - OpenAI models work correctly in both streaming and non-streaming modes.\n\n## Current Behavior\n\nWhen using streaming mode with Claude models:\n- The model acknowledges it should use the tool in its response\n- The tool is never actually called\n- Only 1 request is made (initial prompt only)\n\n## Expected Behavior\n\nTools should be called in both streaming and non-streaming modes, as they are with OpenAI models.\n\n## Minimal Reproducible Example\n\nI've created a minimal reproducible example here: https://github.com/JKapostins/pydantic-ai-claude-streaming-issue\n\nThe key code:\n```python\n@agent.tool_plain\ndef get_current_time() -> str:\n    \"\"\"Get the current time.\"\"\"\n    return f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n\n# This works - tool is called\nresult = await agent.run(\"What time is it?\")\n# Output: The current time is 5:25:30 PM on June 3rd, 2025.\n# Requests: 2 (prompt + tool call)\n\n# This DOESN'T work - tool is not called\nasync with agent.run_stream(\"What time is it?\") as result:\n    async for chunk in result.stream_text(delta=True):\n        # Output: I'll help you check the current time using the get_current_time function.\n        # Requests: 1 (prompt only, no tool call)\n```\n\n## Environment\n\n- **OS**: Tested on both:\n  - WSL 2.4.11.0\n  - Windows 11 (26100.4061)\n  - Same results on both platforms\n- **Python version**: 3.11+\n- **pydantic-ai version**: 0.2.14 (latest)\n- **anthropic version**: 0.39.0\n\n## Additional Context\n\n- This appears to be a regression or incomplete fix of #363\n- The issue only affects Anthropic Claude models\n- OpenAI models (gpt-4o, gpt-4o-mini) work correctly with the exact same code\n- Non-streaming mode works perfectly for all models\n\n\n### Example Code\n\n```Python\nhttps://github.com/JKapostins/pydantic-ai-claude-streaming-issue\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.13.2\npydantic-ai 0.2.14\nanthropic 0.39.0\n\nclaude-3-5-sonnet-latest and claude-sonnet-4-0\n```",
      "state": "open",
      "author": "JKapostins",
      "author_type": "User",
      "created_at": "2025-06-03T22:01:22Z",
      "updated_at": "2025-06-20T14:27:36Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1904/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1904",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1904",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:47.412162",
      "comments": [
        {
          "author": "hadrien",
          "body": "Is it a duplicate of #1007? ",
          "created_at": "2025-06-07T16:55:01Z"
        },
        {
          "author": "DouweM",
          "body": "@JKapostins Can you give the `iter` based approach from https://ai.pydantic.dev/agents/#streaming a try? The `run_stream` based approach will complete as soon as the first valid final response is allowed, which in your case is the string Claude likes to return before calling a tool. `iter` will let ",
          "created_at": "2025-06-10T18:33:20Z"
        },
        {
          "author": "hayk-corpusant",
          "body": "I am also currently seeing this issue on all Sonnet + Opus models (including 4.0), and I'm calling `agent.iter`",
          "created_at": "2025-06-20T03:26:06Z"
        },
        {
          "author": "DouweM",
          "body": "@hayk-corpusant Can you please share an MRE using iter? ",
          "created_at": "2025-06-20T14:27:36Z"
        }
      ]
    },
    {
      "issue_number": 1921,
      "title": "Append \"BinaryContent\" to the prompt in an @agent.tool context ?",
      "body": "### Question\n\nis it possible to add BinaryContent in the query context ? something like that :\n\n```\n@agent.tool\ndef get_image_to_binary( ctx:RunContext, path:Path) -> str:\n    \"\"\" Retourne le contenu de l'image locale défini par 'path' (*.png,*.jpeg,*.jpg,*.gif,*.webp, ...)\"\"\"\n    if path.exists():\n        ext = path.suffix.lower().lstrip('.')\n        if ext not in [\"png\",\"jpeg\",\"jpg\",\"gif\",\"webp\"]:\n            raise ValueError(\"Extension non reconnue pour l'image '{path}' !\")\n        ctx.prompt.append( BinaryContent(data=path.read_bytes(), media_type=\"image/\"+ext) )\n        return \"Image ajoutée !\"\n    else:\n        return f\"Le fichier '{path}' n'existe pas ?!\"\n``` \n(my llm accept binarycontent (image) as input)\n\nto able to do prompts like that:\n\n     What do you see in the image \"c:\\myimage.jpeg\" ?\n\n\n### Additional Context\n\npydantic-ai-0.2.14\n\n",
      "state": "open",
      "author": "manatlan",
      "author_type": "User",
      "created_at": "2025-06-05T12:40:26Z",
      "updated_at": "2025-06-20T14:00:33Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1921/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1921",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1921",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:47.645814",
      "comments": [
        {
          "author": "Kludex",
          "body": "Can you share a snippet I can run?\n\nAlso, did you try it? It should be supported already.",
          "created_at": "2025-06-05T12:49:44Z"
        },
        {
          "author": "manatlan",
          "body": "@Kludex  : Hard to share more .... I've added a \"prompt\" example  ....\n\nif I run `agent.run_sync( [ \"What you see in image\", BinaryContent(...) ] )` -> it works like a charm\n\nBut using an `@agent.tool` to enhance the prompting by itself (to be able to use the simple prompt 'What do you see in the im",
          "created_at": "2025-06-05T13:01:50Z"
        },
        {
          "author": "manatlan",
          "body": "@Kludex  here is a snippet\n\nJust change  URL_LLM, KEY_LLM & IMAGE consts.\n\n\n```\nimport asyncio,os\nfrom pydantic_ai import Agent,BinaryContent,RunContext\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pathlib import Path\n\n# import loggin",
          "created_at": "2025-06-06T06:58:26Z"
        },
        {
          "author": "manatlan",
          "body": "@Kludex ... do you have any advices about my trouble ?",
          "created_at": "2025-06-12T15:40:44Z"
        },
        {
          "author": "Kludex",
          "body": "I can't reproduce the issue with OpenAI as provider.",
          "created_at": "2025-06-13T08:15:24Z"
        }
      ]
    },
    {
      "issue_number": 2039,
      "title": "OpenAI Reasoning Models output no thinking parts",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nSelf-explanatory\n\nI tried both OpenAIModel and OpenAIResponsesModel\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai.models.openai import OpenAIModel, OpenAIModelSettings, OpenAIResponsesModel, OpenAIResponsesModelSettings\nmodel = OpenAIResponsesModel(\"o4-mini\")\nfrom pydantic_ai.direct import model_request\nfrom pydantic_ai.messages import ModelRequest\n\nawait model_request(\n    model,\n    [ModelRequest.user_text_prompt(\"what is 2+2? think for a bit before answering\")], \n    #model_settings=GoogleModelSettings(google_thinking_config={'include_thoughts': True})\n    model_settings=OpenAIResponsesModelSettings(openai_reasoning_effort=\"high\")\n)\n\n#ModelResponse(parts=[TextPart(content='By adding 2 and 2 together, you get 4.')], #usage=Usage(request_tokens=19, response_tokens=212, total_tokens=231, details={'reasoning_tokens': 192, #'cached_tokens': 0}), model_name='o4-mini-2025-04-16', timestamp=datetime.datetime(2025, 6, 20, 7, 59, 59, #tzinfo=TzInfo(UTC)), vendor_id='resp_685514ff149c8192841b450ce9d262480187cf07203a8ab3')\n\n\n\nYou can see reasoning tokens come included in the usage\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.3.1\n```",
      "state": "closed",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-06-20T08:23:17Z",
      "updated_at": "2025-06-20T13:03:02Z",
      "closed_at": "2025-06-20T09:32:46Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2039/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2039",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2039",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:47.872847",
      "comments": [
        {
          "author": "Kludex",
          "body": "For OpenAI Responses, I'vea dded the `openai_reasoning_summary`, and it seems to be the missing piece.\n\n```py\nimport asyncio\n\nfrom rich.pretty import pprint\n\nfrom pydantic_ai.direct import model_request\nfrom pydantic_ai.messages import ModelRequest\nfrom pydantic_ai.models.openai import OpenAIRespons",
          "created_at": "2025-06-20T08:55:19Z"
        },
        {
          "author": "Kludex",
          "body": "It seems to work with Google without any change besides the prompt:\n\n```py\nimport asyncio\n\nfrom rich.pretty import pprint\n\nfrom pydantic_ai.direct import model_request\nfrom pydantic_ai.messages import ModelRequest\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel = Google",
          "created_at": "2025-06-20T08:56:48Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "Thanks! \n\nOk, I see, I can get the after-thoughts of o4-mini, but any way to get the thoughts before the answer? Or is this behaviour expected by openai models?\n\nAll the others i've fiddled with (gemini, claude) seem to be working fine",
          "created_at": "2025-06-20T09:26:05Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "I see that, they dont support it. Will close then, obrigado",
          "created_at": "2025-06-20T09:32:46Z"
        },
        {
          "author": "Kludex",
          "body": "dnd :)",
          "created_at": "2025-06-20T09:34:14Z"
        }
      ]
    },
    {
      "issue_number": 2018,
      "title": "Filter thoughts from responses for Gemini Models",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nThank you for the latest update #907 to support reasoning responses. \nI'm still trying to figure out how to filter the thoughts from the responses. I tried both the GeminiModel as well as the Google Model.\n\nWith Gemini Model. \n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.models.google import GoogleModelSettings\n\nsettings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\nagent = Agent(GeminiModel('gemini-2.5-pro-preview-05-06', provider='google-gla'), model_settings=settings)\n\nresult_sync = agent.run_sync('Which is larger 2^100 or 10^300?')\nprint(result_sync.output)\n```\n\nWith Google Model. \n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\nfrom pydantic_ai.models.google import GoogleModelSettings\n\nsettings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\n\nprovider = GoogleProvider(api_key='XXXXXX')\nmodel = GoogleModel('gemini-2.5-flash', provider=provider)\nagent = Agent(model, model_settings=settings)\n\nresult_sync = agent.run_sync('Which is larger 2^100 or 10^300?')\nprint(result_sync.output)\n\n```\nWith Gemini Model, I do not see any thoughts in the response, but with the Google Model, I do. But for the Google Model, there is no way to filter the thoughts:\n\n`\n[ModelRequest(parts=[UserPromptPart(content='Which is larger 2^100 or 10^300?', timestamp=datetime.datetime(2025, 6, 18, 11, 47, 43, 437919, tzinfo=datetime.timezone.utc))]),\n ModelResponse(parts=[TextPart(content=\"**My Approach to Comparing Exponents**\\n\\nOkay, here's how I'd tackle the comparison of $2^{100}$ and $10^{300}$. First, the goal is clear: figure out which one is bigger.  I immediately note the different bases and exponents, so I need a clever approach.\\n\\nMy initial thought is to simplify.  I can't instantly see a direct comparison, so I look for a way to manipulate them.  The exponents, $100$ and $300$, jump out. 300 is a multiple of 100, which is useful. I can express $10^{300}$ as $(10^3)^{100}$, using the rules of exponents.\\n\\nNow the problem boils down to $2^{100}$ versus $(10^3)^{100}$, or $(1000)^{100}$. A direct comparison of bases seems possible. Since $1000$ is significantly larger than $2$, and both are raised to the same power, $1000^{100}$ is demonstrably bigger.\\n\\nJust to be thorough, or to have another perspective if the initial method was less obvious, I'd consider using logarithms. If I take the base-10 logarithm of both numbers, I get $100 \\\\log_{10}(2)$ and $300 \\\\log_{10}(10)$.  Since $\\\\log_{10}(2)$ is approximately 0.301, the first simplifies to 30.1. The second is simply 300.  The log of $10^{300}$ is vastly larger than the log of $2^{100}$. Because the logarithm is a strictly increasing function, this confirms that $10^{300}$ is the larger number. In this case, I'd probably use the first method as it is more elegant. Either way, $10^{300} > 2^{100}$.\\n\"), TextPart(content=\"Let's compare $2^{100}$ and $10^{300}$.\\n\\nWe can simplify $10^{300}$ by noticing that $300 = 3 \\\\times 100$:\\n\\n$10^{300} = 10^{(3 \\\\times 100)} = (10^3)^{100}$\\n\\nNow, calculate $10^3$:\\n$10^3 = 10 \\\\times 10 \\\\times 10 = 1000$\\n\\nSo, the comparison is now between:\\n$2^{100}$ and $(1000)^{100}$\\n\\nSince both numbers are raised to the same positive power (100), we just need to compare their bases:\\n$2$ vs $1000$\\n\\nClearly, $1000 > 2$.\\n\\nTherefore, $(1000)^{100} > 2^{100}$, which means $10^{300} > 2^{100}$.\\n\\nSo, $\\\\mathbf{10^{300}}$ is larger.\")], usage=Usage(requests=1, request_tokens=19, response_tokens=249, total_tokens=1007, details={'thoughts_tokens': 739, 'text_prompt_tokens': 19}), model_name='models/gemini-2.5-flash-preview-05-20', timestamp=datetime.datetime(2025, 6, 18, 11, 47, 50, 762380, tzinfo=datetime.timezone.utc), vendor_details={'finish_reason': 'STOP'})]\n`\nHow do I filter the thoughts and with Gemini Models, why am I not getting the thoughts at all?\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12\nPydantic AI 0.3.1\n```",
      "state": "open",
      "author": "amiyapatanaik",
      "author_type": "User",
      "created_at": "2025-06-18T11:58:28Z",
      "updated_at": "2025-06-20T04:18:20Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2018/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2018",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2018",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:58.152007",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex Please have a look here!",
          "created_at": "2025-06-18T18:04:27Z"
        },
        {
          "author": "Kludex",
          "body": "I do see the thinking parts when running the following:\n\n```py\nfrom rich.pretty import pprint\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModelSettings\n\nsettings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\nagent = Agent('google-gla:gemini-2",
          "created_at": "2025-06-18T20:56:13Z"
        },
        {
          "author": "amiyapatanaik",
          "body": "> I do see the thinking parts when running the following:\n> \n> from rich.pretty import pprint\n> \n> from pydantic_ai import Agent\n> from pydantic_ai.models.google import GoogleModelSettings\n> \n> settings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\n> agent = Agent('google-",
          "created_at": "2025-06-20T04:18:20Z"
        }
      ]
    },
    {
      "issue_number": 2023,
      "title": "google-vertex provider now uses google-genai but fails due to no default location ",
      "body": "Here's my code:\n\n```py\nfrom pydantic_ai import Agent\n\ndef test_pydantic_ai_agent_sync() -> None:\n    agent = Agent(\n        model=\"google-vertex:gemini-2.0-flash\",\n    )\n    result = agent.run_sync(\n        user_prompt=\"This is a test. Respond with only the word 'okay'.\"\n    )\n    assert result.output.strip() == \"okay\"\n```\n\nAnd I'm getting the error:\n\n```\nImportError: Please install `google-genai` to use the Google model, you can use the `google` optional group — `pip install \"pydantic-ai-slim[google]\"\n```\n\nMy environment specifies `pydantic-ai-slim[vertexai]`.\n\nThe expected behavior here is that the `google-vertex` provider would be used and the `google-genai` package would not be needed.\n\nI am very confused because the above code used to work, and I'm not really sure what changed... perhaps an update or some additional configuration or environment setting got set on my system?\n\nI am seeing this in the stacktrace:\n\n```py\n.venv/lib/python3.13/site-packages/pydantic_ai/agent.py:289: in __init__\n    self.model = models.infer_model(model)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv/lib/python3.13/site-packages/pydantic_ai/models/__init__.py:564: in infer_model\n    from .google import GoogleModel\n```\n",
      "state": "open",
      "author": "dhimmel",
      "author_type": "User",
      "created_at": "2025-06-18T21:44:25Z",
      "updated_at": "2025-06-20T00:57:28Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2023/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2023",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2023",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:58.411088",
      "comments": [
        {
          "author": "Kludex",
          "body": "I sneakily replaced the model used underneath. Instead of using `GeminiModel`, I've changed to `GoogleModel` - there are some models around it.\n\nWhat I did forgot (and that's why this issue exists) is that `vertexai` extra doesn't install `google-genai`. If you replace `pydantic-ai-slim[vertexai]` b",
          "created_at": "2025-06-19T00:32:52Z"
        },
        {
          "author": "dhimmel",
          "body": "> replaced the model used underneath\n\nDo you have the commit that made the change? I'd like to see what release it occurs in.\n\n> Instead of using `GeminiModel`, I've changed to `GoogleModel`\n\nOkay so using `model=\"google-vertex:gemini-2.0-flash\"` will construct a `GoogleModel`, but I can still expli",
          "created_at": "2025-06-19T02:17:12Z"
        },
        {
          "author": "Kludex",
          "body": "- This is the PR that implemented the model: https://github.com/pydantic/pydantic-ai/pull/1751\n- This is the PR that set the `GoogleModel` as default: https://github.com/pydantic/pydantic-ai/pull/1881\n\n> The thing is that the former GeminiModel authentication / configuration just worked but GoogleMo",
          "created_at": "2025-06-19T03:28:46Z"
        },
        {
          "author": "dhimmel",
          "body": "> This is the PR that set the `GoogleModel` as default\n\nGot it. https://github.com/pydantic/pydantic-ai/commit/67c381ecd3d389b55c3f3a3396591b930f9c4f80 first released in [v0.2.18](https://github.com/pydantic/pydantic-ai/releases/tag/v0.2.18).\n\n> So after installing the `pydantic-ai-slim[google]` you",
          "created_at": "2025-06-19T14:10:26Z"
        },
        {
          "author": "dhimmel",
          "body": "From [this page](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#available-regions) about which vertex gen ai region to pick\n\n- `global` is highest availability but excludes some features\n- `us-central1` in Iowa has the most model support. This is what my pydantic-ai requests u",
          "created_at": "2025-06-19T17:04:48Z"
        }
      ]
    },
    {
      "issue_number": 1860,
      "title": "Error in streaming output from connecting to the MCP server",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm using pydantic-ai==0.2.6 (Because when I used version 0.2.11, importing the custom SSE version of the MCP server failed, which clearly indicates a bug, I had no choice but to temporarily use version 0.2.6.)\n\nIf I define `mcp_servers=[server_amap, server_AgentMap]` in the Agent, my code fails to run (**the failure occurs when the function call action is executed**). Conversely, if I comment out this MCP server definition and do not use any MCP servers, the code runs normally. My `server_AgentMap` is normal, The problem is not with it. Can you help me figure out what might be causing this issue? \nExample Code Below\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP, MCPServerStdio\nfrom my_mcp.prompt import DEFAULT_SYSTEM_PROMPT_TEMPLATE_ZH_pydantic\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nimport asyncio\n\nserver_amap = MCPServerStdio(\n    command=r\"C:\\Program Files\\nodejs\\npx.cmd\",\n    args=[\"-y\", \"@amap/amap-maps-mcp-server\"],\n    env={\"AMAP_MAPS_API_KEY\": \"xxx\"}\n)\nserver_AgentMap = MCPServerHTTP(url=\"http://my-serverip:port/sse/\")\n\nmy_model = OpenAIModel(\n    model_name='qwen-max',\n    provider=OpenAIProvider(\n        base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n        api_key='sk-xxx'\n    )\n)\n\nagent = Agent(\n    model=my_model,\n    model_settings={'parallel_tool_calls': True, 'seed': 2025},\n    name='TravelAgent',\n    mcp_servers=[server_amap, server_AgentMap],\n    system_prompt=DEFAULT_SYSTEM_PROMPT_TEMPLATE_ZH_pydantic,\n    retries=3)\n\nasync def main():\n    async with agent.run_mcp_servers():\n        # async with agent.run_stream('帮我规划下去上海东方明珠开会的行程') as response:\n        #     print(await response.get_output())\n\n        async with agent.run_stream('帮我规划下去上海东方明珠开会的行程') as result:\n            async for message in result.stream_text(delta=True):\n                print(message, end='')\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.9\npydantic-ai 0.2.6\n```",
      "state": "closed",
      "author": "illusions-LYY",
      "author_type": "User",
      "created_at": "2025-05-29T13:01:18Z",
      "updated_at": "2025-06-19T18:14:00Z",
      "closed_at": "2025-06-19T18:13:59Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1860/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1860",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1860",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:58.647409",
      "comments": [
        {
          "author": "DouweM",
          "body": "> If I define mcp_servers=[server_amap, server_AgentMap] in the Agent, my code fails to run (the failure occurs when the function call action is executed).\n\n@illusions-LYY Can you please share the error you get?\n\n> when I used version 0.2.11, importing the custom SSE version of the MCP server failed",
          "created_at": "2025-05-29T14:54:17Z"
        },
        {
          "author": "illusions-LYY",
          "body": "> > If I define mcp_servers=[server_amap, server_AgentMap] in the Agent, my code fails to run (the failure occurs when the function call action is executed).\n> \n> [@illusions-LYY](https://github.com/illusions-LYY) Can you please share the error you get?\n> \n> > when I used version 0.2.11, importing t",
          "created_at": "2025-05-30T01:40:38Z"
        },
        {
          "author": "knight9114",
          "body": "The error appears to stem from the client_stream async funtion. It calls streamable_httpclient. The method callis missing the keyword 'seconds' for the sse_read_timeout and therefore treats the variable as days. I think this breaks the max timeout of a socket causing this (poorly named) error.",
          "created_at": "2025-05-30T14:02:35Z"
        },
        {
          "author": "BrandonShar",
          "body": "Good catch @knight9114 . That error was fixed in #1843 so @illusions-LYY should be good to try upgrading again.",
          "created_at": "2025-05-31T00:54:57Z"
        },
        {
          "author": "DouweM",
          "body": "Ok I'll close this assuming https://github.com/pydantic/pydantic-ai/pull/1843 fixed it.",
          "created_at": "2025-06-02T21:14:39Z"
        }
      ]
    },
    {
      "issue_number": 2019,
      "title": "Multi-turn user prompting",
      "body": "### Question\n\nHello!\n\nFirst of all, great work!\nI’m starting to use this library and perhaps have a noobie question that I’d like clarification on.\n\nI’m trying to implement a human-in-the-loop, multi-turn system that’s driven by an agent.\n\nSo far, I have it working via a CLI, by registering a tool that seek clarification from users which pauses the execution of the agent to wait for user input. I did this by merely wrapping Prompt in a @agent.tool decorated function.\n\nHowever, things get messy very quickly when I try to move away from the Prompt pattern to getting the user input from some other external source, e.g. an UI.\n\nI’ve looked into intercepting tool calls via the iter methods, but I feel like it’s a bit messy. I also looked at the graph examples, but find them quite hard to follow. \n\nI am wondering if you have any suggestions or documentation / best-patterns for this sort of use-case? I feel like some kind of hook/callback mechanism is missing (or I couldn’t find it) to make this common use-case simpler?\n\nThanks!\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "fferroni",
      "author_type": "User",
      "created_at": "2025-06-18T12:01:57Z",
      "updated_at": "2025-06-19T14:49:11Z",
      "closed_at": "2025-06-19T00:34:08Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2019/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2019",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2019",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:58.892287",
      "comments": [
        {
          "author": "DouweM",
          "body": "@fferroni Does this example help? https://github.com/pydantic/pydantic-ai/issues/1607#issuecomment-2845597779\n\nIter or graphs shouldn't be necessary in this case, the trick is using an output type instead of a tool, so that the current run will end when the model decides it needs outside input, and ",
          "created_at": "2025-06-18T18:25:29Z"
        },
        {
          "author": "fferroni",
          "body": "Yes, this seems useful, I'll give this a shot.\nI suppose if you have multiple sources to ask from (e.g. different users) then one would externalise that logic (maybe `Ask` output specifies who to ask), but that's fine.\nThanks!",
          "created_at": "2025-06-18T20:59:29Z"
        },
        {
          "author": "fferroni",
          "body": "I suppose somewhat related, in my UI, I also wanted to visualise when a tool is being called, not just when it asks for a user prompt. I can iterate over the model requests, but I feel having some kind of tool callbacks one can over-ride might eliminate the required of iterating over the agent model",
          "created_at": "2025-06-19T00:48:09Z"
        },
        {
          "author": "DouweM",
          "body": "@fferroni We're going to be working with the team at CopilotKit to support their AG-UI standard for frontend-agent communication, which should relieve you of the need to figure out how to do things like streaming tool calls yourself: https://github.com/pydantic/pydantic-ai/issues/1864.",
          "created_at": "2025-06-19T14:49:11Z"
        }
      ]
    },
    {
      "issue_number": 1007,
      "title": "run_stream not working properly with tools in streaming mode",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nWhen using run_stream with tools in streaming mode (delta=True), the agent doesn't properly process the tool's response and returns an empty result. This happens even though the tool is correctly registered and the prompt explicitly asks for information that requires the tool's usage.\n\nThe issue only occurs when using the OpenRouter API endpoint. The same code works correctly when using the official OpenAI API endpoint.\n\nArize Phoenix log:\n`{\"openinference\": {\"span\": {\"kind\": \"LLM\"}}, \"output\": {\"value\": \"{\\\"choices\\\": [{\\\"message\\\": {\\\"tool_calls\\\": [{\\\"function\\\": {\\\"arguments\\\": \\\"{}\\\", \\\"name\\\": \\\"get_current_datetime\\\"}, \\\"index\\\": 0, \\\"id\\\": \\\"call_1dN8omFRAcSlOgZ1oRxO7bBY\\\", \\\"type\\\": \\\"function\\\"}], \\\"role\\\": \\\"assistant\\\"}, \\\"index\\\": 0, \\\"finish_reason\\\": \\\"tool_calls\\\", \\\"native_finish_reason\\\": \\\"tool_calls\\\"}], \\\"id\\\": \\\"gen-1740670893-h1Y8hGVljEaRHRZi6KkV\\\", \\\"created\\\": 1740670893, \\\"model\\\": \\\"openai/gpt-4o-mini\\\", \\\"object\\\": \\\"chat.completion.chunk\\\", \\\"system_fingerprint\\\": \\\"fp_06737a9306\\\", \\\"provider\\\": \\\"OpenAI\\\", \\\"usage\\\": {\\\"completion_tokens\\\": 12, \\\"prompt_tokens\\\": 53, \\\"total_tokens\\\": 65}}\", \"mime_type\": \"application/json\"}, \"input\": {\"value\": \"{\\\"messages\\\": [{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What time is it?\\\"}], \\\"model\\\": \\\"openai/gpt-4o-mini\\\", \\\"n\\\": 1, \\\"stream\\\": true, \\\"stream_options\\\": {\\\"include_usage\\\": true}, \\\"tool_choice\\\": \\\"auto\\\", \\\"tools\\\": [{\\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"get_current_datetime\\\", \\\"description\\\": \\\"Returns the current date and time (local time)\\\", \\\"parameters\\\": {\\\"properties\\\": {}, \\\"type\\\": \\\"object\\\", \\\"additionalProperties\\\": false}}}]}\", \"mime_type\": \"application/json\"}, \"llm\": {\"token_count\": {\"total\": 65, \"prompt\": 53, \"completion\": 12}, \"output_messages\": [{\"message\": {\"tool_calls\": [{\"tool_call\": {\"id\": \"call_1dN8omFRAcSlOgZ1oRxO7bBY\", \"function\": {\"name\": \"get_current_datetime\", \"arguments\": \"{}\"}}}], \"role\": \"assistant\"}}], \"invocation_parameters\": \"{\\\"model\\\": \\\"openai/gpt-4o-mini\\\", \\\"n\\\": 1, \\\"stream\\\": true, \\\"stream_options\\\": {\\\"include_usage\\\": true}, \\\"tool_choice\\\": \\\"auto\\\"}\", \"tools\": [{\"tool\": {\"json_schema\": \"{\\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"get_current_datetime\\\", \\\"description\\\": \\\"Returns the current date and time (local time)\\\", \\\"parameters\\\": {\\\"properties\\\": {}, \\\"type\\\": \\\"object\\\", \\\"additionalProperties\\\": false}}}\"}}], \"model_name\": \"openai/gpt-4o-mini\", \"system\": \"openai\", \"input_messages\": [{\"message\": {\"content\": \"You are a helpful assistant\", \"role\": \"system\"}}, {\"message\": {\"content\": \"What time is it?\", \"role\": \"user\"}}]}}`\n\n### Example Code\n\n```Python\nfrom datetime import datetime\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\n\nasync def get_current_datetime() -> datetime:\n    \"\"\"\n    Returns the current date and time (local time)\n    \"\"\"\n    return datetime.now()\n\n# Create a basic Pydantic AI agent\nagent = Agent(\n    model=OpenAIModel(\n        \"openai/gpt-4o-mini\",\n        openai_client=AsyncOpenAI(\n            base_url=\"https://openrouter.ai/api/v1/\",\n            api_key=\"your-api-key\"\n        )\n    ),\n    system_prompt=\"You are a helpful assistant\",\n    tools=[get_current_datetime]\n)\n\n# This returns empty result\nresult_text = \"\"\nasync with agent.run_stream(\"What time is it?\") as result:\n    async for text in result.stream_text(delta=True):\n        result_text += text\n\nprint(result_text)  # Empty string\n",
      "state": "open",
      "author": "alexandrereyes",
      "author_type": "User",
      "created_at": "2025-02-27T15:57:05Z",
      "updated_at": "2025-06-19T14:39:41Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1007/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1007",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1007",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:59.110833",
      "comments": [
        {
          "author": "Kludex",
          "body": "We released a `Agent().iter()` API that solves this issue.\n\n```py\nimport asyncio\nimport random\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'google-gla:gemini-1.5-flash',\n    deps_type=str,\n    system_prompt=(\n        \"You're a dice game, you should roll the die and see if the numb",
          "created_at": "2025-02-28T13:33:04Z"
        },
        {
          "author": "YazanShannak",
          "body": "> We released a `Agent().iter()` API that solves this issue.\n> \n> import asyncio\n> import random\n> \n> from pydantic_ai import Agent, RunContext\n> \n> agent = Agent(\n>     'google-gla:gemini-1.5-flash',\n>     deps_type=str,\n>     system_prompt=(\n>         \"You're a dice game, you should roll the die a",
          "created_at": "2025-03-27T13:26:06Z"
        },
        {
          "author": "varunmehra5",
          "body": "Hi @Kludex and @YazanShannak,\n\nI can confirm the observation made by @YazanShannak. We are also using `agent.iter()` as recommended, and while it correctly handles the streaming of intermediate steps like tool calls (`CallToolsNode`) and their results (`FunctionToolResultEvent`), the final synthesis",
          "created_at": "2025-04-08T21:05:30Z"
        },
        {
          "author": "varunmehra5",
          "body": "HI, were you able to check this out? ",
          "created_at": "2025-04-26T11:10:23Z"
        },
        {
          "author": "varunmehra5",
          "body": "HI, is there a resolution to this yet?",
          "created_at": "2025-05-17T11:30:29Z"
        }
      ]
    },
    {
      "issue_number": 1571,
      "title": "Add a setting to remove prompts and completions from tracing",
      "body": "### Description\n\nI are currently not able to use the tracing from pydantic-ai in production because the spans contain prompts and completions, which is very sensitive data.\n\nAdding a settings somewhere that disabled the tracing of sensitive data like prompts and completions would be great, and make us able to use the tracing from pydantic-ai out of the box.\n\nA possible solution could be to add a parameter to `InstrumentationSettings` that controls whether sensitive data is included in the traces.\n\n### References\n\nOther libraries, like OpenLLMetry, offer this functionality: https://www.traceloop.com/docs/openllmetry/privacy/traces",
      "state": "open",
      "author": "mikkelalv",
      "author_type": "User",
      "created_at": "2025-04-23T16:16:28Z",
      "updated_at": "2025-06-19T12:30:24Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "OpenTelemetry"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1571/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1571",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1571",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:59.360365",
      "comments": [
        {
          "author": "adtyavrdhn",
          "body": "Is it okay if I pick this one up?",
          "created_at": "2025-06-17T06:51:18Z"
        },
        {
          "author": "alexmojaki",
          "body": "@adtyavrdhn yes please",
          "created_at": "2025-06-17T10:17:03Z"
        },
        {
          "author": "dhimmel",
          "body": "Thanks @adtyavrdhn for https://github.com/pydantic/pydantic-ai/pull/2014 with a `InstrumentationSettings` option to include/exclude content, exemplified in this test case:\n\nhttps://github.com/pydantic/pydantic-ai/blob/810b1db60be2b8ab10f5b0f5b8aacce947ffef64/tests/models/test_instrumented.py#L832-L8",
          "created_at": "2025-06-18T14:08:47Z"
        },
        {
          "author": "adtyavrdhn",
          "body": "@dhimmel I get the requirement. \n\nI think something like this could work:\n\n```\nsettings = InstrumentationSettings(content_filter=remove_last_model_response)\n```\n\n```\ndef remove_model_response(content: str, message_type: ModelRequest | ModelResponse) -> str:\n    if is instance(message_type, ModelResp",
          "created_at": "2025-06-18T16:13:16Z"
        },
        {
          "author": "alexmojaki",
          "body": "I think we still want the basic `include_content=False` as a start and also as an easy option long term, and in the future we can add an additional argument or even allow `include_content` to optionally be a callable.",
          "created_at": "2025-06-19T12:30:24Z"
        }
      ]
    },
    {
      "issue_number": 2026,
      "title": "struggling with concurrent Gemini calls - seems to be auth race conditions",
      "body": "### Question\n\nI am trying to make multiple LLM calls to Gemini (using Vertex), and dispatching the `Agent.run` calls to different workers in `asyncio`.\n\nI get the following error\n```bash\npackages\\pydantic_ai\\providers\\google_vertex.py\", line 174, in _refresh_token\n    assert isinstance(self.credentials.token, str), f'Expected token to be a string, got {self.credentials.token}'  # type: ignore[reportUnknownMemberType]\nAssertionError: Expected token to be a string, got None\n```\nNothing seems to be wrong when I run these separately.\n\nI suspect there is some sort of a race condition happening where at startup, the same agent (declared globally) is trying to auth from multiple threads, and some of the requests are failing silently.\n\nIs there a way to authorize the agent at module load time?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "strunov",
      "author_type": "User",
      "created_at": "2025-06-19T09:13:03Z",
      "updated_at": "2025-06-19T09:40:25Z",
      "closed_at": "2025-06-19T09:40:07Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2026/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2026",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2026",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:59.647773",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 2 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1440\" (100% similar)\n2. \"https://github.com/pydantic/pydantic-ai/issues/1572\" (95% similar)",
          "created_at": "2025-06-19T09:20:12Z"
        },
        {
          "author": "strunov",
          "body": "> PydanticAI Github Bot Found 2 issues similar to this one:\n> \n> 1. \"[Race Condition in GoogleVertexProvider Authentication Refresh Leading to AssertionError with Concurrent Requests #1440](https://github.com/pydantic/pydantic-ai/issues/1440)\" (100% similar)\n> 2. \"[Race conditions with concurrent Ve",
          "created_at": "2025-06-19T09:40:25Z"
        }
      ]
    },
    {
      "issue_number": 1364,
      "title": "Response parts are not timestamped",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nTimestamps are missing for key model response parts.  My use case is probably not uncommon: \n\n1. Model generates text\n2. tool call happens and produces a side effect (for example, renders a card in a UI, or produces a log event as part of tool call), \n3. ToolReturnPart occurs\n4. More text. \n\nIf I want to correlate all these events, I need each part with something external, I need to have a consistent timestamp for each individual part.\n\nRelated: https://github.com/pydantic/pydantic-ai/issues/1145.  Contrary to that issue's description, most individual message parts do not, in-fact, have timestamps associated. \n\nHas Timestamp: All model request parts\n- `SystemPromptPart` \n- `UserPromptPart`\n- `RetryPromptPart`\n- `ToolReturnPart` \n\nNo timestamp all ModelResponse *parts* \n- `TextPart`\n- `ToolCallPart`\n\n`ModelResponseStreamEvent` also do not have timestamps.\n\n\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.11, 0.52, open ai.\n```",
      "state": "open",
      "author": "erccarls",
      "author_type": "User",
      "created_at": "2025-04-03T17:55:27Z",
      "updated_at": "2025-06-19T09:22:47Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1364/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1364",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1364",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:25:59.909452",
      "comments": [
        {
          "author": "erccarls",
          "body": "Also massive thanks for this amazing framework!",
          "created_at": "2025-04-03T17:56:06Z"
        },
        {
          "author": "olivernaaris",
          "body": "@erccarls I was also missing timestamps for TextPart. Did you end up with some workaround?",
          "created_at": "2025-06-19T09:22:47Z"
        }
      ]
    },
    {
      "issue_number": 2003,
      "title": "Support Gemini generating multiple candidate responses",
      "body": "### Question\n\nSo I switching to pydantic_ai from nest right now. Before I did something like this in js:\n\n```ts\n    // openAi\n    const result: ChatCompletion =\n      await this.getClient().chat.completions.create({\n        model: this.MODEL,\n        messages: [{ role: 'user', content: adPrompt }],\n        response_format: openAischema,\n        n: resultCount, // the amount of choices to generate\n      });\n\n```\n```ts\n    // gemini\n    const model = this.vertexAi.getGenerativeModel({\n    model: this.TEXT_MODEL_ID,\n    safetySettings: this.safetySettings,\n    generationConfig: {\n      responseMimeType: 'application/json',\n      responseSchema: newSchema,\n      candidateCount: resultCount,\n     },\n    });\n    const result = await model.generateContent(adPrompt);\n\n```\nNow when trying to do something like:\n```py\ngemini_settings =     GoogleModelSettings(\n        extra_body={\n            \"candidate_count\": 3, ### or \"candidateCount\"\n        },\n        candidate_count=3, # or this\n        google_safety_settings=[\n            {\n                \"category\": HarmCategory.HARM_CATEGORY_HARASSMENT,\n                \"threshold\": HarmBlockThreshold.BLOCK_ONLY_HIGH,\n            },\n            {\n                \"category\": HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n                \"threshold\": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n            },\n            {\n                \"category\": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n                \"threshold\": HarmBlockThreshold.BLOCK_ONLY_HIGH,\n            },\n            {\n                \"category\": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n                \"threshold\": HarmBlockThreshold.BLOCK_ONLY_HIGH,\n            },\n        ],\n    )\ngoogle_model = GoogleModel(name, provider) # model is flash 2.0 with vertex ai credentials and stuff\nagent = Agent(model=google_model, model_settings=gemini_settings\nresult = agent.run_sync(prompt)\n```\n\nthe result will be one string. I also tried changing output_type to List[str] but that doesnt seem to work as well, since I get a list with 1 entry. same for OpenAI with n. I saw there was a issue (https://github.com/pydantic/pydantic-ai/issues/171) that was completed with the implementation of the ModelSettings but I just cant seem to figure this one out - any pointers what I am missing?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "PhilippAlbrecht-KR",
      "author_type": "User",
      "created_at": "2025-06-17T09:25:45Z",
      "updated_at": "2025-06-19T08:41:10Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2003/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2003",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2003",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:00.159594",
      "comments": [
        {
          "author": "DouweM",
          "body": "@PhilippAlbrecht-KR You're not missing anything, multiple candidates are currently explicitly not supported:\n\nhttps://github.com/pydantic/pydantic-ai/blob/388ecc2db91877b8e1915b5843234142fcb6743d/pydantic_ai_slim/pydantic_ai/models/google.py#L275-L276\n\nhttps://github.com/pydantic/pydantic-ai/blob/38",
          "created_at": "2025-06-17T15:14:57Z"
        },
        {
          "author": "PhilippAlbrecht-KR",
          "body": "Ye sure, my hands are currently full but I will try in the upcoming weeks :)",
          "created_at": "2025-06-19T08:41:10Z"
        }
      ]
    },
    {
      "issue_number": 2011,
      "title": "Fix `ThinkingPartDelta.apply(ThinkingPart)` not updating `ThinkingPart.signature`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHi 👋 I came across an issue while trying to subclass `BedrockConverseModel` to enable mapping `ThinkingPart` to `ReasoningContentBlockOutputTypeDef`:\n\n`ThinkingPartDelta.signature_delta` does not update `ThinkingPart` during `ThinkingPartDelta.apply`, and gets discarded. This issue might apply to other models, not just bedrock.\nhttps://github.com/pydantic/pydantic-ai/blob/9c7480d8c402d58ae42f49e62f9f4a0a5a231da3/pydantic_ai_slim/pydantic_ai/_parts_manager.py#L191-L193\nhttps://github.com/pydantic/pydantic-ai/blob/9c7480d8c402d58ae42f49e62f9f4a0a5a231da3/pydantic_ai_slim/pydantic_ai/messages.py#L765-L766\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.12\nPydantic-AI: 0.3.0\nLLM: `boto3==1.38.33`\n```",
      "state": "closed",
      "author": "CallumJHays",
      "author_type": "User",
      "created_at": "2025-06-18T04:39:46Z",
      "updated_at": "2025-06-18T23:07:56Z",
      "closed_at": "2025-06-18T09:00:10Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2011/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2011",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2011",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:00.408258",
      "comments": [
        {
          "author": "Kludex",
          "body": "Thanks @CallumJHays ! My bad... 👀 \n\n- Would you be kind to try https://github.com/pydantic/pydantic-ai/pull/2012 ? ",
          "created_at": "2025-06-18T05:27:31Z"
        },
        {
          "author": "Kludex",
          "body": "I've preemptively merged it. I'll make a release.",
          "created_at": "2025-06-18T09:00:28Z"
        },
        {
          "author": "CallumJHays",
          "body": "Thanks @Kludex !",
          "created_at": "2025-06-18T23:07:56Z"
        }
      ]
    },
    {
      "issue_number": 1996,
      "title": "Access ModelRequest-ModelResponse timestamps and duration as fields",
      "body": "### Description\n\n## Context\n\n@Kludex suggested opening an issue based on this [Slack thread](https://pydanticlogfire.slack.com/archives/C083V7PMHHA/p1750081177813919).\n\nI believe that access to the request–response duration for a model call should be a built-in metric, on par with what is provided in `usage`. \n\nCurrently, you have to manually `iter()` through the agent graph to track inner request–response timings, which is cumbersome.\n\n## Problem\n\n`ModelResponse` holds a `timestamp`, but it is somewhat unreliable, as it is defined as:\n\n> The timestamp of the response. If the model provides a timestamp in the response (as OpenAI does), that will be used.\n\nConversely, `ModelRequest` does **not** have a timestamp. Its individual parts do, presumably at build time, but the full request itself is not timestamped when it is actually sent.\n\n`ModelResponse` **does** have a timestamp—presumably when the response is received—but its parts do not, which makes sense since all parts arrive in the same response.\n\nThis leads to ambiguity and unnecessary work for anyone wanting to measure or log latency.\n\n## Proposal\n\n* Add a `timestamp` field (local, UTC) to `ModelRequest`, marking the actual send time.\n* Clarify the `timestamp` in `ModelResponse` as the local time the response was received.\n* Add a `provider_timestamp` field to `ModelResponse` (if available, otherwise `None`).\n* Add a `duration` field (`timedelta` or `float` in seconds) to `ModelResponse`, computed as `response.timestamp - request.timestamp`.\n* *(Optional)* Add a `duration` field to agent runs, to capture total run time.\n\nNote also that the graph persistent API tracks both `ts` (timestamp) and `duration` for [NodeSnapshot](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.NodeSnapshot) and [NodeSnapshot](https://ai.pydantic.dev/api/pydantic_graph/persistence/#pydantic_graph.persistence.NodeSnapshot)\n\n### References\n\n_No response_",
      "state": "open",
      "author": "lionpeloux",
      "author_type": "User",
      "created_at": "2025-06-16T19:58:09Z",
      "updated_at": "2025-06-18T22:38:02Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1996/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1996",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1996",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:00.665441",
      "comments": [
        {
          "author": "corytomlinson",
          "body": "Very happy to see this getting some attention. As part of this could you please ensure the timestamp precision is the same on all parts? Currently response parts have lower precision which can cause things to get out of order when parsing conversation history...\n\n<img width=\"378\" alt=\"Image\" src=\"ht",
          "created_at": "2025-06-18T22:38:02Z"
        }
      ]
    },
    {
      "issue_number": 1995,
      "title": "Extracting `new_messages()` in the middle of an `Agent.iter` run to support user-in-the-loop approval of tool calls",
      "body": "### Question\n\nI'm trying to implement an agent which streams outputs to the user and uses tools. Some tools require user approval, which means that we need to\n- suspend the agent pending user approval and thus\n- serialize all the messages so far so we can wake up the agent once we have the user's approval.\n\nI'm trying to use `Agent.iter` to achieve this, with a function along the lines of:\n\n```python\n# Allows sending the user messages over e.g. a WebSocket:\nclass OutputStreamBase(ABC):\n    def on_tool_call_start(self, event: FunctionToolCallEvent) -> None:\n        \"\"\"Handle the start of a tool call event.\"\"\"\n        ...\n\n    @abstractmethod\n    def on_tool_call_end(self, event: FunctionToolResultEvent) -> None:\n        \"\"\"Handle the end of a tool call event.\"\"\"\n        ...\n\n    @abstractmethod\n    def on_part_start(self, index: int, part: TextPartDelta) -> None:\n        \"\"\"Handle the start of a part event.\"\"\"\n        ...\n\n    @abstractmethod\n    def on_part_delta(self, delta: TextPartDelta) -> None:\n        \"\"\"Handle the delta of a part event.\"\"\"\n        ...\n\n    @abstractmethod\n    def on_final_result(self, event: FinalResultEvent) -> None:\n        \"\"\"Handle the final result event.\"\"\"\n        ...\n\n# Allows storing the current state to e.g. a database:\nclass BaseStore(ABC):\n    @abstractmethod\n    async def add_messages(self, messages: Iterable[ModelMessage]) -> None:\n        ...\n\n    @abstractmethod\n    async def get_messages(self) -> list[ModelMessage]:\n        ...\n\n# Track the status of the agent (run):\nRunStatus = Literal['COMPLETED', 'AWAITING_APPROVAL', 'RUNNING']\n\nclass RunResult(BaseModel):\n    status: RunStatus\n    output: str | None = None\n\n# This is the interesting part:\nasync def run_agent_graph_step(\n    agent: Agent,\n    store: BaseStore,\n    sink: OutputStreamBase,\n    user_prompt: Optional[str] = None,\n) -> RunResult:\n    messages = await store.get_messages()\n\n    async with agent.iter(user_prompt, message_history=messages) as run:\n        async for node in run:\n            if agent.is_user_prompt_node(node):\n                pass\n            elif Agent.is_model_request_node(node):\n                async with node.stream(run.ctx) as request_stream:\n                    async for event in request_stream:\n                        if isinstance(event, PartStartEvent):\n                            if isinstance(event.part, TextPartDelta):\n                                sink.on_part_start(event.index, event.part)\n                        elif isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                sink.on_part_delta(event.delta)\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                pass\n                        elif isinstance(event, FinalResultEvent):\n                            pass\n            elif agent.is_call_tools_node(node):\n                async with node.stream(run.ctx) as handle_stream:\n                    async for tool_event in handle_stream:\n                        if isinstance(tool_event, FunctionToolCallEvent):\n                            if requires_approval(tool_event):\n                              # ----> HERE: we need to get new_messages() and serialize them <----\n                              return RunResult(status='AWAITING_APPROVAL')\n                            sink.on_tool_call_start(tool_event)\n                        elif isinstance(tool_event, FunctionToolResultEvent):\n                            sink.on_tool_call_end(tool_event)\n            elif Agent.is_end_node(node):\n                assert (\n                    run.result is not None\n                    and run.result.output == node.data.output\n                )\n                return RunResult(\n                    status='COMPLETED',\n                    output=node.data.output,\n                )\n\n    await store.add_messages(run.result.new_messages())\n    return RunResult(status='RUNNING')\n```\n\nIs there a way to get `new_messages()` (so far) in the middle of an `iter` run? Is there a better way to model user-in-the-loop pausing/resuming of an agent that need tool call approvals?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "tibbe",
      "author_type": "User",
      "created_at": "2025-06-16T14:56:39Z",
      "updated_at": "2025-06-18T19:15:35Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1995/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1995",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1995",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:00.904931",
      "comments": [
        {
          "author": "tibbe",
          "body": "Maybe a different way to phrase this question is: how can I \"single step\" the LLM, using the following logic:\n\n```python\ndef handle_request(prompt: str, approved_tool_call_id: str | None = None) -> State:\n    messages = load_from_db()\n    while True:\n        result = agent.step(user_prompt=prompt, m",
          "created_at": "2025-06-16T15:17:09Z"
        },
        {
          "author": "tibbe",
          "body": "This seems related to #642.",
          "created_at": "2025-06-16T15:24:51Z"
        },
        {
          "author": "DouweM",
          "body": "@tibbe Just after `elif agent.is_call_tools_node(node):`, before you call `async with node.stream(run.ctx) as handle_stream:` to actually run the node (which will start executing tools), you can read `node.model_response` to get the `ModelResponse`, which contains (among other things) the tool calls",
          "created_at": "2025-06-17T00:14:01Z"
        },
        {
          "author": "tibbe",
          "body": "@DouweM that does help. What about the `new_messages()` part? When can I extract those? I need to extract them before running the tool node (i.e. just as I `return f'tool call requires approval: {part.tool_name}'`) so I can resume later when I get the approval from the user.\n\nFrom the documentation ",
          "created_at": "2025-06-17T06:48:20Z"
        },
        {
          "author": "restless",
          "body": "@tibbe You may find https://github.com/pydantic/pydantic-ai/issues/195#issuecomment-2823476059 helpful. Not that it's a nice solution.",
          "created_at": "2025-06-17T08:19:02Z"
        }
      ]
    },
    {
      "issue_number": 2021,
      "title": "Incorrect exception type returned for gemini when token limit is exceeded",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen manually passing a token limit to the agent that's too restrictive and causes the model run to fail, the error we raise is of the wrong type\n\n\n```python\nfrom pydantic_ai import Agent\n\nAgent(\"google-gla:gemini-2.5-pro-preview-05-06\", model_settings=dict(max_tokens=5)).run_sync(\"write a haiku\")\n```\n\nyields\n\n```\nUnexpectedModelBehavior: Content field missing from Gemini response, body: (...)\n```\n\ninstead of `UsageLimitExceeded`\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nAgent(\"google-gla:gemini-2.5-pro-preview-05-06\", model_settings=dict(max_tokens=5)).run_sync(\"write a haiku\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.4\nPydantic 0.3.1\ngoogle-gla:gemini-2.5-pro-preview-05-06\n```",
      "state": "open",
      "author": "metaember",
      "author_type": "User",
      "created_at": "2025-06-18T17:12:07Z",
      "updated_at": "2025-06-18T18:18:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2021/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2021",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2021",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:01.115714",
      "comments": [
        {
          "author": "metaember",
          "body": "I guess there's a deeper question of why this is not an issue with e.g. `'gemini-1.5-flash'` whereas it is with `\"google-gla:gemini-2.5-pro-preview-05-06\"`, at least this pr fixes the error though",
          "created_at": "2025-06-18T17:42:25Z"
        }
      ]
    },
    {
      "issue_number": 1978,
      "title": "Handoffs / sub-agent delegation",
      "body": "Tools and [output functions](https://ai.pydantic.dev/output/#output-functions) can be used to do agent handoff, but still require you to define a wrapper function around `agent.run` that handles the input, output, and handling errors (possibly by bubbling them up to the top-level agent). There's also no way to easily get all of the messages in a multi-agent conversation, as top-level agent currently doesn't any have knowledge of other agents that are called from tools or output functions, so the `AgentRunResult` will only contain the top-level agent messages.\n\nIt would be nice to support `Agent(..., handoff=[agent_one, agent_two])`, similar to how OpenAI does it but of course with a Pydantic AI spin on it.",
      "state": "open",
      "author": "DouweM",
      "author_type": "User",
      "created_at": "2025-06-13T17:00:21Z",
      "updated_at": "2025-06-18T18:08:45Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1978/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1978",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1978",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:01.350000",
      "comments": []
    },
    {
      "issue_number": 2017,
      "title": "Add media processing settings to GoogleModelSettings",
      "body": "### Question\n\nHi,\n\nI noticed that gemini supports additional arguments such as trimming video and downscaling media for cheaper processing - https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding#customize-video-processing\n\nThese two things would definitely be useful, but I haven't noticed any option to use them with PydanticAI, I could be wrong as I'm just learning this library, but I would really appreciate an answer.\n\nBasically, having such an option would reduce the cost of using gemini for video processing by 3 times, which is quite a significant amount + would allow us to hard limit the token usage per video (for example by analysing only the first 5 minutes of the video), which is another useful thing.\n\nBest regards\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Sumered",
      "author_type": "User",
      "created_at": "2025-06-18T10:43:39Z",
      "updated_at": "2025-06-18T18:03:53Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2017/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2017",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2017",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:01.350024",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Sumered Those arguments are not currently supported, but would be quite easy to add:\n1. Here in `GoogleModelSettings`: \n    https://github.com/pydantic/pydantic-ai/blob/f2646dedc34fcdd408b665753df1967e22e3f7a6/pydantic_ai_slim/pydantic_ai/models/google.py#L99-L121\n3. Then here in `GoogleModel._gene",
          "created_at": "2025-06-18T18:03:27Z"
        }
      ]
    },
    {
      "issue_number": 2016,
      "title": "Remove deprecated preview models from LatestGeminiModelNames",
      "body": "### Question\n\nGemini 2.5 Flash and Gemini 2.5 Pro have reached general availability (GA) as of June 17, 2025. Preview endpoints will be discontinued on July 15, 2025.\n\nCurrently pydantic-ai models:\n\n```\nLatestGeminiModelNames = Literal[\n    \"gemini-1.5-flash\",\n    \"gemini-1.5-flash-8b\",\n    \"gemini-1.5-pro\",\n    \"gemini-1.0-pro\",\n    \"gemini-2.0-flash\",\n    \"gemini-2.0-flash-lite-preview-02-05\",\n    \"gemini-2.0-pro-exp-02-05\",\n    \"gemini-2.5-flash-preview-05-20\",\n    \"gemini-2.5-pro-exp-03-25\",\n    \"gemini-2.5-pro-preview-05-06\",\n]\n```\n\n```\nPreview Endpoint Availability and Removal: All existing Gemini 2.5 Flash and Pro preview endpoints (listed below) will continue to be available with their current preview pricing until July 15, 2025. After this date, these preview endpoints will be shut down.\no\tgemini-2.5-flash-preview-04-17\no\tgemini-2.5-flash-preview-05-20\no\tgemini-2.5-pro-preview-03-25\no\tgemini-2.5-pro-preview-05-06\no\tgemini-2.5-pro-preview-06-05\nRequired actions:\nTo ensure uninterrupted service and to use the new GA capabilities, you will need to do the following:\n•\tMigrate to new GA endpoints: Update your applications to use the new GA endpoints (gemini-2.5-flash and gemini-2.5-pro) before July 15, 2025. After this date, the preview endpoints will be decommissioned.\n```\n\nThis is a message recieved from Google for my projects that run on GCP and use pydantic-ai agents. \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ShacharYonai",
      "author_type": "User",
      "created_at": "2025-06-18T08:47:45Z",
      "updated_at": "2025-06-18T17:55:52Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2016/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2016",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2016",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:01.565247",
      "comments": [
        {
          "author": "DouweM",
          "body": "@ShacharYonai Thank you, we can remove them now or once they are shut down on July 15. Can you please submit a PR?",
          "created_at": "2025-06-18T17:54:41Z"
        }
      ]
    },
    {
      "issue_number": 2015,
      "title": "Test using qwen2.5-14b, program freezes when setting agent parameter outputting type=UserProfile",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n# I deployed qwen2.5-14B using the latest vllm and opened the object using the latest pydantic.ai, but when I enabled output type=UserProfile, the program froze\n\n\n### Example Code\n\n```Python\nfrom datetime import date\nfrom typing import Dict, List\nfrom loguru import logger\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic import ValidationError\nfrom typing_extensions import TypedDict\nfrom pydantic_ai import Agent\n\nclass UserProfile(TypedDict, total=False):\n    name: str\n    dob: date\n    bio: str\n\nclass Chatbot:\n    def __init__(self):\n        self.model = OpenAIModel(\n            model_name=\"qwen2.5-14b-instruct\",\n            provider=OpenAIProvider(\n                base_url=\"http://0.0.0.0:7509/v1/\",\n                api_key=\"password\"\n            ),\n        )\n        self.agent = Agent(\n            model=self.model,\n            system_prompt='please extract the user profile information from the following text. The output should be a JSON object with the keys \"name\", \"dob\" (date of birth), and \"bio\" (a short biography). If any information is not available, leave that key out of the JSON object.',\n            output_type=UserProfile\n        )\n\n\nasync def main():\n    chatbot = Chatbot()\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n\n    # async with chatbot.agent.run_stream(user_prompt=user_input) as response:\n    #     async for chunk in response.stream_structured():\n    #         print(chunk, end='', flush=True)\n    async with chatbot.agent.run_stream(user_input) as result:\n        async for message, last in result.stream_structured():\n            # print(last, message)\n            try:\n                profile = await result.validate_structured_output(  \n                    message,\n                    allow_partial=not last,\n                )\n            except ValidationError:\n                continue\n            print(profile)\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.9.21\npydantic-ai 0.3.0\nvllm 0.9.0.1\n```",
      "state": "open",
      "author": "wmj9346464543",
      "author_type": "User",
      "created_at": "2025-06-18T08:43:33Z",
      "updated_at": "2025-06-18T17:53:15Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2015/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2015",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2015",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:01.776363",
      "comments": [
        {
          "author": "DouweM",
          "body": "@wmj9346464543 Thanks for the report. Can you please add [Logfire](https://ai.pydantic.dev/logfire/) so we can figure out exactly where it gets stuck? \n\nI also recommend upgrading to a later Python version.",
          "created_at": "2025-06-18T17:53:12Z"
        }
      ]
    },
    {
      "issue_number": 2009,
      "title": "Make hardcoded tool retry/error messages configurable",
      "body": "### Description\n\nFollowup of #1993 \n\n- Extract hardcoded strings, make them configurable\n- Possibly prepare for i18n\n\n### References\n\n_No response_",
      "state": "open",
      "author": "hovi",
      "author_type": "User",
      "created_at": "2025-06-17T23:37:45Z",
      "updated_at": "2025-06-18T17:51:13Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2009/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2009",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2009",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:01.975314",
      "comments": []
    },
    {
      "issue_number": 2007,
      "title": "Inconsistent naming between `FunctionToolCallEvent.call_id` and `FunctionToolResultEvent.tool_call_id` ",
      "body": "@proever I agree that'd be worth doing, although what bothers me more is the inconsistent naming `tool_call_id` vs `call_id` :D That's part of the public API though, so we could define an alias and use it ourselves but we'd need to keep the old name (at least until v1).\r\n\r\n_Originally posted by @DouweM in https://github.com/pydantic/pydantic-ai/pull/1960#discussion_r2145645608_\r\n            ",
      "state": "open",
      "author": "proever",
      "author_type": "User",
      "created_at": "2025-06-17T20:10:55Z",
      "updated_at": "2025-06-18T17:48:09Z",
      "closed_at": null,
      "labels": [
        "refactor"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2007/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2007",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2007",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:01.975342",
      "comments": [
        {
          "author": "proever",
          "body": "`call_id` is only used in `_agent_graph.py` and as a field of OpenAI's `FunctionCallOutput` (so not part of Pydantic AI).\n\nConversely, `tool_call_id` is used pretty widely everywhere else. \n\nWe made `FunctionToolCallEvent.call_id` a property in #1960 (computed from `self.part.tool_call_id`). I would",
          "created_at": "2025-06-17T20:15:46Z"
        },
        {
          "author": "DouweM",
          "body": "@proever Sounds good, as long as we keep `call_id` around as an alias because it's part of the public API.",
          "created_at": "2025-06-18T17:48:02Z"
        }
      ]
    },
    {
      "issue_number": 1987,
      "title": "Gotcha: usage_limits must be in run* function",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nNot a bug, just a learning to share in case others make the same mistake!\n---\nRecently, I had a few infinite loop agents despite having set `usage_limits` -- after some debugging, I realized that I was passing `usage_limits` key to the Agent, eg,  `Agent(..., usage_limits=UsageLimits(request_limit=3))`.\nI had a lot of `run()` calls, so I suspect I wanted to set it globally.\n\nThe correct usage based on the docs is to pass it to each invocation of the `run*`, eg, `run(..., usage_limits=UsageLimits(request_limit=3))`\n\nMy \"wish\" would be that providing an unsupported kwarg to the Agent would fail explicitly -- not sure where this unsupported argument got slotted.\n\n\nSee a modification of the docs example that DOES NOT WORK:\n\n### Example Code\n\n```Python\n# modification of https://ai.pydantic.dev/agents/#usage-limits\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent, ModelRetry\nfrom pydantic_ai.exceptions import UsageLimitExceeded\nfrom pydantic_ai.usage import UsageLimits\n\nclass NeverOutputType(TypedDict):\n    \"\"\"\n    Never ever coerce data to this type.\n    \"\"\"\n\n    never_use_this: str\n\n\nagent = Agent(\n    'openai:gpt-4.1-mini',\n    retries=3,\n    output_type=NeverOutputType,\n    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n    # THIS IS WRONG!!! IT WILL NOT WORK but it fails silently\n    usage_limits=UsageLimits(request_limit=3)\n)\n\ncounter = 0\n@agent.tool_plain(retries=5)  \ndef infinite_retry_tool() -> int:\n    global counter\n    counter += 1\n    print(f\"Called infinite_retry_tool {counter} times\")\n    if counter == 5:\n        raise Exception(\"Too many retries\")\n    raise ModelRetry('Please try again.')\n\n\ntry:\n    result_sync = agent.run_sync(\n        'Begin infinite retry loop!'  \n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next request would exceed the request_limit of 3\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.2.18\n```",
      "state": "open",
      "author": "svilupp",
      "author_type": "User",
      "created_at": "2025-06-15T08:33:14Z",
      "updated_at": "2025-06-18T17:41:36Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1987/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1987",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1987",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:02.187836",
      "comments": [
        {
          "author": "DouweM",
          "body": "@svilupp Thanks for the report, we use `**_deprecated_kwargs` on a bunch of methods to support old kwarg names, but I agree we could raise an error if something unexpected is passed in there.\n\nGenerally though, we've done a lot of work to make Pydantic AI completely type-safe, so we strongly recomme",
          "created_at": "2025-06-16T21:36:03Z"
        },
        {
          "author": "svilupp",
          "body": "> [@svilupp](https://github.com/svilupp) Thanks for the report, we use `**_deprecated_kwargs` on a bunch of methods to support old kwarg names, but I agree we could raise an error if something unexpected is passed in there.\n> \n> Generally though, we've done a lot of work to make Pydantic AI complete",
          "created_at": "2025-06-18T08:48:42Z"
        },
        {
          "author": "DouweM",
          "body": "@svilupp For the PR, I imagine we'd `_deprecated_kwargs.pop()` the expected keys and then raise a `UserError` if there are keys remaining after we've done that.",
          "created_at": "2025-06-18T17:41:30Z"
        }
      ]
    },
    {
      "issue_number": 907,
      "title": "Reasoning response support",
      "body": "See:\n* [OpenAI reasoning docs](https://platform.openai.com/docs/guides/reasoning)\n* [Deepseek reasoning docs](https://api-docs.deepseek.com/guides/reasoning_model)\n* [Gemini \"thinking\" docs](https://ai.google.dev/gemini-api/docs/thinking)\n\nNo idea yet how this should look, but we should try to support it.",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-02-12T19:46:05Z",
      "updated_at": "2025-06-18T11:37:36Z",
      "closed_at": "2025-06-18T01:27:36Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/907/reactions",
        "total_count": 22,
        "+1": 16,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 6
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/907",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/907",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:02.415775",
      "comments": [
        {
          "author": "Wh1isper",
          "body": "I've used some pretty tricky ways to implement reasoning in the pydantic-ai-bedrock package: https://github.com/ai-zerolab/pydantic-ai-bedrock/pull/15\n\nBased on this practice, I think we need to add a ReasoningPart, and then the Model will implement the object transformation based on that. On top of",
          "created_at": "2025-03-05T06:10:24Z"
        },
        {
          "author": "soichisumi",
          "body": "I would like to request support for Claude's Extended thinking.\nWhile it may be difficult to be compatible with other models, this feature provides robust and flexible thinking and integration with tool use.\nhttps://docs.anthropic.com/en/docs/build-with-claude/extended-thinking?q=thinking#how-extend",
          "created_at": "2025-03-21T01:06:24Z"
        },
        {
          "author": "arty-hlr",
          "body": "Maybe openrouter's reasoning tokens should also be added: https://openrouter.ai/docs/use-cases/reasoning-tokens for people who use Claude 3.7 through it for example. At the moment no reasoning is shown.",
          "created_at": "2025-04-03T14:50:07Z"
        },
        {
          "author": "arty-hlr",
          "body": "Link to relevant slack conversation: \nhttps://pydanticlogfire.slack.com/archives/C083V7PMHHA/p1743405703872439",
          "created_at": "2025-04-07T14:03:35Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "Any news on this?",
          "created_at": "2025-04-14T08:55:27Z"
        }
      ]
    },
    {
      "issue_number": 1990,
      "title": "Gemini fails with INVALID_ARGUMENT when using tool without arguments and `output_type` at the same time",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI couldn't find this issue open already.\n\nRunning the code example throws the following error:\n```\n  File \"/Users/barperach/aironworks/aironworksserver/venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/barperach/aironworks/aironworksserver/venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 300, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/barperach/aironworks/aironworksserver/venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 353, in _make_request\n    model_response = await ctx.deps.model.request(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        ctx.state.message_history, model_settings, model_request_parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/barperach/aironworks/aironworksserver/venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 206, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/barperach/aironworks/aironworksserver/venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 27, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/barperach/aironworks/aironworksserver/venv/lib/python3.13/site-packages/pydantic_ai/models/gemini.py\", line 154, in request\n    async with self._make_request(\n               ~~~~~~~~~~~~~~~~~~^\n        messages, False, cast(GeminiModelSettings, model_settings or {}), model_request_parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ) as http_response:\n    ^\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/barperach/aironworks/aironworksserver/venv/lib/python3.13/site-packages/pydantic_ai/models/gemini.py\", line 245, in _make_request\n    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=r.text)\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.0-flash, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Request contains an invalid argument.\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n```\n\nIt seems that something has changed on google side.\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent,RunContext\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic import BaseModel\n\nclass TextMessage(BaseModel):\n    message: str\n\n\nmodel_gemini = GeminiModel(\n    'gemini-2.0-flash',\n)\n\naironworks_agent = Agent[None, TextMessage](\n    system_prompt=\"You are a common assitant answering questions\",\n    model=model_gemini,\n    output_type=TextMessage,\n    instrument=True,\n    retries=5,\n)\n\n@aironworks_agent.tool\nasync def something(ctx: RunContext[None]) -> TextMessage:\n    return TextMessage(message=\"hello\")\n\nprint(aironworks_agent.run_sync(\"trying something\"))\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic_ai==0.2.18\n```",
      "state": "closed",
      "author": "barp",
      "author_type": "User",
      "created_at": "2025-06-16T01:47:51Z",
      "updated_at": "2025-06-18T10:08:46Z",
      "closed_at": "2025-06-17T00:14:30Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1990/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1990",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1990",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:02.682064",
      "comments": [
        {
          "author": "barp",
          "body": "It seems to be related to the toolConfig parameter of the API, by removing \nthe following lines \nhttps://github.com/pydantic/pydantic-ai/blob/ae0c3ce0b4120e00670abfc3bcfd12793e545be5/pydantic_ai_slim/pydantic_ai/models/gemini.py#L217-L218\n\nthe example code does not fail anymore.",
          "created_at": "2025-06-16T02:04:34Z"
        },
        {
          "author": "barp",
          "body": "It also happens with GoogleModel\n\n```\nfrom pydantic_ai import Agent,RunContext\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic import BaseModel\n\nclass TextMessage(BaseModel):\n    message: str\n\n\nmodel_gemini = GoogleModel(\n    'gemini-",
          "created_at": "2025-06-16T02:13:51Z"
        },
        {
          "author": "LongTCH",
          "body": "I got the same error on gemini before 2.5",
          "created_at": "2025-06-16T02:13:56Z"
        },
        {
          "author": "barp",
          "body": "Yes but the same code worked last week.",
          "created_at": "2025-06-16T02:48:58Z"
        },
        {
          "author": "Pesokrava",
          "body": "It seems the issue is caused if you omit any input arguments in a tool.\n\nAdding a dummy argument to tool resolves the issue.\n\n```Python\n@aironworks_agent.tool\nasync def something(ctx: RunContext[None], _: str) -> TextMessage:\n    return TextMessage(message=\"hello\")\n```\n\nEDIT:\n\nI tested this also wit",
          "created_at": "2025-06-16T08:39:41Z"
        }
      ]
    },
    {
      "issue_number": 1982,
      "title": "Azure Provider requires environment variables",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using Azure Provider, I should be able to either provide the URL and API key in environment variables or as a kwarg. But it doesn't work with just using kwarg, it only works with the environment variables\n\n The example code will result in\n`pydantic_ai.exceptions.UserError: Must provide one of the `azure_endpoint` argument or the `AZURE_OPENAI_ENDPOINT` environment variable`\n\n\n### Example Code\n\n```Python\nmodel = OpenAIModel(\n    model_name,\n    provider=AzureProvider(\n        azure_endpoint='xxx',\n        api_key='xxx',\n        api_version='xxx'\n    )\n)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.12\npydantic ai 0.2.18\n```",
      "state": "closed",
      "author": "TimW9",
      "author_type": "User",
      "created_at": "2025-06-14T05:50:29Z",
      "updated_at": "2025-06-18T07:12:27Z",
      "closed_at": "2025-06-18T05:29:29Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1982/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1982",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1982",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:02.999431",
      "comments": [
        {
          "author": "Kludex",
          "body": "I can't reproduce the exception with your code.\n\n```py\nfrom pydantic_ai.providers.azure import AzureProvider\n\nprovider = AzureProvider(azure_endpoint='xxx', api_key='xxx', api_version='xxx')\n```\n\nThis doesn't raise.\n",
          "created_at": "2025-06-16T16:23:18Z"
        },
        {
          "author": "Kludex",
          "body": "Let me know if I can help here. 🙏 ",
          "created_at": "2025-06-18T05:29:29Z"
        },
        {
          "author": "TimW9",
          "body": "Hi, I tried it again and it doesn't seem to raise anymore. I can't reproduce it anymore. Feel free to close this",
          "created_at": "2025-06-18T07:12:27Z"
        }
      ]
    },
    {
      "issue_number": 797,
      "title": "Azure OpenAI API Streaming Response Causes AttributeError in pydantic-ai",
      "body": "When using pydantic-ai with Azure OpenAI's streaming API, an AttributeError occurs due to the API returning deltas without content fields.\n\n**Steps to Reproduce**\n\n1. Configure Azure OpenAI client with api_version=\"2024-12-01-preview\"\n2. Create an OpenAIModel using the Azure client\n3. Use Agent.run_stream() with the model\n4. Attempt to process streaming response\n\n**Error:**\n\n```\nTraceback (most recent call last):\n  File \".../pydantic_ai/models/openai.py\", line 306, in _get_event_iterator\n    content = choice.delta.content\n             ^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'content'\n```\n\n**Analysis**\nThe error occurs in pydantic-ai's OpenAI model implementation where it assumes every delta in the streaming response contains a content field. However, with Azure OpenAI's API, some deltas (like role initialization) may not include content, resulting in choice.delta.content being None.\n\n**Expected Behavior**\nThe streaming implementation should handle deltas without content fields gracefully, possibly by:\n\n- Checking if delta.content exists before accessing it\n- Skipping deltas without content\n- Or handling special message types separately\n\n**Current Workaround**\n\nUsing API version \"2023-05-15\" works but has limitations:\n\n- Streaming chunks are much larger (several sentences long)\n- Less granular streaming experience compared to standard OpenAI API\n- Makes the streaming appear more \"blocky\" rather than word-by-word\n\n**Environment:**\n\n- pydantic-ai: 0.0.20\n- OpenAI Python package version: 1.60.2\n- Azure OpenAI API version tested: 2024-12-01-preview",
      "state": "open",
      "author": "dabrodev",
      "author_type": "User",
      "created_at": "2025-01-28T16:24:19Z",
      "updated_at": "2025-06-18T05:38:24Z",
      "closed_at": null,
      "labels": [
        "Stale",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/797/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/797",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/797",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:03.244742",
      "comments": [
        {
          "author": "sydney-runkle",
          "body": "Thanks for the report @dabrodev, I think we could probably work around this as you've suggested.",
          "created_at": "2025-01-29T20:05:59Z"
        },
        {
          "author": "Kludex",
          "body": "@dabrodev Can you give an snippet that reproduces the issue? I can't reproduce it here.",
          "created_at": "2025-03-13T16:17:39Z"
        },
        {
          "author": "Kludex",
          "body": "I'm happy to reopen this if someone can reproduce the issue.",
          "created_at": "2025-03-16T10:40:36Z"
        },
        {
          "author": "martgra",
          "body": "@Kludex \n\nDont know what API version in Azure is currently the newest but I encountered this error with the following:\n\npython 3.11\npydantic-ai-slim = {version = \"^0.0.40\", extras = [\"openai\"]}\n\n\nSteps to reproduce. In Azure Open AI -> \n\n1. Set content filter to Asynchronous Filter for given streami",
          "created_at": "2025-03-17T13:05:14Z"
        },
        {
          "author": "martgra",
          "body": "@Kludex Is there a possibility to prfioritize this? Kind of a dealbreaker for us using Azure - I guess that means a lot of enterprises. \n\nImportant - the error only occurs when using Azure output filtering in async mode (if you are to test). My guess this is on their hand so will file bug to them as",
          "created_at": "2025-03-24T14:49:30Z"
        }
      ]
    },
    {
      "issue_number": 748,
      "title": "Gemini causes 'Event loop is closed'  when running inside an async context",
      "body": "Hi,\n\nThis is an attempt to use `google-gla:gemini-2.0-flash-exp` via streamit in async,\n\nCode below causes `RuntimeError: Event loop is closed`, same code works just fine for `openai:gpt-4o`\n\n\n```python\nimport asyncio\n\nimport streamlit as st\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelRequest, UserPromptPart\n\n\nasync def main():\n    st.title(\"Simple chat\")\n\n    # Initialize chat history\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        part1 = message.parts[0]\n        role = \"\"\n        if isinstance(part1, UserPromptPart):\n            role = \"user\"\n        with st.chat_message(\"human\" if role == \"user\" else \"ai\"):\n            st.markdown(message.parts[0].content)\n\n    # Accept user input\n    if prompt := st.chat_input(\"Say something...\"):\n        # Display user message in chat message container\n        st.chat_message(\"user\").markdown(prompt)\n\n        # Add user message to chat history\n        st.session_state.messages.append(\n            ModelRequest(parts=[UserPromptPart(content=prompt)])\n        )\n\n        # Display assistant response in chat message container\n        with st.chat_message(\"assistant\"):\n            response_container = st.empty()  # Placeholder for streaming response\n            agent = Agent(\n                \"google-gla:gemini-2.0-flash-exp\",\n                # \"openai:gpt-4o\", # works!\n                # https://ai.pydantic.dev/results/#structured-result-validation\n                result_type=str,  # type: ignore\n                system_prompt=\"you are a nice bot\",\n            )\n            result = await agent.run(prompt)\n            response_container.markdown(result.data)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "state": "open",
      "author": "asaf",
      "author_type": "User",
      "created_at": "2025-01-22T19:10:25Z",
      "updated_at": "2025-06-18T05:28:53Z",
      "closed_at": null,
      "labels": [
        "question",
        "more info"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 48,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/748/reactions",
        "total_count": 4,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/748",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/748",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:03.468900",
      "comments": []
    },
    {
      "issue_number": 793,
      "title": "How to include thoughts from the Gemini model in Pydantic output?",
      "body": "I need help. Is there a way to include the thoughts from `gemini-2.0-flash-thinking-exp` in the response?\n\nHere is how it works with the Gemini API.\n\n```\nfrom google import genai\n\nclient = genai.Client(api_key='GEMINI_API_KEY', http_options={'api_version':'v1alpha'})\n\nconfig = {'thinking_config': {'include_thoughts': True}}\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Explain how RLHF works in simple terms.',\n    config=config\n)\n\nfor part in response.candidates[0].content.parts:\n    if part.thought:\n        print(f\"Model Thought:\\n{part.text}\\n\")\n    else:\n        print(f\"\\nModel Response:\\n{part.text}\\n\")\n```\n\nThanks for help :) ",
      "state": "closed",
      "author": "MarkusOdenthal",
      "author_type": "User",
      "created_at": "2025-01-28T12:52:58Z",
      "updated_at": "2025-06-18T01:27:36Z",
      "closed_at": "2025-06-18T01:27:36Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/793/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/793",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/793",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:03.468930",
      "comments": [
        {
          "author": "sydney-runkle",
          "body": "Hi!\n\nThanks for your inquiry. This isn't yet supported, but is something we can work to accommodate. I'm guessing we'll have to add  some info to the `parts` of the gemini response.",
          "created_at": "2025-01-30T11:32:37Z"
        },
        {
          "author": "MarkusOdenthal",
          "body": "Thanks for your response :)\n\nI think this would be great, especially for tracking this thinking. I had a great experience using this thinking to improve prompts. Also for deepseek r1 this is missing I guess. \n\nI also could try it but have no idea where to start.\n\nI gemini you need to set the paramet",
          "created_at": "2025-01-30T15:20:39Z"
        },
        {
          "author": "tranhoangnguyen03",
          "body": "Similar to https://github.com/pydantic/pydantic-ai/issues/731?",
          "created_at": "2025-02-04T08:13:41Z"
        },
        {
          "author": "Kludex",
          "body": "On the example in this message, nothing comes out from the `if part.thought` in the example... 🤔 \n\nDo you know an example that actually uses it?",
          "created_at": "2025-03-17T07:26:37Z"
        },
        {
          "author": "Kludex",
          "body": "I tried all the examples from https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash_thinking_mode.ipynb.\n\nIt seems none have `part.thought` being `True`.",
          "created_at": "2025-03-17T07:31:17Z"
        }
      ]
    },
    {
      "issue_number": 2004,
      "title": "Show tool response in the running tool span",
      "body": "### Description\n\nCurrently only the tool arguments are shown.\n\nThis would make the response easier to find.\n\n\n\n### References\n\n_No response_",
      "state": "open",
      "author": "tekumara",
      "author_type": "User",
      "created_at": "2025-06-17T10:58:05Z",
      "updated_at": "2025-06-18T00:26:11Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "OpenTelemetry"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2004/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2004",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2004",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:03.705704",
      "comments": [
        {
          "author": "DouweM",
          "body": "@alexmojaki Is this feasible?",
          "created_at": "2025-06-17T15:15:19Z"
        },
        {
          "author": "alexmojaki",
          "body": "Yes",
          "created_at": "2025-06-17T15:21:11Z"
        }
      ]
    },
    {
      "issue_number": 1814,
      "title": "Register new tools during a run",
      "body": "### Question\n\nI am using Pydantic AI via the graph iteration method. One of the tools that can get called by the model is a meta tool that it can use to create a new tool. I managed to make this work with the `Agent` (the tools are registered to the agent via `property` on another class that returns a `list[Tool]`) with one caveat: it seems this newly-created tool only becomes available on the *next* full run of the `Agent`.\n\nI've also tried adding something like this to each step of the iteration:\n```py\n  async for node in run:\n      # refresh tools (to account for dynamic tools created this run)\n      agent._function_tools = {\n          tool.name: tool for tool in self.deps.tool_manager.pydantic_ai_tools\n      }\n```\nwith seemingly no success.\n\nAny ideas on what I might be doing wrong? Is there an \"official\" way to dynamically register tools during a run? In my example above I am using the private `_function_tools` attribute. I know I can edit or not include a`ToolDefinition` dynamically via `prepare` but I don't think that route supports entirely new tools.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "proever",
      "author_type": "User",
      "created_at": "2025-05-23T11:41:19Z",
      "updated_at": "2025-06-18T00:25:49Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1814/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1814",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1814",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:03.959645",
      "comments": [
        {
          "author": "DouweM",
          "body": "@proever This works for me (with logfire for logging):\n\n```py\nimport logfire\nfrom pydantic_ai import Agent\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n\nagent = Agent(\"gpt-4o-mini\")\n\n\ndef even_cooler_tool():\n    return \"Even cooler!\"\n\n\n@agent.tool_plain\ndef cool_tool():\n    agent.tool_plain",
          "created_at": "2025-05-26T17:22:43Z"
        },
        {
          "author": "proever",
          "body": "thanks for the reply! clearly it is possible, from your example. I guess my only remaining question would be if there is a functional difference to doing what you've done: `agent.tool_plain(even_cooler_tool)` vs manually updating `agent._function_tools` in my example?",
          "created_at": "2025-05-31T23:16:34Z"
        },
        {
          "author": "DouweM",
          "body": "@proever From a quick look at the code, the only differences are in setting the default values as `Tool` is built, and in checking for name conflicts. So I'm not sure why directly setting `agent._function_tools` wasn't working for you. Of course it's not recommended to directly touch private propert",
          "created_at": "2025-06-02T21:28:56Z"
        },
        {
          "author": "proever",
          "body": "I was able to get things working in the end by passing the `Agent` around in my `deps` and calling `ctx.deps.agent._register_tool(tool)`. Still a private method but at least it's working! Thanks again.",
          "created_at": "2025-06-04T16:52:23Z"
        },
        {
          "author": "proever",
          "body": "I actually think that was broken in `0.2.16`, specifically #1918 \n\n```py\nrun_function_tools = {k: dataclasses.replace(v) for k, v in self._function_tools.items()}\n```\n\nby copying the `dict` we can no longer modify it elsewhere and have the changes take effect immediately (ie on that run). \n\nwith thi",
          "created_at": "2025-06-11T15:29:09Z"
        }
      ]
    },
    {
      "issue_number": 1913,
      "title": "Emit unknown tool call event in `Agent.iter`",
      "body": "### Description\n\nI'm iterating over the graph with `Agent.iter` and `node.stream`, as shown in https://ai.pydantic.dev/agents/#streaming. \n\nCurrently, when the model tries to call a tool the `Agent` does not believe is registered (which I was running into in #1814), `_agent_graph.process_function_tools` simply appends a `RetryPromptPart` to `output_parts` without yielding any events: \n\n```py\nelse:\n    output_parts.append(_unknown_tool(call.tool_name, call.tool_call_id, ctx))\n``` \n\nThis means that the `CallToolsNode` does not yield any events in turn, which makes it hard to catch these \"non-events\" and handle them (for example we have a custom message model in our DB and would need to store the fact that the tool call failed, otherwise Anthropic complains about mismatched tool calls & returns). \n\nCurrently I'm doing something like this: \n\n```py\n# assuming `Agent.is_call_tools_node(node) is True`\n# special case handling for retry prompt caused by missing tool\nevent_count = 0\nasync with node.stream(run.ctx) as handle_stream:\n        async for event in handle_stream:\n            event_count += 1\n\nif (\n    event_count == 0\n    and isinstance(node._next_node, ModelRequestNode)\n    and len(node._next_node.request.parts) > 0\n    and isinstance(part := node._next_node.request.parts[0], RetryPromptPart)\n):\n    print(f\"=== RetryPromptPart: {node._next_node.request} ===\")\n    # write to DB here...\n```\n\nI might be twisting things into something Pydantic AI doesn't want to support with my very custom handling here, but it would be nice (for me) if there was an event emitted that I could catch. Or maybe I'm looking at things from the wrong perspective and there's a better way to do this!\n\nAs usual, thanks in advance for any help!\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "proever",
      "author_type": "User",
      "created_at": "2025-06-04T17:06:49Z",
      "updated_at": "2025-06-17T19:30:00Z",
      "closed_at": "2025-06-17T19:30:00Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1913/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1913",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1913",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:04.212361",
      "comments": [
        {
          "author": "DouweM",
          "body": "@proever Thanks for the report. It makes sense to yield `FunctionToolCallEvent` and `FunctionToolResultEvent` for unknown tool calls as well, just like we already do for failed tool calls (as `FunctionToolResultEvent` can wrap a `RetryPromptPart`) in the code surrounding the line you quoted.\n\nWould ",
          "created_at": "2025-06-06T22:42:00Z"
        },
        {
          "author": "proever",
          "body": "yes, happy to give it a shot! might take me a few days though",
          "created_at": "2025-06-10T03:49:38Z"
        },
        {
          "author": "proever",
          "body": "implemented in #1960 ",
          "created_at": "2025-06-17T19:30:00Z"
        }
      ]
    },
    {
      "issue_number": 2005,
      "title": "Is it possible to use Pydantic-AI with the Gemini Files API ?",
      "body": "### Question\n\nI am currently using the google.genai API (and the Files API) directly and would like to switch over to Pydantic-AI, but I'd like to know if there is a way/workaround to make my use case work.\n\nFor context, I am running prompts for image analysis that include 20-30 images each (~15 image examples, ~ 5-15 images to analyze). For prompt tuning and performance testing, I run batches of e.g. 1000 test cases repeatedly. This causes the following issues:\n- Uploading images takes a significant amount of time, slowing down experiments\n- Image examples would get uploaded repeatedly (I could cache the prompt, but see below)\n- The doc states the file API should be used if the total prompt size is over 20mb (see **https://ai.google.dev/gemini-api/docs/files**)\n\nTo solve these issues, I use the Files API and manage (on my end) when to upload a file vs when to get it from cache (the File objects themselves are lightweight). I can replace all the images in my prompt with File objects, which speeds up my tests significantly, because I upload the files once, then can test multiple times during the time the files are cached.\n\nAs I understand it, the File API is not supported. Is there a way I could achieve something similar (e.g. pass the File objects somehow)?\n\nThanks\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "dprov",
      "author_type": "User",
      "created_at": "2025-06-17T13:07:03Z",
      "updated_at": "2025-06-17T19:19:39Z",
      "closed_at": "2025-06-17T15:27:12Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2005/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/2005",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/2005",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:04.494005",
      "comments": [
        {
          "author": "DouweM",
          "body": "@dprov From https://ai.google.dev/gemini-api/docs/files#rest, it looks like uploading a file gives you a file URI and you're expected to pass it to the API as a `file_data` dict with a `file_uri` key. \n\nCan you try using `ImageUrl` from https://ai.pydantic.dev/input/#image-input?\n\nThat's passed to G",
          "created_at": "2025-06-17T15:21:22Z"
        },
        {
          "author": "DouweM",
          "body": "I'll close this assuming that'll work!",
          "created_at": "2025-06-17T15:27:12Z"
        },
        {
          "author": "dprov",
          "body": "Thanks @DouweM, I'll try when I get a chance and report back.",
          "created_at": "2025-06-17T19:19:29Z"
        }
      ]
    },
    {
      "issue_number": 1993,
      "title": "LLM misinterprets \"Plain text responses are not permitted\" message as coming from the user",
      "body": "### Description\n\n# Simple currently working usecase\n\nI am running tool, my LLM responds with plaintext and gets user response:\n```\nPlain text responses are not permitted, please include your response in a tool call\n\nFix the errors and try again.\n```\nLLM retries, responds with the correct tool call and everything is fine.\n\n\n# Problem\n\n\nThe problem happens, when the LLM assumes, that this message is from \"user\" as the rest of chat history (like initial chat message), then it gets confused about the \"user\" wanting 2 conflicting things. It doesn't understand, that this is not a \"real user message\", but just a validator feedback.\n\nWhen I customized the library and modified the feedback text to:\n```\nValidator feedback (not from the user):\nPlain text responses are not permitted, please include your response in a tool call\n\nFix the errors and try again.\n```\n\nSuddenly LLM was able to figure out what is happening and my flow works correctly.\n\n\n# Example\n\n\nExample output, where LLM gets confused and stack between sending plaintext responses and calling the same tool over and over again incorrectly:\n\n```\n<think> Okay, the user wants to connect to a human representative but hasn't provided their email or phone number. The assistant tried to ask for them, but the user's previous message was a tool call that failed because the required info was missing. Now, the user is reminding us to fix the errors and try again.\n\nThe problem is that the assistant can't make a tool call without the required info. So the assistant needs to prompt the user to provide their email or phone number. The user's current message is asking for plain text responses to be included in a tool call, but the assistant can't do that without the necessary info.\n\nThe assistant should respond by asking the user to provide their email or phone number so that it can proceed with connecting them. This way, the user's information is collected, and the tool call can be made once the required data is available.\n\nThe assistant should maintain a polite and helpful tone, ensuring the user understands that they need to provide the necessary details for the connection to happen. Once the user provides the info, the assistant can call the connect_to_human function with the available data. </think>\n\nI'm sorry, but I can't connect you to a human representative right now because I don't have your email or phone number. Could you please provide those details so I can forward your request accordingly?\n```\n\n\n# Solution\n\n\nI want to be flexible and to use wording that aligns as much as possible with what my agent is trying to achieve and minimize anything confusing, therefore I need to be able to modify these texts (also think about i18n).\n\nI think for now I need to make configurable just this line:\n```python\n#messages.py\nreturn f'{description}\\n\\nFix the errors and try again.'\n```\nBut I may end up wanting to modify even specific tool validation messages in `_agent_graph.py` like \"Plain text responses are not permitted, please include your response in a tool call\" mentioned above.\n\nWould be good to do this on per-tool basis rather than global or per-agent. Not sure how to approach this exactly as I am not that much familiar with the library yet. Happy to implement this and make PR, just need some direction to make sure I take the right approach and have good chances of doing it well and merging.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "hovi",
      "author_type": "User",
      "created_at": "2025-06-16T11:59:21Z",
      "updated_at": "2025-06-17T15:36:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1993/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1993",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1993",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:04.770070",
      "comments": [
        {
          "author": "DouweM",
          "body": "@hovi Can you please share what model you're using, and how you're configuring your agent's output type? Some minimal reproducible code would be useful. Models typically don't run into this error because if they're not allowed to use plain text response, we tell them they _have_ to use a tool, so I'",
          "created_at": "2025-06-17T00:27:31Z"
        },
        {
          "author": "hovi",
          "body": "Hi @DouweM \nThis gist is fairly stable and fails 95% of the time after 5 retries, the model used is `qwen3:1.7b`, running locally. I included 2 sample run logs without and with patched `RetryPromptPart` that adds `Validator feedback:\\n` prefix to the user message.\nhttps://gist.github.com/hovi/085915",
          "created_at": "2025-06-17T10:56:27Z"
        },
        {
          "author": "DouweM",
          "body": "@hovi It seems like Qwen is not respecting the [`tool_choice = 'required'` param](https://github.com/pydantic/pydantic-ai/blob/388ecc2db91877b8e1915b5843234142fcb6743d/pydantic_ai_slim/pydantic_ai/models/openai.py#L278), so it's depending exclusively on the prompt guidance.\n\nStill, reading the model",
          "created_at": "2025-06-17T14:51:52Z"
        },
        {
          "author": "lanyuer",
          "body": "I've also encountered this issue several times, but I haven't compiled a dedicated set of reproduction cases yet.\n",
          "created_at": "2025-06-17T15:36:24Z"
        }
      ]
    },
    {
      "issue_number": 1989,
      "title": "call_tool returns un-recoverable McpError; should surface a ModelRetry so the agent can self-correct",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n### Summary\nWhen a pydantic_ai agent is connected to an MCP provider (e.g., GitHub) and the underlying HTTP request fails with a 4xx/5xx, the library raises mcp.shared.exceptions.McpError.\nBecause this exception is not converted into a ModelRetry, the agent cannot inspect the failure, adjust its arguments, or choose a different tool—it simply crashes the entire run.\n\n### Stack trace (truncated)\n````\nTraceback (most recent call last):\n  ...\n  File \"/usr/local/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 758, in run_tool\n    result = await server.call_tool(tool_name, args)\n  File \"/usr/local/lib/python3.11/site-packages/pydantic_ai/mcp.py\", line 130, in call_tool\n    result = await self._client.call_tool(\n  File \"/usr/local/lib/python3.11/site-packages/mcp/client/session.py\", line 264, in call_tool\n    return await self.send_request(\n  File \"/usr/local/lib/python3.11/site-packages/mcp/shared/session.py\", line 286, in send_request\n    raise McpError(response_or_error.error)\nmcp.shared.exceptions.McpError: failed to list branches: GET https://api.github.com/repos/spryx-mcp-api-github/spryx-mcp-api-github/branches?page=1&per_page=30: 404 Not Found []\n````\n\n## Steps to Reproduce\n1.\tConfigure an agent that uses the GitHub MCP client (pydantic_ai.mcp.GitHubMcp or similar).\n2.\tInvoke a tool (e.g., list_branches) with an invalid repository slug, or any other input that causes GitHub to return 404 or 422.\n3.\tObserve that the agent run terminates with an uncaught McpError.\n\n## Expected Behavior\n\t•\tThe error should be wrapped in—or converted to—a ModelRetry (or a tool-level FunctionCallError) so the LLM can:\n\t•\tread the failure message,\n\t•\trewrite its arguments, or\n\t•\tchoose an alternate tool.\n\nThis would align with the existing retry semantics used when the model itself produces invalid JSON.\n\n## Actual Behavior\n\t•\tMcpError bubbles up unhandled, aborting the entire agent run and forcing the host application to implement ad-hoc retry logic outside the library.\n\n## Possible Solution\nat pydantic_ai_slim/pydantic_ai/mcp.py func `call_tool`\n````\n    async def call_tool(\n        self, tool_name: str, arguments: dict[str, Any]\n    ) -> (\n        str\n        | BinaryContent\n        | dict[str, Any]\n        | list[Any]\n        | Sequence[str | BinaryContent | dict[str, Any] | list[Any]]\n    ):\n        \"\"\"Call a tool on the server.\n\n        Args:\n            tool_name: The name of the tool to call.\n            arguments: The arguments to pass to the tool.\n\n        Returns:\n            The result of the tool call.\n\n        Raises:\n            ModelRetry: If the tool call fails.\n        \"\"\"\n        try:\n            result = await self._client.call_tool(\n                self.get_unprefixed_tool_name(tool_name), arguments\n            )\n        except McpError as e:\n            raise ModelRetry(e.error.message)\n\n        content = [self._map_tool_result_part(part) for part in result.content]\n\n        if result.isError:\n            text = \"\\n\".join(str(part) for part in content)\n            raise ModelRetry(text)\n\n        if len(content) == 1:\n            return content[0]\n        return content\n````\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython = 3.11\npydantic_ai = v0.2.18\n```",
      "state": "closed",
      "author": "ppcantidio",
      "author_type": "User",
      "created_at": "2025-06-15T21:14:49Z",
      "updated_at": "2025-06-17T14:36:36Z",
      "closed_at": "2025-06-17T14:36:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1989/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1989",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1989",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:04.993256",
      "comments": [
        {
          "author": "DouweM",
          "body": "@ppcantidio The solution you've suggested looks good, can you please submit a PR?",
          "created_at": "2025-06-16T23:56:15Z"
        },
        {
          "author": "ppcantidio",
          "body": "> @ppcantidio The solution you've suggested looks good, can you please submit a PR?\n\nThank u, i will do that",
          "created_at": "2025-06-16T23:58:27Z"
        }
      ]
    },
    {
      "issue_number": 1774,
      "title": "Please move fastA2A to another module so it can have it's own release cycle",
      "body": "I would very much like to contribute to A2A but it seems it doesn't have a good release cycle due to the size of this repo. Please move it to it's own repo so it's easier to contribute to. As the mod of /r/AgentToAgent I think this library is going to make A2A mainstream and I want to help it get there!",
      "state": "open",
      "author": "robert-at-pretension-io",
      "author_type": "User",
      "created_at": "2025-05-19T23:13:52Z",
      "updated_at": "2025-06-17T14:20:27Z",
      "closed_at": null,
      "labels": [
        "a2a"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1774/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1774",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1774",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:05.290655",
      "comments": [
        {
          "author": "Kludex",
          "body": "Okay. I'll move it.",
          "created_at": "2025-05-20T10:40:31Z"
        },
        {
          "author": "robert-at-pretension-io",
          "body": "@Kludex , I'm a huge fan. Please let me know how I can help with this project? Any task that will push it forward, I'm down.\n\nI'm not just a vibe coder, I've been a professional SWE for 6 years. ",
          "created_at": "2025-05-20T12:59:35Z"
        },
        {
          "author": "tobegit3hub",
          "body": "Any update for this? ",
          "created_at": "2025-06-09T22:45:42Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Is this really necessary?",
          "created_at": "2025-06-17T14:20:27Z"
        }
      ]
    },
    {
      "issue_number": 1928,
      "title": "Is a stream supposed to timeout, in case timeout model setting is defined?",
      "body": "### Question\n\nThe setting is not working when streaming a response.\nSo my question is if this is by design, since it's weird to timeout a stream mid-way, or a mal-function\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-06-05T15:48:07Z",
      "updated_at": "2025-06-17T14:00:45Z",
      "closed_at": "2025-06-17T14:00:45Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1928/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1928",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1928",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:05.603439",
      "comments": [
        {
          "author": "DouweM",
          "body": "@pedroallenrevez The `timeout` option on `ModelSettings` is not used by Pydantic AI directly, but is passed directly to the model SDK, so it's up to the model SDK to implement it correctly. Can you share what model you're using? Some example code would be useful.",
          "created_at": "2025-06-06T22:17:11Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-06-14T14:00:28Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-06-17T14:00:45Z"
        }
      ]
    },
    {
      "issue_number": 1942,
      "title": "Expose method to map messages to model provider format",
      "body": "### Description\n\nCurrently there is no way to convert messages to OpenAI/Anthropic model format. I see that there are methods available for but it's protected in model classes.\n\nThis will allow easier migration to Pydantic AI as people usually store and use messages in OpenAI/Anthropic model format.\n\nAlso if possible it will be great to have a way to that allows you to do opposite as well.\n\n### References\n\n- [Anthropic `_map_messages`](https://github.com/pydantic/pydantic-ai/blob/f58ec533394bbb45af3c941297eff7686d7e3abc/pydantic_ai_slim/pydantic_ai/models/anthropic.py#L286)\n- [OpenAI `_map_messages`](https://github.com/pydantic/pydantic-ai/blob/f58ec533394bbb45af3c941297eff7686d7e3abc/pydantic_ai_slim/pydantic_ai/models/openai.py#L370)",
      "state": "open",
      "author": "slmnsh",
      "author_type": "User",
      "created_at": "2025-06-08T13:37:17Z",
      "updated_at": "2025-06-17T13:44:41Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1942/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1942",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1942",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:05.829505",
      "comments": [
        {
          "author": "Kludex",
          "body": "Is your point that we should make those public functions? ",
          "created_at": "2025-06-08T19:08:31Z"
        },
        {
          "author": "slmnsh",
          "body": "Yes, and maybe add functions to map messages from specific model format to Pydantic messages format. It will make it easy to migrate to PydanticAI.",
          "created_at": "2025-06-08T20:19:15Z"
        },
        {
          "author": "Kludex",
          "body": "@slmnsh The links you provide are actually from PydanticAI to OpenAI and Anthropic, I guess you want the opposite?",
          "created_at": "2025-06-16T16:18:55Z"
        },
        {
          "author": "slmnsh",
          "body": "I want both:\n\n- Pydantic -> OpenAI (make `_map_messages` method public and easily accessible via class method)\n- OpenAI -> Pydantic\n",
          "created_at": "2025-06-16T20:29:22Z"
        },
        {
          "author": "anuar12",
          "body": "We have a custom implementation now, but would gladly migrate to PydanticAI, it's just our data model/schema is custom now ",
          "created_at": "2025-06-17T13:44:41Z"
        }
      ]
    },
    {
      "issue_number": 1985,
      "title": "Pre-validation tool response modifications",
      "body": "### Question\n\nI'm encountering an issue where the model consistently returns a specific but incorrect JSON format when the expected output is a Pydantic BaseModel. This behavior is very consistent, leading to ValidationErrors and unnecessary retries.\n\nExample BaseModel:\n\n```\nclass ResearchOutline(BaseModel):\n    section_titles: list[str]\n    target_audience: str\n```\n\nExample JSON received (to final_result tool):\n\n\n```\nargs = {\n    \"section\" = { \"type\": \"list\", \"value\": [\"Frog Habitat\", \"Natural Predators\"]},\n    \"target_audience\" = {\"type\": \"string\", \"value\": \"General Audience\"}\n}\n```\n\nCurrent Workaround:\nMy current solution is to change the agent's return type to a tool call instead of the BaseModel directly. This tool takes an input argument of `BaseModel | dict`, and inside the tool, if the input is a dict, I manually process the dictionary and create the BaseModel instance. Example below:\n\n```\ndef output_function(outline: ResearchOutline | dict[str, Any]):\n    if isinstance(outline, dict);\n        outline = transform_to_base_model(outline)\n    return outline\n```\n\nWhile this avoids the ValidationError, it has significant downsides:\n**Code Bloat**: I have to create a separate tool for almost every agent that returns a BaseModel, which adds a lot of boilerplate code.\n**Model Confusion**: I'm concerned that adding this extra argument to the tool's description might confuse the model in the future, making it harder to understand what it's supposed to do. Especially since it's possible the model will see the dict parameter and just decide to return any dict it wants.\n\nProposed Solution (for discussion):\nI'm looking for a cleaner way to handle this situation. I'm opening this issue to:\n\n- Determine if others are experiencing similar problems.\n- Discuss potential solutions or workarounds that are less intrusive than my current approach.\n- Get feedback on whether this is a bug in pydantic-ai or an issue with the underlying language model.\n\nI would appreciate any guidance or suggestions on how to best address this issue.\n\n### Additional Context\n\nPython 3.11\nPydantic-ai 0.2.16\n\n(Editted using Gemini)",
      "state": "open",
      "author": "alex-rowden",
      "author_type": "User",
      "created_at": "2025-06-14T10:41:08Z",
      "updated_at": "2025-06-17T00:45:59Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1985/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1985",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1985",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:06.091226",
      "comments": [
        {
          "author": "DouweM",
          "body": "@alex-rowden What model are you using? Can you share an MRE? It sounds like the model is not following the JSON schema correctly.",
          "created_at": "2025-06-16T21:30:15Z"
        },
        {
          "author": "alex-rowden",
          "body": "I don't have an MRE but I can say it seems to only happen when handling a lot of data. I'm using Llama 3.3 70b",
          "created_at": "2025-06-17T00:35:17Z"
        },
        {
          "author": "DouweM",
          "body": "@alex-rowden What model and provider class are you using? ",
          "created_at": "2025-06-17T00:45:59Z"
        }
      ]
    },
    {
      "issue_number": 1997,
      "title": "How to stream events?",
      "body": "### Question\n\nLangchain has [`astream_events`](https://python.langchain.com/docs/how_to/streaming/#using-stream-events) that allows more control over event. which is really helpful for streaming tool calls.\n\nDo we have anything like that?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "slmnsh",
      "author_type": "User",
      "created_at": "2025-06-16T20:45:56Z",
      "updated_at": "2025-06-16T21:37:01Z",
      "closed_at": "2025-06-16T21:37:01Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1997/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1997",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1997",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:08.327342",
      "comments": [
        {
          "author": "DouweM",
          "body": "@slmnsh I think you're looking for https://ai.pydantic.dev/agents/#streaming.",
          "created_at": "2025-06-16T21:37:01Z"
        }
      ]
    },
    {
      "issue_number": 1988,
      "title": "How to interact with SSE MCP server without checking of 'origin'",
      "body": "### Question\n\nI got MCP server and pydanticAI agent in kuber cluster\n\nMCP server is running on 0.0.0.0:3001 with CORS disabled.\n\nAgent tries to connect with it by URL `http://mcp-server:3001` and fails with CORS error:\n\n```shell\n17:24:28.317 process message 2025-06-12 17:24:28,406 - httpx - INFO - HTTP Request: GET http://mcp-server:3001/sse .HTTP/1.1 24545 OK. \n2025-06-12 17:24:28,407 - mcp.client.sse - ERROR - Endpoint origin does not match connection origin: http://0.0.0.0:3001/messag e?sessionId6.58h7del6-6480-4052-aBh-40c1h4d34d90 \n2025-06-12 17:24:28,467 - mcp.client.sse - ERROR - Error in sse_reader: Endpoint origin does not match connection origin: http: //0.0.0.0:3001/messageIsessionId6.58h7del6-6480-4052-aBh-40c1b4d34d90 \n\n```\n\nI expect that the problem is on agent side. Is it possible to switch off this CORS checking in pydanticAI? \nOr can you advice something to make it works?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "palandlom",
      "author_type": "User",
      "created_at": "2025-06-15T17:15:33Z",
      "updated_at": "2025-06-16T21:30:29Z",
      "closed_at": "2025-06-16T21:30:29Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1988/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1988",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1988",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:08.548587",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1371\" (90% similar)",
          "created_at": "2025-06-15T17:20:08Z"
        },
        {
          "author": "Kludex",
          "body": "This is on the underlying package: https://github.com/modelcontextprotocol/python-sdk",
          "created_at": "2025-06-16T07:07:24Z"
        }
      ]
    },
    {
      "issue_number": 1983,
      "title": "Load balancing",
      "body": "### Description\n\nI am running into trouble with rate limits in some scenarios. I would love to see a feature in pydantic ai where I can provide the agent with (multiple) models/fallback models, so the agent could then use the other models if rate limits are reached.\n\nRight now, the only way to achieve this is to define multiple agents, give each a model and wrap them all over the system prompts and tools, which is already really messy and then try catch the rate limits and balance the load elsewhere.\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "TimW9",
      "author_type": "User",
      "created_at": "2025-06-14T05:58:30Z",
      "updated_at": "2025-06-16T21:27:13Z",
      "closed_at": "2025-06-16T21:27:12Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1983/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1983",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1983",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:08.816794",
      "comments": [
        {
          "author": "aristideubertas",
          "body": "There is a Fallback model https://ai.pydantic.dev/api/models/fallback/",
          "created_at": "2025-06-16T08:11:33Z"
        },
        {
          "author": "DouweM",
          "body": "@TimW9 The FallbackModel @aristideubertas pointed out is the recommended way to go. There's also people working on handling rate limits directly: https://github.com/pydantic/pydantic-ai/pull/1734. I'll close the issue considering the question answered, but let me know if anything's still unclear.",
          "created_at": "2025-06-16T21:27:13Z"
        }
      ]
    },
    {
      "issue_number": 1981,
      "title": "Add ability to load conversation history within specified date ranges in CLI",
      "body": "### Description\n\nIn CLI, if program exits with an unhandled exception, chat history will gone away. When we start CLI, if we can remember last conversations it would be nice.\n\nFor example:\n\nagent.to_cli_sync(remember_last=600) # remembers last 10 mins\n\nA new entry to CLI (remember=600):\n>> pyai> what is my name?\n>> Your name is Mert. # retrieved this from last conversations in last 600 seconds\n\nMy Approach:\n* Use JSON structure to store prompt history (or parse prompts & dates manually by RegEx)\n* Retrieve the prompts between specified date\n* Include them to CLI message history\n\n### References\n\n_No response_",
      "state": "open",
      "author": "fswair",
      "author_type": "User",
      "created_at": "2025-06-13T18:46:31Z",
      "updated_at": "2025-06-16T21:25:52Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1981/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1981",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1981",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:09.093946",
      "comments": []
    },
    {
      "issue_number": 1596,
      "title": "Add hook to modify messages before they're sent to LLM",
      "body": "### Description\n\nIn a long agent run, the context can keep growing until the limit is hit. It should be possible to intercept the messages and prune or summarize them.\n\nThis could be a hook that works with any agent run, and/or support for passing a modified `message_history` when iterating over an agent's graph explicitly (https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph).\n\n### References\n\nAs discussed in https://pydanticlogfire.slack.com/archives/C083V7PMHHA/p1745467478925729.",
      "state": "closed",
      "author": "DouweM",
      "author_type": "User",
      "created_at": "2025-04-25T15:20:22Z",
      "updated_at": "2025-06-16T16:14:36Z",
      "closed_at": "2025-06-16T16:14:36Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1596/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1596",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1596",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:09.093975",
      "comments": [
        {
          "author": "dhimmel",
          "body": "- related https://github.com/pydantic/pydantic-ai/issues/1335",
          "created_at": "2025-05-07T19:45:29Z"
        },
        {
          "author": "Wh1isper",
          "body": "Is it possible for `tool` to support this as well? I see that there is now a snapshot of all_messages, but the changes don't take effect. I'd like LLM to be able to call the summarization tool autonomously to compress the context, which would be more natural than rules. \n\nThe problem might be the po",
          "created_at": "2025-05-10T14:04:41Z"
        }
      ]
    },
    {
      "issue_number": 1335,
      "title": "summarise message_history",
      "body": "### Description\n\nI would like to have the ability to pass a summary of `result.all_messages()` rather than whole set of data including tool calls. This can be done manually. but its nice if its available a function to summarise the message history via another LLM call.\n\n### References\n\nhttps://ai.pydantic.dev/message-history/",
      "state": "closed",
      "author": "yrangana",
      "author_type": "User",
      "created_at": "2025-04-02T00:46:03Z",
      "updated_at": "2025-06-16T16:14:36Z",
      "closed_at": "2025-06-16T16:14:36Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1335/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1335",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1335",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:09.309842",
      "comments": [
        {
          "author": "Kludex",
          "body": "I'm not sure if this fits into the library itself.\n\n---\n\nEDIT (17/04/2025): Actually, I see many people asking this... It may be a good feature.",
          "created_at": "2025-04-02T08:26:24Z"
        },
        {
          "author": "AlexEnrique",
          "body": "> @AlexEnrique is this the mechanism you have? You summarize the history?\n\n@Kludex, answering your question from #619, yes, I summarize the message history after some messages, in order to control the growth of the message history / costs.\n\nAfter testing some conversations with Chat GPT, I've notice",
          "created_at": "2025-04-17T17:53:50Z"
        }
      ]
    },
    {
      "issue_number": 1901,
      "title": "Collapsing older message history to avoid blowing up the context size",
      "body": "### Question\n\nHi! I am working on something similar to a SWE agent and am facing an issue where the context size blows up (and the API costs quickly become high), since the various updates and corresponding versions of the files are kept in the message history. There is a strategy described here: https://proceedings.neurips.cc/paper_files/paper/2024/hash/5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference.html, where older messages are collapsed to avoid this and focus on the recent updates. Would it be possible to do something similar with Pydantic AI?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "eiriksteen",
      "author_type": "User",
      "created_at": "2025-06-03T14:44:14Z",
      "updated_at": "2025-06-16T16:14:35Z",
      "closed_at": "2025-06-16T16:14:35Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 20,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1901/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1901",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1901",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:09.543547",
      "comments": [
        {
          "author": "Kludex",
          "body": "Hey @eiriksteen , sorry the delay!\n\nWe don't offer a \"collapsable\" strategy. How would you see us doing that?\n\n\nMaybe something like this?\n\n```py\n\nresult = agent.run_sync()\n\ndef collapse(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"This callback receives the list of messages and perfo",
          "created_at": "2025-06-05T12:35:01Z"
        },
        {
          "author": "webcoderz",
          "body": "One thing I did here, is I started setting limits to messages passed to context, this helped a bit, but what would be extra cool is instead of doing something simple like last n messages.. do some sort of efficient message search and do the top n messages , maybe out of scope but would add a nice bi",
          "created_at": "2025-06-07T02:40:29Z"
        },
        {
          "author": "eiriksteen",
          "body": "I think that having a collapse function like that would be cool. However, for my use case, I would want to apply it during the agent run and not on the result after the run has finished. I imagine there could be plenty of usecases, but for this SWE-agent example I want the most recent message to alw",
          "created_at": "2025-06-07T08:52:26Z"
        },
        {
          "author": "AlexEnrique",
          "body": "Isn't this the same as message history summarization (see #1335)?",
          "created_at": "2025-06-12T05:51:00Z"
        },
        {
          "author": "Kludex",
          "body": "> Isn't this the same as message history summarization (see [#1335](https://github.com/pydantic/pydantic-ai/issues/1335))?\n\nThere is more specific, but pretty much.\n\n---\n\n@AlexEnrique what do you think about the `collapse` callback? Or do you propose a different API?",
          "created_at": "2025-06-12T07:07:15Z"
        }
      ]
    },
    {
      "issue_number": 582,
      "title": "Structured outputs as an alternative to Tool Calling",
      "body": "**Issue Description:**  \r\n\r\nCurrently, `pydantic-ai` implements structured output solely using tool-calling APIs from model providers. While this works in most cases, certain schemas supported by `pydantic` exhibit inconsistencies between model providers.  \r\n\r\nFor instance, the following schema from the [documentation](https://ai.pydantic.dev/results/?h=userprofile#streaming-structured-responses) does not work with Gemini models:  \r\n\r\n```python\r\nclass UserProfile(TypedDict, total=False):\r\n    name: str\r\n    dob: date\r\n    bio: str\r\n\r\nagent = Agent(\r\n    'gemini-2.0-flash-exp',\r\n    result_type=UserProfile,\r\n)\r\nagent.run_sync(\"Generate a synthetic data\")\r\n```\r\n\r\nThis results in the following error:  \r\n\r\n```python\r\nUnexpectedModelBehavior: Unexpected response from gemini 400, body:\r\n{\r\n  \"error\": {\r\n    \"code\": 400,\r\n    \"message\": \"* GenerateContentRequest.tools[0].function_declarations[0].parameters.properties[dob].format: only 'enum' is supported for STRING type\\n\",\r\n    \"status\": \"INVALID_ARGUMENT\"\r\n  }\r\n}\r\n```  \r\n\r\nIn this example, the inconsistency stems from the model provider's limitations. However, based on my observations working with tools like `instructor`, modern LLMs are increasingly proficient at adhering to JSON-format prompts in their text responses. In fact, they often perform better in terms of json content in standard completion modes than in tool-calling modes. The [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) may provide further evidence of this trend.  \r\n\r\n### Feature Request  \r\n\r\nWould it be possible for `pydantic-ai` to implement an alternative mode akin to `instructor`'s [`MD_JSON`](https://python.useinstructor.com/concepts/patching/?h=json#markdown-json-mode) mode? This mode could use prompt engineering to guide the LLM’s output and parse the resulting JSON as raw text rather than relying on tool-calling APIs.  \r\n\r\nSuch a feature would:  \r\n\r\n- Allow broader compatibility with any model capable of following JSON schema prompts.  \r\n- Address model-specific inconsistencies while leveraging `pydantic`'s full schema flexibility.  \r\n\r\nThank you for considering this suggestion!\r\n",
      "state": "open",
      "author": "lazyhope",
      "author_type": "User",
      "created_at": "2025-01-01T17:29:35Z",
      "updated_at": "2025-06-16T12:52:23Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 29,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/582/reactions",
        "total_count": 71,
        "+1": 47,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 16,
        "eyes": 8
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/582",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/582",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:09.783397",
      "comments": []
    },
    {
      "issue_number": 1959,
      "title": "Handle unhandled exceptions gracefully in CLI",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAfter some time, once the prompt is invoked, if the model returns corrupted JSON or an unexpected EOF, it causes errors during execution and the program exits due to unhandled exceptions.\n\nMy questions:\n\n> Can we make exiting the program on agent run failure optional by catching errors? (makes optional program exit on failure)\n\n> Also, when I start the CLI chat again, the agent doesn’t remember the previous conversations. At the very least, can it include prompts from the last 5 minutes?\n\n### Example Code\n\n```Python\n/opt/homebrew/lib/python3.11/site-packages/pydantic_ai/messages.py:536 in args_as_dict           \n                                                                                               \n533 │   │   │   return {}                                                                      \n534 │   │   if isinstance(self.args, dict):                                                   \n535 │   │   │   return self.args                                                               \n❱ 536   │     │   args = pydantic_core.from_json(self.args)                                    \n537 │   │   assert isinstance(args, dict), 'args should be a dict'                      \n538 │   │   return cast(dict[str, Any], args)                                                \n539                                                                                            \n                                                                                              \nValueError: EOF while parsing an object at line 1 column 40\n\nModel is not important. (its Sonnet 4) I want just invoke a new prompt after exception (should be catched when fails)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.7\nPydanticAI 0.2.16\nClaude-Sonnet-4-20250514\n```",
      "state": "open",
      "author": "fswair",
      "author_type": "User",
      "created_at": "2025-06-11T11:23:38Z",
      "updated_at": "2025-06-16T12:51:15Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1959/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1959",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1959",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:09.783435",
      "comments": [
        {
          "author": "fswair",
          "body": "Example corrupted data:\n\n```json\n{\"dir_path\": \"Chatty\"}\n{\"file_path\": \"Chatty/package.json\", \"content\": \"{\\n  \\\"name\\\": \\\"chatty\\\",\\n  \\\"version\\\": \\\"1.0.0\\\",\\n  \\\"description\\\": \n\\\"Anonymous chat web application\\\",\\n  \\\"main\\\": \\\"server.js\\\",\\n  \\\"scripts\\\": {\\n    \\\"start\\\": \\\"node server.js\\\",\\n ",
          "created_at": "2025-06-11T11:38:54Z"
        },
        {
          "author": "DouweM",
          "body": "@fswair It looks like the model is giving you an invalid JSON string for tool call args, and then Pydantic AI is failing when it tries to send those args back as a parsed JSON object. \n\nWe can do 2 things:\n1. Don't try to send back parsed JSON if it's found to invalid, just send the string we receiv",
          "created_at": "2025-06-11T17:46:45Z"
        },
        {
          "author": "fswair",
          "body": "I have basic CRUD tools and too basic prompt that is \"Create a full-stack anonymous chat web application named Chatty.\"\n\nModel is Sonnet 4 .\n\nSecond way is my request, yes!\n\n> Please file a new issue for reusing previous conversation message history in a new CLI run!\n\nSure, will do. Thanks @DouweM \n",
          "created_at": "2025-06-11T18:05:44Z"
        },
        {
          "author": "fswair",
          "body": "I mean file operation tools",
          "created_at": "2025-06-11T18:08:08Z"
        },
        {
          "author": "DouweM",
          "body": "@fswair Thanks for the additional context.\n\nI agree we should handle exceptions in the CLI gracefully (point 2 in my previous comment), but I'm also trying to figure out the source of the error you're running into. \n\nI assumed that we received the tool args as an (invalid JSON) string and then tried",
          "created_at": "2025-06-12T21:19:35Z"
        }
      ]
    },
    {
      "issue_number": 1974,
      "title": "Pydantic AI V1",
      "body": "As discussed previously, we want `pydantic-ai` V1!\n\nWe had said we would release V1 in June, I think it's now going to be July!",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-06-13T14:18:25Z",
      "updated_at": "2025-06-16T08:48:04Z",
      "closed_at": null,
      "labels": [
        "meta"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1974/reactions",
        "total_count": 8,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 4,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1974",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1974",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:10.024157",
      "comments": [
        {
          "author": "aristideubertas",
          "body": "Any plans to natively support Thinking/Reasoning parts and configurations?",
          "created_at": "2025-06-16T08:12:55Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Good idea. I've added #907 as as subissue.",
          "created_at": "2025-06-16T08:48:04Z"
        }
      ]
    },
    {
      "issue_number": 1926,
      "title": "Feature Request: Multi-Model Cost Calculation in Agent Delegation",
      "body": "### Description\n\nImplement a hierarchical usage tracking system that:\n\n- Preserves model-specific cost data throughout delegation chains\n- Aggregates costs across different models\n- Maintains compatibility with existing UsageLimits\n\n### References\n\nhttps://ai.pydantic.dev/multi-agent-applications/",
      "state": "open",
      "author": "bitnom",
      "author_type": "User",
      "created_at": "2025-06-05T14:51:40Z",
      "updated_at": "2025-06-16T07:46:48Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1926/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1926",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1926",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:10.289373",
      "comments": [
        {
          "author": "jimmyhealer",
          "body": "Also very interested in this feature, are there any plans for it on the roadmap?",
          "created_at": "2025-06-16T07:46:47Z"
        }
      ]
    },
    {
      "issue_number": 1896,
      "title": "How to break agent.iter () ?",
      "body": "### Question\n\nHow to break agent.iter()  at any node and any part phase?\n\n### Additional Context\n\nI have an interactive agent communicating with the user. At any time user should be able to interrupt the current agent iter() loop and run a new loop with the new prompt. In order to do this I need to interupt current agent.iter() at any node/part generation",
      "state": "closed",
      "author": "alexey2baranov",
      "author_type": "User",
      "created_at": "2025-06-03T12:49:52Z",
      "updated_at": "2025-06-15T14:00:30Z",
      "closed_at": "2025-06-15T14:00:30Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1896/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1896",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1896",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:10.484205",
      "comments": [
        {
          "author": "DouweM",
          "body": "@alexey2baranov Have you tried using the `break` keyword from inside teh `async with agent.iter` / `async for node`? This seems to work, but please let me know if there's a reason it doesn't work in your exact scenario:\n\n```py\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.agent impo",
          "created_at": "2025-06-03T16:33:06Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "I also recommend returning, because you will get these `generator didn't stop errors`",
          "created_at": "2025-06-03T16:44:08Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-06-11T14:00:32Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-06-15T14:00:29Z"
        }
      ]
    },
    {
      "issue_number": 1971,
      "title": "Quota Warning with OpenAI",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHello Team,\n\nI have been trying to create a simple agent and I get this error of Quota warning for 3 different OpenAI account I used! Can you please help? I also used Azure deployment route with AzureProvider but facing the same issue. I tried to run exapmples also but the same issue!\n\n**Error**\n\n```\n  raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\npydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: gpt-4o, body: {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\n```\n\nAppreciate your help!\n\n### Example Code\n\n```Python\nimport nest_asyncio\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, ModelRetry, RunContext, Tool\nfrom pydantic_ai.models.openai import OpenAIModel\n\nnest_asyncio.apply()\n\nmodel = OpenAIModel(\"gpt-4o\")\n\n# --------------------------------------------------------------\n# 1. Simple Agent - Hello World Example\n# --------------------------------------------------------------\n\"\"\"\nThis example demonstrates the basic usage of PydanticAI agents.\nKey concepts:\n- Creating a basic agent with a system prompt\n- Running synchronous queries\n- Accessing response data, message history, and costs\n\"\"\"\n\nagent1 = Agent(\n    model=model,\n    system_prompt=\"You are a helpful customer support agent. Be concise and friendly.\",\n)\n\n# Example usage of basic agent\nresponse = agent1.run_sync(\"How can I track my order #12345?\")\nprint(response.data)\nprint(response.all_messages())\nprint(response.cost())\n\n\nresponse2 = agent1.run_sync(\n    user_prompt=\"What was my previous question?\",\n    message_history=response.new_messages(),\n)\nprint(response2.data)\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13.2\nPydantic 2.11.6\nOpenAI GPT-4o\n```",
      "state": "closed",
      "author": "milapshah15",
      "author_type": "User",
      "created_at": "2025-06-13T11:37:55Z",
      "updated_at": "2025-06-13T17:03:11Z",
      "closed_at": "2025-06-13T17:03:11Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1971/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1971",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1971",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:10.713444",
      "comments": [
        {
          "author": "DouweM",
          "body": "@milapshah15 Did you follow the instructions in the error message to check our OpenAI quota? This looks very much like an issue with your OpenAI account(s), not something specific to Pydantic AI. ",
          "created_at": "2025-06-13T17:03:11Z"
        }
      ]
    },
    {
      "issue_number": 1969,
      "title": "Extract All Tool Calls with Nested Agents",
      "body": "### Question\n\nWhen running a single agent, we can extract the executed tool calls for the current run like this:\n```\nresult = agent.run_sync(\"just a test\")\nmessages = result.new_messages()\n# extract tool calls from messages by their part\n```\n\nHowever, when I have a multi-agent setup and I execute a sub-agent within my manager-agent, the manager-agent only returns the \"top-level\" tool-calls (i.e. manager agent calling the sub-agent, but not the sub-agents tool calls).\n\nFor now I keep track of every tool executed and create a ToolCallHierarchy that tells me which agent executed which tools. However, I have to do it manually by extracting the tool calls from each agent run and stitching it back together. I was wondering if there is way to do this directly with pydantic-ai (get all tool calls recursively - ideally when using `run_stream`).\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "dwinpolka",
      "author_type": "User",
      "created_at": "2025-06-12T15:19:38Z",
      "updated_at": "2025-06-13T17:00:57Z",
      "closed_at": "2025-06-13T17:00:57Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1969/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1969",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1969",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:10.972019",
      "comments": [
        {
          "author": "DouweM",
          "body": "@dwinpolka Agents don't currently have knowledge of other agents that are called from a tool or output function, so there's not currently a way to do this. We plan to have a native way to do handoffs in the near future, I've created an issue at https://github.com/pydantic/pydantic-ai/issues/1978. I'",
          "created_at": "2025-06-13T17:00:57Z"
        }
      ]
    },
    {
      "issue_number": 1967,
      "title": "Gemini 2.5 Pro Thinking Budget Not Working (possible google bug)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nself explanatory. might be a google error though\n\n### Example Code\n\n```Python\nfrom pydantic_ai.models.gemini import GeminiModelSettings, GeminiModel, ThinkingConfig\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\nfrom pydantic_ai.messages import ModelRequest\nfrom pydantic_ai.direct import model_request\n\nmodel = GeminiModel(\n    \"gemini-2.5-pro-preview-05-06\",\n    provider=GoogleVertexProvider(project_id=\"******\")\n)\n\nawait model_request(model, messages=[ModelRequest.user_text_prompt(\"hey\")], model_settings=GeminiModelSettings(gemini_thinking_config=ThinkingConfig(include_thoughts=True, thinking_budget=1000)))\n\n# {'thinking_config': {'include_thoughts': True, 'thinking_budget': 1000}}\n# {'contents': [{'role': 'user', 'parts': [{'text': 'hey'}]}], 'generationConfig': {'thinking_config': {'include_thoughts': True, 'thinking_budget': 1000}}}\n\nModelHTTPError: status_code: 400, model_name: gemini-2.5-pro-preview-05-06, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unable to submit request because Thinking can't be disabled for this model. Remove 'thinking_config.thinking_budget' from your request and try again.. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython==3.12\npydantic==0.2.16\n```",
      "state": "open",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-06-12T15:05:01Z",
      "updated_at": "2025-06-13T16:52:27Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1967/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1967",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1967",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:11.193802",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex Please have a look",
          "created_at": "2025-06-13T16:52:21Z"
        }
      ]
    },
    {
      "issue_number": 913,
      "title": "PydanticAI Roadmap",
      "body": "People having been asking for a while for a roadmap for PydanticAI, here goes.\n\nFor the most part, I'll drink the GitHub cool ade and add the issues that represent our ordered priorities using the new sub-issue feature.\n\nIn terms of timeline, I'm terrible at predicting how long things will take, so I'm not going to dare to guess when features will land. I'll just say that PydanticAI is becoming a bigger and bigger priority for Pydantic, so we'll have a few people working full time on the library, especially once Pydantic 2.11 is released (https://github.com/pydantic/pydantic/issues/11194).\n\nFeedback here very welcome, also as per #905 I'll be in NYC and SF in the next couple of weeks, so come and say hi and demand your chosen feature in person if you can!\n\n---\n\nThe philosophy here is as follows:\n* build the things people actually want/need in production use cases\n* build the things that have to exist in the framework (e.g. #833 is a big refactor, but it allows patterns that are hard or impossible without it)\n* break APIs ASAP so we can get to a stable state ASAP\n* make the experience of using PydanticAI with [Pydantic Logfire](https://pydantic.dev/logfire) as good as it can possibly be while remaining as open and standards-based as possible, e.g. adopting the [GenAI semantic conventions for OTel](https://opentelemetry.io/docs/specs/semconv/gen-ai/) even if they're still in flux and difficult to use in places\n* Build the sugar on top when the underlying fundamentals are right, to encourage people to adopt PydanticAI early in their development even if things like type safety, native async support and test coverage don't woo them",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-02-12T20:33:19Z",
      "updated_at": "2025-06-13T16:49:57Z",
      "closed_at": "2025-06-13T14:23:49Z",
      "labels": [
        "meta"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/913/reactions",
        "total_count": 79,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 4,
        "confused": 0,
        "heart": 38,
        "rocket": 36,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/913",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/913",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:11.416470",
      "comments": [
        {
          "author": "mikeedjones",
          "body": "This is great!\n\nOne of the things we've found very important/useful, especially with deployed APIs based around LMs, is prompt management and prompt versioning separate from the code, outside version control.\n\nHand in hand with evals we're also quite keen on prompt optimisation - as (first?) impleme",
          "created_at": "2025-02-13T08:18:38Z"
        },
        {
          "author": "ebotiab",
          "body": "> This is great!\n> \n> One of the things we've found very important/useful, especially with deployed APIs based around LMs, is prompt management and prompt versioning separate from the code, outside version control.\n> \n> Hand in hand with evals we're also quite keen on prompt optimisation - as (first",
          "created_at": "2025-02-13T08:48:52Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Thanks @mikeedjones - please create a new issue, keen to discuss this.",
          "created_at": "2025-02-13T09:41:39Z"
        },
        {
          "author": "mikeedjones",
          "body": "> Thanks [@mikeedjones](https://github.com/mikeedjones) - please create a new issue, keen to discuss this.\n\nCool! have opened https://github.com/pydantic/pydantic-ai/issues/921",
          "created_at": "2025-02-13T10:29:40Z"
        },
        {
          "author": "Kludex",
          "body": "For those following this roadmap, Bedrock support landed on version 0.0.36.",
          "created_at": "2025-03-07T12:48:38Z"
        }
      ]
    },
    {
      "issue_number": 704,
      "title": "Parellel node execution in Graphs (AKA Graphs V2)",
      "body": "Follows #528.\n\nIt would be great to be able to run multiple nodes in parallel.\n\nWhile starting two nodes is easy (basically just create multiple asyncio tasks), and defining multiple nodes to run is easily (return a list of nodes instead of a single node), some other stuff is hard:\n\n* how do we manage recording state (see #695) when multiple nodes run in parallel?\n* how to we know when to start a node that relies on multiple other nodes completing?\n* currently nodes define just their outgoing edges, do we need nodes to define their incoming edges too?",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-01-16T18:10:27Z",
      "updated_at": "2025-06-13T14:26:28Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "graph"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/704/reactions",
        "total_count": 15,
        "+1": 15,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin",
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/704",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/704",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:11.628728",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "These are my preliminary thoughts, so please bear with me 😂 \n\n### do we need to define incoming edges too?\nI think so. Any node(s) that is/are a pre-condition for this current node should be registered on the graph so that we will know which nodes to wait for before we kick things off in the current",
          "created_at": "2025-01-18T14:42:47Z"
        },
        {
          "author": "YourTechBud",
          "body": "My two cents.\n\nLet me try and solve these problems:\n- How to merge state being returned for two nodes running in parallel. Especially when they need to update the same field.\n- How to handle wait states (how to we know when to start a node that relies on multiple other nodes completing?)\n\nI am going",
          "created_at": "2025-01-29T03:18:14Z"
        },
        {
          "author": "msukmanowsky",
          "body": "Was thinking about this recently in the context of concurrent execution to support fan out / fan in modes as well as how you'd pull of state persistence in that case.\n\nSide note: since at the moment things are running with asyncio, it's better to think of \"concurrent node execution in graphs\" as par",
          "created_at": "2025-02-11T23:21:48Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "Is this expected any time soon?",
          "created_at": "2025-03-12T18:51:02Z"
        },
        {
          "author": "lukebyrne",
          "body": "I would love to be able to offer any help or assistance, but due to my lack of god level Python abilities like such as @samuelcolvin has I am out of my depth.\n\nI can taste, appreciate and enjoy this delicious meal you have lovingly cooked for us, but alas I can only do the dishes at the end in servi",
          "created_at": "2025-03-18T02:09:23Z"
        }
      ]
    },
    {
      "issue_number": 1754,
      "title": "[Feature Request] Make GoogleGLAProvider Base URL Configurable",
      "body": "### Description\n\nThe current **`GoogleGLAProvider`** in pydantic-ai (as seen in [Google GLA Provider](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/providers/google_gla.py) has its `base_url` property hardcoded to `https://generativelanguage.googleapis.com/v1beta/models/`.\n\nWhile this works for direct access to the Google Generative Language API, it prevents users from easily using this provider with:\n\n- Proxy servers: Users might want to route requests through a proxy (e.g., for rate limiting, logging, or specific network configurations).\n- Self-hosted or compatible endpoints: Some users might be running local models or using third-party services that offer an API compatible with the Google GLA format but at a different base URL.\n- Regional or specific endpoints: Google might offer alternative endpoints not covered by the default.\n\nCurrently, to use a custom base URL, users are forced to create a new provider class that largely duplicates the GoogleGLAProvider logic but overrides the base_url property or sets the client's base URL differently.\n\nSo I'm using the code below as the my current workaround:\n\n```python\nimport os\nimport httpx\nfrom pydantic_ai.exceptions import UserError\nfrom pydantic_ai.models import cached_async_http_client\nfrom pydantic_ai.providers import Provider\n\nclass GoogleGLACompatibleProvider(Provider[httpx.AsyncClient]):\n    \"\"\"Provider for Google Generative Language AI API with configurable base URL.\"\"\"\n    @property\n    def name(self):\n        return \"google-gla-compatible\"\n\n    @property\n    def base_url(self) -> str:\n        # Ensure GEMINI_API_BASE is set\n        base = os.getenv(\"GEMINI_API_BASE\")\n        if not base:\n            # Or provide a default if no env var? Depends on desired behavior.\n            # For now, let's stick to requiring the env var based on the user's example.\n            raise UserError(\"Set the `GEMINI_API_BASE` environment variable.\")\n        # Ensure base URL ends with a slash before appending\n        return base.rstrip(\"/\") + \"/v1beta/models/\"\n\n    @property\n    def client(self) -> httpx.AsyncClient:\n        return self._client\n\n    def __init__(\n        self, api_key: str | None = None, http_client: httpx.AsyncClient | None = None\n    ) -> None:\n        \"\"\"Create a new Google GLA provider with a configurable base URL.\"\"\"\n        api_key = api_key or os.environ.get(\"GEMINI_API_KEY\")\n        if not api_key:\n            raise UserError(\n                \"Set the `GEMINI_API_KEY` environment variable or pass it via `GoogleGLACompatibleProvider(api_key=...)`\"\n                \"to use the Google GLA provider.\"\n            )\n        self._client = http_client or cached_async_http_client(\n            provider=\"google-gla-compatible\" # Using a different provider name to avoid cache conflicts\n        )\n        # Set base_url _after_ client is created\n        self._client.base_url = self.base_url  # Use the property to get the base URL\n        self._client.headers[\"X-Goog-Api-Key\"] = api_key\n```\n\nThis workaround is cumbersome and requires users to maintain their own provider class, which is not ideal.\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "ShangjinTang",
      "author_type": "User",
      "created_at": "2025-05-19T02:25:18Z",
      "updated_at": "2025-06-13T13:36:18Z",
      "closed_at": "2025-06-13T13:36:18Z",
      "labels": [
        "documentation",
        "Feature request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1754/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1754",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1754",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:11.933144",
      "comments": [
        {
          "author": "ShangjinTang",
          "body": "I propose modifying the GoogleGLAProvider class to accept an optional base_url parameter in its __init__ method. If this parameter is provided, it should override the default base_url property.\n\nFor example, the __init__ method could be updated like this:\n\n```python\nclass GoogleGLAProvider(Provider[",
          "created_at": "2025-05-19T02:27:54Z"
        },
        {
          "author": "DouweM",
          "body": "@ShangjinTang The `GoogleGLAProvider` is being replaced with a new `GoogleProvider` in  https://github.com/pydantic/pydantic-ai/pull/1373. Once that lands, this'd be a good addition!",
          "created_at": "2025-05-19T22:15:31Z"
        },
        {
          "author": "pixelsoccupied",
          "body": "Hi there, just ran into this issue as well (was looking to use cloudflare ai gateway) \n\nI think we can potentially improve this without adding any additional param with simple check just before this [line](https://github.com/pydantic/pydantic-ai/blob/a7cb6d5fcc795e49cbf16cea3c7b3030b1d8112f/pydantic",
          "created_at": "2025-05-20T01:19:30Z"
        },
        {
          "author": "ShangjinTang",
          "body": "> Hi there, just ran into this issue as well (was looking to use cloudflare ai gateway)\n> \n> I think we can potentially improve this without adding any additional param with simple check just before this [line](https://github.com/pydantic/pydantic-ai/blob/a7cb6d5fcc795e49cbf16cea3c7b3030b1d8112f/pyd",
          "created_at": "2025-05-20T07:50:40Z"
        },
        {
          "author": "Kludex",
          "body": "With the `GoogleModel` it's possible to pass the `base_url` via `google.Client`: https://ai.pydantic.dev/models/google/#provider-argument\n\nIs that enough? Maybe we should document?",
          "created_at": "2025-05-20T16:16:48Z"
        }
      ]
    },
    {
      "issue_number": 1966,
      "title": "Bedrock: Running agent without tools fails when using messages from an agent with tools",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm encountering an issue when transferring messages from one agent to another using  Bedrock model.\n\nThe scenario works fine in two cases:\n\n* When neither agent uses tools.\n* When both agents use tools.\n\nHowever, it fails when I pass messages from an agent that uses tools to another that does not.\n\nThis behavior only occurs when using a Bedrock model. If I use `openai:gpt-4`, the issue does not happen, and the conversation works as expected.\n\n### Expected behavior:\nThe receiving agent without tools should be able to continue the conversation using messages from an agent with tools, regardless of tool usage history.\n\n### Actual behavior:\nAn error occurs or the conversation fails when messages from a tool-using agent are passed to a non-tool agent under Bedrock.\n\nModel:\n\n* Working: openai:gpt-4\n* Failing: Bedrock model\n\n```\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the Converse operation: The toolConfig field must be defined when using toolUse and toolResult content blocks.\n```\n\n### Example Code\n\n```Python\nimport logfire\nfrom dotenv import load_dotenv\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider\n\nload_dotenv()\nlogfire.configure(send_to_logfire=\"if-token-present\")\nlogfire.instrument_pydantic_ai()\n\nmodel = BedrockConverseModel(\n    model_name=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    provider=BedrockProvider(\n        region_name=\"us-west-2\",\n    ),\n)\n\nmodel_with_tools = Agent(model=model, output_type=bool) # This one uses tools for the output\nmodel_without_tools = Agent(model=model, output_type=str)\n\n\nasync def main():\n    response = await model_with_tools.run(\"Are you a LLM?\")\n    messages = response.new_messages()\n    response = await model_without_tools.run(\"Create a poem\", message_history=messages)\n    print(response.output)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.12.7\npydantic-ai: 0.2.16\n```",
      "state": "open",
      "author": "felipemonroy",
      "author_type": "User",
      "created_at": "2025-06-12T14:05:08Z",
      "updated_at": "2025-06-13T11:03:57Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1966/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1966",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1966",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:12.179694",
      "comments": [
        {
          "author": "Kludex",
          "body": "@leandrodamascena Do you know if this is an issue with Bedrock or if it's supposed to work like this?",
          "created_at": "2025-06-13T08:22:29Z"
        },
        {
          "author": "leandrodamascena",
          "body": "Hi @Kludex, let me inspect the request to see how pydantic-ai is working in this case. In theory it should work, but let me investigate a bit more to see the whole api request.\n\nThanks for tagging me.",
          "created_at": "2025-06-13T08:52:45Z"
        },
        {
          "author": "leandrodamascena",
          "body": "Ok, I figured out what is going on on the Bedrock side and probably need you help to understand more about pydantic-ai workflow. Let me break down: \n\n**First Bedrock Call**:\n\nWhen making this first call to the Bedrock API, the model is able to understand the question and respond with the expected an",
          "created_at": "2025-06-13T09:49:56Z"
        },
        {
          "author": "Kludex",
          "body": "From what I understood, what happens is the following:\n\n1. The user defines an agent with X tools.\n2. The user calls the LLM with those tools in tool config.\n3. The user uses the message history that contains the used tools from the previous execution on the new agent.\n3.1. This new agent doesn't ha",
          "created_at": "2025-06-13T10:32:34Z"
        },
        {
          "author": "leandrodamascena",
          "body": "> From what I understood, what happens is the following:\n> \n> 1. The user defines an agent with X tools.\n> 2. The user calls the LLM with those tools in tool config.\n\nI'm using the example sent by @felipemonroy. He is not defining any explicit tools, but Pydantic-ai defines the `final_result` tool b",
          "created_at": "2025-06-13T11:03:57Z"
        }
      ]
    },
    {
      "issue_number": 1216,
      "title": "Does Pydantic AI Support Image Inputs with Detail Settings?",
      "body": "Hi!\n\nI'm working on a project where I need to send image inputs to models like GPT-4o and specify the detail level (low or high). Is there currently a way to set the desired detail level when sending image inputs?\n\nThanks for your help!\n\nBest regards.",
      "state": "open",
      "author": "bastiansg",
      "author_type": "User",
      "created_at": "2025-03-23T21:10:18Z",
      "updated_at": "2025-06-13T10:52:54Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1216/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1216",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1216",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:17.411812",
      "comments": [
        {
          "author": "Kludex",
          "body": "Hi @bastiansg ,\n\nNot yet. I think we need a mechanism to include provider-specific metadata for `AudioUrl`, `ImageUrl`, etc.",
          "created_at": "2025-04-09T15:52:39Z"
        },
        {
          "author": "GilesStrong",
          "body": "+1 on this.\nFor OpenAI models, the detail is hardcoded to 'auto', and I am running into an issue in which that this is being mapped to low-detail mode, when in fact I need high-detail.\nI would be happy to submit a PR, however it would likely not be provider-agnostic...",
          "created_at": "2025-06-11T04:34:05Z"
        },
        {
          "author": "Kludex",
          "body": "@GilesStrong You can propose an API. We can ignore the detail in providers that don't have it. Do you know or can you check which providers support it?",
          "created_at": "2025-06-13T10:52:54Z"
        }
      ]
    },
    {
      "issue_number": 1632,
      "title": "Streamable HTTP Implementaion",
      "body": "### Question\n\nNow that the [Streamable HTTP](https://github.com/modelcontextprotocol/modelcontextprotocol/pull/206) is merged on the MCP repo. When will this be implemented on pydantic ai? Currently only SSE + HTTP is supported, when will Streamable HTTP come?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "rusenbb",
      "author_type": "User",
      "created_at": "2025-05-02T15:04:32Z",
      "updated_at": "2025-06-13T08:42:44Z",
      "closed_at": "2025-06-13T08:42:44Z",
      "labels": [
        "Feature request",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1632/reactions",
        "total_count": 8,
        "+1": 8,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1632",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1632",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:17.670613",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-10T14:00:22Z"
        },
        {
          "author": "jeffreymp17",
          "body": "is there a plan to added it to the future Pydantic AI versions ?",
          "created_at": "2025-05-13T19:16:17Z"
        },
        {
          "author": "BrandonShar",
          "body": "Just put up #1716 for this! It's working well for me locally.",
          "created_at": "2025-05-13T21:05:44Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-22T14:00:32Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-25T14:00:35Z"
        }
      ]
    },
    {
      "issue_number": 1572,
      "title": "Race conditions with concurrent Vertex AI agents",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHi team,\n\nI am experiencing a race condition because of the way Vertex AI is used inside PydanticAI.\n\nAs per the [Gemini section](https://ai.pydantic.dev/models/gemini/#configuration_1) of the docs,\n\nBecause google.auth.default() requires network requests and can be slow, it's not run until you call agent.run().\n\nThis causes a race condition when running two or more agents concurrently and the agents fail the authentication to Vertex AI.\n\n```\n\"/Users/luisbolanostrujillo/Dragonfly/Repositories/dragonfly-prototype-2/.venv/lib/python3.13/site-packages/pydantic_ai/providers/google_vertex.py\", line 169, in _refresh_token\n    assert isinstance(self.credentials.token, str), f'Expected token to be a string, got {self.credentials.token}'  # type: ignore[reportUnknownMemberType]\n           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: Expected token to be a string, got None\n```\n\nBecause it is a race condition, it does not happen 100% of the times with two agents, but the below code reproduces the error if ran multiple times\n\n### Example Code\n\n```Python\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\n\n\nasync def main():\n    model = GeminiModel(\"gemini-2.0-flash\", provider=\"google-vertex\")\n    agent = Agent(model)\n\n    task1 = asyncio.create_task(agent.run(\"What is the capital of Spain?\"))\n    task2 = asyncio.create_task(agent.run(\"What is the capital of France?\"))\n\n    responses = await asyncio.gather(task1, task2)\n\n    print(\"Response 1:\", responses[0])\n    print(\"Response 2:\", responses[1])\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nLooking at your source code, I managed to circumvent the problem by refreshing the token in advanced.\n\n```python\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\n\n\nasync def main():\n    provider = GoogleVertexProvider()\n    provider.client._auth.credentials = await provider.client._auth._get_credentials()\n    await provider.client._auth._refresh_token()\n\n    model = GeminiModel(\"gemini-2.0-flash\", provider=provider)\n    agent = Agent(model)\n\n    task1 = asyncio.create_task(agent.run(\"What is the capital of Spain?\"))\n    task2 = asyncio.create_task(agent.run(\"What is the capital of France?\"))\n\n    responses = await asyncio.gather(task1, task2)\n\n    print(\"Response 1:\", responses[0])\n    print(\"Response 2:\", responses[1])\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic==2.11.3\npydantic-ai==0.1.3\ngoogle-auth==2.39.0\n```",
      "state": "closed",
      "author": "ldbolanos",
      "author_type": "User",
      "created_at": "2025-04-23T16:36:48Z",
      "updated_at": "2025-06-13T08:23:27Z",
      "closed_at": "2025-06-13T08:23:27Z",
      "labels": [
        "bug",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1572/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1572",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1572",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:18.000303",
      "comments": [
        {
          "author": "Kludex",
          "body": "We need to fix this. I'm moving the implementation to use `genai.Client`, so it should be fixed in some days.",
          "created_at": "2025-04-25T09:08:58Z"
        },
        {
          "author": "Kludex",
          "body": "The `GoogleModel`/`GoogleProvider` solves this issue:\n\n```py\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\n\n\nasync def main():\n    model = GoogleModel('gemini-2.0-flash', provider='google-vertex')\n    agent = Agent(model)\n\n    task1 = asyncio.create_",
          "created_at": "2025-05-20T16:14:29Z"
        }
      ]
    },
    {
      "issue_number": 1943,
      "title": "Extra `ModelRequest` message when using only `message_history`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen you try to generate with just `message_history`, The result contains one additional message with no parts and just instructions.\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessagesTypeAdapter\n\nagent = Agent(\"openai:gpt-4o-mini\", instructions=\"You are an Agent\")\n\nmessages = [\n    {\n        \"parts\": [\n            {\n                \"content\": \"How are you?\",\n                \"timestamp\": \"2025-06-08T14:27:04.870961Z\",\n                \"part_kind\": \"user-prompt\",\n            }\n        ],\n        \"instructions\": \"You are an Agent\",\n        \"kind\": \"request\",\n    },\n]\n\nhistory = ModelMessagesTypeAdapter.validate_python(messages)\n\nresult = agent.run_sync(message_history=history)\n\nprint(result.all_messages_json())\n\n# [\n#     {\n#         \"parts\": [\n#             {\n#                 \"content\": \"How are you?\",\n#                 \"timestamp\": \"2025-06-08T14:27:04.870961Z\",\n#                 \"part_kind\": \"user-prompt\",\n#             }\n#         ],\n#         \"instructions\": \"You are an Agent\",\n#         \"kind\": \"request\",\n#     },\n#     {\"parts\": [], \"instructions\": \"You are an Agent\", \"kind\": \"request\"},\n#     {\n#         \"parts\": [\n#             {\n#                 \"content\": \"I'm just a program, but thanks for asking! How can I assist you today?\",\n#                 \"part_kind\": \"text\",\n#             }\n#         ],\n#         \"model_name\": \"gpt-4o-mini-2024-07-18\",\n#         \"timestamp\": \"2025-06-08T14:39:00Z\",\n#         \"kind\": \"response\",\n#     },\n# ]\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13.2\nPydantic AI >=0.2.14\n```",
      "state": "closed",
      "author": "slmnsh",
      "author_type": "User",
      "created_at": "2025-06-08T14:43:24Z",
      "updated_at": "2025-06-12T21:53:05Z",
      "closed_at": "2025-06-12T21:53:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1943/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1943",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1943",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:18.242145",
      "comments": [
        {
          "author": "slmnsh",
          "body": "I think the issue is we are always creating new `start_node`. We should check if `user_prompt` is passed or not and create `start_node` based on that.\n\nref: https://github.com/pydantic/pydantic-ai/blob/78e006c8c7b1e087d57c50dacb00587b46fbdac5/pydantic_ai_slim/pydantic_ai/agent.py#L699-L706",
          "created_at": "2025-06-08T14:49:48Z"
        },
        {
          "author": "DouweM",
          "body": "@slmnsh Thanks for the report, I've implemented a fix in https://github.com/pydantic/pydantic-ai/issues/1943.",
          "created_at": "2025-06-10T19:00:09Z"
        }
      ]
    },
    {
      "issue_number": 1872,
      "title": "Feature request: Pass metadata to MCP server tool calls",
      "body": "### Description\n\nProposed amendment to the runtool function within _agent_graph.py\n\n```py\nmodel_args = args  # Parameters generated by large language models (LLMs)\nclient_deps = ctx.deps  # Locally passed client parameters\nmerged_arguments = {**model_args, **client_deps}  # type: ignore\nresult = await server.call_tool(tool_name, merged_arguments)  # type: ignore\n```\n\nKey Improvements\n\n1. ​​Parameter Merging Mechanism​​\n\nCombines LLM-generated arguments (model_args) with local client dependencies (ctx.deps) using dictionary unpacking\n\n2. ​​Precedence Rule​​\n\nImplements override logic where client_deps values take priority over conflicting model_args\n\n​​3. Context Preservation​​\nMaintains type safety through # type: ignore annotations while enabling local parameter propagation to MCP server\nTechnical Rationale\n\n**This modification addresses the current limitation where:**\n\n MCP servers receive only LLM-generated parameters\n\nLocal context variables (ctx.deps) remain inaccessible at server-side\nClient-specific configurations (auth tokens, environment variables) require workarounds\n\nThe merged dictionary approach aligns with Pydantic-AI's dependency injection system while maintaining MCP protocol compliance\n\n. Implementation would enhance scenarios like:\n\n```py\nctx.deps = {\"api_key\": \"SECRET_123\"} \n```\n\nServer-side: Directly access merged_arguments['api_key']\nInstead of separate auth handling[10](@ref)\n\n### References\n\n_No response_",
      "state": "open",
      "author": "alex-panther",
      "author_type": "User",
      "created_at": "2025-05-30T09:28:48Z",
      "updated_at": "2025-06-12T21:51:59Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1872/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1872",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1872",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:18.461914",
      "comments": [
        {
          "author": "DouweM",
          "body": "@alex-panther I think it'd be unexpected to both users and MCP servers if we unconditionally passed all of the deps. Note also that deps may not be serializable.\n\nI can see the value of being able to modify the arguments sent to the MCP server, though, where \"modify\" can also mean \"add\". We already ",
          "created_at": "2025-06-02T21:43:40Z"
        },
        {
          "author": "alex-panther",
          "body": "@DouweM Yes, it would be great if the new hook allows users to pass custom parameters from the ctx to the MCP server.",
          "created_at": "2025-06-03T07:01:23Z"
        },
        {
          "author": "DouweM",
          "body": "@alex-panther OK. Would you be up to submitting a PR for this?",
          "created_at": "2025-06-06T18:18:46Z"
        }
      ]
    },
    {
      "issue_number": 1968,
      "title": "token usage metrics in InstrumentModel does not work with Ollama",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nClaude:\nI've examined your implementation, and I can see why your usage metrics are returning as null when using the Ollama model. The issue is related to how token usage is being extracted from the Ollama model response.\n\nLooking at your PydanticAIAgent.run method, you're trying to extract usage metrics this way:\n\npython\nif hasattr(self, 'model') and self.model and hasattr(self.model, 'usage') and self.model.usage:\nusage = self.model.usage\nprompt_tokens = getattr(usage, 'prompt_tokens', None)\ncompletion_tokens = getattr(usage, 'completion_tokens', None)\ntotal_tokens = getattr(usage, 'total_tokens', None)\nThe problem is that Ollama models don't expose token usage in the same way as OpenAI models. The Ollama API doesn't return token usage by default, or might structure it differently than the OpenAI API that your code seems to be expecting.\n\nBased on the Pydantic PR you linked (https://github.com/pydantic/pydantic-ai/pull/1898,) this is a new feature, and it looks like it's not fully implemented for all model types.\n\nHow to fix it:\nCheck how you're initializing the Ollama model. Looking at the code, you're initializing Ollama through the OpenAI provider:\npython\nlogger.info(f\"Initializing Ollama model '{config.model}' via OpenAIModel and OpenAIProvider.\")\nImplement a wrapper or adapter for Ollama responses that explicitly captures and formats token usage. You may need to modify your Ollama model initialization to capture usage stats.\nCheck for Ollama-specific usage stats: The Ollama client might return usage stats in a different form. For example, it could be accessed through a different property name or structure.\n\nhttps://github.com/ollama/ollama/blob/45f56355d557b7130c7c07bbd6e1b634a758d946/llm/server.go#L724\ntype CompletionResponse struct {\nContent string json:\"content\"\nDoneReason DoneReason json:\"done_reason\"\nDone bool json:\"done\"\nPromptEvalCount int json:\"prompt_eval_count\"\nPromptEvalDuration time.Duration json:\"prompt_eval_duration\"\nEvalCount int json:\"eval_count\"\nEvalDuration time.Duration json:\"eval_duration\"\n}\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython:3.12-slim\npydantic-ai:0.2.17\nollama: 0.9.0\ndevstral:latest\n```",
      "state": "open",
      "author": "georgiedekker",
      "author_type": "User",
      "created_at": "2025-06-12T15:18:01Z",
      "updated_at": "2025-06-12T15:27:49Z",
      "closed_at": null,
      "labels": [
        "need confirmation",
        "OpenTelemetry"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1968/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1968",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1968",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:18.687582",
      "comments": [
        {
          "author": "alexmojaki",
          "body": "Please format this issue properly, share example code, and don't rely on Claude for everything.\n\n> Looking at your PydanticAIAgent.run method, you're trying to extract usage metrics this way:\n> \n> python if hasattr(self, 'model') and self.model and hasattr(self.model, 'usage') and self.model.usage:\n",
          "created_at": "2025-06-12T15:27:49Z"
        }
      ]
    },
    {
      "issue_number": 664,
      "title": "Gemini Agent run from cached context",
      "body": "I have a corpus that I cache using Gemini context caching.\r\n```\r\nfrom google.generativeai import caching\r\n\r\ncache = caching.CachedContent.create(\r\n    model='models/gemini-1.5-flash-001',\r\n    display_name='book_123_abc', # used to identify the cache\r\n    system_instruction=(\r\n        'Your job is to answer the user\\'s query based on the book you have access to.'\r\n    ),\r\n    contents=[md_book_123_abc]\r\n)\r\n```\r\n\r\nI want to set up a gemini agent dedicated to this cached corpus. Does Pydantic AI support this workflow?",
      "state": "closed",
      "author": "tranhoangnguyen03",
      "author_type": "User",
      "created_at": "2025-01-13T06:06:57Z",
      "updated_at": "2025-06-12T14:00:36Z",
      "closed_at": "2025-06-12T14:00:36Z",
      "labels": [
        "question",
        "more info",
        "Stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 28,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/664/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/664",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/664",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:18.959008",
      "comments": []
    },
    {
      "issue_number": 1964,
      "title": "Missing documentation for `UserPromptNode` et. al.",
      "body": "### Description\n\n`UserPromptNode` (and the other node types) are directly user-exposed if you use e.g. `agent.iter`. However, there are no API reference docs for these and thus figuring out what the different field of these classes means is largely left up to guessing/experimenting.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "tibbe",
      "author_type": "User",
      "created_at": "2025-06-12T08:53:03Z",
      "updated_at": "2025-06-12T13:47:23Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1964/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1964",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1964",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:18.959038",
      "comments": [
        {
          "author": "tibbe",
          "body": "To give an example of a question I was trying to answer: in `ModelRequest` there are both `parts` and `instructions`. How do these relate? How do they get sent to the LLM? Are they concatenated?\n\n(My original use case is trying to run `agent.iter` and then handle the various possible outputs for the",
          "created_at": "2025-06-12T08:57:47Z"
        }
      ]
    },
    {
      "issue_number": 1889,
      "title": "Structured Output Streaming Returning as Single Chunk(Bedrock Anthropic)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nStructured streaming doesn't work with Claude models when using via Bedrock.\n\nI tried using stream whales example as-is and it seems to work fine with the openai gpt model used in that example.\nWhen using a Claude model, the streaming does not work and data is sent as one chunk.\n\nCan confirm if I change output type to just a string, streaming works fine but in my use case I need structured output.\n\n\nI have seen similar issues when using Claude models directly:\n- #1665 \n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12\nPydanticAI 0.2.12\n```",
      "state": "open",
      "author": "bkwon94",
      "author_type": "User",
      "created_at": "2025-06-02T20:22:38Z",
      "updated_at": "2025-06-12T13:28:39Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1889/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1889",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1889",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:19.193799",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 2 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1665\" (95% similar)\n2. \"https://github.com/pydantic/pydantic-ai/issues/823\" (95% similar)",
          "created_at": "2025-06-02T20:30:08Z"
        },
        {
          "author": "felipemonroy",
          "body": "Confirmed using the following agent in the Stream whales example\n\n```python\nmodel = BedrockConverseModel(\n    model_name=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    provider=BedrockProvider(\n        region_name=\"us-west-2\",\n    ),\n)\n\nagent = Agent(model=model, output_type=list[Whale])\n```",
          "created_at": "2025-06-12T13:28:39Z"
        }
      ]
    },
    {
      "issue_number": 1893,
      "title": "With `uvloop`, `to_cli` breaks",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen I'm using uvloop instead of asyncio, the cli implementation has threading issues with writing buffers. I assume its from writing to message history and some sort of unsafe access.\n```\nException in thread Thread-10:\nException in threading.excepthook:\nI'd be happy to help you write! Could you let me know what you'd like to write about? For example:                 \n\n • A story or creative piece                                                                                       \n • An essay or article                                                                                             \n • A letter or email                                                                                               \n • A poem                                                                                                          \n • A script or dialogue                                                                                            \n • Technical documentation                                                                                         \n • Something else entirely          Traceback (most recent call last):\n  File \"/Users/msu/reevo/ai-101/001-basics/main.py\", line 10, in <module>\n    uvloop.run(agent.to_cli())\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/Users/msu/.local/share/mise/installs/python/3.12.8/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/msu/.local/share/mise/installs/python/3.12.8/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1775, in to_cli\n    await run_chat(stream=True, agent=self, deps=deps, console=Console(), code_theme='monokai', prog_name=prog_name)\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 254, in run_chat\n    messages = await ask_agent(agent, text, stream, console, code_theme, deps, messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 277, in ask_agent\n    with status, ExitStack() as stack:\n                 ^^^^^^^^^^^\n  File \"/Users/msu/.local/share/mise/installs/python/3.12.8/lib/python3.12/contextlib.py\", line 610, in __exit__\n    raise exc_details[1]\n  File \"/Users/msu/.local/share/mise/installs/python/3.12.8/lib/python3.12/contextlib.py\", line 595, in __exit__\n    if cb(*exc_details):\n       ^^^^^^^^^^^^^^^^\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/rich/live.py\", line 175, in __exit__\n    self.stop()\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/rich/live.py\", line 147, in stop\n    with self.console:\n         ^^^^^^^^^^^^\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/rich/console.py\", line 866, in __exit__\n    self._exit_buffer()\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/rich/console.py\", line 824, in _exit_buffer\n    self._check_buffer()\n  File \"/Users/msu/reevo/ai-101/.venv/lib/python3.12/site-packages/rich/console.py\", line 2033, in _check_buffer\n    self._write_buffer()\nI'd be happy to help you write! Could you let me know what you'd like to write about? For example:                 \n\nException in thread Thread-10:\nException in threading.excepthook:\nI'd be happy to help you write! Could you let me know what you'd like to write about? For example:\n```\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nimport uvloop\n\nagent = Agent(\n    \"anthropic:claude-4-sonnet-20250514\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\n\nuvloop.run(agent.to_cli())\n# keep conversing and it'll break at some point.\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.8\nAnthropic: anthropic:claude-4-sonnet-20250514\n\n\npydantic                                 2.11.5\npydantic-ai                              0.2.12\npydantic-ai-slim                         0.2.12\npydantic-core                            2.33.2\npydantic-evals                           0.2.12\npydantic-graph                           0.2.12\npydantic-settings                        2.9.1\n```",
      "state": "open",
      "author": "msu-reevo",
      "author_type": "User",
      "created_at": "2025-06-03T01:21:37Z",
      "updated_at": "2025-06-12T11:13:42Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1893/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1893",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1893",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:19.422326",
      "comments": [
        {
          "author": "DouweM",
          "body": "@msu-reevo Thanks for the report. I spent some time conversing with the agent using your example code, but couldn't reproduce the issue. Do you have additional detail on the type of conversation that seems to trigger this issue? Are you using long input messages, messages that generate long output, ",
          "created_at": "2025-06-03T15:43:34Z"
        },
        {
          "author": "msu-reevo",
          "body": "@DouweM Thanks for that input, I tried to narrow down how to input chat messages to reproduce.\n\nIt seems if I wait until the output is finished before typing then it won't error.\n\nHowever, If I type continuously while output is streaming and submit over and over I can reliably cause a Threading erro",
          "created_at": "2025-06-04T00:01:54Z"
        },
        {
          "author": "msu-reevo",
          "body": "@DouweM Also tried some longer prompts with Anthropic and it also breaks.\n\neg. try `how would you build software defined infra? i'd like a python DSL. think very very hard`\n\nIt's failed 4/4 times and will show a `Exception in thread Thread-2:`",
          "created_at": "2025-06-04T00:15:19Z"
        },
        {
          "author": "Kamalesh-Seervi",
          "body": "shall try this out",
          "created_at": "2025-06-12T11:13:42Z"
        }
      ]
    },
    {
      "issue_number": 1957,
      "title": "Docs: Add OAuth Integration Guide to MCP Server Documentation",
      "body": "## Problem\n\nThe Pydantic MCP server documentation doesn't cover OAuth, which is essential for production servers that need to access upstream services (Google Drive, GitHub, Slack, etc.).\n\n## Proposed Solution\n\nAdd an \"OAuth\" section with:\n\n* Link to MCP Security best practices and MCP OAuth docs in [Pydantic MCP Server docs](https://ai.pydantic.dev/mcp/server/)  \n* Guide and Working example using [Pomerium](https://github.com/pomerium/pomerium), an open source identity-aware proxy (IAP)\n* Add a call out for others to add theirs",
      "state": "open",
      "author": "nickytonline",
      "author_type": "User",
      "created_at": "2025-06-10T23:10:45Z",
      "updated_at": "2025-06-12T11:11:45Z",
      "closed_at": null,
      "labels": [
        "MCP"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1957/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1957",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1957",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:19.687109",
      "comments": [
        {
          "author": "nickytonline",
          "body": "Quick context: This proposal came out of mine and @wasaga's discussion with @Kludex at our booth at the AI Engineer World's Fair about securing agentic access.",
          "created_at": "2025-06-10T23:15:05Z"
        },
        {
          "author": "nickytonline",
          "body": "Hey @Kludex! Great meeting you last week. Happy to PR up the Pomerium guide/example if you're looking to go forward with that addition to the docs.",
          "created_at": "2025-06-11T13:23:22Z"
        },
        {
          "author": "Kamalesh-Seervi",
          "body": "can I work in this\n",
          "created_at": "2025-06-12T11:11:45Z"
        }
      ]
    },
    {
      "issue_number": 1954,
      "title": "providing `temperature` to `o3-mini-2025-01-31` results in `unsupported_parameter` error",
      "body": "### Question\n\no3-mini seems not to support the temperature parameter. I am creating an `Agent` with a `ModelSettings` object where I specify the temperature. The model/provider can vary depending on the end user.\n\nIt would be nice if there was a setting to \"drop\" unsupported parameters in the settings somehow. I would rather not have to add a check for each model, I think the benefit of using Pydantic AI is that it handles those aspects for you. \n\nIs the `ModelProfile` (#1782/#1835) that was recently introduced possibly a way to support these special \"edge\" cases?\n\n### Additional Context\n\n`pydantic-ai v0.2.16`\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: o3-mini-2025-01-31, body: {'message': \"Unsupported parameter: 'temperature' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_parameter'}\n```",
      "state": "closed",
      "author": "proever",
      "author_type": "User",
      "created_at": "2025-06-10T17:32:00Z",
      "updated_at": "2025-06-11T17:11:38Z",
      "closed_at": "2025-06-11T17:11:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1954/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1954",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1954",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:19.933324",
      "comments": [
        {
          "author": "DouweM",
          "body": "@proever Yep that's a perfect candidate for `ModelProfile`! Implemented in https://github.com/pydantic/pydantic-ai/pull/1956.",
          "created_at": "2025-06-10T20:14:54Z"
        },
        {
          "author": "proever",
          "body": "great, thanks so much for the quick reply!",
          "created_at": "2025-06-11T15:34:50Z"
        }
      ]
    },
    {
      "issue_number": 1919,
      "title": "Supporting OpenAI's Flex Processing",
      "body": "### Description\n\nOpenAI released a new service tier called flex.\nIt reduces the cost of the tokens at the expense of performance/availability.\nIs it possible to add a service_tier parameter to the OpenAIModel and the ability to control the timeout value?\n\n### References\n\n[OpenAI documentation](https://platform.openai.com/docs/guides/flex-processing?api-mode=chat)",
      "state": "closed",
      "author": "empezarcero",
      "author_type": "User",
      "created_at": "2025-06-05T09:22:17Z",
      "updated_at": "2025-06-11T12:47:27Z",
      "closed_at": "2025-06-11T12:45:46Z",
      "labels": [
        "Feature request",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1919/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1919",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1919",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:20.204630",
      "comments": [
        {
          "author": "Kludex",
          "body": "Yeah, it's possible.\n\nWe'd want to add the field `openai_service_tier` on the  `OpenAIModelSettings`. Would you like to contribute?",
          "created_at": "2025-06-05T12:01:53Z"
        },
        {
          "author": "empezarcero",
          "body": "@Kludex, I created https://github.com/pydantic/pydantic-ai/pull/1923",
          "created_at": "2025-06-05T12:58:35Z"
        },
        {
          "author": "empezarcero",
          "body": "Feature [merged](https://github.com/pydantic/pydantic-ai/commit/fe03daf36e5083a64358b5b22c34c0a5c7dfd761) to main",
          "created_at": "2025-06-11T12:45:46Z"
        }
      ]
    },
    {
      "issue_number": 1910,
      "title": "Add API reference to llms.txt",
      "body": "### Description\n\nI was wondering if it can be possible to add the api reference pages to the llms.txt documentation. I find that it can be far more precise when running coding agents for them to have direct access to the api documentation.\n\n### References\n\nhttps://ai.pydantic.dev/api/agent/",
      "state": "closed",
      "author": "rvargas42",
      "author_type": "User",
      "created_at": "2025-06-04T11:37:15Z",
      "updated_at": "2025-06-10T20:34:34Z",
      "closed_at": "2025-06-10T20:34:34Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1910/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Viicos"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1910",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1910",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:20.445554",
      "comments": [
        {
          "author": "Kludex",
          "body": "@rvargas42 I think so. We'd need to add it to this list:\n\nhttps://github.com/pydantic/pydantic-ai/blob/aeaa8cfe7586e6e9e00e27ab0f8abebcf68f9ce8/mkdocs.yml#L225-L252\n\nWould you like to contribute?",
          "created_at": "2025-06-05T12:00:18Z"
        },
        {
          "author": "rvargas42",
          "body": "Great ill make a pr asap!",
          "created_at": "2025-06-05T14:02:56Z"
        }
      ]
    },
    {
      "issue_number": 1953,
      "title": "Can't save a EvaluationReport to json and load it again",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen running an evaluation, which might take a while, you often want to dump the results to load them again.\n\nI would suspect that this would be possible using:\n```py\nreport.model_validate_json(report.model_dump_json(indent=2))\n```\n\nIt might be that I am simply not using it as intended.\n\n### Example Code\n\n```Python\n# using example from docs\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance\n\ncase1 = Case(\n    name=\"simple_case\",\n    inputs=\"What is the capital of France?\",\n    expected_output=\"Paris\",\n    metadata={\"difficulty\": \"easy\"},\n)\n\n\nclass MyEvaluator(Evaluator[str, str]):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset = Dataset(\n    cases=[case1],\n    evaluators=[IsInstance(type_name=\"str\"), MyEvaluator()],\n)\n\n\nasync def guess_city(question: str) -> str:\n    return \"Paris\"\n\n\nreport = dataset.evaluate_sync(guess_city)\nreport.print(include_input=True, include_output=True, include_durations=False)\n\n# gives an error:\nreport.model_validate_json(report.model_dump_json(indent=2))\n# TypeError: Can't instantiate abstract class Evaluator without an implementation for abstract method 'evaluate'\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-evals: 0.2.14\npython: 3.12\n```",
      "state": "open",
      "author": "KennethEnevoldsen",
      "author_type": "User",
      "created_at": "2025-06-10T12:08:11Z",
      "updated_at": "2025-06-10T19:41:11Z",
      "closed_at": null,
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1953/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1953",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1953",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:20.655144",
      "comments": [
        {
          "author": "DouweM",
          "body": "@KennethEnevoldsen Thanks for raising this. Here's some minimal code to reproduce this:\n\n```py\nfrom pydantic import TypeAdapter\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\nfrom pydantic_evals.evaluators.evaluator import EvaluationResult\n\n\nclass MyEvaluator(Evaluator[str, str])",
          "created_at": "2025-06-10T19:39:22Z"
        }
      ]
    },
    {
      "issue_number": 1941,
      "title": "A2A - generate Function Tools from A2A client via agent cards",
      "body": "### Description\n\nA2A is promising, and it's promising that Pydantic AI is implementing some capabilities in this area, but I like a path to implement a multiagent system in Pydantic AI over A2A.\n\nI think the following would help:\n1. `A2AClient` should have a reflection method: `.agent_card()` to retrieve the agent card structure.\n2. `A2AClient` should have a `.as_tool() : Tool` method to generate a Tool object that can be used with `Agent`.\n3. Maybe some dependency-injection creature comforts to help stitch this together.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "jbalonso",
      "author_type": "User",
      "created_at": "2025-06-07T20:48:24Z",
      "updated_at": "2025-06-10T18:35:43Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "a2a"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1941/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1941",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1941",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:20.870452",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex Can you please have a look here, as our resident A2A expert?",
          "created_at": "2025-06-10T18:35:43Z"
        }
      ]
    },
    {
      "issue_number": 1902,
      "title": "__init__.py in the root directory is not created for some reason",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIm getting this error: \n```\n    my_agent = Agent(\n  File \"D:\\...\\venv\\lib\\site-packages\\pydantic_ai\\agent.py\", line 282, in __init__\n    self.model = models.infer_model(model)\n  File \"D:\\...\\venv\\lib\\site-packages\\pydantic_ai\\models\\__init__.py\", line 559, in infer_model\n    from .mistral import MistralModel\n  File \"D:\\...\\venv\\lib\\site-packages\\pydantic_ai\\models\\mistral.py\", line 14, in <module>\n    from .. import ModelHTTPError, UnexpectedModelBehavior, _utils\nImportError: cannot import name 'ModelHTTPError' from 'pydantic_ai' (D:\\PycharmProjects\\MFA\\venv\\lib\\site-packages\\pydantic_ai\\__init__.py)\n```\n\nWhile trying to execute code like this:\n```py\nmy_agent = Agent(...)\n```\n\nThis error corrected with replacing this code:\n```py\nfrom .. import ModelHTTPError, UnexpectedModelBehavior, _utils\nfrom .._utils import generate_tool_call_id as _generate_tool_call_id, now_utc as _now_utc\nfrom ..messages import (\n    BinaryContent,\n    DocumentUrl,\n    ImageUrl,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    ModelResponsePart,\n    ModelResponseStreamEvent,\n    RetryPromptPart,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    VideoUrl,\n)\nfrom ..profiles import ModelProfileSpec\nfrom ..providers import Provider, infer_provider\nfrom ..settings import ModelSettings\nfrom ..tools import ToolDefinition\nfrom ..usage import Usage\n```\n(file models/mistral.py)\nto this code:\n```py\nfrom pydantic_ai.exceptions import ModelHTTPError, UnexpectedModelBehavior\nfrom pydantic_ai import _utils\nfrom pydantic_ai._utils import generate_tool_call_id as _generate_tool_call_id, now_utc as _now_utc\nfrom pydantic_ai.messages import (\n    BinaryContent,\n    DocumentUrl,\n    ImageUrl,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    ModelResponsePart,\n    ModelResponseStreamEvent,\n    RetryPromptPart,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    VideoUrl,\n)\nfrom pydantic_ai.profiles import ModelProfileSpec\nfrom pydantic_ai.providers import Provider, infer_provider\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_ai.tools import ToolDefinition\nfrom pydantic_ai.usage import Usage\n```\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai.agent import Agent\n\nmy_agent = Agent(...)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.10.11, pydantic_ai 0.2.12, mistralai 1.8.1\n```",
      "state": "open",
      "author": "TypeHintsFun",
      "author_type": "User",
      "created_at": "2025-06-03T15:59:03Z",
      "updated_at": "2025-06-10T18:29:27Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1902/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1902",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1902",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:21.072128",
      "comments": [
        {
          "author": "TypeHintsFun",
          "body": "For some reason `pydantic_ai/__init__.py` file is not exists in my installation. This is the reason for the above error",
          "created_at": "2025-06-03T18:12:35Z"
        },
        {
          "author": "TypeHintsFun",
          "body": "pydantic_ai was installed by just `pip install pydantic_ai`",
          "created_at": "2025-06-03T18:13:37Z"
        },
        {
          "author": "Rikhil-Nell",
          "body": "Hey man, I get that what I am about to ask is not helpful to you, but can you tell me how do you use toolcall and response parts? to me the userpart and textpart are intuitive, but i don't know how to get the tool call data from my agent and pass it as memory, my agents always recall the same tool m",
          "created_at": "2025-06-10T17:07:22Z"
        },
        {
          "author": "DouweM",
          "body": "@Rikhil-Nell That looks unrelated to this issue, please ask [on Slack](https://logfire.pydantic.dev/docs/join-slack/) or in a new issue.",
          "created_at": "2025-06-10T18:23:34Z"
        },
        {
          "author": "DouweM",
          "body": "@TypeHintsFun I'm afraid I can't reproduce this, I just did a clean install of `pydantic_ai` and `pydantic_ai/__init__.py` is there. Can you try deleting your venv and installing again?",
          "created_at": "2025-06-10T18:29:27Z"
        }
      ]
    },
    {
      "issue_number": 1795,
      "title": "A2A - \"message/send\" and \"message/stream\" RPC methods are not implemented and they seem to be core of A2A concept",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAccording to A2A spec - for main communication \"message/send\" and \"message/stream\" RPC methods are used (https://google.github.io/A2A/specification/#71-messagesend).\n\nin FastA2A seems that \"tasks/send\" is the main RPC method but I believe it does the right job just has confusing method name value. (https://github.com/pydantic/pydantic-ai/blob/v0.2.6/fasta2a/fasta2a/applications.py)\n\nI couldn't find any \"tasks/send\" method explanation in A2A spec (only \"tasks/get\" seems to be there).\n\nthere are more method examples in Google's demo (https://github.com/google/A2A/blob/6743038e9f789f571d7f3d9af79fbb053bb61a73/demo/ui/service/server/server.py#L64) but for most of them I couldn't find any info in spec. \n\nHowever I believe \"message/send\" and \"message/stream\" RPC methods suppose to be supported.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython : 3.13.2\npydantic(ai) : 0.2.6\nLLM - local\n```",
      "state": "open",
      "author": "benjisss",
      "author_type": "User",
      "created_at": "2025-05-21T11:55:18Z",
      "updated_at": "2025-06-09T22:42:48Z",
      "closed_at": null,
      "labels": [
        "need confirmation",
        "a2a"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1795/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1795",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1795",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:21.299237",
      "comments": [
        {
          "author": "Kludex",
          "body": "Yeah, they are just not implemented yet. \n\nYeah should support it. ",
          "created_at": "2025-05-23T08:21:04Z"
        },
        {
          "author": "ShangjinTang",
          "body": "Same issue here.\n\n```plain\npydantic_core._pydantic_core.ValidationError: 1 validation error for tagged-union[JSONRPCRequest,JSONRPCRequest,JSONRPCRequest,JSONRPCRequest,JSONRPCRequest,JSONRPCRequest]\n\nInput tag 'message/send' found using 'method' does not match any of the expected tags: 'tasks/send'",
          "created_at": "2025-05-28T04:04:05Z"
        },
        {
          "author": "tkellogg",
          "body": "Any updates here? Is this something that's being worked on? If not, I can take a whack at it",
          "created_at": "2025-06-09T16:20:43Z"
        },
        {
          "author": "tobegit3hub",
          "body": "`fasta2a` does not work for latest A2A spec.",
          "created_at": "2025-06-09T22:42:47Z"
        }
      ]
    },
    {
      "issue_number": 1888,
      "title": "Pydantic-AI warning when using 'dict' as parameter in a tool with Gemini (2.5, 2.0 Flash)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWarning when using Gemini even though there is no problem passing dictionary parameters to tools:\n\n```\npython3.13/site-packages/pydantic_ai/profiles/google.py:37: UserWarning: `additionalProperties` is not supported by Gemini; it will be removed from the tool JSON schema. Full schema: {'additionalProperties': False, 'properties': {'params': {'additionalProperties': True, 'type': 'object'}}, 'required': ['params'], 'type': 'object'}\n\nSource of additionalProperties within the full schema: {'type': 'object', 'additionalProperties': True}\n\nIf this came from a field with a type like `dict[str, MyType]`, that field will always be empty.\n\nIf Google's APIs are updated to support this properly, please create an issue on the PydanticAI GitHub and we will fix this behavior.\n  warnings.warn(\n```\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.gemini import GeminiModelSettings, ThinkingConfig\n\ndef echo(params: dict ) -> str:\n    \"\"\"Echo the params back so we know the tool was called.\n    params: dict\n    \"\"\"\n    return f\"{params=}\"\n\ndef main():\n    agent = Agent(tools=[Tool(echo)])    \n    reply = agent.run_sync(\n        model=\"gemini-2.5-flash-preview-05-20\",\n        model_settings=GeminiModelSettings(gemini_thinking_config=ThinkingConfig(thinking_budget=0)),\n        user_prompt=\"Use the echo tool to execute echo({'a':43,'b':'foo'})\", \n        )\n    print(\"LLM said:\", reply.output)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13.3\nGemini Flash 2.5, 2.0 \nPydantic 2.11.5\nPydantic-ai 0.2.12\n```",
      "state": "closed",
      "author": "devorboy",
      "author_type": "User",
      "created_at": "2025-06-02T19:32:35Z",
      "updated_at": "2025-06-09T19:45:16Z",
      "closed_at": "2025-06-09T19:45:16Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1888/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1888",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1888",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:21.585995",
      "comments": [
        {
          "author": "DouweM",
          "body": "@devorboy Getting the LLM to return an arbitrary `dict` requires a JSON schema with `type: object` and `additionalProperties: true`, but per https://ai.google.dev/gemini-api/docs/structured-output#json-schemas `additionalProperties` is not supported. If you're expecting specific keys, I recommend de",
          "created_at": "2025-06-02T22:30:04Z"
        },
        {
          "author": "devorboy",
          "body": "It does seem to work with Gemini 2.0 and 2.5 ( in a tool call, it fills the 'params' dict with proper value), but it is shaky - Only works when the output_type is 'str', for some reason, when the output type is a pydantic type the dict is always empty. Not sure why, but indeed this is not stable eno",
          "created_at": "2025-06-03T21:28:00Z"
        }
      ]
    },
    {
      "issue_number": 1237,
      "title": "Gemini unable to stream structured output",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nGemini unable to stream structured output. It just return the full structured output directly, not by chunk.\n\n### Example Code\n\n```Python\nfrom pydantic_ai.providers.google_gla import GoogleGLAProvider\nfrom pydantic_ai.models.gemini import GeminiModel, GeminiModelSettings\nfrom pydantic import BaseModel\n\nmodel = GeminiModel(\n    model_name=\"gemini-2.0-flash-001\",\n    provider=GoogleGLAProvider(api_key=config[\"GEMINI_API_KEY\"]),\n)\n\nclass Response(BaseModel):\n    sentiment: Sentiment = Field(description=\"The sentiment of the user's message.\")\n    response: str = Field(\n        description=\"Response to the user's message in Traditional Chinese (Taiwan).\"\n    )\n\nagent = Agent(\n    model,\n    result_type=Response,\n    model_settings=GeminiModelSettings(temperature=0.0),\n)\nasync with agent.run_stream(\n    user_input, message_history=message_history\n) as result:\n    previous_response = \"\"\n    \n    async for message, last in result.stream_structured(debounce_by=0.5):\n        try:\n            response = await result.validate_structured_result(\n                message, allow_partial=not last\n            )\n            print(\"response\", response)  # Response(sentiment=Sentiment(sentiment='neutral'), response='...')\n\n            if response.response:\n                # Only print the new part that wasn't in the previous response\n                new_content = response.response[len(previous_response):]\n                if new_content:\n                    print(new_content, end=\"\", flush=True)\n                previous_response = response.response\n\n        except Exception as e:\n            continue\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.11.11\nPydantic: 2.10.6\nPydantic-ai-slim: 0.0.43\n```",
      "state": "open",
      "author": "myway840731",
      "author_type": "User",
      "created_at": "2025-03-25T13:09:41Z",
      "updated_at": "2025-06-09T18:55:43Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1237/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1237",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1237",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:21.827992",
      "comments": [
        {
          "author": "monojack",
          "body": "Same. And using `result.stream()` would also print twice, but still the full output (not chunks)\n\n```python\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\n\nclass StreamingResponseModel(BaseModel):\n    response: str = Field(description=\"Response to the user\")\n    n",
          "created_at": "2025-05-09T20:29:01Z"
        },
        {
          "author": "HomemadeToast57",
          "body": "I'm experiencing this too!\n\n```python\nfrom datetime import date\nimport asyncio\n\nfrom pydantic import ValidationError\nfrom typing_extensions import TypedDict\nfrom config import settings\nfrom pydantic_ai import Agent\n\n\nclass UserProfile(TypedDict, total=False):\n    name: str\n    dob: date\n    bio: str",
          "created_at": "2025-06-09T18:54:45Z"
        }
      ]
    },
    {
      "issue_number": 1558,
      "title": "Add Resources, Prompts to MCP Client",
      "body": "### Description\n\nWould like ability to also access prompts, and resources from agents instantiated with mcp servers. \n\n### References\n\n_No response_",
      "state": "open",
      "author": "agertz7",
      "author_type": "User",
      "created_at": "2025-04-21T02:05:09Z",
      "updated_at": "2025-06-09T16:23:38Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1558/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1558",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1558",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:22.091867",
      "comments": [
        {
          "author": "shlomi-schwartz",
          "body": "I’ve implemented an MCP server that stores prompts for multiple use cases. Could you let me know if there has been any progress on adding prompt-retrieval support to the pydantic-ai client?",
          "created_at": "2025-06-04T10:00:15Z"
        },
        {
          "author": "taaha",
          "body": "bump",
          "created_at": "2025-06-09T16:23:37Z"
        }
      ]
    },
    {
      "issue_number": 1846,
      "title": "strict` mode on function call is currently not supported for grok models",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAny ideas how to work around this? Works for mistral okay.\n\n500: Unexpected error: 500: Error processing AI query: 500: Error running data analyst: status_code: 400, model_name: grok-3-mini-fast-latest, body: Invalid request content: `strict` mode on function call is currently not supported.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython version is 3.12\npydantic-ai version is 0.2.10\nLLM client is xai and model is 'grok-3-latest'(tried other models)\n```",
      "state": "closed",
      "author": "Luca-Blight",
      "author_type": "User",
      "created_at": "2025-05-28T00:12:12Z",
      "updated_at": "2025-06-07T00:26:51Z",
      "closed_at": "2025-05-28T14:26:48Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1846/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1846",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1846",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:22.404253",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Luca-Blight I'm working on fixing exactly this class of issues in https://github.com/pydantic/pydantic-ai/pull/1835 and https://github.com/pydantic/pydantic-ai/pull/1842! Can you verify you're currently doing exactly what's in https://ai.pydantic.dev/models/openai/#grok-xai, with `OpenAIModel` and ",
          "created_at": "2025-05-28T14:25:52Z"
        },
        {
          "author": "Luca-Blight",
          "body": "That's great to hear! Here's the code i'm working with, it appears correct based on the docs.\n\n```\n\t\tprovider = OpenAIProvider(\n\t\t\tapi_key=config.secrets.get(\"GROK_API_KEY\"),\n\t\t\tbase_url=\"https://api.x.ai/v1\",\n\t\t\thttp_client=httpx.AsyncClient(),\n\t\t)\n\t\tmodel = OpenAIModel(\n\t\t\tmodel_name,\n\t\t\tprovider=",
          "created_at": "2025-05-28T15:07:35Z"
        },
        {
          "author": "akarca",
          "body": "I solved it using pydantic-ai==0.2.12 and the code below @Luca-Blight \n\n```\nmodel = OpenAIModel(\n        \"grok-3\",\n        provider=OpenAIProvider(\n            api_key=\"***\",\n            base_url=\"https://api.x.ai/v1\",\n        ),\n)\nmodel.profile.openai_supports_strict_tool_definition = False  # This",
          "created_at": "2025-06-01T23:20:10Z"
        },
        {
          "author": "Luca-Blight",
          "body": "@akarca thanks for sharing! Seems like a simple bug to fix.",
          "created_at": "2025-06-02T01:53:41Z"
        },
        {
          "author": "DouweM",
          "body": "@Luca-Blight @akarca If you use the new [`GrokProvider`](https://ai.pydantic.dev/models/openai/#grok-xai) this should work automatically!",
          "created_at": "2025-06-02T21:30:30Z"
        }
      ]
    },
    {
      "issue_number": 1907,
      "title": "visualize graph in logfire",
      "body": "### Description\n\nPush graph’s mermaid code to logfire and render it there. Additionally, mermaid standard allows for defining links such that you can click on the node and it could take you to the logfire’s span for that node\n\n### References\n\nlanggraph + langsmith \n\n![Image](https://github.com/user-attachments/assets/e0b265b5-4994-4835-b775-e817e0dcb88d)",
      "state": "open",
      "author": "smallstepman",
      "author_type": "User",
      "created_at": "2025-06-04T00:26:40Z",
      "updated_at": "2025-06-06T22:47:14Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "graph"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1907/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1907",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1907",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:22.696827",
      "comments": [
        {
          "author": "DouweM",
          "body": "@smallstepman Thanks for the feature request!\n\n@dmontagu I imagine this is something we can do once the new version of pydantic_graph lands.",
          "created_at": "2025-06-06T22:47:12Z"
        }
      ]
    },
    {
      "issue_number": 1908,
      "title": "add a notion of subgraphs for mermaid viz",
      "body": "### Description\n\nWhen nesting graphs, in logfire, they are correctly laid out in span tree making them easy to browse. \n\nA nice addition would be to be able to also render them with mermaid such that top level graph is stateDiagram, and each of the subgraphs are a `state`. \n\n### References\n\nhttps://mermaid.js.org/syntax/stateDiagram.html#composite-states",
      "state": "open",
      "author": "smallstepman",
      "author_type": "User",
      "created_at": "2025-06-04T00:34:19Z",
      "updated_at": "2025-06-06T22:46:47Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "graph"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1908/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1908",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1908",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:22.923878",
      "comments": []
    },
    {
      "issue_number": 1909,
      "title": "Can you add support for ssl certificate files in MCPServerHTTP?",
      "body": "### Description\n\nCan you add support for ssl certificate files in MCPServerHTTP?\n\n### References\n\n_No response_",
      "state": "open",
      "author": "kverdecia",
      "author_type": "User",
      "created_at": "2025-06-04T04:27:20Z",
      "updated_at": "2025-06-06T22:46:05Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1909/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1909",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1909",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:22.923892",
      "comments": [
        {
          "author": "DouweM",
          "body": "@kverdecia Thanks for the feature request. \n\nWe use the official MCP SDK, so I think they'd need to support this first, which is being discussed in https://github.com/modelcontextprotocol/python-sdk/issues/870.",
          "created_at": "2025-06-06T22:46:04Z"
        }
      ]
    },
    {
      "issue_number": 1936,
      "title": "asgi_lifespan requirement is not specified in test dependencies",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHello,\n\n`asgi_lifespan` is used in both:\n* [tests/fasta2a/test_applications.py](https://github.com/pydantic/pydantic-ai/blob/dab34b7951dd60691ed1190946a4359420407d08/tests/fasta2a/test_applications.py#L7)\n* [tests/test_a2a.py](https://github.com/pydantic/pydantic-ai/blob/dab34b7951dd60691ed1190946a4359420407d08/tests/test_a2a.py#L4)\n\nBut this is not declared as a test dependency in any `pyproject.toml` file.\n\n```\nTraceback:\ntests/fasta2a/test_applications.py:7: in <module>\n    from asgi_lifespan import LifespanManager\nE   ModuleNotFoundError: No module named 'asgi_lifespan'\n```\n\n```\nTraceback:\ntests/test_a2a.py:4: in <module>\n    from asgi_lifespan import LifespanManager\nE   ModuleNotFoundError: No module named 'asgi_lifespan'\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nv0.2.14\n```",
      "state": "closed",
      "author": "rasmi",
      "author_type": "User",
      "created_at": "2025-06-06T17:35:42Z",
      "updated_at": "2025-06-06T22:28:40Z",
      "closed_at": "2025-06-06T22:26:54Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1936/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1936",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1936",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:23.148638",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex This is FastA2A so please have a look!",
          "created_at": "2025-06-06T22:10:14Z"
        },
        {
          "author": "Kludex",
          "body": "It's a dev dependency, it's listed in the `pyproject.toml`.",
          "created_at": "2025-06-06T22:26:54Z"
        },
        {
          "author": "rasmi",
          "body": "Ah, forgive me, I had used the wrong search query. I will fix my environment, thanks!",
          "created_at": "2025-06-06T22:28:39Z"
        }
      ]
    },
    {
      "issue_number": 1868,
      "title": "Unable to instantiate a Pydantic model with attributes having type UserContent, ModelMessage",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAm building a simple base model for passing agent run requests through a REST API. When I run the below reproducer I get\n\n```\npydantic.errors.PydanticUserError: `Request` is not fully defined; you should define `ImageUrl`, then call `Request.model_rebuild()`.\n```\n\n\n### Example Code\n\n```Python\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai.messages import ModelMessage, UserContent\n\n\nclass Request(BaseModel):\n    prompt: str | list[UserContent]\n    message_history: list[ModelMessage] | None = None\n\n\nf = Request(prompt=\"hi\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.13.3\npydantic==2.11.5\npydantic-ai==0.2.12\npydantic-ai-slim==0.2.12\npydantic-evals==0.2.12\npydantic-graph==0.2.12\npydantic-settings==2.9.1\npydantic_core==2.33.2\n```",
      "state": "closed",
      "author": "brau0300",
      "author_type": "User",
      "created_at": "2025-05-30T01:01:17Z",
      "updated_at": "2025-06-06T19:34:52Z",
      "closed_at": "2025-06-06T19:34:52Z",
      "labels": [
        "bug",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1868/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1868",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1868",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:23.388598",
      "comments": [
        {
          "author": "DouweM",
          "body": "@brau0300 Can you see if it works if you explicitly import all of `ImageUrl | AudioUrl | DocumentUrl | VideoUrl | BinaryContent`, which is what `UserContent` is an alias for?\n\n```py\nfrom pydantic_ai.messages import ModelMessage, UserContent, ImageUrl, AudioUrl, DocumentUrl, VideoUrl, BinaryContent\n`",
          "created_at": "2025-06-02T21:45:31Z"
        },
        {
          "author": "brau0300",
          "body": "confirmed that with the following the bug cannot be reproduced:\n```\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai.messages import ModelMessage, UserContent\nfrom pydantic_ai.messages import ModelMessage, UserContent, ImageUrl, AudioUrl, DocumentUrl, VideoUrl, BinaryContent\n\n\nclass Request(Ba",
          "created_at": "2025-06-04T18:11:10Z"
        }
      ]
    },
    {
      "issue_number": 1877,
      "title": "Requests to some models via OpenRouter fail due to response timestamp parsing error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm frequently running into a problem where requests via OpenRouter fail due to request timestamp parsing. This does not happen if I call the LLM provider directly. This also does not happen with all models. I'm running into this quite often with OpenAI's o4-mini and o3-mini models, but I have not encountered this with gpt-4.1 and gemini models.\n\nHere's the error code, it's always the same year and same line where the error happens.\n\n```python\nTraceback (most recent call last):\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 587, in use_span\n    yield span\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 260, in iter\n    yield GraphRun[StateT, DepsT, RunEndT](\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 718, in iter\n    yield agent_run\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 468, in run\n    async for _ in agent_run:\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1923, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 294, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 347, in _make_request\n    model_response = await ctx.deps.model.request(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/models/instrumented.py\", line 176, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/models/wrapper.py\", line 27, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 212, in request\n    model_response = self._process_response(response)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/clickbuddy/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 311, in _process_response\n    timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: year 57384 is out of range\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai v0.2.12\npydantic v2.11.5\npython 3.12.9\n```",
      "state": "closed",
      "author": "sepsi77",
      "author_type": "User",
      "created_at": "2025-05-31T17:56:01Z",
      "updated_at": "2025-06-06T18:25:33Z",
      "closed_at": "2025-06-06T18:25:33Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1877/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1877",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1877",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:25.454620",
      "comments": [
        {
          "author": "sepsi77",
          "body": "Here's a minimal script that produces the error for me.\n\n```python\nasync def test_pydantic_agent():\n    model_id = \"openrouter:openai/o4-mini\"\n\n    @dataclass\n    class AgentContext:\n        id: UUID\n        credits: int\n        foo: str\n\n    context = AgentContext(\n        id=uuid4(),\n        credi",
          "created_at": "2025-06-01T02:47:17Z"
        },
        {
          "author": "sepsi77",
          "body": "You don't even need the dependencies, even this produces the same error.\n\n```python\nasync def test_pydantic_agent():\n    model_id = \"openrouter:openai/o4-mini\"\n\n    agent = Agent(\n        model=model_id,\n    )\n\n    results = await agent.run(user_prompt=\"Hello, my name is Samuel\")\n    print(results)\n",
          "created_at": "2025-06-01T02:50:43Z"
        },
        {
          "author": "sepsi77",
          "body": "It seems like responses in o-series models have timestamps in milliseconds while in other cases they are in seconds. Here are the responses from `o4-mini` \n\n```python\nChatCompletion(id='resp_683bc404bf1881908be53ba8444596a8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=Chat",
          "created_at": "2025-06-01T03:27:28Z"
        },
        {
          "author": "DouweM",
          "body": "@sepsi77 Interesting, that seems like an OpenRouter bug, but one we can work around by checking if the value is much larger than expected and dividing by 1000 in that case. Would you be up for submitting a PR?",
          "created_at": "2025-06-02T21:47:49Z"
        },
        {
          "author": "sepsi77",
          "body": "@DouweM I can, I saw some other discussions while researching this that Pydantic already has some logic for checking whether a timestamp is in seconds or milliseconds. I saw @samuelcolvin talking about it here: https://github.com/pydantic/pydantic/issues/7940\n\nI assume that that logic will be more b",
          "created_at": "2025-06-03T04:02:38Z"
        }
      ]
    },
    {
      "issue_number": 1885,
      "title": "429 (Requests rate limit exceeded) status code from Mistral API are not handled anywhere",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n429 (Requests rate limit exceeded) status code from Mistral API are not handled anywhere, what makes it quite problematic to use it\n\npydantic_ai just throws arror like this:\npydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: mistral-medium-latest, body: {\"message\":\"Requests rate limit exceeded\"}\n\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12.8, pydantic_ai 0.2.12, mistralai 1.81\n```",
      "state": "closed",
      "author": "TypeHintsFun",
      "author_type": "User",
      "created_at": "2025-06-02T13:01:55Z",
      "updated_at": "2025-06-06T18:24:10Z",
      "closed_at": "2025-06-06T18:23:44Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1885/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1885",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1885",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:25.719970",
      "comments": [
        {
          "author": "TypeHintsFun",
          "body": "I just checked and found out that in pydantic_ai there is no system of trying to repeat action after errors like Rate Limit. I think, we should add it",
          "created_at": "2025-06-02T13:09:57Z"
        },
        {
          "author": "TypeHintsFun",
          "body": "Also, it will be nice to add rps, rpm, rph and other limitations like this into UsageLimits (or into some new object, maybe RateLimits)",
          "created_at": "2025-06-02T13:40:54Z"
        },
        {
          "author": "DouweM",
          "body": "@TypeHintsFun `Agent` supports a `retries` argument. Can you try increasing that from the default of 1 and see if it works?\n\nProper rate limiting support is also being implemented in https://github.com/pydantic/pydantic-ai/pull/1734.",
          "created_at": "2025-06-02T22:19:29Z"
        },
        {
          "author": "TypeHintsFun",
          "body": "@DouweM, I just checked it. No, the `retries` argument does not definitely affect 429 processing from Mistral",
          "created_at": "2025-06-03T10:18:47Z"
        },
        {
          "author": "TypeHintsFun",
          "body": "> Proper rate limiting support is also being implemented in github.com/https://github.com/pydantic/pydantic-ai/pull/1734.\n\nHope it will approved soon",
          "created_at": "2025-06-03T10:19:37Z"
        }
      ]
    },
    {
      "issue_number": 677,
      "title": "Add ability to customise model request retry behaviour",
      "body": "Currently there appears to be no easy way to customise the Model request retry behaviour, in cases of request error or unexpected response. \r\n\r\nIn one of my projects I've had to [put exception handling](https://github.com/Finndersen/chatdb/blob/main/src/chatdb/agent.py#L35) around `agent.run()` to catch request failures (e.g. due to `\"Model is overloaded\"` errors from Gemini), however this is inefficient because it re-tries the entire Agent workflow (which may involve multiple model requests).\r\n\r\nIt would be good to be able to customise the retry logic for each individual model request (`Model.request() call`), for both:\r\n- Request errors (due to network issue, rate limiting, authorisation..)\r\n- Invalid/unexpected model response content/format (to be able to prompt model to correct itself part way through a multi-step interaction)\r\n",
      "state": "closed",
      "author": "Finndersen",
      "author_type": "User",
      "created_at": "2025-01-13T21:20:18Z",
      "updated_at": "2025-06-06T18:23:33Z",
      "closed_at": "2025-06-06T18:23:33Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/677/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/677",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/677",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:25.950858",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "agreed 👍 .\r\n\r\nI will say, you could already do this by implementing your own model which wraps the Gemini model and handles retrying.\r\n\r\nYou might also be interested in #516.",
          "created_at": "2025-01-16T10:21:11Z"
        },
        {
          "author": "Finndersen",
          "body": "I'm not too familiar with all the possible failure modes of the model provider SDKs or APIs that could potentially be resolved automatically by some kind of handler. \n\nThere's obviously network or throttling related issues which could be resolved by retrying the entire request.\n\nAre there any other ",
          "created_at": "2025-01-16T13:21:01Z"
        }
      ]
    },
    {
      "issue_number": 1732,
      "title": "Easier way of configuring max_retries for OpenAI and Azure clients/providers",
      "body": "### Description\n\nOpenAI has by default a MAX_RETRIES of 2 in case of a Connection timeout.\n\nYou can configure  it in the `__init__`\n\n```\n    client = AsyncOpenAI(\n        max_retries=0,\n    )\n```\n\nIt would be good if pydantic has an easy way of passing this with either the providers API or some other mechanism, instead of us having to create the client ourselves?\n\n### References\n\nhttps://github.com/openai/openai-python/blob/main/src/openai/_constants.py",
      "state": "closed",
      "author": "vikigenius",
      "author_type": "User",
      "created_at": "2025-05-15T03:48:30Z",
      "updated_at": "2025-06-06T18:23:24Z",
      "closed_at": "2025-06-06T18:23:24Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1732/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1732",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1732",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:26.161785",
      "comments": [
        {
          "author": "DouweM",
          "body": "@vikigenius That'd be a nice property to have on `OpenAIProvider`! We'd welcome a PR.",
          "created_at": "2025-05-21T15:24:51Z"
        }
      ]
    },
    {
      "issue_number": 1927,
      "title": "Gemini Stop Sequences not working",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nSelf-explanatory\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel, GeminiModelSettings\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\n\nmodel_settings = GeminiModelSettings(\n    timeout=11,\n    stop_sequences=[\"3\", \"\\n\"]\n)\nmodel = GeminiModel('gemini-2.0-flash', provider=GoogleVertexProvider(project_id=\"****\", region=\"us-central1\"))\nagent = Agent(model, model_settings=model_settings)\n\nimport time\nstart = time.time()\ntry:\n    r = await agent.run(\"Count until 10, each number in its separate line\")\nfinally:\n    print(time.time() - start)\nprint(r)\n# AgentRunResult(output='1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n')\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython: 3.12.3\npydantic-ai: 0.2.14\n```",
      "state": "closed",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-06-05T15:29:56Z",
      "updated_at": "2025-06-06T18:14:05Z",
      "closed_at": "2025-06-06T18:14:05Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1927/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1927",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1927",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:26.392549",
      "comments": [
        {
          "author": "Kludex",
          "body": "On the way. 👍 \n\nIt's not that it was not working, but it was actually not supported. ",
          "created_at": "2025-06-06T17:01:34Z"
        }
      ]
    },
    {
      "issue_number": 1702,
      "title": "Use `click` instead of `argparse` on PydanticAI CLI",
      "body": "### Description\n\nWe want to simplify the logic we have around the CLI.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "Kludex",
      "author_type": "User",
      "created_at": "2025-05-13T09:30:49Z",
      "updated_at": "2025-06-06T06:04:51Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "refactor",
        "cli"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1702/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1702",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1702",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:26.674131",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Are we sure is this necessary?",
          "created_at": "2025-05-14T01:38:17Z"
        },
        {
          "author": "Kludex",
          "body": "> Are we sure is this necessary?\n\nWe wanted to add `clai serve [--protocol]` which would serve an agent as an A2A server, but also default `clai` to the prompt command. To do this without `click` means to parse the arguments twice, and on each step decide what to do.\n\nAlso, I previously explained th",
          "created_at": "2025-05-21T07:27:19Z"
        },
        {
          "author": "ajanitshimanga",
          "body": "Hi @Kludex ! I'd like to pick up this issue. Is this an issue I can pick up as an entry point into further contributions to the PydanticAI ecosystem? If not, could you point me to what you believe would be.\n\nEdit: In the meantime, I'll be picking this up and put out an initial PR for review and we c",
          "created_at": "2025-05-29T02:55:17Z"
        }
      ]
    },
    {
      "issue_number": 1891,
      "title": "Allow dynamic header creation for MCP Servers - this can be useful for authorization headers",
      "body": "### Description\n\n# Problem\nToday, when initializing [`pydantic_ai.mcp.MCPServerHTTP`](https://ai.pydantic.dev/api/mcp/#pydantic_ai.mcp.MCPServerHTTP), it is possible to provide [static headers](https://ai.pydantic.dev/api/mcp/#pydantic_ai.mcp.MCPServerHTTP.headers) that will be sent along with each request:\n```python\nfrom pydantic_ai.mcp import MCPServerHTTP\n\nmcp_server = MCPServerHTTP(\"http://localhost:8090/mcp\", headers={\"Authorization\": \"Bearer wow-much-secret\"})\n```\n\nThis is fine if your headers are always static (or if your bearer token is static). However, for some cases you might want to dynamically generate the headers before each request.\n\n**Example:** You want your authorization server (Auth0 or Azure AD) to provide you with an access token that should be sent with that request. You cannot generate that token in a static manner because the tokens expire after some time and would need to be refreshed (or renewed). \n\n# Workaround\n\nCurrently, there is a work-around for this, so it is not a blocking issue. \n\nYou create the `MCPServerHTTP` object just before connecting to the MCP server and modify the [`._mcp_servers` ](https://github.com/pydantic/pydantic-ai/blob/v0.2.12/pydantic_ai_slim/pydantic_ai/agent.py#L154) attribute of the agent:\n```python\n# agent.py\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom .constants import AGENT_MODEL_NAME, AGENT_MODEL_URL\n\n\nmodel = OpenAIModel(\n    model_name=AGENT_MODEL_NAME, provider=OpenAIProvider(base_url=AGENT_MODEL_URL)\n)\nmy_agent = Agent(\n    model=model,\n    deps_type=dict[str, str],\n)\n```\n```python\n# main.py\nfrom .agent import my_agent\nfrom .auth import get_token # Custom code\nfrom pydantic_ai.mcp import MCPServerHTTP\n\nasync def something() -> None:\n    mcp_server = MCPServerHTTP(\"http://localhost:8090/mcp\", headers={\"Authorization\": get_token()})\n    # The line below is the ugly part :-(\n    my_agent._mcp_servers = [MCPServerHTTP(url=MCP_SERVER_URL, headers=headers)]\n    async with my_agent.run_mcp_servers():\n       log.info(\"Connected to the MCP Server. Running agent...\")\n       ... # Do something\n```\n\nNot sure how the API should be for allowing dynamically created headers, though. \n\n### References\n\n_No response_",
      "state": "open",
      "author": "RRRajput",
      "author_type": "User",
      "created_at": "2025-06-02T21:13:40Z",
      "updated_at": "2025-06-05T21:27:02Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1891/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1891",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1891",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:26.901200",
      "comments": [
        {
          "author": "whvneo",
          "body": "With the current release of mcp python-sdk you can provide your custom httpx_client_factory.\n\nFor this we need just small changes in pydantic-ai\n\nI created a PR for this but was struggling a bit with the typecheck but the essential commit is [PR 1932](https://github.com/pydantic/pydantic-ai/pull/193",
          "created_at": "2025-06-05T21:27:00Z"
        }
      ]
    },
    {
      "issue_number": 1813,
      "title": "Got \"TypeError: unsupported operand type(s) for +: 'int' and 'list'\" from usage calculator",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI ran into `TypeError: unsupported operand type(s) for +: 'int' and 'list'` when using gemini via GoogleModel with Gemini API.\n\nTrace:\n```\n...\n  File \"...\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 337, in _make_request\n    return self._finish_handling(ctx, model_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 361, in _finish_handling\n    ctx.state.usage.incr(response.usage)\n  File \"...\\.venv\\Lib\\site-packages\\pydantic_ai\\usage.py\", line 48, in incr\n    self.details[key] = self.details.get(key, 0) + value\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~\nTypeError: unsupported operand type(s) for +: 'int' and 'list'\n```\n\nAfter some digging, I found that this error is prone to happen when there are repetitive requests, and the `incr_usage.details` value for each request is:\n- First request (success): `{'thoughts_token_count': 70}`\n- Second request (still success): `{'thoughts_token_count': 68}`\n- Thrid request (error): `{'cache_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>, 'token_count': 3950}], 'cached_content_token_count': 3950, 'thoughts_token_count': 50}`\n\nIt's clear that `cache_tokens_details` field is causing the problem, and this might be a Google problem, but some additional edge case handling is nice though.\n\n### Example Code\n\n```Python\nfrom __future__ import annotations as _annotations\nimport os\nimport asyncio\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\nfrom dotenv import load_dotenv\nimport logfire\n\nload_dotenv()\nlogfire.configure(send_to_logfire=False)\n\nprovider = GoogleProvider(api_key=os.getenv(\"GOOGLE_GEMINI_API_KEY\"))\n\nmodel = GoogleModel(model_name=\"gemini-2.5-flash-preview-05-20\", provider=provider)\n\n\nclass Test(BaseModel):\n    output: str\n\nemail_writer_agent = Agent(\n    model=model,\n    output_type=Test,\n    instructions=f\"\"\"\nHere is a pretty long prompt to illustrate the usage of the Google Gemini API. You only need to response with a simple \"okay\" and nothing else. Ignore any non-sense below.\n\n{\"The old lantern shop stood on a street no one quite remembered the name of, tucked between a towering, glass-fronted bakery that smelled perpetually of burnt sugar and a silent, dusty bookstore. Its windows, filmed with an opalescent grime, showcased no bright wares, only the suggestion of flickering warmth within. Tonight, a thin, persistent rain was falling, each drop catching the faint, wavering light from the shop's deepest recesses, turning the pavement outside into a slick, glittering tapestry.\" * 50}\n\"\"\",\n    retries=1,\n)\n\nasync def main():\n    trials = 5\n    for i in range(trials):\n\n        result = await email_writer_agent.run(\"\"\"Hi\"\"\")\n        print(result.output)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\nIf you still not be able to replicate the problem, try increasing the string multiplier.\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.0\n\npydantic                                 2.11.4\npydantic-ai-slim                         0.2.6\npydantic_core                            2.33.2\npydantic-graph                           0.2.6\ngoogle-genai                             1.16.1\n```",
      "state": "closed",
      "author": "TrickyWhiteCat",
      "author_type": "User",
      "created_at": "2025-05-23T02:03:10Z",
      "updated_at": "2025-06-05T16:19:28Z",
      "closed_at": "2025-05-28T13:01:44Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1813/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1813",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1813",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:27.133170",
      "comments": [
        {
          "author": "fswair",
          "body": "I am opening a PR for this.",
          "created_at": "2025-05-23T15:27:17Z"
        },
        {
          "author": "DouweM",
          "body": "@Kludex This is related to your comment here:\n\nhttps://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/google.py#L478-L480\n\nhttps://github.com/pydantic/pydantic-ai/pull/1815 is an attempt at fixing this in `Usage`. Can you take this please?",
          "created_at": "2025-05-26T15:52:30Z"
        },
        {
          "author": "paullium",
          "body": "hi, wondering if there's a workaround for this in the meantime?",
          "created_at": "2025-05-27T22:42:52Z"
        },
        {
          "author": "Kludex",
          "body": "- Should be solved in https://github.com/pydantic/pydantic-ai/pull/1752.",
          "created_at": "2025-05-28T12:57:05Z"
        },
        {
          "author": "krau",
          "body": "I got a similar error at the same location:\n```\n...\n  File \".../.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 352, in _make_request\n    return self._finish_handling(ctx, model_response)\n           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n  File \".../.venv/lib/python3.13/sit",
          "created_at": "2025-06-03T16:02:53Z"
        }
      ]
    },
    {
      "issue_number": 981,
      "title": "Adding custom metadata attributes to result",
      "body": "Hi. I want to add custom attributes to all result types (e.g: intermediate state of system) that can be accessed by sth like:\nresult.metadata.X_value\nHow is it possible in @tool and @system_prompt methods?\nsth like: `ctx.metadata.X_value = \"y\" -> result.metada.X_value`",
      "state": "closed",
      "author": "bbkgh",
      "author_type": "User",
      "created_at": "2025-02-25T05:58:15Z",
      "updated_at": "2025-06-05T14:00:36Z",
      "closed_at": "2025-06-05T14:00:36Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/981/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/981",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/981",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:27.378317",
      "comments": [
        {
          "author": "Kludex",
          "body": "What would be the motivation for this?",
          "created_at": "2025-04-17T15:23:29Z"
        },
        {
          "author": "bbkgh",
          "body": "In addition to the final result, I needed the detailed results of the tools. For example, if the final result was \"No\", I needed the exact result of the API call that led to it for informing user (Storing it in the database would have been overkill).",
          "created_at": "2025-04-18T05:04:00Z"
        },
        {
          "author": "bbkgh",
          "body": "sth like: https://docs.agno.com/agents/state",
          "created_at": "2025-04-18T18:12:28Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-26T14:00:31Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-04-29T14:00:40Z"
        }
      ]
    },
    {
      "issue_number": 1887,
      "title": "`instructions` and `system_prompt` not working for GeminiModel or GoogleModel",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nThese two parameters don't work. Whereas the following snippets work:\n```\nmodel = GoogleModel(\n                'gemini-2.5-pro-preview-05-06',\n                provider=GoogleProvider(\n                    vertexai=True,\n                    location=\"us-central1\",  # The genai model is not currently available in our default europe-west2.\n                    project=self.settings.project_id\n                )\n            )\nagent = Agent(\n    model,\n    output_type=self.output_validation,\n)\nresponse = agent.run_sync(prompt)\n\n\nmodel = GeminiModel(\n    'gemini-2.5-pro-preview-05-06',\n    provider=\"google-vertex\",\n)\nagent = Agent(\n    model,\n    output_type=self.output_validation,\n)\n\nresponse = agent.run_sync(prompt)\n\n```\nChanging the `user_prompt` parameter from the run_sync call to the `Agent` call, either as `instructions` or `system_prompt` results into the following error:\n```\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 853, in run_sync\n    return get_event_loop().run_until_complete(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 468, in run\n    async for _ in agent_run:\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1923, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 294, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 347, in _make_request\n    model_response = await ctx.deps.model.request(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_ai/models/google.py\", line 176, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/pydantic_ai/models/google.py\", line 273, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/google/genai/models.py\", line 7424, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/google/genai/models.py\", line 6385, in _generate_content\n    request_dict = _GenerateContentParameters_to_vertex(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/google/genai/models.py\", line 2355, in _GenerateContentParameters_to_vertex\n    for item in t.t_contents(\n                ^^^^^^^^^^^^^\n  File \"/Users/javier/Library/Caches/pypoetry/virtualenvs/input-etl-RXC5_C8V-py3.12/lib/python3.12/site-packages/google/genai/_transformers.py\", line 478, in t_contents\n    raise ValueError('contents are required.')\nValueError: contents are required.\n\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12\npydantic-ai 0.2.12\ngoogle-genai 1.17.0\n```",
      "state": "closed",
      "author": "JavierLopezT",
      "author_type": "User",
      "created_at": "2025-06-02T16:25:14Z",
      "updated_at": "2025-06-05T12:46:11Z",
      "closed_at": "2025-06-05T12:46:11Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1887/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1887",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1887",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:27.663765",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex Can you please have a look at this is as our google.genai guru?",
          "created_at": "2025-06-02T22:22:01Z"
        },
        {
          "author": "Kludex",
          "body": "What is the snippet that doesn't work? Can you please also share the imports?",
          "created_at": "2025-06-03T06:03:52Z"
        },
        {
          "author": "JavierLopezT",
          "body": "> What is the snippet that doesn't work? Can you please also share the imports?\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nmodel = GoogleModel(\n                \"gemini-2.5-pro-preview-05-06\",\n         ",
          "created_at": "2025-06-04T08:25:37Z"
        },
        {
          "author": "Kludex",
          "body": "This is a bug on the `google-genai` part, because they are validating the content wrongly...\n\n- That said, I've added a path on our side to fix it: https://github.com/pydantic/pydantic-ai/pull/1922",
          "created_at": "2025-06-05T12:44:38Z"
        }
      ]
    },
    {
      "issue_number": 1826,
      "title": "Add Cerebras to models.",
      "body": "### Description\n\nIt's a similar company to groq, and I think it has sufficient traction for adoption into pydantic. \nSome notable companies that use Cerebras are mistral(\"le chat platform\"), perplexity(\"sonar api\"), and HuggingFace\n\nwebsite: https://cloud.cerebras.ai/\ndocumentation: [cerebras docs](https://inference-docs.cerebras.ai/introduction)\n\n### References\n\n_No response_",
      "state": "open",
      "author": "Luca-Blight",
      "author_type": "User",
      "created_at": "2025-05-25T19:10:33Z",
      "updated_at": "2025-06-05T12:28:16Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1826/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1826",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1826",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:27.898395",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Luca-Blight Would this require a new model class for a custom API, or is their API OpenAI-compatible meaning users could use the existing `OpenAIModel` with a new provider class, like the ones listed in https://ai.pydantic.dev/models/openai/#openai-compatible-models?",
          "created_at": "2025-05-26T16:35:10Z"
        },
        {
          "author": "Luca-Blight",
          "body": "That's a good question. yes I think they are openai compatible per docs below, so perhaps nothing is needed for this.\n\nhttps://inference-docs.cerebras.ai/resources/openai",
          "created_at": "2025-05-26T21:46:01Z"
        },
        {
          "author": "DouweM",
          "body": "@Luca-Blight Ok, so this should already work by building an `OpenAIProvider` with overridden `base_url` and `api_key`, but it would be nice to add a new `CerebrasProvider` nonetheless -- especially as https://github.com/pydantic/pydantic-ai/pull/1835 will let it automatically select an appropriate m",
          "created_at": "2025-05-27T18:18:53Z"
        },
        {
          "author": "smallstepman",
          "body": "duplicate of #723 \n\nI'm currently using this \n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\ncerebras_provider = OpenAIProvider(base_url=\"https://api.cerebras.ai/v1\", api_key=os.environ.get(\"CEREBRAS",
          "created_at": "2025-05-28T13:18:06Z"
        },
        {
          "author": "DouweM",
          "body": "@smallstepman Thanks for the heads-up. I'm going to hold off on implementing `CerebrasProvider` until we know exactly what `json_schema_transformer` it needs.\n\nWith the new feature in https://github.com/pydantic/pydantic-ai/pull/1835 (merged but not yet released), you can specify this using a `Model",
          "created_at": "2025-05-28T19:59:59Z"
        }
      ]
    },
    {
      "issue_number": 1917,
      "title": "Hi, can I pass output_validator as a parameters like tool calls?",
      "body": "### Question\n\nI found this example:\n\n```\n\n@agent.output_validator\nasync def validate_sql(ctx: RunContext[DatabaseConn], output: Output) -> Output:\n    if isinstance(output, InvalidRequest):\n        return output\n    try:\n        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n    except QueryError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n```\n\nbut not sure if there is other way than use decorator on a function, as I want to reuse this function else where. \nLike tools we have parameter tools = [Tool()]\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "dinhngoc267",
      "author_type": "User",
      "created_at": "2025-06-05T07:01:55Z",
      "updated_at": "2025-06-05T07:02:44Z",
      "closed_at": "2025-06-05T07:02:32Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1917/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1917",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1917",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:28.145760",
      "comments": [
        {
          "author": "dinhngoc267",
          "body": "Oh nvm I figured it out lol. I'm so silly \n",
          "created_at": "2025-06-05T07:02:32Z"
        }
      ]
    },
    {
      "issue_number": 1641,
      "title": "Setup for Cursor to understand Pydantic AI",
      "body": "### Question\n\nQuestion to the community. Does anyone successfuly managed a cursor setup in which it does not mix some paradigms of langchain with PydanticAI?\n\nI have tried adding the Pydantic AI docs but it still hallucinates as hell (i.e. pipes agent calls on a default model)\n\nI truly love the simplicity of the tool, it would be awesome to have a setup in Cursor.\n\n### Additional Context\n\nCursor: 0.49.x\nPydantic AI: 0.1.9",
      "state": "closed",
      "author": "zdzarsky",
      "author_type": "User",
      "created_at": "2025-05-03T15:42:11Z",
      "updated_at": "2025-06-04T12:11:47Z",
      "closed_at": "2025-05-14T19:15:11Z",
      "labels": [
        "documentation",
        "question",
        "Stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1641/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1641",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1641",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:28.463591",
      "comments": [
        {
          "author": "Kludex",
          "body": "Can't you index the docs here?\n\n<img width=\"935\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fbe9d2a0-00d5-461b-94ea-a8e3de18819c\" />",
          "created_at": "2025-05-03T16:22:57Z"
        },
        {
          "author": "zdzarsky",
          "body": "Yup, this is something I use from day one, but hallucination is a real issue for me. I am building versions of agents and benchmark them on the same task. I often prompt something like: \n\n```\nBuild an agent like @my_agent_v10.py using @PydanticAI\nDo X instead of Y\nDo Z instead of A\n...\n```\n\nSuddenly",
          "created_at": "2025-05-05T13:42:37Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-13T14:00:33Z"
        },
        {
          "author": "Kludex",
          "body": "I got feedback on Slack saying that they are using the `https://ai.pydantic.dev/llms.txt` and `https://ai.pydantic.dev/llms-full.txt`, and it has been working great for them.\n\nI'm not sure what else can we do here.",
          "created_at": "2025-05-14T17:11:43Z"
        },
        {
          "author": "zdzarsky",
          "body": "I think this is more than enough! Thank you for your help! It looks like it finally understands the code! Additionaly with a prompt \"When building agents do not encapsulate them in classes\".",
          "created_at": "2025-05-14T19:15:11Z"
        }
      ]
    },
    {
      "issue_number": 1683,
      "title": "Anthropic Web Search Tool",
      "body": null,
      "state": "open",
      "author": "Kludex",
      "author_type": "User",
      "created_at": "2025-05-10T17:48:10Z",
      "updated_at": "2025-06-04T10:52:45Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1683/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1683",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1683",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:28.680138",
      "comments": [
        {
          "author": "afarntrog",
          "body": "Here's a link: https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool",
          "created_at": "2025-05-11T20:11:22Z"
        },
        {
          "author": "zugaldia",
          "body": "For OpenAI, there is a way to use built-in tools as [described here](https://ai.pydantic.dev/models/openai/#openai-responses-api) that unlocks web search, file search, and computer use. Is there a similar mechanism that can be used for Anthropic (or other providers for that matter)? ",
          "created_at": "2025-06-04T10:52:44Z"
        }
      ]
    },
    {
      "issue_number": 1197,
      "title": "Guardrails",
      "body": "### Description\n\nWe're building a personal banking agent and need guardrails to ensure agent's safe operations. And I think this feature would benefit multiple industries with sensitive data requirements (banking, healthcare, legal, etc.).\n\n### Referrence\n\nAs a reference, I would like to suggest OpenAI's agents SDK guardrail implementation. It is defined very similarly to the way Pydantic AI defines tools and system prompts\n\n```python\nfrom pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    InputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    TResponseInputItem,\n    input_guardrail,\n)\n\nclass MathHomeworkOutput(BaseModel):\n    is_math_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent( \n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking you to do their math homework.\",\n    output_type=MathHomeworkOutput,\n)\n\n\n@input_guardrail\nasync def math_guardrail( \n    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n) -> GuardrailFunctionOutput:\n    result = await Runner.run(guardrail_agent, input, context=ctx.context)\n\n    return GuardrailFunctionOutput(\n        output_info=result.final_output, \n        tripwire_triggered=result.final_output.is_math_homework,\n    )\n\n\nagent = Agent(  \n    name=\"Customer support agent\",\n    instructions=\"You are a customer support agent. You help customers with their questions.\",\n    input_guardrails=[math_guardrail],\n)\n```\n\nCool thing about it is that the input guardrail is running in parallel with an agent request and doesn't slow down the request. If the guardrail fails it throws an exception, which can then be handled by developer. Although parallel run can then slightly bring up the cost, so it would be great to have a flag for it\n\nAnd I am willing to this a shot myself this weekend, if you guys are okay with it\n\n### References\n\nhttps://openai.github.io/openai-agents-python/guardrails/",
      "state": "open",
      "author": "mattaliev",
      "author_type": "User",
      "created_at": "2025-03-21T06:31:01Z",
      "updated_at": "2025-06-04T03:32:05Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1197/reactions",
        "total_count": 12,
        "+1": 11,
        "-1": 1,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1197",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1197",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:28.949899",
      "comments": [
        {
          "author": "zerubeus",
          "body": "You can't already do this in a validation step, an agent for validation and then handoff? ",
          "created_at": "2025-03-21T10:01:05Z"
        },
        {
          "author": "mattaliev",
          "body": "Ye, but this will be a sequential workflow and it will slow down the request. I can make it run in parallel for my specific use case, but i don't think i am the only developer who would like to use such functionality. That's why I suggested it here",
          "created_at": "2025-03-21T10:19:07Z"
        },
        {
          "author": "zerubeus",
          "body": "I'm curious, how can you make it run in parallel? ",
          "created_at": "2025-03-21T10:27:08Z"
        },
        {
          "author": "mattaliev",
          "body": "You can run input rail and original request in parallel, but not output rail.\n\nHere is how OpenAI did it in their agents SDK: https://github.com/openai/openai-agents-python/blob/main/src/agents/run.py\n\nMore specifically, here is a snippet of code starting on line 210\n\n```python\ninput_guardrail_resul",
          "created_at": "2025-03-21T10:49:53Z"
        },
        {
          "author": "mattaliev",
          "body": "Note, for the guardrail implementation, I don't just want to copy OpenAI's implementation, i just think it's a great foundation for this feature. Additionally, I would add the following:\n\n- Automatic instrumenting of guardrail in logfire\n- Include rails in output messages\n- And provide agent with co",
          "created_at": "2025-03-21T10:53:17Z"
        }
      ]
    },
    {
      "issue_number": 1899,
      "title": "Can the full system prompt be seen after updating dynamically via support dependencies?",
      "body": "### Question\n\nIn the examples illustrating dependency injection like [this simple example](https://github.com/pydantic/pydantic-ai/blob/b0d3ebdffec88b413b19c11c6f30c79a22d4f081/docs/agents.md?plain=1#L638) or [this slightly more advanced example](https://github.com/pydantic/pydantic-ai/blob/b0d3ebdffec88b413b19c11c6f30c79a22d4f081/docs/dependencies.md?plain=1#L56) is there a way to see the final system prompt, with dynamic injection of deps, after running the agent with dependency injection?  Like after this line in the latter example\n\n```python\nresult = await agent.run('Tell me a joke.', deps=deps)\n```\n\nI can see [the system prompt message](https://ai.pydantic.dev/api/messages/) in documentation, but not sure how to use it / dig it out in a real example.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "neonwatty",
      "author_type": "User",
      "created_at": "2025-06-03T14:02:31Z",
      "updated_at": "2025-06-04T00:59:36Z",
      "closed_at": "2025-06-04T00:59:35Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1899/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1899",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1899",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:29.202297",
      "comments": [
        {
          "author": "DouweM",
          "body": "@neonwatty You can get all of the messages sent to the model from `result.all_messages()` (or `result.all_messages_json()`). If you print that out on your terminal, you'll find a `ModelRequest` object with `SystemPromptPart`s as well as an `instructions` field if you've used that feature.\n\nLet me kn",
          "created_at": "2025-06-03T16:38:18Z"
        },
        {
          "author": "neonwatty",
          "body": "That did the trick - thanks!  \n\nlink to `.all_messages` docs for any future visitors\n\nhttps://ai.pydantic.dev/message-history/#messages-and-chat-history",
          "created_at": "2025-06-04T00:59:35Z"
        }
      ]
    },
    {
      "issue_number": 1880,
      "title": "MCP SSE not working since v0.2.10",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nSSE transport does no longer seem to work against after merging support for Streamble HTTP in https://github.com/pydantic/pydantic-ai/pull/1716 by @BrandonShar.\n\n### Sample MCP Server\n\n```python\nfrom mcp.server import FastMCP\n\nMCP = FastMCP(name=\"dummy_mcp_server\")\n\n\n@MCP.tool(name=\"dummy_mcp_tool\")\nasync def dummy_tool() -> str:\n    return \"dummy_tool_response\"\n\n\nif __name__ == \"__main__\":\n    MCP.run(transport=\"sse\")\n```\n\n### Sample MCP Client\n\n```python\nimport asyncio\n\nfrom openai import AsyncOpenAI\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nOPENAI_CLIENT = AsyncOpenAI()\n\nLLM = OpenAIModel(\n    \"gpt-4o-mini\",\n    provider=OpenAIProvider(openai_client=OPENAI_CLIENT),\n    system_prompt_role=\"system\",\n)\nAGENT = Agent(\n    model=LLM,\n    system_prompt=\"You are a helpful assistant.\",\n    mcp_servers=[MCPServerHTTP(\"http://localhost:8000/sse\")],\n)\n\n\nasync def main() -> None:\n    async with AGENT.run_mcp_servers():\n        result = await AGENT.run(\"List available tools.\")\n\n    print(result.output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Error\n\n> asyncio.exceptions.CancelledError: Cancelled by cancel scope 104066a50\n\n<details>\n\n<summary>Full Traceback</summary>\n\n```\nan error occurred during closing of asynchronous generator <async_generator object streamablehttp_client at 0x103f3a880>\nasyncgen: <async_generator object streamablehttp_client at 0x103f3a880>\n  + Exception Group Traceback (most recent call last):\n  |   File \"/path/to/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | BaseExceptionGroup: unhandled errors in a TaskGroup (2 sub-exceptions)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    |     yield\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    |     resp = await self._pool.handle_async_request(req)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    |     raise exc from None\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    |     response = await connection.handle_async_request(\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n    |     return await self._connection.handle_async_request(request)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n    |     raise exc\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n    |     ) = await self._receive_response_headers(**kwargs)\n    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n    |     event = await self._receive_event(timeout=timeout)\n    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 217, in _receive_event\n    |     data = await self._network_stream.read(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 32, in read\n    |     with map_exceptions(exc_map):\n    |          ^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n    |     self.gen.throw(value)\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    |     raise to_exc(exc) from exc\n    | httpcore.ReadTimeout\n    | \n    | The above exception was the direct cause of the following exception:\n    | \n    | Traceback (most recent call last):\n    |   File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 391, in handle_request_async\n    |     await self._handle_post_request(ctx)\n    |   File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 251, in _handle_post_request\n    |     async with ctx.client.stream(\n    |                ^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1583, in stream\n    |     response = await self.send(\n    |                ^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n    |     response = await self._send_handling_auth(\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    |     response = await self._send_handling_redirects(\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    |     response = await self._send_single_request(request)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    |     response = await transport.handle_async_request(request)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    |     with map_httpcore_exceptions():\n    |          ^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n    |     self.gen.throw(value)\n    |   File \"/path/to/venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    |     raise mapped_exc(message) from exc\n    | httpx.ReadTimeout\n    +---------------- 2 ----------------\n    | Traceback (most recent call last):\n    |   File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 492, in streamablehttp_client\n    |     yield (\n    | GeneratorExit\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 464, in streamablehttp_client\n    async with anyio.create_task_group() as tg:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 778, in __aexit__\n    if self.cancel_scope.__exit__(type(exc), exc, exc.__traceback__):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 457, in __exit__\n    raise RuntimeError(\nRuntimeError: Attempted to exit cancel scope in a different task than it was entered in\nan error occurred during closing of asynchronous generator <async_generator object MCPServerHTTP.client_streams at 0x103ffb740>\nasyncgen: <async_generator object MCPServerHTTP.client_streams at 0x103ffb740>\nTraceback (most recent call last):\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/mcp.py\", line 378, in client_streams\n    yield read_stream, write_stream\nGeneratorExit\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/mcp.py\", line 372, in client_streams\n    async with streamablehttp_client(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n    await self.gen.athrow(value)\nRuntimeError: athrow(): asynchronous generator is already running\nTraceback (most recent call last):\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/streams/memory.py\", line 111, in receive\n    return self.receive_nowait()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/streams/memory.py\", line 106, in receive_nowait\n    raise WouldBlock\nanyio.WouldBlock\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/path/to/venveport/dummy_client.py\", line 31, in <module>\n    asyncio.run(main())\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/path/to/venveport/dummy_client.py\", line 24, in main\n    async with AGENT.run_mcp_servers():\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1693, in run_mcp_servers\n    await exit_stack.enter_async_context(mcp_server)\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 659, in enter_async_context\n    result = await _enter(cm)\n             ^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/mcp.py\", line 143, in __aenter__\n    await self._client.initialize()\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/session.py\", line 133, in initialize\n    result = await self.send_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/shared/session.py\", line 283, in send_request\n    response_or_error = await response_stream_reader.receive()\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/streams/memory.py\", line 119, in receive\n    await receive_event.wait()\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 1774, in wait\n    await self._event.wait()\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/locks.py\", line 212, in wait\n    await fut\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 104066a50\n```\n\n</details>\n\n### Alternatives\n\nI tried to switch to `streamable-http` in `MCP.run`, and that failed as well.\n\n> mcp.shared.exceptions.McpError: Session terminated\n\n<details>\n\n<summary>Full Traceback</summary>\n\n```\nan error occurred during closing of asynchronous generator <async_generator object MCPServerHTTP.client_streams at 0x10e763840>\nasyncgen: <async_generator object MCPServerHTTP.client_streams at 0x10e763840>\n  + Exception Group Traceback (most recent call last):\n  |   File \"/path/to/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | BaseExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 492, in streamablehttp_client\n    |     yield (\n    |   File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/mcp.py\", line 378, in client_streams\n    |     yield read_stream, write_stream\n    | GeneratorExit\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/mcp.py\", line 372, in client_streams\n    async with streamablehttp_client(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n    await self.gen.athrow(value)\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 464, in streamablehttp_client\n    async with anyio.create_task_group() as tg:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 778, in __aexit__\n    if self.cancel_scope.__exit__(type(exc), exc, exc.__traceback__):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 457, in __exit__\n    raise RuntimeError(\nRuntimeError: Attempted to exit cancel scope in a different task than it was entered in\nan error occurred during closing of asynchronous generator <async_generator object streamablehttp_client at 0x10e6a2710>\nasyncgen: <async_generator object streamablehttp_client at 0x10e6a2710>\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/aniray1/Desktop/Bodhi/bodhi-forecast/bodhi-product/bodhi-product-forecast-test-mcp/bug_report/dummy_client.py\", line 24, in main\n    async with AGENT.run_mcp_servers():\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1693, in run_mcp_servers\n    await exit_stack.enter_async_context(mcp_server)\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 659, in enter_async_context\n    result = await _enter(cm)\n             ^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/mcp.py\", line 143, in __aenter__\n    await self._client.initialize()\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/session.py\", line 133, in initialize\n    result = await self.send_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/shared/session.py\", line 297, in send_request\n    raise McpError(response_or_error.error)\nmcp.shared.exceptions.McpError: Session terminated\n\nDuring handling of the above exception, another exception occurred:\n\nRuntimeError: aclose(): asynchronous generator is already running\nTraceback (most recent call last):\n  File \"/Users/aniray1/Desktop/Bodhi/bodhi-forecast/bodhi-product/bodhi-product-forecast-test-mcp/bug_report/dummy_client.py\", line 31, in <module>\n    asyncio.run(main())\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/aniray1/Desktop/Bodhi/bodhi-forecast/bodhi-product/bodhi-product-forecast-test-mcp/bug_report/dummy_client.py\", line 24, in main\n    async with AGENT.run_mcp_servers():\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1693, in run_mcp_servers\n    await exit_stack.enter_async_context(mcp_server)\n  File \"/usr/local/Cellar/python@3.12/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 659, in enter_async_context\n    result = await _enter(cm)\n             ^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/pydantic_ai/mcp.py\", line 143, in __aenter__\n    await self._client.initialize()\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/client/session.py\", line 133, in initialize\n    result = await self.send_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/venv/lib/python3.12/site-packages/mcp/shared/session.py\", line 297, in send_request\n    raise McpError(response_or_error.error)\nmcp.shared.exceptions.McpError: Session terminated\n```\n</details>\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.10\nMCP 1.9.2\nPydantic 2.11.5\nPydantic AI >=0.2.10 (0.2.9 is working)\nOpenAI gpt-4o-mini\n```",
      "state": "closed",
      "author": "yarnabrina",
      "author_type": "User",
      "created_at": "2025-06-01T09:34:09Z",
      "updated_at": "2025-06-03T18:26:48Z",
      "closed_at": "2025-06-03T18:26:48Z",
      "labels": [
        "need confirmation",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1880/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1880",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1880",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:29.425562",
      "comments": [
        {
          "author": "JohnUiterwyk",
          "body": "I'm aslo experiencing this with an sse endpoint since this change. It seems to be something to do with the initialize step.",
          "created_at": "2025-06-01T19:14:44Z"
        },
        {
          "author": "BrandonShar",
          "body": "There was an error introduced in #1833 that was patched in #1843 and released in 0.2.12, that may be the issue you're experiencing. Also discussed in #1860 ",
          "created_at": "2025-06-01T20:02:41Z"
        },
        {
          "author": "kverdecia",
          "body": "I've been able to use sse tools in version 0.1.11.\nThe problem seems to be in version 0.1.11. \nWhen calling the .run_mcp_servers() method, it's calling the /sse endpoint with POST method instead of GET.",
          "created_at": "2025-06-02T07:37:06Z"
        },
        {
          "author": "Stefano-Pedretti",
          "body": "Issue also present in version 0.2.12",
          "created_at": "2025-06-02T09:31:16Z"
        },
        {
          "author": "DouweM",
          "body": "Sorry you're running into this -- we'd been under the impression that https://github.com/pydantic/pydantic-ai/pull/1716 was backward-compatible with SSE servers, but it appears not. We do intend to keep supporting them, but we may need separate classes for SSE and Streamable HTTP servers.\n\n@Kludex w",
          "created_at": "2025-06-02T21:51:35Z"
        }
      ]
    },
    {
      "issue_number": 1890,
      "title": "BaseModel output type no longer working for Gemini models",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm running into a new Gemini issue using BaseModel output types. Code that was working yesterday is now returning an error.\n```\nValidationError: 1 validation error for _GeminiResponse\ncandidates.0.content.parts.0.text.text\n  Field required [type=missing, input_value={'thought': True, 'though...Kme/E40jb0xxL+qYyMsw=='}, input_type=dict]\n```\n\nOpenAI models are fine so it's Gemini specific.\n\nWorking:\n```python\nagent = Agent(\n    model=\"google-gla:gemini-2.5-flash-preview-05-20\",\n    output_type=str\n)\n\nresult = await agent.run(\"What's up?\")\nprint(result.output)\n```\n\nNot working:\n```python\nclass SimpleResponse(BaseModel):\n    output: str = Field(description=\"The output of the LLM.\")\n\nagent = Agent(\n    model=\"google-gla:gemini-2.5-flash-preview-05-20\",\n    output_type=SimpleResponse\n)\n\nresult = await agent.run(\"What's up?\")\nprint(result.output)\n```\n\n### Example Code\n\n```Python\nclass SimpleResponse(BaseModel):\n    output: str = Field(description=\"The output of the LLM.\")\n\nagent = Agent(\n    model=\"google-gla:gemini-2.5-flash-preview-05-20\",\n    output_type=SimpleResponse\n)\n\nresult = await agent.run(\"What's up?\")\nprint(result.output)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.11\npydantic-ai-slim                         0.2.12\npydantic-core                            2.33.1\npydantic-graph                           0.2.12\npydantic                                 2.11.3\n```",
      "state": "closed",
      "author": "pjsample",
      "author_type": "User",
      "created_at": "2025-06-02T21:04:03Z",
      "updated_at": "2025-06-03T17:20:31Z",
      "closed_at": "2025-06-03T17:18:03Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1890/reactions",
        "total_count": 12,
        "+1": 12,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1890",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1890",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:29.632927",
      "comments": [
        {
          "author": "MacroMackie",
          "body": "This is affecting us too - seems to specifically affect us in situations where: thinking_budget>0.\n\nThe gemini api seems to be returning:\n```\nparts: [\n    {'thought': True, 'thoughtSignature': 'AZs1mZxelyMPKoHcvslvjy5WZYI2HosTK...'},\n    {'functionCall': {'name': 'final_result_XyzResponse', 'args': ",
          "created_at": "2025-06-02T21:47:01Z"
        },
        {
          "author": "DouweM",
          "body": "@MacroMackie That's right.\n\nAs a temporary workaround, you can disable thinking: https://ai.pydantic.dev/models/gemini/#disable-thinking\n\n@Kludex Can you please have a look at this as you've been working on Google and Thinking?",
          "created_at": "2025-06-02T22:38:22Z"
        },
        {
          "author": "omarbaker8",
          "body": "switching to GoogleModel class rather than GeminiModel would solve it",
          "created_at": "2025-06-03T00:18:35Z"
        },
        {
          "author": "nguyen-tran-ds-vns",
          "body": "> switching to GoogleModel class rather than GeminiModel would solve it\n\nCan you show us how? I can't seem to find the docs showing how to use GoogleModel class.",
          "created_at": "2025-06-03T04:56:36Z"
        },
        {
          "author": "nguyen-tran-ds-vns",
          "body": "> [@MacroMackie](https://github.com/MacroMackie) That's right.\n> \n> As a temporary workaround, you can disable thinking: https://ai.pydantic.dev/models/gemini/#disable-thinking\n> \n> [@Kludex](https://github.com/Kludex) Can you please have a look at this as you've been working on Google and Thinking?",
          "created_at": "2025-06-03T06:08:47Z"
        }
      ]
    },
    {
      "issue_number": 934,
      "title": "Feature Request: Synchronous Calls",
      "body": "I would love it if I had the option use the library entirely with synchronous calls. This way, I have full freedom to enable or disable this, pursue other concurrency models, and would have less issues interfacing with other libraries that are opinionated about open event loops.\n\nUnfortunately, `run.sync` doesn't solve this. I'd actually prefer if synchronous calls were a 1st class citizen and there was a thin wrapper or arg to make things async instead of vice versa. Unfortunately, I understand that this isn't necessarily easy to express in python, meaning it often leads to two full copies of the api for sync and async. Still, I think there's a good reason this is available in e.g. OpenAI's client.\n\n### Addendum (feel free to ignore)\n\nMore broadly (please excuse if I'm overstepping my bounds), but I wonder a bit if this is a downstream issue of a broader problem with what the identity of this library is. When I read:\n\n> Agent Framework / shim to use Pydantic with LLMs \n\n> PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.\n\n> Every agent framework and LLM library in Python uses Pydantic, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling.\n\nI think \"Oh cool, the Pydantic team is making an agent framework. I'll use that if I need to do something involving agentic workflows, probably overkill otherwise.\"\n\nBut then when I hear:\n\n> PydanticAI\n\n> PydanticAI is a Python Agent Framework designed to make it less painful to build production grade applications with Generative AI.\n\n> We built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI app development.\n\nMy thought is, \"Oh! Amazing! The Pydantic team is going bring that 'minimal, just works, clean' feel compared to lots of other recent general AI frameworks.\"\n\nPersonally I would love a philosophy that is something like\n\n1. PydanticAI: makes input/output data validation easy, standardize ways of calling apis, standardize logging/observability (or connect to other observability tools)\n2. PydanticAgents or PydanticWorkflows w/e: build complex workflows, graphs, etc. on top of PydanticAI\n3. ???\n\nOverall, I'm sort of hoping for `Pydantic <-> PydanticAI` and `FastAPI <-> PydanticAgents` or something like this. But right now they're blending together. I think the tension between these two viewpoints leads to some design consequences:\n\n- As a user, it's easier for me to build agent workflows on top of what I call PydanticAI + some features of PydanticAgents being implemented, but it's much harder to build on top of what I call fully-featured PydanticAgents with only some of PydanticAI implemented. A key example of this would be the decision prioritize more agentic features, while [multi-modal support still doesn't feel particularly clean](https://github.com/pydantic/pydantic-ai/issues/760). If thinking generally about AI, multi-modal input would be one of the first things I think should be supported, before more complex features.\n- Maybe if you're thinking agents, then it might be \"obvious\" that async is a reasonable default (maybe not). But if thinking just \"good foundation for ai\" this would be an optional extra feature request.\n- Certainly the default usage of just passing things into an `Agent` works great and feels clean. But also [there are situations](https://ai.pydantic.dev/models/#custom-openai-client) when it seems like you need to create a `Client` just to pass it into a `Model` just to pass that into an `Agent`. Maybe this is outdated docs, but if not, that doesn't feel right. Using a custom endpoint in the `OpenAI` client outside of this library requires just passing the different endpoint and api key as args, not instantiating 3 classes. It's not a big deal, but I wonder if more things like this will happen if `Agent` is the first class citizen, instead of viewed as built \"on top of\" `PydanticAI`.\n- The choice to have a simple centralized way to call loads of providers (as opposed to \"bring your own client\"), great data validation on inputs and outputs, and logfire makes it feel like this is a general foundation for AI, not just a way to run agent workflows (and that's great!).\n- Having more of a separation between the philosophy of PydanticAI and PydanticAgents also helps with possible paradigm shifts. Maybe in a year or two, we won't really be thinking about \"agents,\" maybe we will. But we'll always want full-featured standards for calling AI apis, data validation in and out, logging, observability, etc.\n- Said another way: what defines an _agent_? Is it a client + state? Model + state? Or just a client or a model? A useful coordination point of many different abilities? I think it might be useful to have building blocks to create many different types of agents in the long run.\n\nThanks for all the hard work, curious what you think. I wouldn't write this if I wasn't super excited about this library!\n",
      "state": "open",
      "author": "JonathanRayner",
      "author_type": "User",
      "created_at": "2025-02-16T23:22:45Z",
      "updated_at": "2025-06-03T16:55:25Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/934/reactions",
        "total_count": 11,
        "+1": 6,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 5,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/934",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/934",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:30.214263",
      "comments": [
        {
          "author": "dmontagu",
          "body": "Can you expand on this:\n\n> I'd actually prefer if synchronous calls were a 1st class citizen and there was a thin wrapper or arg to make things async instead of vice versa. Unfortunately, I understand that this isn't necessarily easy to express in python, meaning it often leads to two full copies of",
          "created_at": "2025-02-20T21:18:00Z"
        },
        {
          "author": "dmontagu",
          "body": "I'll also note that the approach described [here](https://www.psycopg.org/articles/2024/09/23/async-to-sync/), which is how psycopg maintains a sync and async version of the codebase, may be feasible for us. Just need to adapt [this surprisingly-not-huge script](https://github.com/psycopg/psycopg/bl",
          "created_at": "2025-02-20T21:37:32Z"
        },
        {
          "author": "Kludex",
          "body": "We agreed on working on this.\n\nThe idea is that we are going to generate the sync API via an CST script.\n\nWhenever I work on my priorities, I'll get back to this. ",
          "created_at": "2025-02-28T20:57:20Z"
        },
        {
          "author": "cspiecker",
          "body": "Hi @Kludex ,\nJust checking in to see if there’s any update on the synchronous API feature. Is it currently being worked on, or is there an estimated timeline for its release? Appreciate all the work you’re doing\n",
          "created_at": "2025-06-03T16:55:24Z"
        }
      ]
    },
    {
      "issue_number": 1895,
      "title": "[REOPEN] Original Issue #1860: Error in streaming output from connecting to the MCP server",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAs stated in the subject, the issue in #1860 has not yet been resolved. Could you please continue to help me address the two questions I raised in my original post? Thank you.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.9\npydantic-ai 0.2.6/0.2.12\n```",
      "state": "closed",
      "author": "illusions-LYY",
      "author_type": "User",
      "created_at": "2025-06-03T09:14:03Z",
      "updated_at": "2025-06-03T15:26:47Z",
      "closed_at": "2025-06-03T15:26:46Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1895/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1895",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1895",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:30.463538",
      "comments": [
        {
          "author": "Kirouane-Ayoub",
          "body": "Hi, I'm also experiencing the same issue as described in #1860.\n\n###  Environment:\n\n- pydantic-ai version: 0.2.12\n- Python version: 3.12.3\n",
          "created_at": "2025-06-03T11:37:02Z"
        },
        {
          "author": "DouweM",
          "body": "Thank you, I'll reopen the original issue.",
          "created_at": "2025-06-03T15:26:46Z"
        }
      ]
    },
    {
      "issue_number": 1790,
      "title": "NotCapable: Requires env access, run again with the --allow-env flag",
      "body": "```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nimport logfire\n\nlogfire.configure(token='pylf_v1_us_HC1Jj30qrvN3ZqgQ7psJQPLcDZz5BrgpMZ0W77C17Sjb')\n\nlogfire.instrument_mcp()\nlogfire.instrument_pydantic_ai()\n\nserver = MCPServerStdio('deno',\n    args=[\n        'run',\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ])\nagent = Agent('deepseek:deepseek-chat', mcp_servers=[server])\n\n\nasync def main():\n    async with agent.run_mcp_servers():\n        result = await agent.run(\"Hi, what is the final result of `__import__('os').system('ls'). Run python program\")\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.w\n\nif __name__ == '__main__':\n    import asyncio\n    asyncio.run(main())\n```\n\n\n```\n06:41:25.105 MCP request: initialize\nLogfire project URL: https://logfire-us.pydantic.dev/subowei2020/starter-project\n06:41:27.711 agent run\n06:41:27.714   MCP request: tools/list\n06:41:27.717   chat deepseek-chat\n06:41:33.667   MCP request: tools/list\n06:41:33.669   running 1 tool\n06:41:33.669     running tool: run_python_code\n06:41:33.672       MCP request: tools/call run_python_code\nPyodide has suffered a fatal error. Please report this to the Pyodide maintainers.\nThe cause of the fatal error was:\nNotCapable: Requires env access, run again with the --allow-env flag\n    at Object.toObject (ext:deno_os/30_os.js:134:12)\n    at normalizeSpawnArguments (ext:deno_node/internal/child_process.ts:569:39)\n    at Object.spawnSync (node:child_process:146:8)\n    at __emscripten_system (file:///mnt/d/code/pydantic-ai/mcp-run-python/node_modules/.deno/pyodide@0.27.6/node_modules/pyodide/pyodide.asm.js:10:288451)\n    at <anonymous> (wasm://wasm/0268c42a:1:4937777)\n    at <anonymous> (wasm://wasm/0268c42a:1:4067484)\n    at <anonymous> (wasm://wasm/0268c42a:1:2147653)\n    at <anonymous> (wasm://wasm/0268c42a:1:1855333)\n    at <anonymous> (wasm://wasm/0268c42a:1:2763994)\n    at <anonymous> (wasm://wasm/0268c42a:1:2721689) {\n  name: \"NotCapable\",\n  pyodide_fatal_error: true\n}\nerror: Uncaught (in promise) NotCapable: Requires env access, run again with the --allow-env flag\n    at Object.toObject (ext:deno_os/30_os.js:134:12)\n    at normalizeSpawnArguments (ext:deno_node/internal/child_process.ts:569:39)\n    at Object.spawnSync (node:child_process:146:8)\n    at __emscripten_system (file:///mnt/d/code/pydantic-ai/mcp-run-python/node_modules/.deno/pyodide@0.27.6/node_modules/pyodide/pyodide.asm.js:10:288451)\n    at <anonymous> (wasm://wasm/0268c42a:1:4937777)\n    at <anonymous> (wasm://wasm/0268c42a:1:4067484)\n    at <anonymous> (wasm://wasm/0268c42a:1:2147653)\n    at <anonymous> (wasm://wasm/0268c42a:1:1855333)\n    at <anonymous> (wasm://wasm/0268c42a:1:2763994)\n    at <anonymous> (wasm://wasm/0268c42a:1:2721689)\n```",
      "state": "closed",
      "author": "Subway2023",
      "author_type": "User",
      "created_at": "2025-05-21T06:52:05Z",
      "updated_at": "2025-06-03T14:00:52Z",
      "closed_at": "2025-06-03T14:00:52Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1790/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1790",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1790",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:30.681695",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Subway2023 Can you try adding the `--allow-env` flag in your `args`, as the error suggests?",
          "created_at": "2025-05-21T15:47:59Z"
        },
        {
          "author": "Subway2023",
          "body": "I ran it directly using python test.py. So where should I add --allow-env? This Python file doesn't support argument parsing.\"",
          "created_at": "2025-05-22T15:37:41Z"
        },
        {
          "author": "DouweM",
          "body": "@Subway2023 The error seems to be coming from `deno`, which your code is passing as the command to `MCPServerStdio`, and which is passed the `args` list that's just below it. So you'll want to add it to that args list.",
          "created_at": "2025-05-22T20:33:16Z"
        },
        {
          "author": "Subway2023",
          "body": "I modified args as follows, but the same error still occurred.\n```python\nserver = MCPServerStdio('deno',\n    args=[\n        'run',\n        \"--allow-env=PORT,HOME,PATH\",\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-r",
          "created_at": "2025-05-23T02:38:06Z"
        },
        {
          "author": "DouweM",
          "body": "@Subway2023 Including when you pass `--allow-env` without specific env names, the error still states `Requires env access, run again with the --allow-env flag`?",
          "created_at": "2025-05-23T13:38:02Z"
        }
      ]
    },
    {
      "issue_number": 1809,
      "title": "See eval reasons in report printed report",
      "body": "### Question\n\n\n\nI'm running an eval against a dataset with LLMJudge as an evaluator. The table that is printed with this code:\n\n```\n report = dataset.evaluate_sync(get_mongo_query)\n report.print(include_input=True, include_output=True)\n```\n\ndoes not include the evaluation reason explaining why the LLMJudge failed. I see in the LLMJudge code that `include_reason=True` so wondering if I'm missing some configuration or if there's some other way to see this output. I didn't see any documentation about this.\n\n```\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID     ┃ Inputs             ┃ Outputs           ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ simple_case │                    │ ...     │ ✔✗         │    16.1s │\n```\n\n### Additional Context\n\npydantic_evale v0.2.6\npython v3.12.3",
      "state": "closed",
      "author": "brian-swantide",
      "author_type": "User",
      "created_at": "2025-05-22T18:09:13Z",
      "updated_at": "2025-06-03T14:00:50Z",
      "closed_at": "2025-06-03T14:00:50Z",
      "labels": [
        "question",
        "Stale",
        "evals"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1809/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1809",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1809",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:30.929265",
      "comments": [
        {
          "author": "dmontagu",
          "body": "The reasons are present in the EvaluationReport object if you dump it / interact with it programmatically, and are present in the produced spans (for viewing in Logfire or another OTel-compatible observability platform).\n\nI didn't add them (yet 🙂) for console printing because I had a hard time imagi",
          "created_at": "2025-05-23T13:33:41Z"
        },
        {
          "author": "brian-swantide",
          "body": "Thanks for the thorough explanation! When I was thinking about it I was imagining something similar to what you proposed (separate table with a row per evaluation), but possibly filtering that table to only show evaluations that failed. So for every failed evaluation a more detailed view of the reas",
          "created_at": "2025-05-24T05:56:15Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-31T14:00:31Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-06-03T14:00:50Z"
        }
      ]
    },
    {
      "issue_number": 1879,
      "title": "`name cannot be empty` tool call error from Gemini when using OpenAI-compatible API",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using `pydantic-ai` with the OpenAI-compatible API from Gemini (`gemini-2.5-pro-preview-05-06`), I receive the following error:\n\n```\nArguments: (ModelHTTPError(\"status_code: 400, model_name: gemini-2.5-pro-preview-05-06, body: [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents[2].parts[0].function_response.name: Name cannot be empty.\\\\n', 'status': 'INVALID_ARGUMENT'}}]\"),)\n```\n\n### Request Body Sent to Gemini:\n\n```json\n[\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the current time?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"id\": \"pyd_ai_92c10235de384166b78a9a969f6e15d4\",\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"get_current_time\",\n                    \"arguments\": \"{}\"\n                }\n            }\n        ]\n    },\n    {\n        \"role\": \"tool\",\n        \"tool_call_id\": \"pyd_ai_cafa2820d22445f4928db8031862f939\",\n        \"content\": \"\\\"2025-06-01T12:25:59.656221\\\"\"\n    }\n]\n```\n\nAs seen above, the `tool_call_id` in the `tool` message **does not match** the `id` in the assistant’s `tool_calls`. Gemini expects these to match for proper association with the function response. The mismatch causes the backend to return a `400 INVALID_ARGUMENT` error due to a missing or unlinked `function_response.name`.\n\n### Suggested fix:\n\nEnsure that the `tool_call_id` in the `tool` message exactly matches the corresponding `id` from the `tool_calls` array in the assistant message. This should maintain consistency with the OpenAI-compatible API expected schema.\n\n### Example Code\n\n```Python\nfrom openai import AsyncOpenAI\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai import Agent, Tool\nfrom datetime import datetime\nimport asyncio\n\nprovider = OpenAIProvider(\n    openai_client=AsyncOpenAI(\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n        api_key={api_key},  # Replace with your Gemini API key\n    )\n)\n\nmodel = OpenAIModel(model_name=\"gemini-2.5-pro-preview-05-06\", provider=provider)\n\ndef get_current_time() -> datetime:\n    \"\"\"Get the current time in UTC.\"\"\"\n    return datetime.now()\n\nagent = Agent(\n    model=model,\n    tools=[Tool(get_current_time, takes_ctx=False)]\n)\n\nasync def main():\n    response = await agent.run(\"What is the current time?\")\n    print(response)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai: 0.2.12\nopenai SDK: 1.82.1\nPython: 3.12.9\nModel: gemini-2.5-pro-preview-05-06 (via OpenAI-compatible endpoint)\n```",
      "state": "closed",
      "author": "HesamZamanpour",
      "author_type": "User",
      "created_at": "2025-06-01T09:06:24Z",
      "updated_at": "2025-06-02T22:47:18Z",
      "closed_at": "2025-06-02T22:47:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1879/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1879",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1879",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:31.224482",
      "comments": []
    },
    {
      "issue_number": 1886,
      "title": "Deepseek Reasoner - Structured Output?",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm getting a 400 with `deepseek-reasoner`. I think it is because I am using `output_type` with my Agent, so it is expecting structured output. \n\n```\nstatus_code: 400, model_name: deepseek-reasoner, body: {'message': 'deepseek-reasoner does not support this tool_choice', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}\n```\n\nI think with Deepseek 5/28 release, they now support JSON output and function calling: https://api-docs.deepseek.com/guides/json_mode\n\nHowever, the \"example json output\" needs to be in the system prompt unlike other agents. \n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11. Pydantic 0.2.12\n```",
      "state": "closed",
      "author": "tchanxx",
      "author_type": "User",
      "created_at": "2025-06-02T15:23:02Z",
      "updated_at": "2025-06-02T22:21:12Z",
      "closed_at": "2025-06-02T22:21:10Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1886/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1886",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1886",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:31.224510",
      "comments": [
        {
          "author": "DouweM",
          "body": "Thanks for raising this. This will be implemented in https://github.com/pydantic/pydantic-ai/pull/1628.",
          "created_at": "2025-06-02T22:21:10Z"
        }
      ]
    },
    {
      "issue_number": 1883,
      "title": "How to mark a tool as FINAL ?",
      "body": "### Question\n\nI need to break agent's loop immediately after a tool call, I mean without one observation-think cycly. Due pydantic ai agents internaly create a FINAL_RESULT tool for each structured result type I believe there should be a trick how to mark my own custom tool as FINAL_RESULT tool also. \n\nI would happy if you give me advice how to do this \n\n### Additional Context\n\nMy tool serves  as a agent Response/Output . But the response is connected to an extra logic, so I wrap the agent's response into a tool and implement all nessesary logic inside it. Unfortunatelly a got a unexpected result that now my agents call response tool and then do redundant extra agent loop cycle.\n\n- agent cycle\n- agent cycle\n- agent responsetool (implement extra logic inside the tool) # I would like to stop the loop here\n- agent cycle  # this extra cycle with response observation is redundat, I need to skip it \n\n\nIn other words I would like to have some option for Tool class\n`Tool.break_the_loop` or `Tool.is_final_result` which allows me to break the agent's loop immediately after call and treat it's result as a agent's final result \n  ",
      "state": "closed",
      "author": "alexey2baranov",
      "author_type": "User",
      "created_at": "2025-06-02T06:35:02Z",
      "updated_at": "2025-06-02T22:17:41Z",
      "closed_at": "2025-06-02T22:17:40Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1883/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1883",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1883",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:31.508403",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/847\" (95% similar)",
          "created_at": "2025-06-02T06:40:09Z"
        },
        {
          "author": "DouweM",
          "body": "@alexey2baranov I think this is covered by https://ai.pydantic.dev/output/#output-functions, which was released last week!\n\nLet me know if not and we can reopen this issue.",
          "created_at": "2025-06-02T22:17:40Z"
        }
      ]
    },
    {
      "issue_number": 1878,
      "title": "SSE not working in latest MCP version",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHi team,\nI’ve noticed that in the latest release of the MCP protocol, older MCP clients using the `SSE` transport no longer appear to be supported.\n\nI have two main questions:\n\n1. Is SSE planned to be deprecated entirely going forward?\n2. Would it be possible for the MCP system to support **both** the new `streamableHttp` protocol **and** the previous `SSE`-based streaming in parallel, to ensure backward compatibility?\n\nThis would help with smooth transitions for clients still relying on the SSE-based setup.\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython==3.12.9\npydantic-ai==0.2.12\n```",
      "state": "closed",
      "author": "HesamZamanpour",
      "author_type": "User",
      "created_at": "2025-06-01T07:37:16Z",
      "updated_at": "2025-06-02T21:51:15Z",
      "closed_at": "2025-06-02T21:51:14Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1878/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1878",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1878",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:31.736727",
      "comments": [
        {
          "author": "DouweM",
          "body": "@HesamZamanpour Sorry you're running into this -- we'd been under the impression that https://github.com/pydantic/pydantic-ai/pull/1716 was backward-compatible with SSE servers, but it appears not. We do intend to keep supporting them, but we may need separate classes for SSE and Streamable HTTP ser",
          "created_at": "2025-06-02T21:50:30Z"
        },
        {
          "author": "DouweM",
          "body": "There's some more discussion in https://github.com/pydantic/pydantic-ai/issues/1880, I'll close this as a dupe of that.",
          "created_at": "2025-06-02T21:51:14Z"
        }
      ]
    },
    {
      "issue_number": 1876,
      "title": "Passing files to mcp-run-python mcp servier",
      "body": "### Question\n\nHi,\n\nHow can the mcp servier mcp-run-python have access to say a csv file?\nIf I want to run some python code on a csv file, what is the way to do that with `mcp-run-python`\n\nCurrently, I cannot find any such possibility. \n\n\n\n\n\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "nikhilxi",
      "author_type": "User",
      "created_at": "2025-05-31T12:07:19Z",
      "updated_at": "2025-06-02T21:45:57Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1876/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1876",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1876",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:31.988613",
      "comments": []
    },
    {
      "issue_number": 1863,
      "title": "Support \"name\" for naming chat messages with the same role.",
      "body": "### Description\n\nOpen AI supports the name aatribute in a chat message to diffentiate message with the same role.\nThis feature is quite useful for a multi user chat completions.\n\nI would like to see this feature in pydantic ai as well.\n\nI checked the source code of pydantic ai. The function _map_message in openai.py currently does not support this feature.\n\nRegards \nAndreas\n\n### References\n\nhttps://platform.openai.com/docs/api-reference/chat/create",
      "state": "open",
      "author": "whvneo",
      "author_type": "User",
      "created_at": "2025-05-29T19:58:27Z",
      "updated_at": "2025-06-02T21:33:43Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1863/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1863",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1863",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:31.988644",
      "comments": []
    },
    {
      "issue_number": 1820,
      "title": "Force Agent tool use on first request",
      "body": "### Description\n\nWhen assigning tools there's no method or parameter to add that forces the model to use the tool. You either have to instruct it through a system_prompt or assume it makes the decision itself to use the tool.\n\nThe OpenAI SDK has a tool_choice parameter that can be found [here](https://platform.openai.com/docs/api-reference/responses/create#responses/object-tool_choice).\n\nCan you add this or a similar functionality that forces the model to use a specified tool on the first API request?\n\nExample of the tool_choice parameter:\n\n```\nresponse = client.responses.create(\n      model=model,\n      tools=[{\"type\": \"web_search_preview\"}],\n      input=search_query,\n      tool_choice=\"required\",\n  )\n```\n\n### References\n\nhttps://platform.openai.com/docs/api-reference/responses/create#responses/object-tool_choice",
      "state": "open",
      "author": "maxvdv94",
      "author_type": "User",
      "created_at": "2025-05-24T17:35:22Z",
      "updated_at": "2025-06-02T21:13:47Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1820/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1820",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1820",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:31.988656",
      "comments": [
        {
          "author": "Whadup",
          "body": "``` python\nagent = Agent(\n    ....\n        model_settings=OpenAIModelSettings(\n            temperature=chat_context.request.temperature,\n            top_p=chat_context.request.top_p,\n            extra_body={\"tool_choice\": \"required},\n        )\n)",
          "created_at": "2025-05-26T06:23:26Z"
        },
        {
          "author": "madebyjeroen",
          "body": "It would be great if a `tool_choice` parameter could be set directly when running the agent (e.g., via `agent.run`).\n\nPairing this with the tools' `prepare` method (or the new `prepare_tools` method) could enable even more fine-grained and deterministic agent execution, allowing you to dynamically s",
          "created_at": "2025-05-26T07:38:59Z"
        },
        {
          "author": "DouweM",
          "body": "Do we just want to specify this for the _first_ request to the LLM, or do we want a way to dynamically modify the `tool_choice` for every request to the LLM, through some kind of hook function that takes `RunContext` (similar to `prepare_tools`)?\n\nOne factor that complicates this slightly is that Py",
          "created_at": "2025-05-26T16:57:01Z"
        },
        {
          "author": "DouweM",
          "body": "I imagine a hook similar to `prepare_tools` that can be set on the `Agent`, for example `prepare_request` or just `prepare`, that is passed the `ModelRequestParameters`, and would allow you to set an attribute to get the \"you have to use a tool, not go straight to a final response\" behavior. (As `fu",
          "created_at": "2025-05-27T20:10:57Z"
        },
        {
          "author": "simone-gaiarin",
          "body": "First, could you clarify what you mean by \"first request\"? Are you referring to the very first call to run, or to the first internal request the agent makes when run is invoked?\n\nMy suggestion is to support dynamically changing tool_choice on every call to `run`. As I mentioned in #1777, when an age",
          "created_at": "2025-05-30T14:25:57Z"
        }
      ]
    },
    {
      "issue_number": 1873,
      "title": "Pydantic AI using File Descriptor improperly",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nGiven the code below, the `get_file` tool return a File descriptor of an opened file.\n\nThe expected result would be that the agent is not able to complete the reading action as it does not \"know\" how to read the content from a descriptor, while the output is the opposite: \n\n```\n❯ python3 example.py\nThe content of the current file is:\n\n\nfrom pydantic_ai import Agent\nfrom pathlib import Path\n\nagent = Agent('openai:gpt-4.1')\n\n\n@agent.tool_plain\ndef get_file():\n    return Path(__file__).open()\n\n\nresult = agent.run_sync('What is the content of the current file?')\nprint(result.output)\n\n\nThis script creates an AI agent using the pydantic_ai library, defines a function to read the current file, runs the agent with the prompt \"What is the content of the current file?\", and prints the result.\n\n```\n\nIs there an explaination as of why the current behaviour? And is it expected? \n\n@Kludex  tagging you as this is the issue we were discussing during pycon this morning, thanks!\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pathlib import Path\n\nagent = Agent('openai:gpt-4.1')\n\n\n@agent.tool_plain\ndef get_file():\n    return Path(__file__).open()\n\n\nresult = agent.run_sync('What is the content of the current file?')\nprint(result.output)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython version: 3.12.7\npydantic-ai version: 0.2.12\nLLM: openai:gpt-4.1\n```",
      "state": "closed",
      "author": "gaiwan-dev",
      "author_type": "User",
      "created_at": "2025-05-30T17:48:42Z",
      "updated_at": "2025-06-02T15:52:09Z",
      "closed_at": "2025-06-02T15:52:08Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1873/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1873",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1873",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:32.239181",
      "comments": [
        {
          "author": "Kludex",
          "body": "Hi 👋 \n\nI discussed it internally, and this is not really an issue.\n\nThe reason why it happens is because of Pydantic itself. We use a `TypeAdapter` underneath that would do what we can reproduce here:\n\n```py\nfrom pathlib import Path\nfrom typing import Any\n\nimport pydantic\n\nadapter = pydantic.TypeAda",
          "created_at": "2025-06-02T15:52:08Z"
        }
      ]
    },
    {
      "issue_number": 1837,
      "title": "Dockerise the MCP Run Python server",
      "body": "### Description\n\nHey there!\n\nI've read through the following sources relating to the new MCP Run Python server:\n1. https://github.com/pydantic/pydantic-ai/blob/8f83407d870475dfc1885fa5119024dc2ffb44ab/mcp-run-python/README.md\n2. https://ai.pydantic.dev/mcp/run-python/\n\nHowever, when trying to add this as an MCP server using the following configuration:\n```json\n{\n  \"mcpServers\": {\n    \"python-sandbox\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"-N\",\n        \"-R=node_modules\",\n        \"-W=node_modules --node-modules-dir=auto jsr:@pydantic/mcp-run-python stdio\"\n      ]\n    }\n  }\n}\n```\n\nI get an `MCP error -32000: Connection closed`. Whilst this is its own separate issue - as I've experienced this with other MCP servers - I was able to work around it by using dockerised servers. \n\nSo, maybe it's worth also dockerising MCP Run Python? \n\nThank you!\n\n### References\n\n_No response_",
      "state": "open",
      "author": "DevJake",
      "author_type": "User",
      "created_at": "2025-05-27T11:00:09Z",
      "updated_at": "2025-06-02T11:43:17Z",
      "closed_at": null,
      "labels": [
        "good first issue",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1837/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1837",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1837",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:37.491470",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Happy to offer it as a docker image. Should be pretty easy to implement.",
          "created_at": "2025-05-31T06:04:18Z"
        },
        {
          "author": "Kludex",
          "body": "We need to create a job on `.github/workflows` that releases this docker image. ",
          "created_at": "2025-06-02T11:42:07Z"
        }
      ]
    },
    {
      "issue_number": 1783,
      "title": "Support for MCP resources",
      "body": "### Description\n\nIt seems like Pydantic AI only supports MCP Tools, not resources: https://modelcontextprotocol.io/docs/concepts/resources\n\nThese are a core part of the MCP protocol\n\n### References\n\n_No response_",
      "state": "open",
      "author": "hayk-corpusant",
      "author_type": "User",
      "created_at": "2025-05-20T19:21:47Z",
      "updated_at": "2025-06-01T13:24:19Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1783/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1783",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1783",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:37.751193",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "What do you think we should do to resources? Just add them to the context in all cases?",
          "created_at": "2025-05-26T20:17:12Z"
        },
        {
          "author": "BrandonShar",
          "body": "FWIW, I think adding them all to context all the time makes the most sense. I like the idea behind requesting them as-needed, but that blurs with tool-usage so much in practice that I think I would struggle to pick between them. ",
          "created_at": "2025-05-29T23:36:36Z"
        },
        {
          "author": "akhilbatra-angelone",
          "body": "Resource calls can be parameterised for HTTP API integrations with GET methods. Client/Agent may want to filter resource on the params. Might not be a good idea to put all of it in context.\n\nFrameworks like FastMCP consume OPENAPI specs and put all GET endpoints to resources by default unless custom",
          "created_at": "2025-06-01T13:24:18Z"
        }
      ]
    },
    {
      "issue_number": 1874,
      "title": "Unexpected part error when Google model returns empty text delta",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen streaming an agent's events by following the instructions [here](https://ai.pydantic.dev/agents/#streaming), I get the following error when using a Google model:\n\n```\nUnexpected part: video_metadata=None thought=None inline_data=None file_data=None code_execution_result=None executable_code=None function_call=None function_response=None text=''\n```\n\nIt looks like the model returns an empty text delta event, and the `if part.text:` check [here](https://github.com/pydantic/pydantic-ai/blob/v0.2.12/pydantic_ai_slim/pydantic_ai/models/google.py#L402) evaluates to false. Changing this to `if part.text is not None:` should resolve the issue. If this is a welcome change I'm happy to open a PR for this. [Line 450](https://github.com/pydantic/pydantic-ai/blob/v0.2.12/pydantic_ai_slim/pydantic_ai/models/google.py#L450) in the same file has the same `if part.text:` check which might need the same treatment.\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.12\nPydantic 0.2.12\n```",
      "state": "closed",
      "author": "Blue9",
      "author_type": "User",
      "created_at": "2025-05-30T18:17:10Z",
      "updated_at": "2025-06-01T11:29:53Z",
      "closed_at": "2025-06-01T11:29:53Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1874/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1874",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1874",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:38.027040",
      "comments": []
    },
    {
      "issue_number": 58,
      "title": "Vector search and embeddings API",
      "body": "Currently we don't have anything, and the [RAG example](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_examples/rag.py) just uses OpenAI's plain API to generate embeddings.\r\n\r\nIt seems simple enough to add a dedicated API to models to generate embeddings, would wouldn't provide much on top of what the OpenAI SDK already offers, but would help a lot with Gemini where there's currently not interface in what we have.\r\n\r\nI suspect that the vector search part is harder to provide an API for: anything beyond toy examples will require full control of the database being searched, and we're not (yet) building an ORM.\r\n\r\nAm I wrong or missing something?",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2024-11-18T09:30:48Z",
      "updated_at": "2025-05-31T14:47:27Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/58/reactions",
        "total_count": 30,
        "+1": 26,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 4,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/58",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/58",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:38.027057",
      "comments": [
        {
          "author": "davidhewitt",
          "body": "Can the `retriever` pattern be reused for vector search? ",
          "created_at": "2024-11-18T10:18:14Z"
        },
        {
          "author": "samuelcolvin",
          "body": "> Can the `retriever` pattern be reused for vector search?\r\n\r\n`tool` calls already work well for vector search, see https://ai.pydantic.dev/examples/rag/.\r\n\r\nThe main thing we can add is a model agnostic interface for creating embeddings.",
          "created_at": "2024-12-04T14:08:31Z"
        },
        {
          "author": "mertbozkir",
          "body": "Hey Samuel, is there any possibility that instead of agent.tools, pydantic-ai has proper API integration for vector db? so it would be much easier for the users to use retriever pattern. cc: @stephen37",
          "created_at": "2025-01-07T11:24:36Z"
        },
        {
          "author": "jinglesthula",
          "body": "Is there a possibility of having something like a standard API for vector search and then people can build adapters for specific vector db implementations?  Perhaps a reference implementation for one or two of the more popular FOSS vector dbs could be included in PydanticAI to begin with, with commu",
          "created_at": "2025-01-07T23:07:36Z"
        },
        {
          "author": "izzyacademy",
          "body": "Here is a draft suggestion for the Vector Store APIs. \n\nIt could be a separate github and pypi project [pydantic-ai-vectorstores] with the ABCs in the code pydantic-ai so that implementers can extend the interface and implement vector store specific integrations that meet the minimum threshold speci",
          "created_at": "2025-01-20T13:51:06Z"
        }
      ]
    },
    {
      "issue_number": 1663,
      "title": "ModelRetry not support stream output",
      "body": "### Question\nQwen3 thinking mode only support stream model\n\nI find ModelRetry not support stream output\nIs there any other way to support both stream output and ModelDelay simultaneously\n```\nfrom typing import Union\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\nfrom jsform_ai.llm import base_model\n\n\n\n\nclass InvalidRequest(BaseModel):\n    error_message: str\n\n\nOutput = Union[str, InvalidRequest]\nagent: Agent[dict,Output] = Agent(\n    base_model,\n    # output_type=str,  # type: ignore\n    deps_type=dict,\n    system_prompt='you are a helpful assistant.',\n    retries=3\n)\n\n\n@agent.output_validator\nasync def validate_res(ctx: RunContext[dict], output: str) -> Output:\n    if isinstance(output, InvalidRequest):\n        return output\n    print(f\"output:{output}\")\n    num = ctx.deps[\"num\"]\n    if str(num) not in output:\n        print(f\"num:{num} not in output:{output}\")\n        ctx.deps[\"num\"] += 1\n        raise ModelRetry(f'error')\n    return output\n\ndef main1():\n    result = agent.run_sync(\n        '猜我手里有几个苹果，1-10，猜错了就继续猜，直到猜对为止', deps={\"num\":1},\n       \n    )\n    print(result.output)\n\nasync def main_stream():\n    async with agent.run_stream(\n        '猜我手里有几个苹果，1-10，猜错了就继续猜，直到猜对为止', deps={\"num\":1},\n    ) as result:\n        async for message in result.stream_text():  \n            print(message)\n\n\n\nasync def main():\n    result = await agent.run(\n        '猜我手里有几个苹果，1-10，猜错了就继续猜，直到猜对为止', deps={\"num\":1},\n    )\n    print(result.output)\n    \n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main_stream())\n    # main1()\n",
      "state": "closed",
      "author": "tianshangwuyun",
      "author_type": "User",
      "created_at": "2025-05-08T05:50:36Z",
      "updated_at": "2025-05-31T14:00:37Z",
      "closed_at": "2025-05-31T14:00:36Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1663/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1663",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1663",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:38.253800",
      "comments": [
        {
          "author": "DouweM",
          "body": "@tianshangwuyun Can you please try the `iter`-based streaming approach in https://ai.pydantic.dev/agents/#streaming and see if it works as expected there?\n\nIf not, are you saying that the output validator is not being called at all, or that the `ModelRetry` error is raised but not being handled corr",
          "created_at": "2025-05-08T11:05:51Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-15T14:00:36Z"
        },
        {
          "author": "tianshangwuyun",
          "body": "the output validator  is called  and the ModelRetry error is raised ,but error is raised but not being handled correctly by PydanticAI to trigger the model to retry  \n\n```\n_output.py\", line 66, in validate\n    raise ToolRetryError(m) from r\npydantic_ai._output.ToolRetryError",
          "created_at": "2025-05-16T01:43:20Z"
        },
        {
          "author": "DouweM",
          "body": "@tianshangwuyun Did you have a chance to try the `iter` based approach I mentioned in my previous comment? If that has the same error, can you please share the updated example code?",
          "created_at": "2025-05-19T19:28:37Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-27T14:00:42Z"
        }
      ]
    },
    {
      "issue_number": 1721,
      "title": "Intended way to test when agents are dynamically created?",
      "body": "### Question\n\nHi! If there's no agent to import and override as it's being created dynamically in runtime, what is the intended way to test it? \nSure, there's a monkeypatch/patch available always, but I was wondering if there's a nice intended way to do it.\nAll the examples in docs show the following line:\n```from weather_app import weather_agent```\nMany runtimes wouldnt't have it as the agent doesn't exist until the runtime.\n\nIs there some kind of global `Agent. override` ?\n\n### Additional Context\n\n\"pydantic-ai==0.2.0\"\nPython 3.12.0",
      "state": "closed",
      "author": "stepacool",
      "author_type": "User",
      "created_at": "2025-05-14T07:34:01Z",
      "updated_at": "2025-05-31T14:00:35Z",
      "closed_at": "2025-05-31T14:00:35Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1721/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1721",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1721",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:38.504331",
      "comments": [
        {
          "author": "DouweM",
          "body": "@stepacool If you can build your `agent` at runtime, can't you also call `agent.override` or pydantic_evals at runtime? It'd be useful to understand a bit better what you're hoping to test and what your code looks like today.",
          "created_at": "2025-05-19T21:29:04Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-27T14:00:41Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-31T14:00:34Z"
        }
      ]
    },
    {
      "issue_number": 1083,
      "title": "UsageLimitExceeded output",
      "body": "### Description\n\nCurently when you encouter a `UsageLimitExceeded` error you get a message like this `pydantic_ai.exceptions.UsageLimitExceeded: The next request would exceed the request_limit of 50`\n\nIt would be helpful if the agent could still produce an output with the given requests on a best effort basis. Is something like this already possible? If not, this would be useful.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "LouisDeconinck",
      "author_type": "User",
      "created_at": "2025-03-09T16:48:21Z",
      "updated_at": "2025-05-31T08:36:47Z",
      "closed_at": null,
      "labels": [
        "usage"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1083/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1083",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1083",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:38.767911",
      "comments": [
        {
          "author": "LouisDeconinck",
          "body": "I've been thinking about how we could handle the `UsageLimitExceeded` exception more gracefully, so users get something useful instead of just an error. Here are three suggestions:\n\n**1. Output the Conversation History**\n\n*   What: Attach the full list of messages (user inputs and agent responses) t",
          "created_at": "2025-03-14T07:08:44Z"
        },
        {
          "author": "yrangana",
          "body": "I am having the same issue. If we can make a final LLM call when `UsageLimitExceeded` with the current history/documents, that would be ideal.",
          "created_at": "2025-03-24T05:17:36Z"
        },
        {
          "author": "dfreeman06",
          "body": "Similar issue here. Having access to history on exceptions would be useful to propagate. ",
          "created_at": "2025-03-26T19:58:33Z"
        },
        {
          "author": "mkay1375",
          "body": "We've overcome this issue by patching pydantic AI code using wrapt until the clean solution is implemented:\n\n```python\nfrom pydantic_ai import UsageLimitExceeded\nfrom pydantic_ai._agent_graph import GraphAgentState\nfrom pydantic_graph import GraphRunContext\nfrom wrapt import wrap_function_wrapper\n\n\n",
          "created_at": "2025-05-31T08:36:46Z"
        }
      ]
    },
    {
      "issue_number": 1429,
      "title": "For list-like `result_models`, LLM tries to call `final_result` multiple times",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen `result_model` is primarily a list of results, we see the models often try to call `final_result` multiple times, once with each of the results, instead of once with the list of results. We've observed this behavior across several models (gpt-4o, claude-sonnet-3.5). See the example code provided.\n\n\nWe've tried the obvious things:\n\n1. Adding an instruction to the prompt (`Call the final_result tool exactly once with the list of scores.`). This reduces but doesn't eliminate the problem\n2. Adding result validators and giving the model a second chance. Again this improves the rate of success but doesn't eliminate the problem — and exactly what the model is confused about isn't quite clear\n\nI'm not sure if this is truly \"bug\" with Pydantic AI, except to the extent that I think moving to native structured outputs for models that support it vs tool calling would probably fix it, unless there is something to fix with respect to the tool call schema that would clarify things for the models.\n\nIf I'm doing something obviously wrong, or there's a better workaround I didn't think of, maybe this becomes a documentation thing.\n\nThanks in advance!\n\n\n\n### Example Code\n\n```Python\nfrom typing import Literal\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.settings import ModelSettings\n\n\nclass Sentiment(BaseModel):\n    number: int\n    explanation: str\n    emotion: Literal[\"happy\", \"sad\", \"angry\", \"neutral\"]\n\n\nclass TextSentiments(BaseModel):\n    sentiments: list[Sentiment]\n\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    system_prompt=\"Given a list of text strings, you analyze the sentiment for each.\",\n    result_type=TextSentiments,\n    model_settings=ModelSettings(\n        temperature=0.0,\n    ),\n)\n\nresults = agent.run_sync(\n    \"\"\"1. I'm happy\n    2. I'm sad\n    3. I'm angry\n    4. I'm neutral\"\"\"\n)\n```\n\nWe only see one result:\n```\nprint(result)\n> AgentRunResult(\n    data=TextSentiments(\n        sentiments=[\n            Sentiment(\n                number=1,\n                explanation=\"The text expresses a positive emotion of happiness.\",\n                emotion=\"happy\",\n            )\n        ]\n    )\n)\n```\n\nIf we inspect the messages, we see that the model misunderstood how to return the result:\n```\nprint(results.all_messages)\n> [\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name=\"final_result\",\n                args='{\"sentiments\": [{\"number\": 1, \"explanation\": \"The text expresses a positive emotion of happiness.\", \"emotion\": \"happy\"}]}',\n                tool_call_id=\"call_BiK6SYUkDn0hNwy0tP6fKLUl\",\n                part_kind=\"tool-call\",\n            ),\n            ToolCallPart(\n                tool_name=\"final_result\",\n                args='{\"sentiments\": [{\"number\": 2, \"explanation\": \"The text expresses a negative emotion of sadness.\", \"emotion\": \"sad\"}]}',\n                tool_call_id=\"call_oiDX6CQ12vKN62ba15FE4JTV\",\n                part_kind=\"tool-call\",\n            ),\n            ToolCallPart(\n                tool_name=\"final_result\",\n                args='{\"sentiments\": [{\"number\": 3, \"explanation\": \"The text expresses a negative emotion of anger.\", \"emotion\": \"angry\"}]}',\n                tool_call_id=\"call_7ivwxOTvb2tjtWIsoLE8g2wQ\",\n                part_kind=\"tool-call\",\n            ),\n            ToolCallPart(\n                tool_name=\"final_result\",\n                args='{\"sentiments\": [{\"number\": 4, \"explanation\": \"The text expresses a neutral emotion without any strong feelings.\", \"emotion\": \"neutral\"}]}',\n                tool_call_id=\"call_5DaaUKt6aQHEMeblYBDDzGuX\",\n                part_kind=\"tool-call\",\n            ),\n        ],\n        model_name=\"gpt-4o-2024-08-06\",\n        timestamp=datetime.datetime(2025, 4, 9, 17, 29, 4, tzinfo=datetime.timezone.utc),\n        kind=\"response\",\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name=\"final_result\",\n                content=\"Final result processed.\",\n                tool_call_id=\"call_BiK6SYUkDn0hNwy0tP6fKLUl\",\n                timestamp=datetime.datetime(\n                    2025, 4, 9, 17, 29, 5, 792471, tzinfo=datetime.timezone.utc\n                ),\n                part_kind=\"tool-return\",\n            ),\n            ToolReturnPart(\n                tool_name=\"final_result\",\n                content=\"Result tool not used - a final result was already processed.\",\n                tool_call_id=\"call_oiDX6CQ12vKN62ba15FE4JTV\",\n                timestamp=datetime.datetime(\n                    2025, 4, 9, 17, 29, 5, 792477, tzinfo=datetime.timezone.utc\n                ),\n                part_kind=\"tool-return\",\n            ),\n            ToolReturnPart(\n                tool_name=\"final_result\",\n                content=\"Result tool not used - a final result was already processed.\",\n                tool_call_id=\"call_7ivwxOTvb2tjtWIsoLE8g2wQ\",\n                timestamp=datetime.datetime(\n                    2025, 4, 9, 17, 29, 5, 792480, tzinfo=datetime.timezone.utc\n                ),\n                part_kind=\"tool-return\",\n            ),\n            ToolReturnPart(\n                tool_name=\"final_result\",\n                content=\"Result tool not used - a final result was already processed.\",\n                tool_call_id=\"call_5DaaUKt6aQHEMeblYBDDzGuX\",\n                timestamp=datetime.datetime(\n                    2025, 4, 9, 17, 29, 5, 792482, tzinfo=datetime.timezone.utc\n                ),\n                part_kind=\"tool-return\",\n            ),\n        ],\n        kind=\"request\",\n    ),\n]\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.0.55\n```",
      "state": "open",
      "author": "kwood",
      "author_type": "User",
      "created_at": "2025-04-09T17:53:47Z",
      "updated_at": "2025-05-30T22:19:15Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1429/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1429",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1429",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:38.995247",
      "comments": [
        {
          "author": "marcoeg",
          "body": "I see the same behavior. A prompt with tool use to the pre-defined tool with empty argument  -  `final_result` tool that breaks the agent. \n```\n  {'role': 'assistant', 'content': [{'id': 'toolu_01MKTUTwWxkRhdTCq9RQ34Rz', 'type': 'tool_use', 'name': 'final_result', 'input': {}}]}\n```",
          "created_at": "2025-04-16T16:44:16Z"
        },
        {
          "author": "kwood",
          "body": "Yes, specifically with Anthropic models, I'm seeing them return empty result no matter how much prompt engineering I try.\n\nI don't see this behavior when using the Anthropic SDK — though I'm not sure if it's because the schema I provide is \"simpler\" (i.e., handwritten, without `$defs`) or it's becau",
          "created_at": "2025-04-18T14:23:59Z"
        },
        {
          "author": "prescod",
          "body": "Here's my example of the same problm:\n\n```python\nimport asyncio\nfrom pydantic import BaseModel\nfrom typing import List\nfrom pydantic_ai import Agent\n\n\nclass CapitalEntry(BaseModel):\n    country: str\n    capital: str\n\n\nclass CapitalsResponse(BaseModel):\n    result: List[CapitalEntry]\n\n\nasync def main",
          "created_at": "2025-05-30T22:18:11Z"
        },
        {
          "author": "prescod",
          "body": "Could the textual description of the `final_result` tool be tweaked to discourage this?",
          "created_at": "2025-05-30T22:19:13Z"
        }
      ]
    },
    {
      "issue_number": 1854,
      "title": "Anthropic models failing by injecting phantom empty message",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen running Anthropic models with Pydantic AI, jobs consistently fail around message 10 due to this error: `'message': 'messages.12: all messages must have non-empty content except for the optional final assistant message'`. \n\nAll of our messages have content in the Pydantic array, but when Pydantic AI converts to the Anthropic array, it injects additional empty messages - sometimes 2 or more - into the new array. In addition, message 12 in the Pydantic array is often mislabeled as a neighbor (see truncated json), meaning message 12 in neither array is actually empty - despite the error saying it is.\n\nIncluded are these two arrays from a failing job - they have been truncated to address the \"message 12\" error. Notice the mismatch of indexes within each array, as well as the presence of a phantom empty \"user\" message in the Anthropic array. Both message arrays were grabbed from the \"Raw Data\" tab in Logfire.\n[truncated_anthropic_messages.json](https://github.com/user-attachments/files/20495275/truncated_anthropic_messages.json)\n[truncated_pydantic_messages.json](https://github.com/user-attachments/files/20495276/truncated_pydantic_messages.json)\n\n### Example Code\n\n```Python\nNot applicable - data mismatch in Pydantic AI\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython = 3.13.1\nPydantic AI = 0.2.11\nLLM Client = claude-3-7-sonnet-20250219\n```",
      "state": "closed",
      "author": "nbenedefinite",
      "author_type": "User",
      "created_at": "2025-05-28T21:37:59Z",
      "updated_at": "2025-05-30T16:47:04Z",
      "closed_at": "2025-05-30T16:46:50Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1854/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1854",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1854",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:39.262800",
      "comments": [
        {
          "author": "DouweM",
          "body": "@nbenedefinite This was supposed to be fixed in https://github.com/pydantic/pydantic-ai/pull/1027 which was released in v0.2.11. Can you please double check that you're on that version?",
          "created_at": "2025-05-29T14:35:48Z"
        },
        {
          "author": "nbenedefinite",
          "body": "Yes, this update fixed it. Thank you!",
          "created_at": "2025-05-30T16:46:47Z"
        }
      ]
    },
    {
      "issue_number": 1777,
      "title": "How to force a tool call in a run?",
      "body": "### Question\n\nHow can I force the use of a tool in a specific run of the Agent? \n(or is there a better approach than forcing a tool call?)\n\nAs an example in [LiteLLM](https://docs.litellm.ai/docs/completion/input) it is possible to pass `tool_choice` to `completion`.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "simone-gaiarin",
      "author_type": "User",
      "created_at": "2025-05-20T08:48:40Z",
      "updated_at": "2025-05-30T14:13:44Z",
      "closed_at": "2025-05-30T14:13:42Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1777/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1777",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1777",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:39.467532",
      "comments": [
        {
          "author": "DouweM",
          "body": "@simone-gaiarin The way in PydanticAI to force the model to take a specific action to end the run is using `output_type`: https://ai.pydantic.dev/output/. Right now, that can only be a type, which could be a Pydantic model that you'll use as input for a function afterwards. In https://github.com/pyd",
          "created_at": "2025-05-21T15:32:52Z"
        },
        {
          "author": "simone-gaiarin",
          "body": "Thanks, that makes sense and looks like a clean way to force tool usage.\n\nOne thing I was trying to reconcile is the recommendation to instantiate agents once and reuse them (e.g., as module-level singletons). That makes sense for general use, especially when the agent has access to multiple tools a",
          "created_at": "2025-05-26T09:51:11Z"
        },
        {
          "author": "DouweM",
          "body": "> However, in chat-style apps, it might be useful to let users explicitly force a specific tool—for example, \"search the web for this.\" With LiteLLM, this is straightforward since completions are stateless and you can pass tool_choice per call.\n\n@simone-gaiarin That sounds more like https://github.c",
          "created_at": "2025-05-26T18:21:15Z"
        },
        {
          "author": "simone-gaiarin",
          "body": "Hi, #1820 is exactly what I was asking. I'll reply there.",
          "created_at": "2025-05-30T14:13:42Z"
        }
      ]
    },
    {
      "issue_number": 1561,
      "title": "Inferred strict=true may cause compatibility issues with OpenAI-compatible servers",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n## 🐛 Bug Description\nWhen defining tools with required parameters using the MCP Server. the pydantic-ai  automatically infers and inserts \"strict\": true into the request, even if the original tool definition explicitly sets strict to null.\n\nThis behavior causes compatibility issues with some OpenAI-compatible servers that do not support the strict field, as the pydantic-ai automatically infers and adds **strict: true**.\n\n## 🔍 Tool Registration JSON Example\nHere is the actual JSON representation of a registered tool:\n```json\n[\n  {\n    \"name\": \"test\",\n    \"description\": \"test required\",\n    \"parameters_json_schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"some\": {\n          \"type\": \"string\",\n          \"description\": \"anything\"\n        }\n      },\n      \"required\": [\"some\"],\n      \"additionalProperties\": false,\n      \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n    },\n    \"outer_typed_dict_key\": null,\n    \"strict\": null\n  }\n]\n```\n\n## ❌ Actual Request Payload\n\nDespite the tool definition having strict as null, the request payload includes the inferred strict: true:\n```json\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"hello\" }],\n  \"model\": \"qwen/qwq-32b\",\n  \"n\": 1,\n  \"stream\": true,\n  \"stream_options\": { \"include_usage\": true },\n  \"tool_choice\": \"auto\",\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"test\",\n        \"description\": \"test required\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"some\": {\n              \"type\": \"string\",\n              \"description\": \"anything\"\n            }\n          },\n          \"required\": [\"some\"],\n          \"additionalProperties\": false\n        },\n        \"strict\": true  // ← Automatically inferred as true despite being null in the original definition\n      }\n    }\n  ]\n}\n```\nThis request will cause compatibility issues with OpenAI-compatible servers that do not support the strict field.\n\n<img width=\"856\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/07cb55d0-61cd-4714-b2a9-93ee9f78fbc2\" />\n\n\n\n\n### Example Code\n\n```Python\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\n\nimport { z } from \"zod\";\n\n// Create an MCP server\nconst server = new McpServer({\n  name: \"MCP Test\",\n  version: \"1.0.0\",\n});\n\nserver.tool(\"test\", \"test required\", {\n  some: z.string().describe(\"anything\")\n}, async () => {\n  return {\n    content: []\n  };\n});\n\nconst transport = new StdioServerTransport();\n\nserver.connect(transport);\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython==3.13\npydantic-ai==0.1.3\n```\n\nNote: This issue was drafted and refined with the help of ChatGPT.\n\n",
      "state": "closed",
      "author": "chizukicn",
      "author_type": "User",
      "created_at": "2025-04-21T14:26:53Z",
      "updated_at": "2025-05-30T09:40:40Z",
      "closed_at": "2025-05-26T23:30:01Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1561/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1561",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1561",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:39.687752",
      "comments": [
        {
          "author": "DouweM",
          "body": "@chizukicn Thanks for raising this. This is really part of the bigger issue where \"OpenAI-compatible\" APIs are really only compatible with specific (older) _versions_ of the OpenAI API, and even then they may behave differently on the details. Our `OpenAIModel` always targets the latest version of t",
          "created_at": "2025-04-21T18:09:19Z"
        },
        {
          "author": "DouweM",
          "body": "@chizukicn I'm gonna look into fix this -- just for completeness, which OpenAI-compatible API were you using here? I can see the model is Qwen but not the provider.",
          "created_at": "2025-05-20T15:06:21Z"
        },
        {
          "author": "chizukicn",
          "body": "> [@chizukicn](https://github.com/chizukicn) I'm gonna look into fix this -- just for completeness, which OpenAI-compatible API were you using here? I can see the model is Qwen but not the provider.\n\nThanks for looking into this! I'm using the OpenAI-compatible API provided by a Chinese cloud servic",
          "created_at": "2025-05-21T02:56:17Z"
        },
        {
          "author": "chizukicn",
          "body": "@DouweM  What I find a bit hard to understand is that, based on packet capture, when there is a required parameter in the MCP tool, it includes the argument `strict=True`; however, if the parameter is optional, the `strict` argument is not present. ",
          "created_at": "2025-05-23T08:46:37Z"
        },
        {
          "author": "DouweM",
          "body": "@chizukicn See here: https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py#L1014. OpenAI only supports strict mode when there are no optional fields.",
          "created_at": "2025-05-23T13:40:57Z"
        }
      ]
    },
    {
      "issue_number": 1869,
      "title": "TypeError: unhashable type: 'list'",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n![Image](https://github.com/user-attachments/assets/48e84416-d8c4-4b95-b9e3-2c064edfdb0f)\nWhy does the error 'TypeError: unhashable type:'list' 'occur even with this line of code\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.10.16\n\nPydantic 2.11.5\n\nPydanticAI 0.2.9\n```",
      "state": "open",
      "author": "Aahoneybee",
      "author_type": "User",
      "created_at": "2025-05-30T02:42:14Z",
      "updated_at": "2025-05-30T08:44:22Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1869/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1869",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1869",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:39.938432",
      "comments": [
        {
          "author": "Kludex",
          "body": "Can you bump PydanticAI? I can't see any reason why this would be happening from a simple look. I can't reproduce it.",
          "created_at": "2025-05-30T08:44:21Z"
        }
      ]
    },
    {
      "issue_number": 732,
      "title": "Graph run_stream",
      "body": "Hey,\n\nGraphs may take long time to execute, especially when dealing with multi agents with iterations,\n\n\nWould love to see streaming graph execition\n\nEffects #704 #695 \n\n",
      "state": "closed",
      "author": "asaf",
      "author_type": "User",
      "created_at": "2025-01-21T17:46:34Z",
      "updated_at": "2025-05-30T01:31:41Z",
      "closed_at": "2025-02-28T13:34:58Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/732/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/732",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/732",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:40.171953",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "I think you want [`Graph.next`](https://ai.pydantic.dev/graph/#custom-control-flow).\n\nIf you want to implement streaming within a node, you can just implement that as a new method on the node. I'll try to add an example soon.",
          "created_at": "2025-01-21T19:01:45Z"
        },
        {
          "author": "dmontagu",
          "body": "Also, if you want within-node streaming, I think our current plan is to combine the `Graph.next` referenced by Samuel above with a user-defined method on the node that does streaming, and then manually handling the streaming in the code that loops over the nodes via `Graph.next`. I am planning to ta",
          "created_at": "2025-01-24T00:04:54Z"
        },
        {
          "author": "asaf",
          "body": "I guess having `stream()` on the graph level could be nice but raises more complexity such when graphs should be interrupted (e.g., when human in the loop is required), so I see your point of the Graph.next.\n\nNext() moves the execution flow entirely to the control of the user, which probably would w",
          "created_at": "2025-01-24T19:48:29Z"
        },
        {
          "author": "dmontagu",
          "body": "Yes, this is precisely what we had in mind. It's reassuring to see someone else independently working out the details in the same way!\n\nIf you run into any issues achieving your goals while taking this approach, we'll definitely want to address them — please let us know.",
          "created_at": "2025-01-24T19:50:33Z"
        },
        {
          "author": "asaf",
          "body": "I see your point :)\nI'll keep this issue opened as a reminder to create a graph sample with streaming! 10x guys!",
          "created_at": "2025-01-24T20:01:04Z"
        }
      ]
    },
    {
      "issue_number": 1852,
      "title": "Allow pydantic-evals LLMJudge to see expected_output values",
      "body": "### Description\n\nFirst of all, thanks for building such a useful project!\n\nI have a suggestion for an extension to the `LLMJudge` evaluator in pydantic-evals: I think it would be useful to have the option to pass in the `ReportCase.expected_output` value during evaluation.\n\nMy use case involves evaluating an agent on its ability to extract factual information from text: I would like to be able to have evaluation cases that look something like:\n```\n{\n  \"name\": \"Contract Term\",\n  \"inputs\": {\n    \"question\": \"What is the term (in months or years) of the contract?\",\n    \"contract_id\": \"some_id_value\"\n  },\n  \"expected_output\": \"5 years\"\n},\n```\nand have outputs of `5 years` and `60 months` both be marked correct. (I realize I could specify the desired unit in the prompt in this case, but I have other cases which don't lend themselves to strict output formatting / unit normalization.)\n\nWith this extension I could evaluate against this case with something like\n```\nllm_judge = LLMJudge(\n    rubric=(\n        \"The generated answer should factually agree with the expected output, but does not need to be exactly the same. \"\n    ),\n    include_input=True,\n    include_expected_output=True,\n)\n```\n\nThanks!\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "hinnefe2",
      "author_type": "User",
      "created_at": "2025-05-28T19:34:11Z",
      "updated_at": "2025-05-29T19:44:26Z",
      "closed_at": "2025-05-29T19:44:26Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1852/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1852",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1852",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:40.427989",
      "comments": []
    },
    {
      "issue_number": 1856,
      "title": "Feature request: ability to run evaluators against only a subset of the data",
      "body": "@DouweM and I discussed this, we want something that lets you select a specific field of the data in the EvaluatorContext before applying some existing evaluator (like Equals or IsInstance or LLMJudge). Here's some essentially-pseudocode that maybe conveys what I had in mind:\n\n```python\nfrom dataclasses import replace, dataclass\nfrom typing import Callable, Any\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.evaluators import Equals, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass Selector:\n    attributes: list[str]\n    \n    def __call__(self, value: Any) -> Any:\n        \"\"\"Selects the attribute from the value based on the attributes list.\"\"\"\n        for attr in self.attributes:\n            if isinstance(value, dict):\n                value = value.get(attr)\n            else:\n                value = getattr(value, attr, None)\n        return value\n\nclass SelectorEvaluator[InputT, OutputT, MetadataT](Evaluator[InputT, OutputT, MetadataT]):\n    input_selector: Selector | Callable[[InputT], Any] | None = None\n    output_selector: Selector | Callable[[OutputT], Any] | None = None\n    metadata_selector: Selector | Callable[[MetadataT | None], Any] | None = None\n\n    evaluator: Evaluator[Any, Any, Any]\n\n    def evaluate(self, ctx: EvaluatorContext[InputT, OutputT, MetadataT]):\n        if self.input_selector is not None:\n            ctx = replace(ctx, inputs=self.input_selector(ctx.inputs))\n        if self.output_selector is not None:\n            ctx = replace(ctx, output=self.output_selector(ctx.output), expected_output=None if ctx.expected_output is None else self.output_selector(ctx.expected_output))\n        if self.metadata_selector is not None and ctx.metadata is not None:\n            ctx = replace(ctx, metadata=self.metadata_selector(ctx.metadata))\n        return self.evaluator.evaluate(ctx)\n```",
      "state": "open",
      "author": "dmontagu",
      "author_type": "User",
      "created_at": "2025-05-28T22:43:54Z",
      "updated_at": "2025-05-29T19:28:24Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1856/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1856",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1856",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:40.428013",
      "comments": [
        {
          "author": "DouweM",
          "body": "@dmontagu Looks like you already started building something like this for LLMJudge specifically in https://github.com/pydantic/pydantic-ai/pull/1433!",
          "created_at": "2025-05-29T19:28:24Z"
        }
      ]
    },
    {
      "issue_number": 1859,
      "title": "`new_messages_json` gives error `PydanticSerializationError: Unable to serialize unknown type: <class 'ValueError'>`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI've got new messages from agent response\nOne of them has ValueError raised by my pydantic model during agent tried to initialize that model\nI want to save that message to user history in db\nBut when I try to dump json messages I get an error\n\n\n### Example Code\n\n```Python\n>>> response.new_messages()\n[\n    ModelRequest(parts=[UserPromptPart(content=\"Sure, it's eur. I new used one and dont need any room specified\", ...)]),\n    ModelResponse(parts=[ToolCallPart(tool_name='final_result', args='{  \"query_params\": {},  \"body\": {    \"country_id\": 499,    \"sale_price\": {      \"range\": null,      \"exact_value\": null,      \"approximate_value\": 50000000    },    \"total_area\": {      \"range\": null,      \"exact_value\": null,      \"approximate_value\": null    },    \"bedrooms\": null,    \"bathrooms\": null,    \"condition_types\": [\"used\"]  }}', ...),\n    ModelRequest(parts=[RetryPromptPart(content=[{'type': 'value_error', 'loc': ('body', 'total_area'), 'msg': 'Value error, Exactly one value must be not null.. No value is not null in model.', 'input': {'range': None, 'exact_value': None, 'approximate_value': None}, 'ctx': {'error': ValueError('Exactly one value must be not null.. No value is not null in model.')}}], tool_name='final_result', ...),\n    ModelResponse(parts=[TextPart(content=\"I apologize for the confusion earlier. Could you please specify some more details about the property you're interested in, such as its total area?\")], ...)\n]\n\n>>> json.loads(response.new_messages_json())\nFile \"/usr/local/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2164, in new_messages_json\n    return _messages.ModelMessagesTypeAdapter.dump_json(self.new_messages(output_tool_return_content=content))\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/pydantic/type_adapter.py\", line 631, in dump_json\n    return self.serializer.to_json(\n           ~~~~~~~~~~~~~~~~~~~~~~~^\n        instance,\n        ^^^^^^^^^\n    ...<11 lines>...\n        context=context,\n        ^^^^^^^^^^^^^^^^\n    )\n    ^\npydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'ValueError'>\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython3.13\npydantic-ai==0.2.11\n```",
      "state": "closed",
      "author": "v-anzhurov",
      "author_type": "User",
      "created_at": "2025-05-29T09:01:30Z",
      "updated_at": "2025-05-29T19:10:05Z",
      "closed_at": "2025-05-29T19:10:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1859/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1859",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1859",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:40.671035",
      "comments": [
        {
          "author": "DouweM",
          "body": "@v-anzhurov Thanks for the report, I'll have a look at fixing this later today.",
          "created_at": "2025-05-29T14:52:25Z"
        }
      ]
    },
    {
      "issue_number": 1808,
      "title": "Issues using non-\"OpenAIModel\" models with Azure AI Foundry",
      "body": "### Question\n\n✅  Question / request still persists on most up-to-date version of PydanticAI.\n✅   I checked open/closed issues and pull requests before opening this issue.\n\n## Problem description\nHey everyone! This is the first issue I'm opening, so pardon any rookie mistakes on the style / reporting side. \n\nWithin my application, I want to support various different LLMs hosted within Azure AI Foundry. For `OpenAIModels` (eg. gpt-4.1), this works really well following the examples from your [documentation page on Azure AI Foundry](https://ai.pydantic.dev/models/openai/#azure-ai-foundry).\n\nI am however having issues using other models. Most of my testing was on the `Mistral-Large-2411` model, so I'll just describe my issue implementing this model.\n\nI assumed that usage of `MistralModel` and `MistralProvider` are required for using this particular model. As a reference, I used the template from https://github.com/pydantic/pydantic-ai/pull/1617:\n\n```python\nmodel = MistralModel(\n    \"XXX-mistral-large-2411\",\n    provider=MistralProvider(\n            api_key=chat_settings.AZURE_MISTRAL_LARGE_KEY,\n            base_url=chat_settings.AZURE_MISTRAL_LARGE_ENDPOINT,\n        ),\n)\n```\n(*Notes: XXX is the project name which I redacted here; The endpoint, as well as model names are known-good and work in a separate implementation not based on PydanticAI.*)\n\nThe result to using it like this yields a `HTTPValidationError`:\n\n![Image](https://github.com/user-attachments/assets/28cce0cf-9775-47ef-96b9-ebbd238fe7aa)\n\n<details>\n<summary>Stack trace here</summary>\n\n<pre>\n<code>\nTraceback (most recent call last):\n  File \"/app/app/chat/router.py\", line 232, in ai_completion\n    await task\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 166, in run\n    async for _node in graph_run:\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"(...)/chat.py\", line 63, in run\n    async with chat_agent.run_stream(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 996, in run_stream\n    async with node._stream(graph_ctx) as streamed_response:  # pyright: ignore[reportPrivateUsage]\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 309, in _stream\n    async with ctx.deps.model.request_stream(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_ai/models/instrumented.py\", line 190, in request_stream\n    async with super().request_stream(\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_ai/models/wrapper.py\", line 36, in request_stream\n    async with self.wrapped.request_stream(messages, model_settings, model_request_parameters) as response_stream:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_ai/models/mistral.py\", line 169, in request_stream\n    response = await self._stream_completions_create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic_ai/models/mistral.py\", line 232, in _stream_completions_create\n    response = await self.client.chat.stream_async(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/mistralai/chat.py\", line 739, in stream_async\n    raise models.HTTPValidationError(data=data)\nmistralai.models.httpvalidationerror.HTTPValidationError: {}\n</code>\n</pre>\n\n</details>\n\n## Related issues\nThis request is similar to https://github.com/pydantic/pydantic-ai/issues/940 which has been closed. \n\n\n## Question\n- Am I using the model wrong here or is this a limitation of PydanticAI? \n- For the case of other models also available from Azure AI Foundry, for instance `llama-3-3-70B`, how am I able to find a reference on how to use them? I wasn't able to find a `LlamaModel` or `MetaModel` or something similar.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "schmidt-marvin",
      "author_type": "User",
      "created_at": "2025-05-22T13:52:22Z",
      "updated_at": "2025-05-29T14:06:38Z",
      "closed_at": "2025-05-26T17:30:14Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1808/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1808",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1808",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:40.907480",
      "comments": [
        {
          "author": "DouweM",
          "body": "> I assumed that usage of `MistralModel` and `MistralProvider` are required for using this particular model.\n\n@schmidt-marvin It's a bit confusing, but model classes implement specific APIs, and provider classes implement API base URLs and authentication. So in, your case, no matter which actual mod",
          "created_at": "2025-05-26T17:30:14Z"
        },
        {
          "author": "schmidt-marvin",
          "body": "Ah, I see - Thank you so much for the swift response and explanation!",
          "created_at": "2025-05-27T12:09:49Z"
        },
        {
          "author": "ttamg",
          "body": "@DouweM Related to this, am I right in understanding that the **Azure AI Inference** library which we are now starting to use instead of the OpenAI library with the AzureOpenAI provider is then a different `Provider` which is not yet implemented in Pydantic AI Agents?  \n\nThis python package seems to",
          "created_at": "2025-05-28T22:04:48Z"
        },
        {
          "author": "DouweM",
          "body": "@ttamg Using a different SDK would require a new Model class, which (as you can see from OpenAIModel) is quite a bit of work to build and maintain as we add new features to PydanticAI. So if a provider has an OpenAI-compatible API that works with the OpenAI SDK, we prefer using the existing OpenAIMo",
          "created_at": "2025-05-29T14:06:37Z"
        }
      ]
    },
    {
      "issue_number": 1857,
      "title": "OpenAI errors not handled - openai timeout error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nOpenAI errors not being catched by agent. It propagates up until the result.output as string\n\nexample response received\n```\nstatus_code: 400, model_name: gpt-4.1, body: {'message': 'Timeout while downloading https://some-image-url/image.pny', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_image_url'}\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai                              0.2.0\npython 3.12.8\nopenai                                   1.75.0\n\nmodel: gpt-4.1\n```",
      "state": "closed",
      "author": "ryan-aquino-tt",
      "author_type": "User",
      "created_at": "2025-05-29T05:15:49Z",
      "updated_at": "2025-05-29T06:19:15Z",
      "closed_at": "2025-05-29T06:19:14Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1857/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1857",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1857",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:41.153671",
      "comments": [
        {
          "author": "ryan-aquino-tt",
          "body": "Closing issue due to false alarm",
          "created_at": "2025-05-29T06:19:14Z"
        }
      ]
    },
    {
      "issue_number": 1855,
      "title": "Gemini API does not always return a parts, causing type validation error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nOn rare occasions, Gemini 2.5-flash does not return a **_parts_** field for one or more content chunks in its response.\nThis causes a type validation error as pydantic-ai treats the **_parts_** as a required property. In fact it is not required by google's own specification.\n\nWe have observed this with simple text input and **str** as the output model.\nWe are not using any tools, MCP servers etc. Essentially just a direct LLM call.\n\nThe resulting error is:\n ```\n    |               response = _gemini_response_ta.validate_json(data)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 446, in validate_json\n    |     return self.validator.validate_json(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    | pydantic_core._pydantic_core.ValidationError: 1 validation error for typed-dict\n    | candidates.0.content.parts\n    |   Field required [type=missing, input_value={'role': 'model'}, input_type=dict]\n```\n\nThe model causing the issue seems to be this:\n\n```\n\nclass _GeminiContent(TypedDict):\n    role: Literal['user', 'model']\n    parts: list[_GeminiPartUnion]\n\n```\n\nSpecifically the parts list is not optional, but when reading the documentation from Google's own python SDK we see:\n[Google Gemini SDK - types](https://github.com/googleapis/python-genai/blob/main/google/genai/types.py\n)\n[Google Gemini SDK - chats](https://github.com/googleapis/python-genai/blob/main/google/genai/chats.py)\n\n```\nclass Content(_common.BaseModel):\n  \"\"\"Contains the multi-part content of a message.\"\"\"\n\n  parts: Optional[list[Part]] = Field(\n      default=None,\n      description=\"\"\"List of parts that constitute a single message. Each part may have\n      a different IANA MIME type.\"\"\",\n  )\n  role: Optional[str] = Field(\n      default=None,\n      description=\"\"\"Optional. The producer of the content. Must be either 'user' or\n      'model'. Useful to set for multi-turn conversations, otherwise can be\n      empty. If role is not specified, SDK will determine the role.\"\"\",\n  )\n\n```\n\nWe can see that the parts are in fact optional and that Google's own SDK handles this case.\nThis error is really hard to re-produce because in most instances the model does actually return a parts list.\nBut when calling this same agent many times with sightly different input prompts, we occasionally get the error.\n\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.settings import ModelSettings\n\ndoc_summary_agent = Agent(\n    model=model_name.value,\n    output_type=str,\n    instructions=agent_instructions,\n)\n\ntry:\n    result = await asyncio.wait_for(\n        doc_summary_agent.run(\n            rephrased_question,\n            model_settings=ModelSettings(\n                temperature=0.0,\n                # General heuristic to cut off requests at same size of input\n                max_tokens=len(agent_instructions + rephrased_question)\n                // 4,\n            ),\n        ),\n        timeout=60.0,\n    )\nexcept ...\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nv0.2.11\n```",
      "state": "closed",
      "author": "NickG-NZ",
      "author_type": "User",
      "created_at": "2025-05-28T22:27:28Z",
      "updated_at": "2025-05-29T02:40:48Z",
      "closed_at": "2025-05-29T02:40:48Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1855/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1855",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1855",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:41.395577",
      "comments": [
        {
          "author": "fswair",
          "body": "https://github.com/pydantic/pydantic-ai/issues/631",
          "created_at": "2025-05-28T23:29:23Z"
        }
      ]
    },
    {
      "issue_number": 1844,
      "title": "Support Stateless MCP Server Connections",
      "body": "### Description\n\nWith the streaming http protocol, MCP servers now have the option to be stateless and not care about long-running sessions. If we don't need long-running session support, that also allows us to skip the initial connection to the MCP server that occurs in agent.run_mcp_servers() which is a little performance boost and reduces start-up coupling with any MCP servers.\n\nI'm handling this in a project right now by overriding the MCPServerHTTP's __aenter__ method and running everything except for the client initialize and set logging methods.\n\nI'm happy to put up a PR for this, but I wanted to see if you'd have interest in the idea first. I think the simplest way to handle it would be an additional argument on the MCPServerHTTP class called something like stateless or maybe something more obvious like skip_initial_connection.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "BrandonShar",
      "author_type": "User",
      "created_at": "2025-05-27T20:04:50Z",
      "updated_at": "2025-05-28T21:17:50Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1844/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1844",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1844",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:41.730012",
      "comments": [
        {
          "author": "DouweM",
          "body": "@BrandonShar Do I understand correctly that we'd basically always want this with Streamable HTTP servers, and we just need the current eager behavior for SSE servers? Ideally we'd be able to detect which the user is doing.",
          "created_at": "2025-05-28T14:53:34Z"
        },
        {
          "author": "BrandonShar",
          "body": "I _think_ we'd still need to support the initalization for protocol reasons (but maybe we could put off the initialization until the first request?). Streamable HTTP servers can optionally support the concept of \"state\" via a persistent session id that's passed each time by the client.\n\nThe [protoco",
          "created_at": "2025-05-28T15:26:16Z"
        },
        {
          "author": "maxonics",
          "body": "I tried to use a mcp server in stateless mode and indeed it doesn't work. Would be great to support it 🙂",
          "created_at": "2025-05-28T19:30:42Z"
        },
        {
          "author": "DouweM",
          "body": "@BrandonShar I haven't read up on the details here, but nothing you write sounds unreasonable and the demand is clearly there (hi @maxoniboo!) so if you could submit a PR and call out any decisions you made that we could go another direction on, that'd be great! I find it easier to reason about real",
          "created_at": "2025-05-28T21:01:30Z"
        },
        {
          "author": "BrandonShar",
          "body": "> I find it easier to reason about real code than protocol specs\n\nYou and me both! 😄 \n\nSounds good, I'll get something up in the next day or two.\n\nalso, @maxoniboo did you notice a specific error? It should work fine with stateless servers, it currently just makes some unnecessary extra calls for th",
          "created_at": "2025-05-28T21:17:49Z"
        }
      ]
    },
    {
      "issue_number": 1657,
      "title": "evals progress bar",
      "body": "### Description\n\nFor evals that take awhile, it would be nice to have a progress bar output progress.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "tekumara",
      "author_type": "User",
      "created_at": "2025-05-07T12:44:32Z",
      "updated_at": "2025-05-28T20:54:20Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "good first issue",
        "evals"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1657/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1657",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1657",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:41.975167",
      "comments": [
        {
          "author": "DouweM",
          "body": "@tekumara I agree -- we'd accept a PR to add this!",
          "created_at": "2025-05-08T08:23:03Z"
        },
        {
          "author": "davide-andreoli",
          "body": "Hello @DouweM, I've seen that this has not been implemented and I'd like to take it up if that's ok!",
          "created_at": "2025-05-28T17:36:15Z"
        },
        {
          "author": "DouweM",
          "body": "@davide-andreoli Go for it!",
          "created_at": "2025-05-28T20:54:19Z"
        }
      ]
    },
    {
      "issue_number": 1782,
      "title": "Add ModelProfile to let model-specific behaviors be configured independent of the model class",
      "body": "### Description\n\nModel classes were built for a specific API and model family, encoding not just the hard API spec but also various behaviors related to those specific models' capabilities and limitations, e.g. what JSON schemas or multi-modal input types are supported.\n\nThis breaks down when a model class is used with a different API or a different family of models that don't match all of those hard and soft assumptions:\n- `OpenAIModel` is used with various [ostensibly-OpenAI-compatible APIs](https://ai.pydantic.dev/models/openai/#openai-compatible-models) and a wide variety of models.\n- `BedrockModel` and `GroqModel` are used with a wide variety of models.\n\nSo far, the biggest class of issues (at least one filed a day) has been in JSON schema handling. `OpenAIModel` and `GeminiModel` both implement their own transformers ([OpenAI](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py#L1012), [Gemini](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/gemini.py#L795)), but people using `OpenAIModel` and `BedrockModel` with other models have been running into API errors when using particular models, even if others on the same provider may work fine (suggesting this really is model-specific, and not something that an OpenAI-compatible API \"should\" handle consistently across all models). Resolving this currently requires manually defining a model subclass and applying one of the existing JSON schema transformer, sometimes with tweaks:\n- `OpenAIModel` + Together.xyz + Qwen fails, but works with Llama https://github.com/pydantic/pydantic-ai/issues/1659\n- `OpenAIModel` + OpenRouter + Gemini fails https://github.com/pydantic/pydantic-ai/issues/1735\n- `BedrockModel` + Nova fails, but works with others https://github.com/pydantic/pydantic-ai/issues/1623\n\nWe're going to run into something similar with [Structured Output Modes](https://github.com/pydantic/pydantic-ai/issues/582#issuecomment-2748884106):\n- `OpenAIModel` + pre-4o doesn't support `json_schema`, only `json_object` (and tool calls and manual JSON)\n- Some models don't support `json_schema` or `json_object`, only tool calls or manual JSON\n- Some models don't support tool calls at all, only manual JSON\n- `GeminiModel` (and presumably `BedrockModel` + Gemini) doesn't support `json_schema` alongside tools\n\nBuilt-in tools may also be different between providers used with the same model class (e.g. `OpenAIModel` + OpenRouter), or between models on the same provider (as some models may not support tool calls at all):\n- OpenAI supports web search, file search, computer use https://platform.openai.com/docs/guides/tools\n- Anthropic supports web search https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/web-search-tool\n- Gemini supports code execution https://ai.google.dev/gemini-api/docs/code-execution\n\nThat's not the end of it, unfortunately, as we've already seen some other axes where different models may need different handling to get the most out of them:\n\n- Different models may need to be prompted differently to be most effective at outputting JSON https://github.com/pydantic/pydantic-ai/issues/582#issuecomment-2748884106\n- `BedrockModel` + some models don't support tool use https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html\n- `OpenAIModel` + PPInfra + Qwen doesn't support `strict` on tool definitions https://github.com/pydantic/pydantic-ai/issues/1561\n- `OpenAIModel` + Grok doesn't support `strict` on tool definitions https://github.com/pydantic/pydantic-ai/issues/1846\n- `BedrockModel` + Claude doesn't support tool choice https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/bedrock.py#L305\n- `BedrockModel` + Mistral (and others?) require tool result to be passed as an object instead of string https://github.com/pydantic/pydantic-ai/issues/1649\n- With model classes that cover many models, like `OpenAIModel` and `BedrockModel`, not all will support all multi-modal input types (video, audio, image, docs).\n- Claude doesn't natively do parallel tool calls and recommends providing an explicit `batch` tool https://github.com/pydantic/pydantic-ai/issues/1769\n\nI think it's time to pull some model and model family-specific details out of the model classes, generalize them, and allow them to be tweaked on a model-by-model basis.\nThis'll be somewhat similar to `ModelSettings`, but instead of properties to be passed directly to the model API, these new properties will determine how PydanticAI builds its request payload to get the most out of each specific model and work around limitations.\n\nThere'd be global defaults, model class/family defaults layered on top of that, model-specific overrides provided by the model class file, and the ability for users to tweak the settings further, or even use the settings defined by one model class (e.g. `GeminiModel`'s specification for 2.5 Pro) with another model class (like `OpenAIModel` + OpenRouter + Gemini).\n\nBecause we're basically describing how the model likes to be talked to, I'm leaning towards the name `ModelProfile` or `ModelSpec` or something similar -- but very open to other suggestions. \n\nIt'd look something like this:\n\n```python\n@dataclass\nclass ModelProfile:\n  json_schema_transformer: Literal['openai', 'gemini'] | type[WalkJsonSchema]\n  supported_output_modes: set[Literal['tool', 'json_schema', 'json_object', 'manual_json']]\n  default_output_mode: Literal['tool', 'json_schema', 'json_object', 'manual_json']\n\n  # definitely not all necessary right away, but to give you an idea\n  built_in_tools: dict[str, dict[str, Any]]\n  manual_json_prompt: str\n  tool_use: bool\n  strict_tools: bool\n  tool_choice: bool\n  tool_result_type: Literal['string', 'object']\n  multi_modal_input_types: set[Literal['video', 'audio', 'image', 'docs']]\n  offer_batch_tool: bool\n\n# models/__init__.py\nDEFAULT_PROFILE = ModelProfile(...)\n\n# models/openai.py\nDEFAULT_OPENAI_PROFILE = replace(DEFAULT_PROFILE, json_schema_transformer='openai', ...)\n\nOPENAI_PROFILES = {}\nOPENAI_PROFILES['gpt-4'] = replace(DEFAULT_OPENAI_PROFILE, supported_output_modes={'tool', 'json_object', 'manual_json'})\nOPENAI_PROFILES['gpt-4o'] = replace(OPENAI_PROFILES['gpt-4'], supported_output_modes={'tool', 'json_schema', 'manual_json'})\n\n# models/gemini.py\nDEFAULT_GEMINI_PROFILE = replace(DEFAULT_PROFILE, json_schema_transformer='gemini', ...)\n\nGEMINI_PROFILES = {}\nGEMINI_PROFILES['gemini-2.0-flash-001'] = replace(DEFAULT_GEMINI_PROFILE)\n\n# models/anthropic.py\nDEFAULT_ANTHROPIC_PROFILE = replace(DEFAULT_PROFILE, ...)\n\nANTHROPIC_PROFILES = {}\nANTHROPIC_PROFILES['claude-3-5-sonnet-20240620'] = replace(DEFAULT_ANTHROPIC_PROFILE, ...)\n\n# models/bedrock.py\nDEFAULT_BEDROCK_PROFILE = replace(DEFAULT_PROFILE)\n\nBEDROCK_PROFILES = {}\nBEDROCK_PROFILES['us.anthropic.claude-3-5-sonnet-20240620'] = ANTHROPIC_PROFILES['claude-3-5-sonnet-20240620'] # or some cleverer way to read these automatically based on name\n\n# my_agent.py\nmodel = OpenAIModel(model_name='gpt-4o')\n\nopenrouter_provider = \nmodel = OpenAIModel(\n  \"google/gemini-2.0-flash-001\", \n  provider=OpenAIProvider(base_url=\"https://openrouter.ai/api/v1\", ...), \n  profile=GEMINI_PROFILES['gemini-2.0-flash-001']\n)\n\nmodel = AnthropicModel(model_name='claude-3-5-sonnet-20240620')\n\nmodel = BedrockModel(model_name='llama3.3', profile=replace(DEFAULT_PROFILE, json_schema_transformer='gemini'))\n\n# could also work, if we merge in the defaults (or just set those on the dataclass/pydantic model?)\nmodel = BedrockModel(model_name='llama3.3', profile=ModelProfile(json_schema_transformer='gemini')) \n```\n\nI'd start by implementing this for `json_schema_transformer` as that's the main one causing issues today, but since we have the output modes in the pipeline, I'd rather implement this with a new class from the get-go rather than with a `json_schema_transformer` argument directly set on `Model`.\n\n@dmontagu @Kludex Thoughts? :)\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "DouweM",
      "author_type": "User",
      "created_at": "2025-05-20T18:52:30Z",
      "updated_at": "2025-05-28T19:58:56Z",
      "closed_at": "2025-05-28T19:58:56Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1782/reactions",
        "total_count": 6,
        "+1": 5,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1782",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1782",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:42.199746",
      "comments": [
        {
          "author": "dmontagu",
          "body": "This looks good. I think the GEMINI_PROFILES etc. should not include duplicate values for each model name, and instead should just include one key for each distinct profile value, and under the hood we should use a function that selects the appropriate profile as a function of the model name.\n\nI wou",
          "created_at": "2025-05-20T23:22:50Z"
        },
        {
          "author": "Kludex",
          "body": "I'm not sure about the name, but I agree with the idea. 👍",
          "created_at": "2025-05-21T05:22:53Z"
        }
      ]
    },
    {
      "issue_number": 1659,
      "title": "Erratic performance when using nested schemas",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nFunction call fails on some LLM when using nested classes as `output_type`\n\nIt fails with:\n\n- `Qwen/Qwen2.5-72B-Instruct-Turbo` on together.ai\n- `Qwen/Qwen3-4B-fast` on nebius.com\n- `Qwen/Qwen3-235B-A22B-fp8-tput` on together.ai\n\nIt succeeds with:\n\n- `meta-llama/Llama-3.3-70B-Instruct-Turbo` on together.ai\n- `Qwen3-4B-Q6_K.gguf` local on llama.cpp\n\nTo replicate the bug, see the example code:\nWhen using the classes `Count`, `Size`, `Name` as `output_type`, all models succeed, when using `NameAndCount` some models fails. The error messages are the following:\n\nfor togethe.ai models `pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: Qwen/Qwen2.5-72B-Instruct-Turbo, body: {'message': 'invalid tools grammar: Aborted(). Build with -sASSERTIONS for more info.', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}`\n\nfor the nebius.com model: `pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: Qwen/Qwen3-4B-fast, body: {'detail': \"Invalid request. Please check the parameters and try again. Details: 1 validation error for list[function-wrap[__log_extra_fields__()]]\\n  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]`\n\nPossibly related to https://github.com/pydantic/pydantic-ai/issues/1561 and https://github.com/pydantic/pydantic-ai/issues/1414\n\n\n### Example Code\n\n```Python\nimport os\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.settings import ModelSettings\n\n\nclass Count(BaseModel):\n    count: int = Field(\n        description=\"count\",\n    )\n\n\nclass Size(BaseModel):\n    size: float = Field(\n        description=\"size\",\n    )\n\n\nclass Name(BaseModel):\n    name: str = Field(\n        description=\"name\",\n    )\n\n\nclass NameAndCount(BaseModel):\n    name: str = Field(\n        description=\"name\",\n    )\n    count: Count\n\n# fails:\n# pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: Qwen/Qwen3-235B-A22B-fp8-tput, body: {'message': 'invalid tools grammar: Aborted(). Build with -sASSERTIONS for more info.', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}\nprovider = OpenAIModel(\n    model_name=\"Qwen/Qwen3-235B-A22B-fp8-tput\",\n    provider=OpenAIProvider(\n        base_url=\"https://api.together.xyz/v1\",\n        api_key=os.getenv(\"TOGETHER_API_KEY\"),\n    ))\n\n# succeeds\n# provider = OpenAIModel(\n#     model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n#     provider=OpenAIProvider(\n#         base_url=\"https://api.together.xyz/v1\",\n#         api_key=os.getenv(\"TOGETHER_API_KEY\"),\n#     ))\n\n# fails:\n# pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: Qwen/Qwen2.5-72B-Instruct-Turbo, body: {'message': 'invalid tools grammar: Aborted(). Build with -sASSERTIONS for more info.', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}\n# provider = OpenAIModel(\n#     model_name=\"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n#     provider=OpenAIProvider(\n#         base_url=\"https://api.together.xyz/v1\",\n#         api_key=os.getenv(\"TOGETHER_API_KEY\"),\n#     ))\n\n# fails:\n# pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: Qwen/Qwen3-4B-fast, body: {'detail': \"Invalid request. Please check the parameters and try again. Details: 1 validation error for list[function-wrap[__log_extra_fields__()]]\\n  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\"}\n# provider = OpenAIModel(\n#         model_name=\"Qwen/Qwen3-4B-fast\",\n#         provider=OpenAIProvider(\n#             base_url=\"https://api.studio.nebius.com/v1\",\n#             api_key=os.getenv(\"NEBIUS_API_KEY\"),\n#         ),\n#     )\n\n# succeeds (Qwen3-4B-Q6_K.gguf on llama.cpp)\n# provider = OpenAIModel(\n#     model_name=\"Qwen3-4B-Q6_K.gguf\",\n#     provider=OpenAIProvider(\n#         base_url=\"http://localhost:8080/v1\",\n#         api_key=\"llamacpp\",\n#     ),\n# )\n\nmodel_settings = ModelSettings(\n    max_tokens=2048,\n    temperature=0.0,\n    timeout=60,\n)\n\nagent = Agent(\n    provider,\n    output_type=Size, # using Name, Count, Size works fine, but using NameAndCount fails\n    model_settings=model_settings,\n    retries=2,\n    instrument=True,\n    system_prompt=\"\"\"\n    #Instructions:\n    - Extract the count, the name and the size from the user input.\"\"\",\n)\n\n\nasync def main():\n    import logfire\n    from loguru import logger\n\n    logfire.configure(\n        environment=\"test\",\n        service_name=\"bug.py\",\n        console=logfire.ConsoleOptions(verbose=True),\n        send_to_logfire=True,\n    )\n    logger.configure(handlers=[logfire.loguru_handler()])\n    logfire.instrument_httpx(capture_all=True)\n\n    user_input = \"\"\"\n      <UserInput>\n      The count is 9, the name is 'test', the size is 55\n      </UserInput>\n\"\"\"\n    result = await agent.run(user_input)\n    print(result.output)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.8 (main, Jan 14 2025, 23:36:58) [Clang 19.1.6 ] on darwin\nPydantic-ai 0.1.10\nQwen/Qwen3-235B-A22B-fp8-tput\nmeta-llama/Llama-3.3-70B-Instruct-Turbo\nQwen/Qwen2.5-72B-Instruct-Turbo\nQwen/Qwen3-4B-fast\nQwen3-4B-Q6_K.gguf\n```",
      "state": "closed",
      "author": "aus70",
      "author_type": "User",
      "created_at": "2025-05-07T19:42:40Z",
      "updated_at": "2025-05-28T19:43:22Z",
      "closed_at": "2025-05-21T05:20:21Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1659/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1659",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1659",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:42.484128",
      "comments": [
        {
          "author": "tranhoangnguyen03",
          "body": "To me this looks like LLM-specific limitations or the due to the way the api provider sets up their prompt formatting. This is not stemming from pydantic-ai. ",
          "created_at": "2025-05-08T02:18:08Z"
        },
        {
          "author": "DouweM",
          "body": "@aus70 Do you know if the issue is in _how_ PydanticAI formats the JSON schema for the `NameAndCount` model, or if these models fundamentally don't support less-than-trivial JSON schemas? In the latter case, there's nothing we can do, but if some tweaks to the JSON schema would make the models accep",
          "created_at": "2025-05-08T09:02:06Z"
        },
        {
          "author": "aus70",
          "body": "@DouweM The issue is _correlated_ to the non-trivial JSON schema, but it's not caused directly by it, more precisely by the presence in the schema of a nested schema ($ref), because by hard-coding in a test call a JSON schema with references (see below an example related to a different set of classe",
          "created_at": "2025-05-08T13:37:15Z"
        },
        {
          "author": "DouweM",
          "body": "@aus70 I ran your failing script locally, and the tools definition that's tripping up Qwen looks like this:\n\n```json\n[\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"final_result\",\n      \"description\": \"The final response which ends this conversation\",\n      \"parameters\": {\n        \"pr",
          "created_at": "2025-05-08T15:39:20Z"
        },
        {
          "author": "aus70",
          "body": "`QwenModel` solves all problems with Qwen models and - unsurprisingly - works also with meta-llama/Llama-3.3-70B-Instruct-Turbo. PR anybody?",
          "created_at": "2025-05-08T21:36:06Z"
        }
      ]
    },
    {
      "issue_number": 1847,
      "title": "How to track tool calls and their arguments within `run()`",
      "body": "### Question\n\nFirst of all, thank you for the amazing package!\n\nWhat would be the recommended way to track individual messages/tool calls sent to the LLM while it runs in the inner loop?\n\n#### Example use case\n I don't want to use full streaming, but would like to stream updates after every \"inner\" turn/stage to the user (eg, \"Expanding query\" -> \"Searching for documents\" -> \"Ranking the retrieved documents\" -> \"Generating an answer\")\n\nI thought of 4 options:\n- hijack the OTEL logs (seemed too involved)\n- adjust all tools and simply send the updates when they are called\n- add some \"beforeware\" and decorate the tool calls to update me when called\n- use the .iter() method to walk through the graph as it unfolds\n\n#### Questions on using `.iter()`\n\nI chose option 4 (`.iter()` method), but I had two questions that I couldn't find answers for:\n- getting the information requires reaching inside of the message parts - Can I rely on the current interfaces and fields to stay the same for the foreseeable future (tool_name, args; within reason obviously)\n\n- I wasn't entirely clear on what the nodes mean / when they are triggered. \n  Is my understanding correct that:\n  - UserPromptNode: user message provided (ie, this is the \"plane\" user is on)\n  - ModelRequestNode: the request sent to the model (outbound, followed by the actual LLM call)\n  - CallToolsNode: the model response, which includes the requested tool calls (inbound, before the execution of tools)\n  - ... - tools are executed\n  - ModelRequestNode: we send the requested tool results to the model (outbound)\n  - CallToolsNode: we receive the final message that we won (inbound)\n  - End: stopping condition detected and we surface this to the user (ie, we move back to the \"plane\" user is on). There is no new information here.\n\nIs my understanding correct? Are there perhaps some utilities that I'm missing to extract the requested tools etc. easily?\n\n\nMWE (adapted roulette example):\n```python\n# /// script\n# requires-python = \">=3.12\"\n# dependencies = [\n#     \"pydantic-ai\",\n# ]\n# ///\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.usage import UsageLimits\nimport asyncio\nimport json\nroulette_agent = Agent(  \n    'openai:gpt-4.1-mini',\n    deps_type=int,\n    output_type=str,\n    system_prompt=(\n        'Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'\n        'They can play upto 3 times if they did not win.'\n        'Announce the winning number if they win.'\n    ),\n    max_steps=3,\n)\n\n\n@roulette_agent.tool\nasync def roulette_wheel(ctx: RunContext[int], square: int) -> str:  \n    \"\"\"check if the square is a winner\"\"\"\n    return 'winner' if square == ctx.deps else 'loser'\n\nasync def main():\n    nodes = []\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with roulette_agent.iter('Put my money on square seventeen and then increase the number by 1 with each run', deps=success_number, usage_limits=UsageLimits(request_limit=3)) as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n\n    return nodes\n            \nif __name__ == \"__main__\":\n    # Run the agent\n    success_number = 18  \n    nodes = asyncio.run(main())\n    print('\\n\\n'.join([str(node) for node in nodes]))\n```\n\nOutput:\n```plaintext\nUserPromptNode(user_prompt='Put my money on square seventeen and then increase the number by 1 with each run', instructions=None, instructions_functions=[], system_prompts=('Use the `roulette_wheel` function to see if the customer has won based on the number they provide.They can play upto 3 times if they did not win.Announce the winning number if they win.',), system_prompt_functions=[], system_prompt_dynamic_functions={})\n\nModelRequestNode(request=ModelRequest(parts=[SystemPromptPart(content='Use the `roulette_wheel` function to see if the customer has won based on the number they provide.They can play upto 3 times if they did not win.Announce the winning number if they win.', timestamp=datetime.datetime(2025, 5, 28, 8, 28, 11, 824605, tzinfo=datetime.timezone.utc)), UserPromptPart(content='Put my money on square seventeen and then increase the number by 1 with each run', timestamp=datetime.datetime(2025, 5, 28, 8, 28, 11, 824608, tzinfo=datetime.timezone.utc))]))\n\nCallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='roulette_wheel', args='{\"square\": 17}', tool_call_id='call_OD9Ch5aq4MCSHq2pt17lp5Kd'), ToolCallPart(tool_name='roulette_wheel', args='{\"square\": 18}', tool_call_id='call_3EZFG4MPNooSei4LXTNnAjyW'), ToolCallPart(tool_name='roulette_wheel', args='{\"square\": 19}', tool_call_id='call_bEtaoVWNtzcrxfW1vZdAdZPT')], usage=Usage(requests=1, request_tokens=106, response_tokens=64, total_tokens=170, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0}), model_name='gpt-4.1-mini-2025-04-14', timestamp=datetime.datetime(2025, 5, 28, 8, 28, 12, tzinfo=datetime.timezone.utc), vendor_id='chatcmpl-Bc6QuPPqmbM1a115zbHwmOptOzPMh'))\n\nModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='roulette_wheel', content='loser', tool_call_id='call_OD9Ch5aq4MCSHq2pt17lp5Kd', timestamp=datetime.datetime(2025, 5, 28, 8, 28, 13, 571405, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='roulette_wheel', content='winner', tool_call_id='call_3EZFG4MPNooSei4LXTNnAjyW', timestamp=datetime.datetime(2025, 5, 28, 8, 28, 13, 571436, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='roulette_wheel', content='loser', tool_call_id='call_bEtaoVWNtzcrxfW1vZdAdZPT', timestamp=datetime.datetime(2025, 5, 28, 8, 28, 13, 571448, tzinfo=datetime.timezone.utc))]))\n\nCallToolsNode(model_response=ModelResponse(parts=[TextPart(content='You did not win on square 17.\\nCongratulations! You won on square 18.\\nYou did not win on square 19. \\n\\nThe winning number is 18.')], usage=Usage(requests=1, request_tokens=191, response_tokens=36, total_tokens=227, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0}), model_name='gpt-4.1-mini-2025-04-14', timestamp=datetime.datetime(2025, 5, 28, 8, 28, 14, tzinfo=datetime.timezone.utc), vendor_id='chatcmpl-Bc6Qw7eirpKv7PmLBUqrVbK9aP1fi'))\n\nEnd(data=FinalResult(output='You did not win on square 17.\\nCongratulations! You won on square 18.\\nYou did not win on square 19. \\n\\nThe winning number is 18.'))\n```\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "svilupp",
      "author_type": "User",
      "created_at": "2025-05-28T08:45:14Z",
      "updated_at": "2025-05-28T17:27:18Z",
      "closed_at": "2025-05-28T14:45:19Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1847/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1847",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1847",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:42.740269",
      "comments": [
        {
          "author": "DouweM",
          "body": "@svilupp `iter` is indeed the right way to go here.\n\n> Can I rely on the current interfaces and fields to stay the same for the foreseeable future (tool_name, args; within reason obviously)\n\nYes, it's part of the public API so we're committed to not breaking it without clear communication and a sign",
          "created_at": "2025-05-28T14:45:19Z"
        },
        {
          "author": "svilupp",
          "body": "Amazing, thank you!",
          "created_at": "2025-05-28T17:27:17Z"
        }
      ]
    },
    {
      "issue_number": 1834,
      "title": "A dataset for finetuning so LLM can work much better with PydanticAI",
      "body": "### Description\n\nI would be a game changer if Pydantic AI team would work on a dataset of structured output and tool calling / function calling examples which meets Pydantic AI expectations. The current state of LLMs, even the latest ones (like Qwen3) still struggle with structured outputs, both in reasoning and non reasoning modes. Pydantic could provide a fine tuning dataset that could change the game.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "ItzAmirreza",
      "author_type": "User",
      "created_at": "2025-05-26T14:53:14Z",
      "updated_at": "2025-05-28T14:53:52Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1834/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1834",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1834",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:43.001738",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/979\" (90% similar)",
          "created_at": "2025-05-26T15:00:12Z"
        }
      ]
    },
    {
      "issue_number": 1845,
      "title": "Bedrock Prompt Caching",
      "body": "### Description\n\nSome providers like Deepseek and OpenAI handle prompt caching server side.\n\nBedrock, like Anthropic, does not. Now that prompt caching is generally available on Bedrock, are there any plans to implement it?\n\n### References\n\n_No response_",
      "state": "open",
      "author": "aristide1997",
      "author_type": "User",
      "created_at": "2025-05-27T23:33:54Z",
      "updated_at": "2025-05-28T14:45:43Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1845/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1845",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1845",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:43.217330",
      "comments": []
    },
    {
      "issue_number": 1849,
      "title": "Store OpenRouter provider metadata in ModelResponse vendor details",
      "body": "### Description\n\nHi! \nI absolutely love the work you are doing at pydantic-ai. It has very quickly become a critical part of our production codebase. We also rely heavily on Openrouter for provider and model routing, and to provide extra capacity under heavy load. One feature I would love to see is if the provider selected by Openrouter was captured in the ModelResponse. This would be extremely valuable to us for debugging and analytics purposes. The `vendor_details` attribute seems like a natural fit. I think implementing this would require creating an `OpenRouterModel` class with a modified `_process_response` method\n\n## Current Behavior\n```py\nfrom os import getenv\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openrouter import OpenRouterProvider\n\nmodel = OpenAIModel(\n    model_name=\"anthropic/claude-3.7-sonnet\",\n    provider=OpenRouterProvider(api_key=getenv(\"OPENROUTER_API_KEY\"))\n\n)\nagent = Agent(model)\nresult = await agent.run(\"What is the meaning of life?\")\nresult.new_messages()[-1].vendor_details is None\n>>> True\n```\n\n## Desired Behavior\n```py\nfrom os import getenv\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenRouterModel\nfrom pydantic_ai.providers.openrouter import OpenRouterProvider\n\nmodel = OpenRouterModel(\n    model_name=\"anthropic/claude-3.7-sonnet\",\n    provider=OpenRouterProvider(api_key=getenv(\"OPENROUTER_API_KEY\"))\n\n)\nagent = Agent(model)\nresult = await agent.run(\"What is the meaning of life?\")\nprint(result.new_messages()[-1].vendor_details)\n>>> {\"provider\": \"Google\"}\n```\n\n## Super Basic Implementation\nWould be great to see this officially supported, and I would be happy to contribute with some guidance!\n```py\n# pydantic_ai.models.openrouter\nfrom openai.types import chat\nfrom pydantic_ai.messages import ModelResponse\nfrom pydantic_ai.models.openai import OpenAIModel\n\nclass OpenRouterModel(OpenAIModel):\n    def _process_response(self, response: chat.ChatCompletion) -> ModelResponse:\n        model_response = super()._process_response(response=response)\n        openrouter_provider = response.provider if hasattr(response, \"provider\") else None\n        if openrouter_provider:\n            vendor_details = getattr(model_response, \"vendor_details\", {})\n            vendor_details[\"provider\"] = openrouter_provider\n        return model_response\n```\n        \n\n### References\n\n_No response_",
      "state": "open",
      "author": "DanKing1903",
      "author_type": "User",
      "created_at": "2025-05-28T10:16:41Z",
      "updated_at": "2025-05-28T14:36:23Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1849/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1849",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1849",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:43.217347",
      "comments": [
        {
          "author": "DouweM",
          "body": "@DanKing1903 Sounds reasonable! Can you please submit a PR?",
          "created_at": "2025-05-28T14:36:19Z"
        }
      ]
    },
    {
      "issue_number": 1831,
      "title": "Add LangChain Tool support",
      "body": "### Description\n\nLangChain provides many tools (https://python.langchain.com/docs/integrations/tools/), while the tools available in Pydantic-AI right now are somewhat limited (https://ai.pydantic.dev/common-tools/). If an easy way to load LangChain tools in Pydantic-AI existed then it would be easier and quicker to write more capable agents.\n\nAgent frameworks like smolagents support loading tools from LangChain (https://huggingface.co/docs/smolagents/tutorials/tools#use-langchain-tools):\n\n```python\nfrom langchain.agents import load_tools\n\nsearch_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n\nagent = CodeAgent(tools=[search_tool], model=model)\n\nagent.run(\"How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?\")\n```\n\nI propose a similar way to load the tools be available in Pydantic-AI. Something like:\n\n```python\nfrom langchain_community.tools import DuckDuckGoSearchResults\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, Tool\n\nsearch_langchain = DuckDuckGoSearchResults()\nsearch_pydantic  = Tool.from_langchain(search_langchain)\n\nclass ObservedAge(BaseModel):\n    observation: str\n    age: int\n\nagent = Agent(\n    model=\"openai:gpt-4o-mini\",\n    output_type=ObservedAge,\n    tools=[search_pydantic],\n)\n\nresult = agent.run_sync(\"How old is Obama?\")\n\n# 09:36:49.899 agent run\n# 09:36:49.900   chat gpt-4o-mini\n# 09:36:51.076   running 1 tool\n# 09:36:51.077     running tool: duckduckgo_results_json\n# 09:36:51.992   chat gpt-4o-mini\n# AgentRunResult(output=ObservedAge(observation='Obama is currently 63 years old, born on August 4, 1961.', age=63))\n```\n\nI have written some code to implement this which I will provide in a PR shortly. When reviewing the Pydantic-AI code the following occurred to me:\n\n* the `pydantic_ai.tool.Tool` class has no base class that describes the desired behaviour.\n* the LangChain BaseTool defines json schemas for the args and similar which could more directly be translated to the `pydantic_ai.tool.Tool` class.\n* future integrations of this type might benefit from a similar approach.\n\nTo make my PR simple I have introduced a minimal change. I think it would be good to discuss a more maintainable and extensible approach.\n\n### References\n\nLangChain tools: https://python.langchain.com/docs/integrations/tools/\n\nSmolAgents tool can load from LangChain: https://huggingface.co/docs/smolagents/tutorials/tools#use-langchain-tools",
      "state": "open",
      "author": "matthewfranglen",
      "author_type": "User",
      "created_at": "2025-05-26T09:44:37Z",
      "updated_at": "2025-05-28T14:29:41Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1831/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1831",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1831",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:43.482155",
      "comments": [
        {
          "author": "DouweM",
          "body": "@matthewfranglen Thanks Matthew! The feature makes sense, let's discuss the implementation on the PR.",
          "created_at": "2025-05-26T16:10:09Z"
        }
      ]
    },
    {
      "issue_number": 1841,
      "title": "GoogleModel VertexAI Service Account Credentials fail with invalid_scope",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHey,\n\nSo I am pretty new to python and pydantic AI but I am struggling with the GoogleModel example from the docs: https://ai.pydantic.dev/models/google/#vertex-ai-enterprisecloud\n\nWhen executing my example code I get this error:\n`RefreshError('invalid_scope: Invalid OAuth scope or ID token audience provided.', {'error': 'invalid_scope', 'error_description': 'Invalid OAuth scope or ID token audience provided.'})`\n\nbut it only happens when I try to read the file and use it as credentials for the GoogleProvider. Setting the file path for the env variable with `os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]` works.\n\nany Ideas what I might be missing?\n\n### Example Code\n\n```Python\nimport os\nfrom google.oauth2 import service_account\nfrom pydantic_ai.providers.google import GoogleProvider\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai import Agent\n\nscript_dir = os.path.dirname(os.path.abspath(__file__))\ncredentials_path = os.path.join(script_dir, \"cred.json\")\n\ncredentials = service_account.Credentials.from_service_account_file(\n    credentials_path\n)\n\n#### >>> this works\n# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n\n#### >>> this does not work when using the same credentials from a file\ngcp_provider = GoogleProvider(\n    credentials=credentials,\n    project=credentials.project_id,\n    location=\"europe-west1\",  # Belgium,\n    vertexai=True,  # Use Vertex AI\n)\n\nflash_model = GoogleModel(\"gemini-2.0-flash\", provider=gcp_provider)\nagent = Agent(model=flash_model)\nresult = agent.run_sync(\"test prompt, write something in english\")\nprint(f\"Result: {result.output}\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nusing:\npython 3.13.3\npydantic-ai-slim[google,openai,vertexai] 0.2.10\ngemini-2.0-flash\n```",
      "state": "closed",
      "author": "PhilippAlbrecht-KR",
      "author_type": "User",
      "created_at": "2025-05-27T15:48:57Z",
      "updated_at": "2025-05-28T13:16:01Z",
      "closed_at": "2025-05-28T13:16:01Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1841/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1841",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1841",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:43.721311",
      "comments": [
        {
          "author": "vancoykendall",
          "body": "I had add a scopes input when using a creds file\n```python\ncredentials = service_account.Credentials.from_service_account_file(\n    \"<creds_path>.json\",\n    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n)\n```\nIt looks like when you don't give a file or info object it uses the default go",
          "created_at": "2025-05-27T20:07:21Z"
        },
        {
          "author": "PhilippAlbrecht-KR",
          "body": "Yes this works, thanks!",
          "created_at": "2025-05-28T07:10:12Z"
        }
      ]
    },
    {
      "issue_number": 1817,
      "title": "Client tool calling support",
      "body": "### Description\n\nHave a look at this demo from [CopilotKit](https://www.copilotkit.ai/) (not affiliated, they have an interesting product and I believe Pydantic AI should allow users to build experiences like they do): https://docs.copilotkit.ai/images/copilot-action-example.gif\n\n### Background\n\nThe demo illustrates how a conversational chatbot can perform actions in the frontend itself (client side).\n\nThis is _almost_ the same as what Pydantic AI allows us to achieve with tool calling, _except_ that in this case, the **tool call request is sent to the client**, and the client is the one that performs the action and sends the response back to the agent, which can then send out a final message indicating success or failure, depending on what the tool response from the client is.\n\nThis might sound similar to https://github.com/pydantic/pydantic-ai/issues/1189, but that issue talks about returning a tool call _response_ to the client, what this issue is focused on is returning a tool call _request_ directly to the client.\n\n### Current library limitations\n\n#### L1. Inability to define Tools without their implementation\n\nCurrently, defining a [`Tool`](https://ai.pydantic.dev/api/tools/#pydantic_ai.tools.Tool) requires that a `function` is specified. This is, of course quite sensible for most use cases except when the function is _not_ executed by us.\n\nAlso worth keeping in mind is that setting aside the implementation of the function, the _definition_ (parameters) of the function will be coming from the client and not statically defined by us.\n\n#### L2. Inability to cleanly end an Agent run from a tool call request\n\n[`iter`](https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph) gets us quite far, however [it seems that](https://github.com/pydantic/pydantic-ai/issues/195#issuecomment-2823476059) it requires manual message construction which is not that straightforward (at least not documented in a clear way)\n\n### Approaches\n\nThis issue is, from a library perspective, quite awkward admittedly. But I believe from a user experience support for flows like this would be a game-changer.\n\nHere's a few approaches that I believe are possible to address this issue:\n\n#### A1. Ending an Agent run when a client tool is encountered\n\nThis is pretty much just having a clear solution to L2, and having L1 addressed which would allow us to define a tool without it's implementation, and then we can just `End` the run during an Agent's `iter` . Message history is preserved. All is good and well...\n\n....almost. The only thing left to be addressed would be how the tool response coming from the client would be consumed (do we manually patch the history ourselves or do we allow something like a `ClientToolResponse` input?)\n\n#### A2: Having `ClientToolRequest` as an output type\n\nBasically, something like this:\n\n```python\nagent = Agent(\n    'openai:gpt-4o',\n    system_prompt=(\n        'You are a useful agent that helps the user manage '\n        'their todo list items.'\n    ),\n    client_tools=[add_todo_tool]\n)\n\n# or\n\nagent = Agent(\n    'openai:gpt-4o',\n    system_prompt=(\n        'You are a useful agent that helps the user manage '\n        'their todo list items.'\n    ),\n    output_type=[str, ClientToolRequest(add_todo_tool)]\n)\n\nresponse = agent.run_sync(\"Add tomatoes to my todo list\")\n\nresponse # ClientToolRequest(tool=ClientTool(...), args={...})\n\n# then, after the client responds...\n\nresponse = agent.run_sync(ClientToolResponse(...))\n```\n\n### Should Pydantic AI support this at all?\n\nDespite frontend/client-side tool calling being an incredibly valuable feature, the fact that it's a bit awkward/non-straightforward may perhaps suggest that this shouldn't be the library's responsibility at all.\n\nHowever, I'd still wager that this sort of feature can find its place inside this library. There already exists other libraries that have the notion of client-side tool call execution.\n\n#### Libraries that support client tool calling\n\n#### [CopilotKit](https://www.copilotkit.ai/)\n\nSee: [Frontend Actions](https://docs.copilotkit.ai/guides/frontend-actions)\n\n#### [AI SDK](https://ai-sdk.dev/)\n\nSee: [Chatbot Tool Usage](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage). This demonstrates how they can define server-side _and_ client-side tools (which are handled by the client via [`onToolCall`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat#on-tool-call))\n\n#### [ElevenLabs](https://elevenlabs.io/)\n\n(I have not used this platform. I just found happened upon this page when looking into client-side tool calling)\n\nSee: [Client tools](https://elevenlabs.io/docs/conversational-ai/customization/tools/client-tools)\n\n#### Similar / related disussions:\n\n* MCP: https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/218\n\n### References\n\n_No response_",
      "state": "open",
      "author": "zaidhaan",
      "author_type": "User",
      "created_at": "2025-05-24T12:31:38Z",
      "updated_at": "2025-05-28T13:07:06Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1817/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1817",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1817",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:43.971411",
      "comments": [
        {
          "author": "DouweM",
          "body": "@zaidhaan As PydanticAI is not tightly coupled to a frontend framework, unlike the examples you've mentioned, I don't think we could implement this as seamlessly or client-tool-call-specifically as they have. \n\nBut I agree we should have a way to end a run on a tool call, execute a tool and get a re",
          "created_at": "2025-05-26T17:15:19Z"
        }
      ]
    },
    {
      "issue_number": 1736,
      "title": "Incorrect usage calculation for gemini models in stream mode",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nUsage for gemini models in stream mode is calculated incorrectly. \nRequest_tokens seems to be multiplied by number of chunks. As a result total_tokens is also too high.\n\n\n\n### Example Code\n\n```Python\nimport asyncio\nfrom pydantic_ai import Agent\n\n\nasync def main():\n    agent = Agent(\n        \"google-gla:gemini-2.0-flash\",\n    )\n\n    prompt = \"\"\"return only \"word_1 word_2 word_3 word_4 word_5 word_6 word_7 word_8 word_9 word_10\" \"\"\"\n\n    results = await agent.run(prompt)\n    print(f\"Run usage:\\n {results.usage()}\")\n\n    async with agent.run_stream(prompt) as results:\n        chunks = len([chunk async for chunk in results.stream_text(debounce_by=None)])\n        print(f\"Stream run usage ({chunks} chunks):\\n {results.usage()}\")\n\n\nasyncio.run(main())\n\n# Run usage:\n#  Usage(requests=1, request_tokens=36, response_tokens=32, total_tokens=68, details=None)\n# Stream run usage (4 chunks):\n#  Usage(requests=1, request_tokens=147, response_tokens=32, total_tokens=179, details=None)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.13.2\npydantic-ai 0.2.4\n```",
      "state": "closed",
      "author": "zahariash",
      "author_type": "User",
      "created_at": "2025-05-15T19:07:28Z",
      "updated_at": "2025-05-28T12:59:27Z",
      "closed_at": "2025-05-28T12:59:27Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1736/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1736",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1736",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:46.127599",
      "comments": [
        {
          "author": "amiyapatanaik",
          "body": "Take a look at #1577 \nThis is a known issue that needs to be resolved asap. ",
          "created_at": "2025-05-16T10:56:17Z"
        },
        {
          "author": "DouweM",
          "body": "https://github.com/pydantic/pydantic-ai/pull/1752 looks good, I intend to merge that soon.",
          "created_at": "2025-05-19T19:22:41Z"
        }
      ]
    },
    {
      "issue_number": 1380,
      "title": "How do I handle pydantic_ai.exceptions.UnexpectedModelBehavior",
      "body": "### Question\n\nHello,\n\nI have the following problem. I'm running a pretty simple Agent workflow where I call a LLM that has to use a simple tool.\nThe Problem is that, if I hook claude-3.5-Sonnet into the system, that it uses the tool without generating a followup answer, which causes pydantic, to raise a UnexpectedModelBhevaior Excekption.\n\nThat itself isn't much of a problem, but I'm losing the initial answer that the LLM gave me before using the tool.\nHere is an example of how the issue arises:\nVia Logfire I can trace how the initial response looks like:\n```\n[{\n\"content\": \" **System Prompt Example** \",\n\"role\": \"system\",\n\"gen_ai.system\": \"anthropic\",\n\"gen_ai.message.index\": 0,\n\"event.name\": \"gen_ai.system.message\"\n},{\n\"content\": \" **User input**\",\n\"role\": \"user\",\n\"gen_ai.system\": \"anthropic\",\n\"gen_ai.message.index\": 0,\n\"event.name\": \"gen_ai.user.message\"\n},{\n\"index\": 0,\n\"message\": {\n\"role\": \"assistant\",\n\"content\": \"**Im going to use the tool and then I'm finished**\",\n\"tool_calls\": [\n{\n\"id\": \"toolu_01CTZyDzTmLv2F5JiGUnj8mS\",\n\"type\": \"function\",\n\"function\": {\n\"name\": \"give_evaluation\",\n\"arguments\": {\n\"evaluation\": 1\n}}}]},\n\"gen_ai.system\": \"anthropic\",\n\"event.name\": \"gen_ai.choice\"\n}\n]\n```\n\nBut this is not the response object that I get retuned by the \n`client.agent.run_sync(prompt, message_history=message_history).`\nThere is a next step, that adds an empty response from the LLM and what I'm receiving is the following:\n\n```\n[\n{\n\"content\": \"System Prompt Example \",\n\"role\": \"system\",\n\"gen_ai.system\": \"anthropic\",\n\"gen_ai.message.index\": 0,\n\"event.name\": \"gen_ai.system.message\"\n},\n{\n\"content\": \"User input\",\n\"role\": \"user\",\n\"gen_ai.system\": \"anthropic\",\n\"gen_ai.message.index\": 0,\n\"event.name\": \"gen_ai.user.message\"\n},{\n\"role\": \"assistant\",\n\"content\": \"Im going to use the tool and then I'm finished\",\n\"tool_calls\": [\n{\n\"id\": \"toolu_01CTZyDzTmLv2F5JiGUnj8mS\",\n\"type\": \"function\",\n\"function\": {\n\"name\": \"give_evaluation\",\n\"arguments\": {\n\"einstufung\": 1\n}}}],\n\"gen_ai.system\": \"anthropic\",\n\"gen_ai.message.index\": 1,\n\"event.name\": \"gen_ai.assistant.message\"\n},{\n\"content\": 1,\n\"role\": \"tool\",\n\"id\": \"toolu_01CTZyDzTmLv2F5JiGUnj8mS\",\n\"name\": \"give_evaluation\",\n\"gen_ai.system\": \"anthropic\",\n\"gen_ai.message.index\": 2,\n\"event.name\": \"gen_ai.tool.message\"\n},{\n**\"index\": 0,\n\"message\": {\n\"role\": \"assistant\"\n},**\n\"gen_ai.system\": \"anthropic\",\n\"event.name\": \"gen_ai.choice\"\n}]\n```\n\nAs you can see, it added another empty response, which causes the raise of the Exception. Even tho I have this insight with logfire, in my code I can't fetch this part, because if the exception raises, the response object doesn't get returned, and I'm losing the initial response. All of this is just one run. \n\nIs there a way to fix this behavior?\n\nHere is the snippet of the agent for additional context:\n\n```\nself.agent = Agent(\n            model=model,\n            retries=retries,\n            system_prompt=sys_prompt\n        )\n        self.model_name = model.model_name\n        self.last_response = None\n        self.observer = observer\n\n        @self.agent.tool_plain\n        def give_evaluation(evaluation: int) -> int:\n            \"\"\"Evaluate following scenario\"\"\n            self.observer.log_evaluation(str(evaluation))\n            return evaluation\n```\n\n### Additional Context\n\n- Python 3.12\n- pydantic-ai 0.0.43",
      "state": "open",
      "author": "MirrahOH",
      "author_type": "User",
      "created_at": "2025-04-04T15:24:47Z",
      "updated_at": "2025-05-27T20:29:21Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1380/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1380",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1380",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:46.406507",
      "comments": [
        {
          "author": "matankley",
          "body": "@MirrahOH I'm facing the same issue. did you find a way to workaround it ?",
          "created_at": "2025-04-07T12:07:53Z"
        },
        {
          "author": "MirrahOH",
          "body": "Hey, sadly not. I just fetch the Error and continue. Idk if that might help you, but I catch the input of the tool inside the tool itself, so I know how it was used, but I'm still losing the previous message. ",
          "created_at": "2025-04-10T17:00:45Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-18T14:00:34Z"
        },
        {
          "author": "IngLP",
          "body": "I have the same problem!\n",
          "created_at": "2025-04-30T11:27:47Z"
        },
        {
          "author": "callmeBalloch",
          "body": "Same problem lately with OpenRouter and Gemini",
          "created_at": "2025-05-27T20:29:20Z"
        }
      ]
    },
    {
      "issue_number": 1839,
      "title": "Implement the MCP server Streamable HTTP transport for the Pydantic AI MCP client",
      "body": "### Description\n\nThank you for your great work on Pydantic AI. I noticed that your MCP client documentation reads:\n`The name \"HTTP\" is used since this implemented will be adapted in future to use the new [Streamable HTTP](https://github.com/modelcontextprotocol/specification/pull/206) currently in development.`\n\nThe provided link shows that the implementation was completed by fastmcp in March. Would be great to support it in Pydantic AI.\n\n### References\n\nhttps://github.com/modelcontextprotocol/specification/pull/206\nhttps://ai.pydantic.dev/mcp/client/",
      "state": "closed",
      "author": "demux79",
      "author_type": "User",
      "created_at": "2025-05-27T12:29:06Z",
      "updated_at": "2025-05-27T18:29:29Z",
      "closed_at": "2025-05-27T13:24:54Z",
      "labels": [
        "MCP"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1839/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1839",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1839",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:46.693656",
      "comments": [
        {
          "author": "DouweM",
          "body": "Done in https://github.com/pydantic/pydantic-ai/pull/1716, about to be released!",
          "created_at": "2025-05-27T13:24:54Z"
        },
        {
          "author": "demux79",
          "body": "Awesome, sorry, I had only searched in the open issues.",
          "created_at": "2025-05-27T18:29:28Z"
        }
      ]
    },
    {
      "issue_number": 1623,
      "title": "Amazon Nova (Bedrock) limitations with tool schema",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWas trying to use one of the Nova foundation models on Bedrock, but ran into issues with tool-calling and structured output.\n\nThe Nova models seem to be a bit more strict (or less capable, depending who you ask) with regards to tool schemas. The top-level schema must be `\"type\": \"object\"` and the only fields allowed are `\"type\"`, `\"properties\"`, and `\"required\"`. All other fields are disallowed and result in a hard API error.\n\n> See the second NOTE on the [Nova docs page](https://docs.aws.amazon.com/nova/latest/userguide/tool-use-definition.html)\n\nIts a pretty annoying limitation, but means that many of the fields that Pydantic generates with the JSON Schema are not allowed. Most critically, `\"$defs\" are not allowed, so the schema would need to be transformed somehow to allow tool calling. \n\nI know Nova is probably low-priority (and I seem to recall reading somewhere that Anthropic Bedrock was the main use-case for now), but there is nothing that would indicate that this wouldn't work. This might need to be called out in the docs and/or generate a warning/error when trying to use tools and structured output with a Nova model.\n\n### Example Code\n\n```Python\nclass Something(BaseModel):\n   foo: str\n   bar: list[int]\n\nclass SomethingElse(BaseModel):\n    something: Something\n    extra: dict[str, str]\n\nagent = Agent(\n    model=\"bedrock:us.amazon.nova-pro-v1:0\", \n    # model=\"bedrock:us.amazon.nova-lite-v1:0\",   # or this\n    # model=\"bedrock:us.amazon.nova-micro-v1:0\",  # or this\n    instructions=\"You are a helpful assistant.\",\n    output_type=Something,\n)\n\n@agent.tool_plain\ndef frobnicate(something_else: SomethingElse):\n   print({something_else=!r})\n\n_ = agent.run_sync(\"frobnicate, and then do what you need to do...\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n`python>=3.13`\n`pydantic-ai==0.1.8`\n`pydantic==2.11.3`\n`boto3==1.38.4` (Bedrock)\n```",
      "state": "closed",
      "author": "chasewalden",
      "author_type": "User",
      "created_at": "2025-04-30T23:06:31Z",
      "updated_at": "2025-05-27T17:17:21Z",
      "closed_at": "2025-05-21T05:20:27Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1623/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1623",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1623",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:46.975432",
      "comments": [
        {
          "author": "DouweM",
          "body": "We already implement something similar for OpenAI at https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py#L967 and Gemini at https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/gemini.py#L764, so I'm not surprised other mod",
          "created_at": "2025-05-01T20:51:53Z"
        },
        {
          "author": "aristideubertas",
          "body": "@DouweM I have also experienced this problem, would you be able to give a snippet of your reccomended approach? Or if something could be added to the docs. \n\nThis is also model specific, not Bedrock-wide, because Claude works fine.",
          "created_at": "2025-05-05T17:54:44Z"
        },
        {
          "author": "chasewalden",
          "body": "@DouweM , overriding the schema generator seems like it'll work fine for tools but will not work for structured output. ",
          "created_at": "2025-05-05T18:14:27Z"
        },
        {
          "author": "tamir-alltrue-ai",
          "body": "Can confirm that this also affects llama models served on bedrock.\n\nWith Llama-3.3-70B I'm able to get structured output, but not tool calls.",
          "created_at": "2025-05-05T19:15:38Z"
        },
        {
          "author": "DouweM",
          "body": "@aristideubertas @chasewalden @tamir-alltrue-ai I've shared an example of a similar JSON schema modification to make Qwen on Together.ai work with the OpenAIModel at https://github.com/pydantic/pydantic-ai/issues/1659#issuecomment-2863505322. Let me know if you're having trouble adjusting it for `Be",
          "created_at": "2025-05-08T16:02:07Z"
        }
      ]
    },
    {
      "issue_number": 1830,
      "title": "Connect to Azure Foundry Hub agent to use built-in BingGroundingTool",
      "body": "### Question\n\nAzure is retiring the Bing Search API in favor of a direct tool call method for AI Agents via a knowledgebase called Bing-Grounding. However, this requires an Agent to be created in the azure foundry which has this connection. Is it possible to connect to that agent via a Pydantic Agent, and use the Grounding tool in the tool choice? If so, it would be amazing with an example\n\ncurrently the examples of how to connect to these (that i cannot get working) uses\n```\nfrom azure.ai.agents.models import BingGroundingTool\nfrom azure.ai.projects import AIProjectClient\n```\n### Additional Context\n\nThis should be based on the agent id provided in Azure\n\n_No response_",
      "state": "open",
      "author": "TjobberTjob",
      "author_type": "User",
      "created_at": "2025-05-26T09:20:01Z",
      "updated_at": "2025-05-27T16:51:56Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1830/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1830",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1830",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:47.210592",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex This looks related to the build-in tools work you're doing.",
          "created_at": "2025-05-26T16:15:36Z"
        }
      ]
    },
    {
      "issue_number": 1735,
      "title": "Different behaviour with Gemini models using OpenAI+OpenRouter",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using a gemini model as follows:\n\n```python\nopenrouter_provider = OpenAIProvider(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n)\nmodel = OpenAIModel(\n    \"google/gemini-2.0-flash-001\", provider=openrouter_provider\n)\n```\nIt creates a different `parameters_json_schema` that causes the model **repeatedly** fail to follow the schema, while the GeminiModel + VertexProvider rarely fails.\n\n![Image](https://github.com/user-attachments/assets/ee76d2e6-5949-437a-a0c0-bf40f5347bc3)\n\nLeft: OpenAIModel with OpenAIProvider/OpenRouter\nRight: GeminiModel with VertexProvider\n\n<details>\n<summary>Error Message</summary>\n\n```\n15:50:03.343 agent run\n15:50:03.344   chat google/gemini-2.0-flash-001\nTraceback (most recent call last):\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_output.py\", line 234, in validate\n    output = self.type_adapter.validate_json(tool_call.args, experimental_allow_partial=pyd_allow_partial)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 468, in validate_json\n    return self.validator.validate_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for GroundedRating\nreasoning\n  Input should be an object [type=model_type, input_value='The speech was well-stru...red and easy to follow.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 459, in _handle_tool_calls\n    result_data = output_tool.validate(call)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_output.py\", line 244, in validate\n    raise ToolRetryError(m) from e\npydantic_ai._output.ToolRetryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/chenghao/Developer/ai-reports/reproduce/debug.py\", line 92, in <module>\n    asyncio.run(test_analyze_sales_conversation(data))\n  File \"/Users/chenghao/.local/share/uv/python/cpython-3.12.6-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/.local/share/uv/python/cpython-3.12.6-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/.local/share/uv/python/cpython-3.12.6-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/Developer/ai-reports/reproduce/debug.py\", line 77, in test_analyze_sales_conversation\n    result = await agent.run(\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 451, in run\n    async for _ in agent_run:\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1812, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 810, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 783, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 380, in run\n    async with self.stream(ctx):\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/chenghao/.local/share/uv/python/cpython-3.12.6-macos-aarch64-none/lib/python3.12/contextlib.py\", line 217, in __aexit__\n    await anext(self.gen)\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 394, in stream\n    async for _event in stream:\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 443, in _run_stream\n    async for event in self._events_iterator:\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 421, in _run_stream\n    async for event in self._handle_tool_calls(ctx, tool_calls):\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 464, in _handle_tool_calls\n    ctx.state.increment_retries(ctx.deps.max_result_retries)\n  File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 70, in increment_retries\n    raise exceptions.UnexpectedModelBehavior(\npydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (0) for result validation\n```\n</details>\n\n\n### Example Code\n\n```Python\nimport asyncio\nimport os\nfrom typing import Annotated, Literal\n\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, BinaryContent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.models.instrumented import InstrumentationSettings\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n\nclass GradingReasoning(BaseModel):\n    \"\"\"The reasoning for the grade of the communication.\"\"\"\n\n    overall_reasoning: Annotated[\n        str,\n        Field(description=\"The reasoning for the overall grade of the communication.\"),\n    ]\n    grade_justification: Annotated[\n        str,\n        Field(\n            description=\"The justification for the chosen grade of the communication.\"\n        ),\n    ]\n    grade_level_justification: Annotated[\n        str,\n        Field(\n            description=\"The justification for the chosen grade level of the communication.\"\n        ),\n    ]\n\n\nclass GroundedRating(BaseModel):\n    \"\"\"A grounded rating is a rating that is related to a specific transcript. Used in model generation.\"\"\"\n\n    reasoning: Annotated[\n        GradingReasoning, Field(description=\"The reasoning for the rating.\")\n    ]\n    grade: Annotated[\n        Literal[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"],\n        Field(description=\"The grade of the communication.\"),\n    ]\n    summary: Annotated[str, Field(description=\"The summary of the issues.\")]\n\n\nload_dotenv(override=True)\n\n\nasync def test_analyze_sales_conversation(audio_bytes: bytes) -> None:\n    openrouter_provider = OpenAIProvider(\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n    )\n    vertex_provider = GoogleVertexProvider(\n        service_account_file=os.environ[\"SERVICE_ACCOUNT_JSON\"],\n    )\n\n    gemini_model = GeminiModel(\"gemini-2.0-flash-001\", provider=vertex_provider)\n    openai_model = OpenAIModel(\n        \"google/gemini-2.0-flash-001\", provider=openrouter_provider\n    )\n\n    agent = Agent(\n        model=openai_model,\n        # model=gemini_model,\n        system_prompt=\"What can you tell me about the speech?\",\n        retries=0,\n        instrument=True,\n    )\n    result = await agent.run(\n        [\n            \"Here is the audio file to analyze:\",\n            BinaryContent(audio_bytes, media_type=\"audio/wav\"),\n        ],\n        output_type=GroundedRating,\n    )\n    print(result.output)\n\n\nif __name__ == \"__main__\":\n    data = open(\n        \"....wav\",\n        \"rb\",\n    ).read()\n    asyncio.run(test_analyze_sales_conversation(data))\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic                                     2.11.4\npydantic-ai                                  0.2.0\npydantic-ai-slim                             0.2.0\npydantic-core                                2.33.2\npydantic-evals                               0.2.0\npydantic-graph                               0.2.0\npydantic-settings                            2.9.1\nopenai                                       1.75.0\ngoogle-genai                                 1.15.0\n```",
      "state": "closed",
      "author": "ChenghaoMou",
      "author_type": "User",
      "created_at": "2025-05-15T16:09:58Z",
      "updated_at": "2025-05-27T12:56:45Z",
      "closed_at": "2025-05-21T05:20:12Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1735/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1735",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1735",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:47.456007",
      "comments": [
        {
          "author": "DouweM",
          "body": "@ChenghaoMou Since you're using the `OpenAIModel`, the Gemini-specific JSON schema transformations aren't applied.\n\nYou can do so manually with a custom `OpenAIModel` subclass that uses `_GeminiJsonSchema`, like this:\n\n```py\nfrom pydantic_ai.models import ModelRequestParameters\nfrom pydantic_ai.mode",
          "created_at": "2025-05-19T21:53:33Z"
        },
        {
          "author": "ChenghaoMou",
          "body": "@DouweM Thanks a ton for the help! I can confirm that it solves my issue. Feel free to close this issue for your later one.",
          "created_at": "2025-05-20T10:01:06Z"
        },
        {
          "author": "DouweM",
          "body": "@ChenghaoMou With the changes in https://github.com/pydantic/pydantic-ai/pull/1835, which include a new `OpenRouterProvider` that automatically reads the `OPENROUTER_API_KEY` env var, and automatic JSON schema transformer selection based on the `google/` prefix in the model name, you should be able ",
          "created_at": "2025-05-26T23:39:51Z"
        },
        {
          "author": "ChenghaoMou",
          "body": "@DouweM thanks for the quick turnaround!\n\nI have tested your branch with `OpenAIModel(\"google/gemini-2.0-flash-001\", provider=openrouter_provider),`. \n\nHere is an issue I found:\n\n```\n             │   File \"/Users/chenghao/Developer/ai-reports/.venv/lib/python3.12/site-packages/pydantic_ai/providers/",
          "created_at": "2025-05-27T12:36:02Z"
        },
        {
          "author": "DouweM",
          "body": "@ChenghaoMou Good catch, that's what I get for leaving tests to the end :) Fixed in the PR!",
          "created_at": "2025-05-27T12:56:44Z"
        }
      ]
    },
    {
      "issue_number": 1836,
      "title": "how mcp_run_python install library?",
      "body": "### Question\n\nI make an agent by LLM and `mcp_run_python`, and my task is to fetch data from the local api, but it hints me to install `requests` library instead of automatically doing it. The code is below:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nserver = MCPServerStdio('deno',\n    args=[\n        'run',\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ],\n)\n\nmodel = OpenAIModel(\n    \"gpt-4o\",\n    provider=OpenAIProvider(api_key=''),\n)\nagent = Agent(model, mcp_servers=[server])\n\n\nasync def main():\n    async with agent.run_mcp_servers():\n        result = await agent.run(\n            'Write code to commit the post request (http://localhost:2020/api/v3/) ' \\\n            'to fetch data, and the request payload is ' \\\n            '{\"data\":\"xx\",\"path\":\"/path/to/file\"}.' \\\n            'You may first need to pip request library and you should return the post request result.'\n        )\n    print(result.output)\n\nif __name__ == '__main__':\n    import asyncio\n    asyncio.run(main())\n```\n\n\nThe LLM response is:\n\n> It seems that the `requests` library cannot be installed due to network access restrictions. Unfortunately, without the `requests` library, I am unable to make the POST request to fetch the data.\n> \n> Is there any other way I can assist you?\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "Riofd",
      "author_type": "User",
      "created_at": "2025-05-27T06:37:02Z",
      "updated_at": "2025-05-27T09:17:20Z",
      "closed_at": "2025-05-27T09:17:02Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1836/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1836",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1836",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:47.696802",
      "comments": [
        {
          "author": "Riofd",
          "body": "Solve it by changing prompts.",
          "created_at": "2025-05-27T09:17:02Z"
        }
      ]
    },
    {
      "issue_number": 1756,
      "title": "ssl.SSLError: unknown error (_ssl.c:3036) when trying to integrate pydantic ai into my django app, but the unittest working just fine",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nso im trying to build a django-chatbot-app powered with htmx websocket. when im trying to import the simple agent model into my django websocket app, i got an error of `ssl.SSLError: unknown error (_ssl.c:3036)`. here is the snippets of the code.\n\nthen when im importing the agent variable into the websocket of consumers.py, even that im just importing it, i got the error ssl.SSLError: unknown error (_ssl.c:3036) \n\nim using linux Fedora 42 for the OS, \n\nthe thing is, when i try it in as unittest, it all works just fine as intended\ni tried different ai client, from deepseek and google gemini flash, they both suffer the same issue when importing it to the django app.\n\nNOTE: i will try to ask about this in django community too in the sametime\n\nagents.py\n```python\nfrom pydantic_ai import Agent\nimport os\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\nmodel = OpenAIModel(\n    \"deepseek-chat\",\n    provider=DeepSeekProvider(api_key=os.getenv(\"DEEPSEEK_API_KEY\")),\n)\nagent = Agent(model)\n```\n\n\n\nconsumers.py\n```python\nfrom channels.generic.websocket import WebsocketConsumer\nfrom uuid import UUID\nfrom .agents import agent\nfrom .models import ChatRoom, ChatMessage\nfrom pprint import pprint as print\nfrom django.contrib.auth.models import User\nfrom channels.auth import UserLazyObject\nfrom django.template.loader import render_to_string\nimport json\n\n\nclass ChatConsumer(WebsocketConsumer):\n    def connect(self):\n        # self.scope: dict = self.scope\n        self.user: User = self.scope[\"user\"]\n        self.room_uuid = UUID(\n            self.scope.get(\"url_route\").get(\"kwargs\").get(\"room_uuid\")\n        )\n        # print(self.scope)\n        # print(self.room_uuid)\n        # print(type(self.room_uuid))\n        # self.messages = list(ChatMessage.objects.filter(room__user=self.user))\n        self.chat_room, _ = ChatRoom.objects.get_or_create(\n            user=self.user, room_uuid=self.room_uuid\n        )\n        # TODO: add way to load previous messages by uuid\n        self.messages: list[ChatMessage] = self.load_messages()\n        print(self.messages)\n        return self.accept() if self.chat_room.user == self.user else self.close()\n\n    def receive(self, text_data=None, bytes_data=None):\n        print(text_data)\n        text_json = json.loads(str(text_data))\n        print(text_json)\n\n        # print(self.scope)\n        # print(self.scope[\"user\"])\n        # print(type(self.scope[\"user\"]))\n        # self.messages.append(text_json)\n        human_message = ChatMessage(\n            room=self.chat_room,\n            content=text_json[\"message\"],\n            owner=ChatMessage.OwnerEnum.USER,\n        )\n        bot_message = ChatMessage(\n            room=self.chat_room,\n            content=\"\",\n            owner=ChatMessage.OwnerEnum.AI,\n        )\n\n        self.messages.append(human_message)\n        self.messages.append(bot_message)\n        # user_message_html = render_to_string(\n        #     \"chat/partial_chat.html\",\n        #     {\"message_text\": text_json[\"message\"], \"is_system\": False},\n        # )\n        user_message_html = render_to_string(\n            \"cotton/htmxhumanmessage.html\",\n            context={\"message\": text_json[\"message\"], \"is_system\": False},\n        )\n        bot_message_html = render_to_string(\n            \"cotton/htmxhumanmessage.html\",\n            context={\"message\": \"this is bot reply message\", \"is_system\": True},\n        )\n        self.send(text_data=user_message_html)\n\n        self.send(text_data=bot_message_html)\n\n    def close(self, code=None, reason=None):\n        print(\"final result is...\")\n        if len(self.messages) > 0:\n            print(code)\n            self.save_messages()\n            return super().disconnect(code)\n        self.delete_chatroom()\n        return super().close(code, reason)\n\n    def disconnect(self, code):\n        print(\"final result is...\")\n        if len(self.messages) > 0:\n            self.save_messages()\n            return super().disconnect(code)\n        self.delete_chatroom()\n        print(\"success disconnect connection\")\n        return super().disconnect(code)\n\n    def save_messages(self):\n        for m in self.messages:\n            m.save()\n\n    def load_messages(self) -> list[ChatMessage]:\n        try:\n            chat_room = ChatRoom.objects.get(room_uuid=self.room_uuid)\n            messages = list(\n                ChatMessage.objects.filter(room=chat_room).order_by(\"created_at\")\n            )\n            if len(messages) > 0:\n                return messages\n        except ChatRoom.DoesNotExist:\n            pass\n        return []\n\n    def delete_chatroom(self) -> None:\n        self.chat_room.delete()\n```\n\nunittest snipet code, run it with python -m unittest tests.TestAgent\ntests.py\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom unittest import TestCase\nfrom dotenv import load_dotenv\nload_dotenv()\n\nclass TestAgent(TestCase):\n\n    def test_agent_multiple_chat(self):\n        \"\"\"\n        NOTE: use gemini flash\n        \"\"\"\n        model = GeminiModel(\"gemini-2.0-flash\")\n        agent = Agent(model, system_prompt=\"You are a very helpful assistant\")\n        result = agent.run_sync(\"tell me a joke for bapak2 in bahasa indonesia\")\n        print(result.output)\n        result2 = agent.run_sync(\n            \"Jelaskan maksudnya\", message_history=result.new_messages()\n        )\n        print(result2.output)\n\n```\n\n### Example Code\n\n```Python\n# this script is combined version of agents.py and consumers.py\nfrom pydantic_ai import Agent\nimport os\nfrom dotenv import load_dotenv\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\nload_dotenv()\n\nmodel = OpenAIModel(\n    \"deepseek-chat\",\n    provider=DeepSeekProvider(api_key=os.getenv(\"DEEPSEEK_API_KEY\")),\n)\nagent = Agent(model)\n\nfrom channels.generic.websocket import WebsocketConsumer\nfrom uuid import UUID\nfrom .agents import agent\nfrom .models import ChatRoom, ChatMessage\nfrom pprint import pprint as print\nfrom django.contrib.auth.models import User\nfrom channels.auth import UserLazyObject\nfrom django.template.loader import render_to_string\nimport json\n\n\nclass ChatConsumer(WebsocketConsumer):\n    def connect(self):\n        # self.scope: dict = self.scope\n        self.user: User = self.scope[\"user\"]\n        self.room_uuid = UUID(\n            self.scope.get(\"url_route\").get(\"kwargs\").get(\"room_uuid\")\n        )\n        self.chat_room, _ = ChatRoom.objects.get_or_create(\n            user=self.user, room_uuid=self.room_uuid\n        )\n        # TODO: add way to load previous messages by uuid\n        self.messages: list[ChatMessage] = self.load_messages()\n        return self.accept() if self.chat_room.user == self.user else self.close()\n\n    def receive(self, text_data=None, bytes_data=None):\n        print(text_data)\n        text_json = json.loads(str(text_data))\n        print(text_json)\n        human_message = ChatMessage(\n            room=self.chat_room,\n            content=text_json[\"message\"],\n            owner=ChatMessage.OwnerEnum.USER,\n        )\n        bot_message = ChatMessage(\n            room=self.chat_room,\n            content=\"\",\n            owner=ChatMessage.OwnerEnum.AI,\n        )\n\n        self.messages.append(human_message)\n        self.messages.append(bot_message)\n        user_message_html = render_to_string(\n            \"cotton/htmxhumanmessage.html\",\n            context={\"message\": text_json[\"message\"], \"is_system\": False},\n        )\n        bot_message_html = render_to_string(\n            \"cotton/htmxhumanmessage.html\",\n            context={\"message\": \"this is bot reply message\", \"is_system\": True},\n        )\n        self.send(text_data=user_message_html)\n        self.send(text_data=bot_message_html)\n\n    def close(self, code=None, reason=None):\n        print(\"final result is...\")\n        if len(self.messages) > 0:\n            print(code)\n            self.save_messages()\n            return super().disconnect(code)\n        self.delete_chatroom()\n        return super().close(code, reason)\n\n    def disconnect(self, code):\n        print(\"final result is...\")\n        if len(self.messages) > 0:\n            self.save_messages()\n            return super().disconnect(code)\n        self.delete_chatroom()\n        print(\"success disconnect connection\")\n        return super().disconnect(code)\n\n    def save_messages(self):\n        for m in self.messages:\n            m.save()\n\n    def load_messages(self) -> list[ChatMessage]:\n        try:\n            chat_room = ChatRoom.objects.get(room_uuid=self.room_uuid)\n            messages = list(\n                ChatMessage.objects.filter(room=chat_room).order_by(\"created_at\")\n            )\n            if len(messages) > 0:\n                return messages\n        except ChatRoom.DoesNotExist:\n            pass\n        return []\n\n    def delete_chatroom(self) -> None:\n        self.chat_room.delete()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.13.3\npydantic                                 2.11.4\npydantic-ai                              0.1.9\npydantic-ai-examples                     0.1.9\npydantic-ai-slim                         0.1.9\npydantic-core                            2.33.2\npydantic-evals                           0.1.9\npydantic-graph                           0.1.9\npydantic-settings                        2.9.1\ndeepseek ai chat client api\ngemini flash ai chat client api\n```",
      "state": "closed",
      "author": "RexsyBima",
      "author_type": "User",
      "created_at": "2025-05-19T06:30:27Z",
      "updated_at": "2025-05-27T06:27:23Z",
      "closed_at": "2025-05-27T06:21:50Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1756/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1756",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1756",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:47.976289",
      "comments": [
        {
          "author": "RexsyBima",
          "body": "so i moved my code from my fedora os machine back to the windows, then its all working fine, it seems the issue is something to do with the fedora distro rather than the code, i will ask about this in fedora forum instead. sorry ",
          "created_at": "2025-05-19T09:22:30Z"
        },
        {
          "author": "crazy-matt",
          "body": "@RexsyBima Hi\nHave you found anything or heard about Fedora on this one. I'm facing the same issue when running an MCP server with uv and python 3.12.",
          "created_at": "2025-05-25T19:34:35Z"
        },
        {
          "author": "RexsyBima",
          "body": "I feel it has something to do with Python and OpenSSL version compatibility, since i was running the program with uv which will install the latest python version but not openssl version, running the program with standar python works fine",
          "created_at": "2025-05-26T03:01:48Z"
        },
        {
          "author": "crazy-matt",
          "body": "Yeah same observation, using the python binary `/usr/bin/python3.13`, it works fine. That's the one deployed with Fedora and compatible with `OpenSSL 3.2.4 11 Feb 2025 (Library: OpenSSL 3.2.4 11 Feb 2025)` deployed with Fedora too.\nUsing another python like the one I was deploying with mise (a packa",
          "created_at": "2025-05-26T22:44:17Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Sorry you're having trouble, but this definitely isn't an issue with pydantic-ai. Your error is likely coming from Python or OpenSSL.",
          "created_at": "2025-05-27T06:21:50Z"
        }
      ]
    },
    {
      "issue_number": 1806,
      "title": "404 reponse",
      "body": "### Question\n\nWhile executing below code, result.output is 404, would like to understand what is this. Second question, is it necessary to run agent in with block \"async with agent.run_mcp_servers():\" for running MCP servers.\n\nUsing GPT4-0 Azure here.  \n\n\n```py\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.openai import OpenAIModel,OpenAIModelSettings\nfrom pydantic_ai.tools import Tool, ToolDefinition\nfrom pydantic_ai.providers.azure import AzureProvider\nimport nest_asyncio\nfrom pydantic import BaseModel\nfrom pydantic_ai.mcp import MCPServerHTTP\nnest_asyncio.apply()\n\nmodel_settings = OpenAIModelSettings(max_tokens=1000, temperature=0.9, \n                                     top_p=0.95, \n                                     seed=455, \n                                     timeout=3.0, \n                                     parallel_tool_calls=False,\n                                     stop_sequences=['\\n\\n'], )\n\n\nclass AddNumbers(BaseModel):\n    a: int\n    b: int\n\n# server = MCPServerHTTP(url='http://127.0.0.1:7860/gradio_api/mcp/sse') \n\nasync def  only_if_42(ctx: RunContext[AddNumbers], tool_def: ToolDefinition) -> ToolDefinition | None:\n    if ctx.deps.a == 42:\n        return tool_def\n    return None\n\nasync def add_two_numbers(ctx: RunContext[AddNumbers]) -> int:\n    return ctx.deps.a + ctx.deps.b\n\ntools = Tool(add_two_numbers,\n             takes_ctx=True,max_retries=3, \n             name='add_two_numbers',\n             description='Add two numbers', require_parameter_descriptions=False, \n             prepare=only_if_42,)\n\n\n\nagent = Agent(model, output_type=int, \n              instructions=\"always starts with howdy\", \n              deps_type=AddNumbers, \n              system_prompt=\"You are a cool teacher\", \n              name=\"Cool Agent\",\n              model_settings=model_settings,\n              output_retries=3,\n              tools=[tools],\n            #   mcp_servers=[server],\n              end_strategy='early',\n\n              )\n# async with agent.run_mcp_servers():\nresult =  agent.run_sync('what is this?', deps=AddNumbers(a=43, b=5))\n\nresult.output\n```\n**Sometime it does provide random number like 46 or something. Also doesn't follow instructions. Like use Howdy**\n\nOutput\n\n`404\n`\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "kauabh",
      "author_type": "User",
      "created_at": "2025-05-22T07:48:04Z",
      "updated_at": "2025-05-27T02:58:59Z",
      "closed_at": "2025-05-26T17:39:37Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1806/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1806",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1806",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:48.215546",
      "comments": [
        {
          "author": "DouweM",
          "body": "@kauabh I suggest adding Logfire (https://ai.pydantic.dev/logfire/) to debug what's being sent to and returned from the LLM, that'll help you debug this and other issues.\n\nI think the return value 404 just indicates that the model doesn't know what to do, so it borrows the \"Not Found\" error code fro",
          "created_at": "2025-05-26T17:39:34Z"
        },
        {
          "author": "kauabh",
          "body": "Hey @DouweM thanks for responding, the issue was in my code itself. It was **output_type=int** which are morphing any meaningful response from LLM.",
          "created_at": "2025-05-27T02:58:58Z"
        }
      ]
    },
    {
      "issue_number": 886,
      "title": "Add `id` and `finish_reason` to `ModelResponse`",
      "body": "These fields would be used to populate `gen_ai.response.id` and `gen_ai.response.finish_reasons` in https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/#genai-attributes",
      "state": "open",
      "author": "alexmojaki",
      "author_type": "User",
      "created_at": "2025-02-10T14:30:42Z",
      "updated_at": "2025-05-27T02:53:36Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "good first issue",
        "OpenTelemetry"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/886/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/886",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/886",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:48.431472",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "@alexmojaki please take a few minutes to add a description to your issue.",
          "created_at": "2025-02-11T05:54:03Z"
        },
        {
          "author": "nakranivaibhav",
          "body": "I would like to take this up. Some context would be really helpful.",
          "created_at": "2025-04-05T16:36:06Z"
        },
        {
          "author": "Wh1isper",
          "body": "related: https://github.com/pydantic/pydantic-ai/issues/509",
          "created_at": "2025-05-27T02:53:34Z"
        }
      ]
    },
    {
      "issue_number": 509,
      "title": "finish_reason for RunResult",
      "body": "Is there a way to view the finish_reason for a RunResult? (eg: end_turn, max_tokens, refusal, etc.)\r\n\r\nI'm using vertex and I thought it might be available in RunReason.usage().details, but that's actually None for me.\r\n\r\nThe reason I ask is that when I have a completion that is longer than max_tokens, I need to know that so that I can give the llm a chance to finish the completion.\r\n\r\nI'm using pydantic-ai-slim[vertexai]==0.0.14 and VertexAIModel('gemini-1.5-flash')",
      "state": "open",
      "author": "milest",
      "author_type": "User",
      "created_at": "2024-12-20T05:09:17Z",
      "updated_at": "2025-05-27T02:52:57Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/509/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/509",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/509",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:48.719724",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "I think that's a reasonable request.",
          "created_at": "2024-12-20T07:03:05Z"
        },
        {
          "author": "ianardee",
          "body": "Indeed this is quite useful as we want to be able adjust `max_tokens` so as to better manage our usage for various providers.\n\nRight now when I run using a `BaseModel` as the `result_type`, I'm getting these results with `max_tokens=5` (failing on purpose):\n\nOpenAI: gpt-4o-2024-11-20\n```\npydantic_ai",
          "created_at": "2025-04-04T11:02:08Z"
        },
        {
          "author": "Kludex",
          "body": "I was implementing this when I noticed that the OpenAI Responses API doesn't have this field. Is the field not necessary? 🤔 ",
          "created_at": "2025-04-18T11:16:29Z"
        },
        {
          "author": "milest",
          "body": "I think it's in `ResponseOutputMessage.status='incomplete'`\n\nThe details appear to be in `incomplete_details`\n\n`main.py`\n```python\nimport os\nfrom pprint import pp\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n",
          "created_at": "2025-04-18T13:37:41Z"
        },
        {
          "author": "Wh1isper",
          "body": "We just spent a lot of time solving the problem of rising retry counts for llm tool calls, and it was caused by small max_token configuration by provider(used default value). \n\nBeing able to see `finish_reason=length` could help us monitor the similar issues.",
          "created_at": "2025-05-27T02:52:56Z"
        }
      ]
    },
    {
      "issue_number": 822,
      "title": "OpenRouter API leads to pydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (1) for result validation. Deeper dive shown: \"Plain text responses are not permitted, please call one of the functions instead\"",
      "body": "\n**Title:** OpenRouter API Not Providing Structured Response - Exceeded Maximum Retries for Result Validation\n\n---\n\n**Description:**\n\nWhen using the OpenRouter API with `meta-llama/llama-3.3-70b-instruct`, I encounter an issue where the structured response is not returned, resulting in an exception after exceeding the maximum number of retries. However, the same code works as expected when using the `openai:gpt-4o-mini` model.\n\n---\n\n**Steps to Reproduce:**\n\n1. **Setup Code:**\n\n   ```python\n   import asyncio\n   import json\n   from pydantic import BaseModel\n   from some_module import (\n       search_company_summary,\n       OpenAIModel,\n       Agent,\n       OPENROUTER_API_KEY,\n   )\n\n   class CityLocation(BaseModel):\n       city: str\n       country: str\n\n   async def main():\n       company_name_to_search = \"20n Bio\"\n       # Use test data\n       search_results_with_verification = await search_company_summary(\n           company_name=company_name_to_search, use_test_data=True\n       )\n       print(\"\\n--- Final Results with Verification ---\")\n       print(json.dumps(search_results_with_verification, indent=2))\n\n       # Using OpenRouter API model\n       openrouter_model = OpenAIModel(\n           'meta-llama/llama-3.3-70b-instruct',\n           base_url='https://openrouter.ai/api/v1',\n           api_key=OPENROUTER_API_KEY,\n       )\n       agent = Agent(openrouter_model, result_type=CityLocation, result_tool_name='city_location')\n\n       # Using GPT 4o MINI model\n       agent = Agent('openai:gpt-4o-mini', result_type=CityLocation, result_tool_name='city_location')\n       result = await agent.run('Where the olympics held in 2012?')\n       print(result.data)\n       # Expected output:\n       #   city='London' country='United Kingdom'\n\n       print(result.cost())\n\n   if __name__ == \"__main__\":\n       asyncio.run(main())\n   ```\n\n2. **Run the script with OpenRouter API:**\n\n   ```plaintext\n   (parsely_v012325) PS D:\\VSCode Projects\\parsely_Jan25> & \"d:/VSCode Projects/parsely_Jan25/parsely_v012325/Scripts/python.exe\" \"d:/VSCode Projects/parsely_Jan25/test.py\"\n   d:\\VSCode Projects\\parsely_Jan25\\test.py:375: LogfireNotConfiguredWarning: No logs or spans will be created until logfire.configure() has been called. Set the environment variable LOGFIRE_IGNORE_NO_CONFIG=1 or add ignore_no_config=true in pyproject.toml to suppress this warning.\n   result = await agent.run('Where the olympics held in 2012?')\n   INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n   INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n   Traceback (most recent call last):\n     File \"d:\\VSCode Projects\\parsely_Jan25\\test.py\", line 382, in \n       asyncio.run(main())\n     File \"C:\\Users\\hshum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py\", line 190, in run\n       return runner.run(main)\n                ^^^^^^^^^^^^^^^\n     File \"C:\\Users\\hshum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py\", line 118, in run\n       return self._loop.run_until_complete(task)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n     File \"C:\\Users\\hshum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 653, in run_until_complete\n       return future.result()\n                ^^^^^^^^^^^^^^\n     File \"d:\\VSCode Projects\\parsely_Jan25\\test.py\", line 375, in main\n       result = await agent.run('Where the olympics held in 2012?')\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n     File \"D:\\VSCode Projects\\parsely_Jan25\\parsely_v012325\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 311, in run\n       final_result, tool_responses = await self._handle_model_response(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n     File \"D:\\VSCode Projects\\parsely_Jan25\\parsely_v012325\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 1108, in _handle_model_response\n       return await self._handle_text_response(text, run_context, result_schema)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n     File \"D:\\VSCode Projects\\parsely_Jan25\\parsely_v012325\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 1126, in _handle_text_response\n       self._incr_result_retry(run_context)\n     File \"D:\\VSCode Projects\\parsely_Jan25\\parsely_v012325\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 1300, in _incr_result_retry\n       raise exceptions.UnexpectedModelBehavior(\n   pydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (1) for result validation\n   (parsely_v012325) PS D:\\VSCode Projects\\parsely_Jan25>\n   ```\n\n3. **Run the script with GPT 4o MINI (Successful Case):**\n\n   ```plaintext\n   (parsely_v012325) PS D:\\VSCode Projects\\parsely_Jan25> & \"d:/VSCode Projects/parsely_Jan25/parsely_v012325/Scripts/python.exe\" \"d:/VSCode Projects/parsely_Jan25/test.py\"\n   d:\\VSCode Projects\\parsely_Jan25\\test.py:375: LogfireNotConfiguredWarning: No logs or spans will be created until logfire.configure() has been called. Set the environment variable LOGFIRE_IGNORE_NO_CONFIG=1 or add ignore_no_config=true in pyproject.toml to suppress this warning.\n   result = await agent.run('Where the olympics held in 2012?')\n   INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n   city='London' country='United Kingdom'\n   ```\n\n---\n\n**Additional Context:**\n\nThe issue seems to be related to the way the OpenRouter API handles structured responses. The error message indicates that the model behavior is unexpected, and the code exceeds the maximum retries for result validation:\n\n```python\nraise exceptions.UnexpectedModelBehavior(\n    pydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (1) for result validation\n```\n\nI suspect that the OpenRouter API does not allow plain text responses and expects a structured function call response. Meanwhile, the GPT 4o MINI model handles the structured response correctly.\n\n---\n\n**Core Function Example (Related to Entity Description):**\n\n```python\nasync def process_entity_description(content_text: str) -> EntityAndDescription:\n    \"\"\"\n    Processes the input content text to extract entity information and break it into sections.\n\n    Args:\n        content_text: The input text content.\n\n    Returns:\n        An EntityAndDescription object containing entity name, overall description, and section breakdowns.\n    \"\"\"\n    try:\n        with capture_run_messages() as messages:\n            entity_and_description_agent_result = await entity_and_description_agent.run(\n                user_prompt=(\n                    \"Please break down the following text into smaller, manageable sections, \"\n                    \"identify the key points, and provide a clear, decontextualized description for each section:\\n\\n\"\n                    f\"{content_text}\"\n                )\n            )\n            \n            return entity_and_description_agent_result.data\n\n    except UnexpectedModelBehavior as e:\n        logger.error(f\"UnexpectedModelBehavior in process_entity_description: {e}\")\n        logger.error(f\"Cause: {repr(e.__cause__)}\")\n        logger.error(f\"Messages: {messages}\")\n        raise e\n    except Exception as e:\n        logger.error(f\"Error in process_entity_description: {e}\")\n        raise e\n```\n\n---\n\n**Expected Behavior:**\n\n- When using the OpenRouter API, the model should return a structured response without exceeding the retry limit.\n- The agent should successfully validate the result and output the expected structured data.\n\n**Actual Behavior:**\n\n- The OpenRouter API returns a plain text response, leading to a validation failure after one retry, and the following error is raised:\n\n  ```plaintext\n  pydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (1) for result validation\n  ```\n\n- In contrast, using the GPT 4o MINI model produces the expected output.\n\n---\n\n**Environment:**\n\n- **Python Version:** 3.11 (as per traceback)\n- **Relevant Libraries/Packages:** pydantic_ai, httpx\n- **APIs:** OpenRouter API (`https://openrouter.ai/api/v1`), OpenAI API (for GPT 4o MINI)\n\n---\n\n**Additional Notes:**\n\n- The warning about `LogfireNotConfiguredWarning` is noted but does not appear to be directly related to the structured response issue.\n- The issue was identified by closely inspecting the captured run messages and traceback.\n\n---\n\n**Request:**\n\nCould you please investigate why the OpenRouter API is not returning the structured response and causing a result validation failure? Any insights into how to configure the API or modify the code to handle this behavior would be appreciated.\n\n---\n\n",
      "state": "closed",
      "author": "HomenShum",
      "author_type": "User",
      "created_at": "2025-01-30T22:27:00Z",
      "updated_at": "2025-05-26T19:35:24Z",
      "closed_at": "2025-03-29T14:00:51Z",
      "labels": [
        "more info",
        "Stale",
        "need confirmation"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/822/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/822",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/822",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:49.028296",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Please format the could and terminal output as markdown properly.\n\nThis is unreadable.",
          "created_at": "2025-01-31T04:34:42Z"
        },
        {
          "author": "HomenShum",
          "body": "> Please format the could and terminal output as markdown properly.\n> \n> This is unreadable.\n\nThank you for the feedback, updated with edit.",
          "created_at": "2025-01-31T23:52:52Z"
        },
        {
          "author": "sydney-runkle",
          "body": "Hmm, this just seems like a model limitation to me...",
          "created_at": "2025-02-04T01:18:24Z"
        },
        {
          "author": "jonchun",
          "body": "Agreed with @sydney-runkle - Llama 70b can struggle to properly call tools sometimes. Can you try posting some `DEBUG` level output so we can see the actual HTTP requests/responses from httpx? Make sure you strip out any sensitive data -- can't remember if it's included off the top of my head.",
          "created_at": "2025-02-07T11:27:04Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-02-14T14:00:44Z"
        }
      ]
    },
    {
      "issue_number": 1502,
      "title": "How to use streaming in sync ?",
      "body": "### Question\n\nCannot find any example on how to use streaming text chunks in **NON-async** context\n\ntrying something like this:\n```python\nimport asyncio\nfrom datetime import datetime\nfrom pydantic_ai import Agent\n\n\ndef get_event_loop():\n    try:\n        event_loop = asyncio.get_event_loop()\n    except RuntimeError:\n        event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(event_loop)\n    return event_loop\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    system_prompt=(\n        'You are are a helpful assistant that always answer with too many words loosing some times context but finally getting the right answer.'\n        'Also always call tool get_current_time to find the current time to know if answer make sense.'\n        'Basically be a bit annoying but always correct. it must be at least three paragraphs long.'\n    ),\n)\n\n\n@agent.tool\ndef get_current_time(ctx) -> str:\n    \"\"\"Get the current time.\"\"\"\n    print(f' --> call get_current_time <--')\n    return str(datetime.now())\n\n\nasync def agent_stream_deltas(agent):\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        async for chunk in response.stream_text(delta=True):\n            yield chunk\n\n\ndef agent_stream_sync(agent):\n    loop = get_event_loop()\n    gen = agent_stream_deltas(agent)\n    while True:\n        try:\n            chunk = loop.run_until_complete(gen.__anext__())\n            yield chunk\n        except StopAsyncIteration:\n            pass\n\n\nif __name__ == '__main__':\n    for chunk in agent_stream_sync(agent):\n        print(chunk, end='', flush=True)\n\n```\n\nit prints chunks as they arrive - but et the end crashes/freezes:\n\n\n```\n --> call get_current_time <--\nThe capital of the United Kingdom\n,...\n. The city is an exuberant mix of old and new, tradition and innovation, known for its diverse communities and vibrant urban fabric. Therefore, while my explanation may have taken you on a slightly winding path filled with relevant details, London stands firmly as the city's heart and soul of the United Kingdom.Failed to detach context\\\n\nTraceback (most recent call last):\n  File \"/private/tmp/aa_differencingly_kusti/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/private/tmp/aa_differencingly_kusti/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x1020d87c0> at 0x104550c00> was created in a different Context\n```\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "vitalik",
      "author_type": "User",
      "created_at": "2025-04-16T11:53:22Z",
      "updated_at": "2025-05-26T14:46:10Z",
      "closed_at": null,
      "labels": [
        "question",
        "asyncio"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1502/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1502",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1502",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:49.298721",
      "comments": [
        {
          "author": "DouweM",
          "body": "@vitalik Thanks for reporting this!\n\nThere are two things going on here:\n\n1. The script freezes because it never breaks out of the `while True` loop: in `except StopAsyncIteration`, can you change `pass` to `break`?\n\n2. `agent.run_stream` automatically wraps the work in a logfire span (even if you'r",
          "created_at": "2025-04-17T15:09:42Z"
        },
        {
          "author": "cspiecker",
          "body": "I’m also running into this issue. Would love to see proper sync support or a clean fix for the span issue. Thanks for looking into it!",
          "created_at": "2025-05-26T14:46:09Z"
        }
      ]
    },
    {
      "issue_number": 1440,
      "title": "Race Condition in GoogleVertexProvider Authentication Refresh Leading to AssertionError with Concurrent Requests",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using pydantic-ai with the GoogleVertexProvider, running multiple agents can lead to an AssertionError: **Expected token to be a string, got None within the _VertexAIAuth._refresh_token method.**\n\nThis suggests a race condition during the Google Cloud authentication token refresh process when handled concurrently by the provider's authentication mechanism.\n\nTo Reproduce\n\nSteps to reproduce the behavior:\n\n1. Set up an application using pydantic-ai with GoogleVertexProvider.\n2. Configure it to use service account credentials\n3. Instantiate multiple Agent objects that use GeminiModel \n4. Trigger multiple .run() calls on these agents concurrently (e.g., using asyncio.gather or similar).\n5. Observe the logs for the AssertionError.\n\n**Expected behavior**\nConcurrent requests using the GoogleVertexProvider should handle authentication token refreshes safely without race conditions, allowing all requests to proceed successfully.\n\n**Logs and Traceback**\n'\n  backend-1  |     await self._refresh_token()\n  backend-1  |   File \"/app/.venv/lib/python3.10/site-packages/pydantic_ai/providers/google_vertex.py\", line 169, in _refresh_token\n  backend-1  |     assert isinstance(self.credentials.token, str), f'Expected token to be a string, got {self.credentials.token}'  # type: ignore[reportUnknownMemberType]\n  backend-1  | AssertionError: Expected token to be a string, got None\n`\n\n**Possible Solution / Workaround**\nInstantiating a separate GoogleVertexProvider for each concurrent:\n           dedicated_http_client = httpx.AsyncClient()\n            agent_provider_instance = GoogleVertexProvider(\n                service_account_file=settings.GOOGLE_APPLICATION_CREDENTIALS,\n                project_id=settings.GOOGLE_VERTEX_PROJECT,\n                region=settings.GOOGLE_VERTEX_REGION,\n                http_client=dedicated_http_client\n            )\n\n**Additional context**\nThe error seems to occur specifically within the _refresh_token coroutine, likely because the self.credentials.refresh(Request()) call is not thread-safe or coroutine-safe when operating on the same credential object instance from multiple concurrent execution contexts.\n\n\n\n### Example Code\n\n```Python\nimport asyncio\nimport httpx\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Replace these with your actual values\nSERVICE_ACCOUNT_FILE = \"path/to/your/service-account.json\"\nPROJECT_ID = \"your-project-id\"\nREGION = \"us-central1\"\nMODEL_NAME = \"gemini-2.0-flash\"\n\nasync def run_agent(agent_id: int, provider: GoogleVertexProvider):\n    \"\"\"Run an agent with the given provider.\"\"\"\n    try:\n  \n        agent = Agent(MODEL_NAME, system_prompt=f\"Agent {agent_id} system prompt\")\n        result = await agent.run(\"Hello, how are you?\")\n        logger.info(f\"Agent {agent_id} completed successfully\")\n        return result\n    except Exception as e:\n        logger.error(f\"Agent {agent_id} failed: {e}\", exc_info=True)\n        return None\n\nasync def main():\n    # Create a single provider instance (this is the problematic setup)\n    shared_provider = GoogleVertexProvider(\n        service_account_file=SERVICE_ACCOUNT_FILE,\n        project_id=PROJECT_ID,\n        region=REGION\n    )\n\n    # Create multiple agents that share the same provider\n    agents = [\n        run_agent(i, shared_provider) for i in range(5)  # Run 5 agents concurrently\n    ]\n\n    # Run all agents concurrently\n    results = await asyncio.gather(*agents)\n    logger.info(f\"Results: {results}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n\"pydantic>=2.10.3\",\n    \"pydantic-settings>=2.6.1\",\n    \"pydantic-ai>=0.0.55\",\n    \"google-cloud-aiplatform>=1.88.0\",\n```",
      "state": "open",
      "author": "mmichelli",
      "author_type": "User",
      "created_at": "2025-04-10T18:04:23Z",
      "updated_at": "2025-05-26T12:03:14Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1440/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1440",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1440",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:49.506559",
      "comments": [
        {
          "author": "MariaHei",
          "body": "Can confirm, the following usually throws an AssertionError:\n```\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\nfrom pydantic_ai import Agent\nimport asyncio\n\nmodel = GeminiModel('gemini-1.5-flash', provider=GoogleVertexProvider(",
          "created_at": "2025-04-22T13:13:32Z"
        },
        {
          "author": "Jflick58",
          "body": "Seeing this behavior as well in 0.2.0 ",
          "created_at": "2025-05-12T21:28:12Z"
        },
        {
          "author": "Kludex",
          "body": "This should be solved with the new `GoogleModel`.\n\nPlease check https://ai.pydantic.dev/models/google/#google. 🙏 ",
          "created_at": "2025-05-21T07:07:25Z"
        },
        {
          "author": "fabien-marty",
          "body": "fixed for me with the new google provider",
          "created_at": "2025-05-26T12:03:13Z"
        }
      ]
    },
    {
      "issue_number": 1828,
      "title": "Falid to use pyinstaller to package pytantic-ai project",
      "body": "### Question\n\nI use a lot of methods of pyinstaller, including hidden-import and datas, but failed to package pytantic-ai project\n\nmy code:\n`from pydantic_ai import Agent`\n`print(Agent)`\n\nmy result：\n`Traceback (most recent call last):`\n  `File \"tst.py\", line 1, in <module>`\n  `File \"PyInstaller\\loader\\pyimod02_importers.py\", line 450, in exec_module`\n  `File \"pydantic_ai\\__init__.py\", line 49, in <module>`\n  `File \"importlib\\metadata\\__init__.py\", line 984, in version`\n  `File \"importlib\\metadata\\__init__.py\", line 957, in distribution`\n  `File \"importlib\\metadata\\__init__.py\", line 548, in from_name`\n`importlib.metadata.PackageNotFoundError: No package metadata was found for pydantic_ai_slim`\n`[PYI-21836:ERROR] Failed to execute script 'tst' due to unhandled exception!`\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "fengzengping",
      "author_type": "User",
      "created_at": "2025-05-26T08:10:08Z",
      "updated_at": "2025-05-26T08:34:25Z",
      "closed_at": "2025-05-26T08:34:25Z",
      "labels": [
        "question",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1828/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1828",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1828",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:49.740898",
      "comments": [
        {
          "author": "Kludex",
          "body": "I copied your code, installed pyinstaller via uv (`uv pip install pyinstaller`) and run `pyinstaller main.py`.\n\nIt worked here: `35540 INFO: Build complete! The results are available in: /Users/marcelotryle/dev/pydantic/pydantic-ai/dist`.",
          "created_at": "2025-05-26T08:24:43Z"
        },
        {
          "author": "Kludex",
          "body": "Maybe you don't have `pydantic-ai` installed in your virtual environment? I'm not sure how pyinstaller works.",
          "created_at": "2025-05-26T08:25:28Z"
        },
        {
          "author": "fengzengping",
          "body": "> Maybe you don't have `pydantic-ai` installed in your virtual environment? I'm not sure how pyinstaller works.\n\nThanks for your help.\nI fixed the issue by LLM\n\nmethod:\npyinstaller tst.py --hidden-import pydantic_ai_slim --copy-metadata pydantic_ai_slim --collect-data pydantic_ai_slim",
          "created_at": "2025-05-26T08:31:19Z"
        }
      ]
    },
    {
      "issue_number": 195,
      "title": "Manual intervention on tool calls",
      "body": "It seems like for all agents, tool calls are automatic. I.e Pydantic automatically runs the tool and passes the result back and continues the run.\r\n\r\nCan I manually intervene such that I want to manually run the tools and manage if I want to continue the run or not depending on the result?\r\n\r\n",
      "state": "open",
      "author": "vikigenius",
      "author_type": "User",
      "created_at": "2024-12-09T15:56:22Z",
      "updated_at": "2025-05-26T06:13:38Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/195/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/195",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/195",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:50.020330",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "This would be solved by #142.\r\n\r\n@dmontagu this is an argument to keep the `ctx.end_run(result)` idea.",
          "created_at": "2024-12-09T18:26:38Z"
        },
        {
          "author": "vikigenius",
          "body": "@samuelcolvin  I am not sure if I understand the PR. Is it ending the run after the tool execution or before?\r\n\r\nFor example, if I don't use any framework in OpenAI I have to manually inspect the tool_calls and call them myself. I want to be able to do that here for some agents.",
          "created_at": "2024-12-09T20:59:26Z"
        },
        {
          "author": "samuelcolvin",
          "body": "If you want to simply end the with specific data type, just use `result_type,` you can change the name of that tool with `result_tool_name`, e.g.:\r\n\r\n```py\r\nfrom pydantic import BaseModel\r\n\r\nfrom pydantic_ai import Agent\r\n\r\n\r\nclass CityLocation(BaseModel):\r\n    city: str\r\n    country: str\r\n\r\n\r\nagent",
          "created_at": "2024-12-10T09:52:56Z"
        },
        {
          "author": "conditionsofexistence",
          "body": "Hi guys! First off thanks a lot for your work on this library. \n\nI wanted to check in on this: I think the ask here is the ability configure the Agent so that when it decides to make a tool call, it just returns the tool call parameters and perhaps a callable to the actual function, but does not cal",
          "created_at": "2025-02-16T18:19:29Z"
        },
        {
          "author": "rubell",
          "body": "It is not exactly what you asked, but wouldn't 'agent.iter' work in this case?\n\n```python\nfrom pydantic_ai import CallToolsNode\n\nagent = Agent(...)\nasync with agent.iter(\"Some prompt.\") as aiter:\n    async for response in aiter:\n        if isinstance(response, CallToolsNode):\n            if should_b",
          "created_at": "2025-03-10T05:39:39Z"
        }
      ]
    },
    {
      "issue_number": 840,
      "title": "GoogleSearchTool for GeminiModel and VertexAIModel",
      "body": "One of the best things about using gemini is its ability to lean (ground?) on google search\nI can't figure out a way to include the GoogleSearchTool in my model configuration or agent through pydantic_ai to make this work\nIts trivial when using the google sdk directly. Am i missing something?\n\nCurrently i use a custom tool that just makes raw calls to gemini with grounding turned on, but thats not really very efficient or correct",
      "state": "open",
      "author": "fredmonroe",
      "author_type": "User",
      "created_at": "2025-02-01T16:55:07Z",
      "updated_at": "2025-05-25T23:10:29Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "toolsets"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/840/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/840",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/840",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:50.242950",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "Here is one way to do it.\n\nYou can import the dependency[1]  and create a tool python function [2] that will be attached to the Agent [3]\n\n### References:\n- [1] https://pypi.org/project/googlesearch-python/ \n- [2] https://ai.pydantic.dev/tools/\n- [3] https://ai.pydantic.dev/agents/",
          "created_at": "2025-02-01T17:25:03Z"
        },
        {
          "author": "samuelcolvin",
          "body": "@izzyacademy I think the OP is talking about [Gemini Search grounding](https://ai.google.dev/gemini-api/docs/grounding?lang=rest) which is completely different.",
          "created_at": "2025-02-01T18:33:32Z"
        },
        {
          "author": "sydney-runkle",
          "body": "I think this falls under the bucket of toolset support, which we definitely want to support here.",
          "created_at": "2025-02-04T01:12:05Z"
        },
        {
          "author": "tpotjj",
          "body": "Is there something I can do in order to implement this?\nReally want to get a real-time data API in PydanticAI for my use cases 😁",
          "created_at": "2025-02-11T13:22:36Z"
        },
        {
          "author": "tcrapts",
          "body": "I am also eagerly awaiting this feature. \n\n[AISDK has a very nice implementation](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai#search-grounding), which we could have the same. :)\n\n",
          "created_at": "2025-05-08T04:49:32Z"
        }
      ]
    },
    {
      "issue_number": 1749,
      "title": "Support for Dynamic Instructions",
      "body": "### Description\n\nHi everyone,\n\nFirst off, thank you to all contributors for this fantastic library — your work is greatly appreciated!\n\nI recently ran into a limitation when passing message history to an agent: the agent’s system prompts are overridden by any system prompts present in the message history. To avoid this, one can switch to using instructions instead of system prompts according to the docs, since system prompts/instructions in the message history do not override the agent's configured instructions. So far so good!\n\nHowever, I couldn't find any equivalent to dynamic system prompts when using instructions. In my case, I need to extend the agent's instructions with runtime-specific information. Supporting dynamic instructions would enable this use case and bring feature parity with dynamic system prompts.\n\nThanks for considering this!\n\n### References\n\n_No response_",
      "state": "open",
      "author": "xflashxx",
      "author_type": "User",
      "created_at": "2025-05-17T06:24:39Z",
      "updated_at": "2025-05-24T15:58:39Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1749/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1749",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1749",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:50.499812",
      "comments": [
        {
          "author": "kauabh",
          "body": "Hey @xflashxx  I believe dynamic instruction are supported. Check below examples \n\nExample 1\n\n```\nfrom pydantic_ai import Agent\n\nuser_is_logged_in = False\ninstructions = [\n    \"Reply with user Name John \" if user_is_logged_in else \"Reply with user Name Guest \",\n]\n\nagent = Agent(model=model,  name = ",
          "created_at": "2025-05-17T11:53:37Z"
        },
        {
          "author": "xflashxx",
          "body": "Hey @kauabh , thanks for your reply!\n\nYou're right — this is currently possible. However, your second example comes with a trade-off: it requires introducing additional dependencies directly into the agent, which can quickly lead to boilerplate, especially when working with agents inside a graph/nod",
          "created_at": "2025-05-17T18:01:04Z"
        },
        {
          "author": "kauabh",
          "body": "Hmmm, I see your point. Personally, managing all dependencies in one place sounds cleaner to me, but your example does raise some important points.",
          "created_at": "2025-05-17T18:27:50Z"
        },
        {
          "author": "kauabh",
          "body": "I believe you must have seen this example earlier from documentation (just sharing for the sake of completness ) https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.instructions but yeah it still require defining deps\n\n```\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test', deps_",
          "created_at": "2025-05-17T18:33:27Z"
        },
        {
          "author": "xflashxx",
          "body": "Yes, I saw that, but I couldn’t find anything analogous to dynamic system prompts for instructions.",
          "created_at": "2025-05-17T18:46:18Z"
        }
      ]
    },
    {
      "issue_number": 1681,
      "title": "How can i get content after the Agent finished Tool calling with stream call",
      "body": "### Question\n\nThis is a demo,the main feature is return the stream response after the agent do the tool calling,the fault is the finally response is empty string.The target function is `query`.\n\n```py\nimport fastapi\nfrom fastapi.responses import StreamingResponse,FileResponse,JSONResponse\nimport time\nimport logfire\nfrom pathlib import Path\nfrom typing import Annotated,Any\nimport random\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom dotenv import load_dotenv\nimport os\nfrom pydantic_ai import Agent,RunContext,Tool\nimport json\n\nload_dotenv()\ndeepseek_key=os.getenv(\"DEEPSEEK_API\")\nbase_url=os.getenv(\"DEEPSEEK_API_BASE\")\n\nsystem_prompt =\"\"\"\\\n You're a dice game, you should roll the die and see if the number\nyou get back matches the user's guess. If so, tell them they're a winner.\nUse the player's name in the response.\n\"\"\"\n\ndef roll_die()-> str:\n  \"\"\"Roll a six sided die and return the result.\"\"\"\n  return str(random.randint(1,6))\n\ndef get_player_name(ctx:RunContext[str])-> str:\n  \"\"\"Get the player's name\"\"\"\n  return ctx.deps\n\nmodel=OpenAIModel(\n  \"deepseek-chat\",\n   provider=OpenAIProvider(base_url=base_url,api_key=deepseek_key)\n  )\n\nagent =Agent(\n  model,\n  system_prompt=system_prompt,\n  deps_type=str,\n  tools=[roll_die,Tool(get_player_name,takes_ctx=True)],\n  instrument=True\n)\n\nlogfire.configure(send_to_logfire='if-token-present')\n\napp = fastapi.FastAPI()\nlogfire.instrument_fastapi(app,capture_headers=True)\nTHIS_DIR=Path(__file__).parent\n\ndef generate_large_text(turns:int):\n  for i in range(turns):\n    yield f\"This is line {i+1}\\n\"\n    time.sleep(0.5)\n\n@app.get(\"/stream/{turns}\")\nasync def stream_response(turns:int):\n  return StreamingResponse(generate_large_text(turns),media_type=\"text/plain\")\n\n@app.get(\"/\")\nasync def index():\n  return FileResponse((THIS_DIR/\"agent_home.html\"),media_type=\"text/html\")\n\n# this is the main function\n@app.post(\"/query\")\nasync def query(prompt:Annotated[str,fastapi.Form()],uname:Annotated[str,fastapi.Form()])-> StreamingResponse:\n  async def stream_messages(randInt:int=0):\n    async with agent.run_stream(prompt,deps=uname) as result:\n      async for text in result.stream(debounce_by=0.01):\n        print(\"text:\",text)\n        yield json.dumps({\"id\":randInt,\"text\":text}).encode('utf-8')+b'\\n'\n  ranInt = random.randint(0,199)\n  print({\"uname\":uname,\"prompt\":prompt})\n  return StreamingResponse(stream_messages(ranInt),media_type='text/plain')\n\nif __name__==\"__main__\":\n  import uvicorn\n  uvicorn.run(\"agnet_api_demo:app\",host=\"0.0.0.0\",reload=True,port=7474)\n```\n\nThis is the log.u can see the tool calling is after stream response.\n\n![Image](https://github.com/user-attachments/assets/3ef6c43d-dce3-4bc5-bed5-4bdbc8eae654)\n\nHow can i return the content after tool calling with stream response?\n\n\n\n### Additional Context\n\npydantic                                 2.11.3\npydantic-ai                              0.0.46\npydantic-ai-examples                     0.0.46\npydantic-ai-slim                         0.0.46\npydantic_core                            2.33.1\npydantic-graph                           0.0.46\npydantic-settings                        2.8.1\n--------\npython 3.10.12\n---------\nLLM deepseek-chat",
      "state": "closed",
      "author": "wiselyG",
      "author_type": "User",
      "created_at": "2025-05-10T10:35:48Z",
      "updated_at": "2025-05-23T14:00:38Z",
      "closed_at": "2025-05-23T14:00:36Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1681/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1681",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1681",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:50.747876",
      "comments": [
        {
          "author": "DouweM",
          "body": "@wiselyG Can you try switching to the `iter` approach documented in https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph, which gives you more visibility into which node is being streamed (e.g. `CallToolsNode`)?",
          "created_at": "2025-05-12T13:25:43Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-19T14:00:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-23T14:00:36Z"
        }
      ]
    },
    {
      "issue_number": 1679,
      "title": "How to ensure Agent retries MCP server tool calls with retries",
      "body": "I'm using `pydantic-ai` with an `MCPServerHTTP` for a smart home assistant. I want the agent to perform a two-step process for control tasks:\n1.  First, call a tool to query/list available smart home devices.\n2.  Then, use the information from step 1 (specifically the `entity_id`) to call another tool to control the target device.\n\nI've set `retries=3` on the `Agent`, expecting that if any part of this process (especially the tool calls via MCP server) fails, it would be retried.\n\nHowever, when I give a command like \"Turn off the monitor light bar,\" the agent seems to skip the device listing/identification step and immediately asks me for the `entity_ID`. This suggests it's not attempting the desired two-step process or retrying the initial discovery phase.\n\nIf the user's query was just about device status (e.g., \"Is the light on?\"), I'd expect it to perform the query step but not necessarily proceed to a control step. The current issue is about the control scenario where discovery should precede action.\n\n**Code (Python):**\n```python\nimport asyncio\nimport os\nimport sys\nfrom pydantic_ai.mcp import MCPServerHTTP\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom typing import List # Removed Union as it wasn't used\n\n# Dummy config values for reproducibility\nclass DummyConfig:\n    def get(self, key):\n        if key == 'ha_auth_token':\n            return \"DUMMY_HA_AUTH_TOKEN\"\n        if key == 'ha_server_url':\n            return \"http://localhost:8123/api/\" # Replace with your HA server URL\n        if key == 'sf_api_key':\n            return \"DUMMY_SF_API_KEY\"\n        if key == 'sf_base_url':\n            return \"DUMMY_SF_BASE_URL\" # Replace with your LLM provider base URL\n        return None\n\nconfig = DummyConfig()\n\nheaders = {\n    \"Authorization\": f\"Bearer {config.get('ha_auth_token')}\",\n}\nserver = MCPServerHTTP(url=config.get('ha_server_url'), headers=headers)\n\nsystem_prompt_en = \"\"\"You are a smart home assistant designed to help users control smart home devices.\nWhen a user asks you to turn a device on or off:\n1. First, call the tool to query the device list and entities to determine which device the user intends to control. You can judge based on name similarity.\n2. When calling the interface to switch the device and passing arguments:\n   Prioritize using domain, device_class, and entity_id to locate the device, rather than name.\n   Correct example, using entity_id:\n     <arguments>{\n       \"domain\": [\"switch\"],\n       \"device_class\": [\"switch\"],\n       \"entity_id\": \"switch.monitor_light_bar_switch\"\n     }</arguments>\n3. When the device switching interface returns \"isError\": false, it means the switch was successful.\n\"\"\"\n\nagent: Agent = Agent(\n    model=OpenAIModel(\n        model_name=\"THUDM/GLM-4-9B-0414\", # Example model\n        provider=OpenAIProvider(api_key=config.get('sf_api_key'), base_url=config.get('sf_base_url'))\n    ),\n    mcp_servers=[server],\n    system_prompt=system_prompt_en,\n    retries=3,\n    output_retries=3\n)\n\nasync def main():\n    async with agent.run_mcp_servers():\n        result = await agent.run('Turn off the monitor light bar.')\n        print(result)\n        print(f\"Agent Output: {result.output}\")\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n**Agent Output (Translated):**\n```\nAgentRunResult(output='\\nSorry, it seems there was a technical issue while trying to turn off the monitor light bar. To better assist you, please tell me the exact name or entity ID of the monitor light bar so I can assist you directly. If possible, you can find the relevant name in your smart home device list and provide it to me.')\n\nAgent Output: \nSorry, it seems there was a technical issue while trying to turn off the monitor light bar. To better assist you, please tell me the exact name or entity ID of the monitor light bar so I can assist you directly. If possible, you can find the relevant name in your smart home device list and provide it to me.\n```\n\n**Expected Behavior:**\nFor a command like \"Turn off the monitor light bar\":\n1.  Agent attempts to call a tool (via `MCPServerHTTP`) to list/query devices to find the \"monitor light bar\".\n2.  If this initial tool call fails or doesn't yield enough info, it should be retried (due to `retries=3`).\n3.  Once the device is identified (e.g., `entity_id: \"switch.monitor_light_bar_switch\"` is found), the agent attempts a second tool call to control this specific `entity_id`.\n4.  This second tool call should also be retried upon failure.\n5.  The agent should only ask the user for an `entity_id` if the entire multi-step process (including retries for each tool call) genuinely fails to identify or control the device.\n\n---\n\nHere is an image for Agent to use tools twice in Cherry Studio.\n\n![Image](https://github.com/user-attachments/assets/b1ec4f2b-3a3b-44ce-9316-b8d908018609)\n\nAnother example to ask the tempature,only use tool once.\n\n![Image](https://github.com/user-attachments/assets/79b1ad8a-96cc-41e9-a1a0-7e84af6f2ca8)\n\n---\n\n**Question:**\nHow can I configure or guide the `Agent` to reliably attempt this multi-step tool usage (e.g., first list/query devices, then control a specific device by its ID)? Specifically, how do I ensure the `retries` parameter applies to each tool call within such a sequence made via the `MCPServerHTTP` before the agent gives up and asks the user for direct input like an `entity_id`?\n\nThanks!\n",
      "state": "closed",
      "author": "xy3xy3",
      "author_type": "User",
      "created_at": "2025-05-09T17:01:54Z",
      "updated_at": "2025-05-23T14:00:38Z",
      "closed_at": "2025-05-23T14:00:38Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1679/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1679",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1679",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:51.007944",
      "comments": [
        {
          "author": "DouweM",
          "body": "@xy3xy3 Can you confirm you're on a PydanticAI v0.1.9 or later, so it includes https://github.com/pydantic/pydantic-ai/pull/1618 for MCP error handling?\n\nYour code and prompt seem to be properly set up to get the \"Expected Behavior\" you've described. It would be good to know what the LLM is actually",
          "created_at": "2025-05-12T13:58:32Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-19T14:00:37Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-23T14:00:37Z"
        }
      ]
    },
    {
      "issue_number": 1590,
      "title": "Structured Output fails with text output + Behaviour inconsistency",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nSituation:\n* I need LLM text output, to show it to the user, where the LLM reasons and explains.\n* I want a structured output.\n\nThis is NOT working in streaming mode, but it does in NON-streaming mode.\n\nSee below tests. The first fails. The second passes.\n\n### Example Code\n\n```Python\nfrom typing import Union\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\n\nclass CallAgent(BaseModel):\n    agent_name: str\n\n\nagent = Agent(\n    model=\"google_gla:gemini-2.5-flash-preview-04-17\",\n    output_type=Union[str, CallAgent],\n    instructions=\"Say hello and then transfer the user to 'user_assistant' agent\",\n)\n\n\nasync def test_output_with_str(): # FAILS\n    async with agent.run_stream(user_prompt=\"Hello\") as result:\n        async for msg, is_last in result.stream_structured():\n            print(msg)\n    assert result.get_output() == CallAgent(agent_name=\"user_assistant\")\n\n\nasync def test_output_with_str_no_stream(): # PASSES\n    result = await agent.run(user_prompt=\"Hello\")\n    assert result.output == CallAgent(agent_name=\"user_assistant\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydanticai 0.1.4\nany LLM\nPython 3.12\n```",
      "state": "open",
      "author": "IngLP",
      "author_type": "User",
      "created_at": "2025-04-25T10:23:40Z",
      "updated_at": "2025-05-23T13:54:34Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1590/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1590",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1590",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:51.298941",
      "comments": [
        {
          "author": "IngLP",
          "body": "It seems the fix can be this simple in agent.py, for agent responses including a tool call. unfortunately, this doesn't work if the agent just produces a text message.\n\n<img width=\"899\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1f96cd6e-671f-41d2-98d3-5de873c855d4\" />",
          "created_at": "2025-04-25T10:29:44Z"
        },
        {
          "author": "DouweM",
          "body": "@IngLP Can you please change `output_type=Union[str, CallAgent]` to `output_type=CallAgent` and see if it works as expected? That'll still allow the model to talk to the user before doing the handoff tool call, but will not cause PydanticAI to treat a text response as sufficient to complete the agen",
          "created_at": "2025-04-25T17:57:37Z"
        },
        {
          "author": "IngLP",
          "body": "Tried it, it doesn't work. Removing str from output_type PREVENTS the LLM to output text.",
          "created_at": "2025-04-26T09:24:07Z"
        },
        {
          "author": "IngLP",
          "body": "Moreover, this also blocks you from using stream_text(), which I need to show reasoning progress to the user.",
          "created_at": "2025-04-26T09:27:29Z"
        },
        {
          "author": "DouweM",
          "body": "@IngLP Thanks for trying that, you're right that that would not be the desired result... \n\nTo help us debug this further, can you please port your code to the new `iter` based approach described in https://github.com/pydantic/pydantic-ai/issues/1007#issuecomment-2690662109 (see the link to the docs ",
          "created_at": "2025-04-28T18:09:13Z"
        }
      ]
    },
    {
      "issue_number": 1765,
      "title": "Add vendor_id to streamed result",
      "body": "vendor_id was added to model response in #1547, but not added to streamed response. ",
      "state": "open",
      "author": "peterHoburg",
      "author_type": "User",
      "created_at": "2025-05-19T17:49:09Z",
      "updated_at": "2025-05-23T06:35:45Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1765/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "peterHoburg"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1765",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1765",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:51.585450",
      "comments": [
        {
          "author": "Kludex",
          "body": "Want to try to implement this @peterHoburg ?",
          "created_at": "2025-05-21T07:04:41Z"
        },
        {
          "author": "peterHoburg",
          "body": "@Kludex Definitely. I started doing it at the PyCon sprint, but didn't finish before I had to leave. ",
          "created_at": "2025-05-21T16:10:26Z"
        },
        {
          "author": "Kludex",
          "body": "Cool. I've assigned it to you. Thanks!\n\nLet us know if you need help. 🙏 ",
          "created_at": "2025-05-23T06:35:44Z"
        }
      ]
    },
    {
      "issue_number": 1811,
      "title": "Unexpected error when using `claude-sonnet-4-20250514` with v`0.2.6`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen trying to use the recently released Claude Sonnet model, I get this error:\n\n```\n.../pydantic_ai/models/__init__.py\", line 474, in infer_model\n    provider, model_name = model.split(':', maxsplit=1)\n                           ^^^^^^^^^^^\nAttributeError: 'tuple' object has no attribute 'split'\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython `3.12.10`\npydantic-ai `0.2.6`\nanthropic `0.52.0`\n```",
      "state": "closed",
      "author": "jerry-reevo",
      "author_type": "User",
      "created_at": "2025-05-22T20:02:52Z",
      "updated_at": "2025-05-22T20:22:43Z",
      "closed_at": "2025-05-22T20:22:30Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1811/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1811",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1811",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:51.791563",
      "comments": [
        {
          "author": "jerry-reevo",
          "body": "Issue was unrelated to `pydantic-ai`",
          "created_at": "2025-05-22T20:22:41Z"
        }
      ]
    },
    {
      "issue_number": 1418,
      "title": "Error: ModelHTTPError 400 - 'n' Argument Incompatible with gpt-4o-mini-search-preview",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen attempting to use the `gpt-4o-mini-search-preview` model with the OpenAI Python client, a `ModelHTTPError` is thrown due to the inclusion of the `n` parameter in the request. This parameter is hardcoded in the method `OpenAIModel._completions_create`, resulting in a **400 Bad Request** with the following message:\n\n```\nModelHTTPError: status_code: 400, model_name: gpt-4o-mini-search-preview, \nbody: {\n  'message': 'Model incompatible request argument supplied: n', \n  'type': 'invalid_request_error', \n  'param': None, \n  'code': None\n}\n```\n\n### Cause\n\nThe issue arises because `n=1` is being passed unconditionally to `self.client.chat.completions.create(...)`, but the `gpt-4o-mini-search-preview` model does not support the `n` parameter.\n\n```console\n\nin OpenAIModel._completions_create(self, messages, stream, model_settings, model_request_parameters)\n    264 try:\n--> 265     return await self.client.chat.completions.create(\n    266         model=self._model_name,\n    267         messages=openai_messages,\n    268         n=1,\n    269         parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),\n    270         tools=tools or NOT_GIVEN,\n    271         tool_choice=tool_choice or NOT_GIVEN,\n    272         stream=stream,\n    273         stream_options={'include_usage': True} if stream else NOT_GIVEN,\n    274         max_completion_tokens=model_settings.get('max_tokens', NOT_GIVEN),\n    275         temperature=model_settings.get('temperature', NOT_GIVEN),\n    276         top_p=model_settings.get('top_p', NOT_GIVEN),\n    277         timeout=model_settings.get('timeout', NOT_GIVEN),\n    278         seed=model_settings.get('seed', NOT_GIVEN),\n    279         presence_penalty=model_settings.get('presence_penalty', NOT_GIVEN),\n    280         frequency_penalty=model_settings.get('frequency_penalty', NOT_GIVEN),\n    281         logit_bias=model_settings.get('logit_bias', NOT_GIVEN),\n    282         reasoning_effort=model_settings.get('openai_reasoning_effort', NOT_GIVEN),\n    283         user=model_settings.get('openai_user', NOT_GIVEN),\n    284     )\n    285 except APIStatusError as e:\n\n```\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\n\nmodel = OpenAIModel(\"gpt-4o-mini-search-preview\")\nagent = Agent(model=model)\nagent.run_sync(\"Oscar winners 2025?\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.10.16\npydantic-ai==0.0.53\nopenai==1.68.2\n```",
      "state": "closed",
      "author": "caotanduc",
      "author_type": "User",
      "created_at": "2025-04-09T04:36:51Z",
      "updated_at": "2025-05-22T17:05:07Z",
      "closed_at": "2025-05-22T17:05:07Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1418/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1418",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1418",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:52.016694",
      "comments": []
    },
    {
      "issue_number": 1796,
      "title": "Enums seem to mess up PydanticAI calls",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIt seems that if I use an `Enum` somewhere in my `output_type` that the internals start complaining about invalid schemas. This could depend on the LLM backend, but figured that I should report it. \n\n### Example Code\n\nThis totally runs fine. \n\n```Python\nfrom enum import Enum\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', output_type=str)\n\nawait agent.run('I want to buy a hawai pizza')\n```\n\nThis does not. \n\n```python\nfrom enum import Enum\nfrom pydantic_ai import Agent\n\nclass PizzaKind(str, Enum):\n    FUNGI: \"fungi\"\n    HAWAI: \"hawai\"\n    PEPPERONI: \"pepperoni\"\n    \nagent = Agent('openai:gpt-4o', output_type=PizzaKind)\n\nawait agent.run('I want to buy a hawai pizza')\n```\n\n```\nTraceback (most recent call last):\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 277, in _completions_create\n    return await self.client.chat.completions.create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<23 lines>...\n    )\n    ^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<45 lines>...\n    )\n    ^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid schema for function 'final_result': In context=('properties', 'response'), schema must have a 'type' key.\", 'type': 'invalid_request_error', 'param': 'tools[0].function.parameters', 'code': 'invalid_function_parameters'}}\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/marimo/_runtime/executor.py\", line 101, in execute_cell_async\n    return await eval(cell.last_expr, glbls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  Cell \nmarimo:///Users/vincentwarmerdam/Development/youtube-material/examples/pydanticai-demo.py#cell=cell-8\n\n, line 8, in <module>\n    await agent.run('I want to buy a hawai pizza')\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 468, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 1942, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 279, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 332, in _make_request\n    model_response = await ctx.deps.model.request(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        ctx.state.message_history, model_settings, model_request_parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 203, in request\n    response = await self._completions_create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        messages, False, cast(OpenAIModelSettings, model_settings or {}), model_request_parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vincentwarmerdam/.cache/uv/archive-v0/GE4TLftHD6X2RdjhZYhL_/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 304, in _completions_create\n    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gpt-4o, body: {'message': \"Invalid schema for function 'final_result': In context=('properties', 'response'), schema must have a 'type' key.\", 'type': 'invalid_request_error', 'param': 'tools[0].function.parameters', 'code': 'invalid_function_parameters'}\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nFor future reference, it would be nice if I could do `agent.debug_info()` to just be able to generate this information automatically instead of manually. \n\nPython 3.13.3\n\nopenai==1.79.0\npydantic-ai==0.2.6\n```",
      "state": "closed",
      "author": "koaning",
      "author_type": "User",
      "created_at": "2025-05-21T12:30:13Z",
      "updated_at": "2025-05-22T09:16:07Z",
      "closed_at": "2025-05-21T16:30:33Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1796/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1796",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1796",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:52.016719",
      "comments": [
        {
          "author": "DouweM",
          "body": "@koaning I think your enum is improperly defined. It works like this, assigning the enum string values instead of using annotations:\n\n```py\nfrom enum import Enum\n\nfrom pydantic_ai import Agent\n\n\nclass PizzaKind(str, Enum):\n    FUNGI = \"fungi\"\n    HAWAI = \"hawai\"\n    PEPPERONI = \"pepperoni\"\n\n\nagent =",
          "created_at": "2025-05-21T16:30:33Z"
        },
        {
          "author": "koaning",
          "body": "d0h. I really need to stop writing code in new libraries at the end of my day. \n\nSorry for the noise folks!",
          "created_at": "2025-05-22T09:16:07Z"
        }
      ]
    },
    {
      "issue_number": 1102,
      "title": "Authentication issue with new GeminiModel(provider=GoogleVertexProvider(...))",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI've just updated to v0.0.36.\n\nI tried switching to the new way of declaring an agent with Vertex AI API:\n\n```python\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\n\nagent = Agent(\n    model=GeminiModel('gemini-1.5-flash', provider=GoogleVertexProvider(region='asia-southeast1', service_account_file=creds_path))\n)\n```\n\nIn this case, calling it with `.run()` always returns this following error:\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 403, model_name: gemini-1.5-flash-002, body: {\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"Request had insufficient authentication scopes.\",\n    \"status\": \"PERMISSION_DENIED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"method\": \"google.ai.generativelanguage.v1beta.GenerativeService.GenerateContent\",\n          \"service\": \"generativelanguage.googleapis.com\"\n        }\n      }\n    ]\n  }\n}\n```\n\nHowever, if I still use the deprecated way, it works just fine:\n\n```python\nfrom pydantic_ai.models.vertexai import VertexAIModel\n\nagent = Agent(\n    model=VertexAIModel('gemini-1.5-flash', region='asia-southeast1', service_account_file=creds_path)\n)\n```\n\nMy service account includes role \"Vertex AI Service Agent\".\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.13\npydantic-ai 0.0.36\n```",
      "state": "closed",
      "author": "dmitrybabanovforreal",
      "author_type": "User",
      "created_at": "2025-03-12T03:46:28Z",
      "updated_at": "2025-05-22T02:55:30Z",
      "closed_at": "2025-05-22T02:55:29Z",
      "labels": [
        "Stale",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1102/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1102",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1102",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:52.265175",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/575\" (95% similar)",
          "created_at": "2025-03-12T03:50:06Z"
        },
        {
          "author": "Kludex",
          "body": "@dmitrybabanovforreal Can you try to see if https://github.com/pydantic/pydantic-ai/pull/1119 fixes it? I couldn't try with a file yet. I'll try tomorrow.\n\nI couldn't find any other difference in the implementations.",
          "created_at": "2025-03-13T16:45:06Z"
        },
        {
          "author": "dmitrybabanovforreal",
          "body": "@Kludex No, I'm still getting the same error",
          "created_at": "2025-03-14T02:53:53Z"
        },
        {
          "author": "Kludex",
          "body": "Ok, I've tried now. I've created a service account with a single role: \"Vertex AI Service Agent\", and I can't reproduce it.\n\nCan you try the latest version, or try to recreate the service account key and see if it reproduces it?",
          "created_at": "2025-03-14T07:36:25Z"
        },
        {
          "author": "Kludex",
          "body": "Can you try the new `GoogleModel`? See https://ai.pydantic.dev/models/google/#google.\n\nIt should solve this issue.",
          "created_at": "2025-05-21T07:47:35Z"
        }
      ]
    },
    {
      "issue_number": 823,
      "title": "Anthropic streaming returns in single chunk when agent `result_type` is specified",
      "body": "I just tried Anthropic streaming (pydantic-ai v0.0.20) and the stream comes in as one big chunk, not as an actual stream (although there is no longer an error about anthropic streaming not being supported). I can confirm the code is correct since a simple switch to \"openai:gpt-4o\" model creates an actual stream.\n\n```\n...\nasync with agent.run_stream(\n    user_prompt=prompt,\n    deps=agent_deps,\n    message_history=message_history\n) as result:\n    # Stream the model's response\n    async for chunk in result.stream():\n        print(\"\\nStreaming chunk:\", chunk)\n        yield f\"event: document_messages\\ndata: {json.dumps(chunk)}\\n\\n\"\n...\n```\n\nI discovered that this only happens if I specify the `result_type` (streaming works fine without):\n```\nclass DocumentModelResponse(TypedDict, total=False):\n    model_response: str\n    document_md: str\n\nagent = Agent(\n    agent_data[\"model\"],\n    deps_type=AgentDeps,\n    result_type=DocumentModelResponse,\n    tools=tools,\n    system_prompt=system_prompt\n)\n```",
      "state": "closed",
      "author": "seunggs",
      "author_type": "User",
      "created_at": "2025-01-31T01:40:10Z",
      "updated_at": "2025-05-21T23:09:49Z",
      "closed_at": "2025-05-21T23:09:49Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/823/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/823",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/823",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:57.595864",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "My guess is this will be fixed by #582.",
          "created_at": "2025-01-31T16:13:06Z"
        },
        {
          "author": "seunggs",
          "body": "@samuelcolvin thanks for the response! \n\nHmm, I don't have a deep understanding of that discussion but isn't that ticket related to structured outputs? Claude works fine with `result_type` for non-streaming use cases so why would that ticket resolve the streaming issue?",
          "created_at": "2025-01-31T17:24:17Z"
        },
        {
          "author": "sydney-runkle",
          "body": "I can confirm that I also experienced this issue with anthropic.",
          "created_at": "2025-02-04T01:17:12Z"
        },
        {
          "author": "seunggs",
          "body": "Thanks for confirming @sydney-runkle - let me know if you have an ETA on this. Would really love to get streaming work for my app. Thanks!",
          "created_at": "2025-02-04T02:53:50Z"
        },
        {
          "author": "seunggs",
          "body": "@sydney-runkle Is there an ETA on this by any chance?",
          "created_at": "2025-02-10T21:06:04Z"
        }
      ]
    },
    {
      "issue_number": 1665,
      "title": "Structured streaming doesn't work with claude (Stream Whale example)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nStructured streaming doesn't doesn’t work with Claude.\n\nI tried using stream whales example as-is. While using openai:gpt-4, we see the expected streaming behavior with the table populating progressively. However, when switching to anthropic:claude-3-7-sonnet-latest, the output appears all at once rather than streaming incrementally.\n\n\n### Example Code\n\n```Python\nimport os\nfrom typing import Annotated\n\nimport logfire\nfrom dotenv import load_dotenv\nfrom pydantic import Field, ValidationError\nfrom pydantic_ai import Agent\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.table import Table\nfrom typing_extensions import NotRequired, TypedDict\n\nload_dotenv()\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire=\"if-token-present\")\n\n\nclass Whale(TypedDict):\n    name: str\n    length: Annotated[float, Field(description=\"Average length of an adult whale in meters.\")]\n    weight: NotRequired[\n        Annotated[\n            float,\n            Field(description=\"Average weight of an adult whale in kilograms.\", ge=50),\n        ]\n    ]\n    ocean: NotRequired[str]\n    description: NotRequired[Annotated[str, Field(description=\"Short Description\")]]\n\n\nagent = Agent(\"anthropic:claude-3-7-sonnet-latest\", output_type=list[Whale], instrument=True)\n\n\nasync def main():\n    console = Console()\n    with Live(\"\\n\" * 36, console=console) as live:\n        console.print(\"Requesting data...\", style=\"cyan\")\n        async with agent.run_stream(\"Generate me details of 5 species of Whale.\") as result:\n            console.print(\"Response:\", style=\"green\")\n\n            async for message, last in result.stream_structured(debounce_by=0.01):\n                try:\n                    whales = await result.validate_structured_output(message, allow_partial=not last)\n                except ValidationError as exc:\n                    if all(e[\"type\"] == \"missing\" and e[\"loc\"] == (\"response\",) for e in exc.errors()):\n                        continue\n                    else:\n                        raise\n\n                table = Table(\n                    title=\"Species of Whale\",\n                    caption=\"Streaming Structured responses from GPT-4\",\n                    width=120,\n                )\n                table.add_column(\"ID\", justify=\"right\")\n                table.add_column(\"Name\")\n                table.add_column(\"Avg. Length (m)\", justify=\"right\")\n                table.add_column(\"Avg. Weight (kg)\", justify=\"right\")\n                table.add_column(\"Ocean\")\n                table.add_column(\"Description\", justify=\"right\")\n\n                for wid, whale in enumerate(whales, start=1):\n                    table.add_row(\n                        str(wid),\n                        whale[\"name\"],\n                        f'{whale[\"length\"]:0.0f}',\n                        f\"{w:0.0f}\" if (w := whale.get(\"weight\")) else \"…\",\n                        whale.get(\"ocean\") or \"…\",\n                        whale.get(\"description\") or \"…\",\n                    )\n                live.update(table)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython = 3.10 \npydantic ai = 0.1.10\nllm = anthropic:claude-3-7-sonnet-latest\n```",
      "state": "closed",
      "author": "sgondala",
      "author_type": "User",
      "created_at": "2025-05-08T08:01:50Z",
      "updated_at": "2025-05-21T23:09:48Z",
      "closed_at": "2025-05-21T23:09:48Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1665/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1665",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1665",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:57.924698",
      "comments": [
        {
          "author": "DouweM",
          "body": "@sgondala Thanks for the report, I've implemented a fix in https://github.com/pydantic/pydantic-ai/pull/1669.",
          "created_at": "2025-05-08T12:57:18Z"
        }
      ]
    },
    {
      "issue_number": 1654,
      "title": "QWEN3-32B failed to call tool without argument in streaming",
      "body": "### Question\n\nThe following content is translated using translation tools, and there may be certain misunderstandings. \n## Problem\nWhen I use qwen3-32b as the model, when calling tools without parameters in streaming mode, the model always returns argument as None, which causes pydantic-ai to always think that the returned ToolCallPart is incomplete, treating it as ToolCallPartDelta, and ultimately causing it to be unable to call the tool. \n\nThere are three conditions that trigger this scenario: \n1. Using the stream method of ModelRequestNode in the iter method. \n2. The tool used does not require parameters. \n3. The model itself lacks the ability to return a non-None argument.\n\n## Code\n```python\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    PartDeltaEvent,\n    TextPartDelta,\n    ToolCallPartDelta,\n)\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIModel(\n    \"qwen3-32b\",\n    provider=OpenAIProvider(\n        api_key=\"sk-xxxxx\",\n        base_url=\"http://127.0.0.1:8080\",\n    ),\n)\nweather_agent = Agent(\n    model,\n    # \"deepseek:deepseek-chat\",\n    system_prompt=\"You are a weather agent. Please use get_weather() to get the weather.\",\n)\n\n\n@weather_agent.tool_plain\ndef get_weather() -> str:\n    return \"24°C and sunny.\"\n\n\noutput_messages: list[str] = []\n\n\nasync def main():\n    user_prompt = \"What will the weather be like?\"\n\n    # Begin a node-by-node, streaming iteration\n    async with weather_agent.iter(user_prompt) as run:\n        async for node in run:\n            if Agent.is_model_request_node(node):\n                async with node.stream(run.ctx) as request_stream:\n                    async for event in request_stream:\n                        if isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                print(event.delta.content_delta, end=\"\", flush=True)\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                print(\n                                    f\"[Request] Part {event.index} args_delta={event.delta.args_delta}\"\n                                )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n## Output\n```powershell\n# use qwen3-32b\nPS D:\\Code\\python\\agent-demo> & D:/Code/python/agent-demo/.venv/Scripts/python.exe d:/Code/python/agent-demo/test.py\n<think>\nOkay, the user is asking about the weather. I need to use the get_weather function. Wait, the function parameters are empty. Hmm, maybe the function doesn't require any arguments. But how does it know where to get the weather for? Oh, maybe it's designed to use the user's location automatically. I should check the function description, but it's empty here. Well, the instructions say to use get_weather, so I'll call it without any parameters. Let's see, the tool call should be a JSON object with the name and arguments. Since there are no parameters, arguments will be an empty object. Alright, that should work.\n</think>\n\n# use deepseek-chat\nPS D:\\Code\\python\\agent-demo> & D:/Code/python/agent-demo/.venv/Scripts/python.exe d:/Code/python/agent-demo/test.py\n[Request] Part 1 args_delta={}\nThe weather will be 24°C and sunny. Enjoy the pleasant day!\n```\n\n## Question\nMy question is, can this be considered a BUG in the streaming mode of pydantic-ai, or should it be seen as a problem caused by insufficient model capabilities?\n\n### Additional Context\n\npydantic-ai: 0.1.3\npython:3.12.8",
      "state": "closed",
      "author": "wusskk",
      "author_type": "User",
      "created_at": "2025-05-07T08:21:17Z",
      "updated_at": "2025-05-21T22:53:09Z",
      "closed_at": "2025-05-21T22:53:09Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1654/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1654",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1654",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:58.133548",
      "comments": [
        {
          "author": "DouweM",
          "body": "@wusskk Thanks for looking into this, I agree it's a bug that a [`ToolCallPartDelta`](https://github.com/pydantic/pydantic-ai/blob/dd22595464ea430d9fcea388e506bf0a1697a9a4/pydantic_ai_slim/pydantic_ai/messages.py#L637) will never turn into a fully-formed `ToolCallPart` until it's passed a non-`None`",
          "created_at": "2025-05-08T08:21:30Z"
        },
        {
          "author": "mdfareed92",
          "body": "Ran into the same issue with qwen2.5 32b using openai model. Wrote a dirty fix to solve the issue.\n\nAdded new method in [_parts_manager.py](https://github.com/pydantic/pydantic-ai/blob/dd22595464ea430d9fcea388e506bf0a1697a9a4/pydantic_ai_slim/pydantic_ai/_parts_manager.py#L124)\n\n```py\ndef handle_too",
          "created_at": "2025-05-16T22:37:47Z"
        }
      ]
    },
    {
      "issue_number": 1794,
      "title": "Pre-request check for request_tokens_limit exceeded",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nA request_tokens_limit check happens **after** the model returns a response, by which time the tokens have already been used and a response has already been produced, but the UsageLimitExceeded exception prevents the response from returning.\n\nIdeally this would happen before the request is made. Which would require estimating tokens on the client side, eg using [tiktoken](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for openai models or similar.\n\nIf this pre-request check is deemed infeasible perhaps the response can be returned nonetheless, since it has been produced, and only subsequent requests raise an exception.\n\neg:\n\n![Image](https://github.com/user-attachments/assets/61d0bc83-d8c3-4aae-953d-f8879ab0fc66)\n\n```\nTraceback (most recent call last):\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/opentelemetry/trace/__init__.py\", line 587, in use_span\n    yield span\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 261, in iter\n    yield GraphRun[StateT, DepsT, RunEndT](\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 683, in iter\n    yield agent_run\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 451, in run\n    async for _ in agent_run:\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 1798, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 810, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 783, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 270, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 329, in _make_request\n    return self._finish_handling(ctx, model_response, request_usage)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 356, in _finish_handling\n    ctx.deps.usage_limits.check_tokens(ctx.state.usage)\n  File \"/Users/mcbob/.venv/lib/python3.11/site-packages/pydantic_ai/usage.py\", line 112, in check_tokens\n    raise UsageLimitExceeded(\npydantic_ai.exceptions.UsageLimitExceeded: Exceeded the request_tokens_limit of 5000 (request_tokens=11725)\n```\n\n### Example Code\n\n```Python\nagent_response = await self.agent.run(\n                    user_prompt=last_user_message,\n                    message_history=history,\n                    usage_limits=UsageLimits(request_tokens_limit=5000),\n                )\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.2.6\n```",
      "state": "open",
      "author": "tekumara",
      "author_type": "User",
      "created_at": "2025-05-21T11:04:23Z",
      "updated_at": "2025-05-21T20:41:31Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1794/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1794",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1794",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:58.370815",
      "comments": [
        {
          "author": "DouweM",
          "body": "@tekumara I agree ideally we'd count request tokens beforehand, but lacking something like `tiktoken` for other models makes that infeasible -- although I'd be open to a proposal to implement this in a clean way for just OpenAI and letting users opt-in to that behavior.\n\nLetting an already-completed",
          "created_at": "2025-05-21T16:21:41Z"
        }
      ]
    },
    {
      "issue_number": 1798,
      "title": "Gemini 2.5 flash model Agent breaks with MCP servers",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHey there, I tried switching my agent to run on Gemini Flash rather than Anthropic / Claude and it seems something about the structure returned from tools breaks Gemini:\n\n```\n\nFile \"<obfuscated>/.venv/lib/python3.11/site-packages/pydantic_ai/models/gemini.py\", line 245, in _make_request\n    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=r.text)\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.5-flash-preview-05-20, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"created_after\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"created_before\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"tags\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"include_segments\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"include_related\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"query_audio\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"query_img\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"cuts\\\" at 'tools.function_declarations[17].parameters.properties[4].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"cuts\\\" at 'tools.function_declarations[18].parameters.properties[3].value': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools.function_declarations[16].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"created_after\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[16].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"created_before\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[16].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"tags\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[16].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"include_segments\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[16].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"include_related\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[16].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"query_audio\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[16].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"query_img\\\" at 'tools.function_declarations[16].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[17].parameters.properties[4].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"cuts\\\" at 'tools.function_declarations[17].parameters.properties[4].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[18].parameters.properties[3].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"cuts\\\" at 'tools.function_declarations[18].parameters.properties[3].value': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example Code\n\n```Python\nThere's a repo here:\n\nhttps://github.com/burningion/pydantic-video-editing-agent\n\n`uv run agent.py` works, but `uv run gemini-agent.py` fails with the above error.\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11,\n\nPydanticAI 0.2.6\n\nGoogle Gemini\n```",
      "state": "closed",
      "author": "burningion",
      "author_type": "User",
      "created_at": "2025-05-21T15:07:33Z",
      "updated_at": "2025-05-21T17:33:00Z",
      "closed_at": "2025-05-21T16:41:30Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1798/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1798",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1798",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:58.597609",
      "comments": [
        {
          "author": "DouweM",
          "body": "@burningion I think the issue is that in https://github.com/burningion/video-editing-mcp/blob/072b077778febc83d9a7e3e0c583e238c3aa2d6d/src/video_editor_mcp/server.py#L458-L491, the properties are added at the wrong level in the dict :) They have to be under `properties`, but are currently at the roo",
          "created_at": "2025-05-21T16:41:30Z"
        },
        {
          "author": "burningion",
          "body": "Hey! Amazing catch, thanks so much for the quick help and turnaround.",
          "created_at": "2025-05-21T17:32:59Z"
        }
      ]
    },
    {
      "issue_number": 1582,
      "title": "Pydantic AI incompatible with latest databricks ML runtime (16.3ML)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nDue to library mismatches, cannot use pydantic-ai with databricks' latest ML runtime https://docs.databricks.com/aws/en/release-notes/runtime/16.3ml\n\nBecause no versions of pydantic-ai match >0.1.4,<0.2.0\n and pydantic-ai (0.1.4) depends on pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,groq,mcp,mistral,openai,vertexai] (0.1.4), pydantic-ai (>=0.1.4,<0.2.0) requires pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,groq,mcp,mistral,openai,vertexai] (0.1.4).\n\nAnd because pydantic-ai-slim[anthropic,bedrock,cli,cohere,evals,groq,mcp,mistral,openai,vertexai] (0.1.4) depends on requests (>=2.32.3), pydantic-ai (>=0.1.4,<0.2.0) requires requests (>=2.32.3).\n\nSo, because <my-lib> depends on both requests (2.32.2) and pydantic-ai (^0.1.4), version solving failed.\n\n\n### Example Code\n\n```Python\nSome code you could use to create/test a compatible python environment for the 16.3ML runtime on ubuntu:\n\n##\n# ubuntu packages\n#\n\n# sudo apt-get update\nsudo apt install pkg-config\nsudo apt install libhdf5-dev\nsudo apt install libsnappy-dev build-essential\nsudo apt install libcairo2-dev pkg-config build-essential\nsudo apt install libgirepository1.0-dev gobject-introspection build-essential pkg-config\nsudo apt install libdbus-1-dev pkg-config build-essential\n\n##\n# Python\n#\n\nwget https://www.python.org/ftp/python/3.12.3/Python-3.12.3.tgz\ntar -xzvf Python-3.12.3.tgz\ncd Python-3.12.3/; ./configure --enable-optimizations; make -j `nproc`;\nsudo rm /usr/local/bin/python3.11\nsudo make altinstall\n\npython3.12 -V\npython3.12 -m pip install --upgrade pip\npython3.12 -m pip install virtualenv\n\n##\n# Virtualenv\n#\n\npip cache purge\npython3.12 -m virtualenv -p /usr/local/bin/python3.12 venv\nsource venv/bin/activate\npip install --upgrade pip\npip install poetry\npoetry install\n\n\nthen have the following requirements in pyproject.toml:\n\n[tool.poetry.dependencies]\npython = \"3.12.3\"\nabsl-py = \"1.0.0\"\naccelerate = \"1.4.0\"\naiohttp = \"3.9.5\"\naiohttp-cors = \"0.7.0\"\naiosignal = \"1.2.0\"\nalembic = \"1.14.1\"\nannotated-types = \"0.7.0\"\nanyio = \"4.2.0\"\nargcomplete = \"3.5.3\"\nargon2-cffi = \"21.3.0\"\nargon2-cffi-bindings = \"21.2.0\"\narrow = \"1.2.3\"\nastor = \"0.8.1\"\nasttokens = \"2.0.5\"\nastunparse = \"1.6.3\"\nasync-lru = \"2.0.4\"\nattrs = \"23.1.0\"\naudioread = \"3.0.1\"\nautocommand = \"2.2.2\"\nazure-core = \"1.32.0\"\nazure-cosmos = \"4.3.1\"\nazure-identity = \"1.20.0\"\nazure-storage-blob = \"12.23.0\"\nazure-storage-file-datalake = \"12.17.0\"\nBabel = \"2.11.0\"\nbackoff = \"2.2.1\"\n\"backports.tarfile\" = \"1.2.0\"\nbcrypt = \"3.2.0\"\nbeautifulsoup4 = \"4.12.3\"\nblack = \"24.4.2\"\nbleach = \"4.1.0\"\nblinker = \"1.7.0\"\nblis = \"0.7.11\"\nboto3 = \"1.34.69\"\nbotocore = \"1.34.69\"\nBrotli = \"1.0.9\"\ncachetools = \"5.3.3\"\ncatalogue = \"2.0.10\"\ncategory-encoders = \"2.6.3\"\ncertifi = \"2024.6.2\"\ncffi = \"1.16.0\"\nchardet = \"4.0.0\"\ncharset-normalizer = \"2.0.4\"\ncircuitbreaker = \"2.0.0\"\nclick = \"8.1.7\"\ncloudpathlib = \"0.20.0\"\ncloudpickle = \"2.2.1\"\ncmdstanpy = \"1.2.5\"\ncolorful = \"0.5.6\"\ncolorlog = \"6.9.0\"\ncomm = \"0.2.1\"\ncomposer = \"0.29.0\"\nconfection = \"0.1.5\"\nconfigparser = \"5.2.0\"\ncontourpy = \"1.2.0\"\ncoolname = \"2.2.0\"\ncryptography = \"42.0.5\"\ncycler = \"0.11.0\"\ncymem = \"2.0.11\"\nCython = \"3.0.11\"\ndacite = \"1.9.2\"\ndatabricks-automl-runtime = \"0.2.21\"\ndatabricks-feature-engineering = \"0.8.0\"\ndatabricks-sdk = \"0.30.0\"\ndatasets = \"3.3.2\"\ndbl-tempo = \"0.1.26\"\ndbus-python = \"1.3.2\"\ndebugpy = \"1.6.7\"\ndecorator = \"5.1.1\"\ndeepspeed = \"0.16.4\"\ndefusedxml = \"0.7.1\"\nDeprecated = \"1.2.18\"\ndill = \"0.3.8\"\ndistlib = \"0.3.8\"\ndm-tree = \"0.1.9\"\ndocstring-to-markdown = \"0.11\"\neinops = \"0.8.1\"\nentrypoints = \"0.4\"\nevaluate = \"0.4.3\"\nexecuting = \"0.8.3\"\nfacets-overview = \"1.1.1\"\nFarama-Notifications = \"0.0.4\"\nfastjsonschema = \"2.21.1\"\nfasttext-wheel = \"0.9.2\"\nfilelock = \"3.13.1\"\nFlask = \"2.2.5\"\nflatbuffers = \"25.2.10\"\nfonttools = \"4.51.0\"\nfqdn = \"1.5.1\"\nfrozenlist = \"1.4.0\"\nfsspec = \"2023.5.0\"\nfuture = \"0.18.3\"\ngast = \"0.4.0\"\ngitdb = \"4.0.11\"\nGitPython = \"3.1.37\"\ngoogle-api-core = \"2.20.0\"\ngoogle-auth = \"2.21.0\"\ngoogle-auth-oauthlib = \"1.2.1\"\ngoogle-cloud-core = \"2.4.2\"\ngoogle-cloud-storage = \"2.10.0\"\ngoogle-crc32c = \"1.6.0\"\ngoogle-pasta = \"0.2.0\"\ngoogle-resumable-media = \"2.7.2\"\ngoogleapis-common-protos = \"1.68.0\"\ngql = \"3.5.0\"\ngraphql-core = \"3.2.4\"\ngreenlet = \"3.0.1\"\ngrpcio = \"1.60.0\"\ngrpcio-status = \"1.60.0\"\ngunicorn = \"20.1.0\"\ngviz-api = \"1.10.0\"\ngymnasium = \"0.28.1\"\nh11 = \"0.14.0\"\nh5py = \"3.11.0\"\nhjson = \"3.1.0\"\nholidays = \"0.54\"\nhtmlmin = \"0.1.12\"\nhttpcore = \"1.0.7\"\nhttplib2 = \"0.20.4\"\nhttpx = \"0.28.1\"\nhuggingface-hub = \"0.29.1\"\nidna = \"3.7\"\nImageHash = \"4.3.1\"\nimageio = \"2.33.1\"\nimbalanced-learn = \"0.12.3\"\nimportlib-metadata = \"6.0.0\"\nimportlib_resources = \"6.5.2\"\ninflect = \"7.3.1\"\nipyflow-core = \"0.0.201\"\nipykernel = \"6.28.0\"\nipython = \"8.25.0\"\nipython-genutils = \"0.2.0\"\nipywidgets = \"7.7.2\"\nisodate = \"0.6.1\"\nisoduration = \"20.11.0\"\nitsdangerous = \"2.2.0\"\n\"jaraco.context\" = \"5.3.0\"\n\"jaraco.functools\" = \"4.0.1\"\n\"jaraco.text\" = \"3.12.1\"\njax-jumpy = \"1.0.0\"\njedi = \"0.19.1\"\nJinja2 = \"3.1.4\"\njiter = \"0.8.2\"\njmespath = \"1.0.1\"\njoblib = \"1.4.2\"\njoblibspark = \"0.5.1\"\njson5 = \"0.9.6\"\njsonpatch = \"1.33\"\njsonpointer = \"3.0.0\"\njsonschema = \"4.19.2\"\njsonschema-specifications = \"2023.7.1\"\njupyter-events = \"0.10.0\"\njupyter-lsp = \"2.2.0\"\njupyter_client = \"8.6.0\"\njupyter_core = \"5.7.2\"\njupyter_server = \"2.14.1\"\njupyter_server_terminals = \"0.4.4\"\njupyterlab = \"4.0.11\"\njupyterlab-pygments = \"0.1.2\"\njupyterlab_server = \"2.25.1\"\nkeras = \"3.8.0\"\nkiwisolver = \"1.4.4\"\nlangchain = \"0.3.19\"\nlangchain-core = \"0.3.39\"\nlangchain-text-splitters = \"0.3.6\"\nlangcodes = \"3.5.0\"\nlangsmith = \"0.1.133\"\nlanguage_data = \"1.3.0\"\nlaunchpadlib = \"1.11.0\"\n\"lazr.restfulclient\" = \"0.14.6\"\n\"lazr.uri\" = \"1.0.6\"\nlazy_loader = \"0.4\"\nlibclang = \"15.0.6.1\"\nlibrosa = \"0.10.2\"\nlightgbm = \"4.5.0\"\nlightning-utilities = \"0.12.0\"\nlinkify-it-py = \"2.0.0\"\nllvmlite = \"0.42.0\"\nlz4 = \"4.3.2\"\nMako = \"1.2.0\"\nmarisa-trie = \"1.2.0\"\nMarkdown = \"3.4.1\"\nmarkdown-it-py = \"2.2.0\"\nMarkupSafe = \"2.1.3\"\nmatplotlib = \"3.8.4\"\nmatplotlib-inline = \"0.1.6\"\nmccabe = \"0.7.0\"\nmdit-py-plugins = \"0.3.0\"\nmdurl = \"0.1.0\"\nmemray = \"1.15.0\"\nmistune = \"2.0.4\"\nml-dtypes = \"0.4.1\"\nmlflow-skinny = \"2.19.0\"\nmore-itertools = \"10.3.0\"\nmosaicml-cli = \"0.6.41\"\nmosaicml-streaming = \"0.11.0\"\nmpmath = \"1.3.0\"\nmsal = \"1.31.1\"\nmsal-extensions = \"1.2.0\"\nmsgpack = \"1.1.0\"\nmultidict = \"6.0.4\"\nmultimethod = \"1.12\"\nmultiprocess = \"0.70.16\"\nmurmurhash = \"1.0.12\"\nmypy = \"1.10.0\"\nmypy-extensions = \"1.0.0\"\nnamex = \"0.0.8\"\nnbclient = \"0.8.0\"\nnbconvert = \"7.10.0\"\nnbformat = \"5.9.2\"\nnest-asyncio = \"1.6.0\"\nnetworkx = \"3.2.1\"\nninja = \"1.11.1.1\"\nnltk = \"3.8.1\"\nnodeenv = \"1.9.1\"\nnotebook = \"7.0.8\"\nnotebook_shim = \"0.2.3\"\nnumba = \"0.59.1\"\nnumpy = \"1.26.4\"\noauthlib = \"3.2.0\"\noci = \"2.146.0\"\nopenai = \"1.64.0\"\nopencensus = \"0.11.4\"\nopencensus-context = \"0.1.3\"\nopentelemetry-api = \"1.30.0\"\nopentelemetry-sdk = \"1.30.0\"\nopentelemetry-semantic-conventions = \"0.51b0\"\nopt_einsum = \"3.4.0\"\noptree = \"0.14.0\"\noptuna = \"3.6.1\"\noptuna-integration = \"3.6.0\"\norjson = \"3.10.15\"\noverrides = \"7.4.0\"\npackaging = \"24.1\"\npandas = \"1.5.3\"\npandocfilters = \"1.5.0\"\nparamiko = \"3.4.0\"\nparso = \"0.8.3\"\npathspec = \"0.10.3\"\npatsy = \"0.5.6\"\npexpect = \"4.8.0\"\nphik = \"0.12.4\"\npillow = \"10.3.0\"\npip = \"24.2\"\nplatformdirs = \"3.10.0\"\nplotly = \"5.22.0\"\npluggy = \"1.0.0\"\npmdarima = \"2.0.4\"\npooch = \"1.8.2\"\nportalocker = \"2.10.1\"\npreshed = \"3.0.9\"\nprometheus-client = \"0.14.1\"\nprompt-toolkit = \"3.0.43\"\nprophet = \"1.1.5\"\nproto-plus = \"1.26.0\"\nprotobuf = \"4.24.1\"\npsutil = \"5.9.0\"\npsycopg2 = \"2.9.3\"\nptyprocess = \"0.7.0\"\npure-eval = \"0.2.2\"\npy-cpuinfo = \"9.0.0\"\npy-spy = \"0.4.0\"\npyarrow = \"15.0.2\"\npyasn1 = \"0.4.8\"\npyasn1-modules = \"0.2.8\"\npybind11 = \"2.13.6\"\npyccolo = \"0.0.65\"\npycparser = \"2.21\"\npydantic = \"2.8.2\"\npydantic_core = \"2.20.1\"\npyflakes = \"3.2.0\"\nPygments = \"2.15.1\"\nPyGObject = \"3.48.2\"\nPyJWT = \"2.7.0\"\nPyNaCl = \"1.5.0\"\npyodbc = \"5.0.1\"\npyOpenSSL = \"24.0.0\"\npyparsing = \"3.0.9\"\npyright = \"1.1.294\"\npytesseract = \"0.3.10\"\npython-dateutil = \"2.9.0.post0\"\npython-editor = \"1.0.4\"\npython-json-logger = \"2.0.7\"\npython-lsp-jsonrpc = \"1.1.2\"\npython-lsp-server = \"1.10.0\"\npython-snappy = \"0.6.1\"\npytoolconfig = \"1.2.6\"\npytorch-ranger = \"0.1.1\"\npytz = \"2024.1\"\nPyWavelets = \"1.5.0\"\nPyYAML = \"6.0.1\"\npyzmq = \"25.1.2\"\nquestionary = \"2.1.0\"\nray = \"2.37.0\"\nreferencing = \"0.30.2\"\nregex = \"2023.10.3\"\nrequests = \"2.32.2\"\nrequests-oauthlib = \"1.3.1\"\nrequests-toolbelt = \"1.0.0\"\nrfc3339-validator = \"0.1.4\"\nrfc3986-validator = \"0.1.1\"\nrich = \"13.3.5\"\nrope = \"1.12.0\"\nrpds-py = \"0.10.6\"\nrsa = \"4.9\"\n\"ruamel.yaml\" = \"0.18.10\"\n\"ruamel.yaml.clib\" = \"0.2.12\"\ns3transfer = \"0.10.2\"\nsafetensors = \"0.4.4\"\nscikit-image = \"0.23.2\"\nscikit-learn = \"1.4.2\"\nscipy = \"1.13.1\"\nseaborn = \"0.13.2\"\nSend2Trash = \"1.8.2\"\nsentence-transformers = \"3.4.1\"\nsentencepiece = \"0.2.0\"\nsetuptools = \"74.0.0\"\nshap = \"0.46.0\"\nshellingham = \"1.5.4\"\nsimplejson = \"3.17.6\"\nsix = \"1.16.0\"\nslicer = \"0.0.8\"\nsmart-open = \"5.2.1\"\nsmmap = \"5.0.0\"\nsniffio = \"1.3.0\"\nsoundfile = \"0.12.1\"\nsoupsieve = \"2.5\"\nsoxr = \"0.5.0.post1\"\nspacy = \"3.7.5\"\nspacy-legacy = \"3.0.12\"\nspacy-loggers = \"1.0.5\"\nSQLAlchemy = \"2.0.30\"\nsqlparse = \"0.4.2\"\nsrsly = \"2.5.1\"\nssh-import-id = \"5.11\"\nstack-data = \"0.2.0\"\nstanio = \"0.5.1\"\nstatsmodels = \"0.14.2\"\nsympy = \"1.13.1\"\ntabulate = \"0.9.0\"\ntangled-up-in-unicode = \"0.2.0\"\ntenacity = \"8.2.2\"\ntensorboard = \"2.18.0\"\ntensorboard-data-server = \"0.7.2\"\ntensorboard-plugin-profile = \"2.18.0\"\ntensorboardX = \"2.6.2.2\"\ntensorflow = \"2.18.0\"\ntensorflow-estimator = \"2.15.0\"\ntermcolor = \"2.5.0\"\nterminado = \"0.17.1\"\ntextual = \"2.1.1\"\ntf_keras = \"2.18.0\"\nthinc = \"8.2.5\"\nthreadpoolctl = \"2.2.0\"\ntifffile = \"2023.4.12\"\ntiktoken = \"0.7.0\"\ntinycss2 = \"1.2.1\"\ntokenize-rt = \"4.2.1\"\ntokenizers = \"0.21.0\"\ntomli = \"2.0.1\"\n# torch = \"2.6.0+cpu\"\n# torch-optimizer = \"0.3.0\"\n# torcheval = \"0.0.7\"\n# torchmetrics = \"1.6.0\"\n# torchvision = \"0.21.0+cpu\"\ntornado = \"6.4.1\"\ntqdm = \"4.66.4\"\ntraitlets = \"5.14.3\"\ntransformers = \"4.49.0\"\ntypeguard = \"4.4.2\"\ntyper = \"0.15.1\"\ntypes-protobuf = \"3.20.3\"\ntypes-psutil = \"5.9.0\"\ntypes-pytz = \"2023.3.1.1\"\ntypes-PyYAML = \"6.0.0\"\ntypes-requests = \"2.31.0.0\"\ntypes-setuptools = \"68.0.0.0\"\ntypes-six = \"1.16.0\"\ntypes-urllib3 = \"1.26.25.14\"\ntyping_extensions = \"4.11.0\"\nuc-micro-py = \"1.0.1\"\nujson = \"5.10.0\"\n# unattended-upgrades\" = \"0.1\"\nuri-template = \"1.3.0\"\nurllib3 = \"1.26.16\"\nvalidators = \"0.34.0\"\nvirtualenv = \"20.26.2\"\nvisions = \"0.7.5\"\nwadllib = \"1.3.6\"\nwasabi = \"1.1.3\"\nwcwidth = \"0.2.5\"\nweasel = \"0.4.1\"\nwebcolors = \"24.11.1\"\nwebencodings = \"0.5.1\"\nwebsocket-client = \"1.8.0\"\nwebsockets = \"11.0.3\"\nWerkzeug = \"3.0.3\"\nwhatthepatch = \"1.0.2\"\nwheel = \"0.43.0\"\nwordcloud = \"1.9.4\"\nwrapt = \"1.14.1\"\nxgboost = \"2.0.3\"\nxgboost-ray = \"0.1.19\"\nxxhash = \"3.4.1\"\nyapf = \"0.33.0\"\nyarl = \"1.9.3\"\nydata-profiling = \"4.9.0\"\nzipp = \"3.17.0\"\nzstd = \"1.5.5.1\"\n\n\n(requirements copied from here - https://docs.databricks.com/aws/en/assets/files/requirements-16.3-56f5570762316276d2323933fd727dd0.txt)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12.3\npydantic-ai c\n```",
      "state": "open",
      "author": "lukaszdz",
      "author_type": "User",
      "created_at": "2025-04-24T18:00:53Z",
      "updated_at": "2025-05-21T17:31:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1582/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1582",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1582",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:58.896726",
      "comments": [
        {
          "author": "lukaszdz",
          "body": "@Kludex  Just tried to install pydantic-ai in the context of the requirements from the 16.3ML runtime, and got this error:\n\n```\npoetry add pydantic-ai@latest\nUsing version ^0.1.6 for pydantic-ai\n\nUpdating dependencies\nResolving dependencies... (0.0s)\n\nBecause no versions of pydantic-ai match >0.1.6,",
          "created_at": "2025-04-25T16:49:54Z"
        },
        {
          "author": "Kludex",
          "body": "We need to drop google-auth as well.",
          "created_at": "2025-04-25T18:30:26Z"
        },
        {
          "author": "lukaszdz",
          "body": "Would definitely recommend using a fresh python 3.12.3 + virtualenv w/ the dependencies above, then testing updated versions of pydantic. Could be more than `google-auth` in conflict. The different databricks runtimes (16.3 and 16.3ML) have different requirements.txt's.",
          "created_at": "2025-04-25T20:01:15Z"
        },
        {
          "author": "Kludex",
          "body": "I can't really install the dependencies on my machine (maybe because I'm on MacOS).\n\nI recommend installing `pydantic-ai-slim` with the specific extras that you need, and if there's any issue, I'm happy to fix it.",
          "created_at": "2025-05-21T07:30:16Z"
        },
        {
          "author": "lukaszdz",
          "body": "Thanks for looking into this. Databricks summit (20,000 attendees) is in early June, so your team may miss on word of mouth growth if pydantic-ai isn't compatible with these databrick runtimes out of the box.",
          "created_at": "2025-05-21T17:31:05Z"
        }
      ]
    },
    {
      "issue_number": 1797,
      "title": "Query whether a model/provider supports a given file before making the request",
      "body": "### Description\n\nCurrently there is some logic, often in `_map_user_prompt`, for each provider (AFAICT at least) that determines whether the given `UserContent` is supported. \n\nFor example, for Anthropic:\n\n```py\n    elif isinstance(item, BinaryContent):\n        if item.is_image:\n            yield ImageBlockParam(\n                source={'data': io.BytesIO(item.data), 'media_type': item.media_type, 'type': 'base64'},  # type: ignore\n                type='image',\n            )\n        elif item.media_type == 'application/pdf':\n            yield DocumentBlockParam(\n                source=Base64PDFSourceParam(\n                    data=io.BytesIO(item.data),\n                    media_type='application/pdf',\n                    type='base64',\n                ),\n                type='document',\n            )\n        else:\n            raise RuntimeError('Only images and PDFs are supported for binary content')\n    elif isinstance(item, ImageUrl):\n        yield ImageBlockParam(source={'type': 'url', 'url': item.url}, type='image')\n    elif isinstance(item, DocumentUrl):\n        if item.media_type == 'application/pdf':\n            yield DocumentBlockParam(source={'url': item.url, 'type': 'url'}, type='document')\n        elif item.media_type == 'text/plain':\n            response = await cached_async_http_client().get(item.url)\n            response.raise_for_status()\n            yield DocumentBlockParam(\n                source=PlainTextSourceParam(data=response.text, media_type=item.media_type, type='text'),\n                type='document',\n            )\n        else:  # pragma: no cover\n            raise RuntimeError(f'Unsupported media type: {item.media_type}')\n    else:\n        raise RuntimeError(f'Unsupported content type: {type(item)}')  # pragma: no cover\n```\n\nIt would be nice to be able to run a version of that logic on a given `UserContent` *before* we attempt to actually call the endpoint, just to check if a document is supported (and not end up with an exception if it isn't).\n\nAlso, a question: does this logic currently take into account different models from the same provider supporting different document types? \n\n### References\n\n_No response_",
      "state": "open",
      "author": "proever",
      "author_type": "User",
      "created_at": "2025-05-21T13:52:30Z",
      "updated_at": "2025-05-21T17:17:48Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1797/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1797",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1797",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:59.116432",
      "comments": [
        {
          "author": "DouweM",
          "body": "> Also, a question: does this logic currently take into account different models from the same provider supporting different document types?\n\n@proever It does not, that is one of the cases considered in https://github.com/pydantic/pydantic-ai/issues/1782. That would also let you check the model's su",
          "created_at": "2025-05-21T16:33:22Z"
        },
        {
          "author": "proever",
          "body": "Thanks again! Something like `model.profile.supported_media_types` would be super helpful I think, yes.",
          "created_at": "2025-05-21T17:17:47Z"
        }
      ]
    },
    {
      "issue_number": 1770,
      "title": "more connection control for the LLM api",
      "body": "### Description\n\nfor the service which calls LLM apis for multiple clients, I think these are required\n\n## pre-warm https connections\n\nto reduce latency from server to API, I think it's better to establish https connection before the service.\nwhen servers or users are with multiple hops to the API, it will take more times to establish https connection.\nin most cases, we can add `HEAD` to https with multiple connections when server startup \n\n## explicit connection pool control\n\nfor each provider it has different value of keep-alive connection and concurrent connection, it's better to control by the developer explicit.\n\n## websocket support \n\ncan you support websocket realtime api for the openai?\n\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "darjeeling",
      "author_type": "User",
      "created_at": "2025-05-19T19:59:19Z",
      "updated_at": "2025-05-21T16:42:03Z",
      "closed_at": "2025-05-21T16:42:03Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1770/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1770",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1770",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:59.378439",
      "comments": [
        {
          "author": "DouweM",
          "body": "@darjeeling Since these are separate feature requests that'll need their own discussion and implementation, would you mind creating separate issues? I'd also love to see examples of how you'd like this to be used.",
          "created_at": "2025-05-21T15:27:51Z"
        },
        {
          "author": "darjeeling",
          "body": "@DouweM ok I'll make separate feature request and make some examples",
          "created_at": "2025-05-21T15:29:16Z"
        }
      ]
    },
    {
      "issue_number": 1776,
      "title": "Support for Gemini 2.5 code_execution tool",
      "body": "### Description\n\nAdd support for the new `code_execution` tool in Gemini 2.5. This would enable direct code execution within the model, unlocking powerful new capabilities and improving code-related tasks.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "LongTCH",
      "author_type": "User",
      "created_at": "2025-05-20T08:25:46Z",
      "updated_at": "2025-05-21T15:29:03Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1776/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1776",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1776",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:59.669470",
      "comments": []
    },
    {
      "issue_number": 1771,
      "title": "support batch processing",
      "body": "### Description\n\nto reduce pricing, I think it's good to support batch process.\n\nsome models support that. how's your priority?\n\n[anthropic batch process](https://docs.anthropic.com/en/docs/build-with-claude/batch-processing#message-batches-api)\n[OpenAI Batch API](https://platform.openai.com/docs/guides/batch)\n\n### References\n\n_No response_",
      "state": "open",
      "author": "darjeeling",
      "author_type": "User",
      "created_at": "2025-05-19T20:02:51Z",
      "updated_at": "2025-05-21T15:28:47Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1771/reactions",
        "total_count": 6,
        "+1": 6,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1771",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1771",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:59.669501",
      "comments": []
    },
    {
      "issue_number": 1687,
      "title": "GeminiModel Validation Fails when Gemini sends MALFORMED_FUNCTION_CALL response",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhile using the gemini-2.0-flash model with pydantic-ai, I encountered a validation error after sending a prompt. The model's response looked like this:\n\n```json\n{\n  \"candidates\": [\n    {\n      \"content\": {},\n      \"finishReason\": \"MALFORMED_FUNCTION_CALL\"\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 33168,\n    \"totalTokenCount\": 33168,\n    \"promptTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 33168\n      }\n    ]\n  },\n  \"modelVersion\": \"gemini-2.0-flash\"\n}\n```\n\nThis caused a Pydantic validation error, specifically:\n\n```\ncandidates.0.content.role\n  Field required [type=missing, input_value={}, input_type=dict]\n\ncandidates.0.content.parts\n  Field required [type=missing, input_value={}, input_type=dict]\n\ncandidates.0.finishReason\n  Input should be 'STOP', 'MAX_TOKENS' or 'SAFETY' [type=literal_error, input_value='MALFORMED_FUNCTION_CALL', input_type=str]\n```\n\nThe error occurs in pydantic_ai.models.gemini.GeminiModel.request() at line 154, when validating the Gemini response using _gemini_response_ta.validate_json(data).\n\nIt seems that either:\n\nThe Gemini API returned an unexpected finishReason, or\n\nThe response schema in pydantic-ai needs to be updated to include this new value.\n\nUntil the root issue is resolved, a workaround could be to catch the ValidationError and patch the response for fallback parsing. For example:\n\n```\ntry:\n    response = _gemini_response_ta.validate_json(data)\nexcept ValidationError as e:\n    raw = json.loads(data)\n    raw[\"candidates\"] = [{\n        \"finish_reason\": \"STOP\", \n        \"content\": {\n            \"parts\": [{\n                \"text\": \"Gemini sent malformed response. Use newer models to avoid issues.\",\n            }],\n            \"role\": \"model\"\n        }\n    }]\n    response = _gemini_response_ta.validate_json(json.dumps(raw))\n```\n\nLet me know if you'd like me to help test any fix. Thanks!\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.10.5\nPydantic-AI 0.1.11\nLLM Google GLA - Gemini-2.0-Flash\n```",
      "state": "closed",
      "author": "fswair",
      "author_type": "User",
      "created_at": "2025-05-11T14:57:03Z",
      "updated_at": "2025-05-21T15:26:04Z",
      "closed_at": "2025-05-12T11:45:57Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1687/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1687",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1687",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:26:59.669511",
      "comments": [
        {
          "author": "DouweM",
          "body": "This was previously reported and discussed in https://github.com/pydantic/pydantic-ai/issues/631. We'd love a PR to add this as a valid `finishReason` and ideally handle it internally by telling Gemini to try again. Do you have some code that reliably reproduces this issue? That'd make it a lot easi",
          "created_at": "2025-05-12T11:45:58Z"
        },
        {
          "author": "fswair",
          "body": "Hello, @DouweM . My code contains an exclusive method, can I send you the case privately?\n\nI ran same case after days and still causes same validation error. \n\nI can provide better information about this issue.",
          "created_at": "2025-05-21T11:57:38Z"
        },
        {
          "author": "DouweM",
          "body": "@fswair Yes, can you join the Slack channel (https://ai.pydantic.dev/help/) and send me a DM there?",
          "created_at": "2025-05-21T15:18:54Z"
        },
        {
          "author": "fswair",
          "body": "@DouweM i published new information at https://github.com/pydantic/pydantic-ai/pull/1688 can u check? \n\nok, I will join slack.",
          "created_at": "2025-05-21T15:26:03Z"
        }
      ]
    },
    {
      "issue_number": 1742,
      "title": "Support ACP protocol",
      "body": "### Description\n\nIBM also released a new protocol called ACP:\n\nhttps://agentcommunicationprotocol.dev/introduction/welcome\n\nWhat is ACP?\n\nThe Agent Communication Protocol (ACP) is an open standard with open governance for agent interoperability. It defines a standardized RESTful API supporting synchronous, asynchronous, and streaming interactions. In ACP, agents are services that exchange multimodal messages, with the protocol remaining agnostic to their internal implementations and requiring only minimal specifications for compatibility.\n\n\n### References\n\n_No response_",
      "state": "open",
      "author": "ararog",
      "author_type": "User",
      "created_at": "2025-05-16T00:10:25Z",
      "updated_at": "2025-05-21T15:25:08Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "a2a"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1742/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1742",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1742",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:00.016043",
      "comments": []
    },
    {
      "issue_number": 138,
      "title": "Prompt caching support",
      "body": "Is there prompt caching support? I couldn't find evidence in the docs.\r\n\r\nIf not, would love to have that baked in.",
      "state": "closed",
      "author": "voberoi",
      "author_type": "User",
      "created_at": "2024-12-04T14:20:36Z",
      "updated_at": "2025-05-21T14:56:06Z",
      "closed_at": "2025-05-21T14:56:06Z",
      "labels": [
        "Feature request",
        "caching"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 23,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/138/reactions",
        "total_count": 31,
        "+1": 31,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/138",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/138",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:00.016131",
      "comments": []
    },
    {
      "issue_number": 1755,
      "title": "UnexpectedModelBehavior",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [ ] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHi\n\nI used the roulette_agent  example from the docs.. However, sometimes the agent works and sometimes does not.\nI get the following Error:\n\n`Exception has occurred: UnexpectedModelBehavior\nExceeded maximum retries (1) for result validation\n  File \"D:\\Projects\\tryPydanticAI\\main.py\", line 22, in <module>\n    result = roulette_agent.run_sync('Put my money on square eighteen',deps=my_number)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (1) for result validation`\n\nFurther, can u point me to any additional parameters if any, for LLM to be deterministic or at least be restrictive to a particular set of guidelines.\n\nThanks\n\n### Example Code\n\n```Python\nimport os\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent,RunContext\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.models.openai import OpenAIModel\n\n\nollama_model = OpenAIModel(\n    model_name='llama3.2:1b', provider=OpenAIProvider(base_url='http://localhost:11434/v1')\n)\n\nroulette_agent  = Agent(model=ollama_model, deps_type=int,output_type=bool,system_prompt=('Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'),)\n\n@roulette_agent.tool\nasync def roulette_wheel(ctx:RunContext[int],square:int)->str:\n    \"\"\"Check if the square is a winner\"\"\"\n    return 'You won !!!' if square == ctx.deps else 'Lose :('\n\nif __name__ == '__main__':    \n    my_number = 18\n    result = roulette_agent.run_sync('Put my money on square eighteen',deps=my_number)\n    print(result.output)\n    result = roulette_agent.run_sync('I bet five is the winner',deps=my_number)\n    print(result.output)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython - 3.12\npydantic AI - 0.2.4\nLLM Model - llama3.2:1b\n```",
      "state": "closed",
      "author": "iincognitioo",
      "author_type": "User",
      "created_at": "2025-05-19T05:24:55Z",
      "updated_at": "2025-05-21T14:53:32Z",
      "closed_at": "2025-05-21T14:53:32Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1755/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1755",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1755",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:00.016142",
      "comments": [
        {
          "author": "iincognitioo",
          "body": "**When Worked:**\n`[UserPromptNode(user_prompt='Put my money on square eighteen', instructions=None, instructions_functions=[], system_prompts=('Use the `roulette_wheel` function to see if the customer has won based on the number they provide.',), system_prompt_functions=[], system_prompt_dynamic_fun",
          "created_at": "2025-05-19T05:42:42Z"
        },
        {
          "author": "DouweM",
          "body": "@iincognitioo By passing `output_type=bool` to `Agent`, you're causing PydanticAI to enforce that the model only responds with a boolean, but it seems like the model is not following this instruction. In the examples you've shared, you can see that PydanticAI is already telling the model this (`Plai",
          "created_at": "2025-05-19T22:21:18Z"
        },
        {
          "author": "iincognitioo",
          "body": "Thanks...Yes, definitely i can use other models. Not an issue.",
          "created_at": "2025-05-20T06:27:28Z"
        }
      ]
    },
    {
      "issue_number": 1598,
      "title": "Recursive `$ref`s in JSON Schema are not supported by Gemini",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm currently using Pydantic AI with a complex use-case with a bunch of output types and I've found that got the following error. I've managed to get it down to a MWE (linked below). \n\nThe only stuff I've found online about this are the following, which mention that Gemini doesn't support recursive definitions. \n\n- https://github.com/googleapis/python-genai/issues/319\n- https://discuss.ai.google.dev/t/recursive-schema-definition/68607\n\nSince the docs mention that Gemini is using a custom interface with Pydantic and HTTPX, I think this is the right place to report this? Without this, I think it makes doing more complex output structures impossible?\n\nPlease let me know if I can provide any more information about this! \n\n<details>\n<summary>Traceback from the MWE</summary>\n\n```python\nTraceback (most recent call last):\n  File \"/Users/amit/Develop/gptnt/run.py\", line 26, in <module>\n    result = agent.run_sync(\"Make a tree node with two children named 'A' and 'B'\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 766, in run_sync\n    return get_event_loop().run_until_complete(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 686, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 436, in run\n    async for _ in agent_run:\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1745, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 800, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 773, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 271, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 324, in _make_request\n    model_request_parameters = ctx.deps.model.customize_request_parameters(model_request_parameters)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/gemini.py\", line 164, in customize_request_parameters\n    output_tools=[_customize_tool_def(tool) for tool in model_request_parameters.output_tools],\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/gemini.py\", line 159, in _customize_tool_def\n    return replace(t, parameters_json_schema=_GeminiJsonSchema(t.parameters_json_schema).walk())\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 41, in walk\n    handled = self._handle(schema)\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 80, in _handle\n    schema = self._handle_object(schema)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 96, in _handle_object\n    handled_properties[key] = self._handle(value)\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 84, in _handle\n    schema = self._handle_union(schema, 'anyOf')\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 127, in _handle_union\n    handled = [self._handle(member) for member in members]\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 82, in _handle\n    schema = self._handle_array(schema)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 118, in _handle_array\n    schema['items'] = self._handle(items)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/_json_schema.py\", line 88, in _handle\n    schema = self.transform(schema)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/amit/Develop/gptnt/.venv/lib/python3.12/site-packages/pydantic_ai/models/gemini.py\", line 825, in transform\n    raise UserError(f'Recursive `$ref`s in JSON Schema are not supported by Gemini: {schema[\"$ref\"]}')\npydantic_ai.exceptions.UserError: Recursive `$ref`s in JSON Schema are not supported by Gemini: #/$defs/Recursive\n```\n\n</details>\n\n### Example Code\n\n```Python\nfrom pathlib import Path\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\n\n\nclass Recursive(BaseModel):\n    value: str\n    children: list[\"Recursive\"] | None = None\n\n\nif __name__ == \"__main__\":\n    agent = Agent(\n        model=GeminiModel(\n            \"gemini-2.0-flash\",\n            provider=GoogleVertexProvider(service_account_file=Path(\"storage/keys/gemini.json\")),\n        ),\n        output_type=Recursive,\n    )\n    result = agent.run_sync(\"Dance\")\n    print(result)  # noqa: T201\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.8\npydantic v2.11.3\npydantic-ai v0.1.6\n```",
      "state": "closed",
      "author": "amitkparekh",
      "author_type": "User",
      "created_at": "2025-04-25T19:07:35Z",
      "updated_at": "2025-05-21T13:51:58Z",
      "closed_at": "2025-05-21T13:51:56Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1598/reactions",
        "total_count": 8,
        "+1": 8,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1598",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1598",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:00.311115",
      "comments": [
        {
          "author": "jeffss00",
          "body": "I'm having the same issue on 0.1.3. This was _not_ an issue with a prior version (can't recall which I updated from unfortunately). Happening with 2.0 and 2.5 flash on genai api.  ",
          "created_at": "2025-04-28T01:47:05Z"
        },
        {
          "author": "dmontagu",
          "body": "If you include `$ref` or `$defs` in the schema, you get the following error, coming from the API response from Gemini:\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.5-flash-preview-04-17, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload ",
          "created_at": "2025-04-28T14:03:11Z"
        },
        {
          "author": "dextercorley19",
          "body": "I am able to use a recursive response schema with the native gemini api, but when using the same schema with the pydantic ai run agent am getting the following error: \n\n`pydantic_ai.exceptions.UserError: Recursive `$ref`s in JSON Schema are not supported by Gemini: #/$defs/BoundingBox`  \n\nThe workin",
          "created_at": "2025-04-30T20:37:23Z"
        },
        {
          "author": "brgsk",
          "body": "Hi, any updates on this? Someone got it working?",
          "created_at": "2025-05-13T13:59:09Z"
        },
        {
          "author": "dmontagu",
          "body": "@dextercorley19 @brgsk I just tried again making a request to Gemini with a recursive schema and got an error again. In particular, here's the schema I used (from the schema above):\n```\n{'$defs': {'Recursive': {'properties': {'value': {'type': 'string'}, 'children': {'items': {'$ref': '#/$defs/Recur",
          "created_at": "2025-05-15T13:27:08Z"
        }
      ]
    },
    {
      "issue_number": 1763,
      "title": "Anthropic streaming with tool calls doesn't give model a final turn",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nrunning the below code gives the following outputs\n\nfor non-streaming:\n```\nThe current user is John, age 30.\\nThe current time is May 19, 2025, 6:10:50 PM\n```\n\nand for streaming:\n\n```\nI can help you find the current user name and time. Let me retrieve that information for you.\n```\n\n\n\nif I change the model to `openai:gpt-4o` instead of `anthropic:claude-3-7-sonnet-latest` I get the expected results:\n\nfor non-streaming:\n```\nThe user's name is John, and the current time is 2025-05-19T18:13:04.\n```\n\nand for streaming:\n```\nThe user's name is John, and the current time is 18:13 on May 19, 2025.\n```\n\nTo me this feels like a bug, since the non-streaming call to the same model produces the \"correct\" (IMO) behavior, as do streaming calls to other providers (I tested OpenAI and Gemini). Please let me know if I can help at all!\n\n### Example Code\n\n```Python\nfrom datetime import datetime\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, DocumentUrl, ImageUrl\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nagent = Agent(model=\"anthropic:claude-3-7-sonnet-latest\")\n\n\n@agent.tool_plain\ndef get_current_time() -> datetime:\n    return datetime.now()\n\n\n@agent.tool_plain\ndef get_user() -> User:\n    return User(name=\"John\", age=30)\n\n\n@agent.tool_plain\ndef get_company_logo() -> ImageUrl:\n    return ImageUrl(url=\"https://iili.io/3Hs4FMg.png\")\n\n\n@agent.tool_plain\ndef get_document() -> DocumentUrl:\n    return DocumentUrl(\n        url=\"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n    )\n\n\n\nres = await agent.run(\"What is the user name? And what is the current time?\")\nprint(res)\nprint(res.all_messages())\n\nprint(\"--------------------------------\")\nstream =  agent.run_stream(\"What is the user name? And what is the current time?\")\nasync with stream as result:\n    async for text in result.stream_text():\n        print(text)\n\n    print(result.all_messages())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython v3.12.8\n\n\nanthropic                          0.51.0\npydantic                           2.10.6\npydantic-ai                        0.2.4\npydantic-ai-slim                   0.2.4\npydantic-core                      2.27.2\npydantic-evals                     0.2.4\npydantic-extra-types               2.10.3\npydantic-graph                     0.2.4\npydantic-settings                  2.8.1\n```",
      "state": "closed",
      "author": "proever",
      "author_type": "User",
      "created_at": "2025-05-19T17:21:00Z",
      "updated_at": "2025-05-21T13:35:49Z",
      "closed_at": "2025-05-21T13:35:49Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1763/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1763",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1763",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:00.549273",
      "comments": [
        {
          "author": "proever",
          "body": "some debugging output\n\n```\nANTHROPIC_MODEL_DEBUG: request_stream called. Messages: [ModelRequest(parts=[UserPromptPart(content='What is the user name? And what is the current time?', timestamp=datetime.datetime(2025, 5, 19, 18, 3, 51, 880068, tzinfo=datetime.timezone.utc), part_kind='user-prompt')],",
          "created_at": "2025-05-19T18:14:59Z"
        },
        {
          "author": "DouweM",
          "body": "@proever That's indeed a downside of using `run_stream`, as it'll end as soon as it gets the first valid final response, and since we haven't specified a specific `output_type`, the first text output is accepted.\n\nTo work around this, you have a few options:\n1. Try to explicitly prompt Clause not to",
          "created_at": "2025-05-19T22:45:23Z"
        },
        {
          "author": "proever",
          "body": "Thanks for your quick response! I was indeed able to fix it using the graph iteration method you recommended :)",
          "created_at": "2025-05-21T13:35:49Z"
        }
      ]
    },
    {
      "issue_number": 1464,
      "title": "Decimal Type Issue: Issue in generating a valid JSON schema for certain Pydantic types when preparing the request for Gemini",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nRunning the below code which uses Geminin API with a little complex structured output parsing results in error::\n\n2025-04-12 23:57:46,085 - INFO - Running THREE COMPLEX FIELDS test using ONLY pydantic-ai Agent...\n2025-04-12 23:57:46,147 - INFO - pydantic-ai Agent initialized with model 'gemini-1.5-flash-latest' and 3 complex fields included\n2025-04-12 23:57:46,147 - INFO - Calling agent.run_sync...\n2025-04-12 23:57:47,672 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent \"HTTP/1.1 400 Bad Request\"\n2025-04-12 23:57:47,674 - ERROR - Unexpected error during Agent execution: ModelHTTPError: status_code: 400, model_name: gemini-1.5-flash-latest, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unable to submit request because `final_result` functionDeclaration `parameters.payment_milestones.amount` schema didn't specify the schema type field. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n\n2025-04-12 23:57:47,694 - ERROR - Traceback (most recent call last):\n/contract_structured_output/verysimple.py\", line 126, in <module>\n    result = agent.run_sync(sample_contract_text)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 578, in run_sync\n    return get_event_loop().run_until_complete(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\no/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 329, in run\n    async for _ in agent_run:\n/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 1454, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 790, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 763, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 264, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 317, in _make_request\n    model_response, request_usage = await ctx.deps.model.request(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_ai/models/gemini.py\", line 135, in request\n    async with self._make_request(\n/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n/lib/python3.11/site-packages/pydantic_ai/models/gemini.py\", line 242, in _make_request\n    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=r.text)\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-1.5-flash-latest, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unable to submit request because `final_result` functionDeclaration `parameters.payment_milestones.amount` schema didn't specify the schema type field. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n\n\n\n--- Extraction Failed (Unexpected Error) ---\n\n### Example Code\n\n```Python\n# test_pydantic_ai_agent_only_three_complex.py\n\nimport os\nfrom pydantic_ai import Agent\nfrom pydantic import (\n    BaseModel, Field, ValidationError, field_validator\n)\nimport decimal\nfrom typing import List, Optional\nfrom datetime import date\nimport logging\n\n# --- Configuration ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nif not GOOGLE_API_KEY:\n    logging.warning(\"GOOGLE_API_KEY environment variable not found...\")\n\n\nclass AddressDetail(BaseModel):\n    street: Optional[str] = None; city: Optional[str] = None; state_province: Optional[str] = None; postal_code: Optional[str] = None; country: Optional[str] = None\n    def __repr__(self) -> str: parts = [f\"{k}={v!r}\" for k, v in self.model_dump().items() if v is not None]; return f\"<AddressDetail({', '.join(parts)})>\"\n\nclass PartyDetail(BaseModel):\n    name: str = Field(...); role: Optional[str] = Field(None); address: Optional[AddressDetail] = Field(None); authorized_signatory_name: Optional[str] = Field(None)\n    def __repr__(self) -> str: addr_repr = repr(self.address) if self.address else 'None'; return f\"<PartyDetail(name={self.name!r}, role={self.role!r}, address={addr_repr}, signatory={self.authorized_signatory_name!r})>\"\n\n\nclass SoftwareProductDetail(BaseModel):\n    name: str = Field(..., description=\"Primary name of the software or product\")\n    version: Optional[str] = Field(None, description=\"Specific version identifier, if mentioned\")\n    modules_features: Optional[List[str]] = Field(default_factory=list, description=\"List of included modules or key features\")\n    description: Optional[str] = Field(None, description=\"Brief description of the software/product\")\n    def __repr__(self) -> str:\n        num_features = len(self.modules_features) if self.modules_features else 0\n        return f\"<SoftwareProductDetail(name={self.name!r}, version={self.version!r}, num_features={num_features})>\"\n\nclass PaymentMilestoneDetail(BaseModel):\n    description: str = Field(..., description=\"What triggers or describes this payment\")\n    amount: Optional[decimal.Decimal] = Field(None, description=\"Monetary amount\") # Using standard Decimal\n    currency: Optional[str] = Field(None, description=\"Currency code (e.g., USD, EUR, INR)\")\n    due_date_description: Optional[str] = Field(None, description=\"When the payment is due (e.g., 'Net 30', 'Upon signing', specific date)\")\n    due_date: Optional[date] = Field(None, description=\"Specific calendar due date, if available\")\n    def __repr__(self) -> str: # Keep simple repr\n        amt_repr = repr(self.amount) if self.amount is not None else 'None'\n        curr_repr = self.currency or 'N/A'\n        return f\"<PaymentMilestoneDetail(desc={self.description!r}, amount={amt_repr} {curr_repr}, due={self.due_date_description or self.due_date or 'N/A'!r})>\"\n\n# --- PARTIALLY Simplified Pydantic Model ---\nclass ContractAnalysisResult(BaseModel):\n    \"\"\"Structure with parties, products, and payments added back.\"\"\"\n\n    # Keep basic fields\n    contract_title: Optional[str] = Field(None, description=\"The main title of the contract document.\")\n    effective_date: Optional[str] = Field(None, description=\"The effective date (YYYY-MM-DD string).\")\n    contract_id_reference: Optional[str] = Field(None, description=\"Any unique identifier or reference number.\")\n\n    parties: List[PartyDetail] = Field(default_factory=list, description=\"List of all identified parties involved in the contract.\")\n\n    primary_software_products: List[SoftwareProductDetail] = Field(default_factory=list, description=\"Details of the main software/products being licensed or sold.\")\n    payment_milestones: List[PaymentMilestoneDetail] = Field(default_factory=list, description=\"Breakdown of payment amounts, schedules, and currencies.\")\n\n    force_majeure_clause_present: Optional[bool] = Field(None, description=\"Is there a Force Majeure clause?\")\n    governing_law: Optional[str] = Field(None, description=\"The governing law jurisdiction.\")\n\n    # Add back validator IF it only refers to existing fields\n    @field_validator('parties')\n    def check_at_least_two_parties(cls, v):\n        # Keep print for warning, avoid raising error during debug\n        if isinstance(v, list) and len(v) < 2:\n            print(f\"Warning: Fewer than two parties extracted ({len(v)} found).\")\n        return v\n\n# --- End Model Definition ---\n\n\n# --- Sample Contract Text (Remains the same) ---\nsample_contract_text = \"\"\"\nMASTER SOFTWARE LICENSE AND SERVICES AGREEMENT - Ref: MSSA-2024-TS-SL\nThis Master Software License and Services Agreement (\"Agreement\") is effective as of July 15, 2024 (\"Effective Date\"),\nby and between Quantum Dynamics Inc., a Delaware corporation with offices at 1 Quantum Leap, Palo Alto, CA 94301 (\"Quantum\"), and\nGlobal Synergy Partners LLC, a New York limited liability company with its principal place of business at 789 World Ave, New York, NY 10001 (\"Synergy\").\nRECITALS\nA. Quantum develops and licenses proprietary software known as \"FusionPlatform\" version 3.0, including modules \"DataCore\" and \"AnalyticsSuite\".\nB. Synergy wishes to license FusionPlatform and receive related support services.\nAGREEMENT\nLICENSE GRANT. Quantum grants Synergy a non-exclusive, worldwide, 3-year subscription license to use FusionPlatform v3.0 (DataCore & AnalyticsSuite modules) for up to 250 named users solely for Synergy's internal business operations. Sub-licensing is prohibited.\nSERVICES. Quantum will provide Standard Support (8x5, Pacific Time) and basic implementation services as outlined in Exhibit A.\nFEES. Synergy shall pay Quantum a total Subscription Fee of $150,000 USD annually, due Net 30 days from the start of each subscription year. Implementation Services fee is a one-time charge of $25,000 USD due upon signing. Late payments accrue 1.5% monthly interest.\nTERM AND TERMINATION. This Agreement commences on the Effective Date and continues for three (3) years. It renews automatically for successive 1-year terms unless either party provides written notice of non-renewal at least 90 days prior to the end of the then-current term. Either party may terminate for material breach upon 30 days' written notice if the breach remains uncured.\nCONFIDENTIALITY. Obligations last for 5 years post-termination.\nGOVERNING LAW & JURISDICTION. This Agreement is governed by the laws of the State of California, without regard to conflict of law principles. Disputes shall be resolved exclusively in the state or federal courts located in Santa Clara County, California.\nACCEPTANCE. Use of the software constitutes acceptance. Delivery shall occur within 5 business days of Effective Date.\nIN WITNESS WHEREOF... [Signatures]\nExhibit A: Services Description (Attached)\nAmendment No. 1: Pricing Adjustment (Dated Aug 1, 2024 - Attached)\n\"\"\"\n# --- Main Execution using pydantic-ai Agent (Remains the same) ---\nif __name__ == \"__main__\":\n    logging.info(\"Running THREE COMPLEX FIELDS test using ONLY pydantic-ai Agent...\")\n\n    GEMINI_MODEL = \"gemini-1.5-flash-latest\"\n    agent = None\n    try:\n        agent = Agent(\n            model=GEMINI_MODEL,\n            result_type=ContractAnalysisResult # Use the schema with 3 complex fields\n            # api_key=GOOGLE_API_KEY # Add if necessary\n        )\n        logging.info(f\"pydantic-ai Agent initialized with model '{GEMINI_MODEL}' and 3 complex fields included\")\n\n        logging.info(\"Calling agent.run_sync...\")\n        result = agent.run_sync(sample_contract_text)\n        logging.info(\"agent.run_sync finished.\")\n\n        # ... (Result processing remains the same) ...\n        if result and hasattr(result, 'data') and result.data:\n            if isinstance(result.data, ContractAnalysisResult):\n                print(\"\\n--- Successfully Extracted Three Complex Fields Info (using pydantic-ai Agent) ---\")\n                print(result.data.model_dump_json(indent=2))\n                print(\"--- End ---\")\n            # ...(rest of the result/error handling) ...\n            else:\n                 logging.error(f\"Agent returned data, but it's not the expected ContractAnalysisResult type. Type: {type(result.data)}\")\n                 print(\"\\n--- Extraction returned unexpected data type ---\")\n                 print(f\"Returned data:\\n{result.data}\")\n        elif result and hasattr(result, 'error') and result.error:\n             logging.error(f\"Agent run failed with error: {result.error}\")\n             print(\"\\n--- Extraction Failed (Agent reported error) ---\")\n             print(f\"Error: {result.error}\")\n        else:\n            logging.error(\"Agent run did not return expected data or error structure.\")\n            print(\"\\n--- Extraction Failed (Unexpected Agent result) ---\")\n            print(f\"Raw agent result: {result}\")\n\n    # ... (Exception handling remains the same, including specific RecursionError catch) ...\n    except RecursionError as e: # Catch RecursionError specifically\n        logging.error(f\"!!! RecursionError occurred with 3 complex fields added back: {e}\")\n        import traceback\n        logging.error(traceback.format_exc())\n        print(\"\\n--- Extraction Failed (Recursion Error with 3 Complex Fields) ---\")\n    except Exception as e:\n        logging.error(f\"Unexpected error during Agent execution: {type(e).__name__}: {e}\")\n        import traceback\n        logging.error(traceback.format_exc())\n        print(\"\\n--- Extraction Failed (Unexpected Error) ---\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.9\ngoogle-generativeai                      0.8.4\njsonref                                  1.1.0\npydantic                                 2.11.3\npydantic-ai                              0.0.55\npydantic-ai-slim                         0.0.55\npydantic_core                            2.33.1\npydantic-evals                           0.0.55\npydantic-graph                           0.0.55\npydantic-settings                        2.6.1\n```",
      "state": "closed",
      "author": "RahulPant1",
      "author_type": "User",
      "created_at": "2025-04-12T18:41:38Z",
      "updated_at": "2025-05-21T08:16:15Z",
      "closed_at": "2025-05-21T08:16:15Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1464/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1464",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1464",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:00.827676",
      "comments": [
        {
          "author": "RahulPant1",
          "body": "Debugging further it seems the issue is related to : Gemini function-calling issue related to the lack of JSON Schema type inference for decimal.Decimal.\nLine: amount: Optional[decimal.Decimal] = Field(None, description=\"Monetary amount\") # Using standard Decimal\n\nChanging this line to 'float' avoid",
          "created_at": "2025-04-13T02:58:43Z"
        },
        {
          "author": "DouweM",
          "body": "@RahulPant1 All right, here's a minimal failing example that fails with the same `schema didn't specify the schema type field` error:\n\n```py\nfrom typing import Optional\nfrom pydantic_ai import Agent\nfrom pydantic import BaseModel\nfrom decimal import Decimal\n\nclass PaymentMilestoneDetail(BaseModel):\n",
          "created_at": "2025-04-17T19:22:09Z"
        },
        {
          "author": "Kludex",
          "body": "We fixed the JSON schema that we sent to Gemini. 🙏 ",
          "created_at": "2025-05-21T08:16:15Z"
        }
      ]
    },
    {
      "issue_number": 1444,
      "title": "Gemini models don't support result_types with `Literal` values",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen the result class of a Gemini agent has a `result_value` that contains a field typed as a Python `Literal` -- calls to the Google api fails. \n\nThe translation from the literal -> a valid Google compatible value seems to be incorrect.\n\nWhen making the api call, this is the error you get back.\n```\n raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=r.text)\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.0-flash, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"const\\\" at 'tools.function_declarations[0].parameters.properties[0].value': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools.function_declarations[0].parameters.properties[0].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"const\\\" at 'tools.function_declarations[0].parameters.properties[0].value': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai.providers.google_gla import GoogleGLAProvider\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai import Agent as AiAgent\n\nfrom typing_extensions import Literal\nfrom pydantic import BaseModel\n\nclass Response(BaseModel):\n    response: Literal[\"research\"]\n\ngemini_provider = GoogleGLAProvider(api_key=settings.GEMINI_API_KEY)\nmodel = GeminiModel(\"gemini-2.0-flash\", provider=gemini_provider)\n\n\nagent1 = AiAgent[str, Response](\n    name=\"test_agent\",\n    model=model,\n    deps_type=str,\n    system_prompt=\"You are a helpful agent, return a.\",\n    result_type=Response\n)\n\nasync def try_chat_agent() -> Response:\n    result = await agent1.run(\"research\", deps=\"deps\")\n    print(result)\n    return result.data\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.13 \npydantic-slim 0.0.55\nllm: gemini 2.0 flash\n```",
      "state": "closed",
      "author": "Soloniss",
      "author_type": "User",
      "created_at": "2025-04-10T21:14:23Z",
      "updated_at": "2025-05-21T08:14:44Z",
      "closed_at": "2025-05-21T08:14:42Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1444/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1444",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1444",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:01.050714",
      "comments": [
        {
          "author": "railscard",
          "body": "I can confirm, have the same issue with google gemini models and Python `Literal`",
          "created_at": "2025-04-12T20:57:57Z"
        },
        {
          "author": "Kludex",
          "body": "We now adapt the JSON schema to different providers.",
          "created_at": "2025-05-21T08:14:42Z"
        }
      ]
    },
    {
      "issue_number": 129,
      "title": "How can one cache the tool calls?",
      "body": null,
      "state": "closed",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2024-12-03T12:27:48Z",
      "updated_at": "2025-05-21T08:10:17Z",
      "closed_at": "2025-05-21T08:10:17Z",
      "labels": [
        "Feature request",
        "caching"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/129/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/129",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/129",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:01.296794",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "do you mean the results of a call? This should be possible with raw python, but I need to understand more about what you mean?",
          "created_at": "2024-12-03T18:18:14Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "Maybe if we want an agent to repeat the exact same tool calls given a similar user message? So after some runs, we want to lock-in an agent for a specific task and run the same tools in the same sequence every time.\r\nDon't know if that's what Pedro meant but I'm curious about this feature nonetheles",
          "created_at": "2024-12-04T02:39:01Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Seems a reasonable thing to do, @pedroallenrevez is that what you want?\r\n\r\nIt's worth noting you could do this already with your own logic, potentially using [`functools.cache`](https://docs.python.org/3/library/functools.html#functools.cache).",
          "created_at": "2024-12-04T10:56:23Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "@samuelcolvin sorry for my lack of description in my recent issues.\r\nYeah what I really mean is, not the tool call itself, I know I can control the logic there.\r\n\r\nBut to pydantic-ai's internals, there will be a first call to an LLM and that first call will \"calculate\" the arguments for any of the c",
          "created_at": "2024-12-08T18:38:13Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "Wouldn't that also require an LLM call?\r\nSo if we have:\r\n`get_weather(day:str, location:str)`\r\n1. The LLM call looks at the messages so far and gives us these args: day=\"Tuesday\", location=\"London\"\r\n2. We cache these args for this tool.\r\n3. After a couple of messages we need to call `get_weather` ag",
          "created_at": "2024-12-08T20:33:12Z"
        }
      ]
    },
    {
      "issue_number": 839,
      "title": "Dynamic system prompt is not updated for each `agent_model.run`",
      "body": "Hi there,\n\nFollow up on my slack question.\n\nThe `dynamic system prompt` is very sweet to provide up-to-date context to the LLM. However inside a single run many steps can occur, with the ability to mutate the state retained in `ctx.deps` (through tool calls).\n\nAt the moment `dynamic system prompt` is `updated` at the very begining of each run and then is `static` for the whole `run`.\n\n**I would like to request an option to have more fine-grained update for** `dynamic system prompt` so it can `refresh` for each `agent_model.request()`.\nMaybe this can be done by mutating the said prompt right inside a `tool-call` through `ctx-messages` ... but that would be also ineficient as several tool calls might be \"batched\" in the same step (in case of `parallel_tool_calls` enabled).\n\nSee bellow a `mwe` where it would be beneficial to provide the LLM with the up-to-date `model_overview` not only at run start (through a `dynamic system prompt`) but also in-between `tool-calls` :\n\n```python\nimport asyncio\nimport time\nfrom typing import Annotated, List, Optional\n\nimport logfire\nfrom pydantic import BaseModel, BeforeValidator, Field\nfrom pydantic_ai import (\n    Agent,\n    RunContext,\n    capture_run_messages,\n)\nfrom rich.table import Table\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.rule import Rule\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_ai.messages import (\n    ModelMessagesTypeAdapter,\n    ModelMessage,\n    ModelRequestPart,\n    ModelResponsePart,\n    RetryPromptPart,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n)\nfrom pydantic_ai.result import RunResult\n\nPARALLEL_TOOL_CALLS = False\n\n# Define a validator function\ndef to_lowercase(value: str) -> str:\n    return value.lower() if isinstance(value, str) else value\n\n\n# Use BeforeValidator to enforce lowercase\nLowerStr = Annotated[str, BeforeValidator(to_lowercase)]\n\n\nclass Room(BaseModel):\n    name: LowerStr = Field(description=\"The name of the room.\")\n    area: Optional[float] = Field(\n        default=None, description=\"The area of the room in square meters (m2).\"\n    )\n\n\nclass House(BaseModel):\n    rooms: List[Room] = Field(default_factory=list)\n\n\ndeps = House(rooms=[])\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    deps_type=House,\n    system_prompt=(\"You are a helpful assistant that can add rooms to a house model.\"),\n    model_settings=ModelSettings(\n        parallel_tool_calls=PARALLEL_TOOL_CALLS,\n    ),\n)\n\n\ndef model_overview(model: House) -> str:\n    return (\n        f\"The current house has {len(model.rooms)} rooms :\\n\\n\"\n        f\"{'\\n'.join([f'* {room.name}{f\" ({room.area}m2)\" if room.area is not None else \"(_m2)\"}' for room in model.rooms])}.\"\n    )\n\n\n@agent.system_prompt(dynamic=True)\ndef dynamic_system_prompt(ctx: RunContext[House]) -> str:\n    return model_overview(ctx.deps)\n\n\n@agent.tool\ndef add_house(ctx: RunContext[House], room: Room) -> Optional[Room]:\n    \"\"\"Add a room to the house on user request.\n\n    Args:\n        room (Room): The room to add to the house.\n\n    Returns:\n        Room: The room added to the house on success, None otherwise.\n    \"\"\"\n    try:\n        ctx.deps.rooms.append(room)\n        return room\n    except Exception as e:\n        logfire.error(f\"Error adding room to house: {e}\")\n        return None\n\n\n@agent.tool\ndef update_house(ctx: RunContext[House], room_name: str, area: float) -> str:\n    \"\"\"Update the area of a room in the house on user request.\n\n    Args:\n        room_name (str): The name of the room to update.\n        area (float): The area of the room to update.\n\n    Returns:\n        Room: The room updated to the house on success, None otherwise.\n    \"\"\"\n    rooms = list(filter(lambda room: room.name == room_name.lower(), ctx.deps.rooms))\n    if len(rooms) == 0:\n        raise ValueError(f\"No room found with name : `{room_name}`\")\n    elif len(rooms) > 1:\n        raise ValueError(f\"Multiple rooms found with same name : `{room_name}`\")\n    room_to_update = rooms[0]\n    old_area = room_to_update.area\n    room_to_update.area = area\n    return f\"Room `{room_name}` updated : `{round(old_area, 2) if old_area is not None else '_'}m2` -> `{round(area, 2)}m2`.\"\n\n\ndeps = House()\nconsole = Console(record=True, width=100, color_system=\"256\")\n\n\ndef print_run_messages(\n    messages: List[ModelMessage], result: RunResult, run_number: int\n):\n    i = run_number\n    console.print(\n        Rule(title=f\"Run ({i+1})\", style=\"yellow\", characters=\"=\", end=\"\\n\\n\")\n    )\n    for j, m in enumerate(messages):\n        console.print(\n            Rule(\n                title=f\"{m.kind.capitalize()} ({i+1}.{j+1})\",\n                style=\"yellow\",\n                characters=\"-\",\n                end=\"\\n\\n\",\n            )\n        )\n        for p in m.parts:\n            if isinstance(p, SystemPromptPart):\n                content = p.content\n                style = \"cyan\"\n            elif isinstance(p, UserPromptPart):\n                content = p.content\n                style = \"bright_cyan\"\n            elif isinstance(p, ToolCallPart):\n                content = (\n                    f\"id: {p.tool_call_id}\\n\"\n                    f\"name: {p.tool_name}\\n\"\n                    f\"args:\\n\"\n                    f\"{p.args_as_json_str()}\"\n                )\n                style = \"cyan\"\n            elif isinstance(p, RetryPromptPart):\n                content = str(p.content)\n                style = \"cyan\"\n            elif isinstance(p, ToolReturnPart):\n                content = (\n                    f\"id: {p.tool_call_id}\\n\"\n                    f\"name: {p.tool_name}\\n\"\n                    f\"content:\\n\"\n                    f\"{p.content}\"\n                )\n                style = \"magenta\"\n            elif isinstance(p, TextPart):\n                content = p.content\n                style = \"magenta\"\n            else:\n                continue\n\n            console.print(Panel(str(content), title=f\"{p.part_kind}\", style=style))\n        console.print(\"\")\n    console.print(\"\")\n\n\ndef print_usage(results:List[RunResult], timings:List[float]):\n    table = Table(title=agent.model.name(), style=\"bright_yellow\", expand=True)\n    table.add_column(\"# Run\")\n    table.add_column(\"# Steps\")\n    table.add_column(\"Input Tokens\")\n    table.add_column(\"Output Tokens\")\n    table.add_column(\"Total Tokens\")\n    table.add_column(\"Ellapsed Time (s)\")\n    usage_totals = {\n        \"steps_count\": 0,\n        \"input_tokens\": 0,\n        \"output_tokens\": 0,\n        \"total_tokens\": 0,\n        \"ellapsed_time\": 0.0,\n    }\n    \n    for i, r in enumerate(results):\n        usage = r.usage()\n        table.add_row(\n            str(i+1),\n            str(len(r.new_messages())),\n            str(usage.request_tokens),\n            str(usage.response_tokens),\n            str(usage.total_tokens),\n            f\"{timings[i]:.2f}\",\n        )\n        usage_totals[\"steps_count\"] += len(r.new_messages())\n        usage_totals[\"input_tokens\"] += usage.request_tokens\n        usage_totals[\"output_tokens\"] += usage.response_tokens\n        usage_totals[\"total_tokens\"] += usage.total_tokens\n        usage_totals[\"ellapsed_time\"] += timings[i]\n    \n    table.add_row(\n        f\"[bold]Total[/bold]\",\n        f\"[bold]{usage_totals['steps_count']}[/bold]\",\n        f\"[bold]{usage_totals['input_tokens']}[/bold]\",\n        f\"[bold]{usage_totals['output_tokens']}[/bold]\",\n        f\"[bold]{usage_totals['total_tokens']}[/bold]\",\n        f\"[bold]{usage_totals['ellapsed_time']:.2f}[/bold]\",\n        style=\"bright_yellow\",\n    )\n\n    console.print(Panel(table, style=\"yellow\", title=\"Usage\"))\n\n\nasync def main():\n    user_prompts = [\n        \"In my house, there is 4 rooms : a kitchen, a living room and a bathroom of 6m2.\",\n        \"The kitchen is 12m2. There is also 2 bedrooms, one of 10m2 and the other of 12m2. Oh no the kitchen is 21m2 !\",\n    ]\n    results:List[RunResult] = []\n    timings:List[float] = []\n    for i, user_prompt in enumerate(user_prompts):\n        start_time = time.perf_counter()\n        with capture_run_messages() as messages:\n            result = await agent.run(\n                user_prompt=user_prompt,\n                deps=deps,\n            )\n            results.append(result)\n            print_run_messages(messages, result, i)\n        end_time = time.perf_counter()\n        timings.append(end_time - start_time)\n    \n    console.print(Rule(title=\"Report\", style=\"yellow\", characters=\"*\", end=\"\\n\\n\"))\n    console.print(Panel(model_overview(deps), title=\"system-prompt\", style=\"red\"))\n    print_usage(results, timings)\n    console.save_svg(f\"chat_parallel_{str(PARALLEL_TOOL_CALLS).lower()}.svg\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n![Image](https://github.com/user-attachments/assets/c0a475a6-6775-4662-972f-fa6b45c5fba5)\n![Image](https://github.com/user-attachments/assets/bdaa5d7a-4e07-4ff5-863c-a982b295dbb9)\n\n",
      "state": "open",
      "author": "lionpeloux",
      "author_type": "User",
      "created_at": "2025-02-01T14:03:48Z",
      "updated_at": "2025-05-21T08:09:40Z",
      "closed_at": null,
      "labels": [
        "Stale",
        "system prompts"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 20,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/839/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/839",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/839",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:01.518863",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "I am not sure there is a way to do that with the current version. \n\nYou might want to leverage pydantic-graph and then instantiate the agent each time with the current system prompt that you need. Just a thought.",
          "created_at": "2025-02-01T17:27:50Z"
        },
        {
          "author": "reachcreator",
          "body": "`history = [\n        ModelRequest(parts=[SystemPromptPart(content=await get_system_instructions(thread_id))])\n    ]`\n\nI've been doing this with dynamic system prompts - passing to the run the chat history but first loading the dynamic system prompt, then adding the rest of the chat history to the hi",
          "created_at": "2025-02-02T23:10:26Z"
        },
        {
          "author": "lionpeloux",
          "body": "@reachcreator I am not quite sure to understand your proposal.\nAre you updating the `SystemPromptPart` at each `step` inside a `run` ?\n\nWould you mind adding a more complete example ?",
          "created_at": "2025-02-03T12:36:22Z"
        },
        {
          "author": "reachcreator",
          "body": "> [@reachcreator](https://github.com/reachcreator) I am not quite sure to understand your proposal. Are you updating the `SystemPromptPart` at each `step` inside a `run` ?\n> \n> Would you mind adding a more complete example ?\n\nSure thing. Example here is from a chatbot so each 'run' is just a new use",
          "created_at": "2025-02-03T16:25:09Z"
        },
        {
          "author": "lionpeloux",
          "body": "Ok thanks but this is not what I am trying to achieve.\n\nI already have a dynamic system prompt that is re-evaluated prior each run.\nBUT my use case is when tool-calls happen in the run : then you have several back-and-forth (aka steps)  going under the wood until the LLM finally replies to the user ",
          "created_at": "2025-02-03T16:30:54Z"
        }
      ]
    },
    {
      "issue_number": 1455,
      "title": "How can I export file(s) from Pyodide to local when using MCP Run Python?",
      "body": "### Question\n\nMy code will create a few files. I want to export them from the Pyodide environment to my local machine. Do you know what I can do?\n\nthanks!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "sangshuduo",
      "author_type": "User",
      "created_at": "2025-04-11T17:26:37Z",
      "updated_at": "2025-05-21T08:07:04Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1455/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1455",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1455",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:01.748254",
      "comments": [
        {
          "author": "maxschulz-COL",
          "body": "And maybe related to that: how can I get files into pyodide? E.g. LLM creates analysis code for data, I would like to run it sandboxed, how to get the data into the environment?",
          "created_at": "2025-04-16T08:26:53Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Sorry for the slow reply.\n\n@sangshuduo there's currently no way to do this, I guess we could provide that functionality - probably the easiest way would be to mount a directory in pyodide and run pyodide with that as the CWD.\n\n@maxschulz-COL I'm planning to add support for something like that, most ",
          "created_at": "2025-04-16T08:34:10Z"
        }
      ]
    },
    {
      "issue_number": 1048,
      "title": "imperative way to build graphs",
      "body": "### Description\n\nSo instead of declaring classes, you building nodes like\n\n```py\nnode1 = Node(...)\nnode2 = Node(...)\n\ngraph = Graph(nodes=[node1, node2])\n```\n\none advantage of this is it would provide a way for visual graph building tools to create graphs, like langflow or Comfy UI.\n\nObviously this would require either:\n* all nodes taking no arguments\n* all nodes taking the same arguments\n* all nodes being pydantic models or pydantic dataclasses providing runtime type validation\n\n### References\n\n_No response_",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-04T14:46:17Z",
      "updated_at": "2025-05-21T08:06:48Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "graph"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1048/reactions",
        "total_count": 9,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1048",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1048",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:01.967981",
      "comments": [
        {
          "author": "Kludex",
          "body": "Is this still planned? ",
          "created_at": "2025-04-17T15:31:57Z"
        },
        {
          "author": "Kludex",
          "body": "@dmontagu is working on this.",
          "created_at": "2025-05-21T08:06:47Z"
        }
      ]
    },
    {
      "issue_number": 1151,
      "title": "Documentation for debounce_by in streaming_text section",
      "body": "### Description\n\n Debounce_by is really nice - but a lot of frustration with Azure Open AI comes from clunky streaming. This is mainly a Azure and content filter problem - however adding documentation on how debounce_by works in the context of streaming will prevent some headache from us stuck with Azure Open AI and already wrapping our heads around WA to smooth out those streams.\n\nAn idea would be to perhaps add a few words in the paragraph here to explain ```debounce_by```. Its also referenced in some examples but not intuitive what it does and why you should / should not set None or another value. \n\nhttps://ai.pydantic.dev/results/#streaming-text\n\nPerhaps only a info with a small rewrite of the doc would be good?\n\n>debounce_by groups the response chunks. None means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. For text streaming this will group tokens into larger chunks.\n\n\n### References\n\n_No response_",
      "state": "open",
      "author": "martgra",
      "author_type": "User",
      "created_at": "2025-03-17T16:12:23Z",
      "updated_at": "2025-05-21T07:48:08Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1151/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1151",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1151",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:02.205477",
      "comments": []
    },
    {
      "issue_number": 944,
      "title": "Feature Request: provide an easy way to include your (versioned) API docs in LLM contexts",
      "body": "Claude Sonnet and some of the other LLMs don't seem to pull the pedantic-ai API into their weights yet. They will always lag recent API/SDK releases so providing a way to manually include them into an LLM context might be helpful, either programmatically or using interactive tools like NotebookLM. \n\nPerhaps the build could aggregate the API docs into a unicode file, using a standard format and naming convention, for inclusion as a release asset.",
      "state": "open",
      "author": "jb747",
      "author_type": "User",
      "created_at": "2025-02-19T12:16:23Z",
      "updated_at": "2025-05-21T07:46:29Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/944/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": "Version 1.0",
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/944",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/944",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:02.205509",
      "comments": [
        {
          "author": "jb747",
          "body": "It might save a nuclear reactor or two if adopted across projects :>). \n\nIt will be increasingly common that an LLM is invoked in, say, vscode to work on a project which has used a specific version of an API. The frontier LLMs now have contexts which are large enough to load the full API metadata fo",
          "created_at": "2025-02-27T03:56:00Z"
        },
        {
          "author": "Kludex",
          "body": "We provide the https://ai.pydantic.dev/llms.txt.\n\nIs that enough? See https://llmstxt.org/ for the standard.\n\nMaybe we need to document it?",
          "created_at": "2025-02-27T11:12:52Z"
        },
        {
          "author": "jb747",
          "body": "I think it is a good step, but:\n\n1. it is not versioned. Developers are often working with code that uses a specific version of a project. Expecting all LLMs to hoover up the source code and metadata of arbitrary projects is labor-intensive and optimistic.\n2. it is not necessary to create a separate",
          "created_at": "2025-03-01T15:51:02Z"
        },
        {
          "author": "jb747",
          "body": "I see this [IETF draft](https://www.ietf.org/archive/id/draft-ietf-httpapi-api-catalog-05.html) but it loses the close connection between code/binary and the API. Changesets are another type of metadata closely associated with releases that would be useful to an LLM. A JSON schema object might be a ",
          "created_at": "2025-03-01T16:24:13Z"
        },
        {
          "author": "jb747",
          "body": "security will also become more important as devs start to deploy agents. Executing un-vetted binaries and pulling arbitrary tokens into the context of a prompt is inherently risky. Perhaps digital signatures would help.. potentially more release metadata.",
          "created_at": "2025-03-01T18:31:50Z"
        }
      ]
    },
    {
      "issue_number": 999,
      "title": "More granular usage tracking",
      "body": "### Description\n\nI'd like to be able to track the token usage for each LLM call instead of just summing them at the end. I'm open to making a PR for this but not sure of the best way to handle where it should go.\n\nMy use case is instrumenting my app with per call tracking for cost analysis of different features / tools.\n\nI guess ultimately I'm thinking of enhancing the Usage model and functionality to include a calls attribute with per call information.  Does that make sense?",
      "state": "closed",
      "author": "gmr",
      "author_type": "User",
      "created_at": "2025-02-26T23:20:33Z",
      "updated_at": "2025-05-21T07:44:32Z",
      "closed_at": "2025-05-21T07:44:32Z",
      "labels": [
        "usage"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/999/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/999",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/999",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:02.564209",
      "comments": [
        {
          "author": "Kludex",
          "body": "The `usage` is now a field on the `ModelResponse`. You now have the granularity that you requested. 🙏 ",
          "created_at": "2025-05-21T07:44:32Z"
        }
      ]
    },
    {
      "issue_number": 662,
      "title": "Python Code ResultType Validation",
      "body": "It would be very useful to have a built in validator for when we want our LLM to generate python code (or ideally any type of code, but in this case I am using python. \r\n\r\nCurrently I am using this approach:\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\nfrom pydantic_ai import Agent, RunContext, ModelRetry\r\n\r\nclass CodeSnippet(BaseModel):\r\n    code: str\r\n\r\n# Initialize the agent with the desired model and result type\r\nagent = Agent(\r\n    model='openai:gpt-4o', \r\n    result_type=CodeSnippet,\r\n    retries=0\r\n)\r\n\r\n# Implement the result validator to check for syntax errors\r\n@agent.result_validator\r\nasync def validate_python_code(ctx: RunContext, result: CodeSnippet) -> CodeSnippet:\r\n    try:\r\n        # Attempt to compile the code to check for syntax errors\r\n        compile(result.code, '<string>', 'exec')\r\n    except SyntaxError as e:\r\n        # Raise ModelRetry to prompt the agent to generate a new result\r\n        raise ModelRetry(f'Syntax error in generated code: {e}') from e\r\n    return result\r\n\r\n# Use the agent to generate Python code based on a prompt\r\ndef main():\r\n    prompt = 'Generate a Python function that adds two numbers, but introduce a syntax error.'\r\n    result = agent.run_sync(prompt)\r\n    print('Generated Python Code:')\r\n    print(result.data.code)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\nAny ideas if this is something that you would like to implement?",
      "state": "closed",
      "author": "aristideubertas",
      "author_type": "User",
      "created_at": "2025-01-12T18:21:01Z",
      "updated_at": "2025-05-21T07:43:31Z",
      "closed_at": "2025-05-21T07:43:29Z",
      "labels": [
        "Feature request",
        "common tools"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/662/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/662",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/662",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:02.811564",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "we might support something like this in future, e.g. as a toolset, see #110.\r\n\r\nBut the Python should be run in an effective sandbox, not in the local process!",
          "created_at": "2025-01-16T10:07:12Z"
        },
        {
          "author": "Kludex",
          "body": "Duplicated of https://github.com/pydantic/pydantic-ai/issues/910.\n\nLet's watch that issue. 🙏 ",
          "created_at": "2025-02-27T11:32:45Z"
        },
        {
          "author": "Kludex",
          "body": "Ah wait, sorry, you want the code itself as a result? 🤔\n\nIs there any LLM that sends that field back already?",
          "created_at": "2025-02-27T11:33:36Z"
        },
        {
          "author": "Kludex",
          "body": "I think this is what you want: https://ai.pydantic.dev/mcp/run-python/#mcp-run-python",
          "created_at": "2025-05-21T07:43:29Z"
        }
      ]
    },
    {
      "issue_number": 1788,
      "title": "MCP Server calling issue",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nTrying to call an MCP server. In one set of code it is working fine but in another it gives server not found error\n\nWorking Version \n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP\nimport asyncio \n\nserver = MCPServerHTTP(url='http://127.0.0.1:7860/gradio_api/mcp/sse')  \nagent = Agent(model=model, mcp_servers=[server])  \n\n\nasync def main():\n    async with agent.run_mcp_servers():  \n        result = await agent.run('Count word for Hello number of H')\n    print(result.output)\n\nasyncio.run(main())\n```\nOutput\n`The letter \"H\" appears 1 time in the word \"Hello\".`\n\nNot working version\n```\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.openai import OpenAIModel,OpenAIModelSettings\nfrom pydantic_ai.tools import Tool, ToolDefinition\nfrom pydantic_ai.providers.azure import AzureProvider\nimport nest_asyncio\nfrom pydantic import BaseModel\nfrom pydantic_ai.mcp import MCPServerHTTP\nnest_asyncio.apply()\n\nclass AddNumbers(BaseModel):\n    a: int\n    b: int\n\nserver = MCPServerHTTP(url='http://127.0.0.1:7860/gradio_api/mcp/sse') \n\nasync def  only_if_42(ctx: RunContext[AddNumbers], tool_def: ToolDefinition) -> ToolDefinition | None:\n    if ctx.deps.a == 42:\n        return tool_def\n    return None\n\nasync def add_two_numbers(ctx: RunContext[AddNumbers]) -> int:\n    return ctx.deps.a + ctx.deps.b\n\ntools = Tool(add_two_numbers,\n             takes_ctx=True,max_retries=3, \n             name='add_two_numbers',\n             description='Add two numbers', require_parameter_descriptions=False, \n             prepare=only_if_42,)\n\n\n\nagent = Agent(model, output_type=int, \n              instructions=\"always starts with howdy\", \n              deps_type=AddNumbers, \n              system_prompt=\"You are very witty teacher\", \n              name=\"Cool Agent\",\n              model_settings=model_settings,\n              output_retries=3,\n              tools=[tools],\n              mcp_servers=[server],\n              end_strategy='early',\n\n              )\nresult =  agent.run_sync('hello world', deps=AddNumbers(a=42, b=3))\n```\n\noutput\n\n`MCP server is not running: MCPServerHTTP(url='http://127.0.0.1:7860/gradio_api/mcp/sse', headers=None, timeout=5, sse_read_timeout=300, log_level=None)`\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPydnatic-ai:2.11.4\nPython: 3.12\nLLM: Azure\n```",
      "state": "closed",
      "author": "kauabh",
      "author_type": "User",
      "created_at": "2025-05-21T06:05:35Z",
      "updated_at": "2025-05-21T07:38:32Z",
      "closed_at": "2025-05-21T07:38:31Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1788/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1788",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1788",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:03.052614",
      "comments": [
        {
          "author": "kauabh",
          "body": "No issues, saw in documentation async with agent.run_mcp_servers(). It will be good to know why it was decided to run it under with block though.",
          "created_at": "2025-05-21T07:38:31Z"
        }
      ]
    },
    {
      "issue_number": 1453,
      "title": "Are multiple system prompt messages \"normal\"?",
      "body": "### Question\n\nI noticed `@agent.system_prompt` creates another role:system message - meaning you could end up with several, depending on how many dynamic pieces you define.\n\nI don't think this is documented anywhere explicitly, and I actually can't find a good source saying whether models (or which models) are able to reliably capture all these system messages if they are separate.\n\nIn addition, I'm across several pieces of code/issues that implicitly rely on the fact* that there would be a single system message that appears first, or not at all.\n",
      "state": "closed",
      "author": "diego898",
      "author_type": "User",
      "created_at": "2025-04-11T14:37:54Z",
      "updated_at": "2025-05-21T07:37:55Z",
      "closed_at": "2025-05-21T07:37:55Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1453/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1453",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1453",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:03.290896",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-19T14:00:44Z"
        },
        {
          "author": "DouweM",
          "body": "@diego898 Can you please share the code/issues that implicitly rely on there being just a single system prompt message?\n\nOur system prompts example shows off this behavior by using multiple `@agent.system_prompt` calls, which can be a mix of static and dynamic (context-dependent): https://ai.pydanti",
          "created_at": "2025-04-21T18:59:46Z"
        }
      ]
    },
    {
      "issue_number": 1601,
      "title": "Using the Gemini model via Vertex ignores provider and uses GLA instead",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nCalling gemini using the following vertex setup results in a validation error:\n\npydantic_ai.exceptions.UserError: Set the `GEMINI_API_KEY` environment variable or pass it via `GoogleGLAProvider(api_key=...)`to use the Google GLA provider.\n\n```py\nmodel = GeminiModel(\n    \"gemini-2.0-flash\",\n    provider=GoogleVertexProvider(\n        service_account_info=service_account_info,\n        project_id=project,\n        region=location,\n    ),\n)\nagent = Agent(model)\n\nresult = agent.run_sync(...)\n```\n\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n\"pydantic[email]==2.11.3\",\n\"pydantic-ai>=0.1.6\",\n\nPython 3.13.3\n```",
      "state": "open",
      "author": "parhammmm",
      "author_type": "User",
      "created_at": "2025-04-26T13:01:57Z",
      "updated_at": "2025-05-21T07:31:14Z",
      "closed_at": null,
      "labels": [
        "Stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1601/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1601",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1601",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:03.528965",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex Can you have a look please as you were working on Google auth?",
          "created_at": "2025-04-29T21:54:49Z"
        },
        {
          "author": "parhammmm",
          "body": "@Kludex if you can point me where to look I can have a go at fixing it; I couldn't pin point where to look",
          "created_at": "2025-05-09T14:21:43Z"
        },
        {
          "author": "DouweM",
          "body": "@parhammmm The big Google refactor is in https://github.com/pydantic/pydantic-ai/pull/1373, can you check that out and see if the error still exists?",
          "created_at": "2025-05-12T10:55:38Z"
        },
        {
          "author": "Kludex",
          "body": "Can you please try the new `GoogleModel`? https://ai.pydantic.dev/models/google/#google",
          "created_at": "2025-05-21T07:31:08Z"
        }
      ]
    },
    {
      "issue_number": 901,
      "title": "deep research example",
      "body": "Would love to see a pydantic AI deep research example.\n\nThere's a couple of open source examples (https://x.com/minchoi/status/1889435097713004952):\n - https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research\n - https://github.com/assafelovic/gpt-researcher\n - https://github.com/epuerta9/deep-research-py\n\n",
      "state": "open",
      "author": "raybellwaves",
      "author_type": "User",
      "created_at": "2025-02-12T02:44:02Z",
      "updated_at": "2025-05-21T07:28:52Z",
      "closed_at": null,
      "labels": [
        "examples"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/901/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/901",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/901",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:03.731070",
      "comments": [
        {
          "author": "dmontagu",
          "body": "Agreed it's worth building/porting one of those examples to use pydantic-ai, thanks for the links and suggestion",
          "created_at": "2025-02-12T22:02:16Z"
        },
        {
          "author": "grahamannett",
          "body": "Hey, I am actually working on this and have put something together at:\n\nhttps://github.com/grahamannett/deepre\n\nIt uses pydantic-ai and is similar to how [this reflex example](https://github.com/reflex-dev/reflex-llm-examples/tree/main/open_deep_researcher) works but as I have learned more about pyd",
          "created_at": "2025-02-17T23:43:40Z"
        },
        {
          "author": "grahamannett",
          "body": "Still figuring out how exactly to string all of this together using pydantic-ai, heres a _somewhat working_ cli version that I was aiming to contribute as a 1 file example (although now i am not sure how feasible 1 file will be).\n\nhttps://github.com/grahamannett/deepre/blob/main/scripts/run-example.",
          "created_at": "2025-02-22T01:44:29Z"
        },
        {
          "author": "dmontagu",
          "body": "@grahamannett thanks for sharing this! I would love to include something like this as a one-file example. We'll need to find some time to review your implementation more thoroughly but it's very helpful to see stuff like this. (I think we will prioritize adding more involved examples like this in th",
          "created_at": "2025-02-25T16:08:20Z"
        },
        {
          "author": "lars20070",
          "body": "Maybe you find this [minimal deep research workflow](https://github.com/lars20070/deepresearcher2) helpful. It is inspired by the [`local-deep-researcher`](https://github.com/langchain-ai/local-deep-researcher) LangChain example.\n\nIf anyone is interested, I can try to boil it down to a one-file exam",
          "created_at": "2025-04-22T09:16:41Z"
        }
      ]
    },
    {
      "issue_number": 1401,
      "title": "CI is not testing evals without logfire",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nPydantic is tested like this:\n\nhttps://github.com/pydantic/pydantic-ai/blob/84c6c5d23ba59e5feae4188e1080ac6a85e00b63/.github/workflows/ci.yml#L153-L163\n\nPresumably, that's:\n1. Slim\n2. With Evals (and others)\n3. With Evals and Logfire (and others)\n\nAs I noticed in https://github.com/pydantic/pydantic-ai/pull/1400#issuecomment-2784882285, though, some of the Evals tests are being skipped in mode 2, so we're not actually testing Evals without Logfire:\n\nhttps://github.com/pydantic/pydantic-ai/actions/runs/14304998822/job/40086857591\n\n```\ntests/evals/test_dataset.py ssssssssssssssssssssssssssssss                                                                                     [  3%]\ntests/evals/test_evaluator_base.py ........                                                                                                    [  3%]\ntests/evals/test_evaluator_common.py ssssssssssssss                                                                                            [  5%]\ntests/evals/test_evaluator_context.py ...                                                                                                      [  5%]\ntests/evals/test_evaluator_spec.py .....                                                                                                       [  6%]\ntests/evals/test_evaluators.py sssssssssssssss                                                                                                 [  7%]\ntests/evals/test_llm_as_a_judge.py ....                                                                                                        [  8%]\ntests/evals/test_otel.py sssssssssssssssssssssssss                                                                                             [ 10%]\ntests/evals/test_render_numbers.py sssssssssssssssssssssssssssssssssssssssssssssssssssssssssss                                                 [ 16%]\ntests/evals/test_reporting.py ssssss                                                                                                           [ 17%]\ntests/evals/test_reports.py ssssss                                                                                                             [ 17%]\ntests/evals/test_utils.py ..........                                                                                                           [ 18%]\n```\n\nThis is why https://github.com/pydantic/pydantic-ai/issues/1375 and https://github.com/pydantic/pydantic-ai/issues/1399 weren't caught in CI.\n\nThe underlying issue is that the test checks if pydantic_evals can be imported, and if not, it assumes it's in mode 1 and doesn't need to run those tests at all. But the reason why pydantic_evals couldn't be installed in this case is not because it was skipped intentionally, but rather because of #1375: opentelemetry-sdk was missing as a dependency, and only available if logfire happened to be installed.\n\nWhich is why that evals-without-logfire test didn't start running until I fixed _that_ issue in https://github.com/pydantic/pydantic-ai/pull/1400.\n\nAnd the reason the test is failing on that PR is because it has a hard dependency on logfire, even though it should be optional.\n\nI can look at fixing the test to make logfire optional in that same PR, but I thought this higher level issue was worth tracking separately.\n\n---\n\nNote also that in `tests/evals/test_dataset.py`, \"is logfire installed\" is implicitly being checked as part of the \"is pydantic-evals installed\" check, but in `tests/evals/test_evaluators.py` and `tests/evals/test_evaluator_common.py`, `import logfire` is actually explicitly stated inside the `with try_import() as imports_successful` context manager. I assume this was written before it was decided to make logfire optional for evals.\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13, pydantic-ai 0.0.53\n```",
      "state": "open",
      "author": "DouweM",
      "author_type": "User",
      "created_at": "2025-04-08T00:38:49Z",
      "updated_at": "2025-05-21T07:28:25Z",
      "closed_at": null,
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1401/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1401",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1401",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:03.948969",
      "comments": [
        {
          "author": "dmontagu",
          "body": "I think it makes sense to add a second file's worth of tests of evals behavior when logfire isn't installed. In particular, things like the behavior of the `context_subtree` and just generally that you don't get errors.",
          "created_at": "2025-04-25T18:37:50Z"
        }
      ]
    },
    {
      "issue_number": 1595,
      "title": "Expose Agent as a CLI for testing during development",
      "body": "### Description\n\nIt'd be nice to have the ability to do `agent.to_cli()` and get an experience similar to https://ai.pydantic.dev/cli/ for testing out an agent without having to build a full chat UI.\n\n### References\n\nAs discussed in https://pydanticlogfire.slack.com/archives/C083V7PMHHA/p1745525917655549.",
      "state": "closed",
      "author": "DouweM",
      "author_type": "User",
      "created_at": "2025-04-25T15:14:15Z",
      "updated_at": "2025-05-21T07:28:10Z",
      "closed_at": "2025-05-21T07:28:09Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1595/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1595",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1595",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:05.997384",
      "comments": [
        {
          "author": "DouweM",
          "body": "cc @samuelcolvin as you've been working on the CLI",
          "created_at": "2025-04-25T15:14:28Z"
        },
        {
          "author": "Kludex",
          "body": "I want this as well.",
          "created_at": "2025-04-28T12:44:58Z"
        },
        {
          "author": "benomahony",
          "body": "This could be cool inspiration: https://github.com/darrenburns/elia",
          "created_at": "2025-04-30T11:48:01Z"
        },
        {
          "author": "AndrewHannigan",
          "body": "+1 I'm also looking for this",
          "created_at": "2025-05-04T17:43:37Z"
        },
        {
          "author": "Kludex",
          "body": "Maybe the right approach is `pai run main:agent`?\n\nJust a thought... 🤔",
          "created_at": "2025-05-04T20:48:49Z"
        }
      ]
    },
    {
      "issue_number": 1786,
      "title": "FileNotFoundError on to_cli() call (when prompt sent)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI have checked, this is not a duplicated issue currently.\n\nI was using to_cli_sync method at MacOS;\n\nThe prompt_history path was `/users/<name>/.pydantic-ai/prompt-history.txt`\n\nAnd I got:\n\n`Unhandled exception in event loop:\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/input/vt100.py\", line 162, in callback_wrapper\n    callback()\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/application/application.py\", line 714, in read_from_input_in_context\n    context.copy().run(read_from_input)\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/application/application.py\", line 694, in read_from_input\n    self.key_processor.process_keys()\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/key_binding/key_processor.py\", line 273, in process_keys\n    self._process_coroutine.send(key_press)\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/key_binding/key_processor.py\", line 188, in _process\n    self._call_handler(matches[-1], key_sequence=buffer[:])\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/key_binding/key_processor.py\", line 323, in _call_handler\n    handler.call(event)\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/key_binding/key_bindings.py\", line 127, in call\n    result = self.handler(event)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/shortcuts/prompt.py\", line 807, in _accept_input\n    self.default_buffer.validate_and_handle()\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/buffer.py\", line 1891, in validate_and_handle\n    self.append_to_history()\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/buffer.py\", line 1365, in append_to_history\n    self.history.append_string(self.text)\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/history.py\", line 76, in append_string\n    self.store_string(string)\n  File \"/opt/homebrew/lib/python3.11/site-packages/prompt_toolkit/history.py\", line 299, in store_string\n    with open(self.filename, \"ab\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\nException [Errno 2] No such file or directory: '/Users/mert/.pydantic-ai/prompt-history.txt'`\n\nI thought you forgot to make dirs and create history file for path to store prompts there.\n\n\n\nI'm opening a PR now.\n\n### Example Code\n\n```Python\nimport pydantic_ai._cli as cli\nhistory_path = cli.PROMPT_HISTORY_PATH\nhistory_path.parent.mkdir(parents=True, exist_ok=True)\nhistory_path.touch(exist_ok=True)\n```\n\nit's all done!\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.7\nPydantic_AI 0.2.5\n```",
      "state": "closed",
      "author": "fswair",
      "author_type": "User",
      "created_at": "2025-05-20T23:23:28Z",
      "updated_at": "2025-05-21T07:08:14Z",
      "closed_at": "2025-05-21T07:08:14Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1786/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1786",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1786",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:06.226792",
      "comments": [
        {
          "author": "fswair",
          "body": "PR: https://github.com/pydantic/pydantic-ai/pull/1787",
          "created_at": "2025-05-20T23:34:38Z"
        }
      ]
    },
    {
      "issue_number": 1383,
      "title": "EndStrategy not documented",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nThe [`Agent.__init__` documentation](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent.__init__) for `end_strategy` says \"See [EndStrategy](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.EndStrategy) for more information\". Except the documentation on `EndStrategy` is completely empty.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nlatest version, it's on the web site\n```",
      "state": "open",
      "author": "phemmer",
      "author_type": "User",
      "created_at": "2025-04-04T23:49:14Z",
      "updated_at": "2025-05-21T07:06:43Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1383/reactions",
        "total_count": 4,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 1,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1383",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1383",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:06.465235",
      "comments": [
        {
          "author": "kauabh",
          "body": "Same, not sure how to use it properly",
          "created_at": "2025-05-17T10:28:59Z"
        },
        {
          "author": "kauabh",
          "body": "Hey @phemmer  Was able to find usage of `EndStrategy` It is not the part of documentation,  \n\n\n```\nEndStrategy = Literal['early', 'exhaustive']\n\"\"\"The strategy for handling multiple tool calls when a final result is found.\n\n- `'early'`: Stop processing other tool calls once a final result is found\n-",
          "created_at": "2025-05-17T11:12:52Z"
        }
      ]
    },
    {
      "issue_number": 1175,
      "title": "How to stream from inside a tool?",
      "body": "Imagine the following case:\n\n```python\nfrom pydantic_ai import Agent, RunContext\n\nqa_agent = Agent(  \n    'openai:gpt-4o',\n    system_prompt=(\n        'Use the `qa` function to reply to user's questions about pydantic ai '\n    ),\n)\n\n@qa.tool\nasync def qa(ctx: RunContext[int], query: str) -> AsyncGenerator[str | Reference, None]:  \n    \"\"\"Answer users questions about pydantic ai, by inspecting documentation\"\"\"\n    docs = get_context(...)\n    async for token in prompt(docs, ...):\n        yield token # Can be a Reference to a document, or just text \n```\n\nThis would also need the agent to end earlier, which is a known issue that other people are asking for.\nI also know that this can be circumvented by using the graph implementation.\nBut I also don't see a way to stream results from inside of a node in the graph implementation.\nIs this a pattern that is accounted for, or is planned for the future?\nCan we do this right now?",
      "state": "open",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-03-19T10:39:33Z",
      "updated_at": "2025-05-21T06:07:51Z",
      "closed_at": null,
      "labels": [
        "duplicate"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1175/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1175",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1175",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:06.735693",
      "comments": [
        {
          "author": "pedroallenrevez",
          "body": "@Kludex ? Any idea?",
          "created_at": "2025-03-24T12:45:46Z"
        },
        {
          "author": "Kludex",
          "body": "Hi @pedroallenrevez , sorry the delay.\n\nCan you try to explain a use case that you'd need this? I'm having a hard time seeing a real case scenario that you'd want a generator.",
          "created_at": "2025-04-17T15:42:09Z"
        },
        {
          "author": "wylansford",
          "body": "@Kludex \n\nThis is quite useful for frontend functionality. One specific example is tracking the progress of a longer running tool call and streaming those results in real time to the user. \n\n",
          "created_at": "2025-05-18T02:57:52Z"
        },
        {
          "author": "A3Ackerman",
          "body": "@wylansford I'm achieving something similar by passing a buffer/queue into tools via Deps. All my responses are written to the buffer instead of directly yielded to the caller. ",
          "created_at": "2025-05-21T06:07:49Z"
        }
      ]
    },
    {
      "issue_number": 1747,
      "title": "A2A AgentCard - missing values and some of them misspelled",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI've tried to use latest google a2a example on pydantic-ai a2a agent.\nI got errors on agentCard retrieval. Seems that fields \"authentication\" and \"capabilities\" are required , and fields \"defaultInputModes\" , \"defaultOutputModes\" are misspelled (in FastA2A they seem to be default_input_modes and  default_output_modes,  and both authentication and capabilities are not in response).\n\nI've checked google spec here : https://google.github.io/A2A/specification/#55-agentcard-object-structure\nBelow errors from running google example :\n\nAn error occurred: 4 validation errors for AgentCard\nauthentication\n  Field required [type=missing, input_value={'name': 'BJ llm agent', ...', 'url': 'www.test.com'}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ncapabilities\n  Field required [type=missing, input_value={'name': 'BJ llm agent', ...', 'url': 'www.test.com'}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ndefaultInputModes\n  Field required [type=missing, input_value={'name': 'BJ llm agent', ...', 'url': 'www.test.com'}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ndefaultOutputModes\n  Field required [type=missing, input_value={'name': 'BJ llm agent', ...', 'url': 'www.test.com'}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython: 3.13.2\npydantic-ai : 0.2.4\nllm - local openai like\n```",
      "state": "closed",
      "author": "benjisss",
      "author_type": "User",
      "created_at": "2025-05-16T15:07:44Z",
      "updated_at": "2025-05-21T05:30:58Z",
      "closed_at": "2025-05-21T05:30:58Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1747/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1747",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1747",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:07.014471",
      "comments": [
        {
          "author": "SecretiveShell",
          "body": "This is also effecting me, the agent card model is diverging from the spec.\n\nThe official A2A python examples define capabilities as required but fastA2A defines them as optional.",
          "created_at": "2025-05-19T13:03:05Z"
        },
        {
          "author": "SecretiveShell",
          "body": "for reference the official google implementation of this base model can be found here https://github.com/google/A2A/blob/18998ab681e886d8bb0512d2b358040290e97d18/samples/python/common/types.py#L340",
          "created_at": "2025-05-19T13:06:01Z"
        },
        {
          "author": "DouweM",
          "body": "@Kludex Assigning to you as this is A2A related!",
          "created_at": "2025-05-19T21:59:22Z"
        }
      ]
    },
    {
      "issue_number": 1649,
      "title": "Support Tool Calling with Llama 3.3 on Bedrock",
      "body": "### Question\n\n### Description\n\nI'm using the [BedrockConverseModel](https://ai.pydantic.dev/api/models/bedrock/#pydantic_ai.models.bedrock.BedrockConverseModel) to use models hosted on Bedrock. When working with Llama-3.3-70B, tool calls do not work. \n\nThis seems to be a similar issue to https://github.com/pydantic/pydantic-ai/issues/1623, and following @DouweM [advice](https://github.com/pydantic/pydantic-ai/issues/1623#issuecomment-2845748817) I've attempted to create a custom `GenerateToolJsonSchema` to match Llama 3.3's schema (which is the same as [llama 3.1](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#json-based-tool-calling)) but found it confusing. I'm hoping someone with more knowledge on the what's going on under the hood can help me out here. Considering that Llama3 is one of the top 5 most popular Llms, I hope others would find this useful too. \n\n### Code snippet\n\nBelow is an example showing that tools work fine when working with Anthropic on Bedrock, but not Llama. This is how I convinced myself it's not a problem with bedrock specifically or my prompt, but rather a difference in the underlying model\n\n```python\nfrom pydantic_ai.providers.bedrock import BedrockProvider\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nimport boto3\nimport random\n\nfrom pydantic_ai import RunContext, Agent\n\n# create an agent with the model id parameterized.\n# uses the \"dice rolling\" example from the pydantic ai docs\n# https://ai.pydantic.dev/tools/#registering-function-tools-via-agent-argument\ndef agent_with_tool_use(model: str):\n    \n    bedrock_client = boto3.client(\"bedrock-runtime\")\n    model = BedrockConverseModel(\n        model,\n        provider=BedrockProvider(bedrock_client=bedrock_client),\n    )\n    \n    agent = Agent(\n        model,\n        system_prompt=(\n            \"You're a dice game, you should roll the die and see if the number \"\n            \"you get back matches the user's guess. If so, tell them they're a winner. \"\n            \"Use the player's name in the response.\"\n        ),\n        instrument=True,\n    )\n    \n    \n    @agent.tool_plain  \n    def roll_die() -> str:\n        \"\"\"Roll a six-sided die and return the result.\"\"\"\n        return str(random.randint(1, 6))\n    \n    \n    @agent.tool  \n    def get_player_name(ctx: RunContext[str]) -> str:\n        \"\"\"Get the player's name.\"\"\"\n        return ctx.deps\n    \n    return agent\n\n\n# when calling with anthropic, tools are invoked\nanthropic_agent = agent_with_tool_use(\"anthropic.claude-3-5-sonnet-20241022-v2:0\")\nanthropic_agent.run_sync('My guess is 4', deps='Anne')\n#  > AgentRunResult(output='Sorry Anne! You guessed 4, but the die rolled a 5. Better luck next time!')\n\n\n# when calling with llama, it does not invoke the tool\nllama_agent = agent_with_tool_use(\"us.meta.llama3-3-70b-instruct-v1:0\")\nllama_agent.run_sync('My guess is 4', deps='Anne')\n#  > AgentRunResult(output='{\"type\": \"function\", \"name\": \"roll_die\", \"parameters\": {}}') \n```\n\nAny advice, code snippets or help on resolving this would be super appreciated. thanks. \n\n### Logfire output\n\nWhen calling anthropic, you can see that the tool is registered and called\n\n![Image](https://github.com/user-attachments/assets/2bef7024-c375-4a68-b79f-46c45efa0f51)\n\n<details>\n<summary>Full conversation json</summary>\n\n```json\n{\n    \"agent_name\": \"anthropic_agent\",\n    \"all_messages_events\": [\n        {\n            \"content\": \"You're a dice game, you should roll the die and see if the number you get back matches the user's guess. If so, tell them they're a winner. Use the player's name in the response.\",\n            \"role\": \"system\",\n            \"gen_ai.message.index\": 0,\n            \"event.name\": \"gen_ai.system.message\"\n        },\n        {\n            \"content\": \"My guess is 4\",\n            \"role\": \"user\",\n            \"gen_ai.message.index\": 0,\n            \"event.name\": \"gen_ai.user.message\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Let me get your name and roll the die to see if you guessed correctly!\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"tooluse_el55vVyWT2WM-h6oQo1zDA\",\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": \"get_player_name\",\n                        \"arguments\": {}\n                    }\n                }\n            ],\n            \"gen_ai.message.index\": 1,\n            \"event.name\": \"gen_ai.assistant.message\"\n        },\n        {\n            \"content\": \"Anne\",\n            \"role\": \"tool\",\n            \"id\": \"tooluse_el55vVyWT2WM-h6oQo1zDA\",\n            \"name\": \"get_player_name\",\n            \"gen_ai.message.index\": 2,\n            \"event.name\": \"gen_ai.tool.message\",\n            \"functionName\": \"get_player_name\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"tooluse_dWbCrxm6TteRD-S8e-0eVQ\",\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": \"roll_die\",\n                        \"arguments\": {}\n                    }\n                }\n            ],\n            \"gen_ai.message.index\": 3,\n            \"event.name\": \"gen_ai.assistant.message\"\n        },\n        {\n            \"content\": \"2\",\n            \"role\": \"tool\",\n            \"id\": \"tooluse_dWbCrxm6TteRD-S8e-0eVQ\",\n            \"name\": \"roll_die\",\n            \"gen_ai.message.index\": 4,\n            \"event.name\": \"gen_ai.tool.message\",\n            \"functionName\": \"roll_die\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Sorry Anne! You guessed 4, but the die landed on 2. Better luck next time!\",\n            \"gen_ai.message.index\": 5,\n            \"event.name\": \"gen_ai.assistant.message\"\n        }\n    ],\n    \"final_result\": \"Sorry Anne! You guessed 4, but the die landed on 2. Better luck next time!\",\n    \"gen_ai.usage.input_tokens\": 1592,\n    \"gen_ai.usage.output_tokens\": 119,\n    \"model_name\": \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n}\n```\n</details>\n\nBut when we call with Llama, the tool does not seem to be correctly registered and it is not invoked: \n\n<img width=\"1435\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/df03ccac-a04d-432e-bcab-2971a4767e4e\" />\n\n<details>\n<summary>Full conversation history</summary>\n\n```json\n{\n    \"agent_name\": \"llama_agent\",\n    \"all_messages_events\": [\n        {\n            \"content\": \"You're a dice game, you should roll the die and see if the number you get back matches the user's guess. If so, tell them they're a winner. Use the player's name in the response.\",\n            \"role\": \"system\",\n            \"gen_ai.message.index\": 0,\n            \"event.name\": \"gen_ai.system.message\"\n        },\n        {\n            \"content\": \"My guess is 4\",\n            \"role\": \"user\",\n            \"gen_ai.message.index\": 0,\n            \"event.name\": \"gen_ai.user.message\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"{\\\"type\\\": \\\"function\\\", \\\"name\\\": \\\"roll_die\\\", \\\"parameters\\\": {}}\",\n            \"gen_ai.message.index\": 1,\n            \"event.name\": \"gen_ai.assistant.message\"\n        }\n    ],\n    \"final_result\": {\n        \"type\": \"function\",\n        \"name\": \"roll_die\",\n        \"parameters\": {}\n    },\n    \"gen_ai.usage.input_tokens\": 242,\n    \"gen_ai.usage.output_tokens\": 19,\n    \"model_name\": \"us.meta.llama3-3-70b-instruct-v1:0\"\n}\n```\n</details>\n\n### Configuration\n\n**Relevant libraries**\n\n```\npydantic                                        2.10.6\npydantic-ai                                   0.0.55\npydantic-ai-slim                           0.1.9\npydantic_core                               2.27.2\npydantic-evals                              0.0.55\npydantic-graph                             0.1.9\npydantic-settings                         2.9.1\nboto3                                             1.38.8\nboto3-stubs                                  1.34.162\nbotocore                                        1.38.8\nbotocore-stubs                             1.37.38\n```\n\n**Python version** `3.11.6`\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "tamir-alltrue-ai",
      "author_type": "User",
      "created_at": "2025-05-05T20:30:35Z",
      "updated_at": "2025-05-20T18:54:42Z",
      "closed_at": "2025-05-20T18:54:42Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1649/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1649",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1649",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:07.256930",
      "comments": [
        {
          "author": "DouweM",
          "body": "@tamir-alltrue-ai It seems like the tool is being registered to the model, and the model is trying to call it, but as it shows in the second screenshot and JSON snippet, the model is doing so through a regular text message containing JSON, rather than a special `tool_calls` property on the message l",
          "created_at": "2025-05-06T16:31:01Z"
        },
        {
          "author": "tamir-alltrue-ai",
          "body": "Thanks for your response @DouweM \n\nTurns out that `boto3` doesn't use `httpx` nor `requests` - it uses `botocore` which in turn uses `urllib3` under the hood ([source](https://aws.amazon.com/blogs/developer/removing-the-vendored-version-of-requests-from-botocore/)). \n\nThere is no instrumentation for",
          "created_at": "2025-05-06T17:00:12Z"
        },
        {
          "author": "tamir-alltrue-ai",
          "body": "@DouweM I've looked into this a bit further, and I'm starting to think it's a limitation with bedrock. According [to this](https://docs.llamaindex.ai/en/stable/examples/llm/bedrock_converse/):\n\n\n> Claude, Command and Mistral Large models supports native function calling through AWS Bedrock Converse.",
          "created_at": "2025-05-06T21:25:51Z"
        },
        {
          "author": "DouweM",
          "body": "@tamir-alltrue-ai We build the toolResult block here:\n\nhttps://github.com/pydantic/pydantic-ai/blob/dd22595464ea430d9fcea388e506bf0a1697a9a4/pydantic_ai_slim/pydantic_ai/models/bedrock.py#L384-L395\n\nAs you can see, the difference with your example is that we pass `{'text': '<json string>'}` instead ",
          "created_at": "2025-05-08T08:06:43Z"
        },
        {
          "author": "tamir-alltrue-ai",
          "body": "Thanks @DouweM , \n\nYeah I agree the challenge is that different models give different response types. The change you propose is definitely a breaking one - the tests start failing (`nova` stops working for structured output) and anthropic also stops working for tool calling. I also haven't seen it f",
          "created_at": "2025-05-08T14:00:23Z"
        }
      ]
    },
    {
      "issue_number": 1079,
      "title": "enabling the result_type causes issues with the provider's URL",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI am working with a local ollama installation that is accessible at and configured in the provider of OpenAIProvider type (base_url='http://cloudmaster:1143/v1').\n\nWhen I try to instantiate a new Agent everything works well until I add the 'result_type=MyModel' parameter to the Agent() invocation. When I do that, instead of using the `base_url` that was configured in the provider, it goes out and queries the ollama servers.\n\nThe correct behavior would be to call the base_url the `provider` object, but the constructor seems to have issues with it.\n\n### Example Code\n\n```Python\n# Pydantic AI\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclass ResponseModel(BaseModel):\n    \"\"\"Structured response with metadata.\"\"\"\n    response: str\n    needs_escalation: bool\n    follow_up_required: bool\n    sentiment: str = Field(description=\"Customer sentiment analysis\")\n\nmy_agent = Agent(\n    model=OpenAIModel(\n        model_name='olmo2:7b', \n        provider = OpenAIProvider(\n            api_key='tttt',\n            base_url='http://cloudmaster:1143/v1',\n        ),\n    ),\n    system_prompt=\"You are a helpful assistant.\",\n    result_type=ResponseModel,\n)\n\nresponse = my_agent.run_sync(\"How can I track my order #12345?\")\nprint(response.data.model_dump_json(indent=2))\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nFor the Python Environment we use (`pip3 -m venv .venv`):\n- python3 == 3.12.3\n- pip3 == 24.0\n\nFor the LLM, I use the following setting:\n- `ollama serve` on a first computer (cloudmaster::Ubuntu24 server)\n- `olmo2:7b` is the model I am testing with, but same results with other models\n\nTo reproduce the error, simply execute the script above:\n- `python3 test0.py` on the second computer (directly attached to cloudmaster)\n\nThe requirements.txt file (pydantic-ai>=0.0.36):\npydantic>=2.10.0\npydantic-ai>=0.0.36\npython-dotenv>=1.0.0\npytest>=8.0.0\n# crawl4ai>=0.5.1\nsmolagents>=0.0.0\nlangchain>=0.3.20\nlangchain[community]>=0.3.19\nlangchain[chroma]>=0.2.2\nlangchain[huggingface]>=0.1.2\nlangchain[ollama]>=0.2.3\nlangchain-ollama>=0.2.0\n```",
      "state": "closed",
      "author": "opencrypto",
      "author_type": "User",
      "created_at": "2025-03-08T02:59:24Z",
      "updated_at": "2025-05-20T16:52:59Z",
      "closed_at": "2025-05-20T16:52:58Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1079/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1079",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1079",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:07.521206",
      "comments": [
        {
          "author": "opencrypto",
          "body": "The problem is also present if the tools=[ ... ] is added to the Agent() constructor. For example, adding the tools in the previous code:\n`\nmy_agent = Agent(\n    ...\n    system_prompt=\"You are a helpful assistant.\",\n    tools=[\n        Tool(function=throw_dice, name=\"throw_dice\", description=\"Roll a",
          "created_at": "2025-03-08T03:23:14Z"
        },
        {
          "author": "Kludex",
          "body": "Why are you saying the base URL is not being called? ",
          "created_at": "2025-03-08T06:28:51Z"
        },
        {
          "author": "Kludex",
          "body": "Closing this as stale. Happy to continue the conversation if my message is replied.",
          "created_at": "2025-05-20T16:52:58Z"
        }
      ]
    },
    {
      "issue_number": 916,
      "title": "Consider replacing our Gemini API integration with `google-genai` once they have proper async support",
      "body": "See https://github.com/googleapis/python-genai/issues/283.\n\nOnce that is done we can use that library instead of [our own integration](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/gemini.py)",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-02-12T20:50:26Z",
      "updated_at": "2025-05-20T16:51:14Z",
      "closed_at": "2025-05-20T16:51:14Z",
      "labels": [
        "new models"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/916/reactions",
        "total_count": 8,
        "+1": 8,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/916",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/916",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:07.745686",
      "comments": [
        {
          "author": "TheMellyBee",
          "body": "This was closed as completed. Can we get this integrated? ",
          "created_at": "2025-03-17T16:46:21Z"
        },
        {
          "author": "Kludex",
          "body": "We now have the [`GoogleModel`](https://ai.pydantic.dev/models/google/) that uses `google-genai` underneath.",
          "created_at": "2025-05-20T16:51:13Z"
        }
      ]
    },
    {
      "issue_number": 1555,
      "title": "Disable thinking mode in Gemini 2.5 flash",
      "body": "### Description\n\nGemini 2.5 flash has thinking mode enabled by default. This is is expensive. It can be turned off, but Pydantic AI doesn't support this. \n\nAISDK from Vercel does have this. We need something similar:\n\n![Image](https://github.com/user-attachments/assets/b78d7adf-4914-4566-b2db-cb36d963223c)\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "tcrapts",
      "author_type": "User",
      "created_at": "2025-04-20T08:03:12Z",
      "updated_at": "2025-05-20T14:29:41Z",
      "closed_at": "2025-05-20T14:29:41Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1555/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1555",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1555",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:08.028364",
      "comments": [
        {
          "author": "Kludex",
          "body": "I'm working on this already. Should be ready at some point this week.",
          "created_at": "2025-04-20T08:05:19Z"
        },
        {
          "author": "bigs",
          "body": "Hey @Kludex excited to see this coming! Was wondering if, while you were working on thinking options, if #840 might also be relevant as a model-specific option? Just throwing it into your consciousness!",
          "created_at": "2025-04-22T20:02:33Z"
        },
        {
          "author": "amiyapatanaik",
          "body": "A small example of how to achieve this will be very helpful. ",
          "created_at": "2025-05-06T15:52:17Z"
        },
        {
          "author": "tcrapts",
          "body": "@amiyapatanaik refer to the docs; https://ai.pydantic.dev/api/models/gemini/#pydantic_ai.models.gemini.GeminiModelSettings\n\nSomething like this should work:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel, ThinkingConfig\nfrom pydantic_ai.providers.google_gl",
          "created_at": "2025-05-07T09:30:14Z"
        },
        {
          "author": "d-zimmermann",
          "body": "I expected to give do something along the line of:\n\n```\n# Disable thinking (budget 0) and output; only works for GEMINI\nthinkingDisabledConfig = ThinkingConfig(\n    include_thoughts=False,\n    thinking_budget=0\n)\n\nmodel_settings=ModelSettings(    \n    thinking_config=thinkingDisabledConfig,\n    temp",
          "created_at": "2025-05-08T19:27:11Z"
        }
      ]
    },
    {
      "issue_number": 1167,
      "title": "Agents using Gemini models throw error if `result_type` pydantic model contains a `Field` with `examples` defined",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nIf the agent `result_type` is a pydantic model that contains `Fields` with `examples` defined, a `ModelHTTPError` is thrown.\n\n\nWhen running the code below, you get the following error:\n\n```\nERROR: Agent failed with exception: ModelHTTPError\nError message: status_code: 400, model_name: gemini-2.0-flash, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"examples\\\" at 'tools.function_declarations[0].parameters.properties[0].value': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools.function_declarations[0].parameters.properties[0].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"examples\\\" at 'tools.function_declarations[0].parameters.properties[0].value': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example Code\n\n```Python\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = [\n#   \"pydantic\",\n#   \"pydantic-ai\",\n#   \"google-cloud-aiplatform\",\n# ]\n# ///\n\nfrom typing import Annotated\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\n\n# Update this to your service account file path\nSERVICE_ACCOUNT_PATH = \"/path/to/your/service-account-file.json\"\n\n\n# Define a simple response model with one field that includes an example\nclass SimpleResponse(BaseModel):\n    \"\"\"A simple response model with one field that includes an example.\"\"\"\n\n    message: Annotated[\n        str,\n        Field(\n            description=\"A simple message field\",\n            examples=[\n                \"Hello, world!\",\n                \"This is an example message\",\n            ],\n        ),\n    ]\n\n\nasync def run_demo():\n    VERTEX_GEMINI_FLASH_20 = GeminiModel(\n        \"gemini-2.0-flash\",\n        provider=GoogleVertexProvider(service_account_file=SERVICE_ACCOUNT_PATH),\n    )\n\n    agent = Agent(\n        model=VERTEX_GEMINI_FLASH_20,\n        result_type=SimpleResponse,\n        system_prompt=\"You are a helpful assistant that responds with a simple message.\",\n    )\n\n    try:\n        result = await agent.run(\"Please give me a simple message\")\n        print(\"SUCCESS: Agent returned result:\", result)\n    except Exception as e:\n        print(f\"ERROR: Agent failed with exception: {type(e).__name__}\")\n        print(f\"Error message: {str(e)}\")\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run_demo())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12.9\npydantic-ai 0.0.41\n```",
      "state": "closed",
      "author": "barapa",
      "author_type": "User",
      "created_at": "2025-03-18T19:17:25Z",
      "updated_at": "2025-05-20T14:29:26Z",
      "closed_at": "2025-05-20T14:29:25Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1167/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1167",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1167",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:08.296430",
      "comments": [
        {
          "author": "Kludex",
          "body": "Hi @barapa,\n\nWhat's the behavior you'd expect? That's a limitation from the model API itself.",
          "created_at": "2025-03-19T16:46:47Z"
        },
        {
          "author": "barapa",
          "body": "A few thoughts...\n\nIt isn't always strictly necessary to use the model's built-in support for structured output. Before they added it (and still bc of gemini's pretty severe limitations), I often found myself just giving the model the json schema in the system prompt and telling it to return its ans",
          "created_at": "2025-03-19T17:02:57Z"
        },
        {
          "author": "dhimmel",
          "body": "> Or perhaps, for certain features, the framework helps you strip them out before supplying it to gemini?\n\n+1 for the option to prune jsonschema fields to just those supported for a given `provider:model`.",
          "created_at": "2025-05-07T20:06:05Z"
        },
        {
          "author": "Kludex",
          "body": "This was solved at some point last month. We now remove the `examples` field from the JSON schema for Gemini.",
          "created_at": "2025-05-20T14:29:25Z"
        }
      ]
    },
    {
      "issue_number": 1675,
      "title": "Remote MCP Server is not working",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI am trying to use an SSE MCP Server which is already working in Cursor, but doesn't work with Pydantic MCP Client...\n\nHere the logs:\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.5-pro, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[4].parameters.properties[0].value.properties[3].value': Proto field is not repeating, cannot start list.\\nInvalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[4].parameters.properties[0].value.properties[4].value': Proto field is not repeating, cannot start list.\\nInvalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[8].parameters.properties[0].value.properties[1].value': Proto field is not repeating, cannot start list.\\nInvalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[8].parameters.properties[0].value.properties[2].value': Proto field is not repeating, cannot start list.\\nInvalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[8].parameters.properties[0].value.properties[4].value': Proto field is not repeating, cannot start list.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools.function_declarations[4].parameters.properties[0].value.properties[3].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[4].parameters.properties[0].value.properties[3].value': Proto field is not repeating, cannot start list.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[4].parameters.properties[0].value.properties[4].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[4].parameters.properties[0].value.properties[4].value': Proto field is not repeating, cannot start list.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[8].parameters.properties[0].value.properties[1].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[8].parameters.properties[0].value.properties[1].value': Proto field is not repeating, cannot start list.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[8].parameters.properties[0].value.properties[2].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[8].parameters.properties[0].value.properties[2].value': Proto field is not repeating, cannot start list.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[8].parameters.properties[0].value.properties[4].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"type\\\" at 'tools.function_declarations[8].parameters.properties[0].value.properties[4].value': Proto field is not repeating, cannot start list.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nSeems that the tools are not discovered... or something like that. Is there any method to list the tools discovered after initializing an MCPServer?\n\n### Example Code\n\n```Python\n#!/usr/bin/env python\nimport asyncio\nimport logging\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP # Corrected import path\n\nlogging.basicConfig(level=logging.INFO)\n\nlogger = logging.getLogger(\"calendar_test\")\n\nCALENDAR_SERVER_URL = \"https://mcp.composio.dev/composio/server/XXXXX\"\n\n\nasync def test_pydantic_mcp():\n    \"\"\"Test Pydantic AI MCP integration with sanitized server.\"\"\"\n    logger.info(\"Testing Pydantic AI MCP integration\")\n    \n    mcp_server = MCPServerHTTP(url=CALENDAR_SERVER_URL)\n\n    agent = Agent(\n        \"google-gla:gemini-2.5-pro\",\n        system_prompt=\"You are a helpful assistant that can answer questions and help with tasks.\",\n        mcp_servers=[mcp_server]\n    )\n    \n    user_query = \"List the calendar events of today.\"\n    \n    async with agent.run_mcp_servers():\n        logger.info(f\"Executing query: {user_query}\")\n        result = await agent.run(user_query)\n        print(result)\n        logger.info(f\"Got result: {result.output}\")\n\nasync def main():\n    await test_pydantic_mcp()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai[mcp,logfire]==0.1.10\n```",
      "state": "closed",
      "author": "filopedraz",
      "author_type": "User",
      "created_at": "2025-05-09T08:43:00Z",
      "updated_at": "2025-05-20T14:23:47Z",
      "closed_at": "2025-05-20T14:23:39Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1675/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1675",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1675",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:08.530273",
      "comments": [
        {
          "author": "DouweM",
          "body": "@filopedraz Based on the error message, it looks like the tools are being discovered and sent to the LLM, it just doesn't like the JSON schema definitions of some of the tools' arguments.\n\nCan you please add Logfire ([ai.pydantic.dev/logfire](https://ai.pydantic.dev/logfire/)) and share the JSON bod",
          "created_at": "2025-05-12T14:18:50Z"
        },
        {
          "author": "Kludex",
          "body": "This is a problem on the JSON schema, not on the MCP server.\n\nPlease share what @DouweM asked. 🙏 ",
          "created_at": "2025-05-13T11:49:15Z"
        }
      ]
    },
    {
      "issue_number": 1673,
      "title": "How do I stream tool calls that have nested agent streaming responses?",
      "body": "### Question\n\nHi\n\nThe current high level setup:\nI have an agent that has access to a tool called `generate_curriculum` and inside of said tool there are 2 agents that stream structured outputs.\n\nThe top level agent that calls the `generate_curriculum` tool is also process all the chunks to a data stream protocol helper to convert the agent's graph nodes into [this format](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol).\n\nHere is the code for the top level agent.\n\n```py\n   async def chat_stream(\n        self,\n        prompt: str,\n        message_history: list,\n        **kwargs: Any,\n    ) -> AsyncGenerator[bytes, Any]:\n        \"\"\"Stream chat interaction with the agent asynchronously\"\"\"\n\n        async def stream_messages():\n            # Build the input message list\n            input_messages = [prompt]\n\n            deps = kwargs.get(\"deps\", None)\n\n            async with self.iter(\n                input_messages, message_history=message_history, deps=deps\n            ) as agent_run:\n                # Handle intermediate stream responses\n                async for node in agent_run:\n                    async for chunk in to_data_stream_protocol(node, agent_run):\n                        yield chunk\n\n                # After stream completes, get final messages and return with last chunk\n                final_messages = agent_run.result.new_messages_json()\n                \n                # Send final message with new messages for history\n                yield chunk, final_messages\n\n        return stream_messages()\n```\n\nHere is the `generate_curriculum` tool:\n```py\nasync def generate_curriculum(ctx: RunContext[LearningSession], complete_learning_goal_summary: str) -> AsyncGenerator[bytes, Any]:\n    \"\"\"\n    This tool is used to generate a learning roadmap/curriculum for the user based on a comprehensive indepth learning goal that has been discovered through an interview process.\n\n    Underneath the hood, this tool will use the TheoryManagerAgent to generate a title and description for the session that reflects the refined learning goal.\n    And it will also use the TheoryManagerAgent to agentic research and generat a comprehensive roadmap.\n    \"\"\"\n    import logging\n    logger = logging.getLogger(\"app\")\n\n    # Update session title and description now that we have a refined learning goal\n    theory_manager_agent = TheoryManagerAgent(\n        model_name=\"openai:gpt-4.1-mini\"\n    )\n\n    title_and_description = await theory_manager_agent.generate_title_and_description(session_data=StudySessionCreate(\n        preferences=StudyPreferences(\n            study_goal=complete_learning_goal_summary,\n        )\n    ))\n\n    learning_session = ctx.deps.session\n\n    learning_session.title = title_and_description.title\n    learning_session.description = title_and_description.description\n    learning_session.preferences = StudyPreferences(\n        study_goal=complete_learning_goal_summary,\n    )\n\n    await learning_session.save()\n    \n    state = CurriculumState(learning_goal=complete_learning_goal_summary)\n    state.initialize_agents()\n\n    async def stream_curriculum(state):\n        async for chunk in generate_topics(state):\n            logger.info(chunk)\n            if not chunk[0] and chunk[1] != []:\n                yield chunk[1]\n            else:\n                state.topics = chunk[1]\n                async for chunk in parallel_concept_generation(state.topics, state):\n                    if chunk[0] != \"state\":\n                        logger.info(chunk)\n                        yield chunk\n                    else:\n                        state = chunk[1]\n\n        roadmap = roadmap_assembly(state)\n\n        mini_curriculum = MiniCurriculum(\n            study_goal=complete_learning_goal_summary,\n            roadmap=roadmap\n        )\n\n        yield mini_curriculum\n\n        learning_session.mini_curriculum = mini_curriculum\n        await learning_session.save()\n    return stream_curriculum(state)\n```\nFor the sake of reducing this question size, I've omitted the actual generation functions but just know that the agents inside them are being streamed like this\n```py\nasync with agent.run_stream(user_prompt=prompt, output_type=List[TopicNodeBase], deps=SearchData()) as result:\n        async for message, last in result.stream_structured(debounce_by=0.01):  \n            try:\n                chunk = await result.validate_structured_output(  \n                    message,\n                    allow_partial=not last,\n                )\n            except ValidationError:\n                continue\n            yield last, chunk\n```\n\nThe problem and current results:\nThe data is streaming fine in the format I want. I'm basically forcing all the chunks to be formatted to this format:\n`a:{{toolCallId:id, result:{chunk.json()}}}`\n\nAll the chunks are serializing fine and it shows correctly in my network tab but once the stream ends I get this error.\n\n```\npydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'async_generator'>\n```\n\nIt seems to me that the issue is because PydanticAI is trying to handle the final tool result as if it is one object and I guess it doesn't naturally handle AsyncGenerators and quite frankly I can't really think of a way to yield the intermediate chunks while also satisfying PydanticAI's need for the fully serializable object that it intends to parse/validate. \n\nThe overall goal of this is to stream the output of each of the agents so I can display the generation process in my frontend components. Previously my functionality was a synchronous workflow made in a Pydantic Graph but I realized I can't stream with a graph. \n\nI had also tried taking the logic out of each graph node's run function and manually stepping through the graph nodes at a high level and then using streaming versions of said logic in between. That also didn't work because the inner tool calls made by the agents in that graph didn't share the same run context as the higher level agent that called the tool which runs the graph. (The graph agents weren't updating the overall context's message history so it made the openai client fail on subsequent requests).\n\nThats why I'm trying a more flatten version that exists in the tool call itself without it having to be in a graph (which wasn't that necessary in the first place but it was organized nicely and allowed me to easily introduce quality check logic in between)\n\n\nFull Traceback for the Async Generator serialization exception:\n```\nERROR:    Exception in ASGI application\n  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 263, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    |     result = await app(  # type: ignore[func-returns-value]\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    |     return await self.app(scope, receive, send)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/applications.py\", line 112, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    |     raise exc\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 183, in __call__\n    |     raise app_exc\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/opentelemetry/instrumentation/asgi/__init__.py\", line 743, in __call__\n    |     await self.app(scope, otel_receive, otel_send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n    |     await response(scope, receive, send)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 262, in __call__\n    |     with collapse_excgroups():\n    |          ^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n    |     self.gen.throw(value)\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 266, in wrap\n    |     await func()\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 246, in stream_response\n    |     async for chunk in self.body_iterator:\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/app/services/study_session_service.py\", line 684, in interview_chat_stream\n    |     async for chunk in await stream:\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/app/services/learning/base_agent.py\", line 113, in stream_messages\n    |     async for chunk in to_data_stream_protocol(node, agent_run):\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/app/services/learning/agent_utils.py\", line 104, in to_data_stream_protocol\n    |     async with node.stream(run.ctx) as request_stream:\n    |                ^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 278, in stream\n    |     async with self._stream(ctx) as streamed_response:\n    |                ^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 301, in _stream\n    |     async with ctx.deps.model.request_stream(\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/instrumented.py\", line 141, in request_stream\n    |     async with super().request_stream(\n    |                ^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/wrapper.py\", line 37, in request_stream\n    |     async with self.wrapped.request_stream(messages, model_settings, model_request_parameters) as response_stream:\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 210, in request_stream\n    |     response = await self._completions_create(\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 264, in _completions_create\n    |     openai_messages = await self._map_messages(messages)\n    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 331, in _map_messages\n    |     async for item in self._map_user_message(message):\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 394, in _map_user_message\n    |     content=part.model_response_str(),\n    |             ^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/messages.py\", line 372, in model_response_str\n    |     return tool_return_ta.dump_json(self.content).decode()\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 631, in dump_json\n    |     return self.serializer.to_json(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^\n    | pydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'async_generator'>\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 183, in __call__\n    raise app_exc\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/opentelemetry/instrumentation/asgi/__init__.py\", line 743, in __call__\n    await self.app(scope, otel_receive, otel_send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n    await response(scope, receive, send)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 262, in __call__\n    with collapse_excgroups():\n         ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 266, in wrap\n    await func()\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 246, in stream_response\n    async for chunk in self.body_iterator:\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/app/services/study_session_service.py\", line 684, in interview_chat_stream\n    async for chunk in await stream:\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/app/services/learning/base_agent.py\", line 113, in stream_messages\n    async for chunk in to_data_stream_protocol(node, agent_run):\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/app/services/learning/agent_utils.py\", line 104, in to_data_stream_protocol\n    async with node.stream(run.ctx) as request_stream:\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 278, in stream\n    async with self._stream(ctx) as streamed_response:\n               ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 301, in _stream\n    async with ctx.deps.model.request_stream(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/instrumented.py\", line 141, in request_stream\n    async with super().request_stream(\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/wrapper.py\", line 37, in request_stream\n    async with self.wrapped.request_stream(messages, model_settings, model_request_parameters) as response_stream:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 210, in request_stream\n    response = await self._completions_create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 264, in _completions_create\n    openai_messages = await self._map_messages(messages)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 331, in _map_messages\n    async for item in self._map_user_message(message):\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 394, in _map_user_message\n    content=part.model_response_str(),\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic_ai/messages.py\", line 372, in model_response_str\n    return tool_return_ta.dump_json(self.content).decode()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/asher/Desktop/AcquiredIntelligence/Zettel/backend/study-session-service/.venv/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 631, in dump_json\n    return self.serializer.to_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'async_generator'>\n```\n\n\n\n\n\n### Additional Context\n\nPydanticAI Version = 0.1.6\nPython Version = 3.12.9\nI don't think there is anymore relevant information.",
      "state": "closed",
      "author": "asher-aqi",
      "author_type": "User",
      "created_at": "2025-05-09T00:12:17Z",
      "updated_at": "2025-05-20T14:00:39Z",
      "closed_at": "2025-05-20T14:00:38Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1673/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1673",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1673",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:08.783568",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "@dmontagu will take a look when he gets a chance.",
          "created_at": "2025-05-09T17:18:59Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-17T14:00:29Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-20T14:00:38Z"
        }
      ]
    },
    {
      "issue_number": 1758,
      "title": "mcp-run-python 'unable to load prepare_code.py'",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI've been wondering why i can no longer execute code in the sandbox running into the error `error: Uncaught (in promise) Error: import.meta.dirname is not defined, unable to load prepare_code.py`. After trying several things i noticed that the code execution works just fine on v0.0.11 of mcp-run-python but fails on the latest version (0.0.12)\n\n```bash\n11:26:07 ➜ deno run -N -R=node_modules -W=node_modules --node-modules-dir=auto --allow-env jsr:@pydantic/mcp-run-python warmup\nRunning warmup script for MCP Run Python version 0.0.12...\ndebug: loadPackage: Loading annotated-types, micropip, packaging, pydantic, pydantic_core, typing-extensions\ndebug: loadPackage: Loaded annotated-types, micropip, packaging, pydantic, pydantic_core, typing-extensions\nerror: Uncaught (in promise) Error: import.meta.dirname is not defined, unable to load prepare_code.py\n    throw new Error('import.meta.dirname is not defined, unable to load prepare_code.py')\n          ^\n    at loadPrepareCode (https://jsr.io/@pydantic/mcp-run-python/0.0.12/src/runCode.ts:181:11)\n    at runCode (https://jsr.io/@pydantic/mcp-run-python/0.0.12/src/runCode.ts:55:35)\n    at eventLoopTick (ext:core/01_core.js:178:7)\n    at async warmup (https://jsr.io/@pydantic/mcp-run-python/0.0.12/src/main.ts:184:18)\n    at async main (https://jsr.io/@pydantic/mcp-run-python/0.0.12/src/main.ts:28:5)\n    at async https://jsr.io/@pydantic/mcp-run-python/0.0.12/src/main.ts:208:1\n```\n\n```bash\n11:26:13 ✗ deno run -N -R=node_modules -W=node_modules --node-modules-dir=auto --allow-env jsr:@pydantic/mcp-run-python@0.0.11 warmup\nRunning warmup script for MCP Run Python version 0.0.11...\ndebug: loadPackage: Loading annotated-types, micropip, packaging, pydantic, pydantic_core, typing-extensions\ndebug: loadPackage: Loaded annotated-types, micropip, packaging, pydantic, pydantic_core, typing-extensions\ndebug: loadPackage: Loading numpy\ndebug: loadPackage: Loaded numpy\ninfo: numpy array: [1 2 3]\nTool return value:\n<status>success</status>\n<dependencies>[\"numpy\"]</dependencies>\n<output>\nnumpy array: [1 2 3]\n</output>\n<return_value>\n\n[\n  1,\n  2,\n  3\n]\n</return_value>\n\nwarmup successful 🎉\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython -V\nPython 3.11.11\n\ndeno -V\ndeno 2.3.3\n\nuv pip list | grep pydantic-ai                                                                                     \npydantic-ai                   0.2.4\npydantic-ai-slim              0.2.4\n```",
      "state": "closed",
      "author": "akanz1",
      "author_type": "User",
      "created_at": "2025-05-19T09:41:07Z",
      "updated_at": "2025-05-20T10:39:46Z",
      "closed_at": "2025-05-20T10:39:46Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1758/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1758",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1758",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:09.054966",
      "comments": [
        {
          "author": "DouweM",
          "body": "@akanz1 Can you please try the latest version 0.0.13 (https://jsr.io/@pydantic/mcp-run-python)? We made some changes related to `prepare_env.py` in https://github.com/pydantic/pydantic-ai/pull/1461, which shipped in 0.0.13.",
          "created_at": "2025-05-19T22:34:57Z"
        },
        {
          "author": "akanz1",
          "body": "Thank you @DouweM this works and solves the issue for me. \n\nSo the latest version is not automatically used when not explicitly specifying it?\n\nAfter downloading 0.0.13 the command without a version specifier also works `deno run -N -R=node_modules -W=node_modules --node-modules-dir=auto --allow-env",
          "created_at": "2025-05-20T07:13:09Z"
        }
      ]
    },
    {
      "issue_number": 1746,
      "title": "OpenAIModel fails with OpenRouter Gemini: missing \"choices\" and \"created\" keys",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n### Summary\n\nWhen using `OpenAIModel` with OpenRouter and the model `google/gemini-2.0-flash-exp:free`, the agent fails with:\n\nTypeError: 'NoneType' object is not subscriptable\n\n\nThis happens because the response returned by OpenRouter for Gemini does not include the `choices` or `created` keys expected by `_process_response()`.\n\n---\n\n### Steps to Reproduce\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIModel(\n    'google/gemini-2.0-flash-exp:free',\n    provider=OpenAIProvider(\n        base_url='https://openrouter.ai/api/v1',\n        api_key='sk-or-v1-...',\n    ),\n)\n\n\nagent = Agent(model=model, system_prompt=\"Be helpful.\")\nresult = agent.run_sync(\"Tell me a joke.\")\n```\n\n### Observed Error\n- response.created is None, causing datetime.fromtimestamp() to fail\n- response.choices is None, causing choices[0] to throw\n\n### Suggestion\n- Add a fallback timestamp = datetime.now() when created is missing\n- Handle cases where choices is missing — or validate the response format before attempting to parse\n- Possibly support custom response parsers for models that aren't 100% OpenAI-compatible\n\n### Why it matters\nOpenRouter is a growing ecosystem for multimodel access, and many of the available models (Gemini, Cohere, etc.) do not fully follow the OpenAI ChatCompletion schema. Supporting them would improve compatibility.\n\nLet me know if you'd like me to submit a PR or share sample responses.\n\nThanks for the awesome tool!\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12\npydantic>=2.11.4\npydantic-ai-slim[openai]>=0.2.4\npydantic-settings>=2.9.1\n```",
      "state": "open",
      "author": "gkeb",
      "author_type": "User",
      "created_at": "2025-05-16T10:58:51Z",
      "updated_at": "2025-05-20T10:26:27Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1746/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1746",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1746",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:09.328768",
      "comments": [
        {
          "author": "wdhorton",
          "body": "I was able to reproduce this. It only happens sometimes--since OpenRouter has two providers for this model, I'm guessing it happens for one of the providers but not the other. ",
          "created_at": "2025-05-19T15:48:54Z"
        },
        {
          "author": "gkeb",
          "body": "Probably yes. If I set up OpenAI or DeepSeek it works. The problem practically only occurs with Google models. In any case, I only noticed it with them.",
          "created_at": "2025-05-19T20:13:55Z"
        },
        {
          "author": "Kludex",
          "body": "I can't reproduce it. Also, trying many times is hard because there's a lot of 429.",
          "created_at": "2025-05-20T10:16:30Z"
        },
        {
          "author": "Kludex",
          "body": "I've created the `OpenRouterProvider` to make it easier to work with it, also, you can now use `Agent('openrouter:<model_name>')` to use `OpenRouter` - Assuming you have the `OPENROUTER_API_KEY` set.",
          "created_at": "2025-05-20T10:25:34Z"
        },
        {
          "author": "Kludex",
          "body": "See this: https://github.com/pydantic/pydantic-ai/pull/1778/files#r2097604759.\n\nIt didn't fail... How can I reproduce it?",
          "created_at": "2025-05-20T10:26:26Z"
        }
      ]
    },
    {
      "issue_number": 1757,
      "title": "Error while connecting with Gradio MCP",
      "body": "### Question\n\nWhile to trying to connect with Gradio MCP server [code](https://www.gradio.app/guides/building-mcp-server-with-gradio) Getting below error. Even though atleast in Gradio UI tool work as it suppose to be.\n```\nError in post_writer: Client error '404 Not Found' for url 'http://127.0.0.1:7860/gradio_api/mcp/gradio_api/mcp/messages/?session_id=ed478fc640e247fbbb6b171c58de322b'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n```\nPydantic-AI code used\n```\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP\nimport asyncio \n\nserver = MCPServerHTTP(url='http://127.0.0.1:7860/gradio_api/mcp/sse')  \nagent = Agent(model=model, mcp_servers=[server])  \n\n\nasync def main():\n    async with agent.run_mcp_servers():  \n        result = await agent.run('Count word for Hello')\n    print(result.output)\n\nasyncio.run(main())\n```\n\nIs is some handshake error?\n\nYour Pydantic AI version - Latest\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "kauabh",
      "author_type": "User",
      "created_at": "2025-05-19T07:30:09Z",
      "updated_at": "2025-05-20T08:11:44Z",
      "closed_at": "2025-05-19T22:30:54Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1757/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1757",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1757",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:09.577997",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1282\" (100% similar)",
          "created_at": "2025-05-19T07:40:08Z"
        },
        {
          "author": "XQZZK",
          "body": "请问你解决了吗？我也遇到了同样的问题",
          "created_at": "2025-05-19T15:54:53Z"
        },
        {
          "author": "DouweM",
          "body": "@kauabh It looks like the URL is being built incorrectly: `http://127.0.0.1:7860/gradio_api/mcp/gradio_api/mcp/messages/?session_id=ed478fc640e247fbbb6b171c58de322b` should be `http://127.0.0.1:7860/gradio_api/mcp/messages/?session_id=ed478fc640e247fbbb6b171c58de322b`\n\nPydanticAI doesn't do its own ",
          "created_at": "2025-05-19T22:30:54Z"
        },
        {
          "author": "kauabh",
          "body": "If anyone stumble upon this, Raised it on gradio https://github.com/gradio-app/gradio/issues/11225",
          "created_at": "2025-05-20T08:11:43Z"
        }
      ]
    },
    {
      "issue_number": 1731,
      "title": "Support pydantic validators for tool inputs",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen defining a tool that accepts an input with custom field validators the agent instead of retrying to fix the issue will just throw an exception\n\nFor normal pydantic validation issues the agent will try to fix his inputs.\n\n### Example Code\n\n```Python\nimport os\nfrom typing import cast\n\nimport logfire\nfrom pydantic import BaseModel, field_validator\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models import KnownModelName\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\n\n\nclass Location(BaseModel):\n    city: str\n    country: str\n\n    @field_validator('city')\n    @classmethod\n    def validate_city(cls, v):\n        # Using a validator that always raises a validation error\n        raise ValueError(\"This model will always raise a validation error\")\n\nmodel = cast(KnownModelName, os.getenv('PYDANTIC_AI_MODEL', 'openai:gpt-4o-mini'))\nprint(f'Using model: {model}')\nagent = Agent(model, system_prompt=\"You must use the tool to answer the question\")\n\n@agent.tool_plain\ndef show_me_city_coordinates(city: Location):\n    \"\"\"Retrieves the coordinates of a city\"\"\"\n    return \"Location 2\"\n\nif __name__ == '__main__':\n    result = agent.run_sync('show me the coordinates of london')\n    print(result.data)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\naiolimiter==1.1.0\naiorwlock==1.5.0\nalembic==1.15.1\naltair==5.5.0\nannotated-types==0.7.0\nanthropic==0.49.0\nanyio==4.9.0\napispec==6.8.1\nAPScheduler==3.11.0\nargcomplete==3.6.1\nasgiref==3.8.1\nasyncio-atexit==1.0.1\nasyncpg==0.30.0\nattrs==22.2.0\nazure-core==1.32.0\nazure-identity==1.17.1\nbabel==2.17.0\nbackoff==2.2.1\nbcrypt==4.3.0\nbeautifulsoup4==4.12.2\nblinker==1.9.0\nboto3==1.37.18\nbotocore==1.37.18\nbuild==1.2.2.post1\nbytecode==0.16.1\nCacheControl==0.13.1\ncachetools==5.5.2\ncertifi==2025.1.31\ncffi==1.17.1\ncfgv==3.4.0\nchardet==5.2.0\ncharset-normalizer==3.4.1\nchroma-hnswlib==0.7.6\nchromadb==0.6.3\nclick==8.1.8\ncohere==5.14.0\ncolorama==0.4.6\ncoloredlogs==15.0.1\ncourlan==1.3.2\ncoverage==7.7.1\ncryptography==41.0.2\ndateparser==1.2.1\ndb-dtypes==1.4.2\nddtrace==3.5.1\nDeprecated==1.2.18\ndirty-equals==0.9.0\ndistlib==0.3.9\ndistro==1.9.0\ndocstring_parser==0.16\ndurationpy==0.9\nenvier==0.6.1\net_xmlfile==2.0.0\neval_type_backport==0.2.2\nexecuting==2.2.0\nfactory_boy==3.3.3\nFaker==19.3.1\nfakeredis==2.21.3\nfastapi==0.115.12\nfastavro==1.10.0\nfasteners==0.17.3\nfilelock==3.12.2\nflasgger==0.9.7.1\nFlask==3.1.0\nflask-apispec==0.11.4\nFlask-APScheduler==1.13.1\nFlask-Cors==4.0.1\nFlask-Executor==1.0.0\nFlask-Login==0.6.3\nFlask-Migrate==4.1.0\nflask-sock==0.7.0\nFlask-SQLAlchemy==3.1.1\nflatbuffers==25.2.10\nfsspec==2025.3.0\ngevent==24.11.1\ngitdb==4.0.12\nGitPython==3.1.44\ngoogle-ai-generativelanguage==0.6.15\ngoogle-api-core==2.25.0rc0\ngoogle-api-python-client==2.168.0\ngoogle-auth==2.38.0\ngoogle-auth-httplib2==0.2.0\ngoogle-cloud-aiplatform==1.71.1\ngoogle-cloud-bigquery==3.31.0\ngoogle-cloud-core==2.4.3\ngoogle-cloud-resource-manager==1.14.2\ngoogle-cloud-storage==2.19.0\ngoogle-crc32c==1.7.1\ngoogle-genai==1.7.0\ngoogle-generativeai==0.8.5\ngoogle-resumable-media==2.7.2\ngoogleapis-common-protos==1.69.2\ngreenlet==3.1.1\ngriffe==1.6.2\ngroq==0.20.0\ngrpc-google-iam-v1==0.14.2\ngrpcio==1.67.1\ngrpcio-status==1.71.0\ngrpcio-tools==1.67.1\ngunicorn==23.0.0\nh11==0.14.0\nhishel==0.1.1\nhtml2text==2024.2.26\nhtmldate==1.9.3\nhttpcore==1.0.7\nhttpdbg==1.2.1\nhttplib2==0.22.0\nhttptools==0.6.4\nhttpx==0.28.1\nhttpx-cache==0.13.0\nhttpx-sse==0.4.0\nhuggingface-hub==0.29.3\nhumanfriendly==10.0\nidentify==2.6.9\nidna==3.10\nimportlib_metadata==8.6.1\nimportlib_resources==6.5.2\niniconfig==2.1.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njiter==0.9.0\njmespath==1.0.1\njsonpath-ng==1.6.1\njsonschema==4.23.0\njsonschema-specifications==2024.10.1\njusText==3.0.2\nkaleido==0.2.1\nkubernetes==32.0.1\nlegacy-cgi==2.6.3\nlogfire==3.12.0\nlogfire-api==3.12.0\nlupa==2.1\nlxml==5.3.2\nlxml_html_clean==0.4.2\nMako==1.3.9\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmarshmallow==3.22.0\nmarshmallow-sqlalchemy==1.4.2\nmarshmallow_dataclass==8.7.1\nmcp==1.7.1\nmdurl==0.1.2\nmistralai==1.6.0\nmistune==3.1.3\nmmh3==5.1.0\nmock==4.0.3\nmonotonic==1.6\nmpmath==1.3.0\nmsal==1.32.0\nmsal-extensions==1.3.1\nmsgpack==1.1.0\nmypy==0.960\nmypy-extensions==1.0.0\nnarwhals==1.32.0\nnodeenv==1.9.1\nnumpy==2.2.4\noauthlib==3.2.2\nonnxruntime==1.21.1\nopenai==1.77.0\nopenpyxl==3.0.10\nopentelemetry-api==1.31.1\nopentelemetry-exporter-otlp-proto-common==1.31.1\nopentelemetry-exporter-otlp-proto-grpc==1.32.1\nopentelemetry-exporter-otlp-proto-http==1.31.1\nopentelemetry-instrumentation==0.52b1\nopentelemetry-instrumentation-asgi==0.53b1\nopentelemetry-instrumentation-fastapi==0.53b1\nopentelemetry-proto==1.31.1\nopentelemetry-sdk==1.31.1\nopentelemetry-semantic-conventions==0.52b1\nopentelemetry-util-http==0.53b1\norjson==3.10.16\noverrides==7.7.0\npackaging==24.2\npandas==2.2.3\npgvector==0.4.1\nphonenumbers==8.12.54\npillow==11.1.0\npip==25.0.1\npip-chill==1.0.1\nplatformdirs==4.3.7\nplotly==6.0.1\npluggy==1.5.0\nply==3.11\nposthog==4.0.0\npre-commit==2.19.0\nprompt_toolkit==3.0.50\nproto-plus==1.26.1\nprotobuf==5.28.1\npsycopg==3.2.7\npsycopg-binary==3.2.7\npsycopg2-binary==2.9.10\npy-cpuinfo==9.0.0\npyarrow==19.0.1\npyasn1==0.6.1\npyasn1_modules==0.4.1\npycparser==2.22\npydantic==2.11.4\npydantic-ai==0.2.0\npydantic-ai-slim==0.2.0\npydantic-evals==0.2.0\npydantic-graph==0.2.0\npydantic-settings==2.8.1\npydantic_core==2.33.2\npydeck==0.9.1\nPygments==2.19.1\nPyJWT==2.10.1\npyotp==2.6.0\npyparsing==3.2.3\npypdf==3.9.0\nPyPDF2==3.0.1\nPyPika==0.48.9\npyproject_hooks==1.2.0\npytest==8.3.5\npytest-asyncio==0.26.0\npytest-benchmark==5.1.0\npytest-cov==6.1.1\npytest-httpx==0.35.0\npytest-mock==3.14.0\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npython-http-client==3.3.7\npython-multipart==0.0.20\npytz==2025.1\nPyYAML==6.0.2\nredis==5.0.3\nreferencing==0.36.2\nregex==2024.11.6\nrequests==2.32.3\nrequests-oauthlib==2.0.0\nrich==13.9.4\nrpds-py==0.23.1\nrsa==4.9\nruff==0.1.7\ns3transfer==0.11.4\nsendgrid==6.9.7\nsentry-sdk==1.15.0\nserpapi==0.1.5\nsetuptools==77.0.3\nshapely==2.1.0\nshellingham==1.5.4\nsimple-websocket==1.1.0\nsix==1.17.0\nslack_sdk==3.27.1\nsmart-open==6.4.0\nsmmap==5.0.2\nsniffio==1.3.1\nsortedcontainers==2.4.0\nsoupsieve==2.6\nSQLAlchemy==2.0.40\nSQLAlchemy-Utils==0.39.0\nsqlparse==0.5.3\nsse-starlette==2.2.1\nstarkbank-ecdsa==2.2.0\nstarlette==0.46.1\nstreamlit==1.43.2\nsympy==1.14.0\ntabulate==0.9.0\ntenacity==9.0.0\ntld==0.13\ntokenizers==0.21.1\ntoml==0.10.2\ntornado==6.4.2\ntqdm==4.67.1\ntrafilatura==2.0.0\ntwilio==7.9.2\ntypeguard==4.4.2\ntyper==0.15.2\ntypes-requests==2.32.0.20250306\ntyping-inspect==0.9.0\ntyping-inspection==0.4.0\ntyping_extensions==4.12.2\ntzdata==2025.2\ntzlocal==5.3.1\nua-parser==1.0.1\nua-parser-builtins==0.18.0.post1\nuritemplate==4.1.1\nurllib3==2.3.0\nuser-agents==2.2.0\nuvicorn==0.34.0\nuvloop==0.21.0\nvanna==0.7.9\nvertexai==1.71.1\nvirtualenv==20.29.3\nwatchfiles==1.0.5\nwcwidth==0.2.13\nwebargs==8.6.0\nwebsocket-client==1.8.0\nwebsockets==15.0.1\nWerkzeug==3.1.3\nwrapt==1.17.2\nwsproto==1.2.0\nxmltodict==0.14.2\nzipp==3.21.0\nzope.event==5.0\nzope.interface==7.2\n```",
      "state": "closed",
      "author": "barp",
      "author_type": "User",
      "created_at": "2025-05-15T00:42:43Z",
      "updated_at": "2025-05-20T04:53:24Z",
      "closed_at": "2025-05-19T21:45:19Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1731/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1731",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1731",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:09.848635",
      "comments": [
        {
          "author": "DouweM",
          "body": "@barp I just tried your example code, and it seems to work correctly:\n\n<img width=\"700\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/53703afb-a519-4d4f-b63d-f04e563604bb\" />\n\nThe error is passed to the model which gets to try again (one time by default). But since the error message di",
          "created_at": "2025-05-19T21:45:19Z"
        },
        {
          "author": "barp",
          "body": "Yeah after looking on it again it seems to work as expected.\nI will check my specific case again.",
          "created_at": "2025-05-20T04:53:23Z"
        }
      ]
    },
    {
      "issue_number": 1660,
      "title": "Evaluators: Return LLMJudge score",
      "body": "### Description\n\n[Currently, LLMJudge only returns a boolean of pass/fail for its evaluation](https://github.com/pydantic/pydantic-ai/blob/dd22595464ea430d9fcea388e506bf0a1697a9a4/pydantic_evals/pydantic_evals/evaluators/common.py#L180).\nHowever, the [GradingOutput](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py#L22) already returns a score.\nCan LLMJudge return the score as well?\nI'm not sure of the best way to do this:\n* It doesn't look like we should edit EvaluationReason to return both the bool and the float score - the EvaluationScalar makes sense.\n* Instead, could we include EvaluationScalarType or something as input to LLMJudge?\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "jhostyk",
      "author_type": "User",
      "created_at": "2025-05-07T21:14:11Z",
      "updated_at": "2025-05-19T23:41:18Z",
      "closed_at": "2025-05-19T23:41:18Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1660/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1660",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1660",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:10.147149",
      "comments": [
        {
          "author": "DouweM",
          "body": "@jhostyk I think it would make sense to add an attribute on `LLMJudge` to allow switching from using the `pass` boolean to the `score` float. E.g. `evaluators=[LLMJudge(rubric=\"...\", score=True]`. Would you be up for submitting a PR to that effect? We may still change the attribute name (suggestions",
          "created_at": "2025-05-08T10:57:16Z"
        },
        {
          "author": "dmontagu",
          "body": "Closed by #1725 ",
          "created_at": "2025-05-19T23:41:18Z"
        }
      ]
    },
    {
      "issue_number": 1769,
      "title": "New Common Tool: Batch",
      "body": "### Description\n\nDue to some limitations(?) with Claude 3.7 Sonnet, it is much less likely to make parallel tool calls. \n(See here in the [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook/blob/eaacd9cddf70b0663a57895c75a7fd45b1e2ef9a/tool_use/parallel_tools_claude_3_7_sonnet.ipynb))\n\nIt would be helpful for there to be a built-in tool that would make this easy to implement.\n\nSomething along the lines of:\n\n```py\n\nbatch = BatchTool()\n\nagent = Agent( ..., tools = [batch])\n\n@batch.tool\nasync def frobnicate(ctx: RunContext[...], foo: str): ...\n\n@batch.tool_plain\nasync def encabulate(bar: int) : ...\n\n```\n\nThis implicitly creates a single tool with an Input schema similar to:\n\n```py\n@dataclass\nclass Invocation[Name: LiteralString, Args]:\n    name: Name\n    args: Args\n\n@dataclass \nclass FooArgs:\n    foo: str\n\n@dataclass\nclass BarArgs:\n    bar: Int\n\n@agent.tool\ndef batch(ctx: RunContext[...], invocations: list[Annotated[\n    Invocation[Literal['foo'], FooArgs] | Invocation[Literal['bar'], BarArgs],\n    Discriminant('name'),\n]]) -> list[...]:\n   ...\n```\n\n\n\n### References\n\n_No response_",
      "state": "open",
      "author": "chasewalden",
      "author_type": "User",
      "created_at": "2025-05-19T19:37:56Z",
      "updated_at": "2025-05-19T23:04:42Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1769/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1769",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1769",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:10.357864",
      "comments": [
        {
          "author": "chasewalden",
          "body": "I guess an alternative is an option on the Agent constructor / Tool constructor / tool decorator to use batch tool calling, which would automatically create this tool under the hood. \n\nI.e. \n```py\nagent = Agent(..., batch_tools = True) # all tools now merged into a single tool called batch\n\n# or \n\n@",
          "created_at": "2025-05-19T19:42:18Z"
        },
        {
          "author": "DouweM",
          "body": "I like this as something PydanticAI could do automatically for models it knows are bad at parallel tool calling, similar to how in https://github.com/pydantic/pydantic-ai/pull/1628 we're going to be supporting different output modes (e.g. `format=json_schema` in addition to the current `final_result",
          "created_at": "2025-05-19T23:04:41Z"
        }
      ]
    },
    {
      "issue_number": 1759,
      "title": "ModuleNotFoundError upon import of Graph from pydantic-graph",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using pydantic-graph independently, importing Graph leads to a `ModuleNotFoundError` for `opentelemetry`. This is due to the missing `opentelemetry-api` dependency on the [pyproject.toml](https://github.com/pydantic/pydantic-ai/blob/v0.2.4/pydantic_graph/pyproject.toml#L42).\n\n\n```bash\nTraceback (most recent call last):\n  File \"/home/vonsteer/projects/test-pydantic-graph/example.py\", line 7, in <module>\n    from pydantic_graph import BaseNode, End, Graph, GraphRunContext\n  File \"/home/vonsteer/projects/test-pydantic-graph/.venv/lib/python3.12/site-packages/pydantic_graph/__init__.py\", line 2, in <module>\n    from .graph import Graph, GraphRun, GraphRunResult\n  File \"/home/vonsteer/projects/test-pydantic-graph/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 13, in <module>\n    from opentelemetry.trace import Span\nModuleNotFoundError: No module named 'opentelemetry'\n```\n\n### Example Code\n\n```Python\nfrom pydantic_graph import Graph\n\ngraph = Graph(nodes=[])\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nlogfire-api==3.16.0\npydantic==2.11.4\npydantic-core==2.33.2\npydantic-graph==0.2.4\n```",
      "state": "closed",
      "author": "vonsteer",
      "author_type": "User",
      "created_at": "2025-05-19T11:27:59Z",
      "updated_at": "2025-05-19T22:51:13Z",
      "closed_at": "2025-05-19T22:51:13Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1759/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1759",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1759",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:10.571290",
      "comments": [
        {
          "author": "DouweM",
          "body": "@vonsteer Thanks for the report! Fixed in https://github.com/pydantic/pydantic-ai/pull/1773.",
          "created_at": "2025-05-19T22:46:15Z"
        }
      ]
    },
    {
      "issue_number": 1748,
      "title": "Getting Intermediate responses from MCP Tool Calls",
      "body": "### Question\n\nDoes Pydantic AI have a way to receive streamed events/message during a tool call operation on an MCP Server?\n\nUsing the python lib FastMCP, looks like I can \"yield\" messages or use ctx.info to send string back. \n\nWhat I'd like to do is send back to the user (or the LLM) any messages the tool is sending back before it gets the final response.\n\n### Additional Context\n\nFastMCP 2.3.3 \nPydantic AI 0.1.8\nPython 3.10.9",
      "state": "open",
      "author": "Rockyyost",
      "author_type": "User",
      "created_at": "2025-05-16T23:42:46Z",
      "updated_at": "2025-05-19T22:06:37Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1748/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1748",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1748",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:10.848477",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Rockyyost MCP progress notifications are not currently supported by PydanticAI, as you've found. Can you please share some example code of an MCP server that uses these, so we can start thinking about how it'd make sense to surface these in PydanticAI?",
          "created_at": "2025-05-19T22:06:36Z"
        }
      ]
    },
    {
      "issue_number": 1714,
      "title": "Tool Calls Occasionally Fail Silently or Return Incomplete Outputs (Groq + Pydantic AI Integration)",
      "body": "### Question\n\nHi team,\n\nI've been working with `pydantic_ai` for some time now and integrating it into an agentic architecture using both Groq (LLaMA 3.3 70B Versatile) and OpenAI providers.\n\nHere's the repository where the full setup is implemented:\n🔗 [https://github.com/Rikhil-Nell/Multi-Agentic-RAG](https://github.com/Rikhil-Nell/Multi-Agentic-RAG)\n\nAlso here is a deployed streamlit link:\n🔗 [Streamlit App](https://multi-agentic-rag.streamlit.app/)\n(Please note: I'm a student with very limited API credits, so please be mindful if testing.)\n\nThe agent is fairly minimal right now — it uses:\n\n* A RAG vector search tool (`retrieve_relevant_documentation`)\n* A dictionary lookup tool (`call_dictionary` using the Merriam-Webster API)\n\n### ❗ The Problem\n\nTool usage is **highly inconsistent**. At times, everything works perfectly — the agent recognizes intent, calls the right tool, parses arguments, and returns the result correctly.\n\nHowever, during certain stretches (seemingly random), the model stops making actual tool calls and instead returns placeholder-style outputs like:\n\n```\n<function=call_dictionary({\"word\": \"suburbs\"})</function>\n```\n\nThis happens **despite**:\n\n* The tool functions being cleanly defined using `@tool` decorators\n* Well-structured prompts that explicitly direct the model to use tools when needed\n* Confirmed success of the same codebase and logic at other times\n\nA sample system prompt looks like this:\n\n> Be concise, reply professionally. Use the tool `call_dictionary` when asked to define a word. Use `retrieve_relevant_documentation` for queries out of scope. Never respond with the tool call string — only invoke tools directly and wait for the result. Never fabricate an answer. Always begin with RAG if unsure.\n\nI’ve verified that:\n\n* The code path is not skipping function execution\n* There are no exceptions thrown during successful runs\n* This issue does **not** seem to stem from the tool logic itself\n\nIt feels like either:\n\n* The model isn't parsing the system prompt consistently\n* Or there's something flaky in the tool call orchestration layer with `pydantic_ai` or Groq integration\n\n### 💡 Questions / Help Needed\n\n* Is this a known limitation when using Groq models through `pydantic_ai`?\n* Are there internal retry mechanisms or validation steps I can hook into?\n* Are tool calls non-deterministic across inference providers?\n* How can I debug cases where the model outputs a tool call string instead of invoking the function?\n\nI'm not sure if this is a problem with the model, the inference provider, `pydantic_ai`, or my orchestration — but I'd greatly appreciate any pointers or support.\n\n---\n\nyes I used AI for this, I am too emotional and frustrated right now to be able to write a good issue, so I do apologize, I will provide any and all information required for diagnosis if it means my code works please.\n\n\n### Additional Context\n\nPydantic_ai version: 0.1.10\nPython version: 3.13\n",
      "state": "closed",
      "author": "Rikhil-Nell",
      "author_type": "User",
      "created_at": "2025-05-13T19:52:58Z",
      "updated_at": "2025-05-19T21:26:57Z",
      "closed_at": "2025-05-19T21:26:56Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1714/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1714",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1714",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:11.060518",
      "comments": [
        {
          "author": "MarianPetlovanyi",
          "body": "+1 \nHi, I have the same issue.",
          "created_at": "2025-05-15T17:05:20Z"
        },
        {
          "author": "Rikhil-Nell",
          "body": "> +1 Hi, I have the same issue.\n\nthanks a lot for at least letting me know that I am not the only one",
          "created_at": "2025-05-16T14:59:57Z"
        },
        {
          "author": "DouweM",
          "body": "@Rikhil-Nell Unfortunately, this looks like an issue with Llama 3.3 that's also affecting providers other than Groq, like Bedrock: https://github.com/pydantic/pydantic-ai/issues/1649.\n\nPydanticAI passes available tools to the provider over the API (not directly in the prompt), and reads tool calls f",
          "created_at": "2025-05-19T21:26:56Z"
        }
      ]
    },
    {
      "issue_number": 1691,
      "title": "Literal ints don't work for Gemini",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nRelated to: https://github.com/pydantic/pydantic-ai/issues/1553\n\nEven after the fix on that issue, I cannot get integers to work with Gemini:\n\n```python\nfrom typing import Annotated, Literal\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\n\n\nclass Dice(BaseModel):\n    result: Annotated[Literal[1, 2, 3, 4, 5, 6], Field(description=\"The result of a 1d6\")] | None = None\n\n\nagent = Agent(\n    \"google-gla:gemini-2.0-flash\",\n    output_type=Dice,\n    system_prompt=\"You are a dice roller. Roll a 1d6 and return the result.\",\n)\n\nif __name__ == \"__main__\":\n    result = agent.run_sync(\"Roll a 1d6\")\n    print(result.output)\n```\n\n**Error**\n\n```\npydantic_core._pydantic_core.ValidationError: 1 validation error for Dice\nresult\n  Input should be 1, 2, 3, 4, 5 or 6 [type=literal_error, input_value='1', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n```\n\n\n**Version**\n\n```\n❯ poetry show pydantic-ai\n name         : pydantic-ai                                      \n version      : 0.1.12                                           \n description  : Agent Framework / shim to use Pydantic with LLMs \n```\n\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13\nPydantic-AI 0.1.12\n```",
      "state": "closed",
      "author": "scastlara",
      "author_type": "User",
      "created_at": "2025-05-12T12:15:34Z",
      "updated_at": "2025-05-19T19:56:44Z",
      "closed_at": "2025-05-19T19:56:43Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1691/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1691",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1691",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:11.283780",
      "comments": [
        {
          "author": "DouweM",
          "body": "@scastlara Looks like this is a Pydantic issue, because this fails as well:\n\n```py\nfrom typing import Annotated, Literal\n\nfrom pydantic import BaseModel, Field\n\nclass Dice(BaseModel):\n    result: Annotated[Literal[1, 2, 3, 4, 5, 6], Field(description=\"The result of a 1d6\")] | None = None\n\nDice.model",
          "created_at": "2025-05-19T19:56:44Z"
        }
      ]
    },
    {
      "issue_number": 1445,
      "title": "How can I use the strict mode?",
      "body": "### Question\n\nMy model needs strict=True and it is using OpenAIModel. The new functionality is strict=False, Is there a way to modify it?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "ItzAmirreza",
      "author_type": "User",
      "created_at": "2025-04-10T21:18:31Z",
      "updated_at": "2025-05-19T04:03:21Z",
      "closed_at": "2025-04-12T07:14:01Z",
      "labels": [
        "documentation",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1445/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1445",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1445",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:11.554912",
      "comments": [
        {
          "author": "Kludex",
          "body": "I didn't fully understand, but you can pass `strict=True` to the tool.\n\ne.g. `@agent.tool_plain(strict=True)`",
          "created_at": "2025-04-11T10:12:37Z"
        },
        {
          "author": "rmaceissoft",
          "body": "@Kludex related to this—\n\nAccording to #1304, it’s also possible to enable OpenAI's strict mode via `model_settings`, in case you don’t want to apply it to each tool individually and I had two quick observations about that \n\n- It doesn’t seem to be explicitly documented — is there a section in the d",
          "created_at": "2025-04-11T10:56:51Z"
        },
        {
          "author": "dmontagu",
          "body": "We changed #1304 before merging, so strict mode is no longer a model setting, it's just a setting on the tool.\n\nI'll note that I'd like to add some sort of agent-wide `prepare_tools` method that could be defined in one place and make desired changes to tool definitions (similar to what the current `",
          "created_at": "2025-04-11T18:30:14Z"
        },
        {
          "author": "rmaceissoft",
          "body": "@dmontagu Thanks for the clarification, and sorry for the confusion—I should’ve reviewed the PR more closely.\n\nThe idea of a `prepare_tools` function sounds really useful and aligns with the kind of contribution I’d be excited to help with. If there’s interest in prioritizing it, I’d be happy to tak",
          "created_at": "2025-04-11T21:28:51Z"
        },
        {
          "author": "ItzAmirreza",
          "body": "Thanks!",
          "created_at": "2025-04-12T07:14:01Z"
        }
      ]
    },
    {
      "issue_number": 1560,
      "title": "Support for AsyncGenerator in ToolReturnPart.content for Streaming Tool Responses​",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm working on a multi-agent system using PydanticAI, where each agent may invoke tools that return streaming responses. \nCurrently, I'm encountering an issue when a tool returns an AsyncGenerator as its result. \nAssigning this directly to ToolReturnPart.content leads to serialization errors, as AsyncGenerator is not serializable by default.​\n\nExample:\n```python\nagent = Agent(\n    model=OpenAIModel(\"gpt-4o\"),\n)\n\nagent_2 = Agent(\n    model=OpenAIModel(\"gpt-4o\"),\n)\n\n@agent.tool\nasync def tool_1(ctx: RunContext[None]) -> AsyncGenerator[str, None]:\n    async with agent_2.run_stream(\"some question\") as result:\n        async for delta in result.stream_text(delta=True):\n            if delta:\n                yield delta\n```\n\nThe error is :\npydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'async_generator'>\n\n\nAnd it works if \"event.result.content = r\" is added\n\n```python\nelif Agent.is_call_tools_node(node):\n    # A handle-response node => The model returned some data, potentially calls a tool\n    output_messages.append(\n        \"=== CallToolsNode: streaming partial response & tool usage ===\"\n    )\n    r = \"\"\n    async with node.stream(run.ctx) as handle_stream:\n        async for event in handle_stream:\n            if isinstance(event, FunctionToolCallEvent):\n                output_messages.append(\n                    f\"[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})\"\n                )\n            elif isinstance(event, FunctionToolResultEvent):\n                async for chunk in event.result.content:\n                    print(chunk)\n                    r += chunk\n                event.result.content = r\n                output_messages.append(\n                    f\"[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}\"\n                )\n```\n\nIs there a recommended approach to achieve this with the current version of PydanticAI? If not, are there plans to support this functionality in future releases?​\n\nThank you for your assistance and for developing such a powerful tool.​\n\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai-slim==0.1.2\n```",
      "state": "closed",
      "author": "hanil-jihun",
      "author_type": "User",
      "created_at": "2025-04-21T08:44:12Z",
      "updated_at": "2025-05-19T00:07:20Z",
      "closed_at": "2025-05-19T00:07:07Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1560/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1560",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1560",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:11.786351",
      "comments": [
        {
          "author": "wylansford",
          "body": "Is this supported? It's marked as completed, but I still get the same error as of v0.2.4",
          "created_at": "2025-05-18T02:52:17Z"
        },
        {
          "author": "hanil-jihun",
          "body": "> Is this supported? It's marked as completed, but I still get the same error as of v0.2.4\n\nI closed it because I decided not to use pydanticAI",
          "created_at": "2025-05-19T00:05:23Z"
        }
      ]
    },
    {
      "issue_number": 1696,
      "title": "Pydantic base model vs dataclasses",
      "body": "### Question\n\nI've noticed in the pydanticAI documentation that Python dataclasses and Pydantic BaseModel seem to be used interchangeably. I find it unclear when to choose one over the other.\n\nWould it be generally acceptable, or are there any potential drawbacks, if I exclusively use Pydantic BaseModel for all data structures when working with pydanticAI?",
      "state": "closed",
      "author": "tomaszbk",
      "author_type": "User",
      "created_at": "2025-05-12T14:00:35Z",
      "updated_at": "2025-05-16T14:42:10Z",
      "closed_at": "2025-05-16T14:42:09Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1696/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Viicos"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1696",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1696",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:12.009130",
      "comments": [
        {
          "author": "tcrapts",
          "body": "Pydantic models provide runtime validation, dataclasses don't. If runtime validation is required, I use Pydantic models.",
          "created_at": "2025-05-14T12:12:55Z"
        },
        {
          "author": "Viicos",
          "body": "They are pretty much equivalent in terms of validation behavior, but I would recommend using Pydantic models. Pydantic dataclasses were meant for easier transitioning from stdlib dataclasses to Pydantic (as you can mostly just change the import statements).\n\nAs such, Pydantic dataclasses don't have ",
          "created_at": "2025-05-14T14:13:03Z"
        },
        {
          "author": "tomaszbk",
          "body": "I was meaning python's stdlib dataclasses (`from dataclasses import dataclass`). For example in pydanticAI's README there's this example:\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\n\n# skipped code\n\n# SupportDependencies is used to pass data, connections, and l",
          "created_at": "2025-05-14T15:31:24Z"
        },
        {
          "author": "Viicos",
          "body": "Sorry misread your OP. You are free to use both. PydanticAI does not make use of the dependency type at runtime. Providing it as the `Agent.deps_type` argument is purely for static type checking purposes.\n\nThe example in the readme uses stdlib dataclasses because the `SupportDependencies` type is in",
          "created_at": "2025-05-15T09:03:48Z"
        },
        {
          "author": "tomaszbk",
          "body": "@Viicos Thanks for your answer!",
          "created_at": "2025-05-16T14:42:09Z"
        }
      ]
    },
    {
      "issue_number": 1678,
      "title": "Issue with Bedrock and Claude Model: ValidationException on Blank Text Field",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhile using pydantic Bedrock with Claude Model. I often reach the following issue:\n```\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the ConverseStream operation: The text field in the ContentBlock object at messages.21.content.0 is blank. Add text to the text field, and try again.\n```\nIt's not always messages 21, it can be another. But most of the time it happens after a number of tool calls, potentially unsuccessful.\n\nUnfortunately, I'm not sure to understand how to best replicate this. The following snippet led me to face this error in 50% of cases, so it can be used for that. However, I'd need some help to create a more robust code snippet to replicate this error.\n\nI believe we could solve this from ensuring content is not empty in https://github.com/pydantic/pydantic-ai/blob/240b0120bf542e8dbeeb52e7b0f4836f964ededa/pydantic_ai_slim/pydantic_ai/models/bedrock.py#L367C15-L367C28\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent, ModelRetry\nfrom random import randint\nimport asyncio\n\ndef get_random_number():\n    \"\"\"\n    Return a random number using the randint\n    \"\"\"\n    res = randint(0, 100)\n    if res > 50:\n        raise ModelRetry(\"Error: Random number is greater than 50\")\n    return res\n\nagent = Agent(\n    \"bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    system_prompt=\"You are a helpful assistant that generates random numbers.\",\n    retries=3,\n    tools=[get_random_number],\n)\n\nasync def main():\n    async with agent.iter(user_prompt=\"Generate 10 different random numbers and then create a summary of these numbers.\") as result:\n        async for node in result:\n            if Agent.is_model_request_node(node):\n                async with node.stream(result.ctx) as request_stream:\n                    async for event in request_stream:\n                        print(event)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai-slim==0.1.10\nPython 3.10.13\n```",
      "state": "open",
      "author": "celeriev",
      "author_type": "User",
      "created_at": "2025-05-09T15:55:38Z",
      "updated_at": "2025-05-16T13:27:42Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1678/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1678",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1678",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:17.296241",
      "comments": [
        {
          "author": "DouweM",
          "body": "@celeriev Since there's nothing in the code you've shared or PydanticAI that intentionally builds empty text parts, I'm guessing it's Bedrock/Claude itself that's generating them and sending them to us, we're sending them back as part of the chat history along with a tool call response, and then Bed",
          "created_at": "2025-05-12T14:13:02Z"
        },
        {
          "author": "celeriev",
          "body": "Thanks for your tips @DouweM. Here's what I got with the help of logfire:\n\nError:\n`botocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the ConverseStream operation: The text field in the ContentBlock object at messages.25.content.0 is blank. Add text to th",
          "created_at": "2025-05-12T16:17:23Z"
        },
        {
          "author": "DouweM",
          "body": "@celeriev All right, looks like we need a check here for `item.content` being blank:\n\nhttps://github.com/pydantic/pydantic-ai/blob/240b0120bf542e8dbeeb52e7b0f4836f964ededa/pydantic_ai_slim/pydantic_ai/models/bedrock.py#L421\n\nCan you please try that out and submit a PR if it works?",
          "created_at": "2025-05-13T11:29:33Z"
        },
        {
          "author": "celeriev",
          "body": "Hello @DouweM, it is ready for review. This solved the issue for me",
          "created_at": "2025-05-16T13:27:41Z"
        }
      ]
    },
    {
      "issue_number": 782,
      "title": "running into request rate limiting error frequently for openAI models",
      "body": "How can I set request rate limits in pydanticAI ?\n\n```\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\nINFO:openai._base_client:Retrying request to /chat/completions in 25.677000 seconds\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\nINFO:openai._base_client:Retrying request to /chat/completions in 25.201000 seconds\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\nINFO:openai._base_client:Retrying request to /chat/completions in 26.583000 seconds\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\nINFO:openai._base_client:Retrying request to /chat/completions in 26.409000 seconds\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\nINFO:openai._base_client:Retrying request to /chat/completions in 27.608000 seconds\n```",
      "state": "open",
      "author": "saipavankumar-muppalaneni",
      "author_type": "User",
      "created_at": "2025-01-26T19:12:01Z",
      "updated_at": "2025-05-16T05:59:28Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/782/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/782",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/782",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:17.577507",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Seems like a reasonable request, PR welcome.",
          "created_at": "2025-01-27T09:50:05Z"
        },
        {
          "author": "iamaseem",
          "body": "I like to take this can you guys specify the reproduction step @samuelcolvin @saipavankumar-muppalaneni ??\nI also checked the Agent class which retries are present and not in Graph.\n\nPS: The CONTRIBUTE.md is missing.",
          "created_at": "2025-01-28T02:26:53Z"
        },
        {
          "author": "saipavankumar-muppalaneni",
          "body": "It is just an agentic workflow where the agent hits a couple of tools to get back search results for a query, the agent then has to decide if the search results are satisfactory, if not then the agent will again make a search request using the tool, this loop continues until the agent is satisfactor",
          "created_at": "2025-01-29T10:13:08Z"
        },
        {
          "author": "snake-speak",
          "body": "I use `UsageLimits` to get around this when it happens, when searching my database with tools :\n```\nresult = await agent.run(\n    user_input, \n    deps=deps, \n    usage_limits=UsageLimits(request_limit=3),\n    message_history=result.all_messages() if result else None\n    )\n```",
          "created_at": "2025-02-01T05:05:54Z"
        },
        {
          "author": "mkrueger12",
          "body": "+1 this. I may be able to open the PR.",
          "created_at": "2025-03-07T16:27:40Z"
        }
      ]
    },
    {
      "issue_number": 1726,
      "title": "A2A Documentation lacks the client perspective for adoption",
      "body": "Ref: https://ai.pydantic.dev/a2a/#pydanticai-agent-to-a2a-server\n\nThe documentation addresses how an agent can be converted to be A2A-compatible but doesn't include examples or information on how a client should send messages to the agent to receive a response.\n\nI could open a PR to update the documentation to reflect this?",
      "state": "open",
      "author": "dat-adi",
      "author_type": "User",
      "created_at": "2025-05-14T16:20:20Z",
      "updated_at": "2025-05-15T22:49:36Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1726/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dat-adi"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1726",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1726",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:17.834355",
      "comments": [
        {
          "author": "Kludex",
          "body": "Yeah, PR is welcome.\n\nWe also need to add the other supported methods.",
          "created_at": "2025-05-14T17:04:34Z"
        },
        {
          "author": "dat-adi",
          "body": "Hey @Kludex, I haven't changed anything on the `uv.lock` file but seem to be failing the pre-commit stage. Any clues on what could be done here?\n\n```\nclai help output.........................................................Failed\n- hook id: clai-help\n- exit code: 2\n\nerror: Failed to parse `uv.lock`\n",
          "created_at": "2025-05-15T00:42:18Z"
        },
        {
          "author": "dat-adi",
          "body": "Turns out that it was a `uv` version issue. Upgrading it did the trick.\n",
          "created_at": "2025-05-15T19:24:49Z"
        },
        {
          "author": "dat-adi",
          "body": "Done with the documentation for A2A with #1737 \n\nOn another note, I do have a question. Is there a reason as to why we don't run coverage tests on the user's end during pre-commit?\n\nI found myself committing to the PR multiple times before I realized that I could've just run the tests on my system f",
          "created_at": "2025-05-15T22:48:39Z"
        }
      ]
    },
    {
      "issue_number": 496,
      "title": "Configuration and parameters for  `all_messages()` and `new_messages()`",
      "body": "It would be helpful if the `all_messages()` and `new_messages()` methods had an option to exclude the system prompt like `all_messages(system_prompt=False)`. This would probably be a better default behavior too. Why?\r\n\r\nWell, when do you use these methods?\r\n\r\n### 1. Passing messages to the client/frontend\r\n\r\n```python\r\n@app.post(\"/chat\")\r\nasync def chat(data):\r\n    ...\r\n    result = await agent.run(data.message, message_history=data.history)\r\n    return result.all_messages()\r\n```\r\n\r\nYou probably don't want to pass the system prompt along.\r\n\r\n### 2. When storing messages in a database\r\n\r\n```python\r\n...\r\nresult = await agent.run(data.message, message_history=data.history)\r\ndb.table(\"conversations\").insert(result.all_messages_json())\r\n...\r\n```\r\nYou probably don't want to store the system prompt for every conversation.\r\n\r\n### 3. When handing over a conversation to a different agent\r\n\r\n```python\r\nresult1 = agent1.run(message, message_history=history)\r\n...\r\n# handover detected\r\nresult2 = agent2.run(message, message_history=result1.all_messages())\r\n````\r\n\r\nYou want the chat messages for context, but the system prompt of the new agent.\r\n\r\n---\r\n\r\nMore parameters to exclude tool calls or just tool call responses would be another great addition, I think.",
      "state": "open",
      "author": "pietz",
      "author_type": "User",
      "created_at": "2024-12-19T12:06:26Z",
      "updated_at": "2025-05-15T18:51:08Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/496/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/496",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/496",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:18.067953",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "I'm fine to add these parameters, but I do want to point out that current `Agent` won't inject a system prompt if any messages are passed to a run. So we probably want to alter the logic to insert a system prompt if it's not already in the messages.",
          "created_at": "2024-12-19T18:38:03Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "@samuelcolvin So right now if Agents A and B have their own system prompts and dynamic system prompts, and we pass the history from A to B, B would take A's system prompt + whatever was dynamically added in it and ignore its own system prompt and won't even bother calling the dynamic system tool rig",
          "created_at": "2024-12-21T22:57:08Z"
        },
        {
          "author": "josead",
          "body": "> but I do want to point out that current Agent won't inject a system prompt if any messages are passed to a run. \r\n\r\nHey @samuelcolvin, I want to link to #531 Hamza and I discussed Today. \r\nI see some cases where having this ability would be beneficial. If this is something you are planning to supp",
          "created_at": "2024-12-23T13:26:50Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Hi @josead, yes please create a PR.",
          "created_at": "2024-12-23T16:52:50Z"
        },
        {
          "author": "julien-aikho",
          "body": "This is very bad behaviour. This basically makes dynamic prompts useless whenver one wants to maintain history\n",
          "created_at": "2025-01-31T06:37:51Z"
        }
      ]
    },
    {
      "issue_number": 1717,
      "title": "Agent.to_a2a() Missing Capabilities Field Causes ValidationError",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n## Description\n\nWhen using the `to_a2a` method from the `Agent` class, the `capabilities` field required by the `AgentCard` model is neither provided by default nor accepted as an argument. This results in a `ValidationError` when attempting to register the agent at CLI or UI A2A.\n\n---\n\n## Steps to Reproduce\n\n1. Create an agent using `pydantic_ai`:\n\n```python\nfrom pydantic_ai import Agent\nfrom fasta2a import Skill\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nagent = Agent('openai:gpt-4.1')\n\napp = agent.to_a2a(\n    name=\"fun_agent\",\n    url=\"http://localhost:8000\",\n    version=\"1.0.0\",\n    skills=[\n        Skill(\n            id=\"tell_joke\",\n            name=\"Joke Telling Tool\",\n            description=\"Tells a random joke to the user.\",\n        )\n    ],\n    description=\"This is Fun Agent, your friendly assistant for humor! It can tell you jokes to lighten your day.\"\n)\n```\n\n2. Run the script.\n\n---\n\n## Expected Behavior\n\n- The agent should be successfully registered **or**  \n- The `to_a2a` method should expose a `capabilities` parameter to avoid the validation failure.\n\n---\n\n## Actual Behavior\n\nA `ValidationError` is raised when tried to register de agent in the UI or CLI of A2A:\n\n```\npydantic_core._pydantic_core.ValidationError: 1 validation error for AgentCard\ncapabilities\n  Field required [type=missing, input_value={'name': 'fun_agent', 'url': 'http://localhost:8000', 'version': '1.0.0', 'skills': [...], 'description': 'This is Fun Agent, your friendly assistant for humor! It can tell you jokes to lighten your day.'}, input_type=dict]\n```\n\nIf you try to pass `capabilities` directly to `to_a2a`, it results in:\n\n```\nTypeError: Agent.to_a2a() got an unexpected keyword argument 'capabilities'\n```\n\n---\n\n## Root Cause\n\n- `AgentCard` defines `capabilities` as a required field (at A2A).  \n- The `to_a2a` method does **not** accept `capabilities` as an argument and does not provide a default value for it.\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n- Python: 3.12.3  \n- pydantic-ai[a2a]: 0.2.3\n```",
      "state": "closed",
      "author": "alejofig",
      "author_type": "User",
      "created_at": "2025-05-13T22:37:40Z",
      "updated_at": "2025-05-15T14:40:11Z",
      "closed_at": "2025-05-15T14:40:11Z",
      "labels": [
        "need confirmation",
        "a2a"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1717/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1717",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1717",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:18.359980",
      "comments": [
        {
          "author": "Kludex",
          "body": "I can't reproduce the issue... How did you run the script?",
          "created_at": "2025-05-14T18:12:24Z"
        },
        {
          "author": "alejofig",
          "body": "\n```python\nfrom pydantic_ai import Agent\nfrom fasta2a import Skill\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nagent = Agent('openai:gpt-4.1')\n\napp = agent.to_a2a(\n    name=\"fun_agent\",\n    url=\"http://localhost:8000\",\n    version=\"1.0.0\",\n    skills=[\n        Skill(\n            id=\"tell_joke\",\n ",
          "created_at": "2025-05-14T19:33:32Z"
        },
        {
          "author": "Kludex",
          "body": "Well, that's because you are passing it... where do you get the validation error you mentioned?",
          "created_at": "2025-05-14T19:44:02Z"
        },
        {
          "author": "alejofig",
          "body": "If I create the agent and try to add it in the A2A UI, I get an error saying the agent has no capabilities.\n\n\n```\npydantic_core._pydantic_core.ValidationError: 1 validation error for AgentCard\ncapabilities\n  Field required [type=missing, input_value={'name': 'fun_agent', 'url': 'http://localhost:800",
          "created_at": "2025-05-14T19:53:38Z"
        },
        {
          "author": "Kludex",
          "body": "What is the A2A UI?",
          "created_at": "2025-05-14T20:22:41Z"
        }
      ]
    },
    {
      "issue_number": 151,
      "title": "Support for Pydantic Validators",
      "body": "Pydantic Validators subtly do not work.\r\n\r\n## Example\r\n```python\r\nimport os\r\nfrom typing import cast\r\n\r\nimport logfire\r\nfrom pydantic import BaseModel, field_validator\r\nfrom pydantic_ai import Agent\r\nfrom pydantic_ai.models import KnownModelName\r\n\r\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\r\nlogfire.configure(send_to_logfire='if-token-present')\r\n\r\n\r\nclass Location(BaseModel):\r\n    city: str\r\n    country: str\r\n\r\n    @field_validator('city')\r\n    @classmethod\r\n    def validate_city(cls, v):\r\n        # Using a validator that always raises a validation error\r\n        raise ValueError(\"This model will always raise a validation error\")\r\n\r\nmodel = cast(KnownModelName, os.getenv('PYDANTIC_AI_MODEL', 'openai:gpt-4o-mini'))\r\nprint(f'Using model: {model}')\r\nagent = Agent(model, result_type=Location)\r\n\r\nif __name__ == '__main__':\r\n    result = agent.run_sync('barren wasteland?')\r\n    print(result.data)\r\n    print(result.cost())\r\n```\r\n\r\n## Error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/minimal_example.py\", line 33, in <module>\r\n    result = agent.run_sync('barren wasteland?')\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/.venv/lib/python3.10/site-packages/pydantic_ai/agent.py\", line 226, in run_sync\r\n    return loop.run_until_complete(self.run(user_prompt, message_history=message_history, model=model, deps=deps))\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/.venv/lib/python3.10/site-packages/pydantic_ai/agent.py\", line 177, in run\r\n    model_response, request_cost = await agent_model.request(messages)\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/.venv/lib/python3.10/site-packages/pydantic_ai/models/openai.py\", line 132, in request\r\n    response = await self._completions_create(messages, False)\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/.venv/lib/python3.10/site-packages/pydantic_ai/models/openai.py\", line 162, in _completions_create\r\n    openai_messages = [self._map_message(m) for m in messages]\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/.venv/lib/python3.10/site-packages/pydantic_ai/models/openai.py\", line 162, in <listcomp>\r\n    openai_messages = [self._map_message(m) for m in messages]\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/.venv/lib/python3.10/site-packages/pydantic_ai/models/openai.py\", line 243, in _map_message\r\n    content=message.model_response(),\r\n  File \"/Users/skylarbpayne/projects/email-reply-agent/.venv/lib/python3.10/site-packages/pydantic_ai/messages.py\", line 112, in model_response\r\n    description = f'{len(self.content)} validation errors: {json.dumps(self.content, indent=2)}'\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/__init__.py\", line 238, in dumps\r\n    **kw).encode(obj)\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/encoder.py\", line 201, in encode\r\n    chunks = list(chunks)\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/encoder.py\", line 429, in _iterencode\r\n    yield from _iterencode_list(o, _current_indent_level)\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/encoder.py\", line 325, in _iterencode_list\r\n    yield from chunks\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"/Users/skylarbpayne/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type ValueError is not JSON serializable\r\n```\r\n\r\n## Environment\r\n\r\npython: 3.10.14\r\n\r\nuv pip freeze:\r\n```\r\naiofiles==24.1.0\r\naiosqlite==0.20.0\r\nannotated-types==0.7.0\r\nanyio==4.6.2.post1\r\nappnope==0.1.4\r\nasgi-csrf==0.11\r\nasgiref==3.8.1\r\nasttokens==2.4.1\r\nasynciolimiter==1.0.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\ncfgv==3.4.0\r\nchardet==5.2.0\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\nclick-default-group==1.2.4\r\ncolorama==0.4.6\r\ncomm==0.2.2\r\ndatasette==0.65.1\r\ndebugpy==1.8.9\r\ndecorator==5.1.1\r\ndeprecated==1.2.15\r\ndevtools==0.12.2\r\ndistlib==0.3.9\r\ndistro==1.9.0\r\n-e file:///Users/skylarbpayne/projects/email-reply-agent\r\neval-type-backport==0.2.0\r\nexceptiongroup==1.2.2\r\nexecuting==2.1.0\r\nfilelock==3.16.1\r\nflexcache==0.3\r\nflexparser==0.4\r\ngoogle-api-core==2.23.0\r\ngoogle-api-python-client==2.154.0\r\ngoogle-auth==2.36.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-auth-oauthlib==1.2.1\r\ngoogleapis-common-protos==1.66.0\r\ngreenlet==3.1.1\r\ngriffe==1.5.1\r\ngroq==0.13.0\r\nh11==0.14.0\r\nhtml2text==2024.2.26\r\nhttpcore==1.0.7\r\nhttplib2==0.22.0\r\nhttpx==0.28.0\r\nhupper==1.12.1\r\nidentify==2.6.3\r\nidna==3.10\r\nimportlib-metadata==8.5.0\r\niniconfig==2.0.0\r\nipdb==0.13.13\r\nipykernel==6.29.5\r\nipython==8.18.1\r\nitsdangerous==2.2.0\r\njanus==1.1.0\r\njedi==0.19.2\r\njinja2==3.1.4\r\njiter==0.8.0\r\njupyter-client==8.6.3\r\njupyter-core==5.7.2\r\nlogfire==2.6.0\r\nlogfire-api==2.6.0\r\nmarkdown-it-py==3.0.0\r\nmarkupsafe==3.0.2\r\nmatplotlib-inline==0.1.7\r\nmdurl==0.1.2\r\nmergedeep==1.3.4\r\nmypy==1.13.0\r\nmypy-extensions==1.0.0\r\nnest-asyncio==1.6.0\r\nnodeenv==1.9.1\r\noauthlib==3.2.2\r\nopenai==1.56.2\r\nopentelemetry-api==1.28.2\r\nopentelemetry-exporter-otlp-proto-common==1.28.2\r\nopentelemetry-exporter-otlp-proto-http==1.28.2\r\nopentelemetry-instrumentation==0.49b2\r\nopentelemetry-proto==1.28.2\r\nopentelemetry-sdk==1.28.2\r\nopentelemetry-semantic-conventions==0.49b2\r\npackaging==24.2\r\nparso==0.8.4\r\npexpect==4.9.0\r\npip==24.3.1\r\nplatformdirs==4.3.6\r\npluggy==1.5.0\r\npre-commit==4.0.1\r\nprompt-toolkit==3.0.48\r\nproto-plus==1.25.0\r\nprotobuf==5.29.1\r\npsutil==6.1.0\r\nptyprocess==0.7.0\r\npure-eval==0.2.3\r\npyasn1==0.6.1\r\npyasn1-modules==0.4.1\r\npydantic==2.10.3\r\npydantic-ai==0.0.9\r\npydantic-ai-slim==0.0.9\r\npydantic-core==2.27.1\r\npydantic-settings==2.6.1\r\npygments==2.18.0\r\npyparsing==3.2.0\r\npyproject-api==1.8.0\r\npytest==8.3.4\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\npython-multipart==0.0.19\r\npyyaml==6.0.2\r\npyzmq==26.2.0\r\nrequests==2.32.3\r\nrequests-oauthlib==2.0.0\r\nrich==13.9.4\r\nrsa==4.9\r\nruff==0.8.1\r\nsetuptools==75.6.0\r\nsix==1.17.0\r\nsniffio==1.3.1\r\nsqlalchemy==2.0.36\r\nsqlmodel==0.0.22\r\nstack-data==0.6.3\r\ntomli==2.2.1\r\ntornado==6.4.2\r\ntox==4.23.2\r\ntox-uv==1.16.0\r\ntqdm==4.67.1\r\ntraitlets==5.14.3\r\ntyping-extensions==4.12.2\r\nuritemplate==4.1.1\r\nurllib3==2.2.3\r\nuv==0.5.6\r\nuvicorn==0.32.1\r\nvirtualenv==20.28.0\r\nwcwidth==0.2.13\r\nwrapt==1.17.0\r\nzipp==3.21.0\r\n```\r\n\r\n",
      "state": "closed",
      "author": "skylarbpayne",
      "author_type": "User",
      "created_at": "2024-12-05T18:47:55Z",
      "updated_at": "2025-05-15T00:43:06Z",
      "closed_at": "2024-12-08T14:23:07Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/151/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/151",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/151",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:18.603535",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Interesting, thanks for supporting, should be easy to fix.",
          "created_at": "2024-12-06T12:29:14Z"
        },
        {
          "author": "barp",
          "body": "It still doesn't work for tool parameters with field_validators\nOpened #1731 ",
          "created_at": "2025-05-15T00:36:05Z"
        }
      ]
    },
    {
      "issue_number": 1713,
      "title": "The package `pydantic-ai==0.2.3` does not have an extra named `a2a`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI am reading the notes on https://ai.pydantic.dev/a2a/#installation and when i run the install command\"\n\n`uv add 'pydantic-ai[a2a]'`\n\nI am getting this error message\n\n`The package `pydantic-ai==0.2.3` does not have an extra named `a2a``\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython=3.13\npydantic ai=0.2.3\n```",
      "state": "closed",
      "author": "chandy",
      "author_type": "User",
      "created_at": "2025-05-13T17:41:37Z",
      "updated_at": "2025-05-14T21:00:37Z",
      "closed_at": "2025-05-14T17:09:26Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1713/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1713",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1713",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:18.801357",
      "comments": [
        {
          "author": "Kludex",
          "body": "Maybe it's an issue with the cache? Can you try to clean up the cache with `uv cache clean` and try again?\n\nI can't reproduce it.\n\n---\n\nI'll close this preemptively, but please let me know if you still have issues.",
          "created_at": "2025-05-14T17:09:26Z"
        },
        {
          "author": "chandy",
          "body": "I think I found the issue.  I believe a2a is only an extra package on the pydantic-ai-slim page, not the pydantic-ai one.  \n\nHere is the list of extras for slim\nhttps://pypi.org/project/pydantic-ai-slim/\nhttps://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pyproject.toml#L82\n\nversus re",
          "created_at": "2025-05-14T18:42:33Z"
        },
        {
          "author": "Kludex",
          "body": "Thanks @chandy :)\n\nI've pushed https://github.com/pydantic/pydantic-ai/pull/1728 🙏 ",
          "created_at": "2025-05-14T20:11:05Z"
        },
        {
          "author": "chandy",
          "body": "Awesome, glad I could help and thank you all for making such amazing software!",
          "created_at": "2025-05-14T21:00:36Z"
        }
      ]
    },
    {
      "issue_number": 1715,
      "title": "clai requires `OPENAI_API_KEY` even for custom agents that use other models",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAdd the example code to agent.py, then start the chat with:\n\n```\nANTHROPIC_API_KEY=... uv run clai --agent agent:agent\n```\n\nAnd receive the error:\n\n```\nclai - PydanticAI CLI v0.2.3 using openai:gpt-4o\nUsing custom agent: agent:agent\nTraceback (most recent call last):\n  File \"SOME_PATH/.venv/bin/clai\", line 10, in <module>\n    sys.exit(cli())\n             ^^^^^\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/clai/__init__.py\", line 11, in cli\n    _cli.cli_exit('clai')\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 102, in cli_exit\n    sys.exit(cli(prog_name=prog_name))\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 186, in cli\n    agent.model = infer_model(args.model)\n                  ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/pydantic_ai/models/__init__.py\", line 460, in infer_model\n    return OpenAIModel(model_name, provider=provider)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 182, in __init__\n    provider = infer_provider(provider)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/pydantic_ai/providers/__init__.py\", line 50, in infer_provider\n    return OpenAIProvider()\n           ^^^^^^^^^^^^^^^^\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/pydantic_ai/providers/openai.py\", line 67, in __init__\n    self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"SOME_PATH/.venv/lib/python3.12/site-packages/openai/_client.py\", line 349, in __init__\n    raise OpenAIError(\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n```\n\nThe same error occurs if you use non-chat mode.\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\n\nagent: Agent[None, str] = Agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    result_type=str,\n    model_settings={\"temperature\": 0.0},\n)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic_ai: 0.2.3\npython: 3.12.10\n```",
      "state": "closed",
      "author": "matthewfranglen",
      "author_type": "User",
      "created_at": "2025-05-13T20:28:32Z",
      "updated_at": "2025-05-14T20:09:29Z",
      "closed_at": "2025-05-14T20:09:29Z",
      "labels": [
        "cli"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1715/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1715",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1715",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:19.025279",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "yup, I can reproduce. Thanks for reporting.",
          "created_at": "2025-05-13T20:58:15Z"
        }
      ]
    },
    {
      "issue_number": 1606,
      "title": "Add integration to stripe-agent-toolkit",
      "body": "### Description\n\nI would like pydanticAI to integrate with stripe-agent-toolkit which gives developers access to stripe tools to create agents that can take actions in stripe. \n\nThe stripe-agent-toolkit documentation seemed pretty straightforward. They state in their docs that the toolkit works with OpenAI's Agent SDK, LangChain, and CrewAI. They have examples doing it in those frameworks\n\nI have tried implementing their starter example in pydanticAI with the below code.\n\n```\nfrom stripe_agent_toolkit.openai.toolkit import StripeAgentToolkit\nimport os\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=os.environ[\"STRIPE_SECRET_KEY\"],\n    configuration={\n        \"actions\": {\n            \"payment_links\": {\n                \"create\": True,\n            },\n        }\n    },\n)\n\n# PydanticAI\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\n\nmistral_model = MistralModel('mistral-large-latest')\n\nstripe_agent = Agent(\n    mistral_model,\n    name=\"Stripe Agent\",\n    instructions=\"You are an expert at integrating with Stripe\",\n    tools=[stripe_agent_toolkit.get_tools()]\n              )\nresult = stripe_agent.run_sync(\"What tools do you have access to?\")\n\nprint(result.output)\n```\n\nBut when I run this, I get this error;\n\n![Image](https://github.com/user-attachments/assets/bda36675-6eee-4165-a0d6-9f97d76955a7)\n\n\n\n\n### References\n\nhttps://github.com/stripe/agent-toolkit/tree/main?tab=readme-ov-file\n\nhttps://github.com/stripe/agent-toolkit/tree/main/python/examples",
      "state": "closed",
      "author": "Kamal-Moha",
      "author_type": "User",
      "created_at": "2025-04-27T08:40:43Z",
      "updated_at": "2025-05-14T14:00:39Z",
      "closed_at": "2025-05-14T14:00:39Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1606/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1606",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1606",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:19.225891",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kamal-Moha The examples for each framework use the generic tools defined in https://github.com/stripe/agent-toolkit/blob/main/python/stripe_agent_toolkit/tools.py#L39 and then wrap them in a framework-specific Tool definition, like in https://github.com/stripe/agent-toolkit/blob/main/python/stripe_",
          "created_at": "2025-04-29T21:42:42Z"
        },
        {
          "author": "Kamal-Moha",
          "body": "Thanks @DouweM for the guidance on how to approach this. I have made good progress with this using pydantic-ai\n\nHere is my code\n\n**tool.py**\n```\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nimport os\nimport sys\n\nif not __package__:\n    __package__ = (\n        (lambda p, n:\n ",
          "created_at": "2025-05-03T20:25:21Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-11T14:00:32Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-14T14:00:38Z"
        }
      ]
    },
    {
      "issue_number": 1699,
      "title": "CLI fails streaming claude models",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIt appears the CLI (now clai) was regressed back to v0.1.11, since the latest v0.2.0 release of Pydantic-AI, which has re-introduced a failure when streaming claude model via the CLI:\n```\nclai -m claude-3-7-sonnet-latest\nclai - PydanticAI CLI v0.1.11 using claude-3-7-sonnet-latest\nclai ➤ say hello\n# Hello!\n\nHow can I assist you today?\nTraceback (most recent call last):\n  File \"/home/mike/.local/bin/clai\", line 8, in <module>\n    sys.exit(cli())\n             ~~~^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/clai/__init__.py\", line 11, in cli\n    _cli.cli_exit('clai')\n    ~~~~~~~~~~~~~^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/_cli.py\", line 88, in cli_exit\n    sys.exit(cli(prog_name=prog_name))\n             ~~~^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/_cli.py\", line 174, in cli\n    return asyncio.run(run_chat(session, stream, cli_agent, console, code_theme, prog_name))\n           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ~~~~~~~~~~^^^^^^\n  File \"/home/mike/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/home/mike/.local/share/uv/python/cpython-3.13.1-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py\", line 720, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/_cli.py\", line 202, in run_chat\n    messages = await ask_agent(agent, text, stream, console, code_theme, messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/_cli.py\", line 233, in ask_agent\n    async for content in handle_stream.stream_output(debounce_by=None):\n        live.update(Markdown(content, code_theme=code_theme))\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/result.py\", line 114, in stream_output\n    async for response in self.stream_responses(debounce_by=debounce_by):\n        if self._final_result_event is not None:\n            yield await self._validate_response(response, self._final_result_event.tool_name, allow_partial=True)\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/result.py\", line 132, in stream_responses\n    async for _items in group_iter:\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        yield self._raw_stream_response.get()  # current state of the response\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/_utils.py\", line 125, in async_iter_groups_noop\n    async for item in aiterable:\n        yield [item]\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/result.py\", line 211, in aiter\n    async for event in usage_checking_stream:\n        yield event\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/models/anthropic.py\", line 447, in _get_event_iterator\n    self._usage += _map_usage(event)\n                   ~~~~~~~~~~^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.13/site-packages/pydantic_ai/models/anthropic.py\", line 421, in _map_usage\n    getattr(response_usage, 'input_tokens', 0)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + (getattr(response_usage, 'cache_creation_input_tokens', 0) or 0)  # These can be missing, None, or int\n    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n```\nUsing the `--no-stream` argument allows the model to be used without error.\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13.1\nPydantic-AI v0.2.0\nPydanticAI CLI v0.1.11 (note, Pydantic-AI v0.12.1 seems to have been pulled back, along with the v0.1.11 version of clai where briefly this was fixed.)\n```",
      "state": "closed",
      "author": "oshea00",
      "author_type": "User",
      "created_at": "2025-05-12T21:18:32Z",
      "updated_at": "2025-05-13T11:37:33Z",
      "closed_at": "2025-05-13T11:37:32Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1699/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1699",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1699",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:19.470808",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1682\" (95% similar)",
          "created_at": "2025-05-12T21:20:11Z"
        },
        {
          "author": "Kludex",
          "body": "This was fixed in 0.2.1.",
          "created_at": "2025-05-13T11:37:32Z"
        }
      ]
    },
    {
      "issue_number": 1624,
      "title": "Allow definition of Model Settings for LLMJudge",
      "body": "### Description\n\nAlthough it is possible to choose the model, it is not possible to choose specific Model Settings for the LLM Judge; \n\n<img width=\"989\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/373aee97-7b7e-47dd-a5b2-19494aaf395d\" />\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "warp10-simonezambonim",
      "author_type": "User",
      "created_at": "2025-04-30T23:28:25Z",
      "updated_at": "2025-05-13T11:35:00Z",
      "closed_at": "2025-05-13T11:35:00Z",
      "labels": [
        "Feature request",
        "good first issue",
        "help wanted",
        "evals"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1624/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1624",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1624",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:19.741238",
      "comments": [
        {
          "author": "Kludex",
          "body": "@dmontagu Do you have thoughts, or can we add the parameter?",
          "created_at": "2025-05-01T07:40:53Z"
        },
        {
          "author": "dmontagu",
          "body": "I think it's fine to add the parameter, just need to make sure it allows extra fields for the sake of supporting extra settings from vendor-specific subclasses of ModelSettings.",
          "created_at": "2025-05-01T13:10:55Z"
        },
        {
          "author": "assadyousuf",
          "body": "@Kludex @dmontagu Put up PR with a fix for this: https://github.com/pydantic/pydantic-ai/pull/1662. Lmk what you guys think",
          "created_at": "2025-05-08T00:02:52Z"
        }
      ]
    },
    {
      "issue_number": 921,
      "title": "Prompt management, versioning, and optimization",
      "body": "Pydantic-ai generally treats the prompt-templates used to generate the prompts to be sent to the LM as code, for example from the docs\n\n```python\n\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=str,  \n    system_prompt=\"Use the customer's name while replying to them.\",  \n)\n\n\n@agent.system_prompt  \ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.system_prompt\ndef add_the_date() -> str:  \n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.data)\n#> Hello Frank, the date today is 2032-01-02.\n```\n\nThe format strings making up the system prompt would be saved to version control, and treated as code.\n\n[Users have expressed an interest](https://github.com/pydantic/pydantic-ai/issues/913#issuecomment-2655904483) in being able to alter the prompts used by an `Agent` independent of the execution/deployment of the code, as enabled by [GCP's prompt management offering](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/prompt-classes) or [Promptly](https://www.trypromptly.com/), much in the way you might want to update the model used by your ML service with a config change as opposed to a code change.\n\nIt is possible add this kind of capability on-top of pydantic-ai with the `RunContext`, but I think it would be useful to provide examples or in-library support for connecting to a versioned database (or just a `.json`!) of structured prompts for use with the agents.\n\n```python\n\nfrom datetime import date\nfrom dataclasses import dataclass\nimport json\n\nfrom pydantic_ai import Agent, RunContext\n\nwith open(\"prompt_templates.json\") as f:\n    PROMPTS = json.load(f)\n\n@dataclass\nclass MyDeps:  \n    prompt_db: dict[str, str]\n    user_name: str\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps\n)\n\n@agent.system_prompt  \ndef base_instructions(ctx: RunContext[MyDeps]) -> str:\n    return ctx.deps.prompt_db.get(\"base_instructions\")\n\n\n@agent.system_prompt  \ndef add_the_users_name(ctx: RunContext[MyDeps]) -> str:\n    return ctx.deps.prompt_db.get(\"name_template\").format(ctx.deps.user_name)\n\n\n@agent.system_prompt\ndef add_the_date() -> str:  \n    return ctx.deps.prompt_db.get(\"date_template\").format(date.today())\n\n\nresult = agent.run_sync('What is the date?', deps=MyDeps(prompt_db=PROMPTS, user_name=\"Frank\"))\nprint(result.data)\n#> Hello Frank, the date today is 2032-01-02.\n```\n\nFurther down this road is the ability for pydantic-ai to optimize those templates - as implemented in libraries like [dspy](https://github.com/stanfordnlp/dspy) and [adalflow](https://github.com/SylphAI-Inc/AdalFlow), which would be helped if the prompts were in some way parameterized.",
      "state": "open",
      "author": "mikeedjones",
      "author_type": "User",
      "created_at": "2025-02-13T10:29:16Z",
      "updated_at": "2025-05-13T10:12:01Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/921/reactions",
        "total_count": 9,
        "+1": 9,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/921",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/921",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:20.032451",
      "comments": [
        {
          "author": "sydney-runkle",
          "body": "Definitely something that fits into our roadmap, thanks for the detailed feature request!",
          "created_at": "2025-02-14T19:56:45Z"
        },
        {
          "author": "YassinNouh21",
          "body": "Hi there! I'm interested in working on this issue. I've looked at the requirements and would like to implement a prompt management system that addresses it.\n\nI've started designing a solution with the following components:\n\n1. Base Prompt Management:\n   - `PromptTemplate` class for templating prompt",
          "created_at": "2025-03-21T02:33:46Z"
        },
        {
          "author": "flemx",
          "body": "I've been using Jinja2 templates with Pydantic AI for prompt management with great results.  Here's my approach:\n\n\n## Creating a Simple PromptService\n\nHere's a clean approach using a dedicated service class:\n\n```python\nfrom jinja2 import Environment, FileSystemLoader\nfrom typing import Any, Dict, Li",
          "created_at": "2025-04-17T14:00:38Z"
        },
        {
          "author": "2010b9",
          "body": "@flemx thanks for sharing your approach!\n\nI was thinking about using [outlines](https://dottxt-ai.github.io/outlines/latest/welcome/) for prompt templating (they essentially have a wrapper around Jinja2's library). I thought it was a good fit because pydantic.ai does not have any prompt templating f",
          "created_at": "2025-05-12T10:04:20Z"
        },
        {
          "author": "newsbubbles",
          "body": "I have an approach that is open source, it is a standalone lib https://github.com/lks-ai/prowl ... this idea of a very simple but extensible prompt templating system which allows declaration of variables within the prompt. Something like this would be amazing. I use it for multi-step prompts which n",
          "created_at": "2025-05-13T10:12:00Z"
        }
      ]
    },
    {
      "issue_number": 1664,
      "title": "How to handle large numbers of tools?",
      "body": "### Question\n\nI have a large list of tools that I can't directly use in my agent.\nIs it possible to implement a tool registry using Pydantic AI, allowing the model to search and select tools based on the prompt?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "naveen17797",
      "author_type": "User",
      "created_at": "2025-05-08T06:30:53Z",
      "updated_at": "2025-05-13T09:20:53Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1664/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1664",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1664",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:22.149429",
      "comments": [
        {
          "author": "AlexEnrique",
          "body": "Have you considered using multiple agents, each one with a coese subset of tools. You can then use an agent to decide to which agent to route the prompt. This is the supervisor pattern.\n\nAlso, agents searching for tools may require you to add instructions to tool results. From my experience, the LLM",
          "created_at": "2025-05-10T00:18:40Z"
        },
        {
          "author": "naveen17797",
          "body": "@AlexEnrique I would like to stream the tool call events from a sub-agent. From the example provided [here](https://ai.pydantic.dev/agents/#streaming), I was able to get the events from the main agent, but I couldn't do the same with the sub-agent.",
          "created_at": "2025-05-12T01:52:15Z"
        },
        {
          "author": "AlexEnrique",
          "body": "@naveen17797 , with sub-agent you mean using agents as tools, like [this example](https://ai.pydantic.dev/multi-agent-applications/#agent-delegation)\n\nWhat I would try is to create a graph using pydantic-graph with some nodes, like Supervisor and Specialist. Supervisor runs an agent just to decide t",
          "created_at": "2025-05-12T02:13:54Z"
        },
        {
          "author": "aristideubertas",
          "body": "You can prepare tools. I think this will be useful for you\n\nhttps://ai.pydantic.dev/tools/#tool-prepare",
          "created_at": "2025-05-13T09:20:52Z"
        }
      ]
    },
    {
      "issue_number": 527,
      "title": "[BUG] Error responses are not handled correctly for google openai/openrouter ",
      "body": "In case the API returns a 429/Rate limit exceeded, pydantic-ai throws a date-time parsing exception instead of surfacing the appropriate error message from the API around RLE(rate-limit-exceeded).\r\n\r\nThis can easily be replicated by using openrouter with one of the free gemini models.\r\n\r\n\r\n```python\r\nfrom pydantic_ai import Agent\r\n\r\nfrom pydantic_ai.models.openai import OpenAIModel\r\n\r\nmodel = OpenAIModel(\r\n    \"google/gemini-2.0-flash-exp:free\",\r\n    base_url=\"https://openrouter.ai/api/v1\",\r\n    api_key=\"key\",\r\n)\r\n\r\nagent = Agent(\r\n    model=model,\r\n    system_prompt='Be concise, reply with one sentence.',  \r\n)\r\n\r\nresult = agent.run_sync('Who are you?')\r\nprint(result.data)\r\n```\r\n\r\n\r\nThe above returns - \r\n\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Users/sam/dev/openai/openai_demo.py\", line 32, in <module>\r\n    result = agent.run_sync('Who are you?')\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/sam/dev/openai/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 327, in run_sync\r\n    return asyncio.get_event_loop().run_until_complete(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/Users/sam/dev/openai/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 255, in run\r\n    model_response, request_usage = await agent_model.request(messages, model_settings)\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/sam/dev/openai/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 152, in request\r\n    return self._process_response(response), _map_usage(response)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/sam/dev/openai/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 207, in _process_response\r\n    timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: 'NoneType' object cannot be interpreted as an integer\r\n```\r\n\r\nThis happens because the error response is not correctly handled in _process_response - \r\n\r\n```python\r\nChatCompletion(id=None, choices=None, created=None, model=None, object=None, service_tier=None, system_fingerprint=None, usage=None, error={'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': '{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\"\\n  }\\n}\\n', 'provider_name': 'Google'}}, user_id='user_...')\r\n```\r\n\r\nWe should check for the presence of the error object and handle the other fields appropriately.\r\n\r\nNote: I have noticed this with both google's OpenAI compat API and openrouter's gemini API.\r\n\r\nThis is what an example output response may look like",
      "state": "open",
      "author": "sambhav",
      "author_type": "User",
      "created_at": "2024-12-22T21:36:37Z",
      "updated_at": "2025-05-13T08:25:47Z",
      "closed_at": null,
      "labels": [
        "help wanted",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 20,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/527/reactions",
        "total_count": 12,
        "+1": 12,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/527",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/527",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:22.423830",
      "comments": [
        {
          "author": "sambhav",
          "body": "This seems to happen due to inappropriate handling of type-casting by the openai client. OpenAI client always casts the response to a ChatCompletion response type, which they allow additional fields to be set on.\r\n\r\nhttps://github.com/openai/openai-python/blob/89d49335a02ac231925e5a514659c93322f2952",
          "created_at": "2024-12-22T21:56:07Z"
        },
        {
          "author": "sydney-runkle",
          "body": "PRs welcome! Thanks for outlining the issue clearly.",
          "created_at": "2024-12-23T13:33:47Z"
        },
        {
          "author": "110kanishkamedankara110",
          "body": "is this issue fixed?\n",
          "created_at": "2025-02-09T11:32:46Z"
        },
        {
          "author": "lixelv",
          "body": "Nope, it is not fixed. I have the same bug with openrouter.",
          "created_at": "2025-02-18T22:59:54Z"
        },
        {
          "author": "lixelv",
          "body": "Hey, can you please hurry up. 2 months passed, but issue remains...",
          "created_at": "2025-02-18T23:02:35Z"
        }
      ]
    },
    {
      "issue_number": 1029,
      "title": "500: Chat error: Expected code to be unreachable, but got: ModelRequest(parts=[SystemPromptPart(content=",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nHi, has anyone meet this bug yet?\n===================================================\nTRace:\nTraceback (most recent call last):\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_graph\\graph.py\", line 228, in iter\n    yield GraphRun[StateT, DepsT, T](\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\agent.py\", line 464, in iter\n    yield AgentRun(graph_run)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\agent.py\", line 307, in run\n    async for _ in agent_run:\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\agent.py\", line 1298, in __anext__\n    return await self._graph_run.__anext__()\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_graph\\graph.py\", line 738, in __anext__\n    return await self.next(self._next_node)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_graph\\graph.py\", line 727, in next\n    self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_graph\\graph.py\", line 306, in next\n    next_node = await node.run(ctx)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 237, in run\n    return await self._make_request(ctx)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 275, in _make_request\n    model_response, request_usage = await ctx.deps.model.request(\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\models\\vertexai.py\", line 151, in request\n    return await super().request(messages, model_settings, model_request_parameters)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\models\\gemini.py\", line 137, in request\n    async with self._make_request(\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\contextlib.py\", line 181, in __aenter__\n    return await self.gen.__anext__()\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\models\\gemini.py\", line 192, in _make_request\n    sys_prompt_parts, contents = await self._message_to_gemini_content(messages)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\models\\gemini.py\", line 291, in _message_to_gemini_content\n    message_parts.extend(await cls._map_user_prompt(part))\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\pydantic_ai\\models\\gemini.py\", line 343, in _map_user_prompt\n    assert_never(item)\n  File \"C:\\Users\\ASUS\\.conda\\envs\\chatbot_py39\\lib\\site-packages\\typing_extensions.py\", line 2595, in assert_never\n    raise AssertionError(f\"Expected code to be unreachable, but got: {value}\")\nAssertionError: Expected code to be unreachable, but got: ModelRequest(parts=[SystemPromptPart(content='You are an AI assistant customer support for...\n\n===================================================\n\n{\n  \"start_timestamp\": \"2025-03-02T10:35:42.549388Z\",\n  \"trace_id\": \"0195566d0e55c5b9ffde7bcda9dc422f\",\n  \"span_id\": \"eb3788dc1a5f919f\",\n  \"parent_span_id\": null,\n  \"span_name\": \"{agent_name} run {prompt=}\",\n  \"level\": 17,\n  \"service_name\": \"unknown_service\",\n  \"otel_scope_name\": \"pydantic-ai\",\n  \"tags\": [],\n  \"created_at\": 1740911744112.654,\n  \"end_timestamp\": \"2025-03-02T10:35:42.567429Z\",\n  \"kind\": \"span\",\n  \"message\": \"agent run prompt=[ModelRequest(parts=[SystemPromptPart(content='You are an AI a...ime.timezone.utc), part_kind='tool-return')], kind='request')]\",\n  \"is_exception\": true,\n  \"otel_scope_version\": \"3.6.4\",\n  \"service_version\": \"8c80bb6258e273e58f9db5eb48ee7c669a21346d\",\n  \"http_response_status_code\": null,\n  \"semantic_attributes\": null,\n  \"semantic_span_type\": null,\n  \"matched_filter\": true,\n  \"is_extra_span\": false,\n  \"semantic_details\": null,\n  \"day\": \"2025-03-02\",\n  \"duration\": 0.0180411,\n  \"otel_status_code\": \"ERROR\",\n  \"otel_status_message\": \"AssertionError: Expected code to be unreachable, but got: ModelRequest(parts=[SystemPromptPart(content='You are an AI assistant customer support for Jen Beaut...\",\n  \"otel_links\": [],\n  \"otel_events\": [\n    {\n      \"event_name\": \"exception\",\n      \"event_timestamp\": \"2025-03-02T10:35:42.564838Z\",\n      \"attributes\": {\n        \"exception.escaped\": \"True\",\n        \"exception.message\": \"Expected code to be unreachable, but got: ModelRequest(parts=[SystemPromptPart(content='You are an AI assistant customer support for Jen Beaut...\",\n        \"exception.stacktrace\": \"Traceback (most recent call last):\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_graph\\\\graph.py\\\", line 228, in iter\\n    yield GraphRun[StateT, DepsT, T](\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\agent.py\\\", line 464, in iter\\n    yield AgentRun(graph_run)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\agent.py\\\", line 307, in run\\n    async for _ in agent_run:\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\agent.py\\\", line 1298, in __anext__\\n    return await self._graph_run.__anext__()\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_graph\\\\graph.py\\\", line 738, in __anext__\\n    return await self.next(self._next_node)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_graph\\\\graph.py\\\", line 727, in next\\n    self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_graph\\\\graph.py\\\", line 306, in next\\n    next_node = await node.run(ctx)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\_agent_graph.py\\\", line 237, in run\\n    return await self._make_request(ctx)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\_agent_graph.py\\\", line 275, in _make_request\\n    model_response, request_usage = await ctx.deps.model.request(\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\models\\\\vertexai.py\\\", line 151, in request\\n    return await super().request(messages, model_settings, model_request_parameters)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\models\\\\gemini.py\\\", line 137, in request\\n    async with self._make_request(\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\contextlib.py\\\", line 181, in __aenter__\\n    return await self.gen.__anext__()\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\models\\\\gemini.py\\\", line 192, in _make_request\\n    sys_prompt_parts, contents = await self._message_to_gemini_content(messages)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\models\\\\gemini.py\\\", line 291, in _message_to_gemini_content\\n    message_parts.extend(await cls._map_user_prompt(part))\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\pydantic_ai\\\\models\\\\gemini.py\\\", line 343, in _map_user_prompt\\n    assert_never(item)\\n  File \\\"C:\\\\Users\\\\ASUS\\\\.conda\\\\envs\\\\chatbot_py39\\\\lib\\\\site-packages\\\\typing_extensions.py\\\", line 2595, in assert_never\\n    raise AssertionError(f\\\"Expected code to be unreachable, but got: {value}\\\")\\nAssertionError: Expected code to be unreachable, but got: ModelRequest(parts=[SystemPromptPart(content='You are an AI assistant customer support for Jen Beaut...\\n\",\n        \"exception.type\": \"AssertionError\"\n      }\n    }\n  ],\n  \"url_path\": null,\n  \"url_query\": null,\n  \"url_full\": null,\n  \"http_route\": null,\n  \"http_method\": null,\n  \"attributes\": {\n    \"agent\": {\n      \"model\": {\n        \"service_account_file\": null,\n        \"project_id\": null,\n        \"region\": \"us-central1\",\n        \"model_publisher\": \"google\",\n        \"url_template\": \"https://{region}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{region}/publishers/{model_publisher}/models/{model}:\"\n      },\n      \"name\": null,\n      \"end_strategy\": \"early\",\n      \"model_settings\": {\n        \"temperature\": 0.2,\n        \"top_p\": 0.5,\n        \"max_output_tokens\": 800\n      }\n    },\n    \"agent_name\": \"agent\",\n    \"code.filepath\": \"utils\\\\base_agent_PAI.py\",\n    \"code.function\": \"chat_once\",\n    \"code.lineno\": 89,\n    \"logfire.msg_template\": \"{agent_name} run {prompt=}\",\n    \"model_name\": \"gemini-1.5-flash\",\n    \"prompt\": [\n      {\n        \"parts\": [\n          {\n            \"content\": \"You are an AI assistant customer support for Jen Beauty, a beauty service provider specializing in eyelash extensions, nail care, facial spa treatments, and laser hair removal.\\n\\nThe customer may want to:\\n+ Enquire about our beauty services (e.g., eyelash extensions, nails, facials, laser hair removal)\\n+ Book an appointment for a specific service\\n+ Modify or cancel an existing booking\\n+ Check details about their current appointment (time, service, beautician, etc.)\\n+ Ask detailed or complex questions about beauty treatments, aftercare, promotions, and recommendations\\nHere is our service information:\\n{\\n  \\\"name\\\": \\\"Jen Beauty\\\",\\n  \\\"about\\\": \\\"Jen Beauty offers professional nails, manicures, pedicures, lash extensions, facials, waxing, and embroidery services to help you look your best.\\\",\\n  \\\"booking_policy\\\": \\\"Cancellation policy You can cancel or reschedule anytime before the appointment time.\\\",\\n  \\\"instagram\\\": \\\"https://www.instagram.com/Jen.Beauty_Sg/\\\",\\n  \\\"facebook\\\": \\\"https://www.facebook.com/jen.jen.beauty.sg/\\\",\\n  \\\"rating\\\": 5.0,\\n  \\\"reviews\\\": 7,\\n  \\\"opening_hours\\\": {\\n    \\\"Monday\\\": \\\"10:00 - 22:00\\\",\\n    \\\"Tuesday\\\": \\\"10:00 - 22:00\\\",\\n    \\\"Wednesday\\\": \\\"10:00 - 22:00\\\",\\n    \\\"Thursday\\\": \\\"10:00 - 22:00\\\",\\n    \\\"Friday\\\": \\\"10:00 - 22:00\\\",\\n    \\\"Saturday\\\": \\\"10:00 - 22:00\\\",\\n    \\\"Sunday\\\": \\\"10:00 - 22:00\\\"\\n  },\\n  \\\"time_zone\\\": \\\"Singapore Standard Time\\\",\\n  \\\"address\\\": \\\"21a Haji Ln Singapore, Central Singapore Community Development Council 189214\\\",\\n  \\\"contact\\\": {\\n    \\\"phone\\\": \\\"+65 8858 9099\\\",\\n    \\\"email\\\": \\\"jen.beauty9099@gmail.com\\\",\\n    \\\"website\\\": \\\"https://jenbeauty.sg/\\\"\\n  },\\n  \\\"services\\\": {\\n    \\\"Eyelash Service\\\": [\\n      {\\n        \\\"name\\\": \\\"Eye Lash Class 1:1\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$78\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"2D-3D Volume Lash\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$88\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Subtle enhancement for a naturally beautiful, everyday look.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"YY Lash\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$88\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"4D-5D Volume Lash\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$98\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"W Lash\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$98\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Wet Thai Lash\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$98\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"6D-7D Volume Lash\\\",\\n        \\\"duration\\\": \\\"1 hr 30 mins\\\",\\n        \\\"price\\\": \\\"$108\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Subtle enhancement for a naturally beautiful, everyday look.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"8D-9D Volume Lash\\\",\\n        \\\"duration\\\": \\\"2 hrs\\\",\\n        \\\"price\\\": \\\"$108\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Lush, dramatic lashes for a bold, glamorous effect.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Mega Volume 10D up\\\",\\n        \\\"duration\\\": \\\"2 hrs\\\",\\n        \\\"price\\\": \\\"$118\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"High-impact lashes with serious volume for a stunning look.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Mega Volume 15D up\\\",\\n        \\\"duration\\\": \\\"2 hrs\\\",\\n        \\\"price\\\": \\\"$118\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"The ultimate in lash volume - dramatic and eye-catching.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Eyelash Extension Removal\\\",\\n        \\\"duration\\\": \\\"10 mins\\\",\\n        \\\"price\\\": \\\"$10\\\",\\n        \\\"description\\\": \\\"Gentle eyelash removal\\\"\\n      }\\n    ],\\n    \\\"Nails Service\\\": [\\n      {\\n        \\\"name\\\": \\\"Manicure Service (Express)\\\",\\n        \\\"duration\\\": \\\"45 mins\\\",\\n        \\\"price\\\": \\\"$48\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"We understand that your hands and nails are a reflection of your personal style.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Pedicure Service (Express)\\\",\\n        \\\"duration\\\": \\\"45 mins\\\",\\n        \\\"price\\\": \\\"$58\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Our highly skilled nail technicians are passionate about creating stunning nail art and providing top-notch nail care.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Manicure Service Gel (Classic)\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$68\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Treat your hands to a pampering session. Enjoy long-lasting, glossy nails that withstand daily wear.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Pedicure Service Gel (Classic)\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$78\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Indulgent foot care leaving you with gorgeous, long-lasting color.\\\"\\n      }\\n    ],\\n    \\\"Facial Spa\\\": [\\n      {\\n        \\\"name\\\": \\\"BB Glow (Facial Spa)\\\",\\n        \\\"duration\\\": \\\"1 hr 30 mins\\\",\\n        \\\"price\\\": \\\"$128\\\",\\n        \\\"discount\\\": \\\"30% for new customer\\\",\\n        \\\"description\\\": \\\"Durable, foundation-like coverage that minimizes dark spots, redness, and scarring, enhancing your skin\\\\u2019s overall tone, texture, and complexion.\\\"\\n      }\\n    ],\\n    \\\"Laser Hair Removal\\\": [\\n      {\\n        \\\"name\\\": \\\"Small Areas for Women\\\",\\n        \\\"duration\\\": \\\"45 mins\\\",\\n        \\\"price\\\": \\\"$78\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Fingers / Toes, Upper Lips & Chin, Under Arms\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Small Areas for Men\\\",\\n        \\\"duration\\\": \\\"45 mins\\\",\\n        \\\"price\\\": \\\"$98\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Fingers / Toes, Upper Lips & Chin, Under Arms\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Large Areas (Half) for Women\\\",\\n        \\\"duration\\\": \\\"1 hr\\\",\\n        \\\"price\\\": \\\"$88\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Upper / Lower Arms, Upper / Lower Legs, Brazilian\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Large Areas (Half) for Men\\\",\\n        \\\"duration\\\": \\\"1 hr\\\",\\n        \\\"price\\\": \\\"$108\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Upper / Lower Arms, Upper / Lower Legs, Brazilian\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Large Areas (Full) for Women\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$108\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Full Arms, Full Legs, Brazilian\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Large Areas (Full) for Men\\\",\\n        \\\"duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"price\\\": \\\"$128\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\",\\n        \\\"description\\\": \\\"Full Arms, Full Legs, Brazilian\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Full Body Hair Removal for Women\\\",\\n        \\\"duration\\\": \\\"2 hrs\\\",\\n        \\\"price\\\": \\\"$158\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Full Body Hair Removal for Men\\\",\\n        \\\"duration\\\": \\\"2 hrs\\\",\\n        \\\"price\\\": \\\"$178\\\",\\n        \\\"discount\\\": \\\"-30% for new customer\\\"\\n      }\\n    ],\\n    \\\"Facial Embroidery\\\": [\\n      {\\n        \\\"name\\\": \\\"Eyebrow Embroidery\\\",\\n        \\\"duration\\\": \\\"3 hrs\\\",\\n        \\\"price\\\": \\\"$628\\\",\\n        \\\"discount\\\": \\\"-30% for new customer. Long-lasting brow enhancement. Mimics natural hair. Fills gaps and defines shape.\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Lip Embroidery\\\",\\n        \\\"duration\\\": \\\"3 hrs\\\",\\n        \\\"price\\\": \\\"$508\\\",\\n        \\\"discount\\\": \\\"-30% for new customer. Semi-permanent lip color. Enhances shape and fullness. Natural-looking results.\\\"\\n      }\\n    ]\\n  }\\n}\\n- When booking, ask the customer for the following details:\\n+ Services they want\\n+ Date & Time they want to book\\n+ Customer Name (only first necessary, or nickname is good)\\n+ Email\\n+ Phone number \\n- The customer might not specify the service they want, cuz they want to be there and decide. Don't push them too hard to decide right away, put UNKNOWN if they have not decide yet. \\n- Once the customer confirms the booking, respond with: \\\"I will now book the appointment\\\\nYour Appointment ID is a1b2c3d4\\\".Provide a summary of the booking, including all details they provided.\\n- When asked to modify customer order, ask the customer to give the id. Modify one appointment at a time. If it matches one of the scheduled booking, give the user all the information about that order, including [Weekday,Date,Time,Appointment ID,Customer Name,Address,Service,Bill($),Duration(hr),Email]. Ask the user to confirm modification. Then after user confirmation, generate \\\"I will now modify the appointment {The Appointment ID that the user gave}\\\". Then provide the customer with a summary of their new order, datetime, prices, and services ordered. \\n- To cancel the appointment for the customer, please ask the customer to confirm cancellation. Then after user confirmation, generate \\\"I will now cancel the appointment {The Appointment ID that the user gave}\\\". \\n- If a question is too hard, you can't find a satisfactory answer, the customer request to talk to a human customer surport, please generate \\\"I will now transfer the chat to my human colleague!\\\"\\n- If the customer has a special request and insist on it, let them know you will transfer the chat to human customer service agent. Then generate \\\"I will now transfer the chat to my human colleague!\\\"\\n- If your conversation is confirmed to be done, generate \\\"Thank you for using our service, have a good day!\\\"\\n\\n\\nCurrent schedule:\\n[\\n    {\\n        \\\"Weekday\\\": \\\"Monday\\\",\\n        \\\"Date\\\": \\\"1 March, 2025\\\",\\n        \\\"Time\\\": \\\"6:30PM\\\",\\n        \\\"Appointment ID\\\": \\\"Z722B9\\\",\\n        \\\"Customer Name\\\": \\\"John Doe\\\",\\n        \\\"Service\\\": \\\"4D-5D Volume Lash\\\",\\n        \\\"Bill($)\\\": 98,\\n        \\\"Duration\\\": \\\"1 hr 15 mins\\\",\\n        \\\"Email\\\": \\\"johndoe@example.com\\\"\\n    },\\n    {\\n        \\\"Weekday\\\": \\\"Tuesday\\\",\\n        \\\"Date\\\": \\\"2 March 2025\\\",\\n        \\\"Time\\\": \\\"4:00PM\\\",\\n        \\\"Appointment ID\\\": \\\"OI7892\\\",\\n        \\\"Customer Name\\\": \\\"Jane Smith\\\",\\n        \\\"Service\\\": \\\"Manicure Service (Express) + BB Glow (Facial Spa)\\\",\\n        \\\"Bill($)\\\": 90,\\n        \\\"Duration\\\": 2 hrs 15 mins,\\n        \\\"Email\\\": \\\"janesmith@example.com\\\"\\n    }\\n]\\nCurrent time is Monday, 20-02-2025, 18:01:31\\n\\nAssume that there is no overlap in the schedule. Any valid booking time in the future is available.\\n\",\n            \"dynamic_ref\": null,\n            \"part_kind\": \"system-prompt\"\n          },\n          {\n            \"content\": \"Hi booking, YY Lash. March 1st, Im Ben, Ben@gmail\",\n            \"timestamp\": \"2025-03-02T10:35:40.153452+00:00\",\n            \"part_kind\": \"user-prompt\"\n          }\n        ],\n        \"kind\": \"request\"\n      },\n      {\n        \"parts\": [\n          {\n            \"tool_name\": \"final_result\",\n            \"args\": {\n              \"data\": \"I will now book the appointment\\nYour Appointment ID is a1b2c3d4\\nYour appointment is scheduled for Monday, March 1st, 2025 at 6:30PM for YY Lash service.\\nYour Appointment ID is a1b2c3d4\\nCustomer Name: Ben\\nEmail: Ben@gmail\\nService: YY Lash\\nBill($): 88\\nDuration: 1 hr 15 mins\"\n            },\n            \"tool_call_id\": null,\n            \"part_kind\": \"tool-call\"\n          }\n        ],\n        \"model_name\": \"gemini-1.5-flash-001\",\n        \"timestamp\": \"2025-03-02T10:35:42.526466+00:00\",\n        \"kind\": \"response\"\n      },\n      {\n        \"parts\": [\n          {\n            \"tool_name\": \"final_result\",\n            \"content\": \"Final result processed.\",\n            \"tool_call_id\": null,\n            \"timestamp\": \"2025-03-02T10:35:42.540499+00:00\",\n            \"part_kind\": \"tool-return\"\n          }\n        ],\n        \"kind\": \"request\"\n      }\n    ]\n  },\n  \"attributes_json_schema\": \"{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"agent\\\":{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"Agent\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\",\\\"properties\\\":{\\\"model\\\":{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"VertexAIModel\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\"}}},\\\"agent_name\\\":{},\\\"model_name\\\":{},\\\"prompt\\\":{\\\"type\\\":\\\"array\\\",\\\"prefixItems\\\":[{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"ModelRequest\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\",\\\"properties\\\":{\\\"parts\\\":{\\\"type\\\":\\\"array\\\",\\\"prefixItems\\\":[{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"SystemPromptPart\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\"},{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"UserPromptPart\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\",\\\"properties\\\":{\\\"timestamp\\\":{\\\"type\\\":\\\"string\\\",\\\"format\\\":\\\"date-time\\\"}}}]}}},{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"ModelResponse\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\",\\\"properties\\\":{\\\"parts\\\":{\\\"type\\\":\\\"array\\\",\\\"items\\\":{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"ToolCallPart\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\"}},\\\"timestamp\\\":{\\\"type\\\":\\\"string\\\",\\\"format\\\":\\\"date-time\\\"}}},{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"ModelRequest\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\",\\\"properties\\\":{\\\"parts\\\":{\\\"type\\\":\\\"array\\\",\\\"items\\\":{\\\"type\\\":\\\"object\\\",\\\"title\\\":\\\"ToolReturnPart\\\",\\\"x-python-datatype\\\":\\\"dataclass\\\",\\\"properties\\\":{\\\"timestamp\\\":{\\\"type\\\":\\\"string\\\",\\\"format\\\":\\\"date-time\\\"}}}}}}]}}}\",\n  \"otel_scope_attributes\": {},\n  \"service_namespace\": \"\",\n  \"service_instance_id\": \"4f28e878a7c4450687795e703dd8dc8a\",\n  \"process_pid\": 12084,\n  \"otel_resource_attributes\": {\n    \"process.pid\": 12084,\n    \"process.runtime.description\": \"3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]\",\n    \"process.runtime.name\": \"cpython\",\n    \"process.runtime.version\": \"3.9.21\",\n    \"service.instance.id\": \"4f28e878a7c4450687795e703dd8dc8a\",\n    \"service.name\": \"unknown_service\",\n    \"service.version\": \"8c80bb6258e273e58f9db5eb48ee7c669a21346d\",\n    \"telemetry.sdk.language\": \"python\",\n    \"telemetry.sdk.name\": \"opentelemetry\",\n    \"telemetry.sdk.version\": \"1.30.0\"\n  },\n  \"telemetry_sdk_name\": \"opentelemetry\",\n  \"telemetry_sdk_language\": \"python\",\n  \"telemetry_sdk_version\": \"1.30.0\",\n  \"deployment_environment\": null\n}\n\n\n### Example Code\n\n```Python\nimport os\nimport asyncio\nfrom google.auth import default\nfrom fastapi import HTTPException\nimport yaml\nimport logfire\nfrom typing import Optional, Type\nfrom dataclasses import dataclass\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext\nimport json\n\nfrom utils.logfire_utils import scrubbing_callback\nfrom utils.base_agent_PAI import BaseChatbot, BaseDeps, BaseResults, load_yaml_config\nfrom utils.prompt_utils import get_agent_prompt, get_summarizer_prompt\nfrom utils.prompts import *\n\n                 \n# Logfire logging\nLOGFIRE_TOKEN = 'something'\nlogfire.configure(token=LOGFIRE_TOKEN, scrubbing=logfire.ScrubbingOptions(callback=scrubbing_callback))\nlogfire.instrument_httpx(capture_all=True)\n\n\n\n# Set your Google API Key (Embed it securely)\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"gen-lang-client-0102112891-d56f2c707086.json\"\nfrom google.auth import default\ncreds, project = default()\nprint(f\"Authenticated with project: {project}\")\n\n# Dependencies\n@dataclass\nclass SupportDeps(BaseDeps):\n    dep: str\n\nclass SupportResults(BaseResults):\n    data: str\n\nclass SupportChatbot(BaseChatbot):\n    def __init__(self, \n                 system_prompt: Optional[str] = None, \n                 dep_type: Type[BaseDeps] = SupportDeps,  # ✅ Correct default\n                 result_type: Type[BaseResults] = SupportResults, \n                 summmarize_bot: BaseChatbot = None,\n                 helper_bot: BaseChatbot = None,\n                 tools: list = [],\n                 config: dict = None):\n        \n        super().__init__(\n            system_prompt=system_prompt,\n            dep_type=dep_type, \n            result_type=result_type,\n            tools=tools,\n            config=config\n        )\n\n        self.summarizer = summmarize_bot\n        self.helper_bot = helper_bot\n\n        self.summary_report = []\n        self.log = []\n\n    def summarize(self, show: bool = False):\n        \"\"\"Summarizes chat history and extracts order information.\"\"\"\n        content = asyncio.run(self.summarizer.chat_once(self.chat_history))\n\n        try:\n            cur_summary = json.loads(content)\n        except json.JSONDecodeError:\n            print(\"Error: Could not decode summarizer response!\")\n            cur_summary = []\n\n        message = \"[System: Summarized Report for business owner]\"\n        if show: print(message)\n        self.log.append(message)\n        self.summary_report.extend(cur_summary)\n        return self.summary_report\n    \n    def pass2human(self, show = True):\n        message = '[System: Passing to human agent.]'\n        if show: print(message)\n        self.log.append(message)\n\n    def email_biz_owner(self, show = True):\n        message = '[System: Emailing business owner.]'\n        if show: print(message)\n        self.log.append(message)\n\n    def handle_new_order(self, response_txt: str, show: bool):\n        '''\n        Default implementation. Develop upon if needed for specific use cases \n        '''\n        message = '[System: Adding order.]'\n        if show: print(message)\n        self.log.append(message)\n        return response_txt\n    \n    def handle_modify_order(self, response_txt: str, show: bool):\n        '''\n        Default implementation. Develop upon if needed for specific use cases \n        '''\n        message = '[System: Modifying order.]'\n        if show: print(message)\n        self.log.append(message)\n        return response_txt\n    \n    def handle_cancel_order(self, response_txt: str, show: bool):\n        '''\n        Default implementation. Develop upon if needed for specific use cases \n        '''\n        message = '[System: Canceling order.]'\n        if show: print(message)\n        self.log.append(message)\n        return response_txt\n\n    def preprocess_response(self, response_txt: str, show: bool):\n        actions = {\n            BOOKING_TRIGGER_PHRASE: self.handle_new_order,\n            MODIFICATION_TRIGGER_PHRASE: self.handle_modify_order,\n            CANCELLATION_TRIGGER_PHRASE: self.handle_cancel_order,\n        }\n\n        for trigger, action in actions.items():\n            if trigger in response_txt:\n                # If any of the 3 main task\n                action(response_txt, show)\n                \n                self.summarize(show)\n                self.email_biz_owner(show)\n\n                return response_txt\n\n        return response_txt\n    \n    def postprocess_response(self, response_txt: str, show: bool):\n        \"\"\"Handles exit conditions or human escalation.\"\"\"\n        if FINAL_CS_PHRASE in response_txt:\n            return True\n        elif FINAL_CS2HUMAN_PHRASE in response_txt:\n            self.pass2human(show)\n            return True\n        return False\n\n    def chat_user(self, deps: Optional[SupportDeps] = None):\n        \"\"\"Interactive chat loop.\"\"\"\n        print(\"Chatbot initialized. Type 'exit' to quit.\")\n        while True:\n            try:\n                user_input = input(\"User: \")\n                if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n                    print(\"Goodbye!\")\n                    break\n\n                print(f\"Sending Request: {user_input}\") \n                response = self.agent.run_sync(\n                    user_input, \n                    message_history=self.chat_history,\n                    deps=deps)\n                self.chat_history = response.all_messages()\n                text = response.data.data\n\n                if text:  # Check if a result was returned\n                    # Detect tool use and use tools\n                    response_txt = self.preprocess_response(text, show = True) \n                    print(\"Assistant: \", response_txt)\n                    \n                    # Detect exit condition (end convo or pass2human)\n                    exit_condition = self.postprocess_response(response_txt, show = True)\n                    if exit_condition:\n                        break\n\n            except Exception as e:\n                print(f\"An error occurred during inference: {e}\")\n                return None\n            \n\n    async def chat_once(self, \n                        input_prompt: str,\n                        deps: Optional[BaseDeps] = None,\n                        show: bool = True) -> str:\n            \n            exception = ''  # Store exception if any\n\n            while True:\n                try:\n                    response = await self.agent.run(\n                        input_prompt, \n                        message_history=self.chat_history,\n                        deps=deps\n                        )\n                    self.chat_history = response.all_messages()\n                    if response.data.data:  # Check if a result was returned\n                        text = response.data.data \n                        text = self.preprocess_response(text, show)\n                        self.postprocess_response(text, show)\n                        return text\n                    else:\n                        num_trial += 1\n                except Exception as e:\n                    exception = e\n                    break\n\n            print(f\"An error occurred during Gemini inference: {exception}\")\n            return ''\n\n    def save_log(self, filepath: str = \"./out/log.md\"):\n        with open(filepath, 'w') as f:\n            for message in self.log:\n                f.write(message + '\\n')\n\n    def save_summary(self, filepath: str = \"./out/summary.json\"):\n        with open(filepath, 'w') as f:\n            f.write(json.dumps(self.summary_report, indent=4))\n\nif __name__ == \"__main__\":\n    agent_config_path = \"configs/base_agent.yaml\"\n    summarizer_config_path = \"configs/summarizer_agent.yaml\" # A lot more deterministi\n    helper_config_path = \"configs/base_agent.yaml\"\n    prompt_config_path = \"configs/prompt_config.yaml\"\n\n    agent_config = load_yaml_config(agent_config_path)\n    prompt_dict = load_yaml_config(prompt_config_path)\n    system_prompt = get_agent_prompt(**prompt_dict)\n\n    summarizer = BaseChatbot(system_prompt=get_summarizer_prompt(), config=load_yaml_config(summarizer_config_path))\n    # helper_bot = BaseChatbot(system_prompt=HELPER_PROMPT, config=load_yaml_config(helper_config_path))\n\n    agent = SupportChatbot(\n        system_prompt=system_prompt, \n        summmarize_bot=summarizer,\n        # helper_bot=helper_bot\n        config=agent_config)\n    agent.chat_user()\n\n    agent.save_log(filepath = \"./out/log.md\")\n    agent.save_chat_history(filepath = \"./out/history.md\")\n    agent.save_summary(filepath = \"./out/summary.md\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nGemini 1.5-flash. VertexAI\n```",
      "state": "open",
      "author": "BatmanofZuhandArrgh",
      "author_type": "User",
      "created_at": "2025-03-02T10:58:34Z",
      "updated_at": "2025-05-13T07:25:30Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1029/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1029",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1029",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:22.743708",
      "comments": [
        {
          "author": "BatmanofZuhandArrgh",
          "body": "Hope i didn't just reveal any api key in there.\n\nIt seems like the model call tools even tho i didn't want to, then input the ModelRequest as input_prompt. Im confused now",
          "created_at": "2025-03-02T11:00:08Z"
        },
        {
          "author": "shalinibani",
          "body": "I also faced the same error today but for `openai gpt-4o` model. I asked the agent \"what did I ask you so far?\" and it threw the exception:\n```\n File \"/Users/me/dev/misc_repo/mcp-demo/ENV/lib/python3.11/site-packages/pydantic_ai/models/openai.py\", line 192, in request\n    response = await self._comp",
          "created_at": "2025-04-07T16:41:11Z"
        },
        {
          "author": "BatmanofZuhandArrgh",
          "body": "I gave up and write the whole thing from scratch, with trigger phrase parsing and if-else stuff. My life has never been more free haha\n\n(Note that my use case is pretty simple tho)",
          "created_at": "2025-04-08T02:26:27Z"
        },
        {
          "author": "mikebranc",
          "body": "@BatmanofZuhandArrgh  Can you share the code you used to fix this? ",
          "created_at": "2025-04-09T21:32:19Z"
        },
        {
          "author": "wylansford",
          "body": "> [@BatmanofZuhandArrgh](https://github.com/BatmanofZuhandArrgh) Can you share the code you used to fix this?\n\nThe issue for me was due to the de-serialization of the conversation history. When loading the history from some stateful setup, you need to \n\n```\nfrom pydantic_ai.messages import ModelMess",
          "created_at": "2025-04-29T02:35:01Z"
        }
      ]
    },
    {
      "issue_number": 1661,
      "title": "Incorrect ValueRenderer selection for expected_output when building eval report table",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIn `ReportCaseRenderer.build_row` there is a typo :\n\nActual :\nhttps://github.com/pydantic/pydantic-ai/blob/dd22595464ea430d9fcea388e506bf0a1697a9a4/pydantic_evals/pydantic_evals/reporting/__init__.py#L563-L564\n\nExpected\n````python\nif self.include_expected_output:\n            row.append(self.output_renderer.render_value(None, case.expected_output) or EMPTY_CELL_STR)\n````\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nLatest\n```",
      "state": "closed",
      "author": "lionpeloux",
      "author_type": "User",
      "created_at": "2025-05-07T21:40:28Z",
      "updated_at": "2025-05-13T07:07:59Z",
      "closed_at": "2025-05-13T07:07:57Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1661/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1661",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1661",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:23.015558",
      "comments": [
        {
          "author": "DouweM",
          "body": "@lionpeloux Good catch -- can you please submit a PR to fix this?",
          "created_at": "2025-05-08T10:58:32Z"
        },
        {
          "author": "lionpeloux",
          "body": "Sure but I am stuck : https://pydanticlogfire.slack.com/archives/C083V7PMHHA/p1746723117913459\n\nCould not get the pre-commit validation pass.\n`make` seems to pass well. \nAny idea ? It complains about missing field `version` the `uv.lock`\n\nI just clone my fork, install precommit and make install.\n\n``",
          "created_at": "2025-05-08T18:36:40Z"
        },
        {
          "author": "lionpeloux",
          "body": "@DouweM any ideas ? I am stuck on this and could not push my PR ....",
          "created_at": "2025-05-11T19:43:06Z"
        },
        {
          "author": "DouweM",
          "body": "@lionpeloux Can you please update your `uv`? I'd seen this before but updating `uv` fixed it!",
          "created_at": "2025-05-12T11:24:16Z"
        },
        {
          "author": "lionpeloux",
          "body": "@DouweM tks, it resolved my issue.",
          "created_at": "2025-05-13T06:58:23Z"
        }
      ]
    },
    {
      "issue_number": 1604,
      "title": "Support for AWS Profile Names",
      "body": "### Description\n\nI had to write this code because `profile_name` is not an option provided for the BedrockProvider.\n\n```\n    session = boto3.Session(\n        profile_name=profile_name,\n        region_name=region_name,\n    )\n\n    bedrock_client = session.client(\"bedrock-runtime\")\n    provider = BedrockProvider(bedrock_client=bedrock_client)\n    model = BedrockConverseModel(model_name, provider=provider)\n    return model\n```\n\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "prescod",
      "author_type": "User",
      "created_at": "2025-04-26T23:27:39Z",
      "updated_at": "2025-05-12T15:39:32Z",
      "closed_at": "2025-05-12T15:39:32Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1604/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1604",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1604",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:23.290824",
      "comments": [
        {
          "author": "DouweM",
          "body": "Thanks, can you please submit a PR that adds that option to `BedrockProvider.__init__`?",
          "created_at": "2025-04-29T21:43:46Z"
        },
        {
          "author": "Kludex",
          "body": "@prescod What's a profile name? And can you give me a code that I can run it, please?",
          "created_at": "2025-04-30T21:26:59Z"
        },
        {
          "author": "chasewalden",
          "body": "@Kludex, it's an AWS profile. It's used by the `aws` cli and `boto3` (configured in `~/.aws/config` and `~/.aws/credentials`). Each profile can set some default values (region, aws account id, role, etc.) to easily switch between different configurations. \n\nFor example, it can be used to log in with",
          "created_at": "2025-04-30T22:13:17Z"
        },
        {
          "author": "chasewalden",
          "body": "@prescod , currently the options when not passing an explicit `bedrock_client` are passed directly to `boto3.client('bedrock-runtime', ...)`, which is a shortcut to calling `Session.client(...)` on the default session. `profile_name` is a parameter to the `Session.__init__(...)`, and not `Session.cl",
          "created_at": "2025-04-30T22:44:10Z"
        },
        {
          "author": "prescod",
          "body": "Here is an untested diff. I don't know when I will get an hour to do the PR the right way. My next three weeks are pretty intense.\n\n```diff\ndiff --git a/pydantic_ai_slim/pydantic_ai/providers/bedrock.py b/pydantic_ai_slim/pydantic_ai/providers/bedrock.py\nindex 1a8980e..fccb3b3 100644\n--- a/pydantic_",
          "created_at": "2025-05-01T23:24:34Z"
        }
      ]
    },
    {
      "issue_number": 1680,
      "title": "Tool call delta not returning tool_call_id",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen I'm iterating through an agent's graph with `agent.iter(...)`, I'm parsing the nodes and events based on [this example from the documentation](https://ai.pydantic.dev/agents/#streaming). \n\nThe events that are PartStartEvent, FunctionToolCallEvent, and FunctionToolResultEvent return everything fine but my PartDeltaEvent is only returning the args_delta. The rest are none.\n\nHere is my code that is parsing the nodes and events:\n```py\n\nasync def to_data_stream_protocol(node, run):\n    if Agent.is_user_prompt_node(node):\n        ...\n    elif Agent.is_model_request_node(node):\n        async with node.stream(run.ctx) as request_stream:\n            async for event in request_stream:\n                if isinstance(event, PartStartEvent):\n                    if event.part.part_kind == \"text\":\n                        yield \"0:{text}\\n\".format(text=json.dumps(event.part.content))\n                    elif event.part.part_kind == \"tool-call\":\n                        yield 'b:{{\"toolCallId\": \"{tool_call_id}\", \"toolName\": \"{tool_name}\"}}\\n'.format(\n                            tool_call_id=event.part.tool_call_id,\n                            tool_name=event.part.tool_name,\n                        )\n                if isinstance(event, PartDeltaEvent):\n                    if event.delta.part_delta_kind == \"text\":\n                        yield \"0:{text}\\n\".format(\n                            text=json.dumps(event.delta.content_delta)\n                        )\n                    elif event.delta.part_delta_kind == \"tool_call\":\n                        print(\"DEBUG: tool_call_id in delta\", event)\n                        yield 'c:{{\"toolCallId\": \"{tool_call_id}\", \"argsTextDelta\": \"{args_text_delta}\"}}\\n'.format(\n                            tool_call_id=event.delta.tool_call_id,\n                            args_text_delta=event.delta.args_delta,\n                        )\n    elif Agent.is_call_tools_node(node):\n        async with node.stream(run.ctx) as handle_stream:\n            async for event in handle_stream:\n                if isinstance(event, FunctionToolCallEvent):\n                    yield '9:{{\"toolCallId\":\"{tool_call_id}\",\"toolName\":\"{tool_name}\",\"args\":{args}}}\\n'.format(\n                        tool_call_id=event.part.tool_call_id,\n                        tool_name=event.part.tool_name,\n                        args=event.part.args,\n                    )\n                elif isinstance(event, FunctionToolResultEvent):\n                    content = event.result.content\n                    tool_call_id = event.tool_call_id\n                    if isinstance(content, AsyncGenerator) or isinstance(content, AsyncIterable):\n                        async for part in stream_tool_result_data(content, tool_call_id):\n                            yield part\n                    else:\n                        serialisable = _to_serializable(content)\n                        yield f'a:{{\"toolCallId\":\"{tool_call_id}\", \"result\":{json.dumps(serialisable)}}}\\n'\n    elif Agent.is_end_node(node):\n        assert run.result.data == node.data.data\n\n        yield 'd:{{\"finishReason\":\"{reason}\",\"usage\":{{\"promptTokens\":{prompt},\"completionTokens\":{completion}}}}}\\n'.format(\n            reason=(\n                \"tool-call\" if run.result._output_tool_name else \"stop\"\n            ),  # TODO: Add reason determining logic\n            prompt=run.result.usage().request_tokens,\n            completion=run.result.usage().response_tokens,\n        )\n```\n\nAnd here is my debug response for those delta events:\n\n```\nDEBUG: tool_call_id in delta PartDeltaEvent(index=0, delta=ToolCallPartDelta(tool_name_delta=None, args_delta='{\"', tool_call_id=None, part_delta_kind='tool_call'), event_kind='part_delta')\nDEBUG: tool_call_id in delta PartDeltaEvent(index=0, delta=ToolCallPartDelta(tool_name_delta=None, args_delta='complete', tool_call_id=None, part_delta_kind='tool_call'), event_kind='part_delta')\nDEBUG: tool_call_id in delta PartDeltaEvent(index=0, delta=ToolCallPartDelta(tool_name_delta=None, args_delta='_learning', tool_call_id=None, part_delta_kind='tool_call'), event_kind='part_delta')\nDEBUG: tool_call_id in delta PartDeltaEvent(index=0, delta=ToolCallPartDelta(tool_name_delta=None, args_delta='_goal', tool_call_id=None, part_delta_kind='tool_call'), event_kind='part_delta')\nDEBUG: tool_call_id in delta PartDeltaEvent(index=0, delta=ToolCallPartDelta(tool_name_delta=None, args_delta='_summary', tool_call_id=None, part_delta_kind='tool_call'), event_kind='part_delta')\n...\n```\n\nAnd here are excerpts from the surrounding parts so show that the toolcallId is coming in (already processed):\n```\nb:{\"toolCallId\": \"call_Iiv8DU4FKq9MdHKVcn06Pllp\", \"toolName\": \"generate_curriculum\"}\n9:{\"toolCallId\":\"call_Iiv8DU4FKq9MdHKVcn06Pllp\",\"toolName\":\"generate_curriculum\",\"args\":{...}}\na:{\"toolCallId\":\"call_Iiv8DU4FKq9MdHKVcn06Pllp\", \"result\":{...}}\n0:\"\"\n0:\"Here's\"\n0:\" another\"\n...\n```\n\nFor now I can just disable the processing of the delta tool calls until I find a solution so this isn't going to break my application but I thought I'd put notice on this (unless I'm just missing something here).\n\n\n\n\n### Example Code\n\n```Python\nThe method I'm using to stream chat responses and call tools is this:\n\nclass BaseAgent(Agent):\n    def __init__(\n        self,\n        model_name: str,\n        system_prompt: str,\n        learning_materials: Optional[\n            Union[List[str], List[Any]]  # Change to Any to avoid circular import\n        ] = None,\n        tools: Optional[List[Any]] = None,\n        **kwargs,\n    ):\n        # Store original system prompt\n        self.original_system_prompt = system_prompt\n        self.learning_materials = learning_materials or []\n\n        if learning_materials:\n            system_prompt += self.parse_learning_materials(learning_materials)\n\n        # Initialize with the current system prompt (may be enhanced later for StudyMaterial objects)\n        super().__init__(\n            model=model_name,\n            system_prompt=system_prompt,\n            tools=tools if tools is not None else [],\n            instrument=True,\n            **kwargs,\n        )\n\n        self.system_prompt = system_prompt\n        self.tools = tools if tools is not None else []\n        print(self.tools)\n\n   \n\n    async def chat_stream(\n        self,\n        prompt: str,\n        message_history: list,\n        **kwargs: Any,\n    ) -> AsyncGenerator[bytes, Any]:\n        \"\"\"Stream chat interaction with the agent asynchronously\"\"\"\n\n        async def stream_messages():\n            # Build the input message list\n            input_messages = [prompt]\n\n            deps = kwargs.get(\"deps\", None)\n\n            async with self.iter(\n                input_messages, message_history=message_history, deps=deps\n            ) as agent_run:\n                # Handle intermediate stream responses\n                async for node in agent_run:\n                    async for chunk in to_data_stream_protocol(node, agent_run):\n                        yield chunk\n\n                # After stream completes, get final messages and return with last chunk\n                final_messages = agent_run.result.new_messages_json()\n                \n                # Send final message with new messages for history\n                yield chunk, final_messages\n\n        return stream_messages()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n`Python==3.12.9`\n`Pydantic==2.11.4`\n`Pydantic-ai==0.1.10`\n`openai==1.78.0`\nUsing the base `Agent` with `openai:gpt-4.1`\n```",
      "state": "closed",
      "author": "asher-aqi",
      "author_type": "User",
      "created_at": "2025-05-09T17:26:17Z",
      "updated_at": "2025-05-12T15:22:49Z",
      "closed_at": "2025-05-12T15:22:49Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1680/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1680",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1680",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:23.536903",
      "comments": [
        {
          "author": "DouweM",
          "body": "@asher-aqi Thanks for reporting this, would you mind trying if my change in https://github.com/pydantic/pydantic-ai/pull/1694 fixes this?",
          "created_at": "2025-05-12T13:51:06Z"
        }
      ]
    },
    {
      "issue_number": 1667,
      "title": "Gemini: 'Recursive' error for non-recursive output types if multiple fields of the same Pydantic model type within a single model",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIf the agent's result_type contains multiple fields of the same type (which is a pydantic model class) like this\n\n```\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass Axis(BaseModel):\n    label: str\n\nclass Chart(BaseModel):\n    xAxis: Axis\n    yAxis: Axis\n\nAgent(\n    \"google-gla:gemini-2.0-flash\",\n    result_type=Chart,\n).run_sync(\n    'give any chart'\n)\n```\n\nI get this exception\n\n`pydantic_ai.exceptions.UserError: Recursive `$ref`s in JSON Schema are not supported by Gemini: #/$defs/Axis\n`\n\nThis does not happen if both axis were a basic type like string or just one of them is in the model.\nGuess this is a bug, or is this by purpose for some reason?\n\n### Example Code\n\n```Python\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass Axis(BaseModel):\n    label: str\n\nclass Chart(BaseModel):\n    xAxis: Axis\n    yAxis: Axis\n\nAgent(\n    \"google-gla:gemini-2.0-flash\",\n    result_type=Chart,\n).run_sync(\n    'give any chart'\n)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.9\nPydantic 0.1.10\ngoogle-gla:gemini-2.0-flash\n```",
      "state": "closed",
      "author": "marco-rudolph",
      "author_type": "User",
      "created_at": "2025-05-08T12:17:50Z",
      "updated_at": "2025-05-12T15:21:13Z",
      "closed_at": "2025-05-12T15:21:13Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1667/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1667",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1667",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:23.769122",
      "comments": []
    },
    {
      "issue_number": 1094,
      "title": "TypeError in _process_response When response.created is None Using OpenRouter",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nWhen running an Agent with an OpenAIModel using OpenRouter, I receive an error after two tool_plain calls. The error occurs because response.created is None, leading to a TypeError in pydantic_ai\\models\\openai.py when calling datetime.fromtimestamp(response.created, tz=timezone.utc).\n\n### Steps to Reproduce\n\n1. Define an OpenAIModel with OpenRouter as the provider.\n2. Use an Agent to run the model.\n3. After a couple tool calls, observe the TypeError in _process_response.\n\n### Expected Behavior\nThe library should properly handle cases where response.created is None by either:\n- Using a fallback timestamp (e.g., datetime.utcnow())\n- Raising a more descriptive error message\n- Handling API response failures before attempting to parse timestamps\n\n### Full Error\n```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\src\\__main__.py\", line 6, in <module>  \n    asyncio.run(main())\n  File \"C:\\Users\\Maryse\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\src\\main.py\", line 519, in main\n    result = await agent.run(company_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 316, in run\n    async for _ in agent_run:\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 1352, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_graph\\graph.py\", line 734, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_graph\\graph.py\", line 723, in next\n    self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_graph\\graph.py\", line 305, in next\n    next_node = await node.run(ctx)\n                ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 252, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 304, in _make_request\n    model_response, request_usage = await ctx.deps.model.request(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_ai\\models\\openai.py\", line 200, in request\n    return self._process_response(response), _map_usage(response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Lib\\site-packages\\pydantic_ai\\models\\openai.py\", line 295, in _process_response\n    timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object cannot be interpreted as an integer\n    Error: C:\\Users\\Maryse\\Documents\\agents\\company-researcher-agent\\.venv\\Scripts\\python.exe exited with code 1\n```\n\n### Example Code\n\n```Python\nmodel = OpenAIModel(\n        'google/gemini-2.0-flash-001',\n        provider=OpenAIProvider(\n            base_url='https://openrouter.ai/api/v1', api_key=openrouter_api_key\n        ),\n    )\n    agent = Agent(\n        model,\n        result_type=ResponseModel,\n        system_prompt=f'''\n        prompt\n        ''',\n    )\n    result = await agent.run(company_name)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n- Operating system: Windows 10\n- Python version: 3.12\n- PydantiAI version: pydantic-ai-slim[openai] = 0.0.35\n- LLM: google/gemini-2.0-flash-001\n```",
      "state": "closed",
      "author": "LouisDeconinck",
      "author_type": "User",
      "created_at": "2025-03-10T19:30:59Z",
      "updated_at": "2025-05-12T15:10:51Z",
      "closed_at": "2025-03-10T22:42:57Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1094/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1094",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1094",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:23.769155",
      "comments": [
        {
          "author": "bb2103",
          "body": "+1",
          "created_at": "2025-05-12T15:10:49Z"
        }
      ]
    },
    {
      "issue_number": 1676,
      "title": "\"Hook\" before/after mcp server tools calls?",
      "body": "### Question\n\nIs there any way to \"hook\" before/after a MCP server tool is called?\n\nI am using Chainlit and I am trying to show in the UI that the server tool is used i.e.  [Chainlit's Step](https://docs.chainlit.io/api-reference/step-class).\n\nThanks for this great lib!\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "loremaps",
      "author_type": "User",
      "created_at": "2025-05-09T11:37:31Z",
      "updated_at": "2025-05-12T14:15:57Z",
      "closed_at": "2025-05-12T14:15:55Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1676/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1676",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1676",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:24.040279",
      "comments": [
        {
          "author": "DouweM",
          "body": "@loremaps With https://ai.pydantic.dev/agents/#streaming, you can iterate over the back-and-forth with the LLM, and add your own logic when the `FunctionToolCallEvent` is hit!",
          "created_at": "2025-05-12T14:15:55Z"
        }
      ]
    },
    {
      "issue_number": 1607,
      "title": "Human in the loop with Agent in Pydantic Graph",
      "body": "### Question\n\nI want to build an a [chatbot with fastapi](https://ai.pydantic.dev/examples/chat-app/) using pydantic agent and pydantic graph. I use the following graph state:\n\n```\n@dataclass(kw_only=True)\nclass APPState:\n    run_record_id: int | None = None\n    user_input: str | None = None\n    assistant_reply: str | None = None\n    messages: list[ModelMessage] = field(default_factory=list)\n```\n\nThen I follow the doc https://ai.pydantic.dev/graph/#example-human-in-the-loop to have human in the loop. In fact:\n- Set `state.user_input` when receive query from user.\n- Set `state.assistant_reply` and reply it to the user.\n- Set `state.assistant_reply = None` if I just need to run the next node of the graph.\n\nThis works in the normal case.\n\nHowever, I read an [article from medium](https://lalitgehani.medium.com/when-ai-needs-a-helping-hand-pydantic-agents-and-human-in-the-loop-0f5a4bd54c78) yesterday. It shows a tool called `get_human_in_the_loop` that prompts the user for input with a specific question.\n\n```python\n@agent.tool\ndef get_human_in_the_loop(_: RunContext[GetHumanInTheLoopInput], question: str) -> GetHumanInTheLoopOutput:\n    input_str = input(f\"Please provide your input for the question > '{question}': \")\n    return GetHumanInTheLoopOutput(answer=input_str)\n```\n\nI want to let the agent have the `get_human_in_the_loop` tool, and make it compatible with pydantic graph. So:\n- It 's up to LLM to decide when to ask human for input\n- Get human input is a tool just like other functions.\n- The state is persisted between API calls.\n",
      "state": "closed",
      "author": "shizidushu",
      "author_type": "User",
      "created_at": "2025-04-27T09:35:02Z",
      "updated_at": "2025-05-12T14:00:42Z",
      "closed_at": "2025-05-12T14:00:41Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1607/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1607",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1607",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:24.311483",
      "comments": [
        {
          "author": "tianshangwuyun",
          "body": "It is impossible to use input in the generation environment. Is there a mechanism to replace this, such as creating a future=asyncio. Future() and then processing it externally? This method may have endless await. I hope the official can provide a good way",
          "created_at": "2025-04-30T09:59:20Z"
        },
        {
          "author": "DouweM",
          "body": "@shizidushu The Medium example is simpler because it can interrupt the entire execution to ask the user for [`input`](https://docs.python.org/3/library/functions.html#input) on the command line, after which execution will continue on the next line. In a web scenario, the LLM deciding it wants human ",
          "created_at": "2025-04-30T23:03:37Z"
        },
        {
          "author": "DouweM",
          "body": "@shizidushu Instead of making `get_human_in_the_loop` a tool, which you can't end the conversation from, you can make it a possible `output_type`, alongside the real final response. That way, the LLM can choose to call one or the other once it's done all the tool calling needed to determine its next",
          "created_at": "2025-05-01T19:39:42Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-05-09T14:00:33Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-12T14:00:41Z"
        }
      ]
    },
    {
      "issue_number": 1684,
      "title": "[Pydantic Graph] Unexpected behaviour in FullStatePersistence",
      "body": "### Question\n\nIn [count_down_from_persistence.py](https://ai.pydantic.dev/graph/#state-persistence), I replaced FileStatePersistence with FullStatePersistence. The persistence history after the end is not the same in both the cases.\n\nHistory for FileStatePersistence:\n\n```json\n[\n  {\n    \"state\": {\n      \"counter\": 5\n    },\n    \"node\": {\n      \"node_id\": \"CountDown\"\n    },\n    \"start_ts\": \"2025-05-10T22:30:08.463174Z\",\n    \"duration\": 4.887999978109292e-6,\n    \"status\": \"success\",\n    \"kind\": \"node\",\n    \"id\": \"CountDown:8b39bead1166477392f419152160cd2f\"\n  },\n  {\n    \"state\": {\n      \"counter\": 4\n    },\n    \"node\": {\n      \"node_id\": \"CountDown\"\n    },\n    \"start_ts\": \"2025-05-10T22:30:08.464833Z\",\n    \"duration\": 3.698999989865115e-6,\n    \"status\": \"success\",\n    \"kind\": \"node\",\n    \"id\": \"CountDown:777ca6d4a2b84f2abf17d49844923a6a\"\n  },\n  {\n    \"state\": {\n      \"counter\": 3\n    },\n    \"node\": {\n      \"node_id\": \"CountDown\"\n    },\n    \"start_ts\": \"2025-05-10T22:30:08.466064Z\",\n    \"duration\": 3.1830000466470665e-6,\n    \"status\": \"success\",\n    \"kind\": \"node\",\n    \"id\": \"CountDown:268719f7e56c480389569a90a07b0ccc\"\n  },\n  {\n    \"state\": {\n      \"counter\": 2\n    },\n    \"node\": {\n      \"node_id\": \"CountDown\"\n    },\n    \"start_ts\": \"2025-05-10T22:30:08.467180Z\",\n    \"duration\": 2.833999985796254e-6,\n    \"status\": \"success\",\n    \"kind\": \"node\",\n    \"id\": \"CountDown:bd395c57d13e466b8c4f950c2ede80d6\"\n  },\n  {\n    \"state\": {\n      \"counter\": 1\n    },\n    \"node\": {\n      \"node_id\": \"CountDown\"\n    },\n    \"start_ts\": \"2025-05-10T22:30:08.468323Z\",\n    \"duration\": 2.8519999659692985e-6,\n    \"status\": \"success\",\n    \"kind\": \"node\",\n    \"id\": \"CountDown:3e88aeab47884cceb8070699be977d95\"\n  },\n  {\n    \"state\": {\n      \"counter\": 0\n    },\n    \"node\": {\n      \"node_id\": \"CountDown\"\n    },\n    \"start_ts\": \"2025-05-10T22:30:08.469617Z\",\n    \"duration\": 4.914999976790568e-6,\n    \"status\": \"success\",\n    \"kind\": \"node\",\n    \"id\": \"CountDown:4f7f714ba6a24addba516b4a473d25db\"\n  },\n  {\n    \"state\": {\n      \"counter\": 0\n    },\n    \"result\": {\n      \"data\": 0\n    },\n    \"ts\": \"2025-05-10T22:30:08.470297Z\",\n    \"kind\": \"end\",\n    \"id\": \"end:0fccfb59d15b4c258200ae66a6540a1e\"\n  }\n]\n```\n\nHistory for FullStatePersistence (states):\n\n![Image](https://github.com/user-attachments/assets/5627972f-ab0d-4d2c-a952-9c195f71ff42)\n\nHere's the code that uses FullStatePersistence:\n\n```python\nfrom count_down import CountDown, CountDownState, count_down_graph\nfrom pydantic_graph import End\nfrom pydantic_graph.persistence.in_mem import FullStatePersistence\n\n\nasync def main():\n    persistence = FullStatePersistence()\n    state = CountDownState(counter=5)\n    await count_down_graph.initialize(CountDown(), state=state, persistence=persistence)\n\n    done = False\n    while not done:\n        done = await run_node(persistence)\n    print(\"History: \")\n    for step in persistence.history:\n        print(step.state)\n\n\nasync def run_node(persistence) -> bool:\n    async with count_down_graph.iter_from_persistence(persistence) as run:\n        node_or_end = await run.next()\n\n    print(\"Node:\", node_or_end)\n    # > Node: CountDown()\n    # > Node: CountDown()\n    # > Node: CountDown()\n    # > Node: CountDown()\n    # > Node: CountDown()\n    # > Node: End(data=0)\n    return isinstance(node_or_end, End)\n```\n\nExpected behaviour: Both FullStatePersistence and FileStatePersistence should have the same state for all nodes.\n\n\n### Additional Context\n\nPydantic Graph Version: 0.1.10\nPython Version: 3.11.0",
      "state": "open",
      "author": "TheFirstMe",
      "author_type": "User",
      "created_at": "2025-05-10T23:15:13Z",
      "updated_at": "2025-05-12T13:28:46Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1684/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1684",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1684",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:24.669637",
      "comments": [
        {
          "author": "DouweM",
          "body": "@TheFirstMe Good catch, the issue appears to be that `iter_from_persistence` passes `snapshot.state` directly to the new graph run, without deep-copying it, meaning that the node will be passed the same state that was stored in-memory in `persistence.history`, and be allowed to modify it in place:\n\n",
          "created_at": "2025-05-12T13:22:18Z"
        }
      ]
    },
    {
      "issue_number": 1689,
      "title": "MCP Shell Server causes terminal suspension in interactive CLIs",
      "body": "### Question\n\nHi! Not sure whether this is a bug or works as designed, but I suspect it's a bug.\nPlease let me know whether I am not using the library correctly, or if it is a real bug that needs solving.\nThanks!\n\n## Summary\nWhen using MCPServerStdio in an interactive CLI application, the process gets suspended with \"suspended (tty output)\" messages after each command due to terminal foreground process group issues. Calls to the MCPServerStdio are switching the tty group and not restoring it back.\n\n## Description\nWhen using the MCP Shell Server in an interactive CLI application, the process gets suspended after each command execution. This happens because:\n\n- After the MCP shell subprocess is terminated, it leaves the terminal's foreground process group in a state where the main process is no longer in control\n- When the main process tries to write to the terminal, it receives a SIGTTOU signal and gets suspended\n- This manifests as suspended (tty output) messages in the shell\n\n## Mitigation\nI had to wrap calls to the runner with:\n\n```python\ndef ensure_foreground():\n    \"\"\"Ensure this process is in the foreground, ignoring SIGTTOU signal during the operation.\"\"\"\n    try:\n        # Save current SIGTTOU handler\n        old_handler = signal.getsignal(signal.SIGTTOU)\n        # Temporarily ignore SIGTTOU\n        signal.signal(signal.SIGTTOU, signal.SIG_IGN)\n\n        # Set the foreground process group ID\n        fd = sys.stdin.fileno()\n        pgid = os.getpgrp()\n        os.tcsetpgrp(fd, pgid)\n\n        # Restore original handler\n        signal.signal(signal.SIGTTOU, old_handler)\n    except Exception as e:\n        logger.error(f\"Could not set foreground: {e!r}\")\n        sys.exit(1)\n```\nSo.. something like:\n\n```python\nasync def main():\n    mcp_shell = MCPServerStdio(...)\n    agent = Agent(...,\n        mcp_servers=[mcp_shell],\n    )\n    \n    while True:\n        user_input = input(\"You: \")\n        async with agent.run_mcp_servers():\n            result = await agent.run(user_input)\n            \n        # Without this, the process would be suspended on the next terminal I/O\n        ensure_foreground()\n```\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "nzlzcat",
      "author_type": "User",
      "created_at": "2025-05-12T09:11:38Z",
      "updated_at": "2025-05-12T11:39:08Z",
      "closed_at": "2025-05-12T11:39:07Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1689/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1689",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1689",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:24.869126",
      "comments": [
        {
          "author": "DouweM",
          "body": "@nzlzcat `MCPServerStdio` is a thin wrapper around https://github.com/modelcontextprotocol/python-sdk's `stdio_client`, so I'd expect the root of the problem and the right place for it to be fixed to be in their repo. Can you please see if you can reproduce it using `studio_client` directly, and if ",
          "created_at": "2025-05-12T11:39:08Z"
        }
      ]
    },
    {
      "issue_number": 1682,
      "title": "clai fails running claude models with streaming",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nrunning:  `clai -m claude-3-7-sonnet-latest`\n> say hello\nresults in stack trace:\n```\nTraceback (most recent call last):\n  File \"/home/mike/.local/bin/clai\", line 8, in <module>\n    sys.exit(cli())\n             ^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/clai/__init__.py\", line 11, in cli\n    _cli.cli_exit('clai')\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 88, in cli_exit\n    sys.exit(cli(prog_name=prog_name))\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 174, in cli\n    return asyncio.run(run_chat(session, stream, cli_agent, console, code_theme, prog_name))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 684, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 202, in run_chat\n    messages = await ask_agent(agent, text, stream, console, code_theme, messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/_cli.py\", line 233, in ask_agent\n    async for content in handle_stream.stream_output(debounce_by=None):\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/result.py\", line 114, in stream_output\n    async for response in self.stream_responses(debounce_by=debounce_by):\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/result.py\", line 132, in stream_responses\n    async for _items in group_iter:\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/_utils.py\", line 125, in async_iter_groups_noop\n    async for item in aiterable:\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/result.py\", line 211, in aiter\n    async for event in usage_checking_stream:\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/models/anthropic.py\", line 447, in _get_event_iterator\n    self._usage += _map_usage(event)\n                   ^^^^^^^^^^^^^^^^^\n  File \"/home/mike/.local/share/uv/tools/clai/lib/python3.12/site-packages/pydantic_ai/models/anthropic.py\", line 421, in _map_usage\n    getattr(response_usage, 'input_tokens', 0)\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n```\n\nAdding `--no-stream` argument allows me to use the model\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPydanticAI CLI v0.1.11\n```",
      "state": "closed",
      "author": "oshea00",
      "author_type": "User",
      "created_at": "2025-05-10T16:25:45Z",
      "updated_at": "2025-05-12T11:16:47Z",
      "closed_at": "2025-05-12T11:16:47Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1682/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1682",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1682",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:25.153588",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Useful, thanks. Will fix soon. ",
          "created_at": "2025-05-11T08:08:37Z"
        },
        {
          "author": "Kludex",
          "body": "I think part of the fixes are in https://github.com/pydantic/pydantic-ai/pull/1374, fyi",
          "created_at": "2025-05-11T08:12:16Z"
        }
      ]
    },
    {
      "issue_number": 1639,
      "title": "Import Agent throws a TypeError: unhashable type: 'list'",
      "body": "\nTried this example code from the documentation\n```\nfrom pydantic_ai import Agent\n\nagent = Agent(  \n    'google-gla:gemini-1.5-flash',\n    system_prompt='Be concise, reply with one sentence.',  \n)\n\nresult = agent.run_sync('Where does \"hello world\" come from?')  \nprint(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n\nCould be a  bug in this Union : \n```\n```\nOutputDataT = TypeVar('OutputDataT', default=str, covariant=True)\n\"\"\"Covariant type variable for the result data type of a run.\"\"\"\n\nOutputValidatorFunc = Union[\n    Callable[[RunContext[AgentDepsT], OutputDataT_inv], OutputDataT_inv],\n    Callable[[RunContext[AgentDepsT], OutputDataT_inv], Awaitable[OutputDataT_inv]],\n    Callable[[OutputDataT_inv], OutputDataT_inv],\n    Callable[[OutputDataT_inv], Awaitable[OutputDataT_inv]],\n]\n```\n\n\nPython version:  3.9\nName: pydantic-ai\nVersion: 0.1.4\n",
      "state": "closed",
      "author": "ayshaasif",
      "author_type": "User",
      "created_at": "2025-05-03T05:25:27Z",
      "updated_at": "2025-05-09T14:00:32Z",
      "closed_at": "2025-05-09T14:00:32Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1639/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1639",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1639",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:25.388745",
      "comments": [
        {
          "author": "Kludex",
          "body": "What's the error you see? Can you bump pydantic-ai over there?",
          "created_at": "2025-05-06T09:52:51Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-05-09T14:00:31Z"
        }
      ]
    },
    {
      "issue_number": 1155,
      "title": "Unique ID's for SystemPromptPart, UserPromptPart",
      "body": "### Description\n\nI would like to request the addition of unique IDs to both user and system messages. This enhancement would help track messages efficiently, facilitate debugging, and improve overall usability for various use cases.\n\n```python3\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                id='this_is_one_unique_id',\n                content='Be a helpful assistant.',\n                dynamic_ref=None,\n                part_kind='system-prompt',\n            ),\n            UserPromptPart(\n                id='this_is_another_unique_id',\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n                part_kind='user-prompt',\n            ),\n        ],\n        kind='request',\n    )\n]\n```\n\nThe format could be a UUID, incremental ID, or any other suitable unique identifier.\n\nI’d love to contribute a PR for this, but I’m not confident enough in my understanding of the codebase to implement it properly. If the maintainers can provide some guidance or pointers, I’d be happy to try! Otherwise, I appreciate any help in getting this feature added.\n\n### References\n\n- https://platform.openai.com/docs/api-reference/messages/object\n- https://platform.openai.com/docs/api-reference/threads/object",
      "state": "open",
      "author": "jfduque",
      "author_type": "User",
      "created_at": "2025-03-18T00:36:40Z",
      "updated_at": "2025-05-09T04:36:35Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1155/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1155",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1155",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:25.615806",
      "comments": [
        {
          "author": "olivernaaris",
          "body": "Would also be interested in this!",
          "created_at": "2025-03-30T01:03:30Z"
        },
        {
          "author": "hamedf62",
          "body": "this is required feture as user need to Update or Delete a message unique ID per message is more efficient,\ncurrently timestamp as unique id may help but less efficient",
          "created_at": "2025-05-09T04:36:34Z"
        }
      ]
    },
    {
      "issue_number": 1505,
      "title": "Logfire for LLM Evals inside pytest decorator do not work",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI am using pytest in my project and I wanted to include the LLM evals in the same way, however the logging is not sent to logfire when  running pytest.\n\n### Example Code\n\n```Python\nimport logfire\nimport pytest\nfrom pydantic_evals import Dataset\nfrom agents import my_agent\nfrom models import InputModel, OutputModel\n\nTEST_FILE = \"llm-evals/file.yaml\"\n\n...\n\n@pytest.mark.asyncio\nasync def test_therapeutic_area_agent():\n    dataset = Dataset[InputModel, OutputModel].from_file(TEST_FILE)\n    with logfire.span(\"test\"):\n        logfire.info(f\"Loaded dataset with {len(dataset.cases)} cases\")\n\n        report = await dataset.evaluate(my_agent, name=\"my_agent\")\n        report.print(include_input=True, include_output=True, include_expected_output=True, include_durations=True)\n        logfire.info(f\"Average score {report.averages().assertions:.2f}\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai-slim = {extras = [\"anthropic\", \"openai\"], version = \"^0.0.55\"}\npydantic-evals = {extras = [\"logfire\"], version = \"^0.0.55\"}\nAnthropicProvider\n```",
      "state": "open",
      "author": "warp10-simonezambonim",
      "author_type": "User",
      "created_at": "2025-04-16T14:12:54Z",
      "updated_at": "2025-05-08T16:14:50Z",
      "closed_at": null,
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1505/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1505",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1505",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:25.886495",
      "comments": [
        {
          "author": "dmontagu",
          "body": "So, first question — did you call `logfire.configure` somewhere that gets executed as part of the test run? If not, then that might explain why it's not getting sent anywhere — the configure call (with an appropriate token, etc.) has to happen for it to send data anywhere. You can do this in a pytes",
          "created_at": "2025-04-25T20:30:57Z"
        },
        {
          "author": "warp10-simonezambonim",
          "body": "I am sorry, I might have missed to include that on my sample code\n\n```python\n\nfrom shared.observability_handler import configure_observability\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef setup_observability():\n    configure_observability()\n```\n\n\nand `shared/observability_handler.py`\n\n```pyth",
          "created_at": "2025-04-25T22:50:50Z"
        },
        {
          "author": "peppermint-ai-lab",
          "body": "Can confirm. This does not work for us either. Here is the repro project. \n\n[repro-pydantic.zip](https://github.com/user-attachments/files/20106181/repro-pydantic.zip)",
          "created_at": "2025-05-08T16:14:50Z"
        }
      ]
    },
    {
      "issue_number": 1658,
      "title": "Can pydantic-ai support non LLM models (for example text2image models)",
      "body": "### Question\n\nHello,\n\nCan pydantic-ai be used with other models than LLMs ?\n\nFor example Groq as a provider supports  [Text to Speech](https://console.groq.com/docs/text-to-speech#text-to-speech)\n\nOther providers like CloudFlare or Together.ai also supports text2image models (Flux.1-schnell).\n\nIn the documentation there is a chapter about [Image, Audio, Video & Document Input](https://ai.pydantic.dev/input/), but nothing about binary output.\n\nI have some workflows that would benefit of using a unified Agent framework for multi-modal data.  Would it be doable with pydantic-ai ?  any code examples ?\n\nKind regards\n\nNeuromancien\n\n\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "ghost",
      "author_type": "User",
      "created_at": "2025-05-07T15:53:30Z",
      "updated_at": "2025-05-08T09:03:09Z",
      "closed_at": "2025-05-08T09:03:08Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1658/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1658",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1658",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:26.086703",
      "comments": [
        {
          "author": "DouweM",
          "body": "As it stands, PydanticAI is specifically made for building agents with LLM models, not to be a wrapper to call any arbitrary ML model API. To use a specific non-LLM model like this one, I suggest using the official SDK as shown in those docs.",
          "created_at": "2025-05-08T09:03:08Z"
        }
      ]
    },
    {
      "issue_number": 1655,
      "title": "model request timeout",
      "body": "The first request for qwen3 in CentOS 7 environment after server restart often involves a request operation, but when requested again, it becomes normal with a prompt indicating an httpx request operation. However, there is no such issue with Windows 11\n\n\n\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n",
      "state": "closed",
      "author": "tianshangwuyun",
      "author_type": "User",
      "created_at": "2025-05-07T08:39:25Z",
      "updated_at": "2025-05-08T08:22:39Z",
      "closed_at": "2025-05-08T08:22:39Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1655/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1655",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1655",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:26.267380",
      "comments": [
        {
          "author": "DouweM",
          "body": "@tianshangwuyun This sounds like a network issue, not something specific to PydanticAI. Unless you can confirm it only happens with PydanticAI and not other libraries or direct API calls, there's not much we can do here.",
          "created_at": "2025-05-08T08:22:39Z"
        }
      ]
    },
    {
      "issue_number": 1643,
      "title": "How to refine function tool arguments before calling tool itself",
      "body": "### Question\n\nI have this code:\n\n```\nclass ViolationDetail(BaseModel):\n    file_path: str = Field(\n        ...,\n        description=\"The path of the file where the violation occurred.\",\n    )\n    violation: str = Field(\n        ...,\n        description=\"Description of the rule violation and suggestion for improvement.\",\n    )\n\n\nclass RuleCheckResult(BaseModel):\n    violations: List[ViolationDetail] = Field(\n        [],\n        description=\"A list of rule violations found in the code.\",\n    )\n\n\n....\n\n        model = OpenAIModel(\n            \"google/gemini-2.5-pro-preview-03-25\",  # problem in final_tool usage\n            provider=OpenAIProvider(\n                base_url=\"https://openrouter.ai/api/v1\",\n                api_key=openrouter_api_key,\n            ),\n        )\n\n        code_review_agent = Agent(\n            model=model,\n            output_type=RuleCheckResult,\n        )\n```\n\nGemini returns in sth like:\n```\n ModelResponse(parts=[ToolCallPart(tool_name='final_result', args='{\"violations\":[\"{\\\\\"file_path\\\\\": \\\\\"search_v2/config.py\\\\\", \\\\\"violation\\\\\": \\\\\"string...\\\\\"}\",}\"]}', tool_call_id='tool_0_final_result', part_kind='tool-call')], model_name='google/gemini-2.5-pro-preview-03-25', timestamp=datetime.datetime(2025, 5, 5, 11, 21, 30, tzinfo=datetime.timezone.utc), kind='response')\n\n```\n\nIt calls final_result with [string] as violations instead of [object]. How can i fix this issue? i tried some prompt hackings but nothing works. Is there any mechanism for string manipulation of llm response before tool (final_result) calling (call it with correct format)? or custom function as final_result that parses this?\n\nThanks\n\n",
      "state": "closed",
      "author": "bbkgh",
      "author_type": "User",
      "created_at": "2025-05-05T11:37:08Z",
      "updated_at": "2025-05-08T07:54:36Z",
      "closed_at": "2025-05-08T07:54:34Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1643/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1643",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1643",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:26.554779",
      "comments": [
        {
          "author": "HamzaFarhan",
          "body": "You can examine and edit the node before calling it: https://ai.pydantic.dev/agents/#using-next-manually",
          "created_at": "2025-05-07T12:48:41Z"
        },
        {
          "author": "DouweM",
          "body": "@bbkgh Ideally, the model would be better at following the schema, but the easiest way to address this particular mistake would be using Pydantic's `BeforeValidator`:\n\n```py\nfrom _pytest.cacheprovider import json\nfrom pydantic import BaseModel, Field, BeforeValidator\nfrom typing import Annotated, An",
          "created_at": "2025-05-08T07:54:35Z"
        }
      ]
    },
    {
      "issue_number": 1650,
      "title": "Is it possible to trace agents,  tool call/ expected tool calls in Pydantic Eval?",
      "body": "### Question\n\nHi, \n\nIn my case, because an agent can handle a request without calling a tooll (but it's not as good as using tool, I also cannot force the agent  to use tool, as depend on request, for some of them the agent might not need to use tools) so I would like to evaluate the work flow (which agents are involved in multi-agent system) and which tools are called (in order) to know how well the system is designed. \n\nCurrently, In Pydantic Eval, I don't see it tracks this information. Is there anyway to do this? Also is this recommended?  \n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "dinhngoc267",
      "author_type": "User",
      "created_at": "2025-05-06T07:09:52Z",
      "updated_at": "2025-05-08T07:22:50Z",
      "closed_at": "2025-05-08T07:22:50Z",
      "labels": [
        "question",
        "evals"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1650/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1650",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1650",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:26.891885",
      "comments": [
        {
          "author": "dinhngoc267",
          "body": "Hi @Kludex  I think this is a little bit complicated, there might be multiple flows that can achieve an expected result. \n\nSo I think I would need the `EvaluatorContext`  trace message history. Then I can use this data for a custom evaluator. \nWith the current implementation I think I will put messa",
          "created_at": "2025-05-06T09:25:54Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "```python\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, TypeVar\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ToolCallPart\nfrom pydantic_ai.models import KnownModelName\nfrom pydantic_evals import Dataset\n\nResu",
          "created_at": "2025-05-07T12:46:29Z"
        },
        {
          "author": "dinhngoc267",
          "body": "@HamzaFarhan Thank youuuuuuuu ",
          "created_at": "2025-05-08T07:22:13Z"
        }
      ]
    },
    {
      "issue_number": 1106,
      "title": "Token Counter",
      "body": "### Description\n\nIdeally before passing a prompt to a model, we can check the token count. If the token count is too high, I'd like to know upfront so I can reduce the token count on my end. LiteLLM has a token counter with logic that's similar. Could be a decorator that allows me to filter down or reduce the information passed to the prompt.\n\n### References\n\nLiteLLM has a token counter https://docs.litellm.ai/docs/completion/token_usage#3-token_counter",
      "state": "open",
      "author": "msu-reevo",
      "author_type": "User",
      "created_at": "2025-03-12T23:45:00Z",
      "updated_at": "2025-05-08T00:41:27Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "usage"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1106/reactions",
        "total_count": 8,
        "+1": 8,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1106",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1106",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:27.105521",
      "comments": [
        {
          "author": "jdvala",
          "body": "I would love to see this implemented.",
          "created_at": "2025-03-13T12:29:04Z"
        },
        {
          "author": "Kludex",
          "body": "Yeah, makes sense. I think we should implement something similar.\n\nAny suggestions on the API? I think we need a method in the `Agent`...",
          "created_at": "2025-04-17T15:38:34Z"
        },
        {
          "author": "odysseus0",
          "body": "Upvote on this feature. Really useful as I need to decide on what strategy to take (like doing some chunking) if the total size of input tokens is too large.",
          "created_at": "2025-05-08T00:41:26Z"
        }
      ]
    },
    {
      "issue_number": 1233,
      "title": "Intermittent 400 Errors with Tool Calling in Groq API (qwen-qwq-32b)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using the Groq API with the qwen-qwq-32b model and tool calling, I'm experiencing intermittent 400 errors (Bad Request). These errors occur approximately 1 out of every 3-4 identical API calls, making the behavior inconsistent and difficult to debug.\n\n`pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: qwen-qwq-32b, body: {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': \"Elon Musk's contact information is present in our database. His email address is elon.musk@example.com.\"}}\n`\n\n### Example Code\n\n```Python\nimport random\nfrom dotenv import load_dotenv\n\nfrom pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nload_dotenv()\n\n\nclass FinalResult(BaseModel):\n    final_answer: str\n\n\nmodel = \"groq:qwen-qwq-32b\"\n\nagent = Agent(\n    model,\n    deps_type=str,\n    system_prompt=(\"You're helpful AI assistant.\"),\n    result_type=FinalResult,\n)\n\n\n@agent.tool_plain\ndef roll_dice() -> str:\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\n    return str(random.randint(1, 6))\n\n\n@agent.tool_plain\ndef get_all_contacts() -> str:\n    \"\"\"Get all contacts\"\"\"\n    all_contacts = [\n        {\"first_name\": \"John\", \"last_name\": \"Smith\", \"email\": \"john@example.com\"},\n        {\"first_name\": \"Jane\", \"last_name\": \"Smith\", \"email\": \"jane@example.com\"},\n        {\"first_name\": \"Elon\", \"last_name\": \"Musk\", \"email\": \"elon.musk@example.com\"},\n    ]\n    return all_contacts\n\n\ntask = \"Can you check our database and see if we have Elon Musk contact data?\"\n\nresult = agent.run_sync(task)\nprint(result)\nprint(result.data)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.3\npydantic-ai==0.0.43\nqwen-qwq-32b\n```",
      "state": "open",
      "author": "41v4intus",
      "author_type": "User",
      "created_at": "2025-03-25T09:52:48Z",
      "updated_at": "2025-05-07T21:02:32Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1233/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1233",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1233",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:27.335159",
      "comments": [
        {
          "author": "ItzAmirreza",
          "body": "Did you find a way to fix it?",
          "created_at": "2025-04-23T16:11:58Z"
        },
        {
          "author": "therealahnaf",
          "body": "Getting same error with smaller models, llama 3.1 and 4 with 8b and 17b parameters provided by groq. It looks like its unable to parse the output into a structured format. Both llama 3.1 and 4 support tool calling but the error shows as tool_use_failed and from my logs I can see that the tools are i",
          "created_at": "2025-04-28T11:09:19Z"
        },
        {
          "author": "tamir-alltrue-ai",
          "body": "@DouweM similar to the issue I was observing with llama via bedrock: https://github.com/pydantic/pydantic-ai/issues/1649",
          "created_at": "2025-05-07T21:02:31Z"
        }
      ]
    },
    {
      "issue_number": 1652,
      "title": "How to structure multi-message agents to provide few shot examples as separate user-assistant message pairs?",
      "body": "### Question\n\nWhat is the recommended way to define an agent with a multi-message prompt, where multiple messages are used for few shot learning (providing examples of expected responses)?\n\nLet's take for example a prompt with the following structure as role + content pairs:\n\n1. system/developer role: You help users do math.\n2. user role: 1 + 1\n3. assistant role: 2\n4. user role: 6 / 2\n5. assistant role: 3\n6. user role: an `ImageUrl` where the image shows `1 + 1`\n7. assistant role: 2\n8. user role: the actual dynamic input that we want the agent to complete\n\n\n\n### Additional Context\n\nI'm new to `pydantic-ai` and exploring it as an alternative to `litellm`.\n\nLooking through the examples, I found a relevant snippet at\n\nhttps://github.com/pydantic/pydantic-ai/blob/dd22595464ea430d9fcea388e506bf0a1697a9a4/examples/pydantic_ai_examples/chat_app.py#L192-L200\n\nBut let's assume I want to define each message with type safety and actual `pydantic-ai` class constructors.\n\nThe `message_history` argument [of](https://ai.pydantic.dev/api/agent/) `Agent.run` looks especially relevant, but the most common use of this argument in the docs is passing messages from previous agent calls rather than handcrafting them as few-shot examples.",
      "state": "closed",
      "author": "dhimmel",
      "author_type": "User",
      "created_at": "2025-05-06T15:17:16Z",
      "updated_at": "2025-05-07T17:42:55Z",
      "closed_at": "2025-05-07T17:34:30Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1652/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1652",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1652",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:27.588225",
      "comments": [
        {
          "author": "HamzaFarhan",
          "body": "![Image](https://github.com/user-attachments/assets/3f896990-3714-405e-9fa5-df6df893e01f)\n\n```python\nfrom pathlib import Path\n\nfrom pydantic_ai import Agent, BinaryContent, ImageUrl\nfrom pydantic_ai import messages as _messages\n\n\ndef user_message(content: str | ImageUrl | BinaryContent) -> _messages",
          "created_at": "2025-05-07T12:15:11Z"
        },
        {
          "author": "dhimmel",
          "body": "Thanks @HamzaFarhan  tremendously. Let me play around with this solution and then I will either close the issue or comment with any additional questions.\n\nIn the meantime, here is a diagram from the [docs](https://ai.pydantic.dev/api/messages/) to understand the type hierarchy for messages:\n\n![Image",
          "created_at": "2025-05-07T13:01:10Z"
        },
        {
          "author": "dhimmel",
          "body": "So pydantic-ai abstracts away the concept of a role:\n\n1. system/developer role messages are handled by Agent `instructions`/`system_prompt` arguments\n2. user role messages are represented by `ModelRequest`\n3. assistant role messages are represented by `ModelResponse`\n\nAll makes sense and thanks for ",
          "created_at": "2025-05-07T17:34:15Z"
        },
        {
          "author": "dhimmel",
          "body": "Side question: `UserPromptPart` and `ModelResponse` accept a `timestamp` argument we could set explicitly to override the default of now. What consequence if any would custom timestamps have? Are these timestamps serialized into the LLM call or are they just used internally?",
          "created_at": "2025-05-07T17:42:54Z"
        }
      ]
    },
    {
      "issue_number": 1449,
      "title": "Streaming final output while iterating over agent graph",
      "body": "### Question\n\nIs there a way to stream the final agent answer while using the `agent.iter()` method and iterating the graph node by node ? Ideally, I would want to stream the final node message similar to `agent.run_stream()` while having access to the tool calls available through iterating over the graph nodes. \n\n### Additional Context\n\n- Pydantic AI Version: 0.0.40",
      "state": "open",
      "author": "mohamedamr13",
      "author_type": "User",
      "created_at": "2025-04-11T10:54:13Z",
      "updated_at": "2025-05-07T15:04:42Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1449/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1449",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1449",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:27.827983",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-18T14:00:31Z"
        },
        {
          "author": "Zhongbing-Chen",
          "body": "same issue",
          "created_at": "2025-05-07T15:04:41Z"
        }
      ]
    },
    {
      "issue_number": 1210,
      "title": "Using Mistral Agents with tool and message_history leads to \"Unexpected role 'user' after role 'tool'\"",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nWhen using the Mistral model with pydantic-ai agents, passing message history from a previous result to a new run causes an error. The API returns a 400 error with the message \"Unexpected role 'user' after role 'tool'\".\n\n### Example Code\n\n```Python\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.agent import Agent\nfrom pydantic import BaseModel, Field\n\nclass PoemInfo(BaseModel):\n    title: str = Field(..., description=\"Create a fitting titel for your poem\")\n    creator: str = Field(..., description=\"the creator of the poem\")\n    text: str = Field(..., description=\"The complete poem\")\n\nmodel = MistralModel('mistral-small-latest', api_key=\"API_KEY_HERE\")\nagent = Agent(model=model, result_type=PoemInfo)\n\n# This works fine\nresult0 = agent.run_sync(\"Make a Poem, about singing like a bird on a motorbike\")\n\n# This also works fine\nresult1 = agent.run_sync(\"repeat the last answer\")\n\n# This causes the error\nresult2 = agent.run_sync(\"repeat the last answer\", message_history=result0.new_messages())\n\n### LOG Error Message\nmistralai.models.sdkerror.SDKError: API error occurred: Status 400\n{\"object\":\"error\",\"message\":\"Unexpected role 'user' after role 'tool'\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.3 on ubuntu 24.04 amd64\nmistralai==1.6.0\npydantic-ai==0.0.43\n```",
      "state": "open",
      "author": "TKaluza",
      "author_type": "User",
      "created_at": "2025-03-22T14:17:27Z",
      "updated_at": "2025-05-06T21:14:18Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1210/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1210",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1210",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:28.069827",
      "comments": [
        {
          "author": "TKaluza",
          "body": "I have tested it with `OpenAIModel('gpt-4o-mini', ...)` and `AnthropicModel('claude-3-5-haiku-latest', ...` both work as expected. ",
          "created_at": "2025-03-22T14:23:39Z"
        },
        {
          "author": "TKaluza",
          "body": "if someone needs a hotfix:\n\n```python\ndef new_mixtral_messages(result: AgentRunResult) -> list[ModelMessage]:\n    \"\"\"\n    Converts messages to a format compatible with Mistral:\n    - Converts ToolCallPart to TextPart containing the args as text\n    - Skips ToolReturnPart entirely\n    - Preserves mes",
          "created_at": "2025-03-24T19:59:12Z"
        },
        {
          "author": "tamir-alltrue-ai",
          "body": "@TKaluza How did you incorporate this method into your agent? ",
          "created_at": "2025-05-06T21:14:17Z"
        }
      ]
    },
    {
      "issue_number": 1573,
      "title": "Possible extra_headers interface similar to extra_body?",
      "body": "### Description\n\nInspired by [this PR](https://github.com/pydantic/pydantic-ai/pull/1538) which exposed extra_body as a ModelSettings parameter I was curious if we could also expose [extra_headers](https://github.com/pydantic/pydantic-ai/blob/35d6ed49dded87d69c053cb8deb9874b2bfa8d57/pydantic_ai_slim/pydantic_ai/models/openai.py#L287)? My use case is that I need request specific headers to be sent so that certain metadata attributes are attached to a request. This is unique to my specific vendor which allows for [headers to be attached to completion requests](https://portkey.ai/docs/product/observability/metadata#c-url). I have constant headers for auth which are fine to attach to the httpx client object but then there are request specific headers that I need to attach based on dynamic metadata attributes.\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "brandonwatts",
      "author_type": "User",
      "created_at": "2025-04-23T19:03:40Z",
      "updated_at": "2025-05-06T14:01:21Z",
      "closed_at": "2025-05-06T14:01:21Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1573/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1573",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1573",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:28.269917",
      "comments": [
        {
          "author": "Kludex",
          "body": "PR welcome.",
          "created_at": "2025-04-23T19:08:43Z"
        }
      ]
    },
    {
      "issue_number": 1635,
      "title": "Error while importing pydantic_ai",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhile Importing paydantic_ai i am faceing  error - **AttributeError: module 'wrapt' has no attribute 'AdapterFactory'**\n\n```\n(doc-infer) D:\\python\\doc-inference>python\nPython 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\n>>>\n>>>\n>>>\n>>> from pydantic_ai import Agent, ModelRetry\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"D:\\python\\doc-inference\\.venv\\Lib\\site-packages\\pydantic_ai\\__init__.py\", line 3, in <module>\n    from .agent import Agent, CallToolsNode, EndStrategy, ModelRequestNode, UserPromptNode, capture_run_messages\n  File \"D:\\python\\doc-inference\\.venv\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 13, in <module>\n    from opentelemetry.trace import NoOpTracer, use_span\n  File \"D:\\python\\doc-inference\\.venv\\Lib\\site-packages\\opentelemetry\\trace\\__init__.py\", line 83, in <module>\n    from deprecated import deprecated\n  File \"D:\\python\\doc-inference\\.venv\\Lib\\site-packages\\deprecated\\__init__.py\", line 15, in <module>\n    from deprecated.classic import deprecated\n  File \"D:\\python\\doc-inference\\.venv\\Lib\\site-packages\\deprecated\\classic.py\", line 35, in <module>\n    class ClassicAdapter(wrapt.AdapterFactory):\n                         ^^^^^^^^^^^^^^^^^^^^\nAttributeError: module 'wrapt' has no attribute 'AdapterFactory'\n>>> exit()\n```\n\n\n![Image](https://github.com/user-attachments/assets/02044c78-7728-4487-a914-22675bd1dd91)\n\n![Image](https://github.com/user-attachments/assets/c60cb8b4-c51d-4f32-8176-f21f1b4db72e)\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent, ModelRetry\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython : Python 3.11.6 \nOS: Windows\n\n\nD:\\python\\doc-inference>uv add pydantic-ai-slim[openai]\nResolved 144 packages in 995ms\nPrepared 6 packages in 609ms\n░░░░░░░░░░░░░░░░░░░░ [0/13] Installing wheels...                                                                        warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.\n         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\nInstalled 13 packages in 306ms\n + deprecated==1.2.18\n + distro==1.9.0\n + eval-type-backport==0.2.2\n + griffe==1.7.3\n + importlib-metadata==8.6.1\n + jiter==0.9.0\n + logfire-api==3.14.1\n + openai==1.76.2\n + opentelemetry-api==1.32.1\n + pydantic-ai-slim==0.1.9\n + pydantic-graph==0.1.9\n + tqdm==4.67.1\n + typing-inspection==0.4.0\n```",
      "state": "closed",
      "author": "RahulDas-dev",
      "author_type": "User",
      "created_at": "2025-05-02T18:26:33Z",
      "updated_at": "2025-05-05T06:12:30Z",
      "closed_at": "2025-05-05T06:12:30Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1635/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1635",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1635",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:28.462902",
      "comments": [
        {
          "author": "DouweM",
          "body": "@RahulDas-dev Does the source of `D:\\python\\doc-inference\\.venv\\Lib\\site-packages\\wrapt\\__init__.py` match that in https://github.com/GrahamDumpleton/wrapt/blob/f3fa91652266a3a99912e0f8290b574827e845c6/src/wrapt/__init__.py#L13? It imports the `AdapterFactory`, so it should be available to `deprecat",
          "created_at": "2025-05-02T22:12:40Z"
        },
        {
          "author": "RahulDas-dev",
          "body": "I removed the entire virtual environment and performed a fresh installation using uv install. The issue no longer occurs, and I'm currently struggling to reproduce it",
          "created_at": "2025-05-03T09:58:31Z"
        }
      ]
    },
    {
      "issue_number": 1640,
      "title": "new_messages not always including tool-call information",
      "body": "### Question\n\nI have a really simple PydanticAI script where I want to catch (print/log) all user, bot and tool responses. User and bot is easy to log, but I struggle with logging the tool_name and args. For now I loop over new_messages in order to find the relevant info. This works when it's the first time the bot uses that tool, but it seems to cache the response and asking the same question once more it queries the tool (MCP server in my case), but new_messages doesn't contain any tool-call entries. Is there a better way to log all tool calls (name and args) or a way to disable the caching?\n\n```\n        for message in result.new_messages():\n            if hasattr(message, \"parts\"):  # Check if the message has parts\n                for part in message.parts:  # Iterate over message parts\n                    if part.part_kind == \"tool-call\":  # Check if it's a ToolCallPart\n                        print(f\"Tool: {part.tool_name}, Args: {part.args}\")\n\n        # Display the bot's response\n        print(f\"Bot: {result.output}\")\n```\n\n\n### Additional Context\n\nPython 3.12\nLatest version of PydanticAI \nOS: Windows 11",
      "state": "closed",
      "author": "IDmedia",
      "author_type": "User",
      "created_at": "2025-05-03T08:17:05Z",
      "updated_at": "2025-05-03T21:28:50Z",
      "closed_at": "2025-05-03T21:28:49Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1640/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1640",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1640",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:28.667802",
      "comments": [
        {
          "author": "IDmedia",
          "body": "I figured out my mistake. The problem is that I based my code on the ChatState found [here](https://github.com/pydantic/pydantic-ai/issues/196) which saved the message_history. Excluding all tool-callings fixed the issue.",
          "created_at": "2025-05-03T21:28:49Z"
        }
      ]
    },
    {
      "issue_number": 1611,
      "title": "Obsolete note in AnthropicModel documentaion",
      "body": "### Question\n\nHey, isn't the note:  \n\n> The `AnthropicModel` class does not yet support streaming responses.\nWe anticipate adding support for streaming responses in a near-term future release.\n\n in AnthropicModel in pydantic_ai_slim/pydantic_ai/models/anthropic.py obsolete now?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "damyantilev",
      "author_type": "User",
      "created_at": "2025-04-28T15:36:52Z",
      "updated_at": "2025-05-02T10:03:41Z",
      "closed_at": "2025-05-02T10:03:41Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1611/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1611",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1611",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:28.872733",
      "comments": [
        {
          "author": "DouweM",
          "body": "@damyantilev You're right! Can you see if you can submit a PR to fix this?",
          "created_at": "2025-04-30T16:45:42Z"
        },
        {
          "author": "aniketopensesafi",
          "body": "Any updates on this? I am unable to use anthropic models with stream in 0.1.8, where it stops executing after first tool call.",
          "created_at": "2025-05-01T21:09:06Z"
        },
        {
          "author": "Kludex",
          "body": "> Any updates on this? I am unable to use anthropic models with stream in 0.1.8, where it stops executing after first tool call.\n\nDo you have an MRE?",
          "created_at": "2025-05-02T05:10:30Z"
        },
        {
          "author": "damyantilev",
          "body": "\n\n\n> [@damyantilev](https://github.com/damyantilev) You're right! Can you see if you can submit a PR to fix this?\n\nThank you for the reply. I did manage to submit a PR - #1630 ",
          "created_at": "2025-05-02T09:52:20Z"
        }
      ]
    },
    {
      "issue_number": 1518,
      "title": "Exception when bedrock no response",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI've found that sometimes claude does return nothing. :(\n\nsame error when non-streaming\n\ntraceback:\n\n```bash\nTraceback (most recent call last):\n  File \"/Users/jizhongsheng/code/oss/zerolab/lightblue-ai/main.py\", line 194, in <module>\n    asyncio.run(main())\n  File \"/Users/jizhongsheng/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/jizhongsheng/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jizhongsheng/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/jizhongsheng/code/oss/zerolab/lightblue-ai/main.py\", line 172, in main\n    async for event in handle_stream:\n  File \"/Users/jizhongsheng/code/oss/zerolab/lightblue-ai/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 434, in _run_stream\n    async for event in self._events_iterator:\n  File \"/Users/jizhongsheng/code/oss/zerolab/lightblue-ai/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 430, in _run_stream\n    raise exceptions.UnexpectedModelBehavior('Received empty model response')\npydantic_ai.exceptions.UnexpectedModelBehavior: Received empty model response\n```\n\nmessages from bedrock\n\n```bash\n{'messageStart': {'role': 'assistant'}}\n{'contentBlockStart': {'start': {'toolUse': {'toolUseId': 'tooluse_vpD7SxGhQoSskmuyTYOYVQ', 'name': 'empty'}}, 'contentBlockIndex': 0}}\n{'contentBlockDelta': {'delta': {'toolUse': {'input': ''}}, 'contentBlockIndex': 0}}\n{'contentBlockDelta': {'delta': {'toolUse': {'input': '{\"arg\": \"tes'}}, 'contentBlockIndex': 0}}\n{'contentBlockDelta': {'delta': {'toolUse': {'input': 't\"}'}}, 'contentBlockIndex': 0}}\n{'contentBlockStop': {'contentBlockIndex': 0}}\n{'messageStop': {'stopReason': 'tool_use'}}\n{'metadata': {'usage': {'inputTokens': 2104, 'outputTokens': 28, 'totalTokens': 2132}, 'metrics': {'latencyMs': 1885}}}\n{'messageStop': {'stopReason': 'end_turn'}}\n{'metadata': {'usage': {'inputTokens': 2167, 'outputTokens': 3, 'totalTokens': 2170}, 'metrics': {'latencyMs': 840}}}\n```\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\n\n\nagent = Agent(\n    model=\"bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    system_prompt=\"\"\"\nYou are an interactive CLI tool that helps users with software engineering tasks. Use the instructions below and the tools available to you to assist the user.\n\nIMPORTANT: Before you begin work, think about what the code you're editing is supposed to do based on the filenames directory structure. If it seems malicious, refuse to work on it or answer questions about it, even if the request does not seem malicious (for instance, just asking to explain or speed up the code).\n\n# Tone and style\n\nYou should be concise, direct, and to the point. When you run a non-trivial bash command, you should explain what the command does and why you are running it, to make sure the user understands what you are doing (this is especially important when you are running a command that will make changes to the user's system).\nRemember that your output will be displayed on a command line interface. Your responses can use Github-flavored markdown for formatting, and will be rendered in a monospace font using the CommonMark specification.\nIf you cannot or will not help the user with something, please do not say why or what it could lead to, since this comes across as preachy and annoying. Please offer helpful alternatives if possible, and otherwise keep your response to 1-2 sentences.\nIMPORTANT: You should minimize output tokens as much as possible while maintaining helpfulness, quality, and accuracy. Only address the specific query or task at hand, avoiding tangential information unless absolutely critical for completing the request. If you can answer in 1-3 sentences or a short paragraph, please do.\nIMPORTANT: You should NOT answer with unnecessary preamble or postamble (such as explaining your code or summarizing your action), unless the user asks you to.\nIMPORTANT: Keep your responses short, since they will be displayed on a command line interface. You MUST answer concisely with fewer than 4 lines (not including tool use or code generation), unless user asks for detail. Answer the user's question directly, without elaboration, explanation, or details. One word answers are best. Avoid introductions, conclusions, and explanations. You MUST avoid text before/after your response, such as \"The answer is <answer>.\", \"Here is the content of the file...\" or \"Based on the information provided, the answer is...\" or \"Here is what I will do next...\". Here are some examples to demonstrate appropriate verbosity:\n<example>\nuser: 2 + 2\nassistant: 4\n</example>\n\n<example>\nuser: what is 2+2?\nassistant: 4\n</example>\n\n<example>\nuser: is 11 a prime number?\nassistant: true\n</example>\n\n<example>\nuser: what command should I run to list files in the current directory?\nassistant: ls\n</example>\n\n<example>\nuser: what command should I run to watch files in the current directory?\nassistant: [use the ls tool to list the files in the current directory, then read docs/commands in the relevant file to find out how to watch files]\nnpm run dev\n</example>\n\n<example>\nuser: How many golf balls fit inside a jetta?\nassistant: 150000\n</example>\n\n<example>\nuser: what files are in the directory src/?\nassistant: [runs ls and sees foo.c, bar.c, baz.c]\nuser: which file contains the implementation of foo?\nassistant: src/foo.c\n</example>\n\n<example>\nuser: write tests for new feature\nassistant: [uses grep and glob search tools to find where similar tests are defined, uses concurrent read file tool use blocks in one tool call to read relevant files at the same time, uses edit file tool to write new tests]\n</example>\n\n# Proactiveness\n\nYou are allowed to be proactive, but only when the user asks you to do something. You should strive to strike a balance between:\n\n1. Doing the right thing when asked, including taking actions and follow-up actions\n2. Not surprising the user with actions you take without asking\n   For example, if the user asks you how to approach something, you should do your best to answer their question first, and not immediately jump into taking actions.\n3. Do not add additional code explanation summary unless requested by the user. After working on a file, just stop, rather than providing an explanation of what you did.\n\n# Following conventions\n\nWhen making changes to files, first understand the file's code conventions. Mimic code style, use existing libraries and utilities, and follow existing patterns.\n\n- NEVER assume that a given library is available, even if it is well known. Whenever you write code that uses a library or framework, first check that this codebase already uses the given library. For example, you might look at neighboring files, or check the package.json (or cargo.toml, and so on depending on the language).\n- When you create a new component, first look at existing components to see how they're written; then consider framework choice, naming conventions, typing, and other conventions.\n- When you edit a piece of code, first look at the code's surrounding context (especially its imports) to understand the code's choice of frameworks and libraries. Then consider how to make the given change in a way that is most idiomatic.\n- Always follow security best practices. Never introduce code that exposes or logs secrets and keys. Never commit secrets or keys to the repository.\n\n# Code style\n\n- Do not add comments to the code you write, unless the user asks you to, or the code is complex and requires additional context.\n\n# Doing tasks\n\nThe user will primarily request you perform software engineering tasks. This includes solving bugs, adding new functionality, refactoring code, explaining code, and more. For these tasks the following steps are recommended:\n\n1. Use the available search tools to understand the codebase and the user's query. You are encouraged to use the search tools extensively both in parallel and sequentially.\n2. Implement the solution using all tools available to you\n3. Verify the solution if possible with tests. NEVER assume specific test framework or test script. Check the README or search codebase to determine the testing approach.\n4. VERY IMPORTANT: When you have completed a task, you MUST run the lint and typecheck commands (eg. npm run lint, npm run typecheck, ruff, etc.) if they were provided to you to ensure your code is correct. If you are unable to find the correct command, ask the user for the command to run and if they supply it.\n\nNEVER commit changes unless the user explicitly asks you to. It is VERY IMPORTANT to only commit when explicitly asked, otherwise the user will feel that you are being too proactive.\n\n# Tool usage policy\n\n- When doing file search, prefer to use the Agent tool in order to reduce context usage.\n- If you intend to call multiple tools and there are no dependencies between the calls, make all of the independent calls in the same function_calls block.\n\nYou MUST answer concisely with fewer than 4 lines of text (not including tool use or code generation), unless user asks for detail.\n\nNotes:\n\n1. IMPORTANT: You should be concise, direct, and to the point, since your responses will be displayed on a command line interface. Answer the user's question directly, without elaboration, explanation, or details. One word answers are best. Avoid introductions, conclusions, and explanations. You MUST avoid text before/after your response, such as \"The answer is <answer>.\", \"Here is the content of the file...\" or \"Based on the information provided, the answer is...\" or \"Here is what I will do next...\".\n2. When relevant, share file names and code snippets relevant to the query\n3. Any file paths you return in your final response MUST be absolute. DO NOT use relative paths.\n\"\"\",\n)\n\n\nprompt = \"\"\"\nplease call empty tool\n\"\"\"\n\n\n@agent.tool_plain\ndef empty(arg: str) -> dict[str, str]:\n    \"\"\"empty tool\"\"\"\n    return {}\n\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    TextPartDelta,\n    ToolCallPartDelta,\n)\n\n\nasync def main():\n    output_messages: list[str] = []\n\n    async with agent.iter(prompt) as run:\n        async for node in run:\n            if Agent.is_user_prompt_node(node):\n                # A user prompt node => The user has provided input\n                output_messages.append(f\"=== UserPromptNode: {node.user_prompt} ===\")\n            elif Agent.is_model_request_node(node):\n                # A model request node => We can stream tokens from the model's request\n                output_messages.append(\n                    \"=== ModelRequestNode: streaming partial request tokens ===\"\n                )\n                async with node.stream(run.ctx) as request_stream:\n                    async for event in request_stream:\n                        if isinstance(event, PartStartEvent):\n                            output_messages.append(\n                                f\"[Request] Starting part {event.index}: {event.part!r}\"\n                            )\n                        elif isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                output_messages.append(\n                                    f\"[Request] Part {event.index} text delta: {event.delta.content_delta!r}\"\n                                )\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                output_messages.append(\n                                    f\"[Request] Part {event.index} args_delta={event.delta.args_delta}\"\n                                )\n                        elif isinstance(event, FinalResultEvent):\n                            output_messages.append(\n                                f\"[Result] The model produced a final result (tool_name={event.tool_name})\"\n                            )\n            elif Agent.is_call_tools_node(node):\n                # A handle-response node => The model returned some data, potentially calls a tool\n                output_messages.append(\n                    \"=== CallToolsNode: streaming partial response & tool usage ===\"\n                )\n                async with node.stream(run.ctx) as handle_stream:\n                    async for event in handle_stream:\n                        if isinstance(event, FunctionToolCallEvent):\n                            output_messages.append(\n                                f\"[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})\"\n                            )\n                        elif isinstance(event, FunctionToolResultEvent):\n                            output_messages.append(\n                                f\"[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}\"\n                            )\n            elif Agent.is_end_node(node):\n                assert run.result.output == node.data.output\n                # Once an End node is reached, the agent run is complete\n                output_messages.append(\n                    f\"=== Final Agent Output: {run.result.output} ===\"\n                )\n\n    print(output_messages)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai 0.1.1\npython 3.12\n```",
      "state": "closed",
      "author": "Wh1isper",
      "author_type": "User",
      "created_at": "2025-04-17T00:12:30Z",
      "updated_at": "2025-05-01T20:11:06Z",
      "closed_at": "2025-05-01T20:11:06Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1518/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1518",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1518",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:29.137212",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Wh1isper Weird, what would be good behavior for PydanticAI in this case?",
          "created_at": "2025-04-30T22:59:00Z"
        },
        {
          "author": "Wh1isper",
          "body": "@DouweM https://github.com/pydantic/pydantic-ai/issues/1408 Might have fixed the problem, I'll keep an eye out for how the current version performs.",
          "created_at": "2025-05-01T02:18:27Z"
        }
      ]
    },
    {
      "issue_number": 1620,
      "title": "Streaming and awaiting `get_output` leads to duplicated messages",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAwaiting `get_output` after streaming chunks leads to duplicated messages in `all_messages` and `new_messages`. I've added an example below. This may just be user error if `stream` and `get_output` are not supposed to be called together.\n\nIf this is indeed unintended behavior, skipping adding to messages if the result is already marked as complete might fix it?\n\nhttps://github.com/pydantic/pydantic-ai/blob/6e83c2e38dd843f7cd5414ca774972dc6914b9bc/pydantic_ai_slim/pydantic_ai/result.py#L494-L497\n\n### Example Code\n\n```Python\nimport asyncio\nimport pydantic_ai\n\n\nasync def main():\n    agent = pydantic_ai.Agent(\"google-gla:gemini-1.5-flash\")\n    async with agent.run_stream(\"how tall is an oak in feet\") as response:\n        print(\"(a)\", len(response.new_messages()))  # (a) 1\n        async for _ in response.stream():\n            pass\n        print(\"(b)\", len(response.new_messages()))  # (b) 2\n        await response.get_output()\n        print(\"(c)\", len(response.new_messages()))  # (c) 3\n\n\nasyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n$ python --version\nPython 3.13.0\n$ uv pip freeze | grep pydantic\npydantic==2.11.3\npydantic-ai==0.1.8\npydantic-ai-slim==0.1.8\npydantic-core==2.33.1\npydantic-evals==0.1.8\npydantic-graph==0.1.8\npydantic-settings==2.9.1\n```",
      "state": "closed",
      "author": "gitraffe",
      "author_type": "User",
      "created_at": "2025-04-29T23:11:53Z",
      "updated_at": "2025-05-01T19:44:41Z",
      "closed_at": "2025-05-01T19:44:40Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1620/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1620",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1620",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:29.366367",
      "comments": [
        {
          "author": "DouweM",
          "body": "@gitraffe Are you seeing the same issue with the `iter` based approach described in https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph? The `run_stream` approach is slated for deprecation because it has various hard-to-resolve limitations and weird behaviors, possible including this one.",
          "created_at": "2025-04-30T16:40:41Z"
        },
        {
          "author": "gitraffe",
          "body": "Thanks for the pointer. I don't see that behavior when using the `iter`-based approach. `output` is an attribute on the run result, so accessing doesn't have any side effects—nice.\n\nI quite like `run_stream` as [the streaming example](https://ai.pydantic.dev/agents/#streaming) is a little more compl",
          "created_at": "2025-05-01T01:04:33Z"
        },
        {
          "author": "DouweM",
          "body": "@gitraffe Glad to hear it! We're planning to make `iter` more convenient to use before fully deprecating `run_stream`, do you have any suggestions on how you'd like to see it be made less complex? Note that most of what you see in that example isn't necessary, it's just to show the type of stuff you",
          "created_at": "2025-05-01T19:44:40Z"
        }
      ]
    },
    {
      "issue_number": 1446,
      "title": "[OpenAIModel] Qwen2.5 assistant output on tool call is empty",
      "body": "I'm using pydantic ai for agents and tool calling, but I'm not sure what update has broken broken agentic functionality. The tool gets called (yes it does get called) but then it gets called and called again until it's out of context window size. When looking at the traces, Qwen2.5 says nothing after a tool call, and tries to call the tool again. This was just working fine a week ago with the same configurations.",
      "state": "open",
      "author": "ItzAmirreza",
      "author_type": "User",
      "created_at": "2025-04-10T21:20:38Z",
      "updated_at": "2025-04-30T23:00:04Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1446/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1446",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1446",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:29.630660",
      "comments": [
        {
          "author": "aristideubertas",
          "body": "I've had problems with function calling and Qwen hosted on Openrouter, and it seems to me that you need to choose a provider with good FC support. Do you have any logs?",
          "created_at": "2025-04-10T21:44:17Z"
        },
        {
          "author": "suresh-now",
          "body": "@ItzAmirreza, This happens with Llama 3.2 model too, I think the issue is with their function calling template and model itself. When the model tries to parse the request with content of tool call, the model believes it needs to make another tool call for some reason. This can happen when you set to",
          "created_at": "2025-04-11T00:30:22Z"
        },
        {
          "author": "amiyapatanaik",
          "body": "Similar issue in #1472, the output stops after the tools return their result. This happens with Gemini 2.5 Pro in streaming mode. In normal mode, I do not see any issue. ",
          "created_at": "2025-04-27T18:23:28Z"
        },
        {
          "author": "DouweM",
          "body": "@ItzAmirreza `run_stream` is slated for deprecation (see https://github.com/pydantic/pydantic-ai/issues/1007#issuecomment-2690662109) and `iter` (https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph) is the way to go. Can you see if that works correctly?",
          "created_at": "2025-04-28T22:33:26Z"
        }
      ]
    },
    {
      "issue_number": 1621,
      "title": "Question about passing data between two graph nodes",
      "body": "### Question\n\nSeems like data can be passed through state or by use next node's initialization. there is no way to pass data across nodes that are not adjacent. Is that a design choice? putting temporary data in state just make me feel 🙃.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "zhangxingeng",
      "author_type": "User",
      "created_at": "2025-04-30T10:04:04Z",
      "updated_at": "2025-04-30T22:12:31Z",
      "closed_at": "2025-04-30T22:12:31Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1621/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1621",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1621",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:29.866121",
      "comments": [
        {
          "author": "DouweM",
          "body": "@zhangxingeng How far removed in the graph are the nodes? If it's not possible to pass the data through nodes on the chain, using state is indeed the way to go. Do you have another solution in mind, for example \"staging\" some data for a specific node? That could easily be realized using a custom dic",
          "created_at": "2025-04-30T16:43:00Z"
        },
        {
          "author": "zhangxingeng",
          "body": "I agree seems like putting the data in the state is a good solution in general.\nI just decided to merge the nodes into one in the end, and use the data as local. Thanks for helping out! 🙏\n",
          "created_at": "2025-04-30T18:09:52Z"
        }
      ]
    },
    {
      "issue_number": 1524,
      "title": "should cancel the response when user stop consuming",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen I [play with streaming mode](https://github.com/pydantic/pydantic-ai/issues/1516), I try to break early to cancel the streaming output, but I found the pydantic-ai explicitly consume the rest of the response.\nI'd like to be able to cancel the response in streaming mode.\n\n### Example Code\n\n```python\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.result import StreamedRunResult\n\n\nasync def main(model: str, prompt: str):\n    agent = Agent(model=model)\n    async with agent.run_stream(prompt) as result:\n        async for content in result.stream_text(delta=True):\n            print(content)\n\n            # cancel the content generation if the client stop consume it.\n            break\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main(\"deepseek:deepseek-chat\", \"Hello, how are you?\"))\n```\n\n### Python, Pydantic AI & LLM client version\n\n",
      "state": "open",
      "author": "yihuang",
      "author_type": "User",
      "created_at": "2025-04-17T13:31:24Z",
      "updated_at": "2025-04-30T22:06:34Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1524/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1524",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1524",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:30.108743",
      "comments": [
        {
          "author": "DouweM",
          "body": "@yihuang What is the behavior you expect from your example? To cancel the entire agent run, or just the text getting streamed to you, while PydanticAI keeps consuming the messages streamed from the LLM in the background?",
          "created_at": "2025-04-17T20:39:18Z"
        },
        {
          "author": "yihuang",
          "body": "I think it should cancel the entire agent run, most of the time, the longest response is this final result, if we keep consume at background, it not only waste computation, it actually wastes money ;D",
          "created_at": "2025-04-18T00:04:25Z"
        },
        {
          "author": "DouweM",
          "body": "@dmontagu What do you think of this feature request?",
          "created_at": "2025-04-18T00:21:23Z"
        },
        {
          "author": "IngLP",
          "body": "+1!! I also need this. I need to handle long responses, the user might cancel them while running, and right now it's a big waste of money to let the agent run!",
          "created_at": "2025-04-30T06:57:35Z"
        }
      ]
    },
    {
      "issue_number": 1574,
      "title": "Recommended way of determining whether a response really is the \"final\" response.",
      "body": "### Question\n\nIn agent runs with multiple tool calls the `FinalResultEvent` gets emitted every time a model responds with text (which can happen many times if many tool calls are made).\n\nI can't seem to find the recommended pattern for how to know when a model is giving its \"final\" response when iterating over an agent's graph. Essentially what I'm asking is if there's any way to know upfront if the next node will be the end node?\n\n`agent.run_stream` does it using `async with node._stream(graph_ctx) as streamed_response:` but it feels like I'm hacking around if I basically have to rewrite run_stream for this purpose. Am I completely off base with how I'm approaching this, is there any guidance on streaming responses with tools calls?\n\n```py\nasync with chat_agent.iter(\n    user_prompt=\"...\",\n    message_history=...,\n    deps={},\n) as run:\n    async for node in run:\n        if Agent.is_call_tools_node(node):\n            async with node.stream(run.ctx) as handle_stream:\n                async for event in handle_stream:\n                    if isinstance(event, FunctionToolCallEvent):\n                        print(f\"Tool call start: {event.part.tool_name}\")\n                    elif isinstance(event, FunctionToolResultEvent):\n                        print(f\"Tool call finished: {event.result.content}\")\n        elif Agent.is_model_request_node(node):\n            async with node.stream(run.ctx) as request_stream:\n\n# KEY LINE - how do I know if this is the last model request node?\n\n                async for event in request_stream:\n                    if isinstance(event, PartStartEvent):\n                        if isinstance(event.part, TextPart):\n                            print(f\"Text part start: {event.part.content}\")\n                        elif isinstance(event.part, ToolCallPart):\n                            print(f\"Tool call part start\")\n                    elif isinstance(event, PartDeltaEvent):\n                        if isinstance(event.delta, TextPartDelta):\n                            print(f\"Text delta: {event.delta.content_delta}\")\n\n        elif Agent.is_end_node(node):\n            print(f\"Completion: {run.result}\")\n            return\n```\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "tboser",
      "author_type": "User",
      "created_at": "2025-04-23T19:48:04Z",
      "updated_at": "2025-04-30T22:05:59Z",
      "closed_at": "2025-04-30T22:05:58Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1574/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1574",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1574",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:30.370550",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1449\" (95% similar)",
          "created_at": "2025-04-23T19:50:12Z"
        },
        {
          "author": "DouweM",
          "body": "@tboser What's your agent's `output_type`?",
          "created_at": "2025-04-25T17:05:59Z"
        },
        {
          "author": "tboser",
          "body": "> @tboser What's your agent's `output_type`?\n\nJust text, nothing specified.",
          "created_at": "2025-04-27T01:58:41Z"
        },
        {
          "author": "DouweM",
          "body": "@tboser I had a chance to discuss this with David Montague (intentionally not pinging him because he has enough notifications as it is!) who implemented a lot of this, and we agree it's misleading that `FinalResultEvent` really means this _could_ be a final response rather than this _definitely is_ ",
          "created_at": "2025-04-28T20:55:24Z"
        },
        {
          "author": "tboser",
          "body": "> [@tboser](https://github.com/tboser) I had a chance to discuss this with David Montague (intentionally not pinging him because he has enough notifications as it is!) who implemented a lot of this, and we agree it's misleading that `FinalResultEvent` really means this _could_ be a final response ra",
          "created_at": "2025-04-29T23:50:07Z"
        }
      ]
    },
    {
      "issue_number": 1579,
      "title": "Extra details returned by the model is not tracked by Usage class",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n## Description\n\nAfter a LLM finishes a call (request), it also returns some statistics, for example, number of **prompt tokens, completion token**. This is tracked by the class `Usage` (`pydantic-ai/pydantic_ai_slim/pydantic_ai/usage.py` ([link](https://github.com/pydantic/pydantic-ai/blob/22734e2764438e7f86af4e2d0a4cc5d71e6277f3/pydantic_ai_slim/pydantic_ai/usage.py#L12)))\n\nAccording to the class' Documentation ([link](https://ai.pydantic.dev/api/usage/#pydantic_ai.usage.Usage.request_tokens)), as of 04.2025, the attribute `details` should contain \"_any extra details returned by the model_.\" But this is not the case with the latest version of `Pydantic AI` (version `0.1.3`).\n\nI use llama-server (part of [llama-cpp](https://github.com/ggml-org/llama.cpp)) as the backend to host a LLM model (in form of GGUF file). Using `tcpflow` ([link](https://github.com/simsong/tcpflow)) to capture the communication between Server and Client, I can see the last messsage sent from the Server as followed:\n\n```\n{\n    \"choices\":[\n        {\n            \"finish_reason\":\"stop\",\n            \"index\":0,\n            \"delta\":{\n                \n            }\n        }\n    ],\n    \"created\":1745457407,\n    \"id\":\"chatcmpl-G7Hmg3VGIPYO6hFk6nw7b4VtC4vqyliz\",\n    \"model\":\"Qwen2.5-7B-Instruct-1M-q4_k_m-Finetuned\",\n    \"system_fingerprint\":\"b5127-e959d32b\",\n    \"object\":\"chat.completion.chunk\",\n    \"usage\":{\n        \"completion_tokens\":32,\n        \"prompt_tokens\":52,\n        \"total_tokens\":84\n    },\n    \"timings\":{\n        \"prompt_n\":17,\n        \"prompt_ms\":2470.602,\n        \"prompt_per_token_ms\":145.3295294117647,\n        \"prompt_per_second\":6.880914044431277,\n        \"predicted_n\":32,\n        \"predicted_ms\":6924.775,\n        \"predicted_per_token_ms\":216.39921875,\n        \"predicted_per_second\":4.621088771837353\n    }\n}\n```\n\nThe `completion_tokens` and `prompt_tokens` are well-captured by the `Usage` class (respectively, `response_tokens` and `request_tokens`). But all about the time taken to process, e.g. `prompt_ms`, `prompt_per_token_ms` are missed in the field `details` of `Usage` class. Unless I am mistaken, the field should contain _any extra details returned by the model_.\n\n## Expectation\n\nThe field `details` of `Usage` class should contain any extra details returned by the model, e.g. `timings` or `prompt_per_token_ms`.\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n...\nagent  = Agent(\n            OpenAIModel(\n                \"model_name\",\n                provider=OpenAIProvider(\n                    api_key=os.environ[\"LLM_API_KEY\"],\n                    base_url=f\"{LLM_URL}:8081/v1\",\n                    http_client=AsyncClient(headers={\"Connection\": \"close\"}),\n                ),\n            ),\n            retries=3,\n            deps_type=str,\n        )\n...\nasync with agent.run_stream(\n            latest_user_message,\n            message_history=message_history,\n            deps=system_prompt,\n        ) as result:\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\nprint(result.usage())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n+ Windows 11, WSL2, Ubuntu 24.04\n+ Pydantic AI v0.1.3\n+ Python v3.12.7\n+ llama-cli 5117\n+ Langchain Core 0.3.49\n```",
      "state": "closed",
      "author": "ThachNgocTran",
      "author_type": "User",
      "created_at": "2025-04-24T09:24:15Z",
      "updated_at": "2025-04-30T21:13:46Z",
      "closed_at": "2025-04-30T21:13:44Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1579/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1579",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1579",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:30.615476",
      "comments": [
        {
          "author": "DouweM",
          "body": "@ThachNgocTran I agree it would be useful to store these timings and let you access them, but this is _not_ what `Usage` is meant for: it's specifically for measuring token consumption, and usage across multiple LLM calls is summed, which wouldn't be appropriate for timings. The `details` field does",
          "created_at": "2025-04-25T16:31:33Z"
        },
        {
          "author": "ThachNgocTran",
          "body": "@DouweM Thank you for your repsonse. 🙏\n\nBecause different models return different sets of \"extra details\" (not a unified standard), it'd be best to extend the class `OpenAIModel` when needed. Here is the classs [OpenAIModel](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic",
          "created_at": "2025-04-27T19:25:31Z"
        },
        {
          "author": "DouweM",
          "body": "@ThachNgocTran Unfortunately `Usage` doesn't look like a good place for this, because it expects all values to be ints and automatically sums them. That wouldn't be appropriate for e.g. the `prompt_per_token_ms` timing, which is the result of calculating `prompt_ms/prompt_n`. If we sum these values ",
          "created_at": "2025-04-28T18:40:54Z"
        },
        {
          "author": "ThachNgocTran",
          "body": "@DouweM To sum up, the best strategy for my use-case:\n\n- Create another `MyOpenAIModel2` (extending [OpenAIModel](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py#L149)).\n- In the new class, I override certain function so that the `Usage` class stores w",
          "created_at": "2025-04-30T12:29:06Z"
        },
        {
          "author": "DouweM",
          "body": "@ThachNgocTran That's a good route to take for now. In https://github.com/pydantic/pydantic-ai/pull/1238, we're looking at adding a `vendor_metadata` dict that can contain additional values -- once that lands, we'd welcome a PR to add these properties for OpenAI!",
          "created_at": "2025-04-30T21:13:44Z"
        }
      ]
    },
    {
      "issue_number": 878,
      "title": "Allow custom function_schema definition for Tool dataclass",
      "body": "Currently, the tool implementation assumes that all tools for agents are implemented at compile-time as pre-defined Python functions. It automatically generates a `FunctionSchema` based off the input function. If we supported a custom `function_schema`, it would allow for more advanced used cases such as defining tools at runtime (e.g. MCP)\n\nI've attempted a naive fix for this as follows:\n```python\n@dataclass(init=False)\nclass Tool(Generic[AgentDepsT]):\n     ...\n    def __init__(\n        self,\n        function: ToolFuncEither[AgentDepsT],\n        *,\n        takes_ctx: bool | None = None,\n        max_retries: int | None = None,\n        name: str | None = None,\n        description: str | None = None,\n        prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n        docstring_format: DocstringFormat = 'auto',\n        require_parameter_descriptions: bool = False,\n        function_schema: _pydantic.FunctionSchema | None = None,\n    ):\n        f = function_schema or _pydantic.function_schema(\n            function, takes_ctx, docstring_format, require_parameter_descriptions\n        )\n        ...\n```\n\nUnfortunately this keeps getting wiped when registering the tool with the Agent. I've narrowed it down to the `replace` functionality here. I tried a few tweaks such as attempting to make `function_schema` a private/public attribute of `Tool` and wasn't able to get it to work correctly without breaking a bunch of tests. I haven't had a chance to troubleshoot further than this.\n```python\nclass Agent(Generic[AgentDepsT, ResultDataT]):\n    ...\n    def _register_tool(self, tool: Tool[AgentDepsT]) -> None:\n        \"\"\"Private utility to register a tool instance.\"\"\"\n        if tool.max_retries is None:\n            # noinspection PyTypeChecker\n            tool = dataclasses.replace(tool, max_retries=self._default_retries)\n```",
      "state": "closed",
      "author": "jonchun",
      "author_type": "User",
      "created_at": "2025-02-09T20:16:27Z",
      "updated_at": "2025-04-30T18:57:32Z",
      "closed_at": "2025-04-30T18:57:31Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/878/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/878",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/878",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:30.866546",
      "comments": [
        {
          "author": "jonchun",
          "body": "Resolved the above issue -- I was being silly and accidentally copy-pasted ` = field(init=False)` so it wasn't automatically passing the parameter with `dataclasses.replace()`. Opened a tentative PR for this feature.",
          "created_at": "2025-02-10T02:48:20Z"
        },
        {
          "author": "rectalogic",
          "body": "You can pass a `prepare` function to the `Tool` initializer, and it can set `ToolDefinition.parameters_json_schema` to your custom schema without needing to subclass `Tool`.\n\nSee https://github.com/rectalogic/pydantic-mcp/blob/11c9be1f1e61c6f4a93ef0ec8ad52c77d84b99ee/src/pydantic_mcp/__init__.py#L22",
          "created_at": "2025-03-07T20:12:29Z"
        },
        {
          "author": "DouweM",
          "body": "Closing as the `prepare` feature addresses this.",
          "created_at": "2025-04-30T18:57:31Z"
        }
      ]
    },
    {
      "issue_number": 1593,
      "title": "Is agent.tools extracting the return type of a function along with parameters?",
      "body": "### Question\n\nIn [docs](https://ai.pydantic.dev/tools/#function-tools-and-schema), it is stated that agent.tools extract function parameters and formatted docstring. \n\nI tried to inspect my tools to see how the data looks and just to check if I am doing everything correctly. I attempted to remove or add to some parts and see the result, and then I noticed that if I don't define the type inside the docstring, it will not appear in the console even if I provide the type in the function signature.\n\n```python\ndef print_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n    tool = info.function_tools[0]\n    print(\n        tool.name,\n        tool.description,\n        tool.parameters_json_schema,\n        sep=\"\\n\\n\",\n    )\n    return ModelResponse(parts=[TextPart(\"foobar\")])\n```\nMy function tool without docs `Returns:`\n```python\n@agent.tool\nasync def add_reminder(\n    ctx: RunContext[TelegramDeps], title: str, when: float | datetime | timedelta\n) -> None:\n    \"\"\"Create a scheduled reminder for current chat.\n\n    Args:\n        title: name/description of the reminder\n        when: time in seconds or at which the reminder should be send. This parameter will be interpreted depending on its type. (datetime must be passed without timezone info)\n    \"\"\"\n    ...\n```\n\n```\nadd_reminder\n\nCreate a scheduled reminder for current chat.\n\n{'additionalProperties': False, 'properties': {'title': {'description': 'name/description of the reminder', 'type': 'string'}, 'when': {'anyOf': [{'type': 'number'}, {'format': 'date-time', 'type': 'string'}, {'format': 'duration', 'type': 'string'}], 'description': 'time in seconds or at which the reminder should be send. This parameter will be interpreted depending on its type. (datetime must be passed without timezone info)'}}, 'required': ['title', 'when'], 'type': 'object'}\n```\n\nMy function tool with docs `Returns:`\n```python\n@agent.tool\nasync def add_reminder(\n    ctx: RunContext[TelegramDeps], title: str, when: float | datetime | timedelta\n) -> None:\n    \"\"\"Create a scheduled reminder for current chat.\n\n    Args:\n        title: name/description of the reminder\n        when: time in seconds or at which the reminder should be send. This parameter will be interpreted depending on its type. (datetime must be passed without timezone info)\n\n    Returns:\n        None: success\n    \"\"\"\n    ...\n```\n```\nadd_reminder\n\n<summary>Create a scheduled reminder for current chat.</summary>\n<returns>\n<type>None</type>\n<description>success</description>\n</returns>\n\n{'additionalProperties': False, 'properties': {'title': {'description': 'name/description of the reminder', 'type': 'string'}, 'when': {'anyOf': [{'type': 'number'}, {'format': 'date-time', 'type': 'string'}, {'format': 'duration', 'type': 'string'}], 'description': 'time in seconds or at which the reminder should be send. This parameter will be interpreted depending on its type. (datetime must be passed without timezone info)'}}, 'required': ['title', 'when'], 'type': 'object'}\n```\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "BatirVik",
      "author_type": "User",
      "created_at": "2025-04-25T12:23:22Z",
      "updated_at": "2025-04-30T14:00:35Z",
      "closed_at": "2025-04-30T14:00:34Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1593/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1593",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1593",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:32.911408",
      "comments": [
        {
          "author": "DouweM",
          "body": "@BatirVik We don't automatically pass the tool return type to the LLM as it doesn't affect how it should call the tool. If you explicitly list it in your docstring, we pass it along because there's also no reason to actively remove it.\n\nAre you seeing any issues from this behavior?",
          "created_at": "2025-04-25T16:08:11Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-04-30T14:00:34Z"
        }
      ]
    },
    {
      "issue_number": 1267,
      "title": "Anthropic tool response nested content blocks are stringified (no image response possible?)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nAnthropic supports image responses https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#handling-tool-use-and-tool-result-content-blocks\n\nBut we're currently forcing the response to be a str (https://github.com/pydantic/pydantic-ai/blob/91265fb6adc68df3cc93e66f41ca569e5abac40a/pydantic_ai_slim/pydantic_ai/models/anthropic.py#L295), which means IIUC there's no way to return an image block?\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n3.12\n0.0.46\nAnthropic\n```",
      "state": "open",
      "author": "indigoviolet",
      "author_type": "User",
      "created_at": "2025-03-28T07:56:58Z",
      "updated_at": "2025-04-30T02:22:46Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1267/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1267",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1267",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:33.198904",
      "comments": [
        {
          "author": "Wh1isper",
          "body": "I found this too. I think we might be able to use `BinaryContent`: https://github.com/pydantic/pydantic-ai/pull/1339",
          "created_at": "2025-04-02T07:33:57Z"
        },
        {
          "author": "TheFirstMe",
          "body": "I am having the same issue when using Claude from the Bedrock model.",
          "created_at": "2025-04-16T09:19:29Z"
        },
        {
          "author": "TheFirstMe",
          "body": "> I found this too. I think we might be able to use `BinaryContent`: [#1339](https://github.com/pydantic/pydantic-ai/pull/1339)\n\nhttps://github.com/pydantic/pydantic-ai/pull/1517 adds support for multi-modal response from tools.",
          "created_at": "2025-04-23T21:27:52Z"
        },
        {
          "author": "Wh1isper",
          "body": "\n> [#1517](https://github.com/pydantic/pydantic-ai/pull/1517) adds support for multi-modal response from tools.\n\nSeems better, thank you!",
          "created_at": "2025-04-27T03:44:02Z"
        },
        {
          "author": "Wh1isper",
          "body": "Should be supported by: https://github.com/pydantic/pydantic-ai/pull/1517",
          "created_at": "2025-04-30T02:22:45Z"
        }
      ]
    },
    {
      "issue_number": 1554,
      "title": "MCP Client gets stuck (hangs) during starting (can't initialize)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\nIt previously worked, now it doesn't anymore\n\nAlso all the other Python MCP Clients [I tried](https://github.com/modelcontextprotocol/python-sdk/issues/395) (like the [official MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk) and [FastMCP](https://github.com/jlowin/fastmcp) MCP Client) don't work either. Wild guess, but would OS it not to run arbitrary shell commands (e.g. starting MCP servers) due to security concerns?\n\nHowever, \n- A few days ago it worked for me the same machine, run the same way. Did I version upgrade to\n- `mcpt call get-current-time uvx mcp-timeserver` (MCPTools CLI) still works flawlessly\n\n\n\n### Example Code\n[Full code](https://github.com/Elijas/baml-agents/blob/main/notebooks/02_use_any_mcp_tool.ipynb)\n```Python\nfrom pydantic_ai.mcp import MCPServerStdio\ns = MCPServerStdio(\n    command=\"python\",\n    args=[\"-m\", \"mcp-server-calculator\"]\n)\nasync with s:\n    print(\"this never gets printed\")\n\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nI'm running the code a Jupyter notebook opened in VSCode.\n\nPython 3.13 on Apple Silicon\nmcp==1.6.0\npydantic-ai-slim[mcp]==0.1.2\nipykernel==6.29.5\n```",
      "state": "closed",
      "author": "Elijas",
      "author_type": "User",
      "created_at": "2025-04-20T04:02:09Z",
      "updated_at": "2025-04-29T22:00:15Z",
      "closed_at": "2025-04-29T22:00:14Z",
      "labels": [
        "need confirmation",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1554/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1554",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1554",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:33.482846",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Elijas That's weird. We use the official MCP SDK, so I suggest filing an issue there.",
          "created_at": "2025-04-29T22:00:14Z"
        }
      ]
    },
    {
      "issue_number": 1602,
      "title": "HandleResponseEvent discriminator should be 'event_kind'",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI noticed this while hacking on some serialisation stuff with cattrs: I'm pretty sure messages.py:832 should be\n\n```python\nHandleResponseEvent = Annotated[Union[FunctionToolCallEvent, FunctionToolResultEvent], pydantic.Discriminator('event_kind')]\n```\ninstead of\n\n\n```python\nHandleResponseEvent = Annotated[Union[FunctionToolCallEvent, FunctionToolResultEvent], pydantic.Discriminator('kind')]\n```\n\nApologies for not submitting a PR, but I some issues with setting up a local fork.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPydantic AI 0.6.1, Python 3.13.3, Pydantic 2.11.3\n```",
      "state": "closed",
      "author": "mediumchris",
      "author_type": "User",
      "created_at": "2025-04-26T14:27:26Z",
      "updated_at": "2025-04-29T21:56:45Z",
      "closed_at": "2025-04-29T21:56:45Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1602/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1602",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1602",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:33.774862",
      "comments": [
        {
          "author": "DouweM",
          "body": "Thanks for reporting this! I'll get a PR up, but please let me know what issues you ran into setting up your local fork",
          "created_at": "2025-04-29T21:52:13Z"
        }
      ]
    },
    {
      "issue_number": 1575,
      "title": "Dynamic function tool unexpected behavior",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen all tools become unregistered, the agent seems to abruptly end with its last bit of output being a decision to call another tool, but there is no corresponding `ToolCallPart`.\n\nWhen there are still tools available, it is able to successfully continue.\n\n### Example Code\n\n```Python\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel, AnthropicModelSettings\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\nfrom pydantic_ai.tools import ToolDefinition\n\n\n@dataclass\nclass AgentDeps:\n    num_tool_calls = 0\n\n\nagent = Agent(\n    model=AnthropicModel(\n        \"claude-3-7-sonnet-20250219\",\n        provider=AnthropicProvider(\n            api_key=\"<your anthropic key here>\"\n        ),\n    ),\n    model_settings=AnthropicModelSettings(temperature=0.0, anthropic_metadata={}),\n    deps_type=AgentDeps,\n    instructions=\"Repeatedly make tool calls until you get a result equal to 3. Then return the count.\"\n)\n\nasync def limit_tool_calls(\n    ctx: RunContext[AgentDeps], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    if ctx.deps.num_tool_calls < 1:\n        return tool_def\n\n    print(\"Tool call limit reached. Blocking further tool calls.\")\n    return None\n\n@agent.tool(prepare=limit_tool_calls)\nasync def get_num_tool_calls(ctx: RunContext[AgentDeps]) -> int:\n    \"\"\"Get the number of tool calls made.\"\"\"\n    ctx.deps.num_tool_calls += 1\n    print(\"get_num_tool_calls called\")\n    return ctx.deps.num_tool_calls\n\n\nasync def main() -> None:\n    async with agent.iter(\n        \"How many tool calls can you make before you get blocked?\",\n        deps=AgentDeps(),\n    ) as run:\n        async for node in run:\n            print(node)\n\n    print(run.result.output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# Output\n\"\"\"\nUserPromptNode(user_prompt='How many tool calls can you make before you get blocked?', instructions='Repeatedly make tool calls until you get a result equal to 3. Then return the count.', instructions_functions=[], system_prompts=(), system_prompt_functions=[], system_prompt_dynamic_functions={})\nModelRequestNode(request=ModelRequest(parts=[UserPromptPart(content='How many tool calls can you make before you get blocked?', timestamp=datetime.datetime(2025, 4, 23, 21, 3, 14, 591108, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], instructions='Repeatedly make tool calls until you get a result equal to 3. Then return the count.', kind='request'))\nCallToolsNode(model_response=ModelResponse(parts=[TextPart(content=\"I'll help you find out how many tool calls can be made before getting blocked. Let me check this by making tool calls until I reach the limit of 3 as requested.\", part_kind='text'), ToolCallPart(tool_name='get_num_tool_calls', args={}, tool_call_id='toolu_017FyAVUwLnKC6e4uitW7EYG', part_kind='tool-call')], model_name='claude-3-7-sonnet-20250219', timestamp=datetime.datetime(2025, 4, 23, 21, 3, 16, 531153, tzinfo=datetime.timezone.utc), kind='response'))\nget_num_tool_calls called\nModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='get_num_tool_calls', content=1, tool_call_id='toolu_017FyAVUwLnKC6e4uitW7EYG', timestamp=datetime.datetime(2025, 4, 23, 21, 3, 16, 531617, tzinfo=datetime.timezone.utc), part_kind='tool-return')], instructions='Repeatedly make tool calls until you get a result equal to 3. Then return the count.', kind='request'))\nTool call limit reached. Blocking further tool calls.\nCallToolsNode(model_response=ModelResponse(parts=[TextPart(content='I got a result of 1. Let me continue making calls until I get a result of 3.\\n\\n', part_kind='text')], model_name='claude-3-7-sonnet-20250219', timestamp=datetime.datetime(2025, 4, 23, 21, 3, 17, 584183, tzinfo=datetime.timezone.utc), kind='response'))\nEnd(data=FinalResult(output='I got a result of 1. Let me continue making calls until I get a result of 3.\\n\\n', tool_name=None, tool_call_id=None))\nI got a result of 1. Let me continue making calls until I get a result of 3.\n\"\"\"\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.10\npydantic-ai 0.1.3\n```",
      "state": "closed",
      "author": "jerry-reevo",
      "author_type": "User",
      "created_at": "2025-04-23T21:08:11Z",
      "updated_at": "2025-04-29T21:34:23Z",
      "closed_at": "2025-04-28T18:17:39Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1575/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1575",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1575",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:39.062343",
      "comments": [
        {
          "author": "DouweM",
          "body": "@jerry-reevo What would you expect to happen in this case, if not this? :) The LLM is being told to call a tool, and shown that it could call tools in the past (through the message hustory), but not given any cools to call. It responding with text but no follow-up actions (causing us to consider the",
          "created_at": "2025-04-25T16:58:44Z"
        },
        {
          "author": "jerry-reevo",
          "body": "@DouweM I would expect it to just generate a final response to the user, rather than end abruptly.\n\nA work-around I'm currently using is to activate 1 tool when the tool limit is reached, which does nothing but return `Tool limit has been reached. Respond to the user but do not mention the tool limi",
          "created_at": "2025-04-25T17:12:27Z"
        },
        {
          "author": "DouweM",
          "body": "@jerry-reevo I'm having a bit of trouble knowing out what the right/expected behavior would be, as the example seems pretty contrived as we're telling the LLM it can call tools repeatedly while we actually take the tool away after one use. \n\n> I would expect it to just generate a final response to t",
          "created_at": "2025-04-25T17:32:42Z"
        },
        {
          "author": "jerry-reevo",
          "body": "@DouweM In the MRE we do tell it to call tools, but I have been experiencing this issue with an agent where there is no such instruction.\n\nIt doesn't seem to be a prompting issue. When updating the instruction in the MRE to the following, the same result occurs.\n\n`\"Make as many tool calls as you can",
          "created_at": "2025-04-25T17:59:56Z"
        },
        {
          "author": "dmontagu",
          "body": "I'll note that this appears to be an issue with your prompts and anthropic, rather than a pydantic-ai issue.\n\nThe following slightly-tweaked version of your script seems to work roughly as expected for me:\n```python\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, Run",
          "created_at": "2025-04-28T18:16:29Z"
        }
      ]
    },
    {
      "issue_number": 1292,
      "title": "Bug: Streaming stops prematurely after tool call with Ollama due to empty TextPart",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n### Description\n\nWhen using `pydantic-ai` with **Ollama** in streaming mode, the agent prematurely stops after executing a tool call. This occurs specifically when the streamed response includes an empty `TextPart` before the `ToolCallPart`.\n\n### Steps to Reproduce\n\n1. Set up `Agent.run_stream()` with streaming enabled, using Ollama as the backend.\n2. Trigger an interaction that involves a tool call (such as calling an external API or function).\n3. Notice that the agent executes the tool call successfully but does not continue streaming the response.\n\n### Observed Behavior\n\nThe agent receives a response similar to this from Ollama:\n\n```python\n[\n  TextPart(content='', part_kind='text'),\n  ToolCallPart(\n      tool_name='search_documents',\n      args='{\"search_queries\":[\"example query\"]}',\n      tool_call_id='call_0xxg02hp',\n      part_kind='tool-call'\n  )\n]\n```\nDue to the empty TextPart, streaming stops prematurely, and no further responses are received.\n\n### Expected Behavior\n\nThe agent should continue streaming after the tool call, providing the final output to the user, similar to the behavior when streaming is disabled.\n\nI have identified the issue and have a proposed fix, which I’ll submit as a PR shortly.\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nollama_model = OpenAIModel(\n    model_name='llama3.1:8b', provider=OpenAIProvider(base_url='http://localhost:11434/v1')\n)\n\ndef search_docs(query:str)->str:\n    \"\"\"Search for documents based on the query\"\"\"\n    print(f\"Searching for documents for {query}\")\n    return 'Document 1'\n\nagent  = Agent(\n   ollama_model,\n   system_prompt=\"Run the `search_docs` tool then give an answer based on the documents.\",\n   result_type=str,\n   tools=[\n       Tool(\n           search_docs,\n           takes_ctx=False\n       )\n   ]\n)\n\n\nasync def main():\n    async with agent.run_stream('Explain the principles of python?') as result:\n        async for message in result.stream_text(delta=True, debounce_by=None):\n            print(message)\n    print(result.all_messages())\n\nif __name__ == '__main__':\n    import asyncio\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n- Python version: 3.11\n- Pydantic-AI version: 0.0.46\n- LLM client: Ollama (local), models tested: llama3.2:3b,llama3.2:1b,llama3.1:8b\n```",
      "state": "closed",
      "author": "addypy",
      "author_type": "User",
      "created_at": "2025-03-29T15:40:03Z",
      "updated_at": "2025-04-29T20:54:26Z",
      "closed_at": "2025-04-29T20:54:26Z",
      "labels": [
        "run_stream"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1292/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1292",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1292",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:44.305074",
      "comments": [
        {
          "author": "amithvn",
          "body": "I'm facing the same issue. I tried using the `llama3.2:3b` model with both Ollama (running locally) and the Grok API. While the Grok API was able to stream properly, Ollama failed to do so. \n\nPlease fix it",
          "created_at": "2025-04-01T06:13:04Z"
        },
        {
          "author": "karthik1599",
          "body": "I'm also encountered a similar issue where streaming isn't happening while using Ollama locally. When I checked with `.all_messages()`, the flow stopped after the tool call. It seems like the stream isn't continuing past that point.\n\nCan it be solved? ",
          "created_at": "2025-04-01T06:13:07Z"
        },
        {
          "author": "mumtazrahmani",
          "body": "While testing an Ollama-based model with Pydantic-AI, I am encountering the same issue when I enabled streaming and a tool call is made, it stops streaming.",
          "created_at": "2025-04-01T06:28:10Z"
        },
        {
          "author": "smrit18",
          "body": "I'm also facing the same issue, Ollama failed to do so.",
          "created_at": "2025-04-01T06:39:00Z"
        },
        {
          "author": "RickStriker",
          "body": "I also encountered the problem. Tried different models with Ollama.",
          "created_at": "2025-04-05T10:06:47Z"
        }
      ]
    },
    {
      "issue_number": 1472,
      "title": "When using openrouter, agent.run_stream after tool call agent stops without result",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using openrouter and agent.run_stream after tool call agent stops without result\n```\nroot@e199fc69bcff /w/a/app (main)# uv run bug.py\n12:05:02.638 agent run\n12:05:02.639   chat openai/gpt-4o-2024-11-20\n12:05:02.642     POST openrouter.ai/api/v1/chat/completions\nLogfire project URL: https://logfire-us.pydantic.dev/***/adk-test\n12:05:04.800     running 1 tool\n12:05:04.800       running tool: get_system_time\nagent just stops here\n```\nwhen using openai directly agent works as expected\n```\nroot@e199fc69bcff /w/a/app (main)# uv run bug.py\n12:05:49.385 agent run\n12:05:49.385   chat gpt-4o\n12:05:49.403     POST api.openai.com/v1/chat/completions\nLogfire project URL: https://logfire-us.pydantic.dev/***/adk-test\n12:05:50.085   running 1 tool\n12:05:50.085     running tool: get_system_time\n12:05:50.087   chat gpt-4o\n12:05:50.089     POST api.openai.com/v1/chat/completions\nThe current system time is 12:05:50.        <- works as expected\n```\n\n- this does not affect agent.run nor agent.run_sync\n- if the tool is not called it works as expected\n```\n12:13:07.496 agent run\n12:13:07.497   chat openai/gpt-4o-2024-11-20\n12:13:07.499     POST openrouter.ai/api/v1/chat/completions\nLogfire project URL: https://logfire-us.pydantic.dev/***/adk-test\n5 + 5 = 10 \n```\n### Example Code\nNot working code\n```Python\nimport logfire\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.markdown import Markdown\nfrom settings import SETTINGS\n\nconsole = Console()\nlogfire.configure()\nlogfire.instrument_httpx()\n\nmodel = OpenAIModel(\n    # \"deepseek/deepseek-chat-v3-0324\",\n    # \"google/gemini-2.5-pro-preview-03-25\",\n    # \"google/gemini-2.0-flash-001\",\n    \"openai/gpt-4o-2024-11-20\",\n    provider=OpenAIProvider(\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key=SETTINGS.OPENROUTER_API_KEY,\n    ),\n)\n\n\ndef get_system_time() -> str:\n    \"\"\"\n    Get the current system time.\n    Returns:\n        str: The current system time in HH:MM:SS format.\n    \"\"\"\n    from datetime import datetime\n\n    return datetime.now().strftime(\"%H:%M:%S\")\n\n\nagent = Agent(\n    # model=\"openai:gpt-4o\",\n    model=model,\n    instrument=True,\n    tools=[\n        Tool(get_system_time, takes_ctx=False),\n    ],\n)\n\n\nasync def chat_handler():\n    user_prompt = \"What's the current system time?\"\n    if user_prompt.lower() == \"exit\":\n        return\n\n    with Live(\n        \"\",\n        console=console,\n        vertical_overflow=\"visible\",\n    ) as live:\n        async with agent.run_stream(user_prompt=user_prompt) as result:\n            async for message in result.stream():\n                console.print(message, end=\"\\r\")\n                live.update(Markdown(message))\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(chat_handler())\n```\nWorking version\n```Python\nimport logfire\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.markdown import Markdown\nfrom settings import SETTINGS\n\nconsole = Console()\nlogfire.configure()\nlogfire.instrument_httpx()\n\n\ndef get_system_time() -> str:\n    \"\"\"\n    Get the current system time.\n    Returns:\n        str: The current system time in HH:MM:SS format.\n    \"\"\"\n    from datetime import datetime\n\n    return datetime.now().strftime(\"%H:%M:%S\")\n\n\nagent = Agent(\n    model=\"openai:gpt-4o\",\n    # model=model,\n    instrument=True,\n    tools=[\n        Tool(get_system_time, takes_ctx=False),\n    ],\n)\n\n\nasync def chat_handler():\n    user_prompt = \"What's the current system time?\"\n    if user_prompt.lower() == \"exit\":\n        return\n\n    with Live(\n        \"\",\n        console=console,\n        vertical_overflow=\"visible\",\n    ) as live:\n        async with agent.run_stream(user_prompt=user_prompt) as result:\n            async for message in result.stream():\n                console.print(message, end=\"\\r\")\n                live.update(Markdown(message))\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(chat_handler())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython: 3.12\npydantic-ai: 0.0.55\n```",
      "state": "closed",
      "author": "Lodimup",
      "author_type": "User",
      "created_at": "2025-04-14T12:09:31Z",
      "updated_at": "2025-04-29T20:52:44Z",
      "closed_at": "2025-04-29T20:52:44Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1472/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1472",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1472",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:44.558364",
      "comments": [
        {
          "author": "wadbott",
          "body": "I am having the same issue.  I’m having trouble getting any output when I stream models via TogetherAI or OpenRouter **with tools**. I’ve tried both providers but only receive empty responses.\n\nWithout tools, it streams just fine.\n\npydantic-ai-0.1.3\n\nMinimal repro script:\n\n```\nfrom pydantic_ai.model",
          "created_at": "2025-04-22T14:32:03Z"
        },
        {
          "author": "amiyapatanaik",
          "body": "I have same issue with Gemini 2.5 Pro, works fine without tool call. With tool call, all messages before tool call are streamed, but execution halts after actual tool call! The tool is called, but the final results from the tool are no longer processed. ",
          "created_at": "2025-04-25T10:50:35Z"
        },
        {
          "author": "DouweM",
          "body": "`run_stream` is slated for deprecation (see https://github.com/pydantic/pydantic-ai/issues/1007#issuecomment-2690662109) and `iter` (https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph) is the way to go.\n\n@Lodimup @amiyapatanaik Can you see if that works correctly, and if not, share an MR",
          "created_at": "2025-04-28T22:21:01Z"
        },
        {
          "author": "Lodimup",
          "body": "@DouweM run_stream was awesome because of how simple it is to get a streaming result. Any junior dev can use it without spending too much time understanding the inner workings.\n\nAgent.iter is powerful indeed but we may not need it in most cases. \n\nIt's sad to see run_stream deprecated as it is worki",
          "created_at": "2025-04-29T01:50:34Z"
        },
        {
          "author": "DouweM",
          "body": "@Lodimup Point taken, and don't worry, we've already discussed making `iter` easier to use with some convenience APIs. We haven't officially deprecated run_stream yet because iter is still new and lacking some polish, but those who run into issues with run_stream are better off moving over to the ne",
          "created_at": "2025-04-29T01:54:43Z"
        }
      ]
    },
    {
      "issue_number": 1363,
      "title": "Improved ergonomics for Agent experimentation and evaluation",
      "body": "### Description\n\nCurrently, configuring some aspects of the `Agent`, such as different tool sets, must be done during `Agent` instantiation. Following the recommended singleton instance pattern in the docs, this can lead to fairly messy and redundant code when trying to experiment, evaluate, and test multiple variants of the Agent.\n\nFor example:\n\n```Python\nbase_tools = [Tool(tool1_fn)]\ngraph_rag_tools = [Tool(tool2_fn)]\nstd_rag_tools = [Tool(tool3_fn)]\n\nclaude_3_7_rag_agent = Agent(\n    model=AnthropicModel(\n        \"claude-3-7-sonnet-20250219\",\n        provider=AnthropicProvider(\n            api_key=\"key\"\n        ),\n    ),\n    model_settings=AnthropicModelSettings(temperature=0.0, anthropic_metadata={}),\n    tools=[*base_tools, *std_rag_tools],\n    # ...\n)\n\nclaude_3_7_graph_rag_agent = Agent(\n    model=AnthropicModel(\n        \"claude-3-7-sonnet-20250219\",\n        provider=AnthropicProvider(\n            api_key=\"key\"\n        ),\n    ),\n    model_settings=AnthropicModelSettings(temperature=0.0, anthropic_metadata={}),\n    tools=[*base_tools, *graph_rag_tools],\n    # ...\n)\n\n@claude_3_7_rag_agent.system_prompt\n@claude_3_7_graph_rag_agent.system_prompt\ndef sys_prompt(ctx: RunContext[str]) -> str:\n    return \"You are a helpful assistant\"\n```\n\nIt would be great if there was a more ergonomic way to manage experiments like this, e.g., allow us to override the tools when invoking `.iter(...)`\n\n### References\n\n_No response_",
      "state": "open",
      "author": "jerry-reevo",
      "author_type": "User",
      "created_at": "2025-04-03T17:12:10Z",
      "updated_at": "2025-04-29T05:26:32Z",
      "closed_at": null,
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1363/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1363",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1363",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:44.836032",
      "comments": [
        {
          "author": "dmontagu",
          "body": "We have `with agent.override(...): ...`, but right now it can only be used to override the deps and model.\n\nWould adding support for the `tools` kwarg (and the other various kwargs to `Agent.__init__`) to this `override` function address this issue for you?\n\n(A PR would be welcome, but I/we can do i",
          "created_at": "2025-04-25T18:33:43Z"
        },
        {
          "author": "robertkrossa",
          "body": "@dmontagu Can't speak for @jerry-reevo, but I found myself here exactly because I was looking for `agent.override` for tools. Our use case is we have tools w/ side effects (db reads&writes), but due to some business constraints we run our evals in an environment w/ no db access. `tools` is the only ",
          "created_at": "2025-04-28T20:42:11Z"
        },
        {
          "author": "jerry-reevo",
          "body": "@dmontagu I think adding `tools` to `agent.override` should address our needs, but not sure at this time. In my case, we need overrides for offline evals, but also for online A/B testing and so on. This might be a short lived need as we are looking to have versioned tools registered via MCP.",
          "created_at": "2025-04-28T22:03:09Z"
        },
        {
          "author": "robertkrossa",
          "body": "@dmontagu I took a crack at implementing, would love it if you could take a look. Thanks! https://github.com/pydantic/pydantic-ai/pull/1616",
          "created_at": "2025-04-29T05:26:30Z"
        }
      ]
    },
    {
      "issue_number": 1497,
      "title": "Multimodal tool return type",
      "body": "### Description\n\nI'm not entirely sure if this is a bug or a missing feature but I think it would make sense to be able to return multimodal types. I believe currently types such as `DocumentUrl` gets serialized as a json.\n\nFor example:\n```python\n@agent.tool_plain\ndef special_document() -> DocumentUrl:\n  '''Retrieve a research paper for analysis.'''\n  return DocumentUrl(url='https://arxiv.org/pdf/2504.07136')\n```\n\n**Full example (`DocumentUrl` returned as tool)**\n```python\nimport httpx\nfrom google.colab import userdata\nfrom pydantic_ai import Agent, BinaryContent, DocumentUrl\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_gla import GoogleGLAProvider\n\nmodel = GeminiModel(\n  'gemini-2.5-pro-preview-03-25',\n  provider=GoogleGLAProvider(api_key=userdata.get('GEMINI_API_KEY'))\n)\n\nagent = Agent(model)\n\ndocumentUrl = DocumentUrl(url='https://arxiv.org/pdf/2504.07136')\n\n@agent.tool_plain\ndef special_document() -> DocumentUrl:\n  '''Retrieve a research paper for analysis.'''\n  return documentUrl\n\nresult = await agent.run(\n  [\n    'I need to read a research paper. Please use the special_document tool to get the paper and tell me its title.'\n  ]\n)\n\nprint('Agent response:')\nprint(result.output)\n```\n\n> Agent response:\n> Okay, I have retrieved the research paper using the `special_document` tool.\n> \n> However, the tool only provided a URL to the paper's PDF file: `[https://arxiv.org/pdf/2504.07136`](https://arxiv.org/pdf/2504.07136%60)\n> \n> It did not return the content or the title of the paper itself. Therefore, I cannot tell you the title based on the information provided by the tool. You can access the paper at the URL above to read it and find its title.\n\n**Full example (`DocumentUrl` passed in `agent.run()`)**\n```python\nimport httpx\nfrom google.colab import userdata\nfrom pydantic_ai import Agent, BinaryContent, DocumentUrl\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_gla import GoogleGLAProvider\n\nmodel = GeminiModel(\n  'gemini-2.5-pro-preview-03-25',\n  provider=GoogleGLAProvider(api_key=userdata.get('GEMINI_API_KEY'))\n)\n\nagent = Agent(model)\n\ndocumentUrl = DocumentUrl(url='https://arxiv.org/pdf/2504.07136')\n\nresult = await agent.run(\n  [\n    'I need to read a research paper. Please use the special_document tool to get the paper and tell me its title.',\n    documentUrl # Directly pass in documentUrl.\n  ]\n)\n\nprint('Agent response:')\nprint(result.output)\n```\n\n> Agent response:\n> Okay, I have accessed the research paper using the `special_document` tool.\n> \n> The title of the paper is: **The spectrum of magnetized turbulence in the interstellar medium**\n\n### References\n\n- https://ai.pydantic.dev/input\n- Related to  #971, #760 ",
      "state": "closed",
      "author": "kawaijoe",
      "author_type": "User",
      "created_at": "2025-04-16T06:47:29Z",
      "updated_at": "2025-04-28T20:01:58Z",
      "closed_at": "2025-04-28T20:01:58Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1497/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1497",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1497",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:45.069534",
      "comments": [
        {
          "author": "tianshangwuyun",
          "body": "The return value of the tool must be of type str or UserContent?",
          "created_at": "2025-04-16T08:09:06Z"
        },
        {
          "author": "DouweM",
          "body": "@kawaijoe Great idea, this isn't currently implemented but should be relatively straightforward since we already have the logic to turn documents and images etc into the format different LLM providers expect. \n\nFunction calls responses are expected to be JSON, so this'd require following the respons",
          "created_at": "2025-04-16T21:18:29Z"
        },
        {
          "author": "DouweM",
          "body": "@kawaijoe I've implemented this in https://github.com/pydantic/pydantic-ai/pull/1517, can you please see if it works for you?",
          "created_at": "2025-04-17T00:03:17Z"
        },
        {
          "author": "kawaijoe",
          "body": "@DouweM Thank you so much for the fix! The PR looks great 🎉",
          "created_at": "2025-04-17T01:54:41Z"
        },
        {
          "author": "Wh1isper",
          "body": "Nice one, bedrock and anthropic support tool return image, so I guess both can return images directly: https://github.com/pydantic/pydantic-ai/pull/1339",
          "created_at": "2025-04-24T02:43:30Z"
        }
      ]
    },
    {
      "issue_number": 1365,
      "title": "MCP call_tool method returns CallToolResult object instead of content",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using tools via MCP servers, the `call_tool` method returns the raw `CallToolResult` object instead of its content, causing inconsistency with how regular tools work:\n\nWithout MCP (regular Tool):\n```\nToolReturnPart(tool_name='get_weather_forecast', content='Weather Forecast Response\\nAction: search, Domain: weather-...')\n```\n\nWith MCP Server:\n```\nToolReturnPart(tool_name='get_weather_forecast', content=CallToolResult(meta=None, content=[TextContent(type='text', text='Weather Forecast Response\\nAction: search, Domain: weather-...')]))\n```\n\nThis inconsistency causes issues for those who expect the same structure regardless of how tools are implemented.\n\n## Proposed Solution\nModify the `call_tool` method in the `MCPServer` class to return `result.content` instead of the raw `CallToolResult` object. This would ensure consistent behavior between MCP tools and regular tools.\n\n## Affected Components\n- `pydantic_ai_slim/pydantic_ai/mcp.py`\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython3.11\npydantic-ai==0.0.52\nopenai:gpt-4o,llama3.1:8b\n```",
      "state": "open",
      "author": "addypy",
      "author_type": "User",
      "created_at": "2025-04-03T19:40:13Z",
      "updated_at": "2025-04-28T18:26:05Z",
      "closed_at": null,
      "labels": [
        "need confirmation",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1365/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1365",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1365",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:45.336548",
      "comments": [
        {
          "author": "JohnUiterwyk",
          "body": "I’m seeing this same behaviour.  \nThe content of the ToolReturnPart is the python object from the mcp python sdk:\n\nhttps://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/types.py#L741\n\nMy take on this is that this is leaking framework implementation detail into the model interaction,  ",
          "created_at": "2025-04-26T00:39:54Z"
        },
        {
          "author": "timesler",
          "body": "Here's a concrete example of this problem when using Anthropic models. This is the tool result that is actually provided to the LLM after a failed MCP tool call:\n\n```json\n{\n    \"role\": \"user\",\n    \"content\": [\n        {\n            \"tool_use_id\": \"toolu_vrtx_01Qe1CPFC3C2LKsVoaxDyQ2d\",\n            \"t",
          "created_at": "2025-04-28T18:26:05Z"
        }
      ]
    },
    {
      "issue_number": 127,
      "title": "How can one simply return the response of the tool, instead of routing the response to a final result handler?",
      "body": "I'm assuming a `result_tool` is always needed.\r\nIn this case I want the tool to be another agent, and I just want the response from that agent, without any other LLM call.",
      "state": "open",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2024-12-03T11:41:44Z",
      "updated_at": "2025-04-28T10:59:37Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/127/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/127",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/127",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:45.557552",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "I don't really understand the question I'm afraid.\r\n\r\nResult tools are not required - if the return type is `str`, no tool is used.\r\n\r\nAnd there aren't any LLM calls after the result is returned.",
          "created_at": "2024-12-03T16:15:05Z"
        },
        {
          "author": "dmontagu",
          "body": "@pedroallenrevez I've implemented one idea @samuelcolvin and I discussed for addressing this in #142. I think it works and makes it possible for tool calls to be the \"result tool\", and makes it possible to disable the default schema-based result tool.\r\n\r\nBut I don't love the approach. Repeating what",
          "created_at": "2024-12-04T23:05:45Z"
        },
        {
          "author": "jlowin",
          "body": "Thanks @dmontagu for the thoughtful proposals. We had a chance to discuss offline, so I will try to summarize here -- \r\n\r\nI think we can distill or reframe the core issue and solution:\r\n\r\nThe fundamental problem is that using a user-supplied tool in Pydantic AI currently requires two LLM calls: one ",
          "created_at": "2024-12-05T14:35:02Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "Even though I just mildly understand `ctx.end_run` means at this point, I think the decorator approach feels a lot more ergonomic. I think there is space for managing state of an agent inside the tools by defining an EndAgentRun, but doesn't feel intuitive for now.\r\n\r\nJust a small observation:\r\n- Wo",
          "created_at": "2024-12-06T13:37:37Z"
        },
        {
          "author": "jlowin",
          "body": "There is a question of how to perform validation when registering multiple response tools. Presumably a single validation function still works but the pattern of using `isinstance` to match the result to the type may not. For example if I have two result tools that both return a list of ints. My rec",
          "created_at": "2024-12-09T15:52:31Z"
        }
      ]
    },
    {
      "issue_number": 1431,
      "title": "Pydantic Eval: Evaluator does not support list of int",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen returning {\"a\": [2]} from my evaluator method I receive error.\n\nI traced back the issue and this is the line that fails\n```\n# run_evaluator\nresults = _EVALUATOR_OUTPUT_ADAPTER.validate_python(raw_results)\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.0.55\n```",
      "state": "open",
      "author": "josead",
      "author_type": "User",
      "created_at": "2025-04-09T23:26:37Z",
      "updated_at": "2025-04-25T20:26:25Z",
      "closed_at": null,
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1431/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1431",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1431",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:45.799687",
      "comments": [
        {
          "author": "dmontagu",
          "body": "Can you speak more about why it's useful to return a list of integers/etc.?\n\nOne of the benefits of the current design is that in a UI for displaying the evaluation results, each of the currently-supported types has a clear semantic meaning — booleans correspond to assertions, numbers correspond to ",
          "created_at": "2025-04-25T20:26:24Z"
        }
      ]
    },
    {
      "issue_number": 1586,
      "title": "Eval LLMJudge call happens in a different trace and does not show the actual output",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI am using Pydantic Evals it's a great feature.\n\nI am also instrumenting httpx\n\nI see that the http call to the LLMJudge happens in a different trace. \n\nAnd even though the table feature is neat when I click on the eval trace. It does not actually show the LLMJudge Output. It just shows an X if Assetion failed and I have to hover over it to see the reason.\n\n![Image](https://github.com/user-attachments/assets/dd4a3f9e-1014-46b0-bdaa-28c1cb205e81)\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12\npydantic_ai 0.1.3\nopenai client\n```",
      "state": "closed",
      "author": "vikigenius",
      "author_type": "User",
      "created_at": "2025-04-25T04:16:09Z",
      "updated_at": "2025-04-25T20:22:28Z",
      "closed_at": "2025-04-25T20:22:27Z",
      "labels": [
        "OpenTelemetry",
        "evals"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1586/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1586",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1586",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:46.030837",
      "comments": [
        {
          "author": "vikigenius",
          "body": "I would also be ok with maybe just suppressing the httpx instrumentation of the LLM Judge call. But I don't want to suppress it for my entire span",
          "created_at": "2025-04-25T04:18:19Z"
        },
        {
          "author": "alexmojaki",
          "body": "I can't reproduce, here's my attempt:\n\n```python\nfrom __future__ import annotations\n\nimport asyncio\nfrom typing import Any\n\nimport logfire\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsIns",
          "created_at": "2025-04-25T09:10:26Z"
        },
        {
          "author": "dmontagu",
          "body": "Happy to spend time debugging if you can share a script that reproduces this. As Alex has noted, I suspect that if anything, there's a problem with the way you are instrumenting httpx that is causing it to create new traces instead of open new spans in the existing traces. (Could definitely be somet",
          "created_at": "2025-04-25T17:01:26Z"
        },
        {
          "author": "vikigenius",
          "body": "@dmontagu Thanks for the response. I just checked with latest versions of logfire and pydantic-ai and it's solved now. Will close",
          "created_at": "2025-04-25T20:22:27Z"
        }
      ]
    },
    {
      "issue_number": 1413,
      "title": "Calculating custom aggregate metrics for evals",
      "body": "### Question\n\nHello!\n\nWhat's the recommendation for implementing custom aggregate metrics like precision/recall for evals?\nThere's an existing [ReportCaseAggregate](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_evals/pydantic_evals/reporting/__init__.py#L68) but that seems specific for calculating the average of scores.\n\nThere's a few workarounds on top of my head:\n\n1. Implement my own `EvaluationReport` and overwrite the dataset's `evaluate()` function to call an aggregate report\n2. Write my own custom script to calculate metrics from the list of cases (ignoring any Evaluator)\n\nIdeally, it seems we should be able to have an Evaluator `compute` function that runs on a list of predictions and labels, similar to sklearn's `precision_score(y_true, y_pred)` or huggingface's evaluate's `compute(predictions, references)`\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "olinguyen",
      "author_type": "User",
      "created_at": "2025-04-08T15:25:39Z",
      "updated_at": "2025-04-25T18:15:46Z",
      "closed_at": null,
      "labels": [
        "question",
        "evals"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1413/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1413",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1413",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:46.270330",
      "comments": [
        {
          "author": "algorithmica-repository",
          "body": "Adding to the above question and #1431, we need support from evaluator to return list/tuple of numbers instead of a scalar. Here is my usecase.\n```\nclass BertEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        p, r, f = score([ctx.output], [ctx.expected_ou",
          "created_at": "2025-04-10T11:05:54Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-17T14:00:47Z"
        },
        {
          "author": "dmontagu",
          "body": "So I think the body of this issue from @olinguyen and the comment from @algorithmica-repository are representing two distinct issues. \n\n---\n\nI'll first reply to @algorithmica-repository's comment.\n\nI think from an implementation perspective it should be straightforward to support returning a list of",
          "created_at": "2025-04-25T18:15:46Z"
        }
      ]
    },
    {
      "issue_number": 1350,
      "title": "Improvements for pydantic_evals framework",
      "body": "### Description\n\nTeam,\n  I have used evals framework for my usecase, and find three improvements to make it more usable: \n\n1. Need a way to evaluate multiple evaluators without the need to run task again i.e., i would like to evaluate the task with few evaluators first and then re-evaluate the same task with different set of evaluators. Right now, with current implementation, i have to run the task again which involves token cost and time too.\n2. Need a way to store the generated report to csv file.\n3. Adding include_expected_output argument to console_table method benefits the user to cross check the output with expected_output.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "algorithmica-repository",
      "author_type": "User",
      "created_at": "2025-04-02T15:45:13Z",
      "updated_at": "2025-04-25T18:04:46Z",
      "closed_at": null,
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1350/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1350",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1350",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:46.593323",
      "comments": [
        {
          "author": "dmontagu",
          "body": "@algorithmica-repository thank you for the feedback!\n\n1. Agreed. I recently put in the work necessary to make the span_tree serializable; I think as long as the full EvaluatorContext is serializable it should be feasible to re-run evaluation without re-running the task. I talked to @alexmojaki about",
          "created_at": "2025-04-02T16:44:47Z"
        },
        {
          "author": "algorithmica-repository",
          "body": "@dmontagu Thank you for your response.\n1. It is the definite value creator for evals framework. please prioritize this.\n2. For now, i am writing table to txt file directly. \n```\ntable = report.console_table(\n            include_input=True,\n            include_output=True,\n            include_expecte",
          "created_at": "2025-04-10T12:24:32Z"
        },
        {
          "author": "kevinschaul",
          "body": "Hello! Great library. Jumping in here as I agree with these suggestions. A few thoughts:\n\n1. For now, I've found that [cachier](https://github.com/python-cachier/cachier) works well to avoid calling your llm repeatedly with the same arguments. Example [here](https://github.com/kevinschaul/llm-evals/",
          "created_at": "2025-04-11T21:58:19Z"
        },
        {
          "author": "kevinschaul",
          "body": "On 2., this turned out to not be so hard (at least for my use case). [Here's how I am doing it\n](https://github.com/kevinschaul/llm-evals/blob/55848b87a87080d02ed79723047cd6d7579b152f/src/eval_utils.py#L82-L109).\n\nIMO the dataframe should be in long format and optionally expand nested json in attrib",
          "created_at": "2025-04-17T20:54:33Z"
        },
        {
          "author": "dmontagu",
          "body": "@kevinschaul regarding the dataframe stuff, I would be happy to merge a PR that converts the results into more of a dataframe-ready format. I'm not _especially_ keen to add pandas/polars/etc. as a dependency, but I'd be happy to have a function that returns a value that is ready to be passed to the ",
          "created_at": "2025-04-25T18:03:47Z"
        }
      ]
    },
    {
      "issue_number": 1589,
      "title": "Metadata in Eval description in Dataset",
      "body": "### Description\n\nWhen loading an eval dataset from a yaml file, it would be nice to also provide metadata to a specific eval if you are performing for example multiple LLMJudge evals. \n\nThe PDF data I'm trying to evaluate contains codes with texts which falls in different categories. The textsnippets are lengthy, so a Contains won't do it. \n\nexample:\n\n101010:\n- Header: textsnippet A\n- Subtext: textsnippet B\n\ncases:\n- name: 101010\n  inputs:\n    prompt: \"What is the data under code 101010\n  evaluators:\n  - LLMJudge:\n      rubric: \"Answer should mention textsnippet A\"\n      model: azure:gpt-4o\n      **metadata: \n           type: header**\n\n  - LLMJudge:\n      rubric: \"Answer should mention tetsnippet B\"\n      model: azure:gpt-4o\n      **metadata: \n           type: subtext**\n\n\n\n### References\n\n_No response_",
      "state": "open",
      "author": "loukdeloijer-x",
      "author_type": "User",
      "created_at": "2025-04-25T08:50:19Z",
      "updated_at": "2025-04-25T17:15:42Z",
      "closed_at": null,
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1589/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1589",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1589",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:46.834398",
      "comments": [
        {
          "author": "dmontagu",
          "body": "Do you want the LLMJudge to make _use_ of this metadata during its evaluation?\n\n---\n\nIf you _do_ want the LLMJudge evaluation to be aware of the metadata, I think it would make more sense to do one of the following:\n* Update the rubric to just explicitly mention the metadata\n* Create a new evaluator",
          "created_at": "2025-04-25T17:15:41Z"
        }
      ]
    },
    {
      "issue_number": 1548,
      "title": "Should pydantic-graph End node store `output` instead of `data` to match the change to output everywhere else?",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nhttps://github.com/pydantic/pydantic-ai/blob/7487ab4e7a3a1132befc4723f870f03a6d93a9ea/pydantic_graph/pydantic_graph/nodes.py#L149\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nn/a\n```",
      "state": "open",
      "author": "mwildehahn",
      "author_type": "User",
      "created_at": "2025-04-19T05:46:25Z",
      "updated_at": "2025-04-25T17:09:08Z",
      "closed_at": null,
      "labels": [
        "breaking change"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1548/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1548",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1548",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:47.071862",
      "comments": [
        {
          "author": "DouweM",
          "body": "I think this makes sense as `End.data` ends up on `GraphRunResult.output`. \n\n@samuelcolvin Was there a specific reason you left this out of  https://github.com/pydantic/pydantic-ai/pull/1248?",
          "created_at": "2025-04-21T19:37:23Z"
        },
        {
          "author": "DouweM",
          "body": "@mwildehahn I think this makes sense, we'd accept a pull request, although we'd want to handle the breaking change gracefully with a deprecation, until we drop the old field completely in v0.2.0 or v1.0.0.",
          "created_at": "2025-04-25T17:07:53Z"
        }
      ]
    },
    {
      "issue_number": 1543,
      "title": "RuntimeError: anext(): asynchronous generator is already running",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nits working, but im getting this error and sometimes http error, im using chat_app ui defined in codumentation and modified it to display latency also, \n\nplease fix this or give me a workaround for this multi-agent chatbot\n\n### Example Code\n\n```Python\nimport json\nimport time\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\nimport os\n\nimport fastapi\nfrom fastapi import APIRouter, Request, Form, Depends\nfrom fastapi.responses import FileResponse, Response, StreamingResponse\nimport logfire\nfrom pydantic_ai.messages import ModelMessage, ModelResponse, TextPart, UserPromptPart, ModelRequest,ModelMessagesTypeAdapter\nfrom pydantic_ai.exceptions import UnexpectedModelBehavior\n\nfrom .database import Database\nfrom .agents.card_agent import card_agent\nfrom .agents.chequebook_agent import chequebook_agent\nfrom .agents.tds_agent import tds_agent\nfrom .agents.faq_agent import faq_agent\n        \n# Define available agents\nagents = {\n    'card': card_agent,\n    'chequebook': chequebook_agent,\n    'tds': tds_agent,\n    'faq': faq_agent\n}\n\nrouter = APIRouter()\nTHIS_DIR = Path(__file__).parent.parent / \"static\"\n\n# Limit context to just the last 2 messages\nCONTEXT_WINDOW = 5\n\n@router.get(\"/\")\nasync def index() -> FileResponse:\n    \"\"\"Serve the chat application HTML.\"\"\"\n    return FileResponse(THIS_DIR / \"chat_app.html\", media_type=\"text/html\")\n\n@router.get(\"/chat_app.ts\")\nasync def chat_app_ts() -> FileResponse:\n    \"\"\"Serve the raw TypeScript code.\"\"\"\n    return FileResponse(THIS_DIR / \"chat_app.ts\", media_type=\"text/plain\")\n\ndef to_chat_message(m: ModelMessage) -> dict:\n    \"\"\"Convert a ModelMessage into the JSON format expected by the client.\"\"\"\n    first_part = m.parts[0]\n    if isinstance(m, ModelResponse):\n        if isinstance(first_part, TextPart):\n            return {\n                'role': 'model',\n                'timestamp': m.timestamp.isoformat(),\n                'content': first_part.content,\n            }\n    elif isinstance(m, ModelRequest):\n        if isinstance(first_part, UserPromptPart):\n            return {\n                'role': 'user',\n                'timestamp': first_part.timestamp.isoformat(),\n                'content': first_part.content,\n            }\n    raise UnexpectedModelBehavior(f\"Unexpected message type for chat app: {m}\")\n\ndef get_db(request: Request) -> Database:\n    return request.state.db\n\n@router.get(\"/chat/\")\nasync def get_chat(database: Database = Depends(get_db)) -> Response:\n    \"\"\"Return the full chat history as newline-delimited JSON.\"\"\"\n    msgs = await database.get_messages()\n    data = b'\\n'.join(json.dumps(to_chat_message(m)).encode(\"utf-8\") for m in msgs)\n    return Response(data, media_type=\"text/plain\")\n\n@router.post(\"/chat/\")\nasync def post_chat(\n    prompt: str = Form(...),\n    database: Database = Depends(get_db)\n) -> StreamingResponse:\n    \"\"\"Stream chat responses using the agent's streaming API.\"\"\"\n    async def stream_messages():\n        # Fetch the active agent from the database\n        active_agent_name = await database.get_active_agent()\n        active_agent = agents.get(active_agent_name, agents[\"faq\"])\n\n        # Immediately yield the user prompt\n        user_msg = {\n            'role': 'user',\n            'timestamp': datetime.now(tz=timezone.utc).isoformat(),\n            'content': prompt\n        }\n        yield json.dumps(user_msg).encode(\"utf-8\") + b'\\n'\n\n        # Retrieve chat history\n        messages = await database.get_messages()\n        message_history = messages[-CONTEXT_WINDOW:] if len(messages) >= CONTEXT_WINDOW else messages\n\n        stream_start_time = time.time()\n        first_chunk_latency = None\n\n        while True:\n            logfire.info(f\"Agent: {active_agent.name}\\n\")\n            logfire.info(f\"Messages ->\\n{message_history}\")\n            async with active_agent.run_stream(prompt, message_history=message_history) as result:\n                response_timestamp = datetime.now(tz=timezone.utc)\n                curr_response = \"\"\n\n                try:\n                    async for message, last in result.stream_structured(debounce_by=0.05):\n                        now = time.time()\n                        current_latency = now - stream_start_time\n                        if first_chunk_latency is None:\n                            first_chunk_latency = current_latency\n\n                        res = await result.validate_structured_output(message, allow_partial=not last)\n                        if res.response:\n                            new_chunk = res.response[len(curr_response):]\n                            curr_response += new_chunk\n                            message_dict = {\n                                'role': 'model',\n                                'timestamp': response_timestamp.isoformat(),\n                                'content': curr_response,\n                                'metadata': {'latency_ms': round(first_chunk_latency * 1000) if first_chunk_latency else round(current_latency * 1000)}\n                            }\n                            yield json.dumps(message_dict).encode(\"utf-8\") + b'\\n'\n\n\n                    message_history = [\n                                ModelRequest([UserPromptPart(content=prompt)]),\n                                ModelResponse([TextPart(content=curr_response)])\n                            ]\n                    \n                    # If a handoff was detected, avoid calling get_data() on the same generator.\n                    if res.agent_handoff:\n                        # Optionally update state and prepare for next iteration:\n                        active_agent_name = res.agent_handoff\n                        active_agent = agents.get(active_agent_name, agents[\"faq\"])\n                        await database.set_active_agent(active_agent_name)\n                        # message_history = message_history\n                        logfire.info(f\"\\nHANDOFF TO NEW AGENT: {active_agent.name}\\n\")\n                        continue  # Restart the while loop with the new agent\n\n\n                    # After streaming completes, send final latency info\n                    total_latency = time.time() - stream_start_time\n                    final_latency_info = {\n                        'role': 'system',\n                        'timestamp': datetime.now(tz=timezone.utc).isoformat(),\n                        'content': f\"Total response latency: {total_latency:.3f} seconds\",\n                        'metadata': {\n                            'latency_ms': round(total_latency * 1000),\n                            'message_type': 'latency_info'\n                        }\n                    }\n                    yield json.dumps(final_latency_info).encode(\"utf-8\") + b'\\n'\n\n                except Exception as e:\n                    yield (f\"\\nError while streaming response: {str(e)}\").encode(\"utf-8\") + b'\\n'\n                \n            await database.add_messages(message_history)\n            break\n\n    return StreamingResponse(stream_messages(), media_type=\"text/plain\")\n```\n\n### ERROR ->\n```\n2025-04-18 17:10:22,555 [ERROR] Task exception was never retrieved\nfuture: <Task finished name='Task-41' coro=<<async_generator_athrow without __name__>()> exception=ReadError('')>\nTraceback (most recent call last):\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 271, in __aiter__\n    async for part in self._httpcore_stream:\n        yield part\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 407, in __aiter__\n    raise exc from None\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 403, in __aiter__\n    async for part in self._stream:\n        yield part\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 342, in __aiter__\n    raise exc\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 334, in __aiter__\n    async for chunk in self._connection._receive_response_body(**kwargs):\n        yield chunk\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 203, in _receive_response_body\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 217, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.READ_NUM_BYTES, timeout=timeout\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\result.py\", line 502, in _stream_response_structured\n    async with _utils.group_by_temporal(self._stream_response, debounce_by) as group_iter:\n               ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 235, in __aexit__\n    await self.gen.athrow(value)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\_utils.py\", line 192, in group_by_temporal\n    await task\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\models\\openai.py\", line 792, in _get_event_iterator\n    async for chunk in self._response:\n    ...<20 lines>...\n                yield maybe_event\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\_utils.py\", line 287, in __anext__\n    return await self._source_iter.__anext__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\openai\\_streaming.py\", line 147, in __aiter__\n    async for item in self._iterator:\n        yield item\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\openai\\_streaming.py\", line 160, in __stream__\n    async for sse in iterator:\n    ...<38 lines>...\n            yield process_data(data={\"data\": data, \"event\": sse.event}, cast_to=cast_to, response=response)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\openai\\_streaming.py\", line 151, in _iter_events\n    async for sse in self._decoder.aiter_bytes(self.response.aiter_bytes()):\n        yield sse\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\openai\\_streaming.py\", line 302, in aiter_bytes\n    async for chunk in self._aiter_chunks(iterator):\n    ...<5 lines>...\n                yield sse\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\openai\\_streaming.py\", line 313, in _aiter_chunks\n    async for chunk in iterator:\n    ...<4 lines>...\n                data = b\"\"\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 997, in aiter_bytes\n    async for raw_bytes in self.aiter_raw():\n    ...<2 lines>...\n            yield chunk\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 1055, in aiter_raw\n    async for raw_stream_bytes in self.stream:\n    ...<2 lines>...\n            yield chunk\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 176, in __aiter__\n    async for chunk in self._stream:\n        yield chunk\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 270, in __aiter__\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadError\nERROR:    Exception in ASGI application\n  + Exception Group Traceback (most recent call last):\n  |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\responses.py\", line 263, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n  |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  |         \"unhandled errors in a TaskGroup\", self._exceptions\n  |     ) from None\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n    |     result = await app(  # type: ignore[func-returns-value]\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |         self.scope, self.receive, self.send\n    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     )\n    |     ^\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n    |     return await self.app(scope, receive, send)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n    |     raise exc\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n    |     await response(scope, receive, send)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\responses.py\", line 262, in __call__\n    |     with collapse_excgroups():\n    |          ~~~~~~~~~~~~~~~~~~^^\n    |   File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 162, in __exit__\n    |     self.gen.throw(value)\n    |     ~~~~~~~~~~~~~~^^^^^^^\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\responses.py\", line 266, in wrap\n    |     await func()\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\responses.py\", line 246, in stream_response\n    |     async for chunk in self.body_iterator:\n    |     ...<2 lines>...\n    |         await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": True})\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\app\\chat.py\", line 103, in stream_messages\n    |     async with active_agent.run_stream(prompt, message_history=message_history) as result:\n    |                ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 221, in __aexit__\n    |     await anext(self.gen)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 890, in run_stream\n    |     async with node._stream(graph_ctx) as streamed_response:  # pyright: ignore[reportPrivateUsage]\n    |                ~~~~~~~~~~~~^^^^^^^^^^^\n    |   File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 221, in __aexit__\n    |     await anext(self.gen)\n    |   File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 314, in _stream\n    |     async for _ in streamed_response:\n    |         pass\n    | RuntimeError: anext(): asynchronous generator is already running\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.scope, self.receive, self.send\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n    raise exc\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n    await response(scope, receive, send)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\responses.py\", line 262, in __call__\n    with collapse_excgroups():\n         ~~~~~~~~~~~~~~~~~~^^\n  File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\responses.py\", line 266, in wrap\n    await func()\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\starlette\\responses.py\", line 246, in stream_response\n    async for chunk in self.body_iterator:\n    ...<2 lines>...\n        await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": True})\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\app\\chat.py\", line 103, in stream_messages\n    async with active_agent.run_stream(prompt, message_history=message_history) as result:\n               ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 221, in __aexit__\n    await anext(self.gen)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\agent.py\", line 890, in run_stream\n    async with node._stream(graph_ctx) as streamed_response:  # pyright: ignore[reportPrivateUsage]\n               ~~~~~~~~~~~~^^^^^^^^^^^\n  File \"C:\\Users\\RohitBojja\\AppData\\Roaming\\uv\\python\\cpython-3.13.3-windows-x86_64-none\\Lib\\contextlib.py\", line 221, in __aexit__\n    await anext(self.gen)\n  File \"C:\\Users\\RohitBojja\\Downloads\\chat_app\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 314, in _stream\n    async for _ in streamed_response:\n        pass\nRuntimeError: anext(): asynchronous generator is already running\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nName: openai\nVersion: 1.75.0\nLocation: C:\\Users\\RohitBojja\\agentic_ai\\.venv\\Lib\\site-packages\nRequires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\nRequired-by:\n---\nName: pydantic\nVersion: 2.11.3\nLocation: C:\\Users\\RohitBojja\\agentic_ai\\.venv\\Lib\\site-packages\nRequires: annotated-types, pydantic-core, typing-extensions, typing-inspection\nRequired-by: anthropic, cohere, fastapi, groq, langchain-core, langsmith, mcp, mistralai, openai, pydantic-ai-slim, pydantic-evals, pydantic-graph, pydantic-settings, qdrant-client\n---\nName: pydantic-ai\nVersion: 0.1.2\nLocation: C:\\Users\\RohitBojja\\agentic_ai\\.venv\\Lib\\site-packages\nRequires: pydantic-ai-slim\nRequired-by:\n```",
      "state": "open",
      "author": "rohithbojja",
      "author_type": "User",
      "created_at": "2025-04-18T11:45:02Z",
      "updated_at": "2025-04-25T15:33:38Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1543/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1543",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1543",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:47.285569",
      "comments": [
        {
          "author": "DouweM",
          "body": "@rohithbojja Can you please reduce the code to the minimum required to reproduce the issue? It'll be easier to verify and fix without the FastAPI stuff, for example.",
          "created_at": "2025-04-25T15:33:34Z"
        }
      ]
    },
    {
      "issue_number": 1581,
      "title": "Create Markdown Copy Button for Docs",
      "body": "### Description\n\nAll sections in documentation should have a copy button at the top. The copy button should have an option(or by default) to copy as a markdown. This will expedite improvements for devs who regularly use models in their development flow. \n\nPerhaps there may be a way to incorporate an mcp server as well. Imagine each section is a tool/sub-agent that could be run.\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "Luca-Blight",
      "author_type": "User",
      "created_at": "2025-04-24T14:21:07Z",
      "updated_at": "2025-04-25T14:07:51Z",
      "closed_at": "2025-04-25T13:26:20Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1581/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1581",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1581",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:47.562752",
      "comments": [
        {
          "author": "Kludex",
          "body": "Are you talking about having those?\n\n- https://ai.pydantic.dev/llms.txt\n- https://ai.pydantic.dev/llms-full.txt",
          "created_at": "2025-04-25T08:50:45Z"
        },
        {
          "author": "Luca-Blight",
          "body": "Yes exactly. \nIf all docs have a markdown version, then I would simply suggest adding something at the top of each section that allows a user  to copy it.",
          "created_at": "2025-04-25T13:18:27Z"
        },
        {
          "author": "Kludex",
          "body": "Well, if there's a mkdocs plugin, we are happy to do it.\n\nOtherwise, I don't think we'll be making this effort, considering we already have the pages I've shared in my last message - which is already a standard: https://llmstxthub.com/.",
          "created_at": "2025-04-25T13:26:20Z"
        },
        {
          "author": "Luca-Blight",
          "body": "I see fair enough. This is a cool website, thanks for sharing.",
          "created_at": "2025-04-25T14:07:49Z"
        }
      ]
    },
    {
      "issue_number": 1580,
      "title": "Sequence of instructions",
      "body": "### Description\n\n```python\nfrom datetime import datetime\n\nfrom pydantic_ai import Agent\n\nMODEL = \"google-gla:gemini-2.0-flash\"\n\n# INSTEAD OF THIS\n\njoker_agent = Agent(name=\"joker_agent\", model=MODEL, instructions=\"you write jokes\")\npoet_agent = Agent(name=\"poet_agent\", model=MODEL, instructions=\"you write poetry\")\n\n\n@joker_agent.instructions\n@poet_agent.instructions\ndef add_current_time() -> str:\n    return f\"\\n\\n<current_time>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</current_time>\\n\\n\"\n\n\n# I WANT THIS\n\n\ndef add_current_time_instructions() -> str:\n    return f\"\\n\\n<current_time>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</current_time>\\n\\n\"\n\n\njoker_agent = Agent(\n    name=\"joker_agent\", model=MODEL, instructions=(\"you write jokes\", add_current_time_instructions)\n)\npoet_agent = Agent(\n    name=\"poet_agent\", model=MODEL, instructions=(\"you write poetry\", add_current_time_instructions)\n)\n\n```\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "HamzaFarhan",
      "author_type": "User",
      "created_at": "2025-04-24T12:49:06Z",
      "updated_at": "2025-04-25T12:59:09Z",
      "closed_at": "2025-04-25T12:59:09Z",
      "labels": [
        "Feature request",
        "system prompts"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1580/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1580",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1580",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:47.798820",
      "comments": [
        {
          "author": "Kludex",
          "body": "Hi @HamzaFarhan ,\n\nIt makes sense. Wanna help with a PR for this?",
          "created_at": "2025-04-25T08:54:56Z"
        }
      ]
    },
    {
      "issue_number": 1098,
      "title": "[OpenAI] Error Handling for Exceeding Maximum Token Limits",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\n\n**Description:**\n\nWhen inputs exceed the model's maximum context length, Pydantic AI does not provide a clear error message, leading to potential confusion. For example, the following error may occur:\n\nPydantic AI Version: 0.0.36\n\n```\n\nTraceback (most recent call last):\n  File \"/path/to/file.py\", line X, in <module>\n    result = await analysis_agent.run(input_data)\n  File \"/path/to/pydantic_ai/agent.py\", line 316, in run\n    async for _ in agent_run:\n  File \"/path/to/pydantic_ai/agent.py\", line 1352, in __anext__\n    next_node = await self._graph_run.__anext__()\n  File \"/path/to/pydantic_graph/graph.py\", line 734, in __anext__\n    return await self.next(self._next_node)\n  File \"/path/to/pydantic_graph/graph.py\", line 723, in next\n    self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\n  File \"/path/to/pydantic_graph/graph.py\", line 305, in next\n    next_node = await node.run(ctx)\n  File \"/path/to/pydantic_ai/_agent_graph.py\", line 252, in run\n    return await self._make_request(ctx)\n  File \"/path/to/pydantic_ai/_agent_graph.py\", line 304, in _make_request\n    model_response, request_usage = await ctx.deps.model.request(\n  File \"/path/to/pydantic_ai/models/openai.py\", line 200, in request\n    return self._process_response(response), _map_usage(response)\n  File \"/path/to/pydantic_ai/models/openai.py\", line 295, in _process_response\n    timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\nTypeError: 'NoneType' object cannot be interpreted as an integer\n```\n\n\n**Proposal:**\n\n- Implement a pre-check to calculate the token length of the input data before making a request. If the input exceeds the model's maximum context length, raise a descriptive error indicating the issue.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPydantic AI Version: 0.0.36\n```",
      "state": "open",
      "author": "ishswar",
      "author_type": "User",
      "created_at": "2025-03-11T15:22:00Z",
      "updated_at": "2025-04-25T01:55:30Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1098/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1098",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1098",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:48.027264",
      "comments": [
        {
          "author": "ishswar",
          "body": "![Image](https://github.com/user-attachments/assets/74d0d492-d8bc-4661-b888-921c3284df54)\n\n```\n{'code': 400, 'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 207869 tokens (207828 of text input, 4... tool input). Please reduce the length of either on",
          "created_at": "2025-03-11T15:22:34Z"
        },
        {
          "author": "davzucky",
          "body": "I have the same error. The main problem is that any error message would be shallow. In a REST API, you usually check the error code; should the system check if the `error` is set, throw the error back. At least it would not be hidden from the caller.\nIn my case, using version 0.1.3",
          "created_at": "2025-04-21T07:40:48Z"
        },
        {
          "author": "DouweM",
          "body": "It looks like the [OpenAI SDK](https://github.com/openai/openai-python) is not correctly turning this error message into an `APIStatusError` (that we handle), and instead continues on to build an invalid `ChatCompletion` (that still has the original error data on it thanks to Pydantic's `model_extra",
          "created_at": "2025-04-21T19:20:50Z"
        },
        {
          "author": "davzucky",
          "body": "I did more investigation, and this only happens with AzureOpenAI. The company also has a proxy, which may be the root cause. I will continue to investigate. I tried the same code on the official OpenAI endpoint, and the status code is an error, with the error being propagated. @DouweM I will update ",
          "created_at": "2025-04-25T01:55:30Z"
        }
      ]
    },
    {
      "issue_number": 943,
      "title": "When using pydantic-ai with nested model types, it throws the error:",
      "body": "I am encountering an issue when using pydantic-ai with nested models. When attempting to use nested types in the schema, I receive the following error:\n﻿\n\n![Image](https://github.com/user-attachments/assets/08b820ab-7007-4851-80af-f2a348015e87)\n\n![Image](https://github.com/user-attachments/assets/d755c654-0509-4358-bb51-458aa3e2a3dd)\n\npydantic_ai.exceptions.UserError: Schema must be an object\n﻿\nThis error occurs even when the nested models are properly defined within the schema. I would like to know if this behavior indicates that nested models are not currently supported by pydantic-ai, or if there is another underlying issue causing the error.\n﻿\nAny guidance on this would be appreciated.\n﻿\n﻿",
      "state": "closed",
      "author": "Phone-IDE",
      "author_type": "User",
      "created_at": "2025-02-18T21:30:51Z",
      "updated_at": "2025-04-24T12:14:58Z",
      "closed_at": "2025-02-28T14:07:17Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/943/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/943",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/943",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:48.239529",
      "comments": [
        {
          "author": "Kludex",
          "body": "Hi @Phone-IDE , do you have a minimal example I can use to reproduce the issue?",
          "created_at": "2025-02-21T15:00:58Z"
        },
        {
          "author": "Phone-IDE",
          "body": "> Hi [@Phone-IDE](https://github.com/Phone-IDE) , do you have a minimal example I can use to reproduce the issue?\n\nThank you, I am a beginner, and the code looks like this. I noticed that the exception seems to occur during the conversion to the Schema stage.\n\n```import json\nfrom typing import Optio",
          "created_at": "2025-02-28T01:41:20Z"
        },
        {
          "author": "GUIZ4RD",
          "body": "Gettting exactly same error with pydantic-ai 0.1.3 using a RootModel.\n\n```\nclass SyllabusResponse(RootModel[List[\"SyllabusResponse.Module\"]]):\n    \n    class Module(BaseModel):\n        module: str\n        theory: List[str]\n        practice: List[str]\n        exercise: Optional[List[str]] = None\n\n\nag",
          "created_at": "2025-04-24T12:14:58Z"
        }
      ]
    },
    {
      "issue_number": 1137,
      "title": "TypeError: Object of type NotGiven is not JSON serializable when using AsyncOpenAI",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nWhen trying to use the AsyncOpenAI openai_client in the OpenAIProvider class the agent request gets JSON serialization errors. The AsyncOpenAI class works fine by itself but not when used with the new provider classes.\n\nI need to use the AsyncOpenAI class to add default_headers to the request so I can route them through Portkey.\n\n### Example Code\n\n```Python\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\nfrom pydantic_ai import Agent\n\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.models.openai import OpenAIModel\n\nopenai_client = AsyncOpenAI()\n\nmodel = OpenAIModel(\n    model_name=\"gpt-4o\",\n    provider=OpenAIProvider(\n        openai_client=openai_client,\n    ),\n)\n\nagent = Agent()\n\n\nasync def main() -> None:\n    result = await agent.run(model=model, user_prompt=\"why is the sky blue?\")\n    print(result)\n\n\nasyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13.2\nPydanticAI 0.0.40\nOpenAI 1.66.3\n```",
      "state": "closed",
      "author": "mledu",
      "author_type": "User",
      "created_at": "2025-03-15T16:26:25Z",
      "updated_at": "2025-04-24T08:28:12Z",
      "closed_at": "2025-03-18T14:17:08Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1137/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1137",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1137",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:48.521509",
      "comments": [
        {
          "author": "Kludex",
          "body": "I can't reproduce it with your code. Do you have the traceback?",
          "created_at": "2025-03-16T10:58:52Z"
        },
        {
          "author": "Kludex",
          "body": "Is it only failing when using Portkey? 🤔 ",
          "created_at": "2025-03-16T10:59:48Z"
        },
        {
          "author": "mledu",
          "body": "It's failing as soon as I add a AsyncOpenAI client to the provider. I'm testing going directly to OpenAI to eliminate Portkey as an issue.",
          "created_at": "2025-03-17T20:52:42Z"
        },
        {
          "author": "Kludex",
          "body": "Please provide a minimal reproducible example. As I said, it works as expected here.",
          "created_at": "2025-03-18T10:26:01Z"
        },
        {
          "author": "mledu",
          "body": "OK I figured it out, its the order of the imports that are breaking it. If AsyncOpenAI is imported before the pydantic AI imports it causes an error, if its imported after it works fine. This may be a problem as I have ruff cleanup my imports and it likes to put the openai import first.",
          "created_at": "2025-03-18T14:17:08Z"
        }
      ]
    },
    {
      "issue_number": 1562,
      "title": "In graph documentation, why is `all_messages` used rather than `new_messages` when tracking message history state?",
      "body": "### Question\n\nGiven that an incremental addition operation is used, it seems to me that you would want to use `new_messages` rather than `all_messages` as shown here:\n\nhttps://github.com/pydantic/pydantic-ai/blob/f955890ef0e22896fbc9fef41427496099cf0661/docs/graph.md?plain=1#L684\n\nIt seems that using `all_messages` would result in massive amounts of duplication.  Am I missing something here?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "szvsw",
      "author_type": "User",
      "created_at": "2025-04-21T16:00:28Z",
      "updated_at": "2025-04-24T07:25:04Z",
      "closed_at": "2025-04-24T07:25:04Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1562/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "DouweM"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1562",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1562",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:48.730393",
      "comments": [
        {
          "author": "DouweM",
          "body": "@szvsw You're totally right -- would you mind submitting a PR to fix this? :)",
          "created_at": "2025-04-21T18:46:16Z"
        },
        {
          "author": "ag14774",
          "body": "I also just came across this and came here to ask, so I just submitted a PR to fix this",
          "created_at": "2025-04-21T21:02:22Z"
        }
      ]
    },
    {
      "issue_number": 1569,
      "title": "Possible inconsistency with schema for Gemini REST interface",
      "body": "### Question\n\nThe [Gemini docs](https://ai.google.dev/gemini-api/docs/function-calling?example=weather) indicate that functions should be declared as follows.\n\n```json\n\"tools\": [\n  {\n    \"functionDeclarations\": [\n      {\n        \"name\": \"get_current_temperature\",\n        \"description\": \"Gets the current temperature for a given location.\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city name, e.g. San Francisco\"\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    ]\n  }\n]\n```\n\nThe current client implementation instead passes the key `function_declarations` to `httpx.AsyncClient.stream` here.\n\nhttps://github.com/pydantic/pydantic-ai/blob/35d6ed49dded87d69c053cb8deb9874b2bfa8d57/pydantic_ai_slim/pydantic_ai/models/gemini.py#L238\n\nThere is an alias declared here, but it doesn't seem to be respected in the serialization.\n\nhttps://github.com/pydantic/pydantic-ai/blob/35d6ed49dded87d69c053cb8deb9874b2bfa8d57/pydantic_ai_slim/pydantic_ai/models/gemini.py#L644\n\nHere's a demo.\n\n```python\nimport httpx\nimport pydantic_ai\nfrom unittest.mock import patch\n\n\noriginal = httpx.AsyncClient.stream\n\n\ndef patched_stream(self, *args, **kwargs):\n    print(kwargs[\"content\"].decode())\n    return original(self, *args, **kwargs)\n\n\nagent = pydantic_ai.Agent(\"google-gla:gemini-2.0-flash\")\n\n\n@agent.tool_plain\ndef get_animal_size(animal: str):\n    \"\"\"Get the size of an animal.\"\"\"\n    return 99\n\n\nwith patch(\"httpx.AsyncClient.stream\", patched_stream):\n    print(agent.run_sync(user_prompt=\"How big is an elephant.\"))\n```\n\n```bash\n$ python/gemini.py\n{\"contents\":[{\"role\":\"user\",\"parts\":[{\"text\":\"How big is an elephant.\"}]}],\"tools\":{\"function_declarations\":[{\"name\":\"get_animal_size\",\"description\":\"Get the size of an animal.\",\"parameters\":{\"properties\":{\"animal\":{\"type\":\"string\"}},\"required\":[\"animal\"],\"type\":\"object\"}}]}}\n{\"contents\":[{\"role\":\"user\",\"parts\":[{\"text\":\"How big is an elephant.\"}]},{\"role\":\"model\",\"parts\":[{\"functionCall\":{\"name\":\"get_animal_size\",\"args\":{\"animal\":\"elephant\"}}}]},{\"role\":\"user\",\"parts\":[{\"functionResponse\":{\"name\":\"get_animal_size\",\"response\":{\"return_value\":9}}}]}],\"tools\":{\"function_declarations\":[{\"name\":\"get_animal_size\",\"description\":\"Get the size of an animal.\",\"parameters\":{\"properties\":{\"animal\":{\"type\":\"string\"}},\"required\":[\"animal\"],\"type\":\"object\"}}]}}\nAgentRunResult(output='An elephant is size 99.\\n')\n```\n\nThe functions are nevertheless called correctly, but I wanted to check if this is expected behavior?\n\n### Additional Context\n\n```bash\n$ python --version\nPython 3.13.0\n$ uv pip list | grep pydantic\npydantic                  2.11.3\npydantic-ai               0.1.3\npydantic-ai-slim          0.1.3\npydantic-core             2.33.1\npydantic-evals            0.1.3\npydantic-graph            0.1.3\npydantic-settings         2.9.1\n```",
      "state": "closed",
      "author": "gitraffe",
      "author_type": "User",
      "created_at": "2025-04-22T18:49:50Z",
      "updated_at": "2025-04-24T07:23:39Z",
      "closed_at": "2025-04-24T07:23:39Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1569/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1569",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1569",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:48.983170",
      "comments": [
        {
          "author": "DouweM",
          "body": "@gitraffe Great catch, I'm glad Gemini was handling this correctly, but that line really should be:\n\n```py\nfunction_declarations: Annotated[list[_GeminiFunction], pydantic.Field(alias='functionDeclarations')]\n```\n\nThat way, the annotation and field and alias are applied to the entire `function_decla",
          "created_at": "2025-04-23T00:37:17Z"
        }
      ]
    },
    {
      "issue_number": 1483,
      "title": "pydantic_core._pydantic_core.ValidationError: 1 validation error for response_data_typed_dict",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen I use `result_type=list[RequiredResponseShape]`, I encounter a recurring validation issue. \n\nThe pydantic internals attempt to validate the output result but fail, citing the `response` field as missing. The model appears to be returning an object that I am requesting, but it lacks the necessary wrapping within Pydantic’s ‘envelope’.\n\nReverting to `result_type=RequiredResponseShape` resolves this problem.\n\nHere’s the exception:\n\nExceeded maximum retries (3) for result validation\n`pydantic_core._pydantic_core.ValidationError`:\n\n1 validation error for response_data_typed_dict\nresponse\nField required [type=missing, …\n\n### Example Code\n\n```Python\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n@dataclass\nclass SomeAgentDepts:\n    some_dept: SomeType\n\n\ndef create_agent(\n    model: str, api_key: str\n) -> Agent[SomeAgentDepts, list[RequiredResponseShape]]:\n    \n\n    provider = OpenAIProvider(api_key=api_key)\n    model_instance = OpenAIModel(model, provider=provider)\n\n    return Agent(\n        model=model_instance,\n        system_prompt=PROMPT,\n        deps_type=SomeAgentDepts,\n        tools=[some_tool],\n        result_type=list[RequiredResponseShape],\n        retries=3,\n        instrument=True,\n    )\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n* python 3.13.2\n* pydantic-ai-slim 0.0.53\n* openai 1.70.0 (as provided by pydantic-ai-slim[openai])\n```",
      "state": "closed",
      "author": "zhenyakovalyov",
      "author_type": "User",
      "created_at": "2025-04-15T11:13:23Z",
      "updated_at": "2025-04-24T04:58:32Z",
      "closed_at": "2025-04-24T04:58:30Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1483/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1483",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1483",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:49.270391",
      "comments": [
        {
          "author": "Kludex",
          "body": "Can I have a minimal reproducible example? Which model and prompt are you using?",
          "created_at": "2025-04-15T11:20:54Z"
        },
        {
          "author": "zhenyakovalyov",
          "body": "@Kludex, thank you very much for your response.\n\n* model is `gpt-4o-mini`, \n* `RequiredResponseShape` is a quite large proprietary `pydantic` model generally describing the shape of an input to some proprietary API, \n* prompt just gives some domain grounding instructions.\n\nreproducing this may take ",
          "created_at": "2025-04-15T11:29:00Z"
        },
        {
          "author": "DouweM",
          "body": "@zhenyakovalyov Can you please try again with the latest version of Pydantic AI? \n\nIf it's still not working, it would be really helpful if you could provide a minimal example of a model definition that triggers this issue (without revealing anything proprietary).",
          "created_at": "2025-04-17T00:23:12Z"
        },
        {
          "author": "zhenyakovalyov",
          "body": "@DouweM I think it is now solved. Thank you very much!",
          "created_at": "2025-04-24T04:58:30Z"
        }
      ]
    },
    {
      "issue_number": 1565,
      "title": "Iterating over an agent that's a node of a graph",
      "body": "### Question\n\nUse Case: Stream Agent nodes that are a part of Pydantic graph\n\nCurious if functionality already exists? I did a fair bit of digging around docs and code but didn't find anything direct. Unsure how to combine the iteration of each together.\n\nI have written iterators for both an Agent and a Graph separately. \nFor agents, I understand how to iterate/stream each node or part.\nFor graphs, I understand how to iterate over each node in the graph.\nMy question is how to get a stream of the agent that is a node of a pydantic graph. \n\nI was under the impression the Graph and Agent are both Graphs but not sure how to stream the steps of a node execution.\n\nI tried with the WriteEmail/Feedback example but didn't see any methods to get lower level details of the underlying node execution.\n\nExplored:\n- Using generator for `run()` return type but seems like that would conflict with runtime typing of graph\n- Adding Dependency with separate stream to write events to\n- Add separate method to the node for streaming and manually drive iteration?\n\nThanks for your time and an awesome library! Any direction appreciated (: \n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "jettmoy",
      "author_type": "User",
      "created_at": "2025-04-21T20:25:46Z",
      "updated_at": "2025-04-23T15:38:16Z",
      "closed_at": "2025-04-23T15:38:15Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1565/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1565",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1565",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:49.498100",
      "comments": [
        {
          "author": "jettmoy",
          "body": "Closing in favor of info found in #732 and #1452\n\nWould be helpful to have an example of graph streaming in the docs for other looking for the same functionality",
          "created_at": "2025-04-23T15:38:15Z"
        }
      ]
    },
    {
      "issue_number": 1211,
      "title": "Agent run regenerates a dynamic system-prompt if it is missing in 'message_history'",
      "body": "### Description\n\nCurrently, the way an Agent run is implemented (as also stated in the docs) is that if 'message_history' is passed, a dynamic system-prompt is not regenerated even if 'message_history' does not contain a system-prompt. This makes having a fixed size message buffer an unnecessarily inelegant solution; where rather than fetching the N most recent messages, you must first fetch a system-prompt and then prepend it to the message buffer. \n\nThis would be easily solved if the 'run' function just implements the required check and calls the system-prompt generation functions by it self, when it does not find a system-prompt part_kind.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "abshake96",
      "author_type": "User",
      "created_at": "2025-03-22T16:45:02Z",
      "updated_at": "2025-04-23T13:14:27Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale",
        "system prompts"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1211/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1211",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1211",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:49.729276",
      "comments": [
        {
          "author": "tzookb",
          "body": "⏫ \nI just want pass history of messages, as I don't store the system prompt.\nhow can I at least get the system prompt for the agent?",
          "created_at": "2025-03-27T01:39:54Z"
        },
        {
          "author": "Kludex",
          "body": "Does the new `instructions` solve your issue? \n\nSee https://ai.pydantic.dev/agents/#instructions.",
          "created_at": "2025-04-17T14:56:31Z"
        }
      ]
    },
    {
      "issue_number": 1023,
      "title": "System Prompt for Human In The Loop Use Cases",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nHi everybody,\n\nThis is not necessarily a bug, but I am having trouble figuring something out: We allow users to chat with agents. A user enters a message, the agent processes it, and a response is sent back to the user. The user may then enter follow-up questions, which means the conversation history fills up and is also passed back to the agent. So far, so good.\n\nNow, with the first user message, the Pydantic AI Agent uses the provided system prompt that I pass to `Agent(system_prompt=...)`. \n\nHowever, this system message is never exposed to the user. So, when the user receives the result from the agent, they only see their question and the result (one user message and one assistant message).\n\nFollow-up messages now contain a history without a system prompt. The agent then ignores my initial system prompt because the history is not empty (see the Pydantic AI code below).\n\nHow can I ensure it always uses my system prompt? Without it, all follow-up requests fail in my case. I understand the reasoning for not using the system message if there is already a history, but is there a way to enforce it in such Human-in-the-Loop cases?\n\nThanks!!\n\nFrom _agent_graph.py:\n```\n        if message_history:\n            # Shallow copy messages\n            messages.extend(message_history)\n            # Reevaluate any dynamic system prompt parts\n            await self._reevaluate_dynamic_prompts(messages, run_context)\n            return messages, _messages.ModelRequest([_messages.UserPromptPart(user_prompt)])\n        else:\n            parts = await self._sys_parts(run_context)\n            parts.append(_messages.UserPromptPart(user_prompt))\n            return messages, _messages.ModelRequest(parts)\n```\n\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.0.30\n```",
      "state": "closed",
      "author": "jkuehn",
      "author_type": "User",
      "created_at": "2025-03-01T08:44:59Z",
      "updated_at": "2025-04-23T10:41:26Z",
      "closed_at": "2025-04-18T10:53:35Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1023/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1023",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1023",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:49.937584",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/531\" (90% similar)",
          "created_at": "2025-03-01T08:50:11Z"
        },
        {
          "author": "jkuehn",
          "body": "Thanks, GitHub Bot, that is the same issue. Our users are less experienced with Python, and they love Pydantic AI for its simplicity. I see that dynamic system prompt reevaluation is a solution, thanks! \n\nJust to make sure: If there is a message history without a system prompt, why wouldn't the agen",
          "created_at": "2025-03-01T09:27:14Z"
        },
        {
          "author": "jkuehn",
          "body": "After some testing we have patched `_agent_graph.py` to set the system message if there is none. \n\nLet me know if this makes sense to you. We can also prepare a PR with a test for your review.\n\n```\n    async def _prepare_messages(\n        self,\n        user_prompt: str | Sequence[_messages.UserConte",
          "created_at": "2025-03-11T13:08:52Z"
        },
        {
          "author": "Kludex",
          "body": "I think what you want is our new `instructions` parameter: https://ai.pydantic.dev/agents/#instructions.\n\nLet me know if that doesn't solve your case.\n\nSorry the delay!",
          "created_at": "2025-04-18T10:53:35Z"
        },
        {
          "author": "jkuehn",
          "body": "Yes, thanks @Kludex!",
          "created_at": "2025-04-23T10:41:25Z"
        }
      ]
    },
    {
      "issue_number": 736,
      "title": "Problem with content filtering, propose context manager for agent.run that returns the complete response",
      "body": "\nHey there,\n\nI am using `capture_run_messages` from `agent.py` to catch exceptions and print the messages via `AsyncAzureOpenAI`.\n\nMy problem now is the following: I oftentimes get `messages = None` when user upload articles for summarization due to **overly strict content filtering**.\n\nHowever, since the reason for the filtering is in `response.choices[0]` instead `response.choices[0].message` , I cannot log this with the contextmanager `capture_run_messages`. The amount of these messages is pretty significant.\n\nI think it would be great to have context manager that allows accessing the complete response (if I did not overlook it).\n\nEdit: MREs for pydantic-ai without access to response and Azure API with access to response can be found in comments below.",
      "state": "open",
      "author": "tostenzel",
      "author_type": "User",
      "created_at": "2025-01-22T08:44:17Z",
      "updated_at": "2025-04-23T07:35:16Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/736/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/736",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/736",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:50.269468",
      "comments": [
        {
          "author": "tostenzel",
          "body": "Somewhat **minimal reproducible example** for pydantic-ai without a context manager\n\n```py\nimport asyncio\n\nfrom config import MandatorySecretEnvironmentVariables\nfrom openai import AsyncAzureOpenAI\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models import ModelSettings\nfrom pydantic_ai.models.ope",
          "created_at": "2025-01-22T09:19:28Z"
        },
        {
          "author": "tostenzel",
          "body": "Somewhat **minimal reproducible example for Azure API**:\n\n```py\nimport asyncio\n\nfrom openai import AsyncAzureOpenAI\n\nfrom compare_llm_libraries.config import MandatorySecretEnvironmentVariables\n\nSYSTEM_PROMPT = \"Summarize the following text: \"\nUSER_PROMPT = <See example in previous comment>\n\n\nvariab",
          "created_at": "2025-01-22T09:21:05Z"
        },
        {
          "author": "samuelcolvin",
          "body": "@tostenzel when you say \"response\" do you want the raw JSON, or the OpenAI response object?",
          "created_at": "2025-01-22T09:26:38Z"
        },
        {
          "author": "tostenzel",
          "body": "The OpenAI response object.",
          "created_at": "2025-01-22T09:29:19Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Yup, I think that's a fair request, we can add something like that, it'll need to be specific to each model to make it type safe.\n\nBelow is an example of how you can implement this yourself by subclassing the openai model.\n\n**WARING:** this will probably break in a pydantic-ai release soon when we 1",
          "created_at": "2025-01-22T09:42:51Z"
        }
      ]
    },
    {
      "issue_number": 1476,
      "title": "Results from Perplexity Sonar Pro consistently fail validation",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHi team!\n\nI am trying to use Perplexity's sonar pro to reason about data and extract urls.\n\nWhile I have been able to get this to work with litellm, my results from Pydantic AI consistently fail validation. Am I using Pydantic AI incorrectly?\n\nThanks!\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n\nclass UrlOutput(BaseModel):\n    reason: str = Field(description=\"The reason for the URL.\")\n    url: str = Field(description=\"The url pointing to the steam page.\")\n\nmodel = OpenAIModel(\n    'sonar-pro',\n    provider=OpenAIProvider(\n        base_url='https://api.perplexity.ai',\n        api_key=os.environ['PERPLEXITYAI_API_KEY'],\n    ),\n)\n\n\nsearch_agent = Agent(\n    model=model,\n    result_type=UrlOutput,\n    system_prompt=dedent(\n        \"\"\"        You are a web scraping tool that specializes in data\n        data extraction. Given a URL to a subreddit, find information about\n        the games that might be mentioned or another similar game. Once a\n        recommended game is chosen, find the url for the steam page or for the\n        PlayStation Store. Return only the first URL found. Exclude Sekiro from\n        these searches.\n\n        Think critically when identifying the URL and explain your reasoning.\n\n        Format:\n\n        {\"reason\": \"your reasoning\", \"url\": \"the url\"}\"\"\"\n    ),\n)\n\nres = await search_agent.run(\"https://www.reddit.com/r/Sekiro/comments/16tj74y/lies_of_p_vs_wo_long/\")\n\n# Working example using LiteLLM\n\n\nimport os\nfrom textwrap import dedent\n\nfrom google.colab import userdata\nimport litellm\nfrom litellm import completion\nfrom pydantic import BaseModel, Field\n\nos.environ['LITELLM_LOG'] = 'DEBUG'\nos.environ['PERPLEXITYAI_API_KEY'] = userdata.get('PERPLEXITY_API_KEY')\n\n\ndef messages(url: str):\n    return [\n        {\"role\": \"system\", \"content\": dedent(\n        \"\"\"        You are a web scraping tool that specializes in data\n        data extraction. Given a URL to a subreddit, find information about\n        the games that might be mentioned or another similar game. Once a\n        recommended game is chosen, find the url for the steam page or for the\n        PlayStation Store. Return only the first URL found. Exclude Sekiro from\n        these searches.\n\n        Think critically when identifying the URL and explain your reasoning.\n\n        Format:\n\n        {\"reason\": \"your reasoning\", \"url\": \"the url\"}\"\"\"\n        )},\n        {\"role\": \"user\", \"content\": f\"Find steam URLs for this post: {url}.\"},\n    ]\n\ndef pprint_response(resp):\n    print(str(resp).replace(', ', ',\\n    '))\n\nlitellm.enable_json_schema_validation = True\n\nresp = completion(\n    model=\"perplexity/sonar-pro\",\n    messages=messages(\"https://www.reddit.com/r/Sekiro/comments/16tj74y/lies_of_p_vs_wo_long/\"),\n    response_format=UrlOutput,\n)\n\npprint_response(resp)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.12\nPydantic AI: 0.0.55\n```",
      "state": "closed",
      "author": "michael-delphos",
      "author_type": "User",
      "created_at": "2025-04-14T18:42:22Z",
      "updated_at": "2025-04-23T01:56:12Z",
      "closed_at": "2025-04-23T01:56:12Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1476/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1476",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1476",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:50.531185",
      "comments": [
        {
          "author": "DouweM",
          "body": "@michael-delphos Sorry to hear you're running into issues! Can you please update to the latest version of Pydantic AI, and then share the validation error you get?",
          "created_at": "2025-04-17T00:24:35Z"
        },
        {
          "author": "michael-delphos",
          "body": "Thanks for getting back to me so quickly!\n\nJust confirmed that I'm still seeing the error in `0.1.1`\n\nThe error message isn't a validation error:\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 572,",
          "created_at": "2025-04-17T01:13:58Z"
        },
        {
          "author": "DouweM",
          "body": "@michael-delphos Thanks for the additional context. The underlying issue is that Perplexity is returning the result as a text response rather than via a tool call, as PydanticAI expects. \n\nFor now your manual validation approach is the best way to go, but we're tracking a proper solution in https://",
          "created_at": "2025-04-17T18:30:12Z"
        }
      ]
    },
    {
      "issue_number": 1568,
      "title": "Nesting capture_run_messages",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI see this note\n\n```\n    !!! note\n        If you call `run`, `run_sync`, or `run_stream` more than once within a single `capture_run_messages` context,\n        `messages` will represent the messages exchanged during the first call only.\n```\n\nBut I want to be able to nest agents and capture exceptions. What if a tool wraps another agent and that agent can also raise Exceptions?\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent, capture_run_messages\n\n\ndef test_nested_capture_run_messages() -> None:\n    agent1 = Agent('test')\n    agent2 = Agent('test')\n\n    with capture_run_messages() as messages1:\n        assert messages1 == []\n        res1 = agent1.run_sync('Hi Bro')\n        assert res1.output == 'success (no tool calls)'\n        with capture_run_messages() as messages2:\n            assert messages2 == []  # This assertion fails\n            res2 = agent2.run_sync('Hello')\n            assert res2.output == 'success (no tool calls)'\n\n    assert messages1 != messages2\n```\n\nThis doesn't work obviously\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12\npydantic 0.1.3\nopenai client\n```",
      "state": "open",
      "author": "vikigenius",
      "author_type": "User",
      "created_at": "2025-04-22T18:33:35Z",
      "updated_at": "2025-04-23T01:03:12Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1568/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1568",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1568",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:50.732787",
      "comments": [
        {
          "author": "DouweM",
          "body": "@samuelcolvin You implemented this, would this be doable?",
          "created_at": "2025-04-23T01:03:11Z"
        }
      ]
    },
    {
      "issue_number": 1559,
      "title": "max_completion_tokens is not a valid openAI argument",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIn File openai.py the argument `max_completion_tokens` is not valid. It should named `max_tokens`\n\n### Example Code\n\n```Python\n`            return await self.client.chat.completions.create(\n                model=self._model_name,\n                messages=openai_messages,\n                n=1,\n                parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),\n                tools=tools or NOT_GIVEN,\n                tool_choice=tool_choice or NOT_GIVEN,\n                stream=stream,\n                stream_options={'include_usage': True} if stream else NOT_GIVEN,\n                stop=model_settings.get('stop_sequences', NOT_GIVEN),\n                max_completion_tokens=model_settings.get('max_tokens', NOT_GIVEN), \n                temperature=model_settings.get('temperature', NOT_GIVEN),\n                top_p=model_settings.get('top_p', NOT_GIVEN),\n                timeout=model_settings.get('timeout', NOT_GIVEN),\n                seed=model_settings.get('seed', NOT_GIVEN),\n                presence_penalty=model_settings.get('presence_penalty', NOT_GIVEN),\n                frequency_penalty=model_settings.get('frequency_penalty', NOT_GIVEN),\n                logit_bias=model_settings.get('logit_bias', NOT_GIVEN),\n                reasoning_effort=model_settings.get('openai_reasoning_effort', NOT_GIVEN),\n                user=model_settings.get('openai_user', NOT_GIVEN),\n                extra_headers={'User-Agent': get_user_agent()},\n                extra_body=model_settings.get('extra_body'),\n            )`\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nI am using the latest version of PydanticAI\n```",
      "state": "closed",
      "author": "kiransj",
      "author_type": "User",
      "created_at": "2025-04-21T02:38:48Z",
      "updated_at": "2025-04-23T00:05:07Z",
      "closed_at": "2025-04-23T00:05:07Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1559/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1559",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1559",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:50.946508",
      "comments": [
        {
          "author": "DouweM",
          "body": "@kiransj Per [OpenAI's code docs](https://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/types/chat/completion_create_params.py#L111-L127), `max_tokens` is deprecated in favor of `max_completion_tokens`. Are you on an older version, or hitting some kind of e",
          "created_at": "2025-04-21T19:23:40Z"
        }
      ]
    },
    {
      "issue_number": 1404,
      "title": "Shouldn't MCPServer/s available in the library be called MCPClient/s ?",
      "body": "### Question\n\n> class MCPServer(ABC):\n>     \"\"\"Base class for attaching agents to MCP servers.\n> \n>     See <https://modelcontextprotocol.io> for more information.\n>     \"\"\"\n\nI would assume that the structure interfacing to the server is the client? i assume the intention is that the Agent is the client itself, which has a list of _mcp_servers it can connect to, but those servers on their implementation seem like clients to me 🤔 \n\nI wonder if there is some confusion with MCPServerStdio (which acts truly as a server, i think) and the MCPServerHTTP, which is a client (?)\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "nzlzcat",
      "author_type": "User",
      "created_at": "2025-04-08T07:30:27Z",
      "updated_at": "2025-04-22T11:57:54Z",
      "closed_at": "2025-04-17T08:50:38Z",
      "labels": [
        "MCP"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1404/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1404",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1404",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:51.183199",
      "comments": [
        {
          "author": "Viicos",
          "body": "I mentioned this in https://github.com/pydantic/pydantic-ai/pull/1100#discussion_r1995826718, cc @Kludex ",
          "created_at": "2025-04-08T10:23:06Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-15T14:00:50Z"
        },
        {
          "author": "Kludex",
          "body": "I understand the confusion, and maybe we can clarify in the documentation, but for now I think it's not worth it.\n\nThe idea is that the object represents the MCP server.",
          "created_at": "2025-04-17T08:50:38Z"
        },
        {
          "author": "nzlzcat",
          "body": "@Kludex I suggest to at least name it to MCPServerProxy, if you are not keen on moving to MCPClient",
          "created_at": "2025-04-22T11:57:52Z"
        }
      ]
    },
    {
      "issue_number": 1161,
      "title": "Using BinaryContent with CSV media type throws an error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\n\n```\nFile \"/Users/lemi/p2/z/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 201, in request\n    response = await self._completions_create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        messages, False, cast(OpenAIModelSettings, model_settings or {}), model_request_parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/lemi/p2/z/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 269, in _completions_create\n    async for msg in self._map_message(m):\n        openai_messages.append(msg)\n  File \"/Users/lemi/p2/z/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 331, in _map_message\n    async for item in self._map_user_message(message):\n        yield item\n  File \"/Users/lemi/p2/z/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 383, in _map_user_message\n    yield await self._map_user_prompt(part)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/lemi/p2/z/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 425, in _map_user_prompt\n    raise RuntimeError(f'Unsupported binary content type: {item.media_type}')\nRuntimeError: Unsupported binary content type: text/csv\n```\n\n### Example Code\n\n```Python\nresult = analysis_agent.run_sync(\n        [\n            f\"Do something with this CSV file: {csv_file}\",\n            BinaryContent(\n                data=csv_file.read_bytes(),\n                media_type=\"text/csv\",\n            ),\n        ],\n        deps=input_data,\n    )\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.13.1\nOpenAI: 1.66.3\nOpenAI Model: gpt-4o/4.5-preview, o1, o3-mini\nPydantic AI: pydantic-ai-slim[openai]>=0.0.41\n```",
      "state": "open",
      "author": "rhymiz",
      "author_type": "User",
      "created_at": "2025-03-18T14:41:36Z",
      "updated_at": "2025-04-21T19:09:52Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1161/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1161",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1161",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:51.471708",
      "comments": [
        {
          "author": "Kludex",
          "body": "The OpenAI API doesn't support it, what do you think we should do? Convert to txt?",
          "created_at": "2025-03-18T16:45:04Z"
        },
        {
          "author": "rhymiz",
          "body": "@Kludex I think that's probably a sane default",
          "created_at": "2025-03-20T14:56:00Z"
        },
        {
          "author": "davzucky",
          "body": "This is the same for the Markdown type. At the moment, I add the content manually as text, but I expect binary content to handle that. Converting it to text would be the best as well",
          "created_at": "2025-04-21T07:36:35Z"
        },
        {
          "author": "DouweM",
          "body": "This is being implemented in https://github.com/pydantic/pydantic-ai/pull/1552!",
          "created_at": "2025-04-21T19:09:51Z"
        }
      ]
    },
    {
      "issue_number": 1533,
      "title": "Non Recording Span Bug",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nGetting the below error post version update.  Can someone please advise?\n\npydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'opentelemetry.trace.span.NonRecordingSpan'>\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.9\nPydantic AI 0.1.2\nOpenAI 1.70.0\n```",
      "state": "closed",
      "author": "vojomo-mbalick",
      "author_type": "User",
      "created_at": "2025-04-17T20:42:27Z",
      "updated_at": "2025-04-21T08:20:29Z",
      "closed_at": "2025-04-18T19:50:06Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1533/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1533",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1533",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:51.707919",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1525\" (95% similar)",
          "created_at": "2025-04-17T20:50:11Z"
        },
        {
          "author": "DouweM",
          "body": "@vojomo-mbalick Thanks for the report, I'm looking into it. If you have this info handy, what version were you on before you updated?",
          "created_at": "2025-04-17T23:38:02Z"
        },
        {
          "author": "DouweM",
          "body": "@vojomo-mbalick Can you share any tools you've defined for your agent? I'm particularly interested in their return types.",
          "created_at": "2025-04-17T23:45:29Z"
        },
        {
          "author": "DouweM",
          "body": "@vojomo-mbalick Is it possible you have a tool with `return await agent.run(...)` instead of `return await agent.run(...).output`?",
          "created_at": "2025-04-17T23:58:40Z"
        },
        {
          "author": "jerryliu14",
          "body": "> [@vojomo-mbalick](https://github.com/vojomo-mbalick) Is it possible you have a tool with `return await agent.run(...)` instead of `return await agent.run(...).output`?\n\nHi, I have the same issue as well, and do have return await agent.run(...) as the output.\n\nIs this the issue that's causing this ",
          "created_at": "2025-04-18T15:23:02Z"
        }
      ]
    },
    {
      "issue_number": 1282,
      "title": "MCP Sample (HTTP SSE mode) Failed with HTTP 503 Error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nTrying official example here: https://ai.pydantic.dev/mcp/client/\n\nThe `stdio` mode sample runs without any issue but the `sse` mode sample failed with error:\n\n```\n  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/neo/Code/ML/neb/mcp_client.py\", line 26, in <module>\n  |     asyncio.run(run(agent_sse))\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 190, in run\n  |     return runner.run(main)\n  |            ^^^^^^^^^^^^^^^^\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 118, in run\n  |     return self._loop.run_until_complete(task)\n  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n  |     return future.result()\n  |            ^^^^^^^^^^^^^^^\n  |   File \"/Users/neo/Code/ML/neb/mcp_client.py\", line 18, in run\n  |     async with agent.run_mcp_servers():\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 210, in __aenter__\n  |     return await anext(self.gen)\n  |            ^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/Users/neo/Code/ML/neb/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 1299, in run_mcp_servers\n  |     await exit_stack.enter_async_context(mcp_server)\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 650, in enter_async_context\n  |     result = await _enter(cm)\n  |              ^^^^^^^^^^^^^^^^\n  |   File \"/Users/neo/Code/ML/neb/.venv/lib/python3.11/site-packages/pydantic_ai/mcp.py\", line 86, in __aenter__\n  |     self._read_stream, self._write_stream = await self._exit_stack.enter_async_context(self.client_streams())\n  |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 650, in enter_async_context\n  |     result = await _enter(cm)\n  |              ^^^^^^^^^^^^^^^^\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 210, in __aenter__\n  |     return await anext(self.gen)\n  |            ^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/Users/neo/Code/ML/neb/.venv/lib/python3.11/site-packages/pydantic_ai/mcp.py\", line 219, in client_streams\n  |     async with sse_client(\n  |   File \"/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 210, in __aenter__\n  |     return await anext(self.gen)\n  |            ^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/Users/neo/Code/ML/neb/.venv/lib/python3.11/site-packages/mcp/client/sse.py\", line 43, in sse_client\n  |     async with anyio.create_task_group() as tg:\n  |   File \"/Users/neo/Code/ML/neb/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/neo/Code/ML/neb/.venv/lib/python3.11/site-packages/mcp/client/sse.py\", line 53, in sse_client\n    |     event_source.response.raise_for_status()\n    |   File \"/Users/neo/Code/ML/neb/.venv/lib/python3.11/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    |     raise HTTPStatusError(message, request=request, response=self)\n    | httpx.HTTPStatusError: Server error '503 Service Unavailable' for url 'http://localhost:3001/sse'\n```\n\nThe server is already up with command:\n```shell\n❯ npx @pydantic/mcp-run-python sse\nRunning MCP server with SSE transport on localhost:3001\n```\n\nAny hint?\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP\n\nserver_sse = MCPServerHTTP(url='http://localhost:3001/sse')\nagent_sse = Agent('openai:gpt-4o', mcp_servers=[server_sse])\n\nasync def run(agent):\n    async with agent.run_mcp_servers():\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-28?')\n    print(result.data)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run(agent_sse))\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.11.11\npydantic: 2.11.0\npydantic-ai: 0.0.46\nopenai: 1.69.0\n```",
      "state": "closed",
      "author": "neolee",
      "author_type": "User",
      "created_at": "2025-03-28T14:52:28Z",
      "updated_at": "2025-04-19T17:46:34Z",
      "closed_at": "2025-04-12T05:34:36Z",
      "labels": [
        "more info",
        "Stale",
        "need confirmation",
        "MCP"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1282/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1282",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1282",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:51.968568",
      "comments": [
        {
          "author": "neolee",
          "body": "Additional info: `curl` test seems OK:\n```shell\n❯ curl -v http://localhost:3001/sse\n* Host localhost:3001 was resolved.\n* IPv6: ::1\n* IPv4: 127.0.0.1\n*   Trying [::1]:3001...\n* Connected to localhost (::1) port 3001\n> GET /sse HTTP/1.1\n> Host: localhost:3001\n> User-Agent: curl/8.7.1\n> Accept: */*\n>\n",
          "created_at": "2025-04-02T12:32:26Z"
        },
        {
          "author": "samuelcolvin",
          "body": "I don't really understand what's going on here. as you say the server is find with `curl` if you get this 503 again, happy to try to fix, but without more information, I'm not sure what I can do.",
          "created_at": "2025-04-02T12:46:48Z"
        },
        {
          "author": "neolee",
          "body": "@samuelcolvin what kind of info do you need?\n\nTo clarify, I've run the server by `npx @pydantic/mcp-run-python sse` and the `curl -v http://localhost:3001/sse` test seems OK (as shown above), but as the same time the client code sample (copy your official sample from [here](https://ai.pydantic.dev/m",
          "created_at": "2025-04-03T00:50:18Z"
        },
        {
          "author": "neolee",
          "body": "@samuelcolvin tried to dig deeper and found something. The `mcp-run-python` server only listens at the IPv6 address `::1` and the client created using `MCPServerHTTP(url='http://localhost:3001/sse')` seems only connect to IPv4 address (I tried another MCP server which listens IPv4 address and it wor",
          "created_at": "2025-04-03T05:37:39Z"
        },
        {
          "author": "samuelcolvin",
          "body": "I have the same machine and it works fine.\n\nAs it happens, we're switching to deno from npx anyway, please try #1340 and see if that fixes your problem.",
          "created_at": "2025-04-03T07:38:23Z"
        }
      ]
    },
    {
      "issue_number": 1393,
      "title": "how to build codeact agent",
      "body": "### Question\n\nThe agent class is marked @final, how can I extend agent with a pattern like codeact, or do I just have to customize the parsing, build the tool in a system prompt, and then process the output in validateResult to get the final result?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "tianshangwuyun",
      "author_type": "User",
      "created_at": "2025-04-07T02:17:46Z",
      "updated_at": "2025-04-19T14:02:44Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1393/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1393",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1393",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:52.214808",
      "comments": [
        {
          "author": "Kludex",
          "body": "What do you mean by \"a pattern like codeact\"?",
          "created_at": "2025-04-07T07:55:14Z"
        },
        {
          "author": "tianshangwuyun",
          "body": "> What do you mean by \"a pattern like codeact\"?\n\nlike this \nhttps://docs.llamaindex.ai/en/stable/examples/agent/code_act_agent/\nor\nhttps://docs.llamaindex.ai/en/stable/examples/agent/from_scratch_code_act_agent/\n\nI have implemented one based on this, but I don't know how to stream it out,（https://ai",
          "created_at": "2025-04-08T09:21:05Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-15T14:00:52Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-04-19T14:00:47Z"
        }
      ]
    },
    {
      "issue_number": 1454,
      "title": "RuntimeError: Event loop is closed when running official MCP Run Python example code",
      "body": "### Question\n\nHello,\n\nI'm evaluating how to integrate Pydantic MCP Run Python into my project. I tried to run the official example code mentioned in https://ai.pydantic.dev/mcp/run-python/. It always has exceptional text when it exits. I don't know if it can be solved or disabled.\n\nSince the original example code refused to execute due to the system's requirement for permission, I added a few lines to make it runnable.\n\n```\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\ncode = \"\"\"\nimport numpy\na = numpy.array([1, 2, 3])\nprint(a)\na\n\"\"\"\nserver_params = StdioServerParameters(\n    command='deno',\n    args=[\n        'run',\n        '-N',\n        '--allow-read=.',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ],\n)\n\n\nasync def main():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            tools = await session.list_tools()\n            print(len(tools.tools))\n            #> 1\n            print(repr(tools.tools[0].name))\n            #> 'run_python_code'\n            print(repr(tools.tools[0].inputSchema))\n            \"\"\"\n            {'type': 'object', 'properties': {'python_code': {'type': 'string', 'description': 'Python code to run'}}, 'required': ['python_code'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}\n            \"\"\"\n            result = await session.call_tool('run_python_code', {'python_code': code})\n            print(result.content[0].text)\n            \"\"\"\n            <status>success</status>\n            <dependencies>[\"numpy\"]</dependencies>\n            <output>\n            [1 2 3]\n            </output>\n            <return_value>\n            [\n              1,\n              2,\n              3\n            ]\n            </return_value>\n            \"\"\"\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n``` \n\nThe output is:\n\n```\n1\n'run_python_code'\n{'type': 'object', 'properties': {'python_code': {'type': 'string', 'description': 'Python code to run'}}, 'required': ['python_code'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}\n<status>success</status>\n<dependencies>[\"numpy\"]</dependencies>\n<output>\n[1 2 3]\n</output>\n<return_value>\n[\n  1,\n  2,\n  3\n]\n</return_value>\nException ignored in: <function BaseSubprocessTransport.__del__ at 0x7f316985a4d0>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/asyncio/base_subprocess.py\", line 126, in __del__\n    self.close()\n  File \"/usr/lib/python3.10/asyncio/base_subprocess.py\", line 104, in close\n    proto.pipe.close()\n  File \"/usr/lib/python3.10/asyncio/unix_events.py\", line 547, in close\n    self._close(None)\n  File \"/usr/lib/python3.10/asyncio/unix_events.py\", line 571, in _close\n    self._loop.call_soon(self._call_connection_lost, exc)\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 753, in call_soon\n    self._check_closed()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\n    raise RuntimeError('Event loop is closed')\nRuntimeError: Event loop is closed\n```\n\nAny idea? Thanks.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "sangshuduo",
      "author_type": "User",
      "created_at": "2025-04-11T14:42:54Z",
      "updated_at": "2025-04-19T14:00:43Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1454/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1454",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1454",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:52.561781",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-19T14:00:42Z"
        }
      ]
    },
    {
      "issue_number": 1459,
      "title": "Differentiation between use of Server Tools and Client Tools",
      "body": "### Question\n\nI am using Vercel/ai sdk to build my agents. I plan to try out pydantic ai. For vercel/ai, there are server tools and client tools. Any guide on how to use server tools and client tools when using pydantic ai? thanks\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "myhendry",
      "author_type": "User",
      "created_at": "2025-04-12T02:30:45Z",
      "updated_at": "2025-04-19T14:00:41Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1459/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1459",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1459",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:52.894338",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-19T14:00:40Z"
        }
      ]
    },
    {
      "issue_number": 1525,
      "title": "Serialize error during agent.iter():  pydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'opentelemetry.trace.span.NonRecordingSpan'>",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nDuring streaming final output from end node \nI  found issue with \npydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'opentelemetry.trace.span.NonRecordingSpan'>\n\n\n                **elif Agent.is_end_node(node):\n                    # pass\n                    # Convert final output to string\n                    final_output = str(run.result.output)**\n                    **yield f'=== Final Output: {final_output} ===\\n'**\n                    # yield final_output\n\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 268, in __call__\n    await wrap(partial(self.listen_for_disconnect, receive))\n  File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 264, in wrap\n    await func()\n  File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 233, in listen_for_disconnect\n    message = await receive()\n              ^^^^^^^^^^^^^^^\n  File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 563, in receive\n    await self.message_event.wait()\n  File \"/usr/lib/python3.12/asyncio/locks.py\", line 212, in wait\n    await fut\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 7a9c73101970\n\nDuring handling of the above exception, another exception occurred:\n\n  + Exception Group Traceback (most recent call last):\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n  |     result = await app(  # type: ignore[func-returns-value]\n  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n  |     return await self.app(scope, receive, send)\n  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n  |     await super().__call__(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/applications.py\", line 112, in __call__\n  |     await self.middleware_stack(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n  |     raise exc\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n  |     await self.app(scope, receive, _send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n  |     await self.simple_response(scope, receive, send, request_headers=headers)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n  |     await self.app(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n  |     raise exc\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n  |     await app(scope, receive, sender)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 715, in __call__\n  |     await self.middleware_stack(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 735, in app\n  |     await route.handle(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\n  |     await self.app(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\n  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n  |     raise exc\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n  |     await app(scope, receive, sender)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n  |     await response(scope, receive, send)\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 261, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 767, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 264, in wrap\n    |     await func()\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/starlette/responses.py\", line 245, in stream_response\n    |     async for chunk in self.body_iterator:\n    |   File \"/home/peter/organizations/kinit/alfie/agentic-core/routers/agent.py\", line 77, in markdown_streamer\n    |     async with node.stream(run.ctx) as request_stream:\n    |   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 278, in stream\n    |     async with self._stream(ctx) as streamed_response:\n    |   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 301, in _stream\n    |     async with ctx.deps.model.request_stream(\n    |   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    |     return await anext(self.gen)\n    |            ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 209, in request_stream\n    |     response = await self._completions_create(\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 263, in _completions_create\n    |     openai_messages = await self._map_messages(messages)\n    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 329, in _map_messages\n    |     async for item in self._map_user_message(message):\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 392, in _map_user_message\n    |     content=part.model_response_str(),\n    |             ^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic_ai/messages.py\", line 370, in model_response_str\n    |     if isinstance(self.content, dict):\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/peter/organizations/kinit/alfie/.venv/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 631, in dump_json\n    |     return self.serializer.to_json(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^\n    | pydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'opentelemetry.trace.span.NonRecordingSpan'>\n    +------------------------------------\n\n\n\n\n### Example Code\n\n```Python\n@router.post(\"/process-query-stream-markdown\")\nasync def process_query_stream_markdown(request: QueryRequest):\n    logger.info(f\"Processing markdown query: {request.prompt}\")\n    agent = AgenticCore()\n    \n    async def markdown_streamer():\n        from pydantic_ai import Agent\n        from pydantic_ai.messages import (\n            FinalResultEvent,\n            FunctionToolCallEvent,\n            FunctionToolResultEvent,\n            PartDeltaEvent,\n            PartStartEvent,\n            TextPartDelta,\n            ToolCallPartDelta,\n        )\n        output_messages: list[str] = []\n\n        async with agent.manager_agent.agent.iter(request.prompt) as run:\n            async for node in run:\n                if Agent.is_user_prompt_node(node):\n                    # yield f'=== UserPromptNode: {node.user_prompt} ===\\n'\n                    pass\n                elif Agent.is_model_request_node(node):\n                    # yield '=== ModelRequestNode: streaming partial request tokens ===\\n'\n                    async with node.stream(run.ctx) as request_stream:\n                        async for event in request_stream:\n                            if isinstance(event, PartStartEvent):\n                                # content = f'[Request] Starting part {event.index}'\n                                # yield f'{content}\\n'\n                                pass\n                            elif isinstance(event, PartDeltaEvent):\n                                if isinstance(event.delta, TextPartDelta):\n                                    yield event.delta.content_delta\n                                elif isinstance(event.delta, ToolCallPartDelta):\n                                    # content = str(event.delta.args_delta)\n                                    # yield f'{content}\\n'\n                                    pass\n                            elif isinstance(event, FinalResultEvent):\n                                # content = f'[Result] Final output (tool: {event.tool_name})'\n                                # yield f'{content}\\n'\n                                pass\n                elif Agent.is_call_tools_node(node):\n                    # yield '=== CallToolsNode: streaming response & tool usage ===\\n'\n                    async with node.stream(run.ctx) as handle_stream:\n                        async for event in handle_stream:\n                            if isinstance(event, FunctionToolCallEvent):\n                                # content = f'Tool call: {event.part.tool_name}'\n                                # yield f'{content}\\n'\n                                pass\n                            elif isinstance(event, FunctionToolResultEvent):\n                                # Convert tool result to string to avoid serialization issues\n                                # content = str(event.result.content)\n                                # yield f'{content}\\n'\n                                pass\n                elif Agent.is_end_node(node):\n                    # pass\n                    # Problematic part \n                    final_output = str(run.result.output)\n                    yield f'=== Final Output: {final_output} ===\\n'\n                    # yield final_output\n        # except Exception as e:\n        #     yield f\"Error: {str(e)}\\n\"\n\n    return StreamingResponse(markdown_streamer(), media_type=\"text/markdown\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPackage                                  Version\n---------------------------------------- ---------------\naiofiles                                 24.1.0\nalembic                                  1.14.1\nannotated-types                          0.7.0\nanthropic                                0.49.0\nanyio                                    4.8.0\nargcomplete                              3.6.2\nbackoff                                  2.2.1\nboto3                                    1.37.31\nbotocore                                 1.37.31\ncachetools                               5.5.1\ncertifi                                  2024.12.14\ncharset-normalizer                       3.4.1\nclick                                    8.1.8\ncohere                                   5.13.11\ncolorama                                 0.4.6\nDeprecated                               1.2.17\ndistro                                   1.9.0\ndnspython                                2.7.0\nemail_validator                          2.2.0\neval_type_backport                       0.2.2\nexecuting                                2.2.0\nfastapi                                  0.115.12\nfastapi-cli                              0.0.7\nfastavro                                 1.10.0\nfilelock                                 3.17.0\nfiletype                                 1.2.0\nfsspec                                   2024.12.0\ngoogle-auth                              2.38.0\ngoogleapis-common-protos                 1.66.0\ngreenlet                                 3.1.1\ngriffe                                   1.5.5\ngroq                                     0.15.0\nh11                                      0.14.0\nhttpcore                                 1.0.7\nhttptools                                0.6.4\nhttpx                                    0.28.1\nhttpx-sse                                0.4.0\nhuggingface-hub                          0.27.1\nidna                                     3.10\nimportlib_metadata                       8.5.0\niniconfig                                2.0.0\nitsdangerous                             2.2.0\nJinja2                                   3.1.5\njiter                                    0.8.2\njmespath                                 1.0.1\njsonpath-python                          1.0.6\nlogfire                                  3.14.0\nlogfire-api                              3.14.0\nMako                                     1.3.9\nmarkdown-it-py                           3.0.0\nMarkupSafe                               3.0.2\nmcp                                      1.6.0\nmdurl                                    0.1.2\nmistralai                                1.4.0\nmonotonic                                1.6\nmypy-extensions                          1.0.0\nopenai                                   1.75.0\nopentelemetry-api                        1.29.0\nopentelemetry-exporter-otlp-proto-common 1.29.0\nopentelemetry-exporter-otlp-proto-http   1.29.0\nopentelemetry-instrumentation            0.50b0\nopentelemetry-instrumentation-httpx      0.50b0\nopentelemetry-proto                      1.29.0\nopentelemetry-sdk                        1.29.0\nopentelemetry-semantic-conventions       0.50b0\nopentelemetry-util-http                  0.50b0\norjson                                   3.10.15\npackaging                                24.2\npip                                      23.2.1\npluggy                                   1.5.0\nposthog                                  3.17.0\nprompt_toolkit                           3.0.50\nprotobuf                                 5.29.3\npsycopg-binary                           3.2.5\npyasn1                                   0.6.1\npyasn1_modules                           0.4.1\npydantic                                 2.11.3\npydantic-ai                              0.1.2\npydantic-ai-slim                         0.1.2\npydantic_core                            2.33.1\npydantic-evals                           0.1.2\npydantic-extra-types                     2.10.2\npydantic-graph                           0.1.2\npydantic-settings                        2.7.1\nPygments                                 2.19.1\npytest                                   8.3.4\npytest-asyncio                           0.25.3\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.0.1\npython-json-logger                       3.2.1\npython-multipart                         0.0.20\nPyYAML                                   6.0.2\nr2r                                      3.5.10\nregex                                    2024.11.6\nrequests                                 2.32.3\nrich                                     13.9.4\nrich-toolkit                             0.13.2\nrsa                                      4.9\ns3transfer                               0.11.4\nshellingham                              1.5.4\nsix                                      1.17.0\nsniffio                                  1.3.1\nSQLAlchemy                               2.0.38\nsse-starlette                            2.2.1\nstarlette                                0.45.3\ntiktoken                                 0.8.0\ntokenizers                               0.21.0\ntoml                                     0.10.2\ntqdm                                     4.67.1\ntyper                                    0.15.1\ntypes-aiofiles                           24.1.0.20241221\ntypes-requests                           2.32.0.20241016\ntyping_extensions                        4.12.2\ntyping-inspect                           0.9.0\ntyping-inspection                        0.4.0\nujson                                    5.10.0\nurllib3                                  2.3.0\nuvicorn                                  0.34.0\nuvloop                                   0.21.0\nwatchfiles                               1.0.4\nwcwidth                                  0.2.13\nwebsockets                               14.2\nwrapt                                    1.17.2\nzipp                                     3.21.0\n\n\nPython 3.12.3 version\n```",
      "state": "closed",
      "author": "peter-zigo-kinit",
      "author_type": "User",
      "created_at": "2025-04-17T14:52:13Z",
      "updated_at": "2025-04-19T05:08:54Z",
      "closed_at": "2025-04-18T20:45:45Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1525/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1525",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1525",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:53.257431",
      "comments": [
        {
          "author": "DouweM",
          "body": "@peter-zigo-kinit The error suggests that you've defined a tool that returns an object with an `opentelemetry.trace.span.NonRecordingSpan` somewhere in the object tree, that can't be serialized to be passed to the LLM. It would be useful to see the tools you've defined and their return types, but if",
          "created_at": "2025-04-17T20:06:50Z"
        },
        {
          "author": "DouweM",
          "body": "@peter-zigo-kinit Looks like it's a bug on our side -- another user just hit this as well: https://github.com/pydantic/pydantic-ai/issues/1533. Let me look into it.",
          "created_at": "2025-04-17T23:30:17Z"
        },
        {
          "author": "DouweM",
          "body": "@peter-zigo-kinit Is it possible you have a tool with `return await agent.run(...)` instead of `return await agent.run(...).output`?",
          "created_at": "2025-04-17T23:58:23Z"
        },
        {
          "author": "peter-zigo-kinit",
          "body": "Hi @DouweM , \nThanks for the quick support. I am working on a project, which will be fully open-source, so I don't have any restrictions on sharing code.\n\n\nThe small summary of the project: \nWe try to create AI chat assistant for AutoML training. \nCurrently, our main flow structure is most similar t",
          "created_at": "2025-04-18T09:09:35Z"
        },
        {
          "author": "DouweM",
          "body": "@peter-zigo-kinit Thanks, that helps. \n\nWhile it technically worked in the past, if you return the entire `AgentRunResult` from a tool, the entire object will be serialized and sent to the LLM including a lot of PydanticAI-specific stuff that's unnecessary. So what you'll want to do is take the outp",
          "created_at": "2025-04-18T15:29:41Z"
        }
      ]
    },
    {
      "issue_number": 1521,
      "title": "Node attributes vs state",
      "body": "### Question\n\nCould you explain the reasoning or guidelines for deciding when to use an attribute within a node versus using a mutable state? For instance, in the `genai_email_feedback.py` example provided in the documentation, both approaches are utilized, but the criteria for choosing one over the other are unclear.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ag14774",
      "author_type": "User",
      "created_at": "2025-04-17T09:13:44Z",
      "updated_at": "2025-04-18T22:38:21Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1521/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1521",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1521",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:53.475356",
      "comments": [
        {
          "author": "DouweM",
          "body": "@ag14774 The State object is passed to every node, so you can use it for data known upfront that different nodes may need access to (like the `User` in the example), or for keeping track of things that happened during the agent run that different future nodes may need access to (like the message his",
          "created_at": "2025-04-17T19:55:44Z"
        },
        {
          "author": "ag14774",
          "body": "Thank you for the reply.\n\nIf the `User` is known in advance, and there is no plan to mutate it, why not just use it as a dependency instead?\n\nIn addition, in the first example where the number is incremented until it is divisible by 5, it really feels like the graph has a state even though it doesn'",
          "created_at": "2025-04-18T22:37:17Z"
        }
      ]
    },
    {
      "issue_number": 1546,
      "title": "Weather Agent Gradio Issues",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nMRE: Copy and paste the weather_agent.py and weather_agent_gradio.py example from the documentation. Geocoding and Weather API keys not set.\n\nSeeing:\nGradio server launches fine but raises several errors when message is sent.\n\nIssues:\n1. Line 36: `'call_args = ('` raises `AttributeError: 'str' object has no attribute 'args_dict'`  \n    I assume this was due to an type change of ToolCallPart.args. Can fix by changing the code to:\n    `call_args = (\n                        json.dumps(call.args)\n                        if type(call.args) == dict\n                        else call.args\n     )`\n\n2. Line 45: `metadata['id'] = {call.tool_call_id}` raises `ValidationError: Input should be a valid integer [type=int_type, input_value={'call_y9PvwRNt2hcxhxTVl4UGuerV'}, input_type=set]` \n    Fix is pretty simple. Remove curly braces around call.tool_call_id.\n\n3. Fixing the two issues above, the tool now returns an answer correctly but asking two questions (ie \"What is the weather like in Miami?\" then \"What is the weather like in London?\") raises `AttributeError: 'NoneType' object has no attribute 'get' from line 56: gr_message.get('metadata', {}).get('id', '')`. Printing gr_message shows: `{'role': 'user', 'metadata': None, 'content': 'What is the weather like in Miami?', 'options': None}` **AFTER** the question about Miami was already asked, successfully returned, and then the question about London is asked. Not sure why the message is re-appearing. I have also seen gr_message be None altogether.\n\n### Example Code\nhello_world.py (same as weather_agent.py)\n```Python\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom devtools import debug\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\n\nweather_agent = Agent(\n    'openai:gpt-4o',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    system_prompt=(\n        'Be concise, reply with one sentence.'\n        'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n        'then use the `get_weather` tool to get the weather.'\n    ),\n    deps_type=Deps,\n    retries=2,\n    instrument=True,\n)\n\n\n@weather_agent.tool\nasync def get_lat_lng(\n    ctx: RunContext[Deps], location_description: str\n) -> dict[str, float]:\n    \"\"\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \"\"\"\n    if ctx.deps.geo_api_key is None:\n        # if no API key is provided, return a dummy response (London)\n        return {'lat': 51.1, 'lng': -0.1}\n\n    params = {\n        'q': location_description,\n        'api_key': ctx.deps.geo_api_key,\n    }\n    with logfire.span('calling geocode API', params=params) as span:\n        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    if data:\n        return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n    else:\n        raise ModelRetry('Could not find the location')\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \"\"\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \"\"\"\n    if ctx.deps.weather_api_key is None:\n        # if no API key is provided, return a dummy response\n        return {'temperature': '21 °C', 'description': 'Sunny'}\n\n    params = {\n        'apikey': ctx.deps.weather_api_key,\n        'location': f'{lat},{lng}',\n        'units': 'metric',\n    }\n    with logfire.span('calling weather API', params=params) as span:\n        r = await ctx.deps.client.get(\n            'https://api.tomorrow.io/v4/weather/realtime', params=params\n        )\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    values = data['data']['values']\n    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n    code_lookup = {\n        1000: 'Clear, Sunny',\n        1100: 'Mostly Clear',\n        1101: 'Partly Cloudy',\n        1102: 'Mostly Cloudy',\n        1001: 'Cloudy',\n        2000: 'Fog',\n        2100: 'Light Fog',\n        4000: 'Drizzle',\n        4001: 'Rain',\n        4200: 'Light Rain',\n        4201: 'Heavy Rain',\n        5000: 'Snow',\n        5001: 'Flurries',\n        5100: 'Light Snow',\n        5101: 'Heavy Snow',\n        6000: 'Freezing Drizzle',\n        6001: 'Freezing Rain',\n        6200: 'Light Freezing Rain',\n        6201: 'Heavy Freezing Rain',\n        7000: 'Ice Pellets',\n        7101: 'Heavy Ice Pellets',\n        7102: 'Light Ice Pellets',\n        8000: 'Thunderstorm',\n    }\n    return {\n        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        # create a free API key at https://www.tomorrow.io/weather-api/\n        weather_api_key = os.getenv('WEATHER_API_KEY')\n        # create a free API key at https://geocode.maps.co/\n        geo_api_key = os.getenv('GEO_API_KEY')\n        deps = Deps(\n            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n        )\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        debug(result)\n        print('Response:', result.output)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\nweather_agent_gradio.py (with fixes 1 & 2)\n```Python\nfrom __future__ import annotations as _annotations\n\nimport json\nimport os\n\nfrom httpx import AsyncClient\n\nfrom pydantic_ai.messages import ToolCallPart, ToolReturnPart\nfrom hello_world import Deps, weather_agent\n\ntry:\n    import gradio as gr\nexcept ImportError as e:\n    raise ImportError(\n        'Please install gradio with `pip install gradio`. You must use python>=3.10.'\n    ) from e\n\nTOOL_TO_DISPLAY_NAME = {'get_lat_lng': 'Geocoding API', 'get_weather': 'Weather API'}\n\nclient = AsyncClient()\nweather_api_key = os.getenv('WEATHER_API_KEY')\n# create a free API key at https://geocode.maps.co/\ngeo_api_key = os.getenv('GEO_API_KEY')\ndeps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)\n\n\nasync def stream_from_agent(prompt: str, chatbot: list[dict], past_messages: list):\n    chatbot.append({'role': 'user', 'content': prompt})\n    yield gr.Textbox(interactive=False, value=''), chatbot, gr.skip()\n    async with weather_agent.run_stream(\n        prompt, deps=deps, message_history=past_messages\n    ) as result:\n        for message in result.new_messages():\n            for call in message.parts:\n                if isinstance(call, ToolCallPart):\n                    call_args = (\n                        json.dumps(call.args)\n                        if type(call.args) == dict\n                        else call.args\n                    )\n                    metadata = {\n                        'title': f'🛠️ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}',\n                    }\n                    if call.tool_call_id is not None:\n                        metadata['id'] = call.tool_call_id\n\n                    gr_message = {\n                        'role': 'assistant',\n                        'content': 'Parameters: ' + call_args,\n                        'metadata': metadata,\n                    }\n                    chatbot.append(gr_message)\n                if isinstance(call, ToolReturnPart):\n                    for gr_message in chatbot:\n                        print(gr_message)\n                        if (\n                            gr_message.get('metadata', {}).get('id', '')\n                            == call.tool_call_id\n                        ):\n                            gr_message['content'] += (\n                                f'\\nOutput: {json.dumps(call.content)}'\n                            )\n                yield gr.skip(), chatbot, gr.skip()\n        chatbot.append({'role': 'assistant', 'content': ''})\n        async for message in result.stream_text():\n            chatbot[-1]['content'] = message\n            yield gr.skip(), chatbot, gr.skip()\n        past_messages = result.all_messages()\n\n        yield gr.Textbox(interactive=True), gr.skip(), past_messages\n\n\nasync def handle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\n    new_history = chatbot[: retry_data.index]\n    previous_prompt = chatbot[retry_data.index]['content']\n    past_messages = past_messages[: retry_data.index]\n    async for update in stream_from_agent(previous_prompt, new_history, past_messages):\n        yield update\n\n\ndef undo(chatbot, past_messages: list, undo_data: gr.UndoData):\n    new_history = chatbot[: undo_data.index]\n    past_messages = past_messages[: undo_data.index]\n    return chatbot[undo_data.index]['content'], new_history, past_messages\n\n\ndef select_data(message: gr.SelectData) -> str:\n    return message.value['text']\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n        \"\"\"\n<div style=\"display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%\">\n    <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\n    <div>\n        <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\n        <h3 style=\"margin: 0 0 0.5rem 0\">\n            This assistant answer your weather questions.\n        </h3>\n    </div>\n</div>\n\"\"\"\n    )\n    past_messages = gr.State([])\n    chatbot = gr.Chatbot(\n        label='Packing Assistant',\n        type='messages',\n        avatar_images=(None, 'https://ai.pydantic.dev/img/logo-white.svg'),\n        examples=[\n            {'text': 'What is the weather like in Miami?'},\n            {'text': 'What is the weather like in London?'},\n        ],\n    )\n    with gr.Row():\n        prompt = gr.Textbox(\n            lines=1,\n            show_label=False,\n            placeholder='What is the weather like in New York City?',\n        )\n    generation = prompt.submit(\n        stream_from_agent,\n        inputs=[prompt, chatbot, past_messages],\n        outputs=[prompt, chatbot, past_messages],\n    )\n    chatbot.example_select(select_data, None, [prompt])\n    chatbot.retry(\n        handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\n    )\n    chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\n\n\nif __name__ == '__main__':\n    demo.launch()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.11.11\nPydantic: 2.11.1\nPydantic-AI: 0.1.3\nLLM: openai:gpt-4o\n```",
      "state": "open",
      "author": "bootloop-noah",
      "author_type": "User",
      "created_at": "2025-04-18T19:29:36Z",
      "updated_at": "2025-04-18T21:41:43Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1546/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1546",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1546",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:54.704772",
      "comments": []
    },
    {
      "issue_number": 1156,
      "title": "All parameters part of OpenAI Model Settings are currently not mapped during chat completion",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nFor chat completion in the OpenAI model, we are not mapping all the parameters from the model settings \n\nExample : \n`user` parameter is missing in the completions create call. So even if we include it in the OpenAIModelSettings - it is not picked up during the completion API call\n\n![Image](https://github.com/user-attachments/assets/8b666caf-637e-4df3-90c6-3b08bbe08596)\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n>> python3 --version\nPython 3.11.6\n\n>> pip3 list | grep pydantic-ai                                                                                \npydantic-ai        0.0.41\npydantic-ai-slim   0.0.41\n\n>> pip3 list | grep openai                                                                                     \nopenai             1.66.3\n```",
      "state": "closed",
      "author": "ovisek",
      "author_type": "User",
      "created_at": "2025-03-18T07:40:22Z",
      "updated_at": "2025-04-18T13:13:51Z",
      "closed_at": "2025-04-18T11:20:02Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1156/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1156",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1156",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:54.704793",
      "comments": [
        {
          "author": "Kludex",
          "body": "Is there any other field that we should add?",
          "created_at": "2025-03-25T12:46:49Z"
        },
        {
          "author": "ovisek",
          "body": "> Is there any other field that we should add?\n\nWe should ideally add all the supported parameters as per https://platform.openai.com/docs/api-reference/chat/create \nBut we can do that on a need-to basis.",
          "created_at": "2025-03-25T17:11:26Z"
        },
        {
          "author": "Kludex",
          "body": "Most of the parameters are handled now. If we are missing any, please create a new PR.\n\nHappy to add all settings needed. :) ",
          "created_at": "2025-04-18T11:20:02Z"
        },
        {
          "author": "ovisek",
          "body": "Thanks for the update. I noticed that the model parameters are now prepended with `openai`. Example : `openai_user` for `user` and `openai_reasoning_effort` for `reasoning_effort` . I think it would be better to keep the model parameters in sync with the original specs\nReference : \nhttps://github.co",
          "created_at": "2025-04-18T12:31:44Z"
        },
        {
          "author": "Kludex",
          "body": "You mean without the prefix?\n\nThe idea is that you can merge different settings from different providers. If the setting is global can be used by different providers, we put them in the global `ModelSettings`, if it's specific, it goes to the specific `<provider>ModelSettings`.",
          "created_at": "2025-04-18T12:41:05Z"
        }
      ]
    },
    {
      "issue_number": 996,
      "title": "Perplexity AI Citations",
      "body": "### Description\n\nPerplexity AI has a citations field in their response that usually works like this\n\n```python\nresponse_stream = client.chat.completions.create(\n    model=\"sonar-pro\",\n    messages=messages,\n    stream=True,\n)\n\ncitations = []\nfor response in response_stream:\n    print(response.choices[0].delta.content, end=\"\")\n    citations.extend(response.citations)\n```\n\nYou now have access to a list of citations that you can use reliably.\n\nIs there any way in pydantic_ai to access this?\n\nThe perplexity model is not very reliable at producing inline citations. So it would be useful to be able to do this and access their citations.",
      "state": "open",
      "author": "vikigenius",
      "author_type": "User",
      "created_at": "2025-02-26T16:58:02Z",
      "updated_at": "2025-04-18T13:01:22Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "citations"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/996/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/996",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/996",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:55.013433",
      "comments": [
        {
          "author": "Kludex",
          "body": "Hi @vikigenius,\n\nCan you give me an MRE to run Perplexity AI?",
          "created_at": "2025-02-27T11:11:04Z"
        },
        {
          "author": "vikigenius",
          "body": "If you mean using the standard OpenAI API\n\n```python\nimport os\nfrom openai import OpenAI\n\nYOUR_API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": (\n            \"You are an artificial intelligence assistant that helps users answer questions \"\n  ",
          "created_at": "2025-02-27T13:04:51Z"
        },
        {
          "author": "vikigenius",
          "body": "I did manage to regex substitute citations into markdown links by creating a custom model like this.\n\n```python\n# -*- coding: utf-8 -*-\nfrom __future__ import annotations\nimport re\nfrom collections.abc import AsyncIterator\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timez",
          "created_at": "2025-02-27T13:07:15Z"
        },
        {
          "author": "jerry-reevo",
          "body": "@vikigenius Out of curiosity, do you ever see a response chunk split the citation? e.g., \n\nchunk 1: `Statement[1`\nchunk 2: `][2]. Other text`",
          "created_at": "2025-03-05T19:42:53Z"
        },
        {
          "author": "vikigenius",
          "body": "> [@vikigenius](https://github.com/vikigenius) Out of curiosity, do you ever see a response chunk split the citation? e.g.,\n> \n> chunk 1: `Statement[1` chunk 2: `][2]. Other text`\n\nNope, never seen it happen so far",
          "created_at": "2025-03-06T03:31:17Z"
        }
      ]
    },
    {
      "issue_number": 1438,
      "title": "LLM Request Timeout",
      "body": "### Question\n\nHi,\n\nEvery now and then I encounter the issue that the LLM request gets stuck and thus the agent gets stuck without throwing an error.\nIs there a way to set a timeout for LLM requests? And even better, the option to retry if one of them times out?\n\n### Additional Context\n\nPydantic AI 0.0.43\nPython 3.11.7",
      "state": "open",
      "author": "rogerwelo",
      "author_type": "User",
      "created_at": "2025-04-10T15:49:32Z",
      "updated_at": "2025-04-18T11:27:21Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1438/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1438",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1438",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:55.284058",
      "comments": [
        {
          "author": "ItzAmirreza",
          "body": "I'm also experiencing the same issue... For more context, I'm serving Qwen2.5-VL-7B using vLLM, tools do get called, but then response is always empty, and it just keeps doing the tool again and again, until it reaches timeout.",
          "created_at": "2025-04-10T17:05:13Z"
        },
        {
          "author": "Kludex",
          "body": "How can I reproduce this? Please provide code and what I need to run.",
          "created_at": "2025-04-18T11:27:20Z"
        }
      ]
    },
    {
      "issue_number": 1512,
      "title": "Bug validating ToolCallPart when the args is empty",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nFollow-up to this PR: https://github.com/pydantic/pydantic-ai/pull/1407\n\nJust tried the following tool call:\n\n```\nFunctionToolCallEvent(part=ToolCallPart(tool_name='list_allowed_directories', args='', tool_call_id='tooluse_I1mMdQl5R2WGRsWGJndR3A', part_kind='tool-call'), call_id='tooluse_I1mMdQl5R2WGRsWGJndR3A', \nevent_kind='function_tool_call')\n```\n\nIt is failing on my end:\n\n```\nTraceback (most recent call last):\n  File \"/home/arnaud/agent-loop/agent/try_pydantic.py\", line 158, in <module>\n    asyncio.run(try_pydantic())\n  File \"/home/arnaud/.pyenv/versions/3.12.7/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/.pyenv/versions/3.12.7/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/.pyenv/versions/3.12.7/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/agent/try_pydantic.py\", line 153, in try_pydantic\n    await process_node(message, run, table)\n  File \"/home/arnaud/agent-loop/agent/try_pydantic.py\", line 42, in process_node\n    async with node.stream(run.ctx) as request_stream:\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/.pyenv/versions/3.12.7/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 278, in stream\n    async with self._stream(ctx) as streamed_response:\n               ^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/.pyenv/versions/3.12.7/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 301, in _stream\n    async with ctx.deps.model.request_stream(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/.pyenv/versions/3.12.7/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/.venv/lib/python3.12/site-packages/pydantic_ai/models/bedrock.py\", line 247, in request_stream\n    response = await self._messages_create(messages, True, settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/.venv/lib/python3.12/site-packages/pydantic_ai/models/bedrock.py\", line 309, in _messages_create\n    system_prompt, bedrock_messages = await self._map_messages(messages)\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/.venv/lib/python3.12/site-packages/pydantic_ai/models/bedrock.py\", line 422, in _map_messages\n    content.append(self._map_tool_call(item))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/.venv/lib/python3.12/site-packages/pydantic_ai/models/bedrock.py\", line 488, in _map_tool_call\n    'toolUse': {'toolUseId': _utils.guard_tool_call_id(t=t), 'name': t.tool_name, 'input': t.args_as_dict()}\n                                                                                           ^^^^^^^^^^^^^^^^\n  File \"/home/arnaud/agent-loop/.venv/lib/python3.12/site-packages/pydantic_ai/messages.py\", line 511, in args_as_dict\n    args = pydantic_core.from_json(self.args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: EOF while parsing a value at line 1 column 0\n```\n\nLooks like we need to support empty string args in this function too:\nhttps://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/messages.py#L509-L511\n\nShould be as simple as returning an empty dict if args is an empty string. That being said, this was not caught by tests so there are probably some tests to add\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12\npydantic-ai==0.1.1\nLLM -> Sonnet 3.7 on Bedrock\n```",
      "state": "closed",
      "author": "arnaudstiegler",
      "author_type": "User",
      "created_at": "2025-04-16T21:44:48Z",
      "updated_at": "2025-04-18T10:37:54Z",
      "closed_at": "2025-04-18T10:37:53Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1512/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1512",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1512",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:55.561077",
      "comments": []
    },
    {
      "issue_number": 1101,
      "title": "Support arbitrary attributes in `ModelSettings` for use by OpenAI compatible providers",
      "body": "### Description\n\n`ModelSettings` currently only forwards [allow-listed attributes into the HTTP request body](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py#L273-L290).\n\nThe `OpenAIModel` is used by compatible providers, such as [OpenRouter](https://openrouter.ai/), which accept numerous custom parameters in the request body such as [model-routing](https://openrouter.ai/docs/features/model-routing#the-models-parameter) and [provider-routing](https://openrouter.ai/docs/features/provider-routing).\n\nAs far as I can tell, the only way to inject custom parameters into the HTTP request body is to subclass `OpenAIModel` and reimplement a relatively large block of code.\n\nI'd like a way to easily define custom model settings such as by sub-classing `ModelSettings` or by forwarding arbitrary `ModelSettings` attributes on to the HTTP request, or both.\n\n### References\n\nIssue https://github.com/pydantic/pydantic-ai/issues/856#issuecomment-2658327288 is another issue where this topic was discussed and PR https://github.com/pydantic/pydantic-ai/pull/1006 proposes allowing arbitrary attributes.",
      "state": "open",
      "author": "wes-augur",
      "author_type": "User",
      "created_at": "2025-03-12T02:02:08Z",
      "updated_at": "2025-04-18T10:11:08Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "model settings"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1101/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1101",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1101",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:57.358751",
      "comments": [
        {
          "author": "Kludex",
          "body": "How do you see this happening in PydanticAI?",
          "created_at": "2025-04-18T10:11:01Z"
        }
      ]
    },
    {
      "issue_number": 1460,
      "title": "How to set repetition penalty?",
      "body": "### Question\n\nThere isn't an option for it in ModelSettings (specifically openai)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ItzAmirreza",
      "author_type": "User",
      "created_at": "2025-04-12T08:07:20Z",
      "updated_at": "2025-04-18T09:58:54Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1460/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1460",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1460",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:57.627103",
      "comments": [
        {
          "author": "AndreBremer",
          "body": "Duplicate of #1326 \n",
          "created_at": "2025-04-17T23:49:39Z"
        },
        {
          "author": "Kludex",
          "body": "What option is that? I don't see it on OpenAI.",
          "created_at": "2025-04-18T09:58:30Z"
        }
      ]
    },
    {
      "issue_number": 1528,
      "title": "Instruction missing from the message history in logfire",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nThe model instructions are missing from the message history when tracing with an instrumented agent.\n\nI am able to see the instructions if I instrument the OpenAI client that the agent is using with logfire, so they are actually sent, just not traced in the standard PydanticAI instrumentation.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai v0.1.1\n```",
      "state": "closed",
      "author": "mikkelalv",
      "author_type": "User",
      "created_at": "2025-04-17T15:31:45Z",
      "updated_at": "2025-04-18T06:29:26Z",
      "closed_at": "2025-04-18T06:29:26Z",
      "labels": [
        "bug",
        "OpenTelemetry"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1528/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1528",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1528",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:57.876225",
      "comments": []
    },
    {
      "issue_number": 1492,
      "title": "Cannot serialize a ModelRequest with binary content.",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIf I call agent like this:\n\n```\nresponse = agent.run_sync([\n   BinaryContent(data=some_bytes, media_type=some_mime_type)\n] \n```\n\nAnd then call response.new_messages_json(), I get this error:\n\nError serializing to JSON: invalid utf-8 sequence of 1 bytes from index 0\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12, Pydantic AI 0.0.36, openai:gpt-4o\n```",
      "state": "closed",
      "author": "ararog",
      "author_type": "User",
      "created_at": "2025-04-16T01:59:07Z",
      "updated_at": "2025-04-17T21:26:37Z",
      "closed_at": "2025-04-17T07:21:14Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1492/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1492",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1492",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:57.876245",
      "comments": [
        {
          "author": "DouweM",
          "body": "@ararog Good catch, we'll want to change the `bytes` type on `BinaryContent` to the [`Base64Bytes`](https://docs.pydantic.dev/latest/api/types/#pydantic.types.Base64Bytes) type from Pydantic to ensure it's encoded/decoded on serialization/validation.\n\nI'll put up a PR shortly.\n\ncc @Kludex\n\nEdit: We'",
          "created_at": "2025-04-16T21:33:59Z"
        },
        {
          "author": "ararog",
          "body": "I updated Pydantic AI to version 0.1.0, I also changed column type on Postgres from bytea to text, now i can't even restore message due to way as sqlmodel deals with bytes and text column.\n\nIf I use bytes with bytea, base64 get persisted with some issues, so the way as you considering to fix seriali",
          "created_at": "2025-04-16T22:28:13Z"
        },
        {
          "author": "DouweM",
          "body": "@ararog Can you try installing Pydantic from this PR, where I've set it to not just serialize bytes fields as base64, but also validate (decode) base64 back to bytes? https://github.com/pydantic/pydantic-ai/pull/1513",
          "created_at": "2025-04-16T22:30:06Z"
        },
        {
          "author": "Kludex",
          "body": "- Solved in https://github.com/pydantic/pydantic-ai/pull/1513\n\nThanks!",
          "created_at": "2025-04-17T07:21:14Z"
        },
        {
          "author": "ararog",
          "body": "@DouweM @Kludex it is working as expected now, thanks guys!",
          "created_at": "2025-04-17T21:26:36Z"
        }
      ]
    },
    {
      "issue_number": 1495,
      "title": "Best practices for managing tokens in context with tool calling",
      "body": "### Question\n\nHello,\n\nI'm implementing a chatbot which relies on tool calling to retrieve some data when the user asks for a relevant question. My challenge is that since a chat conversation can be multiple turns long, the tool data add up really quickly.\n\nE.g.\n\n1. User: \"Tell me what invoices are due this week\"\n2. [Tool call retrieves raw data of invoices, stripped to essential info]\n3. [LLM converts the tool call output into an answer]\n4. User: \"How about this month?\"\n5. [Repeat tool call / re-structuring]\n6. User: \"How about last 3 months?\"\n(etc.)\n\nSo initially I tried a naive method of removing stale tool call information e.g. in above, when I'm at step 6, the \"tool call raw data\" in step 2 isn't required, so I can purge it from message history. But it requires some extra 're-processing\" of the pydantic output (e.g. if I remove the tool call data, I also need to remove the tool call ID reference, restructure the ModelMessage, etc.) \n\nBut I'm not sure if that's the right way to go about it (since it feels I'm fighting \"against\" pydantic)?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "jpmasud",
      "author_type": "User",
      "created_at": "2025-04-16T04:36:33Z",
      "updated_at": "2025-04-17T21:16:07Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1495/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1495",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1495",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:58.188241",
      "comments": [
        {
          "author": "HamzaFarhan",
          "body": "You could summarize the convo after `n` messages and then just send that as a single message and start building the history again.\nThese blogs have some nice ideas:\nLangChain: [How to add memory to chatbots](https://langchain-ai.github.io/langgraph/concepts/memory/)\nMine 👀 : [Adding a Memory layer t",
          "created_at": "2025-04-17T21:16:05Z"
        }
      ]
    },
    {
      "issue_number": 1457,
      "title": "Disable tool use while having structured outputs",
      "body": "### Question\n\nHello, I'm using OpenRouter with PydanticAI, but unfortunately for multiple models I'm getting this error:\n```\n{'message': 'No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/provider-routing', 'code': 404}\n```\n\nI want to leverage structured outputs with custom result types but don't want to use tools functionality, is it possible to achieve something like this? This would allow me to use much more various models. \n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "dimentary",
      "author_type": "User",
      "created_at": "2025-04-11T18:53:26Z",
      "updated_at": "2025-04-17T18:30:34Z",
      "closed_at": "2025-04-17T18:30:34Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1457/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1457",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1457",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:58.393998",
      "comments": [
        {
          "author": "dimentary",
          "body": "For example, Instructor library supports something like [JSON mode](https://python.useinstructor.com/integrations/openrouter/#json-mode) to handle models that do not support tool use. I basically need something similar with PydanticAI.",
          "created_at": "2025-04-13T15:23:54Z"
        },
        {
          "author": "aristide1997",
          "body": "This is a recurring issue that keeps popping up. Hopefully a solution can be found",
          "created_at": "2025-04-17T17:39:46Z"
        },
        {
          "author": "DouweM",
          "body": "@dimentary We're tracking this in https://github.com/pydantic/pydantic-ai/issues/582, which is already on the roadmap for the near term!",
          "created_at": "2025-04-17T18:25:10Z"
        }
      ]
    },
    {
      "issue_number": 674,
      "title": "Handling Incorrect Error Type in validate_structured_result Causes Context Detachment Issue",
      "body": "When using the agent.run_stream functionality and attempting to catch with an incorrect exception type during validation with validate_structured_result, the code doesn’t handle the error correctly. \r\nInstead, it fails with a context detachment issue.\r\n\r\nThe exception doesn’t get caught, and fail.\r\n\r\nI’m unsure whether this issue falls under PydanticAI’s responsibility or if it’s expected to be handled by the developer. \r\n👉 However, I’m reporting it in case it can help identify a solution for the next developer.\r\n\r\n(You can close this issue if needed.)\r\n\r\nHere is a minimal example to reproduce the issue:\r\n```\r\nasync with agent.run_stream(...):\r\n    async for message, last in result.stream_structured():\r\n        try:\r\n            profile = await result.validate_structured_result(\r\n                message,\r\n                allow_partial=not last,\r\n            )\r\n        except ValidationError:  # <<< The error isn't caught here as expected (incorrect exception type)\r\n            continue\r\n        # Nothing more\r\n```\r\n\r\nThe above code fails to handle errors properly and throws the following traceback:\r\n```\r\nFailed to detach context\r\nTraceback (most recent call last):\r\n  File \"..../pydantic_ai/result.py\", line 302, in stream_structured\r\n    yield msg, False\r\nGeneratorExit\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"..../opentelemetry/context/__init__.py\", line 152, in detach\r\n    _RUNTIME_CONTEXT.detach(token)\r\n  File \"..../opentelemetry/context/contextvars_context.py\", line 50, in detach\r\n    self._current_context.reset(token)  # type: ignore\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: <Token var=<ContextVar name='current_context' default={} at xxx> at xxx> was created in a different Context\r\n```",
      "state": "open",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-01-13T19:04:25Z",
      "updated_at": "2025-04-17T15:56:03Z",
      "closed_at": null,
      "labels": [
        "bug",
        "OpenTelemetry"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/674/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/674",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/674",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:58.630778",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Thanks for reporting.\r\n\r\n@alexmojaki is this a bug with logfire, opentelemetry or how I've done something in pydantic-ai?",
          "created_at": "2025-01-16T10:12:16Z"
        },
        {
          "author": "alexmojaki",
          "body": "https://logfire.pydantic.dev/docs/reference/advanced/generators/\r\n\r\nhttps://github.com/pydantic/pydantic-ai/blob/f01f099f4b7527993d556c85cc2046eb2a3acd9e/pydantic_ai_slim/pydantic_ai/result.py#L291-L302\r\n\r\nThis happens when `with span` contains `yield`",
          "created_at": "2025-01-16T10:33:51Z"
        },
        {
          "author": "martgra",
          "body": "@alexmojaki what would be the correct work-around currently? \n\nI try to slowly move into pydantic-ai but as of now not planning to use logfire at all. \n\nGetting this exception when stream is cancelled from client application. \n\n```python\nasync def get_streaming_response(query: str) -> dict:\n    \"\"\"U",
          "created_at": "2025-03-17T14:00:08Z"
        },
        {
          "author": "alexmojaki",
          "body": "Trying to look into this, things got weirder:\n\n```python\nimport asyncio\n\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\n\nasync def main():\n    async with agent.run_stream('Hello') as stream:\n        async for _ in stream.stream_text():\n            break\n\n\nasyncio.run(main())\n```\n\nlog",
          "created_at": "2025-03-17T15:19:55Z"
        },
        {
          "author": "alexmojaki",
          "body": "Trying to narrow things further:\n\n```python\nimport asyncio\n\nfrom pydantic_ai._utils import group_by_temporal\n\n\nasync def gen():\n    yield 1\n    await asyncio.sleep(0.2)\n    yield 2\n\n\nasync def main():\n    g = gen()\n    async with group_by_temporal(g, 0.1) as t:\n        async for x in t:\n            ",
          "created_at": "2025-03-17T15:28:39Z"
        }
      ]
    },
    {
      "issue_number": 619,
      "title": "Message history is too long with OpenAI",
      "body": "With open ai, I'm getting a max message error. Is there a strategy for avoiding this? I naively assumed that Pydantic might do something on the backend to limit the messages so they don't grow too large (or only send the last 10 or something).",
      "state": "closed",
      "author": "brightwaytony",
      "author_type": "User",
      "created_at": "2025-01-06T16:45:33Z",
      "updated_at": "2025-04-17T15:53:10Z",
      "closed_at": "2025-04-17T15:53:10Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/619/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/619",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/619",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:58.915818",
      "comments": [
        {
          "author": "brightwaytony",
          "body": "In case you want inspiration, here is what llama index is doing: https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
          "created_at": "2025-01-06T16:49:16Z"
        },
        {
          "author": "izzyacademy",
          "body": "@brightwaytony if the message history is too long and exceeds the limit for your LLM, you should be able to limit what is sent over by only using a subset of the list of messages you pass to the run() or run_sync() methods on the Agent object.",
          "created_at": "2025-01-07T14:40:15Z"
        },
        {
          "author": "brightwaytony",
          "body": "> @brightwaytony if the message history is too long and exceeds the limit for your LLM, you should be able to limit what is sent over by only using a subset of the list of messages you pass to the run() or run_sync() methods on the Agent object.\r\n\r\nSure. @samuelcolvin asked me to create this as an i",
          "created_at": "2025-01-07T15:25:00Z"
        },
        {
          "author": "izzyacademy",
          "body": "@brightwaytony that makes sense. Thanks for sharing the context of the issue creation.",
          "created_at": "2025-01-07T15:34:11Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Sorry I haven't got to this yet, PR welcome to implement it.",
          "created_at": "2025-01-18T15:06:28Z"
        }
      ]
    },
    {
      "issue_number": 1215,
      "title": "Unable to use Local LLM via Transformers Library with PydanticAI",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI tried configuring an agent for my locally hosted LLM (Phi-4) that uses the Transformers Library and it does not work. When I put 'phi_4_model' as the 'provider', this does not work.\n\nMaybe the HuggingFace crew can join in on this for help since Transformers is their library?\n\nI hope this is just an ID10T error on my part.\n\nI'm not posting code on my agent because of work sensitivity, but for the purposes of this bug, providing an example with a basic PydantiAI agent with the solution will be good enough.\n\nThanks in advance for all responses.\n\n### Example Code\n\n```Python\ntry:\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    phi_4_tokenizer = AutoTokenizer.from_pretrained(phi_4_model_path, trust_remote_code=True)\n    phi_4_model = AutoModelForCausalLM.from_pretrained(phi_4_model_path, trust_remote_code=True, device_map='auto') # device_map='auto' for GPU if available\n\n    print(f\"Phi-4 model loaded successfully from: {phi_4_model_path}\")\n\n    # --- Basic API Test ---\n    test_prompt = \"What is 2 + 2?\"\n    input_ids = phi_4_tokenizer.encode(test_prompt, return_tensors=\"pt\").to(phi_4_model.device) # Move input to model's device\n\n    with torch.no_grad():\n        output = phi_4_model.generate(input_ids, max_length=32) # Keep max_length short for a quick test\n\n    decoded_output = phi_4_tokenizer.decode(output[0], skip_special_tokens=True)\n    print(f\"\\n--- Basic Phi-4 Model Test ---\")\n    print(f\"Prompt: {test_prompt}\")\n    print(f\"Phi-4 Output: {decoded_output}\")\n    print(\"--- Phi-4 Model Test Complete ---\")\n\n\nexcept Exception as e:\n    print(f\"Error loading Phi-4 model: {e}\")\n    phi_4_model = None  # Handle model loading failure gracefully\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nLatest version of PyDantic, Python, and my model is Phi-4-mini-instruct.\n```",
      "state": "closed",
      "author": "ithllc",
      "author_type": "User",
      "created_at": "2025-03-23T19:56:44Z",
      "updated_at": "2025-04-17T15:42:51Z",
      "closed_at": "2025-04-17T15:42:50Z",
      "labels": [
        "Stale",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1215/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1215",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1215",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:59.185895",
      "comments": [
        {
          "author": "ithllc",
          "body": "Here's the error I get when I try to add the model as the provider, it looks like it is trying to access the model through localhost, which I can confirm that I am not doing (https://localhost:8080/#) :\n\n---------------------------------------------------------------------------\nUserError           ",
          "created_at": "2025-03-24T02:45:50Z"
        },
        {
          "author": "Kludex",
          "body": "I think this is duplicated of https://github.com/pydantic/pydantic-ai/issues/1085 ? ",
          "created_at": "2025-03-24T12:46:40Z"
        },
        {
          "author": "Kludex",
          "body": "- Let's follow #1085.",
          "created_at": "2025-04-17T15:42:50Z"
        }
      ]
    },
    {
      "issue_number": 1190,
      "title": "We should use `output` not `result`",
      "body": "I made the mistake right at the beginning of using the word \"result\" where I should have used \"output\".\n\nWe should rename:\n* Agent kwargs `result_type`, `result_tool_name`, `result_tool_description` to `output_*` - we can have depecaction warnings\n* the `_result` module to `_output` - should be safe\n* other places?",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-20T16:01:26Z",
      "updated_at": "2025-04-17T15:40:55Z",
      "closed_at": "2025-04-17T15:40:54Z",
      "labels": [
        "breaking change"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1190/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1190",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1190",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:27:59.388285",
      "comments": [
        {
          "author": "dmontagu",
          "body": "If we're going to change the `result_*` kwargs to `Agent`, I'd like to at the same time rework the way we specify the `result_*` (or `output_*`) kwargs to just have a single kwarg `output: Sequence[TypeForm[OutputT] | OutputType[OutputT] | Tool[AgentDepsT, OutputT]]` or similar (noting I've added a ",
          "created_at": "2025-03-20T19:00:59Z"
        },
        {
          "author": "Kludex",
          "body": "This was done on 0.1.0.",
          "created_at": "2025-04-17T15:40:54Z"
        }
      ]
    },
    {
      "issue_number": 906,
      "title": "Tokenizer access for each model",
      "body": "Is it possible to have access to the tokenizer through pydantic-ai for each model to perform logic about what to include in the context.  E.g. Include as many documents as possible in the request context until hitting 70% of context length.",
      "state": "closed",
      "author": "Adam-D-Lewis",
      "author_type": "User",
      "created_at": "2025-02-12T19:03:39Z",
      "updated_at": "2025-04-17T15:36:36Z",
      "closed_at": "2025-04-17T15:36:35Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/906/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/906",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/906",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:04.699827",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "can you give an example of how you'd do this using the SDK directly, e.g. openai SDK or anthropic SDK?",
          "created_at": "2025-02-12T19:14:24Z"
        },
        {
          "author": "Adam-D-Lewis",
          "body": "I don't believe it's possible via the OpenAI SDKs currently.  Here's an [issue](https://github.com/openai/openai-python/issues/412) on https://github.com/openai/openai-python repo.\n\nLooks like anthropic does allow this though - https://docs.anthropic.com/en/docs/build-with-claude/token-counting\nAnd ",
          "created_at": "2025-02-12T20:36:21Z"
        },
        {
          "author": "samuelcolvin",
          "body": "thanks!",
          "created_at": "2025-02-12T21:11:33Z"
        },
        {
          "author": "Kludex",
          "body": "- Closing this in favor of https://github.com/pydantic/pydantic-ai/issues/1106",
          "created_at": "2025-04-17T15:36:35Z"
        }
      ]
    },
    {
      "issue_number": 1148,
      "title": "ReferenceChunk (returned by Mistral) not implemented",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nMistral uses a `ReferenceChunk` whenever referencing to documents ([docs](https://docs.mistral.ai/capabilities/citations/)). The Mistral model however does not handle this type of chunk. This makes Mistral unasable with PydanticAI when using it for RAG purposes.\n\nError:\n```\nAssertionError: Other data types like (Image, Reference) are not yet supported,  got <class 'mistralai.models.referencechunk.ReferenceChunk'>, reference_ids=[1, 2, 3, 4, 6, 7, 8, 9, 10] type='reference' make: *** [Makefile.local.mk:28: search] Error 1\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nmistral-large-latest\npydantic-ai 0.0.39\n```",
      "state": "open",
      "author": "tcrapts",
      "author_type": "User",
      "created_at": "2025-03-17T10:21:34Z",
      "updated_at": "2025-04-17T15:33:36Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "citations"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1148/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1148",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1148",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:04.937100",
      "comments": []
    },
    {
      "issue_number": 681,
      "title": "Streaming Nested Partial Objects - streaming only starts after the nested modle is complete (in case it's the first field)",
      "body": "Modifying the whales example at https://ai.pydantic.dev/examples/stream-whales/#running-the-example\r\n\r\n```python\r\nfrom typing import Annotated\r\n\r\nfrom pydantic import Field, ValidationError\r\nfrom typing_extensions import NotRequired, TypedDict\r\n\r\nfrom pydantic_ai import Agent\r\n\r\nclass WhaleInfo(TypedDict):\r\n    length: Annotated[\r\n        float, Field(description='Average length of an adult whale in meters.')\r\n    ]\r\n    weight: NotRequired[\r\n        Annotated[\r\n            float,\r\n            Field(description='Average weight of an adult whale in kilograms.'),\r\n        ]\r\n    ]\r\n    ocean: NotRequired[str]\r\n    description: NotRequired[Annotated[str, Field(description='Short Description')]]\r\n\r\nclass Whale(TypedDict):\r\n    info: WhaleInfo\r\n    name: str\r\n\r\n\r\nagent = Agent('openai:gpt-4', result_type=list[Whale])\r\n\r\n\r\nasync with agent.run_stream(\r\n    'Generate me details of 5 species of Whale.'\r\n) as result:\r\n    async for message, last in result.stream_structured(debounce_by=0.1):\r\n        try:\r\n            whales = await result.validate_structured_result(\r\n                message, allow_partial=not last\r\n            )\r\n        except ValidationError as exc:\r\n            if all(\r\n                e['type'] == 'missing' and e['loc'] == ('response',)\r\n                for e in exc.errors()\r\n            ):\r\n                continue\r\n            else:\r\n                raise\r\n        print(whales)\r\n```\r\n\r\nPart of output:\r\n```\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': ''}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n[{'info': {'length': 30.0, 'weight': 200.0, 'ocean': 'Atlantic', 'description': 'The Humpback Whale is one of the biggest species of baleen whales. Their distinctive body shape with long pectoral fins and a knobbly head is easily recognizable.'}, 'name': 'Humpback Whale'}]\r\n```\r\n\r\nSo the streaming only starts after the first nested model (`WhaleInfo`) is complete.\r\nThis doesn't happen if there is a first field that is not nested, in this case if `name` is the first field.\r\n\r\n@dmontagu tagging you because you are refactoring streaming in #468 ",
      "state": "closed",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-01-14T10:02:26Z",
      "updated_at": "2025-04-17T15:10:49Z",
      "closed_at": "2025-04-17T15:10:48Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/681/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/681",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/681",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:04.937123",
      "comments": [
        {
          "author": "dmontagu",
          "body": "New streaming refactors coming in #833, but I'll note that even after that refactoring there is probably not a good way to fix this easily.\n\nIn particular, if you want to be able to get instances of `Whale` before you have a name, you need to make `name` a not-required property.\n\nI think there's som",
          "created_at": "2025-02-12T23:46:34Z"
        },
        {
          "author": "Kludex",
          "body": "Now we have [`agent.iter()`](https://ai.pydantic.dev/agents/#running-agents) which should be the way to go.\n\nLet me know if that doesn't solve your issue. 🙏 ",
          "created_at": "2025-04-17T15:10:48Z"
        }
      ]
    },
    {
      "issue_number": 1426,
      "title": "How to de(serialize) BinaryContent messages with Graph persistence",
      "body": "### Question\n\nWhen using a stateful graph I can't figure out the correct way of serialising and deserialising the binary content in my messages array to a JSON file.\n\nOn the first run of the graph, I have a messages array like\n\n```\n[ModelRequest(parts=[SystemPromptPart(content='Be concise, reply with one sentence.', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content=['What does this say?', BinaryContent(data=b'%PDF-1.4\\n%\\xc3\\xa4\\xc3\\xbc\\xc3\\xb6\\xc3\\x9f\\n2 0 obj\\n<</Length 3 0 R/Filter/FlateDecode>>\\nstream\\nx\\x9c=\\x8e\\xcb\\n\\x021\\x0cE\\xf7\\xf9\\x8a\\xbbv\\x11\\x93\\xb6\\xe9\\xb40\\x08\\x0e\\xe...\n\\n<F7D77B3D22B9F92829D49FF5D78B8F28> ]\\n>>\\nstartxref\\n12787\\n%%EOF\\n', media_type='application/pdf', kind='binary')], timestamp=datetime.datetime(2025, 4, 9, 14, 59, 32, 927987, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content='The document is labeled as \"Document 1.pdf\" and contains only the text \"Dummy PDF file\".', part_kind='text')], model_name='us.anthropic.claude-3-7-sonnet-20250219-v1:0', timestamp=datetime.datetime(2025, 4, 9, 14, 59, 34, 453261, tzinfo=datetime.timezone.utc), kind='response')]\nNode: Process()\n``` \nIt's stored like this in `persistence.json` when using `ModelMessagesTypeAdapter`\n\n```\n[\n  {\n    \"state\": {\n      \"write_agent_messages\": []\n    },\n    \"node\": {\n      \"node_id\": \"Process\"\n    },\n    \"start_ts\": \"2025-04-09T14:59:32.923304Z\",\n    \"duration\": 1.52847304099123,\n    \"status\": \"success\",\n    \"kind\": \"node\",\n    \"id\": \"Process:bb3635d4f96844ef8f2d4b4d63f96bc4\"\n  },\n  {\n    \"state\": {\n      \"write_agent_messages\": [\n        91,\n        123,\n        \n        93\n      ]\n    },\n    \"node\": {},\n    \"start_ts\": null,\n    \"duration\": null,\n    \"status\": \"created\",\n    \"kind\": \"node\",\n    \"id\": \"Process:318ab43e01814683b14df69d46142876\"\n  }\n]\n```\n\nBut running a second time causes errors, I think because I'm not properly deserialising the JSON file back into messages array.\n\n```\nimport httpx\nfrom pydantic_ai import Agent, BinaryContent\n\nfrom pathlib import Path\n\nfrom pydantic_graph.persistence.file import FileStatePersistence\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\nimport asyncio\n\nfrom pydantic_ai.messages import ModelMessagesTypeAdapter\n\nfrom dataclasses import dataclass, field\n\nimport nest_asyncio\nnest_asyncio.apply()  # This allows for nested event loops in Jupyter Notebooks\n\ndummy_pdf = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\ndummy_pdf_response = httpx.get(dummy_pdf)\n\nagent = Agent(\n    model='bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n    system_prompt='Be concise, reply with one sentence.'\n)\n\n@dataclass\nclass State:\n    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n@dataclass()\nclass Process(BaseNode[State]):\n    async def run(self, ctx: GraphRunContext[State]) -> End:\n        result = await agent.run(\n            [\n                'What does this say?',\n                BinaryContent(data=dummy_pdf_response.content, media_type='application/pdf'),\n            ]\n        )\n\n        print(result.all_messages())\n        ctx.state.write_agent_messages += ModelMessagesTypeAdapter.dump_json(\n            result.all_messages()\n        )\n        # print(ctx.state.write_agent_messages)\n        return Process()\n\n\nasync def main():\n    my_graph = Graph(nodes=(Process,))\n    persistence = FileStatePersistence(Path(f'persistence.json'))\n    # How do I de-serialise the b64 message history back into my messages array?\n    state = State(\n        write_agent_messages=[]\n    )\n    await my_graph.initialize(\n        Process(),\n        state=state,\n        persistence=persistence\n    )\n    async with my_graph.iter_from_persistence(persistence) as run:\n        node = await run.next()\n    print('Node:', node)\n\nasyncio.run(main())\n```\n\nTo summarise, I want to be able to use graph persistence to save and reload my messages array when it contains BinaryContent (like PDF and images). Any pointers appreciated!\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "stephenhibbert",
      "author_type": "User",
      "created_at": "2025-04-09T15:07:58Z",
      "updated_at": "2025-04-17T14:08:14Z",
      "closed_at": "2025-04-17T14:08:14Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1426/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1426",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1426",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:05.171124",
      "comments": [
        {
          "author": "yingying-chen-cko",
          "body": "I've been facing the same issue trying to store the `BinaryContent` pdf in state. \n\nThe LLM model I am using is `bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0`.\n\nThis is the error I am seeing\n```\npydantic_core._pydantic_core.PydanticSerializationError: Error serializing to JSON: invalid utf-8",
          "created_at": "2025-04-09T21:01:30Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-17T14:00:45Z"
        },
        {
          "author": "DouweM",
          "body": "This has been fixed in https://github.com/pydantic/pydantic-ai/pull/1513, released in v0.1.2. Please update your PydanticAI and let us know if you're still hitting issues!",
          "created_at": "2025-04-17T14:07:32Z"
        }
      ]
    },
    {
      "issue_number": 571,
      "title": "Enhance intro page example",
      "body": "* Break the more involved example up into a step by step guide that demonstrates one new feature / pattern at a time\r\n* Give a bit more context before each snippet (perhaps pulling some info from admonitions)\r\n* Consider changing the example content up a bit (maybe not something as high stakes as a bank manager), but still needs to be compelling",
      "state": "closed",
      "author": "sydney-runkle",
      "author_type": "User",
      "created_at": "2024-12-30T15:55:29Z",
      "updated_at": "2025-04-17T14:03:46Z",
      "closed_at": "2025-04-17T14:03:46Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/571/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sydney-runkle"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/571",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/571",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:05.436036",
      "comments": [
        {
          "author": "johnkabler",
          "body": "Looking forward to this Syd.  I tested this thing out today, and even set up Volker's example with Airflow and had great results with OpenAI 4o despite Volker recommending Gemini.\r\n\r\nRegarding the actual issue (more approachable than bank manager), I think you can get some inspiration from the Airfl",
          "created_at": "2024-12-30T21:54:37Z"
        }
      ]
    },
    {
      "issue_number": 1428,
      "title": "Graph Edge Mapping / Looping?",
      "body": "### Question\n\nHi all -- I've been trying to answer this question, but haven't been able to find anything in the docs or issues.\n\nDoes Pydantic Graph support Mapped Edges?  This is one of the main features of LangGraph that has made it impossible for us to switch to anything else.  To illustrate further, here's an example:\n\nSay my graph state looks like:\n\n```python\nclass GraphState(TypedDict):\n    listA: list[str]\n    listB: list[str]\n```\n\nI need to construct a graph that maps each item of `listA` into an agent, who then maps each item of listB to an agent. \n\n![Image](https://github.com/user-attachments/assets/d0472edb-ec6e-41b5-bd0b-b07f3b01f827)\n\nIs this currently possible with Pydantic Graph? \n\nThanks!!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Ox54",
      "author_type": "User",
      "created_at": "2025-04-09T17:10:54Z",
      "updated_at": "2025-04-17T14:00:44Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale",
        "graph"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1428/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1428",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1428",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:05.690683",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-17T14:00:43Z"
        }
      ]
    },
    {
      "issue_number": 1516,
      "title": "run_stream error when break early",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n### Example Code\n\nMinimal reproduce code:\n\n```python\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.result import StreamedRunResult\n\n\nasync def main(model: str, prompt: str):\n    agent = Agent(model=model)\n    async with agent.run_stream(prompt) as result:\n        async for content in result.stream_text(delta=True):\n            print(content)\n\n            # break early will trigger error\n            break\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main(\"deepseek:deepseek-chat\", \"Hello, how are you?\"))\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.8\n\n>>> pydantic_ai.__version__\n'0.1.0'\n\n>>> openai.__version__\n'1.74.0'\n```",
      "state": "open",
      "author": "yihuang",
      "author_type": "User",
      "created_at": "2025-04-16T23:47:45Z",
      "updated_at": "2025-04-17T09:11:10Z",
      "closed_at": null,
      "labels": [
        "run_stream"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1516/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1516",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1516",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:05.941225",
      "comments": [
        {
          "author": "yihuang",
          "body": "investigation shows the issue is `group_by_temporal`, I've tried to replace it with normal async iteration, the issue disappears.\n\nActually, just pass `debounce_by=None` will success.",
          "created_at": "2025-04-17T09:03:26Z"
        },
        {
          "author": "yihuang",
          "body": "minimal reproduction code:\n\n```python\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_ai._utils import group_by_temporal\n\n\nasync def stream():\n    for i in range(10):\n        print(\"streaming\", i)\n        yield i\n     ",
          "created_at": "2025-04-17T09:06:15Z"
        }
      ]
    },
    {
      "issue_number": 1493,
      "title": "AWS Bedrock Inference Profile Support",
      "body": "### Question\n\nFrom the docs I can't tell if Bedrock inference profiles are supported?\nhttps://ai.pydantic.dev/api/models/bedrock/#pydantic_ai.models.bedrock.BedrockModelName\n\nTypically inference profile ID or ARN is used in place of model ID. Is model name here synonymous to model ID from AWS's docs?\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/invoke_model.html\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "shivpatel",
      "author_type": "User",
      "created_at": "2025-04-16T03:00:48Z",
      "updated_at": "2025-04-17T07:48:56Z",
      "closed_at": null,
      "labels": [
        "documentation",
        "question",
        "Stale",
        "bedrock"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1493/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1493",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1493",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:06.156292",
      "comments": [
        {
          "author": "tekumara",
          "body": "The bedrock model names prefixed with `us.` are inference profiles. They have worked for me.",
          "created_at": "2025-04-17T07:38:09Z"
        },
        {
          "author": "Kludex",
          "body": "Yeah, they work. Wanna contribute to the documentation?",
          "created_at": "2025-04-17T07:48:50Z"
        }
      ]
    },
    {
      "issue_number": 1417,
      "title": "A tool cannot return BaseModel object?  but it can return @dataclass object",
      "body": "### Question\n\nHi, \n\nI noticed that a tool call used by a gent cannot return a `BaseModel` object. It gave me this kind of error:\n\n```\n  Input should be a valid string [type=string_type, input_value=ImageDescriptionData(essa...n 1989.', image_path=''), input_type=ImageDescriptionData]\n```\n\nBut when I change the object to `@dataclass` I works fine \n\nWhy is that? \n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "dinhngoc267",
      "author_type": "User",
      "created_at": "2025-04-09T04:28:39Z",
      "updated_at": "2025-04-17T07:39:33Z",
      "closed_at": "2025-04-17T07:39:31Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1417/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1417",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1417",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:06.383806",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-16T14:00:47Z"
        },
        {
          "author": "Kludex",
          "body": "It should be fixed in the latest release. Please try, if not solved, please share a snippet I can reproduce.\n\nI'm happy to reopen this issue with the provided snippet. 🙏 ",
          "created_at": "2025-04-17T07:39:31Z"
        }
      ]
    },
    {
      "issue_number": 1469,
      "title": "pydantic-ai Gemini Schema Generation Fails for Dynamically Created Models with List[KeyValuePair] Replacement for Dict",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n\nOkay, here is a summary of the issue encountered with `pydantic-ai` when trying to adapt Pydantic models for Gemini function calling, suitable for a GitHub issue.\n\n---\n\nWhen using `pydantic-ai` with Gemini function calling, there's an issue generating a compatible schema for Pydantic models that have been dynamically adapted to replace `Dict[K, V]` fields with `List[KeyValuePair]` structures. Even when the dynamically generated Pydantic model produces a valid JSON schema (verified via `Model.model_json_schema()`) that correctly represents the field as an array and has `additionalProperties: False`, `pydantic-ai` seems to internally re-process this schema in a way that re-introduces `additionalProperties`, leading to Gemini rejecting the schema.\n\n**Context:**\n\nGemini's function calling requires strict JSON schemas and explicitly forbids the `additionalProperties` keyword. Standard Pydantic models using `Dict[K, V]` fields naturally generate schemas with `additionalProperties`. The standard workaround is to replace `Dict[K, V]` fields with `List[KeyValuePair]`, where `KeyValuePair` is a simple `BaseModel`.\n\n**Problem Encountered:**\n\nWe attempted to create a generic adapter (`GeminiModelAdapter`) that uses `pydantic.create_model` to dynamically generate a Gemini-compatible version of *any* input Pydantic model. The adapter specifically does the following:\n1.  Identifies fields annotated as `Dict[K, V]`.\n2.  In the dynamically created model, replaces the type of these fields with `List[KeyValuePair]`.\n3.  Uses `pydantic.Field(..., json_schema_extra=lambda schema: schema.update({\"type\": \"array\", \"items\": kv_schema}))` to try and force the correct array schema for these fields during Pydantic's schema generation.\n4.  Creates the dynamic model using `create_model(..., __config__=ConfigDict(extra=\"forbid\"))`.\n\n**Why This *Should* Work:**\n\nDebugging showed that calling `.model_json_schema()` on the dynamically created class *does* produce the expected, seemingly correct JSON schema:\n*   The field derived from the original `Dict` is correctly shown with `\"type\": \"array\"` and `\"items\": { ... KeyValuePair schema ... }`.\n*   There is *no* `\"additionalProperties\"` key within this field's schema definition.\n*   The top-level schema correctly has `\"additionalProperties\": False`.\n\n**Where `pydantic-ai` Seems to Fail:**\n\nDespite the Pydantic-generated schema appearing correct, the error `Unexpected error running agent: Additional properties in JSON Schema are not supported by Gemini` occurs *within `pydantic-ai`'s* processing pipeline, *after* `.model_json_schema()` is called but *before* the schema is sent to Gemini.\n\nThis strongly suggests that `pydantic-ai` performs an internal schema simplification or transformation step that incorrectly re-introduces or infers `additionalProperties` for fields that were *originally* dictionaries in the source model, even if they are correctly represented as arrays in the Pydantic schema provided to `pydantic-ai`.\n\nNotably, a *statically* defined Pydantic model with the exact same `List[KeyValuePair]` structure works perfectly fine with `pydantic-ai` and Gemini. The issue seems specific to how `pydantic-ai` handles schemas generated from models created dynamically via `pydantic.create_model`.\n\n**Suggested Fix:**\n\nIt would be ideal if `pydantic-ai`'s internal schema processing could more reliably trust the schema generated by Pydantic's `.model_json_schema()`, especially when `ConfigDict(extra=\"forbid\")` is used and fields involving `List[BaseModel]` are present. Perhaps the simplification step needs adjustment to avoid re-introducing `additionalProperties` when processing schemas derived from dynamically created models where dictionary fields have been explicitly replaced by array structures. Alternatively, providing a mechanism to bypass this specific simplification could also resolve the issue.\n\n---\n\n\n### Example Code\n\n```Python\n**Minimal Example:**\n\n\nfrom pydantic import BaseModel, Field, create_model, ConfigDict\nfrom pydantic_ai import Agent # Assuming Agent usage\nfrom typing import Dict, List, Any, Type\n\n# 1. Define KeyValuePair\nclass KeyValuePair(BaseModel):\n    key: str\n    value: str\n    model_config = ConfigDict(extra=\"forbid\")\n\n# 2. Define Original Model with a Dict\nclass OriginalModel(BaseModel):\n    name: str\n    metadata: Dict[str, str] | None = None\n\n# 3. Adapter Logic (Simplified Core)\ndef adapt_model(original_cls: Type[BaseModel]) -> Type[BaseModel]:\n    fields = {}\n    kv_schema = KeyValuePair.model_json_schema()\n    \n    for i, (fname, field_info) in enumerate(original_cls.model_fields.items(), 1):\n        compact_name = f\"f{i}\"\n        \n        if get_origin(field_info.annotation) is dict:\n            # Replace Dict with List[KeyValuePair] and force schema\n            field_schema = {\"type\": \"array\", \"items\": kv_schema}\n            processed_field = Field(\n                description=f\"{fname}: {field_info.description or ''}\",\n                json_schema_extra=lambda schema: schema.update(field_schema)\n            )\n            fields[compact_name] = (List[KeyValuePair], processed_field)\n        else:\n            # Keep other fields as is\n            processed_field = Field(\n                default=field_info.default,\n                description=f\"{fname}: {field_info.description or ''}\"\n            )\n            fields[compact_name] = (field_info.annotation, processed_field)\n            \n    AdaptedModel = create_model(\n        f\"Adapted{original_cls.__name__}\",\n        __config__=ConfigDict(extra=\"forbid\"),\n        **fields\n    )\n    return AdaptedModel\n\n# 4. Usage with pydantic-ai (Conceptual)\nAdaptedClass = adapt_model(OriginalModel)\n\n# Schema check (debugging shows this looks correct)\nprint(AdaptedClass.model_json_schema()) \n\n# agent = Agent(..., result_type=AdaptedClass) \n# result = agent.run(...) # This fails with Gemini schema error from pydantic-ai\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nname = \"pydantic-ai\"\nversion = \"0.0.50\"\n\nPython 3.12\n\n\"google-gla:gemini-2.0-flash\"\n```",
      "state": "closed",
      "author": "talkingtoaj",
      "author_type": "User",
      "created_at": "2025-04-13T20:04:48Z",
      "updated_at": "2025-04-17T06:59:44Z",
      "closed_at": "2025-04-17T06:59:42Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1469/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1469",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1469",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:06.618103",
      "comments": [
        {
          "author": "DouweM",
          "body": "@talkingtoaj Pydantic AI now automatically transforms the JSON schema to a format that Gemini likes: https://github.com/pydantic/pydantic-ai/pull/1506. Can you please try again on the latest version, without the custom `adapt_model` logic?",
          "created_at": "2025-04-17T00:37:33Z"
        },
        {
          "author": "Kludex",
          "body": "- This was fixed on https://github.com/pydantic/pydantic-ai/pull/1506 🙏 ",
          "created_at": "2025-04-17T06:59:43Z"
        }
      ]
    },
    {
      "issue_number": 1467,
      "title": "Building graph out of agent iter",
      "body": "### Question\n\nHey,\n\nI'm trying to build a graph out of an agent run.\n\n``` python\nasync def main(request: SequenceRequest) -> SequenceResponse:\n    deps = SequenceDependencies(scene_id=2, db=SequenceDatabase(), data=request)\n    nodes = []\n\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with sequence_agent.iter(request.prompt, deps=deps) as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n    result = agent_run.result\n    history = result.all_messages()\n\n    print(\"\\nNodes\\n\")\n    for node in nodes:\n        print(\"\\n\", type(node))\n        print(node)\n\n    result_graph = Graph(nodes=nodes)\n    result_graph.mermaid_save(\"../tmp/output/graph.png\")\n    ...\n```\n\nNodes in list\n-  <class 'pydantic_ai._agent_graph.UserPromptNode'> UserPromptNode\n-  <class 'pydantic_ai._agent_graph.ModelRequestNode'> ModelRequestNode\n-  <class 'pydantic_ai._agent_graph.CallToolsNode'> CallToolsNode\n-  <class 'pydantic_ai._agent_graph.ModelRequestNode'> ModelRequestNode\n-  <class 'pydantic_ai._agent_graph.CallToolsNode'> CallToolsNode\n- <class 'pydantic_graph.nodes.End'> End\n\nBut get \n\n```\n  File \"C:\\development\\deferred-diffusion\\venv\\Lib\\site-packages\\pydantic_graph\\graph.py\", line 116, in __init__\n    self._register_node(node, parent_namespace)\n  File \"C:\\development\\deferred-diffusion\\venv\\Lib\\site-packages\\pydantic_graph\\graph.py\", line 547, in _register_node\n    self.node_defs[node_id] = node.get_node_def(parent_namespace)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\development\\deferred-diffusion\\venv\\Lib\\site-packages\\pydantic_graph\\nodes.py\", line 108, in get_node_def\n    type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\typing.py\", line 2310, in get_type_hints\n    hints[name] = _eval_type(value, globalns, localns, type_params)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\typing.py\", line 415, in _eval_type\n    return t._evaluate(globalns, localns, type_params, recursive_guard=recursive_guard)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\typing.py\", line 947, in _evaluate\n    eval(self.__forward_code__, globalns, localns),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 1, in <module>\nAttributeError: 'AgentRunResult' object has no attribute 'FinalResult'\n```\n\nMaybe I'm doing something wrong here or misunderstanding, but thought agents nodes was based upon the graph nodes. Or there is some better way to build out diagrams from agent runs. \n\nCheers,\nJoe\n\n### Additional Context\n\npydantic-ai==0.0.55\nPython 3.12.7\nWindows 10 pro",
      "state": "open",
      "author": "JoeGaffney",
      "author_type": "User",
      "created_at": "2025-04-13T17:19:40Z",
      "updated_at": "2025-04-17T00:40:34Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1467/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1467",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1467",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:06.817291",
      "comments": [
        {
          "author": "DouweM",
          "body": "@JoeGaffney To help us debug this, can you please update to the latest version of Pydantic AI and provide a minimal example that triggers the error?",
          "created_at": "2025-04-17T00:40:33Z"
        }
      ]
    },
    {
      "issue_number": 1488,
      "title": "OpenAI strict mode inferred incorrectly",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nUpgrading from 0.0.43 to 0.0.55 breaks my code as OpenAI returns an error regarding the tool schemas.\nThe problem seems to be related to descriptions from the docstring and model references (Enum in this case).\n\nThe following tool results in an error now in version 0.0.55 but not in 0.0.43.\n```\nasync def bad_function(role: UserRole) -> None:\n    \"\"\"\n    Set the role of a user.\n    \n    Args:\n        user_id (int): The ID of the user to set the role for.\n        role (UserRole): The role to set for the user.\n    \"\"\"\n    await asyncio.sleep(1)\n```\n\nError:\n\n> pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gpt-4o, body: {'message': \"Invalid schema for function 'bad_function': context=('properties', 'role'), $ref cannot have keywords {'description'}.\", 'type': 'invalid_request_error', 'param': 'tools[0].function.parameters', 'code': 'invalid_function_parameters'}\n\n\nAfter a bit of digging it looks like removing the line for the `role` argument in the docstring solves the issue.\nAlso, it looks like in version 0.0.43 the OpenAI model was called with `strict=False` whereas now it is called with `strict=True`. Is the `strict` parameter maybe inferred incorrectly from the docstring?\n\n### Example Code\n\n```Python\nimport asyncio\nfrom enum import Enum\nfrom pydantic_ai import Agent\n\n\nclass UserRole(Enum):\n    ADMIN = \"admin\"\n    USER = \"user\"\n\n# passing this function as a tool results in an error from OpenAI because strict mode is True\nasync def bad_function(role: UserRole) -> None:\n    \"\"\"\n    Set the role of a user.\n    \n    Args:\n        user_id (int): The ID of the user to set the role for.\n        role (UserRole): The role to set for the user.\n    \"\"\"\n    await asyncio.sleep(1)\n\n# this function works fine as a tool\nasync def good_function(role: UserRole) -> None:\n    \"\"\"\n    Set the role of a user.\n\n    Args:\n        user_id (int): The ID of the user to set the role for.\n    \"\"\"\n    await asyncio.sleep(1)\n\n\ntest_agent = Agent(\n    'openai:gpt-4o',\n    system_prompt=(\"Prompt\"),\n    tools=[bad_function, good_function]\n)\n\n\nasync def main():\n    result = await test_agent.run('Prompt')\n    print(result.data)\n\n\nasyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPydantic AI 0.0.55\nPython 3.11.7\nOpenAI 1.72.0\n```",
      "state": "closed",
      "author": "rogerwelo",
      "author_type": "User",
      "created_at": "2025-04-15T15:14:24Z",
      "updated_at": "2025-04-16T23:36:25Z",
      "closed_at": "2025-04-16T23:36:25Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1488/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1488",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1488",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:07.056303",
      "comments": [
        {
          "author": "vikigenius",
          "body": "Yep I also have this issue. It seems like some argument names are not allowed in strict mode. In my case it was `country` that was causing issues.\n\nThis seems too restrictive and it might be better to just not do strict mode",
          "created_at": "2025-04-16T14:45:27Z"
        },
        {
          "author": "vikigenius",
          "body": "I was incorrect, it's less about argument names and more about types. It seems like Enums are not being handled now",
          "created_at": "2025-04-16T18:18:37Z"
        },
        {
          "author": "DouweM",
          "body": "Thanks for the report! Looks like this was introduced by https://github.com/pydantic/pydantic-ai/pull/1304, which automatically passes `strict=True` if the schema is determined to be strict-compatible. \n\nThe logic was moved in https://github.com/pydantic/pydantic-ai/pull/1481, but it seems like stri",
          "created_at": "2025-04-16T19:11:15Z"
        }
      ]
    },
    {
      "issue_number": 1439,
      "title": "Agent fails with http 400 in streaming mode, works fine in non-streaming mode for Gemini models",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm having issues with streaming responses for a research agent using latest Gemini models. This works fine:\n`result = await sdk.research_agent.run(question, deps=context)\nprint(result.data)`\n\nbut this fails:\n`async with sdk.research_agent.run_stream(question, deps=context) as stream:\n    async for chunk in stream.stream():\n        print(chunk)`\n\nThis results in:\n`---------------------------------------------------------------------------\nModelHTTPError                            Traceback (most recent call last)\nCell In[7], line 1\n----> 1 async with sdk.research_agent.run_stream(question, deps=context) as stream:\n      2     async for chunk in stream.stream():\n      3         print(chunk)\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:210, in _AsyncGeneratorContextManager.__aenter__(self)\n    208 del self.args, self.kwds, self.func\n    209 try:\n--> 210     return await anext(self.gen)\n    211 except StopAsyncIteration:\n    212     raise RuntimeError(\"generator didn't yield\") from None\n\nFile ~/venvs/nxd/lib/python3.12/site-packages/pydantic_ai/agent.py:690, in Agent.run_stream(self, user_prompt, result_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name)\n    688 if self.is_model_request_node(node):\n    689     graph_ctx = agent_run.ctx\n--> 690     async with node._stream(graph_ctx) as streamed_response:  # pyright: ignore[reportPrivateUsage]\n    692         async def stream_to_final(\n    693             s: models.StreamedResponse,\n    694         ) -> FinalResult[models.StreamedResponse] | None:\n    695             result_schema = graph_ctx.deps.result_schema\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:210, in _AsyncGeneratorContextManager.__aenter__(self)\n    208 del self.args, self.kwds, self.func\n    209 try:\n--> 210     return await anext(self.gen)\n    211 except StopAsyncIteration:\n    212     raise RuntimeError(\"generator didn't yield\") from None\n\nFile ~/venvs/nxd/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:293, in ModelRequestNode._stream(self, ctx)\n    290 assert not self._did_stream, 'stream() should only be called once per node'\n    292 model_settings, model_request_parameters = await self._prepare_request(ctx)\n--> 293 async with ctx.deps.model.request_stream(\n    294     ctx.state.message_history, model_settings, model_request_parameters\n    295 ) as streamed_response:\n    296     self._did_stream = True\n    297     ctx.state.usage.incr(_usage.Usage(), requests=1)\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:210, in _AsyncGeneratorContextManager.__aenter__(self)\n    208 del self.args, self.kwds, self.func\n    209 try:\n--> 210     return await anext(self.gen)\n    211 except StopAsyncIteration:\n    212     raise RuntimeError(\"generator didn't yield\") from None\n\nFile ~/venvs/nxd/lib/python3.12/site-packages/pydantic_ai/models/gemini.py:150, in GeminiModel.request_stream(self, messages, model_settings, model_request_parameters)\n    142 @asynccontextmanager\n    143 async def request_stream(\n    144     self,\n   (...)    147     model_request_parameters: ModelRequestParameters,\n    148 ) -> AsyncIterator[StreamedResponse]:\n    149     check_allow_model_requests()\n--> 150     async with self._make_request(\n    151         messages, True, cast(GeminiModelSettings, model_settings or {}), model_request_parameters\n    152     ) as http_response:\n    153         yield await self._process_streamed_response(http_response)\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:210, in _AsyncGeneratorContextManager.__aenter__(self)\n    208 del self.args, self.kwds, self.func\n    209 try:\n--> 210     return await anext(self.gen)\n    211 except StopAsyncIteration:\n    212     raise RuntimeError(\"generator didn't yield\") from None\n\nFile ~/venvs/nxd/lib/python3.12/site-packages/pydantic_ai/models/gemini.py:242, in GeminiModel._make_request(self, messages, streamed, model_settings, model_request_parameters)\n    240     await r.aread()\n    241     if status_code >= 400:\n--> 242         raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=r.text)\n    243     raise UnexpectedModelBehavior(f'Unexpected response from gemini {status_code}', r.text)\n    244 yield r\n\nModelHTTPError: status_code: 400, model_name: gemini-1.5-pro, body: [{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[0].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[1].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[3].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"additionalProperties\\\" at 'tools.function_declarations[1].parameters': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools.function_declarations[0].parameters.properties[0].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[0].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[0].parameters.properties[1].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[1].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[0].parameters.properties[2].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[2].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[0].parameters.properties[3].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools.function_declarations[0].parameters.properties[3].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[1].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"additionalProperties\\\" at 'tools.function_declarations[1].parameters': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n]`\n\nThe error persists even if I use the Agent iter for streaming by executing each node. Surprisingly this works fine with Open AI models.  It appears as if the tool function definitions are not getting passed properly. \n\nFor reference, I'm passing the tools to the agent like this:\n`Agent(\n        model_obj=get_model('HP_LC'),\n        result_type=str,\n        deps_type=ChatContext,\n        tools=[Tool(research_tool, takes_ctx=True), Tool(references_tool, takes_ctx=True)]\n    )  `\n\nFor now I have to stick to non-streaming mode of operation, but that results in a poor user experience. \n\n### Example Code\nI just changes one of the samples in the documentation by adding a function parameter in the tool call. It immediately fails.  Without any function parameters in the tool, it works fine. \n\n```Python\nimport random\n\nfrom pydantic_ai import Agent, RunContext, Tool\n\n\n\ndef roll_die(how_many_times: int = 1) -> str:\n    \"\"\"\n    Roll a six-sided die how_many_times and return the average result.\n    \"\"\"\n    return str(random.randint(1, 6))\n\n\ndef get_player_name(ctx: RunContext[str]) -> str:\n    \"\"\"\n    Get the player's name.\n    \"\"\"\n    return ctx.deps\n\nagent = Agent(\n    'google-gla:gemini-1.5-pro',  \n    deps_type=str,  \n    system_prompt=(\n        \"You're a dice game, you should roll the die and see if the number \"\n        \"you get back matches the user's guess. If so, tell them they're a winner. \"\n        \"Use the player's name in the response.\"\n    ),\n    tools=[Tool(roll_die, takes_ctx=False), Tool(get_player_name, takes_ctx=True)]\n)\n\n\n\nasync with agent.run_stream('My guess is 4', deps='Anne') as stream:\n    async for chunk in stream.stream():\n        print(chunk)\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.6\npydantic-ai-slim==0.0.55\ngoogle-genai==1.10.0\n```\n\n",
      "state": "open",
      "author": "amiyapatanaik",
      "author_type": "User",
      "created_at": "2025-04-10T16:41:55Z",
      "updated_at": "2025-04-16T20:50:52Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1439/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1439",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1439",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:07.305828",
      "comments": [
        {
          "author": "akram-suplari",
          "body": "I can confirm I am having a similar issue. Using tools and `\"stream_mode\":\"custom\"`\n\nUsing several tools so I assume that's why so many `additionalProperties` errors. \n\nUsing \n- Pydantic version `pydantic-ai[logfire]>=0.0.55` \n- Python 3.12.4\n\n```\nFile \"/.../.venv/lib/python3.12/site-packages/pydant",
          "created_at": "2025-04-10T21:07:01Z"
        },
        {
          "author": "amiyapatanaik",
          "body": "#1398 I saw this issue and tried to run with v.0.0.52, while it ran, the tool call actually did not give any result. So even v0.0.52 has issues, but it does not directly cause an error.  ",
          "created_at": "2025-04-12T12:45:18Z"
        },
        {
          "author": "haljac",
          "body": "I can confirm that this is an issue. In even the most minimalistic reproductions, tools usage with `agent.run` works while `agent.run_stream` does not when using the Gemini models.",
          "created_at": "2025-04-13T18:20:31Z"
        },
        {
          "author": "akram-suplari",
          "body": "Can confirm that my issue has been solved in `v0.1.1` \n\nThanks for fixing this super quickly!",
          "created_at": "2025-04-16T20:50:51Z"
        }
      ]
    },
    {
      "issue_number": 1423,
      "title": "Gemini (Vertex AI) - Having issues with tool calls",
      "body": "### Question\n\nI am trying to switch between multiple providers for tool calling agent workflow. The open AI gpt-40 and claude sonnet 3.7 handle the tools well and get the output I expect to see.\n\nBut when I switch to vertex ai (Gemini) and use any models (current set to pro-2.5), I see the tool to be executed being printed in the output instead of the execution result.\n\nFor example, instead of seeing the output from this tool, this is what I see in the logs:\n`{\\\"tool_code\\\": \\\"default_api.list_group_members(group_id='00gndyustNsB5d7')\\\"}  `\n\n**Question:** : Is this because Gemini models don't follow the same format for tool execution? Or is there something specific I need to put in my code for Gemini models?\n\nMy code:\n\n```\n   if provider == AIProvider.VERTEX_AI:\n        service_account = os.getenv('GOOGLE_APPLICATION_CREDENTIALS') or os.getenv('VERTEX_AI_SERVICE_ACCOUNT_FILE')\n        project_id = os.getenv('VERTEX_AI_PROJECT')\n        region = os.getenv('VERTEX_AI_LOCATION', 'us-central1')\n        model_name = os.getenv('VERTEX_AI_REASONING_MODEL', 'gemini-1.5-pro')\n        \n        vertex_provider = GoogleVertexProvider(\n            service_account_file=service_account,\n            project_id=project_id,\n            region=region\n        )\n        \n        return GeminiModel(model_name, provider=vertex_provider)\n```\n\n  \n\n\n\n### Additional Context\n\nPydantic Ai version : 0.52 - 0.54\nPython version: 3.12.9",
      "state": "closed",
      "author": "fctr-id",
      "author_type": "User",
      "created_at": "2025-04-09T13:47:35Z",
      "updated_at": "2025-04-16T20:47:53Z",
      "closed_at": "2025-04-14T22:15:09Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1423/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1423",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1423",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:07.516515",
      "comments": [
        {
          "author": "fctr-id",
          "body": "Anyone seeing this issue?",
          "created_at": "2025-04-11T19:05:36Z"
        },
        {
          "author": "brunorpinho",
          "body": "> Anyone seeing this issue?\n\nI see this issue as @fctr-id .\n\nPydantic AI: 0.55\nPython: 3.13.2",
          "created_at": "2025-04-14T17:19:31Z"
        },
        {
          "author": "fctr-id",
          "body": "I have been googling a bit and looks like it's the issue with gemini models not able to handle the tools calls properly.",
          "created_at": "2025-04-14T18:27:13Z"
        },
        {
          "author": "fctr-id",
          "body": "try gemini-1.5-pro. It's better and works as expected for these tool calls.  ",
          "created_at": "2025-04-14T18:35:25Z"
        },
        {
          "author": "brunorpinho",
          "body": "@fctr-id I suggest we keep this issue open as gemini-2.5-pro raises errors when used with function call. There will be more users with this same issue.",
          "created_at": "2025-04-14T22:37:52Z"
        }
      ]
    },
    {
      "issue_number": 1501,
      "title": "OpenAI instructions not set correctly",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHello team,\n\nSo I've played around with the new instructions feature and from my tests and from reading the code, it seems to me that the instruction is only applied to the first LLM call. Even within a single `.run`, if there is a tool call, upon returning back from the tool call, the instruction will no longer be applied. Is this intented behavior? I think this is because instructions are extracted by looping the messages in reverse, and since there is a new request made by the tool, it no longer finds the instruction.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12\nPydantic 0.1\nOpenAI completions API\n```",
      "state": "closed",
      "author": "ag14774",
      "author_type": "User",
      "created_at": "2025-04-16T11:34:01Z",
      "updated_at": "2025-04-16T20:37:53Z",
      "closed_at": "2025-04-16T20:37:53Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1501/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1501",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1501",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:07.730980",
      "comments": [
        {
          "author": "DouweM",
          "body": "@Kludex Since you built the instructions feature (https://github.com/pydantic/pydantic-ai/pull/1360), could you have a look at this question please? ",
          "created_at": "2025-04-16T20:19:11Z"
        },
        {
          "author": "Kludex",
          "body": "We solved this a couple of hours ago. It's fixed in 0.1.1.",
          "created_at": "2025-04-16T20:37:48Z"
        }
      ]
    },
    {
      "issue_number": 1500,
      "title": "[graph] Decorate the BaseNode.run coroutine",
      "body": "### Question\n\npy 3.11.9\n\nI want to decorate the run coroutine of the node subclass so I can log the start, end and error points of a node execution without having to write it across all of my nodes.\n\nSince pydantic graph depends heavily on TypeHints, this is becoming a struggle;\n\n```python\n\nP = ParamSpec('P')\nR = TypeVar('R')\n\n...\n#decorator\ndef log_node_run(func: Callable[P, R]):\n\n    async def _log_node_run(*args: P.args, **kwargs: P.kwargs) -> R:\n        args[0].logger.debug('starting node run')\n        result: R = await func(*args, **kwargs)\n        args[0].logger.debug('finished node run')\n        return result\n    return _log_node_run\n\n...\n\n# just to check the static type hint on hover in VSCode\nrun = log_node_run(RouterNode().run)\n# (variable) def run(ctx: ResearchContextType) -> CoroutineType[Any, Any, CoroutineType[Any, Any, CoroutineType[Any, Any, PlannerNode | ChatBotNode | End[str]]]]\n\n```\n\ntraceback\n```txt\nTraceback (most recent call last):\n  File \"C:\\Users\\sertruno\\Desktop\\repos\\technology-intelligence\\pm_planners_ai_agent_workflow\\refactor\\app.py\", line 32, in <module>\n    graph = Graph(nodes=[\n            ^^^^^^^^^^^^^\n  File \"C:\\Users\\sertruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic_graph\\graph.py\", line 124, in __init__\n    self._register_node(node, parent_namespace)\n  File \"C:\\Users\\sertruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic_graph\\graph.py\", line 526, in _register_node\n    self.node_defs[node_id] = node.get_node_def(parent_namespace)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sertruno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic_graph\\nodes.py\", line 116, in get_node_def\n    elif issubclass(return_type_origin, BaseNode):\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen abc>\", line 123, in __subclasscheck__\nTypeError: issubclass() arg 1 must be a class\n```\nHow can I properly decorate/annotate the types of a decorated async run method?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "strunov",
      "author_type": "User",
      "created_at": "2025-04-16T10:43:09Z",
      "updated_at": "2025-04-16T20:34:45Z",
      "closed_at": "2025-04-16T20:34:45Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1500/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1500",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1500",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:08.069475",
      "comments": [
        {
          "author": "strunov",
          "body": ":/ ended up decorating a `_run` method and wrapping that inside of the expected `run() -> MyNode:`\nbut wish I didn't have to do that ",
          "created_at": "2025-04-16T20:26:52Z"
        },
        {
          "author": "DouweM",
          "body": "@strunov This is where [`functools.wraps`](https://docs.python.org/3/library/functools.html#functools.wraps) comes in! It makes sure all the annotations and other attributes of the wrapped function are copied to the new function:\n\nWith this, everything should work as expected:\n\n```py\nimport functool",
          "created_at": "2025-04-16T20:32:10Z"
        }
      ]
    },
    {
      "issue_number": 1509,
      "title": "0.0.53 OpenAI strictness regression on `datetime` args",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nThe tool below runs like this on `0.0.52`\n\n```python\n@agent.tool\ndef get_all_infos(ctx: RunContext, today: datetime) -> str:\n    out = f\"TODAY: {today}\"\n    print(out)\n    return out\n```\n\n```\n$ uv run --python 3.12 --with pydantic-ai==0.0.52 strict.regression.py\nTODAY: 2023-10-16 00:00:00\nToday's date is October 16, 2023. If you have specific information you're seeking, please let me know!\n```\n\n\nbut on 0.1.1 (and since 0.0.53):\n\n```\n$ uv run --python 3.12 --with pydantic-ai==0.1.1 strict.regression.py \n...\nline 289, in _completions_create\n    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gpt-4o, body: \n{'message': \"Invalid schema for function 'get_all_infos': In context=('properties', 'today'), 'format' is not permitted.\",\n 'type': 'invalid_request_error',\n 'param': 'tools[0].function.parameters',\n 'code': 'invalid_function_parameters'}\n```\n\nThe problem goes away passing `strict=False` to `@tool` or something other than `datetime`. Should pydantic AI take care of `datetime` serialization and deserialization? \n\n### Example Code\n\n```Python\n#!/usr/bin/env -S uv run\n# /// script\n# dependencies = [\n#     \"pydantic-ai==0.0.53\",\n# ]\n# ///\n\nimport asyncio\nfrom datetime import datetime\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    name=\"my_agent\",\n    result_type=str,\n    model=\"gpt-4o\",\n)\n\n@agent.tool\ndef get_all_infos(ctx: RunContext, today: datetime) -> str:\n    out = f\"TODAY: {today}\"\n    print(out)\n    return out\n\n\nasync def main():\n    out = await agent.run(\"What's all the info?\")\n    print(out.data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12\npydantic AI from 0.0.53 until 0.1.1\nmodel gpt-4o\n```",
      "state": "closed",
      "author": "enigma",
      "author_type": "User",
      "created_at": "2025-04-16T17:24:07Z",
      "updated_at": "2025-04-16T20:06:05Z",
      "closed_at": "2025-04-16T20:05:50Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1509/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1509",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1509",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:08.333977",
      "comments": [
        {
          "author": "vikigenius",
          "body": "Might also be related to https://github.com/pydantic/pydantic-ai/issues/1488\n\nThe general problem seems to be about strictness inference",
          "created_at": "2025-04-16T18:16:19Z"
        },
        {
          "author": "vikigenius",
          "body": "Might be related to this PR: https://github.com/pydantic/pydantic-ai/pull/1304. Not sure what was happening previously. Was it defaulting to strict=False ?",
          "created_at": "2025-04-16T18:28:34Z"
        },
        {
          "author": "DouweM",
          "body": "@enigma Thanks for reporting this! Looks like our logic for determining strictness-compatibility is not strict enough 😄 See https://github.com/pydantic/pydantic-ai/issues/1488#issuecomment-2810512975 for some more notes on this -- let's use that as the canonical issue for these strictness issues. \n\n",
          "created_at": "2025-04-16T19:14:59Z"
        },
        {
          "author": "dmontagu",
          "body": "(Closing as we'll use #1488 for tracking)",
          "created_at": "2025-04-16T20:06:03Z"
        }
      ]
    },
    {
      "issue_number": 967,
      "title": "Ability to add/remove system prompt for multi-agent systems",
      "body": "Hi.\n\nI am building a multi-agent system and experimenting with the supervisor pattern using [this example as a guide](https://github.com/codematrix/agentic-ai-design-patterns/blob/develop/design_patterns/supervisor/call_centre_multi_agent.py)\n\nThe issue here is that we would like to share the message history between different agents.\nTo solve it, I remove any system prompt parts and rebuild the system prompt for the current agent, before each agent run.\n\nAlthough I could do this, it was in a \"hacky\" way, acessing 'protected' member variables of the `Agent` class.\n\nI would like to question if adding or removing the system prompt makes sense for the PydanticAI framework and if you have any plans to implement it. I can try to help with a PR.\n\n## [Edit]\nAfter some tests, I've seem that this is also having impacts on cache miss for input tokens, probably because I am rebuilding a dynamic system prompt that changes almost every time during a run (current date time). Any suggestions on how to reduce cache miss here?\n\n",
      "state": "closed",
      "author": "AlexEnrique",
      "author_type": "User",
      "created_at": "2025-02-22T15:13:19Z",
      "updated_at": "2025-04-16T14:28:00Z",
      "closed_at": "2025-04-16T14:28:00Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/967/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/967",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/967",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:08.642588",
      "comments": [
        {
          "author": "Kludex",
          "body": "Can you share how are you doing it, and how'd like to work? Code wise.",
          "created_at": "2025-02-24T11:29:24Z"
        },
        {
          "author": "AlexEnrique",
          "body": "Just had time to come up to this issue today.\nI would like to discuss first if this makes sense for the framework or if it is something that we (the lib users) should do.\n\n\n\nHere is what I am doing currently (kind of) - tested with `v0.0.24`:\n```python\nfrom __future__ import annotations\n\nimport asyn",
          "created_at": "2025-02-27T16:29:16Z"
        },
        {
          "author": "jlowin",
          "body": "You may want to take a look at the upcoming https://github.com/PrefectHQ/marvin 3.0 release, which is built on Pydantic AI and aimed at solving this type of tricky control flow situation. Currently Marvin will rebuild the message history dynamically depending on the agent (i.e. swapping out the syst",
          "created_at": "2025-03-03T00:47:12Z"
        },
        {
          "author": "Kludex",
          "body": "I think the `instructions` that we are adding on https://github.com/pydantic/pydantic-ai/pull/1360 solves this issue.\n\nWould you like to confirm it @AlexEnrique ?",
          "created_at": "2025-04-09T13:45:17Z"
        },
        {
          "author": "AlexEnrique",
          "body": "Hi @Kludex, I was planning to come here share some thoughts, after more development and usage.\n\nAfter taking a look at the changes in #1360, I would like to share some considerations.\n\n- Can `instructions` be a sequence of strings instead of a single string, similar to system prompt? In my particula",
          "created_at": "2025-04-09T18:51:32Z"
        }
      ]
    },
    {
      "issue_number": 1361,
      "title": "`pydantic-graph`: persistence across `GraphRun`",
      "body": "### Question\n\nHi there,\n\nWe're in the process of migrating our agent orchestration to Pydantic Graph and are having some trouble understanding how to handle state persistence between separate `GraphRun`.\n\nWe’ve implemented state persistence by subclassing `BaseStatePersistence` into `DatabaseStatePersistence(BaseStatePersistence[StateT, RunEndT])`, as recommended in the documentation for production use. However, we're unsure how to approach long-running or even infinite graphs that don’t have a terminal state.\n\nOur goal is to maintain a single persistent graph file that we continuously update. To satisfy the validators, we've been `.pop()`-ing the `End` node snapshot since our graph is required to return `pydantic_graph.End(result)`, meaning we can’t return None (which would be one way around this). We've done this because the approach otherwise causes issues when running `self._snapshots_type_adapter.validate_json(json.dumps(state_json))` on the second `GraphRun` - i.e. the next time we try to use the graph, as it expects `GraphRunResult` to be nulled.\n\nIs this the correct way to be thinking about long-running processes or graphs that don’t have a natural end state? We’re basically using a set of graphs to track a customer's state lifespan so that our AI use cases can interface with this \"global customer state\" to pick up at a later time! For example, a voice agent will want to understand what actions have been taken on the account prior in other channels by chat agents. `graph_name` in the below reflects a set of agent graphs that any channel could pick up and resume or inspect.\n\n<img width=\"1116\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9e58516a-92e2-4f01-86fd-9ec6748dc63a\" />\n\nAny guidance would be greatly appreciated.\n\nThanks!\n\nP.S. looking forward to the Pydantic London meetup on the 8th!",
      "state": "open",
      "author": "SamComber",
      "author_type": "User",
      "created_at": "2025-04-03T12:56:09Z",
      "updated_at": "2025-04-16T14:00:51Z",
      "closed_at": null,
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1361/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1361",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1361",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:08.909116",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1201\" (95% similar)",
          "created_at": "2025-04-03T13:00:06Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Hi, just FYI I'm going to be on holiday until Tuesday, so will reply then unless @Kludex or @dmontagu can solve your issues sooner.",
          "created_at": "2025-04-03T13:01:36Z"
        },
        {
          "author": "NotSoShaby",
          "body": "@SamComber if you can eventually share your solution for a database stateful graph, that will be helpful (looking for a similar solution as well)",
          "created_at": "2025-04-09T11:04:33Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-16T14:00:50Z"
        }
      ]
    },
    {
      "issue_number": 960,
      "title": "Vertex Model garden models",
      "body": "Is it possible to use other models in the vertex model garden, or is it just gemini models through vertex?",
      "state": "open",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-02-21T13:10:10Z",
      "updated_at": "2025-04-16T13:30:05Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/960/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/960",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/960",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:09.157431",
      "comments": [
        {
          "author": "Kludex",
          "body": "Do you have a reference to which models are you referring to?",
          "created_at": "2025-02-21T13:24:46Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "LIke mistral and anthropic models through vertex",
          "created_at": "2025-02-21T14:43:15Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "Ok @Kludex, basically you can use the already existing models, but with the vertex counterpart clients of the mistral and anthropic packages. \n\nFeels really silly that there is no docs for this.\nThere are typing errors, but (mostly) it works.\n\n```python\n            if \"mistral\" in model.identity.id:",
          "created_at": "2025-02-21T15:27:42Z"
        },
        {
          "author": "ehaca",
          "body": "Hey I came looking for this one - is this something one can contribute to? Would be thrilled to help out if wanted/needed/possible :)",
          "created_at": "2025-04-05T21:19:43Z"
        },
        {
          "author": "ehaca",
          "body": "Hey, I had some ideas and couldn't help starting drafting: https://github.com/pydantic/pydantic-ai/pull/1392 :see_no_evil:",
          "created_at": "2025-04-06T20:19:57Z"
        }
      ]
    },
    {
      "issue_number": 1470,
      "title": "Manipulate message history",
      "body": "### Question\n\nSince message history can be really long, I cannot just feed a whole list into `agent.run` \n\nAnd I have to keep the `SystemPrompt` in the list history chat (so that the instruction won't be lost) \n\nSo can I just do something like: \n```\n history_messages = [history_messages[0]] + history_messages[-(window_size-1):]\n```\n\n\nBut I  was reported an error:  \n\nstatus_code: 400, model_name: gpt-4o-mini, body: {'message': 'litellm.BadRequestError: OpenAIException - Error code: 400 - {\\'error\\': {\\'message\\': \"Invalid parameter: messages with role \\'tool\\' must be a response to a preceeding message with \\'tool_calls\\'.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'messages.[1].role\\', \\'code\\': None}}. Received Model Group=gpt-4o-mini\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}\n\nHas anyone encountered the same problem? \n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "dinhngoc267",
      "author_type": "User",
      "created_at": "2025-04-14T07:00:38Z",
      "updated_at": "2025-04-16T13:10:15Z",
      "closed_at": "2025-04-16T02:00:01Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1470/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1470",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1470",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:09.439598",
      "comments": [
        {
          "author": "diego898",
          "body": "@dinhngoc267 - how did you resolve this?",
          "created_at": "2025-04-16T13:10:13Z"
        }
      ]
    },
    {
      "issue_number": 1372,
      "title": "Breaking Changes: Path to Stability",
      "body": "As we strive to reach a stable release of PydanticAI, breaking changes will occasionally be necessary. This issue serves as a centralized record of such changes and will be updated exclusively by the maintainers whenever they occur. The sole reason for these breaking changes is to achieve stability as swiftly as possible.\n\nWhile we will make efforts to include deprecation warnings when feasible, there may be instances where the effort to maintain legacy methods is too great. In these cases, we will proceed with implementing a direct breaking change to expedite our path to stability.\n\nWe encourage you to subscribe to this issue to stay informed about these updates. We appreciate your understanding and support as we work diligently towards a robust and stable PydanticAI. 🙏",
      "state": "closed",
      "author": "Kludex",
      "author_type": "User",
      "created_at": "2025-04-04T08:51:16Z",
      "updated_at": "2025-04-16T08:35:59Z",
      "closed_at": "2025-04-16T08:35:58Z",
      "labels": [
        "meta"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1372/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1372",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1372",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:09.660534",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Closing this since as per #1487, we now have https://ai.pydantic.dev/changelog/ with a summary of breaking changes.",
          "created_at": "2025-04-16T08:35:58Z"
        }
      ]
    },
    {
      "issue_number": 1271,
      "title": "MCP improvement: `mcp-run-python` should be be able to call back to the client",
      "body": "so research needed into how we should do this, perhaps we can hijack sampling?",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:28:33Z",
      "updated_at": "2025-04-16T08:32:13Z",
      "closed_at": null,
      "labels": [
        "MCP"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1271/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1271",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1271",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:09.856506",
      "comments": [
        {
          "author": "maxschulz-COL",
          "body": "Not sure if related, but would data I/O be a relevant feature for `mcp-run-python`? Unless I overlook smth, I haven't managed to load files to the pyodide environment, but that could be powerful for data analysis etc.?",
          "created_at": "2025-04-16T08:32:12Z"
        }
      ]
    },
    {
      "issue_number": 1489,
      "title": "Cannot leave a model running",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI want to run a model according to an user prompt, stream text, leave the model call tools and leave it continue to answer until it stops by itself.\nCurrently it's impossibile: even without specifying a ResultType, the model writes  \"I will use the tool...\", calls a tool, and everything stops there.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11\nGemini 2.5 Pro Exp 03-25\nPydanticAI 0.0.52 (last working version with gemini)\n```",
      "state": "open",
      "author": "IngLP",
      "author_type": "User",
      "created_at": "2025-04-15T17:11:31Z",
      "updated_at": "2025-04-15T17:11:31Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1489/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1489",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1489",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:10.111058",
      "comments": []
    },
    {
      "issue_number": 1398,
      "title": "Pydantic AI sends wrong JSON payload to Gemini in v0.0.53 but not in v0.0.52",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIn v0.0.53 of pydantic ai, when using structured outputs **and** the *instrument=True*  argument when creating a gemini Agent, the wrong JSON schema is generated and sent to the Gemini API, resulting in the following error:\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.0-flash, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"$defs\\\" at 'tools.function_declarations[0].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$ref\\\" at 'tools.function_declarations[0].parameters.properties[0].value.items': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools.function_declarations[0].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$defs\\\" at 'tools.function_declarations[0].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[0].parameters.properties[0].value.items\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$ref\\\" at 'tools.function_declarations[0].parameters.properties[0].value.items': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nThis error does not occur in v0.0.52 or below.\n\nChange the pydantic-ai version to v0.0.52 in the example code to see the difference. Run the script with `uv run script.py` \n\n### Example Code\n\n```Python\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"google-genai\",\n#     \"python-dotenv\",\n#     \"pydantic-ai-slim[vertexai]==v0.0.53\",\n#     \"pydantic\",\n# ]\n# ///\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nimport dotenv\ndotenv.load_dotenv()\n\nmodel = GeminiModel(\n    \"gemini-2.0-flash\",\n    provider=\"google-vertex\",\n    # GoogleVertexProvider(service_account_file=os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")),\n)\n\nclass PowerSaleEntry(BaseModel):\n    source_document: str = Field(alias=\"Source document\", description=\"URL of source document that contains this data\")\n    year_month: str = Field(alias=\"Year-Month\", description=\"Month and year of the data in yyyy-mm format\")\n    source: str = Field(alias=\"Source\", description=\"Source of the electricity bought\")\n    kwh_purchased: float = Field(alias=\"kWh purchased\")\n    basic_generation_cost: Optional[float] = Field(alias=\"Basic generation cost (PHP)\")\n    total_generation_cost: float = Field(alias=\"Total generation cost for the month (PHP)\")\n    price_per_kwh: Optional[float] = Field(\n        alias=\"Price per kWh\",\n        description=\"Price of electricity bought, or alternatively, average generation cost (PHP/kWh)\",\n        default=None,\n    )\n\nclass PowerSaleEntries(BaseModel):\n    entries: list[PowerSaleEntry] = Field(alias=\"Entries\", description=\"List of electricity sales entries\")\n\nclass Deps(BaseModel):\n    text: str = Field(description=\"Raw text containing electricity sales data\")\n\nextraction_agent = Agent(\n    model,\n    result_type=PowerSaleEntries,\n    system_prompt=\"You are an expert in electricity data.\",\n    deps_type=Deps,\n    instrument=True\n)\n\n@extraction_agent.system_prompt\nasync def system_prompt(context: RunContext[Deps]) -> str:\n    return f\"\"\"\n    You are an expert in electricity data. Your task is to extract the electricity sales data from the text.\n    The text contains multiple entries, each with the following fields:\n    - Source document\n    - Year-Month\n    - Source\n    - kWh purchased\n    - Basic generation cost (PHP)\n    - Total generation cost for the month (PHP)\n\n    Please extract these fields and return them in a structured format.\n\n    This is the text you need to process:\n    {context.deps.text}\n    \"\"\"\n\nasync def extract_power_sales(text: str) -> PowerSaleEntries:\n    context = \"Please extract the electricity sales data from the text.\"\n    result = await extraction_agent.run(context, deps=Deps(text=text))\n    return result\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    sample_text = \"\"\"\n    Source document: https://example.com/doc1\n    Year-Month: 2023-01\n    Source: Coal Plant A\n    kWh purchased: 1000000\n    Basic generation cost (PHP): 500000\n    Total generation cost for the month (PHP): 550000\n\n    Source document: https://example.com/doc2\n    Year-Month: 2023-01\n    Source: Wind Farm B\n    kWh purchased: 200000\n    Basic generation cost (PHP): 100000\n    Total generation cost for the month (PHP): 120000\n\n    Source document: https://example.com/doc3\n    Year-Month: 2023-01\n    Source: Solar Plant C\n    kWh purchased: 300000\n    Basic generation cost (PHP): 150000\n    Total generation cost for the month (PHP): 180000\n    \"\"\"\n\n    result = asyncio.run(extract_power_sales(sample_text))\n\n    print(result.data)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.11\npydantic_ai 0.0.53\n```",
      "state": "closed",
      "author": "dldx",
      "author_type": "User",
      "created_at": "2025-04-07T22:21:45Z",
      "updated_at": "2025-04-15T16:12:51Z",
      "closed_at": "2025-04-14T18:15:55Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1398/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1398",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1398",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:10.111082",
      "comments": [
        {
          "author": "fabioscantamburlo",
          "body": "Same here. I'm experiencing this issue only for 0.0.53",
          "created_at": "2025-04-08T08:11:40Z"
        },
        {
          "author": "xxxpsyduck",
          "body": "I got similar issue\n```bash\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: google-gla:gemini-2.0-flash, body: [{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"additionalProperties\\\" at 'tools.function_declarations[0].parameters': Ca",
          "created_at": "2025-04-09T02:53:24Z"
        },
        {
          "author": "IngLP",
          "body": "I have the same problem with pydantic-ai 0.0.55\n\n```\n        async with self.client.stream(\n            'POST',\n            url,\n            content=request_json,\n            headers=headers,\n            timeout=model_settings.get('timeout', USE_CLIENT_DEFAULT),\n        ) as r:\n            if (statu",
          "created_at": "2025-04-11T09:49:38Z"
        },
        {
          "author": "IngLP",
          "body": "@dmontagu can you please reopen the issue?",
          "created_at": "2025-04-11T09:50:05Z"
        },
        {
          "author": "IngLP",
          "body": "@dmontagu or should I open another issue?",
          "created_at": "2025-04-11T09:50:37Z"
        }
      ]
    },
    {
      "issue_number": 1085,
      "title": "Add HuggingFace to Models.",
      "body": "### Description\n\nThey appear to have a client package built for openai client users, so perhaps that's a good start. \nThis integration would open access to a lot of models.\n\n\n\n### References\n\nhttps://huggingface.co/docs\n\nhttps://huggingface.co/docs/huggingface_hub/guides/inference#openai-compatibility\n\nhttps://huggingface.co/models",
      "state": "open",
      "author": "Luca-Blight",
      "author_type": "User",
      "created_at": "2025-03-09T19:13:13Z",
      "updated_at": "2025-04-15T16:10:05Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 22,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1085/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1085",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1085",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:10.379073",
      "comments": []
    },
    {
      "issue_number": 1222,
      "title": "Should we move to aioboto3?",
      "body": "### Description\n\nWe currently use anyio to wrap boto3's api into async: https://github.com/pydantic/pydantic-ai/blob/3a3b52654dee1ed1732798059fecb07b6ce7f041/pydantic_ai_slim/pydantic_ai/models/bedrock.py#L481-L499\n\nWe might move to aioboto3 for better performance. What do you think? I can help with this.\n\n\n\n\n\n### References\n\naioboto3 have already support `bedrock-runtime`: https://github.com/terricain/aioboto3/issues/317",
      "state": "open",
      "author": "Wh1isper",
      "author_type": "User",
      "created_at": "2025-03-24T07:28:45Z",
      "updated_at": "2025-04-15T10:37:38Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "bedrock"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1222/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1222",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1222",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:10.379095",
      "comments": [
        {
          "author": "Kludex",
          "body": "I see `aioboto3` uses `aiobotocore`. It uses `aiohttp` underneath, I guess?\n\nIt shouldn't be a big change to migrate. Do you see any reference to issues regarding \"bedrock\" on their issue tracker? If not, maybe we should try it out.",
          "created_at": "2025-03-26T11:54:57Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-04-02T14:00:35Z"
        },
        {
          "author": "Wh1isper",
          "body": "> This issue is stale, and will be closed in 3 days if no reply is received.\n\n👀",
          "created_at": "2025-04-02T15:12:07Z"
        },
        {
          "author": "Kludex",
          "body": "Is aioboto3 properly typed? If yes, then let's move onto it. It shouldn't be much of a refactor anyway.",
          "created_at": "2025-04-07T14:10:16Z"
        },
        {
          "author": "Kludex",
          "body": "Wanna open a PR @Wh1isper ? 👀 ",
          "created_at": "2025-04-07T14:10:24Z"
        }
      ]
    },
    {
      "issue_number": 1414,
      "title": "toolcalling with vllm is broken",
      "body": "### Question\n\nI am following the example in https://ai.pydantic.dev/examples/rag/#example-code\n\nI am also using the latest version of vllm (0.8.3) with toolcalling support with  --guided_decoding_backend. \n\nThe rag example doesnt work with any of models listed in https://docs.vllm.ai/en/stable/features/tool_calling.html#llama-models-llama3-json. Some models (llama3.2) correctly returns the tool call, but agent is unable to parse and make a call tool\n\nI used llama3.1 https://docs.vllm.ai/en/latest/features/tool_calling.html#llama-models-llama3-json, the tool call happens repeatedly (never ending), so the token limit exceed, and crashes. \n\nHowever, using openai model works perfectly. Even ollama works (without streaming) \n\n### Additional Context\n\nPydantic AI version - pydantic-ai==0.0.43\nPython - 3.11.0\nvllm - 0.0.83 - https://github.com/vllm-project/vllm/pull/13483 (this doesnt fix the issue I am facing) ",
      "state": "open",
      "author": "suresh-now",
      "author_type": "User",
      "created_at": "2025-04-08T17:52:00Z",
      "updated_at": "2025-04-15T01:51:05Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1414/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1414",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1414",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:10.633531",
      "comments": [
        {
          "author": "aristideubertas",
          "body": "In my experience tool calling with models such as Qwen and LLAMA hasn't worked very where since it seems like they do not natively support function calling, and some sort of parsing has to happen for the tool usage to happen. I think VLLM is supposed to fix this internally with certain options, but ",
          "created_at": "2025-04-09T13:18:40Z"
        },
        {
          "author": "ItzAmirreza",
          "body": "I'm experiencing the same issue. It calls the tools again and again until it runs out of context and timesout. It wasn't a problem a version ago but after I updated my pydantic ai, it doesn't work anymore. Something has definitely changed, but still investigating. Please update me if you found any f",
          "created_at": "2025-04-10T17:12:21Z"
        },
        {
          "author": "webcoderz",
          "body": "Related to: https://github.com/pydantic/pydantic-ai/pull/825",
          "created_at": "2025-04-15T01:51:03Z"
        }
      ]
    },
    {
      "issue_number": 1346,
      "title": "Test fails",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n```\n__________ test_docs_examples[docs/evals.md:158-250:judge_recipes.py] __________\nPrint output changed code:\n  --- before\n  +++ after\n  @@ -234,174 +234,180 @@\n \n \n   report = recipe_dataset.evaluate_sync(transform_recipe)\n   print(report)\n   \"\"\"\n  -     Evaluation Summary: transform_recipe\n  -┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n  -┃ Case ID            ┃ Assertions ┃ Duration ┃\n  -┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n  -│ vegetarian_recipe  │ ✔✔✔        │     10ms │\n  -├────────────────────┼────────────┼──────────┤\n  -│ gluten_free_recipe │ ✔✔✔        │     10ms │\n  -├────────────────────┼────────────┼──────────┤\n  -│ Averages           │ 100.0% ✔   │     10ms │\n  -└────────────────────┴────────────┴──────────┘\n  +                Evaluation Summary: transform_recipe\n  +┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n  +┃ Case ID            ┃ Metrics             ┃ Assertions ┃ Duration ┃\n  +┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n  +│ vegetarian_recipe  │ input_tokens: 136   │ ✔✔✔        │     10ms │\n  +│                    │ output_tokens: 42   │            │          │\n  +│                    │ requests: 1         │            │          │\n  +├────────────────────┼─────────────────────┼────────────┼──────────┤\n  +│ gluten_free_recipe │ input_tokens: 136   │ ✔✔✔        │     10ms │\n  +│                    │ output_tokens: 46   │            │          │\n  +│                    │ requests: 1         │            │          │\n  +├────────────────────┼─────────────────────┼────────────┼──────────┤\n  +│ Averages           │ input_tokens: 136.0 │ 100.0% ✔   │     10ms │\n  +│                    │ output_tokens: 44.0 │            │          │\n  +│                    │ requests: 1.00      │            │          │\n  +└────────────────────┴─────────────────────┴────────────┴──────────┘\n   \"\"\"\n\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.13.2\nPydantic: 2.10.6\n```",
      "state": "closed",
      "author": "medaminezghal",
      "author_type": "User",
      "created_at": "2025-04-02T15:07:22Z",
      "updated_at": "2025-04-12T07:32:00Z",
      "closed_at": "2025-04-03T06:05:56Z",
      "labels": [
        "Stale",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1346/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1346",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1346",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:10.860931",
      "comments": [
        {
          "author": "dmontagu",
          "body": "I'm not able to reproduce this locally.\n\nThe issue is going to be something with the execution in `tests.test_examples.model_logic` causing it to not hit the expected branch when running this test — have you made any other changes in the repo that might have caused a test failure? Does the test fail",
          "created_at": "2025-04-02T15:28:41Z"
        },
        {
          "author": "medaminezghal",
          "body": "@Kludex try use pytest -vv tests/test_examples.py",
          "created_at": "2025-04-02T16:24:55Z"
        },
        {
          "author": "Kludex",
          "body": "Please try to recreate the virtual environment. I cannot reproduce it.",
          "created_at": "2025-04-03T06:05:56Z"
        },
        {
          "author": "medaminezghal",
          "body": "@Kludex  I've followed your contribution guide and I still get the same error. I don't why the issue was closed. This another fail in test I get:\n```\n____________________________ test_vertexai_provider ____________________________\n\nallow_model_requests = None\n\n    @pytest.mark.vcr()\n    async def te",
          "created_at": "2025-04-03T19:55:41Z"
        },
        {
          "author": "medaminezghal",
          "body": "@Kludex I’ve just cloned the repository and use make install and make test to get those errors.",
          "created_at": "2025-04-03T19:59:31Z"
        }
      ]
    },
    {
      "issue_number": 1411,
      "title": "BedrockConverseModel - Add Video Support for AWS Bedrock Integration",
      "body": "### Description\n\nPydantic AI currently supports various content types for Bedrock integration, but lacks native support for video content. This issue aims to enhance the framework by adding:\n\n- A new `VideoUrl` type\n- Extended `BinaryContent` support for video formats\n- Consistent with Bedrock's video content API specifications\n\n### Bedrock Video Content API Specification\n\nCan ignore s3Location and keep only bytes, the same you do with image and document\n\n```txt\n{\n    'video': {\n        'format': 'mkv'|'mov'|'mp4'|'webm'|'flv'|'mpeg'|'mpg'|'wmv'|'three_gp',\n        'source': {\n            'bytes': b'bytes',\n            's3Location': {\n                'uri': 'string',\n                'bucketOwner': 'string'\n            }\n        }\n    }\n}\n```\n\n### Proposed Changes\n\n1. Implement `VideoUrl` class similar to existing `ImageUrl` and `DocumentUrl`\n2. Extend `BinaryContent` to support video formats\n3. Support Bedrock's video format specifications:\n   - Supported formats: mkv, mov, mp4, webm, flv, mpeg, mpg, wmv, 3gp\n   - Support for both URL and byte-based video sources\n\n### Example Usage\n\n```python\nfrom pydantic_ai import Agent, VideoUrl\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\n\nmodel = BedrockConverseModel(model_name=\"us.amazon.nova-pro-v1:0\")\n\nagent = Agent(model=model)\n\nresult_sync = agent.run_sync(\n    [\n        'do stuff with this video',\n        VideoUrl(url='https://my.video.domain/video.mov'),\n    ]\n)\nprint(result_sync.data)\n```\n\nI am open to contributing to this feature and can help implement the proposed changes.\n\n\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "leandrodamascena",
      "author_type": "User",
      "created_at": "2025-04-08T14:33:58Z",
      "updated_at": "2025-04-11T11:00:01Z",
      "closed_at": "2025-04-11T11:00:01Z",
      "labels": [
        "Feature request",
        "bedrock"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1411/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1411",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1411",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:11.115459",
      "comments": [
        {
          "author": "Kludex",
          "body": "Wanna try to open a PR?",
          "created_at": "2025-04-09T08:32:57Z"
        },
        {
          "author": "leandrodamascena",
          "body": "Ei @Kludex! Yeah, sure! Thanks for accepting the idea.\n\nDo you think that make sense I try to include https://github.com/pydantic/pydantic-ai/issues/1396 in this PR or maybe work in a separate PR?",
          "created_at": "2025-04-09T09:09:20Z"
        },
        {
          "author": "Kludex",
          "body": "I'm working on some configuration settings today (and maybe I'll continue till Friday). There are some settings that should be for all models, so I'm trying to check that.\n\nIf I don't add the settings you need, feel free to create a PR next week.",
          "created_at": "2025-04-09T09:16:31Z"
        },
        {
          "author": "leandrodamascena",
          "body": "> I'm working on some configuration settings today (and maybe I'll continue till Friday). There are some settings that should be for all models, so I'm trying to check that.\n> \n> If I don't add the settings you need, feel free to create a PR next week.\n\nSince the Bedrock API is very specific to Bedr",
          "created_at": "2025-04-09T09:50:40Z"
        },
        {
          "author": "Kludex",
          "body": "> > I'm working on some configuration settings today (and maybe I'll continue till Friday). There are some settings that should be for all models, so I'm trying to check that.\n> > If I don't add the settings you need, feel free to create a PR next week.\n> \n> Since the Bedrock API is very specific to",
          "created_at": "2025-04-09T09:58:24Z"
        }
      ]
    },
    {
      "issue_number": 1442,
      "title": "Is sse_client possible to do \"MCP Run Python\" ?",
      "body": "### Question\n\naccording to the MCP Run Python document https://ai.pydantic.dev/mcp/run-python/, seems sse server can be backend to support running Python code too. But only example code is run server stdio mode. I wonder if the server runs in sse mode and if there is a way to run Python code with sse_client?\n\nThanks\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "sangshuduo",
      "author_type": "User",
      "created_at": "2025-04-10T18:49:00Z",
      "updated_at": "2025-04-11T06:15:43Z",
      "closed_at": "2025-04-11T06:14:29Z",
      "labels": [
        "question",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1442/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1442",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1442",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:11.354459",
      "comments": [
        {
          "author": "Kludex",
          "body": "Yeah, it's possible. You'd need to run the application elsewhere with the `sse` transport, and then use the `MCPServerHTTP` with the address of your server.\n\nhttps://github.com/pydantic/pydantic-ai/blob/main/mcp-run-python/src/main.ts#L34",
          "created_at": "2025-04-11T06:14:29Z"
        }
      ]
    },
    {
      "issue_number": 671,
      "title": "TypeError: Object of type ValueError is not JSON serializable",
      "body": "### Description  \r\nSorry if this is already reported or if there is a solution im missing. \r\n\r\nI've encountered a sporadic issue while using a Pydantic model with a custom validator (but i guess that does not have to be related). About **2% of the runs** result in the following error:  \r\n```\r\nException: TypeError: Object of type ValueError is not JSON serializable\r\n```\r\n\r\nI haven’t yet found a reliable way to reproduce the issue, below is a mocked-up code snippet and a stack trace to demonstrate the scenario.\r\n\r\n---\r\n\r\n### Code Example  \r\n\r\n```python\r\nfrom typing import Any\r\nimport pydantic_ai\r\nfrom pydantic import BaseModel, model_validator\r\n\r\nVALID_TYPES = {\"test\": [\"testing\"]}\r\n\r\nclass TypeModel(BaseModel):\r\n    type: str\r\n\r\n    @model_validator(mode=\"before\")\r\n    def validate_type(cls, values: dict[str, Any]):\r\n        type_ = values.get(\"type\")\r\n        if type_ not in VALID_TYPES:\r\n            raise ValueError(\r\n                f\"Invalid type '{type_}'. Valid types are: {list(VALID_TYPES.keys())}.\"\r\n            )\r\n        return values\r\n\r\nagent = pydantic_ai.Agent(model=\"openai:gpt-4o\", result_type=TypeModel)\r\nagent.run_sync(\"toast\")\r\n```\r\n\r\n---\r\n\r\n### Stack Trace  \r\n\r\n```\r\n    response = self.agent.run_sync(prompt)\r\n  File \"/home/site/wwwroot/.python_packages/lib/site-packages/pydantic_ai/agent.py\", line 220, in run_sync\r\n    return asyncio.run(self.run(user_prompt, message_history=message_history, model=model, deps=deps))\r\n  File \"/usr/local/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/home/site/wwwroot/.python_packages/lib/site-packages/pydantic_ai/agent.py\", line 172, in run\r\n    model_response, request_cost = await agent_model.request(messages)\r\n  File \"/home/site/wwwroot/.python_packages/lib/site-packages/pydantic_ai/models/openai.py\", line 125, in request\r\n    response = await self._completions_create(messages, False)\r\n  File \"/home/site/wwwroot/.python_packages/lib/site-packages/pydantic_ai/models/openai.py\", line 155, in _completions_create\r\n    openai_messages = [self._map_message(m) for m in messages]\r\n  File \"/home/site/wwwroot/.python_packages/lib/site-packages/pydantic_ai/models/openai.py\", line 155, in <listcomp>\r\n    openai_messages = [self._map_message(m) for m in messages]\r\n  File \"/home/site/wwwroot/.python_packages/lib/site-packages/pydantic_ai/models/openai.py\", line 236, in _map_message\r\n    content=message.model_response(),\r\n  File \"/home/site/wwwroot/.python_packages/lib/site-packages/pydantic_ai/messages.py\", line 121, in model_response\r\n    description = f'{len(self.content)} validation errors: {json.dumps(self.content, indent=2)}'\r\n  File \"/usr/local/lib/python3.10/json/__init__.py\", line 238, in dumps\r\n    **kw).encode(obj)\r\n  File \"/usr/local/lib/python3.10/json/encoder.py\", line 201, in encode\r\n    chunks = list(chunks)\r\n  File \"/usr/local/lib/python3.10/json/encoder.py\", line 429, in _iterencode\r\n    yield from _iterencode_list(o, _current_indent_level)\r\n  File \"/usr/local/lib/python3.10/json/encoder.py\", line 325, in _iterencode_list\r\n    yield from chunks\r\n  File \"/usr/local/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"/usr/local/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"/usr/local/lib/python3.10/json/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"/usr/local/lib/python3.10/json/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\n```",
      "state": "closed",
      "author": "HampB",
      "author_type": "User",
      "created_at": "2025-01-13T15:37:08Z",
      "updated_at": "2025-04-11T05:39:30Z",
      "closed_at": "2025-04-11T05:39:29Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/671/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/671",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/671",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:11.576082",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Thanks for reporting, should be fairly easy to fix.",
          "created_at": "2025-01-16T10:09:05Z"
        },
        {
          "author": "fatelei",
          "body": "using gemini-1.5-flash has no error",
          "created_at": "2025-01-23T08:00:47Z"
        },
        {
          "author": "Kludex",
          "body": "This issue is solved in main.",
          "created_at": "2025-04-11T05:39:29Z"
        }
      ]
    },
    {
      "issue_number": 1219,
      "title": "add the ability to set a list of allowed tools on an mcp server",
      "body": "### Description\n\nThere are cases where we may want to restrict an agent to only be allowed to use a subset of tools returned by a particular MCP server. For example for a filesystem MCP server, we may want to restrict the agent to tools that are read only.  This can be support by adding a allowed_tools parameter to the MCPServer class. \nInitially we this could be supported through by an explicit list of tool names; subsequent improvements could support regex/glob patterns.\n\n### References\n\n_No response_",
      "state": "open",
      "author": "JohnUiterwyk",
      "author_type": "User",
      "created_at": "2025-03-24T02:07:40Z",
      "updated_at": "2025-04-10T19:16:35Z",
      "closed_at": null,
      "labels": [
        "MCP"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1219/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1219",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1219",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:13.671354",
      "comments": [
        {
          "author": "Kludex",
          "body": "I think this should be part of a `tool_choice` parameter.",
          "created_at": "2025-03-24T07:16:34Z"
        },
        {
          "author": "JohnUiterwyk",
          "body": "My understanding of tool_choice is that it is meant to require that a model call a specific or set of tools, and is done model side. \n\nThis feature request is instead is about limiting the tools provided to the model (and in pydantic ai, a specific agent.)\n\nThere will also be cases were there are a ",
          "created_at": "2025-03-24T08:28:29Z"
        },
        {
          "author": "milani",
          "body": "I run into this issue, but more generally and not for MCP. I define an agent for a set of related tasks that share the same system prompt and I pass it the tools required for all those tasks. But not all tasks need access to all tools.\n\nGiven that the LLM has access to the tools, it sometimes just c",
          "created_at": "2025-03-24T22:18:21Z"
        },
        {
          "author": "Lewik",
          "body": "Maybe this will help?\nhttps://github.com/Lewik/mcp-tools-proxy",
          "created_at": "2025-04-10T19:16:33Z"
        }
      ]
    },
    {
      "issue_number": 1434,
      "title": "otel: consider only one span when only one tool is used",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nRight now, if you run a single tool, you get a parent aggregator span that says it will run one tool. This seems chatty.\n\n<img width=\"1232\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/01e6df7b-215c-48d1-bf1a-96c86cb55189\" />\n\n<img width=\"1458\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/93db4040-b154-453e-b4ab-e72b7c405f50\" />\n\n### Example Code\n\n```Python\nimport os\n\nimport httpx\nfrom openai import AsyncAzureOpenAI\nfrom pydantic_ai import Agent\nfrom pydantic_ai.agent import InstrumentationSettings\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n# This enables all instrumentation in pydantic-ai\nAgent.instrument_all(instrument=InstrumentationSettings(event_mode=\"logs\"))\n\n\ndef get_latest_elasticsearch_version(major_version: int = 0) -> str:\n    \"\"\"Returns the latest GA version of Elasticsearch in \"X.Y.Z\" format.\n\n    Args:\n        major_version: Major version to filter by (e.g. 7, 8). Defaults to latest\n    \"\"\"\n\n    response = httpx.get(\"https://artifacts.elastic.co/releases/stack.json\")\n    response.raise_for_status()\n    releases = response.json()[\"releases\"]\n\n    # Fetch releases and filter out non-release versions (e.g., -rc1) or\n    # those not matching major_version. In any case, remove \" GA\" suffix.\n    versions = []\n    for r in releases:\n        v = r[\"version\"].removesuffix(\" GA\")\n        if \"-\" in r[\"version\"]:\n            continue\n        if major_version and int(v.split(\".\")[0]) != major_version:\n            continue\n        versions.append(v)\n\n    if not versions:\n        raise ValueError(\"No valid versions found\")\n\n    # \"8.9.1\" > \"8.10.0\", so coerce to a numeric tuple: (8,9,1) < (8,10,0)\n    return max(versions, key=lambda v: tuple(map(int, v.split(\".\"))))\n\n\ndef main():\n    provider = \"openai\"\n    if os.getenv(\"AZURE_OPENAI_API_KEY\"):\n        provider = OpenAIProvider(openai_client=AsyncAzureOpenAI())\n    model = OpenAIModel(os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\"), provider=provider)\n    agent = Agent(model, tools=[get_latest_elasticsearch_version])\n\n    result = agent.run_sync(\n        user_prompt=\"What is the latest version of Elasticsearch 8?\",\n        model_settings={\"temperature\": 0},\n    )\n    print(result.data)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.9\npydantic-ai 0.0.55\nOllama 0.6.4 + qwen2.5:0.5b\n```",
      "state": "open",
      "author": "codefromthecrypt",
      "author_type": "User",
      "created_at": "2025-04-10T06:47:29Z",
      "updated_at": "2025-04-10T12:54:00Z",
      "closed_at": null,
      "labels": [
        "OpenTelemetry"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1434/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki",
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1434",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1434",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:13.952363",
      "comments": [
        {
          "author": "alexmojaki",
          "body": "I don't like the idea of having the 'aggregator span' sometimes and not other times.\n\nI'm in favour of just never having this parent span, but @samuelcolvin really wanted it.",
          "created_at": "2025-04-10T08:53:01Z"
        },
        {
          "author": "samuelcolvin",
          "body": "I don't agree with removing the aggregator span, I think in relatively long runs it's very useful to clearly show the overall flow:\n\n* call model\n* call tools\n* call model\n* call tools\n* ...\n\nOf course it's kind of redundant in the common case of one tool, I too thought about removing it when there'",
          "created_at": "2025-04-10T09:11:06Z"
        },
        {
          "author": "codefromthecrypt",
          "body": "so, in the (zipkin) past I remember folks using aggregator spans at times for things that were the same request. for example a fan-out. it was very rare, though, and in otel I don't think it is typical (or you would see aggregator http spans for a retrying client). In the case of tools, I'm not sure",
          "created_at": "2025-04-10T09:17:01Z"
        },
        {
          "author": "alexmojaki",
          "body": "Maybe some OTel semantic conventions are needed here?",
          "created_at": "2025-04-10T12:53:59Z"
        }
      ]
    },
    {
      "issue_number": 1420,
      "title": "How to create dependencies between tools (tool call order constraint)",
      "body": "### Question\n\nI know this might reduce the flexibility and dynamic of an agent but sometimes I want to constraint the order of tools called especially in sequential mode. For example:\n \n\nI have 2 tools which are tool A and tool B.  I want if agent want to use tool A, it has to run tool B first.  Now I have to do something with message to guide the agent the proper tool to call, for example: \n\n\n```\nasync def tool_A(ctx: RunContext[Deps]):\n   if ctx.deps.content is None:\n      return \"Run tool_B first\"\n   ....\n\n@agent.tool \nasync def tool_B()\n``` \n\n\n### Additional Context\n\npydantic-ai: v0.0.36",
      "state": "closed",
      "author": "dinhngoc267",
      "author_type": "User",
      "created_at": "2025-04-09T11:53:40Z",
      "updated_at": "2025-04-10T09:57:02Z",
      "closed_at": "2025-04-10T09:57:01Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1420/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1420",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1420",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:14.190973",
      "comments": [
        {
          "author": "rmaceissoft",
          "body": "@dinhngoc267 What if you enforce the tool call order by using the [prepare method](https://ai.pydantic.dev/tools/#tool-prepare) to dynamically enable or disable a tool based on execution history? In your case, tool A should only be available after tool B has run. Here’s how you can do that:\n\n```\n@da",
          "created_at": "2025-04-09T18:05:27Z"
        },
        {
          "author": "dinhngoc267",
          "body": "@rmaceissoft Hey thank you.  Yes, this is very useful. \n\n\n\n\n\n",
          "created_at": "2025-04-10T09:57:01Z"
        }
      ]
    },
    {
      "issue_number": 1399,
      "title": "Evals fail to run if logfire is not available",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n`Dataset.evaluate_sync` fails with the following error if `logfire` is not available:\n\n```\n    | Traceback (most recent call last):\n    |   File \"/home/DouweM/pydantic-ai-evals-test/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n    |     results[index] = await tsk()\n    |                      ^^^^^^^^^^^\n    |   File \"/home/DouweM/pydantic-ai-evals-test/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/DouweM/pydantic-ai-evals-test/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 907, in _run_task_and_evaluators\n    |     trace_id = f'{context.trace_id:032x}'\n    |                  ^^^^^^^^^^^^^^^^^^^^^^^\n    | TypeError: unsupported format string passed to MagicMock.__format__\n    +------------------------------------\n```\n\nThis is the code in question:\n\nhttps://github.com/pydantic/pydantic-ai/blob/84c6c5d23ba59e5feae4188e1080ac6a85e00b63/pydantic_evals/pydantic_evals/dataset.py#L902-L908\n\nIt expects `LogfireSpan.context` to return a real context object, but instead it returns a `MagicMock` when `logfire` failed to import:\n\nhttps://github.com/pydantic/logfire/blob/23f2bf96b351f8416d71a192c0107b60a39fe0e5/logfire-api/logfire_api/__init__.py#L13-L23\n\nSince the Dataset code already checks for `context is None`, we likely just need to explicitly define `context` to return `None`.\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13, pydantic-ai 0.0.53, logfire-api 3.12.0\n```",
      "state": "closed",
      "author": "DouweM",
      "author_type": "User",
      "created_at": "2025-04-07T23:34:34Z",
      "updated_at": "2025-04-10T09:30:11Z",
      "closed_at": "2025-04-10T09:30:11Z",
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1399/reactions",
        "total_count": 5,
        "+1": 5,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1399",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1399",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:14.388422",
      "comments": []
    },
    {
      "issue_number": 1179,
      "title": "call llm without passing a user prompt, just history",
      "body": "### Description\n\ncurrently all our \"run\" and other variation calls require a user prompt, history and more.\nWhats happening under the hood, we just append the user prompt to the history and send it to the llm.\n\nBut sometimes we just want to call the llm without passing an additional user prompt. I just want to have the history used. I noticed people are just passing an empty string, so I assume that works.\nbut would be nice to have an explicit method, or just having the user_prompt optional.\n\nExample flow:\n\n```\nhistory = [...]\nresult = agent1.run(user_prompt, history)\n\nhistory.add(result.new_message)\n# assuming we have a bunch of tools calls and maybe agent1 response.\n\n# now I want agent2 to give his take here, but I don't want another user_prompt\nagent2.run(\"\", history)\n\n# would be nice just to have, agent2.run(history)\n\n```\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "tzookb",
      "author_type": "User",
      "created_at": "2025-03-19T14:53:52Z",
      "updated_at": "2025-04-09T14:15:32Z",
      "closed_at": "2025-04-09T14:15:22Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1179/reactions",
        "total_count": 8,
        "+1": 6,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1179",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1179",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:14.388440",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "agreed, we should allow `None` as an input or maybe be more explicit and have some kind of marker like `NoFurtherInput` to indicate \"just continue the conversation without adding a message\".",
          "created_at": "2025-03-20T09:27:37Z"
        },
        {
          "author": "Kludex",
          "body": "- This was done in https://github.com/pydantic/pydantic-ai/pull/1406 and https://github.com/pydantic/pydantic-ai/pull/1421.",
          "created_at": "2025-04-09T14:15:23Z"
        }
      ]
    },
    {
      "issue_number": 462,
      "title": "Add the Option to Run Tools Sequentially ",
      "body": "I was implementing an agent to automate Git-Workflows. The Git-Tool enabled the agent to execute Git commands on my Computer. When I asked the agent to add the latest changes and then to commit and push them, it ran into an index lock. This is because all the agent wants to execute all tool calls at once. It would be nice to have the option to either run the tools all at once or sequentially.",
      "state": "closed",
      "author": "renkehohl",
      "author_type": "User",
      "created_at": "2024-12-18T09:56:52Z",
      "updated_at": "2025-04-09T11:08:36Z",
      "closed_at": "2025-04-09T11:08:35Z",
      "labels": [
        "Feature request",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/462/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/462",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/462",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:14.605680",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "@sydney-runkle we can add this to `ModelSettings` I guess.\r\n\r\n@renkehohl you can probably avoid the issue you're having using [`asyncio.Lock`](https://docs.python.org/3/library/asyncio-sync.html#asyncio.Lock), either making the lock global or putting it in dependencies.",
          "created_at": "2024-12-18T10:55:07Z"
        },
        {
          "author": "rubentorresbonet",
          "body": "Same for me, it'd be great",
          "created_at": "2025-01-08T16:42:07Z"
        },
        {
          "author": "dinhngoc267",
          "body": "Hello, have this issue been solved?",
          "created_at": "2025-04-09T09:57:55Z"
        },
        {
          "author": "Kludex",
          "body": "Isn't this solved by setting the `parallel_tool_calls` to `False`? 🤔 \n\nI think this is solved...",
          "created_at": "2025-04-09T10:00:24Z"
        },
        {
          "author": "dinhngoc267",
          "body": "> Isn't this solved by setting the `parallel_tool_calls` to `False`? 🤔\n> \n> I think this is solved...\n\nYes you're right! Thank you:\n\n### Reference: \nhttps://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.parallel_tool_calls",
          "created_at": "2025-04-09T10:53:22Z"
        }
      ]
    },
    {
      "issue_number": 1264,
      "title": "Specify stop tokens / sequences in ModelSettings?",
      "body": "### Description\n\nIs it not possible to specify stop tokens / strings in ModelSettings?\n\nI think this is supported by most, if not all, providers?\n\nI might be missing something, maybe it's possible to do this and I'm missing it somehow. But I took a look at the definitions of ModelSettings and its implementations, and you can set temperature, top_k, etc, but not provide stop strings. This is super important for certain agent definitions, where you might guide the model to respond with some pattern or structure. You might prompt the model for code blocks and tell it to think, and then emit the output in a ``` code block ``` (using the three ticks) and therefore you want to provide \\`\\`\\` as a stop sequence.\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "andysalerno",
      "author_type": "User",
      "created_at": "2025-03-28T00:12:21Z",
      "updated_at": "2025-04-09T09:45:03Z",
      "closed_at": "2025-04-09T09:45:03Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1264/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1264",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1264",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:14.853551",
      "comments": []
    },
    {
      "issue_number": 909,
      "title": "Including common tools / toolsets within PydanticAI",
      "body": "We plan to add the most commonly used tools within PydanticAI, there's more context on #110 which covers how toolsets might work.\n\nThis is the parent issue for adding tools. (I'm drinking the cool ade and trying GitHub parent/child issues 🤷 )",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-02-12T19:53:34Z",
      "updated_at": "2025-04-09T08:47:36Z",
      "closed_at": "2025-04-09T08:47:36Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/909/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 2,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/909",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/909",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:14.853572",
      "comments": [
        {
          "author": "Kludex",
          "body": "We now have the [Common Tools] page in our documentation with Search Tools.\n\nI'll update this issue when we get more.",
          "created_at": "2025-02-27T11:31:33Z"
        }
      ]
    },
    {
      "issue_number": 912,
      "title": "Browser control tool",
      "body": "Similar to OpenAI's [operator](https://openai.com/index/introducing-operator/) (but open source and usable locally) and [browser use](https://github.com/browser-use/browser-use) but without all the baggage of Langchain.\n\nI'm pretty sure the right way to do this is with [playwright](https://github.com/microsoft/playwright), I have some ideas on how to proceed, but I need to experiment.",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-02-12T20:07:31Z",
      "updated_at": "2025-04-09T08:47:28Z",
      "closed_at": "2025-04-09T08:47:27Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/912/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/912",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/912",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:15.067907",
      "comments": [
        {
          "author": "tamir-alltrue-ai",
          "body": "Hi @samuelcolvin  and team, \n\nWe use `browser-use` a lot for our projects, and though I'm really hoping you guys build a version that's as good or better (as I love your stuff and want our entire stack on it) - would you consider adding Logfire instrumentation for `browser-use` so that we could enjo",
          "created_at": "2025-02-21T14:51:06Z"
        },
        {
          "author": "samuelcolvin",
          "body": "this is not necessary, the [playright MCP server](https://github.com/microsoft/playwright-mcp) does an excellent job of this.",
          "created_at": "2025-04-09T08:47:27Z"
        }
      ]
    },
    {
      "issue_number": 890,
      "title": "Run agent only on message list instead of user prompt string?",
      "body": "Hi, first thanks for building this awesome library!\nI work with an existing chat app and it represents a chat as a list of messages, and the goal is the get a next message. \nThe last message is not always sent by a user, there are situations where you have an assistant message last and still want to generate a new one.\n\nSo doing this would be nice:\n``` python\nresult = agent.run_sync(\n    [\n        {\"type\": \"user\", \"content\": 'Where does \"hello world\" come from?'},\n        {\"type\": \"assistant\", \"content\": \"Let me look it up...\"},\n    ]\n)\n```\nI know I can format the chat into the user prompt like this:\n``` python\nmessages = [\n    {\"type\": \"user\", \"content\": \"Where does \"hello world\" come from?\"},\n    {\"type\": \"assistant\", \"content\": \"Let me look it up...\"},\n]\nchat_formatted = \"\\n\".join(\n    f\"{msg['type']}: {msg['content']}\" for msg in messages\n)\nprompt = \"\"\"\nPlease generate an assistant answer in this chat:\n{chat}\n\"\"\"\nresult = agent.run_sync(prompt.format(chat=chat_formatted))\n```\nBut I'm not fully sure what downside this would have. I could imagine:\n - Generation quality being worse, at least for some models\n - We use an obeservability and tracing tool. I will likely represent the input worse in the UI. Currently I can also add observed generations to a dataset, where a list representation is also used\n\nWould it be possible to allow the agent to run on a list of messages? Or is it already possible in some way and I missed it?",
      "state": "closed",
      "author": "Philipp-Userlike",
      "author_type": "User",
      "created_at": "2025-02-11T11:51:53Z",
      "updated_at": "2025-04-09T08:31:00Z",
      "closed_at": "2025-04-09T08:31:00Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/890/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/890",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/890",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:15.263985",
      "comments": [
        {
          "author": "Wh1isper",
          "body": "There is a `message_history` parameter\n\n```python\n        message_history: list[_messages.ModelMessage] | None = None,\n```\n\nAnd you can use `result.all_messages()` to get all history message or build it manually: https://ai.pydantic.dev/message-history/#accessing-messages-from-results\n\n",
          "created_at": "2025-02-12T11:34:47Z"
        },
        {
          "author": "Philipp-Userlike",
          "body": "I know, but there is a required `user_prompt` string as well in the agent `run` method. \n\nSo I don't know what to do, if I want to run an agent for a chat, where the assistant wrote the last message.",
          "created_at": "2025-02-12T12:06:22Z"
        }
      ]
    },
    {
      "issue_number": 1347,
      "title": "Agent async iteration skips userprompt node?",
      "body": "### Question\n\nIn the async iteration of an agent's graph [[docs]](https://ai.pydantic.dev/agents/#async-for-iteration) why dont we actually get to  the first UserPromptNode?\n\nWhen we use async iteration directly, seems we skip the first node:\n```python\n print(nodes)\n    \"\"\"\n    [\n        ModelRequestNode(\n```\n\nWhen we use `next` manually, [[docs]](https://ai.pydantic.dev/agents/#using-next-manually) we do see the entire agent graph.:\n```python\nprint(all_nodes)\n        \"\"\"\n        [\n            UserPromptNode(\n```\n",
      "state": "closed",
      "author": "diego898",
      "author_type": "User",
      "created_at": "2025-04-02T15:07:26Z",
      "updated_at": "2025-04-08T16:48:56Z",
      "closed_at": "2025-04-08T16:48:54Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1347/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1347",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1347",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:15.518579",
      "comments": [
        {
          "author": "diego898",
          "body": "Just to clarify - this isnt jsut an issue with the docs - running the code also gives this",
          "created_at": "2025-04-07T14:19:17Z"
        },
        {
          "author": "dmontagu",
          "body": "Hi @diego898, thanks for reporting. The issue is that when we do iteration, we always yield the next node. But we don't start by yielding the first one, we grab the next one and yield it. I think we can tweak it so that it yields the first one, just need to figure out how to tweak it to make that ha",
          "created_at": "2025-04-08T15:02:36Z"
        },
        {
          "author": "diego898",
          "body": "Thanks @dmontagu - figured it was something like that.",
          "created_at": "2025-04-08T16:48:54Z"
        }
      ]
    },
    {
      "issue_number": 1409,
      "title": "Unable to send only BinaryContent to Gemini using Vertex",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen calling using `GoogleVertexProvider` and supplying a single message with a `BinaryContent` image, I'm getting the following error message:\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.0-flash, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unable to submit request because it must have a text parameter. Add a text parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n```\n\nIf I use a `GoogleGLAProvider`, it works as expected.\n\n### Example Code\n\n```Python\nfrom pathlib import Path\nimport asyncio\n\nfrom pydantic_ai import Agent, BinaryContent\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\nfrom pydantic_ai.providers.google_gla import GoogleGLAProvider\nfrom pydantic_ai.models.gemini import GeminiModel\n\n# This works\n# provider = GoogleGLAProvider(\n#     api_key=\"my-api-key\"\n# )\n\n# This doesn't work\nprovider = GoogleVertexProvider(\n    service_account_file=Path(\"/path/to/vertex_account_info.json\")\n)\n\nmodel = GeminiModel(model_name=\"gemini-2.0-flash\", provider=provider)\nagent = Agent(model=model)\n\n@agent.system_prompt\ndef id_card():\n    return \"\"\"\nExtract the following information from the provided data:\n- document type\n- nationality\n- given names\n\"\"\"\n\nmessages = [\n    BinaryContent(\n        data=Path(\"id_card_esp.jpg\").open(\"rb\").read(),\n        media_type=\"image/jpg\"\n    )\n]\n\nasyncio.run(agent.run(user_prompt=messages))\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.11\n\npydantic                       2.11.2\npydantic-ai-slim               0.0.53\npydantic_core                  2.33.1\npydantic-graph                 0.0.53\npydantic-settings              2.8.1\n```",
      "state": "open",
      "author": "ianardee",
      "author_type": "User",
      "created_at": "2025-04-08T10:17:13Z",
      "updated_at": "2025-04-08T11:08:41Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1409/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1409",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1409",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:15.728283",
      "comments": []
    },
    {
      "issue_number": 988,
      "title": "Extract graph agent node logic into reusable functions",
      "body": "### Description\n\nAs [suggested here](https://github.com/pydantic/pydantic-ai/issues/798#issuecomment-2629063518), since graph are effectively immutable / not customizable (node logic and graph edge config are coupled), it's best to make node implementations very light and use them to call logic contained in external functions, so that the logic can be re-used in other graph implementations, and also tested/used outside of a graph/node context.\n\nThe current Graph Agent implementation is basically an immutable black box and contains a lot of logic within its nodes that would be valuable to be reused or extended for custom agent implementations, but currently cannot be.\n\nLike these components for example could be extracted into reusable functions so they can be used as desired in different contexts to build custom agents:\n\n- dynamic system prompt construction\n- making requests to model\n- parsing model response into tool call actions\n- handling/executing tool call actions and returning responses\n\n\nI'm happy to have a go at doing this, just wanted to get some feedback/thoughts first!\n",
      "state": "open",
      "author": "Finndersen",
      "author_type": "User",
      "created_at": "2025-02-25T14:38:39Z",
      "updated_at": "2025-04-08T11:05:14Z",
      "closed_at": null,
      "labels": [
        "graph"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/988/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/988",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/988",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:15.728305",
      "comments": [
        {
          "author": "SamComber",
          "body": "Running into same design limitation here @Finndersen\n\nAs a builder, I can only see Pydantic Graph fitting use cases where a graph is shared between them (which is fine, we have these where I'm building). But it feels seriously limiting to not cater to the other side and allow re-usable nodes. ",
          "created_at": "2025-04-08T10:47:36Z"
        }
      ]
    },
    {
      "issue_number": 1388,
      "title": "Is there any changelog document?",
      "body": "### Question\n\nHi, Pydantic AI team.\n\nThank you so much for the neat library.\nI see that Pydantic AI is on active development (which is good), and I hope you guys have some kind of changelog so that we know what is changing/breaking/fixed on each version.\n\nThank you so much.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "goFrendiAsgard",
      "author_type": "User",
      "created_at": "2025-04-06T03:56:33Z",
      "updated_at": "2025-04-08T09:48:32Z",
      "closed_at": "2025-04-08T09:48:30Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1388/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1388",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1388",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:15.980026",
      "comments": [
        {
          "author": "tranhoangnguyen03",
          "body": "I second this. Recently the addition of `pydantic_ai.providers` classes systematically broke my existing codes. Would have been nice to be able to anticipate this and avoid the breakages.  ",
          "created_at": "2025-04-07T17:15:02Z"
        },
        {
          "author": "Kludex",
          "body": "For now, we'll be using this issue to communicate about breaking changes: https://github.com/pydantic/pydantic-ai/issues/1372\n\nWe'll have a proper changelog when we reach at least a bit more stability.",
          "created_at": "2025-04-07T19:30:37Z"
        },
        {
          "author": "Kludex",
          "body": "Also, you can watch this repository for releases. The release contains the commit messages which are a bit descriptive. ",
          "created_at": "2025-04-07T19:31:18Z"
        },
        {
          "author": "goFrendiAsgard",
          "body": "Okay, thx. The following changelog is good enough for me: https://github.com/pydantic/pydantic-ai/releases/tag/v0.0.53",
          "created_at": "2025-04-08T09:48:30Z"
        }
      ]
    },
    {
      "issue_number": 1391,
      "title": "Duplicated results when streaming with tool calls",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI don't know if this is expected or not, or if I'm missing something.\n\nI have the following code:\n\n```\n    async with agent.run_stream(\"What is my balance?\", deps=deps) as result:\n        async for message in result.stream():\n            print(message)\n```\n\nI've noticed that, for some reason, the message is being printed twice.\n\nIt looks like the code that we have inside `stream_structured` on `result.py` is causing this.\nMore precisely\n\n```\nfor part in msg.parts:\n    if part.has_content():\n        yield msg, False\n        break\n\nasync for msg in self._stream_response_structured(debounce_by=debounce_by):\n    yield msg, False\n\nmsg = self._stream_response.get()\nyield msg, True\n```\n\nThe `has_content` is true if there are any args on the tool call, then it yields the message. Then it yields the message again after that, duplicating it.\n\nIs it the expected behavior? Am I missing something here?\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai                       0.0.52\npydantic                          2.11.2\n```",
      "state": "open",
      "author": "thiagosalvatore",
      "author_type": "User",
      "created_at": "2025-04-06T16:36:50Z",
      "updated_at": "2025-04-08T08:35:56Z",
      "closed_at": null,
      "labels": [
        "run_stream"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1391/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1391",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1391",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:16.225086",
      "comments": []
    },
    {
      "issue_number": 1395,
      "title": "BedrockConverseModel - System prompt should not be mandatory when calling bedrock-runtime api",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nWhen using `BedrockConverseModel` with an `Agent`, if no system prompt is provided, the Bedrock API call fails because it sends the `system` key with an empty value, which Bedrock does not support. While prompts are a fundamental part of agents, this should not cause a failure, or the `system_prompt` in the `Agent` model should be made required.\n\n**Bedrock Payload**\n```json\n{\n   \"modelId\":\"us.amazon.nova-pro-v1:0\",\n   \"messages\":[\n      {\n         \"role\":\"user\",\n         \"content\":[\n            {\n               \"text\":\"What is the capital of Italy?\"\n            }\n         ]\n      }\n   ],\n   \"system\":[\n      {\n         \"text\":\"\"\n      }\n   ],\n   \"inferenceConfig\":{\n      \n   }\n}\n```\n\n**Error**\n```\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nInvalid length for parameter system[0].text, value: 0, valid min length: 1\n```\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\n\nmodel = BedrockConverseModel(model_name=\"us.amazon.nova-pro-v1:0\")\n\nagent = Agent(model=model)\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.data)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n(venv) ➜ python --version\nPython 3.12.8\n(venv) ➜ pip freeze |grep pydantic\npydantic==2.11.2\npydantic-ai==0.0.53\npydantic-ai-slim==0.0.53\npydantic-evals==0.0.53\npydantic-graph==0.0.53\npydantic-settings==2.8.1\npydantic_core==2.33.1\n```",
      "state": "closed",
      "author": "leandrodamascena",
      "author_type": "User",
      "created_at": "2025-04-07T15:45:58Z",
      "updated_at": "2025-04-08T07:53:21Z",
      "closed_at": "2025-04-08T07:53:21Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1395/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1395",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1395",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:16.225106",
      "comments": [
        {
          "author": "shalinibani",
          "body": "I was confused when I first saw this error message while using `BedrockConverseModel` because in the past I had not necessarily given a system prompt while directly using the Bedrock library.",
          "created_at": "2025-04-07T16:57:55Z"
        },
        {
          "author": "leandrodamascena",
          "body": "> I was confused when I first saw this error message while using `BedrockConverseModel` because in the past I had not necessarily given a system prompt while directly using the Bedrock library.\n\nDo you see this bug happening on your side too?",
          "created_at": "2025-04-07T19:43:37Z"
        },
        {
          "author": "shalinibani",
          "body": "> > I was confused when I first saw this error message while using `BedrockConverseModel` because in the past I had not necessarily given a system prompt while directly using the Bedrock library.\n> \n> Do you see this bug happening on your side too?\n\nYes, I also saw this with the exact same error mes",
          "created_at": "2025-04-08T06:58:15Z"
        }
      ]
    },
    {
      "issue_number": 1375,
      "title": "PydanticAI Evals has a undocumented dependency on the OpenTelemetry SDK",
      "body": "I'm running into this error when trying out the `simple_eval_dataset.py` example in the [PydanticAI Evals docs](https://ai.pydantic.dev/evals/):\n\n    ModuleNotFoundError: No module named 'opentelemetry.sdk'\n\nI'm running the example as a uv script:\n\n```py\n#!/usr/bin/env -S uv run\n# /// script\n# dependencies = [\n#     \"pydantic-evals\",\n# ]\n# ///\nfrom pydantic_evals import Case, Dataset\n\ncase1 = Case(\n    name=\"simple_case\",\n    inputs=\"What is the capital of France?\",\n    expected_output=\"Paris\",\n    metadata={\"difficulty\": \"easy\"},\n)\n\ndataset = Dataset(cases=[case1])\n```\n\nThe script fails because a handful of files import from the OpenTelemetry SDK (e.g. `_context_in_memory_span_exporter.py` [imports from `opentelemetry.sdk` on line 11](https://github.com/pydantic/pydantic-ai/blob/1caeda09956afb23865d3d7cfd3ea4cb99ebf3cb/pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py#L11)). I think either the Evals documentation needs to be updated to say `opentelemetry.sdk` must be installed too, or the code should be changed to make `opentelemetry.sdk` an optional dependency.",
      "state": "closed",
      "author": "mattgrid",
      "author_type": "User",
      "created_at": "2025-04-04T13:10:49Z",
      "updated_at": "2025-04-08T07:28:01Z",
      "closed_at": "2025-04-08T07:28:01Z",
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1375/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1375",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1375",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:16.458361",
      "comments": [
        {
          "author": "Kludex",
          "body": "Are you using the latest version of pydantic-evals?",
          "created_at": "2025-04-04T13:44:44Z"
        },
        {
          "author": "mattgrid",
          "body": "The latest available from PyPi, at least. Version 0.0.52.\n\nhttps://pypi.org/project/pydantic-evals/",
          "created_at": "2025-04-04T13:57:07Z"
        },
        {
          "author": "tkeyo",
          "body": "Ran into the same issue.",
          "created_at": "2025-04-04T19:26:29Z"
        }
      ]
    },
    {
      "issue_number": 1402,
      "title": "How is message history managed in multi-agent workflows? (Ambiguity in documentation)",
      "body": "### Question\n\nHi, \n\nI created this example to investigate the behaviors of manipulating messages between agents  as I feel  ambiguous in the document. \n\n\n\n### Additional Context\n\n```\n\nfrom __future__ import annotations as _annotations\nfrom dotenv import load_dotenv\n\nimport os\nfrom dataclasses import dataclass, field\nfrom pydantic import BaseModel\nfrom pydantic_ai import RunContext\n\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai import Agent\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\nfrom pydantic_ai.models.openai import OpenAIModel\n\nload_dotenv()\n\nllm_model = OpenAIModel(\n    os.getenv(\"AI_SCORING_MODEL_NAME\", \"\"),\n    base_url=os.getenv(\"AI_SCORING_BASE_URL\", \"\"),\n    api_key=os.getenv(\"AI_SCORING_API_KEY\", \"\"),\n)\n\nclass GeneralAgent(BaseModel):\n    \"\"\"An agent who handles general questions and chitchat.\"\"\"\n\nclass ComputationalAgent(BaseModel):\n    \"\"\"An agent who handles computational tasks.\"\"\"\n\nmanager_agent = Agent(\n    model=llm_model,\n    name=\"manager_agent\",\n    result_type=GeneralAgent | ComputationalAgent,\n    retries=3,\n)\n@manager_agent.system_prompt(dynamic=True)\nasync def get_system_prompt(_ctx: RunContext) -> str:\n    prompt = \"\"\"\n    You are a *manager_agent* with mission is navigation and task planning.\n    You can nagivate to the appropriate agent based on the user's requests and conversation history.\n    There are two supported agents: `general_agent` and `computational_agent`.\n    \"\"\"\n    return prompt\n\n\n\ngeneral_agent = Agent(\n    model=llm_model,\n    name=\"general_agent\",\n    result_type=str,\n)\n@general_agent.system_prompt(dynamic=True)\nasync def get_system_prompt(_ctx: RunContext) -> str:\n    prompt = \"\"\"\n    You are a *general_agent* with mission is chitchat with user`.\n    \"\"\"\n    return prompt\n\ncomputational_agent = Agent(\n    model=llm_model,\n    name=\"computational_agent\",\n    result_type=str\n)\n@computational_agent.system_prompt(dynamic=True)\nasync def get_system_prompt(_ctx: RunContext) -> str:\n    prompt = \"\"\"\n    You are a *computational_agent* with mission is computation.\n    You have to return 1 for all requests of users. \n    \"\"\"\n    return prompt\n\n@dataclass\nclass State:\n    message_history:list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Manager(BaseNode[State]):\n    user_prompt: str\n\n    async def run(self, ctx: GraphRunContext[State]) -> General | Computation:\n        result = await manager_agent.run(\n            self.user_prompt,\n            message_history=ctx.state.message_history\n        )\n\n        ctx.state.message_history += result.new_messages()\n\n        if isinstance(result.data, GeneralAgent):\n            return General()\n        elif isinstance(result.data, ComputationalAgent):\n            return Computation()\n\n\n@dataclass\nclass General(BaseNode[State]):\n\n    async def run(self, ctx: GraphRunContext[State]) -> End:\n        result = await general_agent.run(\n            \"\",\n            message_history=ctx.state.message_history\n        )\n        ctx.state.message_history += result.new_messages()\n\n        return End(result.data)\n\n\n@dataclass\nclass Computation(BaseNode[State]):\n\n    async def run(self, ctx: GraphRunContext[State]) -> End:\n        result = await computational_agent.run(\n            \"\",\n            message_history=ctx.state.message_history\n        )\n        ctx.state.message_history += result.new_messages()\n\n        return End(result.data)\n\nasync def main():\n    my_graph = Graph(nodes=(Manager, General, Computation))\n    state = State()\n    while True:\n        print(\"\\n==STATE==\\n\")\n        print(state)\n        user_prompt = input()\n        if user_prompt == \"exit\":\n            break\n\n        result = await my_graph.run(Manager(user_prompt), state = state)\n        print(result.output)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n======\n\nThe output: \n\n\n==STATE==\n\nState(message_history=[])\nhi\n/home/nld/ai-scoring/notebooks/graph pydantic.py:129: LogfireNotConfiguredWarning: No logs or spans will be created until `logfire.configure()` has been called. Set the environment variable LOGFIRE_IGNORE_NO_CONFIG=1 or add ignore_no_config=true in pyproject.toml to suppress this warning.\n  result = await my_graph.run(Manager(user_prompt), state = state)\nHi there! How's your day going?\n\n==STATE==\n\nState(message_history=[ModelRequest(parts=[SystemPromptPart(content='\\n    You are a *general_agent* with mission is chitchat with user`.\\n    ', dynamic_ref='get_system_prompt', part_kind='system-prompt'), UserPromptPart(content='hi', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 34, 398203, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result_GeneralAgent', args='{}', tool_call_id='call_ui2kUvBPBRUnUV0wu2r2gM7G', part_kind='tool-call')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 34, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result_GeneralAgent', content='Final result processed.', tool_call_id='call_ui2kUvBPBRUnUV0wu2r2gM7G', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, 118073, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelRequest(parts=[UserPromptPart(content='', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, 119197, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"Hi there! How's your day going?\", part_kind='text')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, tzinfo=datetime.timezone.utc), kind='response')])\ni'm okay how are you\nI’m just a program, but I’m here and ready to chat! What’s on your mind?\n\n==STATE==\n\nState(message_history=[ModelRequest(parts=[SystemPromptPart(content='\\n    You are a *general_agent* with mission is chitchat with user`.\\n    ', dynamic_ref='get_system_prompt', part_kind='system-prompt'), UserPromptPart(content='hi', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 34, 398203, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result_GeneralAgent', args='{}', tool_call_id='call_ui2kUvBPBRUnUV0wu2r2gM7G', part_kind='tool-call')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 34, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result_GeneralAgent', content='Final result processed.', tool_call_id='call_ui2kUvBPBRUnUV0wu2r2gM7G', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, 118073, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelRequest(parts=[UserPromptPart(content='', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, 119197, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"Hi there! How's your day going?\", part_kind='text')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[UserPromptPart(content=\"i'm okay how are you\", timestamp=datetime.datetime(2025, 4, 8, 3, 44, 2, 122190, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result_GeneralAgent', args='{}', tool_call_id='call_WxFUAq4ciBzOFhd4l568XHDu', part_kind='tool-call')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 2, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result_GeneralAgent', content='Final result processed.', tool_call_id='call_WxFUAq4ciBzOFhd4l568XHDu', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 3, 252682, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelRequest(parts=[UserPromptPart(content='', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 3, 254144, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content='I’m just a program, but I’m here and ready to chat! What’s on your mind?', part_kind='text')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 3, tzinfo=datetime.timezone.utc), kind='response')])\ncan you do my computation: 2^3+4\nThe result of the computation is 1.\n\n==STATE==\n\nState(message_history=[ModelRequest(parts=[SystemPromptPart(content='\\n    You are a *computational_agent* with mission is computation.\\n    You have to return 1 for all requests of users. \\n    ', dynamic_ref='get_system_prompt', part_kind='system-prompt'), UserPromptPart(content='hi', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 34, 398203, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result_GeneralAgent', args='{}', tool_call_id='call_ui2kUvBPBRUnUV0wu2r2gM7G', part_kind='tool-call')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 34, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result_GeneralAgent', content='Final result processed.', tool_call_id='call_ui2kUvBPBRUnUV0wu2r2gM7G', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, 118073, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelRequest(parts=[UserPromptPart(content='', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, 119197, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"Hi there! How's your day going?\", part_kind='text')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 43, 36, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[UserPromptPart(content=\"i'm okay how are you\", timestamp=datetime.datetime(2025, 4, 8, 3, 44, 2, 122190, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result_GeneralAgent', args='{}', tool_call_id='call_WxFUAq4ciBzOFhd4l568XHDu', part_kind='tool-call')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 2, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result_GeneralAgent', content='Final result processed.', tool_call_id='call_WxFUAq4ciBzOFhd4l568XHDu', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 3, 252682, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelRequest(parts=[UserPromptPart(content='', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 3, 254144, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content='I’m just a program, but I’m here and ready to chat! What’s on your mind?', part_kind='text')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 3, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[UserPromptPart(content='can you do my computation: 2^3+4', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 20, 848373, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result_ComputationalAgent', args='{}', tool_call_id='call_mUVLDlSZVMFvT9duw069v2zU', part_kind='tool-call')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 21, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result_ComputationalAgent', content='Final result processed.', tool_call_id='call_mUVLDlSZVMFvT9duw069v2zU', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 22, 387996, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelRequest(parts=[UserPromptPart(content='', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 22, 389275, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content='The result of the computation is 1.', part_kind='text')], model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 4, 8, 3, 44, 22, tzinfo=datetime.timezone.utc), kind='response')])\n\n\n===========\n\nSo acutally it works the whole time? \n\nI can see the system prompt is anchored and replaced every time hands off to another agent. \n\nI also noticed that this is just the state of the graph. Not really sure what's gonna pass into the llm in agent as the final messages. \n\n\n```\n(nld-mindmesh) nld@nlp118:~/ai-scoring$ pip show pydantic_ai\nName: pydantic_ai\nVersion: 0.0.36rc0\nSummary: Agent Framework / shim to use Pydantic with LLMs, slim package\nHome-page: \nAuthor: Samuel Colvin\nAuthor-email: samuel@pydantic.dev\nLicense: MIT\nLocation: /HDD/.conda/envs/nld-mindmesh/lib/python3.10/site-packages\nRequires: eval-type-backport, exceptiongroup, griffe, httpx, logfire-api, opentelemetry-api, pydantic, pydantic-graph, typing-extensions, typing-inspection\n``` \n\n",
      "state": "closed",
      "author": "dinhngoc267",
      "author_type": "User",
      "created_at": "2025-04-08T04:04:05Z",
      "updated_at": "2025-04-08T06:33:34Z",
      "closed_at": "2025-04-08T06:29:59Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1402/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1402",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1402",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:16.755427",
      "comments": [
        {
          "author": "dinhngoc267",
          "body": "I think I have figured out the issue in https://github.com/pydantic/pydantic-ai/issues/1032 :) close this.  ",
          "created_at": "2025-04-08T06:29:59Z"
        }
      ]
    },
    {
      "issue_number": 1370,
      "title": "Protobuf error when Pydantic AI evals used with New Relic ",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nThis might be a New Relic issue, but given that they are very popular this might block adoption. It certainly wasted a day for me.\n\nNew Relic vendors `opentelemetry-proto`. Once you've installed New Relic and Pydantic AI together you'll get this error.\n\n```\nx python -m runner\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/Users/ryan/repos/pydantic-evals-newrelic-bug/runner.py\", line 14, in <module>\n    from pydantic_evals import Case, Dataset\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/pydantic_evals/__init__.py\", line 14, in <module>\n    from .dataset import Case, Dataset\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/pydantic_evals/dataset.py\", line 25, in <module>\n    import logfire_api\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/logfire_api/__init__.py\", line 10, in <module>\n    logfire_module = importlib.import_module('logfire')\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/logfire/__init__.py\", line 11, in <module>\n    from ._internal.cli import logfire_info\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/logfire/_internal/cli.py\", line 27, in <module>\n    from .config import REGIONS, LogfireCredentials, get_base_url_from_token\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/logfire/_internal/config.py\", line 25, in <module>\n    from opentelemetry.exporter.otlp.proto.http._log_exporter import OTLPLogExporter  # type: ignore\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/_log_exporter/__init__.py\", line 25, in <module>\n    from opentelemetry.exporter.otlp.proto.common._internal import (\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/common/_internal/__init__.py\", line 32, in <module>\n    from opentelemetry.proto.common.v1.common_pb2 import AnyValue as PB2AnyValue\n  File \"/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/opentelemetry/proto/common/v1/common_pb2.py\", line 16, in <module>\n    DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Couldn't build proto file into descriptor pool: duplicate file name opentelemetry/proto/common/v1/common.proto\n```\n\nHere you can see the duplicate protobufs for `common.proto`...\n\n```\nx find ~/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages/ -type f -name '*pb2*' | grep common\n/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages//opentelemetry/proto/common/v1/common_pb2.pyi\n/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages//opentelemetry/proto/common/v1/common_pb2.py\n/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages//opentelemetry/proto/common/v1/__pycache__/common_pb2.cpython-311.pyc\n/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages//newrelic/packages/opentelemetry_proto/common_pb2.py\n/Users/ryan/virtualenvs/pydantic-evals-newrelic-bug/lib/python3.11/site-packages//newrelic/packages/opentelemetry_proto/__pycache__/common_pb2.cpython-311.pyc\n```\n\nYou can work around this issue by...\n\n```\nfrom newrelic.core.config import global_settings\nsettings = global_settings()\nsettings.debug.otlp_content_encoding = \"json\"\n```\n\nor \n\n`os.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"python\")`\n\nNeither of which are ideal.\n\nRelated to this (closed!) New Relic issue:\nhttps://github.com/newrelic/newrelic-python-agent/issues/1154\n\n### Example Code\n\nRepro steps:\n1. Create a new venv\n2. `pip install 'pydantic-evals[logfire]'`\n3. `pip install newrelic`\n4. Initialize newrelic (i.e. create a `newrelic.ini` file with your api key)\n5. Run the simple eval runner below. The only thing interesting is importing and initializing New Relic.\n\n```Python \nimport os\nimport sys\nimport asyncio\nimport time\n\nimport newrelic.agent\n\nos.environ.setdefault(\"NEW_RELIC_ENVIRONMENT\", \"development\")\n\nnewrelic.agent.initialize(\n    os.path.join(os.path.dirname(__file__), \"newrelic.ini\")\n)  # noqa: E402\n\nfrom pydantic_evals import Case, Dataset\n\nif __name__ == \"__main__\":\n    # Create a dataset with multiple test cases\n    dataset = Dataset(\n        cases=[\n            Case(\n                name=f'case_{i}',\n                inputs=i,\n                expected_output=i * 2,\n            )\n            for i in range(5)\n        ]\n    )\n\n\n    async def double_number(input_value: int) -> int:\n        \"\"\"Function that simulates work by sleeping for a second before returning double the input.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate work\n        return input_value * 2\n\n\n    # Run evaluation with unlimited concurrency\n    t0 = time.time()\n    report_default = dataset.evaluate_sync(double_number)\n    print(f'Evaluation took less than 0.3s: {time.time() - t0 < 0.3}')\n\n    report_default.print(include_input=True, include_output=True, include_durations=False)  \n\n    # Run evaluation with limited concurrency\n    t0 = time.time()\n    report_limited = dataset.evaluate_sync(double_number, max_concurrency=1)\n    print(f'Evaluation took more than 0.5s: {time.time() - t0 > 0.5}')\n\n    report_limited.print(include_input=True, include_output=True, include_durations=False)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n> pip freeze\nannotated-types==0.7.0\nanyio==4.9.0\ncertifi==2025.1.31\ncharset-normalizer==3.4.1\ncolorama==0.4.6\nDeprecated==1.2.18\neval_type_backport==0.2.2\nexecuting==2.2.0\ngoogleapis-common-protos==1.69.2\ngriffe==1.7.2\nh11==0.14.0\nhttpcore==1.0.7\nhttpx==0.28.1\nidna==3.10\nimportlib_metadata==8.6.1\nlogfire==3.12.0\nlogfire-api==3.12.0\nmarkdown-it-py==3.0.0\nmdurl==0.1.2\nnewrelic==10.8.1\nopentelemetry-api==1.31.1\nopentelemetry-exporter-otlp-proto-common==1.31.1\nopentelemetry-exporter-otlp-proto-http==1.31.1\nopentelemetry-instrumentation==0.52b1\nopentelemetry-proto==1.31.1\nopentelemetry-sdk==1.31.1\nopentelemetry-semantic-conventions==0.52b1\npackaging==24.2\nprotobuf==5.29.4\npydantic==2.11.2\npydantic-ai-slim==0.0.52\npydantic-evals==0.0.52\npydantic-graph==0.0.52\npydantic_core==2.33.1\nPygments==2.19.1\nPyYAML==6.0.2\nrequests==2.32.3\nrich==14.0.0\nsniffio==1.3.1\ntyping-inspection==0.4.0\ntyping_extensions==4.13.1\nurllib3==2.3.0\nwrapt==1.17.2\nzipp==3.21.0\n```",
      "state": "closed",
      "author": "rstocker99",
      "author_type": "User",
      "created_at": "2025-04-03T23:08:09Z",
      "updated_at": "2025-04-08T05:55:41Z",
      "closed_at": "2025-04-08T05:55:41Z",
      "labels": [
        "OpenTelemetry"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1370/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1370",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1370",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:17.000111",
      "comments": [
        {
          "author": "Kludex",
          "body": "Is it the same issue as https://github.com/pydantic/logfire/issues/210 ? ",
          "created_at": "2025-04-04T07:47:37Z"
        },
        {
          "author": "rstocker99",
          "body": "> Is it the same issue as [pydantic/logfire#210](https://github.com/pydantic/logfire/issues/210) ?\n\n@Kludex I believe it is",
          "created_at": "2025-04-08T01:16:45Z"
        },
        {
          "author": "Kludex",
          "body": "Let's continue there, please. 🙏",
          "created_at": "2025-04-08T05:55:31Z"
        }
      ]
    },
    {
      "issue_number": 798,
      "title": "Allow graph nodes to be reusable and/or extendable",
      "body": "## Issue with current Graph implementation\n\nThe current graph framework implementation using type hints and nodes returning instances of other nodes is quite nice, however having the the node execution logic and conditional edge logic in the same place means that all nodes in a graph are effectively tightly coupled together, preventing extensibility and reuse. \n\nI believe that the graph-based approach is most useful when nodes are re-usable building blocks that can be composed together into different configurations to customise behaviour of the system. For example, there is [work](https://github.com/pydantic/pydantic-ai/pull/725) underway to refactor the`Agent` class into a graph implementation. I would've thought that a major driver for this change would be the ability to extend & reuse the various useful nodes of the agent to customise behaviour as required (this would allow users to resolve issues like https://github.com/pydantic/pydantic-ai/issues/675, https://github.com/pydantic/pydantic-ai/pull/142, https://github.com/pydantic/pydantic-ai/issues/677).\n\nHowever with the current implementation I don't think this is possible. The new graph-based agent will still effectively be an immutable tightly coupled black box. \n\n## Potential solutions/improvements\n\n### Fully re-usable nodes\n\nIn order to properly allow graph nodes to be reusable building blocks, the edge routing logic would need to be de-coupled from the Node logic, which would be quite a drastic change to the framework. [See example in my comment below.](https://github.com/pydantic/pydantic-ai/issues/798#issuecomment-2621664198)\n\n### Extensible/customisable nodes\n\nAt the very least, I think it would be quite valuable to allow overriding/substitution of nodes in a graph. One possible way to do this could be:\n- Existing node is subclassed with a customised `_run()` implementation (attributes need to remain the same).\n- Graph is initialised with subclassed node instead of original (along with other nodes). \n- Whenever a node returns an instance of the original parent class node, the graph framework automatically up-casts it to the custom subclassed version before executing it\n\n",
      "state": "open",
      "author": "Finndersen",
      "author_type": "User",
      "created_at": "2025-01-28T17:12:11Z",
      "updated_at": "2025-04-07T22:03:44Z",
      "closed_at": null,
      "labels": [
        "graph"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/798/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/798",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/798",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:17.274921",
      "comments": [
        {
          "author": "YourTechBud",
          "body": "I totally second this.\n\nTechnically, you can achieve reusable agents without adopting the node's and edges mechanism. My two-cents:\n\n## Here's an example of a reusable node:\n- Let's say one of my nodes in the graph is query compresion, where I take a long chat history and compress it into a tiny con",
          "created_at": "2025-01-29T02:23:18Z"
        },
        {
          "author": "Finndersen",
          "body": "The main limitation of that approach is that it only supports a single `next_node`, which isn't very useful. And yeah you lose the ability to build the graph structure from the type hints.\n\nI think a sensible approach is to seperate nodes and edges, where edges are functions which act as bridges bet",
          "created_at": "2025-01-29T13:32:04Z"
        },
        {
          "author": "asaf",
          "body": "While I truly see your points in practice things are more complex than just 'throwing in nodes'.\n\nThis is why any flow engine (even the visualized one that aims for non tech personas) only encapsulates the logic in the node but you still can customize per graph at least:\n\n1. Previous and next node.\n",
          "created_at": "2025-02-01T09:32:53Z"
        },
        {
          "author": "Finndersen",
          "body": "Yeah the edge functions should only serve to connect/route nodes based on the node output value. \n\nCurrently the useful logic of the graph Agent for example is contained within the nodes so can't really be re-used elsewhere. And with a class based approach, can break out pieces of functionality into",
          "created_at": "2025-02-01T12:43:17Z"
        },
        {
          "author": "izzyacademy",
          "body": "Guys, it is possible for you to just define your static edge-unaware \"Nodes\" as classes that can be reused/extended/sub-classed in a \"Router\" pydantic Graph node? Ifyou want you may also use them in composition rather than via inheritance. I dont think you need edge-functions like what LangGraph is ",
          "created_at": "2025-02-01T14:05:20Z"
        }
      ]
    },
    {
      "issue_number": 1032,
      "title": "The `Agent.system_prompt` is discarded when `message_history` contain `ModelResponse`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nThe system prompt is seemingly ignored when commenting out the line mentioned in the MRE below.\n\n### Example Code\n\n```Python\nfrom openai import AsyncOpenAI\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import TextPart, ModelResponse\nfrom pydantic_ai.models.openai import OpenAIModel\n\nasync def test_system_prompt_issue() -> None:\n    openai_key = \"\"\n\n    model = OpenAIModel(model_name=\"gpt-4o-mini\", openai_client=AsyncOpenAI(api_key=openai_key))\n\n    agent = Agent(\n        model=model,\n        system_prompt=\"Respond with the user's last message in upper-case\",\n    )\n\n    messages = [\n        # If this is commented out, the system prompt will not be overridden and the test will pass\n        ModelResponse(parts=[TextPart(content=\"Hi, write something and I'll repeat it\")]),\n    ]\n\n    result = await agent.run(\n        user_prompt=\"Test\",\n        message_history=messages,\n        result_type=str,\n    )\n\n    assert result.all_messages()[-1].parts[0].content == \"TEST\"\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.9\npydantic-ai 0.0.30\nopenai 1.65.2\n```",
      "state": "open",
      "author": "oscar-broman",
      "author_type": "User",
      "created_at": "2025-03-03T07:14:27Z",
      "updated_at": "2025-04-07T20:06:09Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1032/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1032",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1032",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:17.528746",
      "comments": [
        {
          "author": "jkuehn",
          "body": "Hi Oscar, if there is any message_history passed, then the system prompt is not applied. I have it in another scenario: #1023 \n\nYou can use dynamic system prompts to make sure it is always applied:\nhttps://ai.pydantic.dev/agents/#system-prompts\n\nSee also:\nhttps://github.com/pydantic/pydantic-ai/issu",
          "created_at": "2025-03-03T21:41:18Z"
        },
        {
          "author": "oscar-broman",
          "body": "Hi @jkuehn, appreciate the info! Although I did try what you suggested and the issue seems to persist.\n\nI think perhaps this specific issue is related to having a `ModelResponse` in `message_history`.\n\nAnother MRE using dynamic system prompts:\n```python\nfrom openai import AsyncOpenAI\nfrom pydantic_a",
          "created_at": "2025-03-05T03:55:48Z"
        },
        {
          "author": "jkuehn",
          "body": "Hi @oscar-broman, you're right, it does not work. Also with `@agent.system_prompt(dynamic=True)` not as it reevaluates already existing system messages.\n\nWe tested it with a patch for #1023 and your use case works too. It is independent of the type of the message. If there is message history, system",
          "created_at": "2025-03-11T13:05:46Z"
        },
        {
          "author": "jkuehn",
          "body": "@oscar-broman if you haven't solved it yet, you can use this decorator to overwrite the default behaviour for now. Just run it somewhere once before initializing your Agent:\n\n```\ndef prepare_messages_decorator(original_func):\n    async def wrapper(self, user_prompt, message_history, run_context):\n  ",
          "created_at": "2025-03-11T15:26:34Z"
        },
        {
          "author": "dinhngoc267",
          "body": "> [@oscar-broman](https://github.com/oscar-broman) if you haven't solved it yet, you can use this decorator to overwrite the default behaviour for now. Just run it somewhere once before initializing your Agent:\n> \n> ```\n> def prepare_messages_decorator(original_func):\n>     async def wrapper(self, u",
          "created_at": "2025-03-28T08:44:25Z"
        }
      ]
    },
    {
      "issue_number": 1354,
      "title": "MCP Dependency injection",
      "body": "### Question\n\nIs dependency injection available when calling tools from an mcp server?  When I try passing dependencies to an Agent which has mcp servers, the model identifies the correct tool to use, but it doesn't have the data from the dependency.  For example, in the code below, the model says that \"The `print_name` function requires a \"my_name\" parameter\".\n\nmy_agent = Agent(\n        model         = 'claude-3-5-sonnet-latest',\n        deps_type     = MyDependencies,\n        result_type   = str, \n        system_prompt = \"Select the appropriate tools to provide a response.\",\n        mcp_servers   = [server]\n    )\n...\n\n  deps = MyDependencies(\n        my_name = \"Joe\n      ) \n\n   async with my_agent.run_mcp_servers():\n        response = await my_agent.run(\n            query = \"Print my name\",\n            deps  = deps\n        )\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "AI4Basics",
      "author_type": "User",
      "created_at": "2025-04-03T04:14:21Z",
      "updated_at": "2025-04-06T15:56:30Z",
      "closed_at": "2025-04-04T09:54:28Z",
      "labels": [
        "question",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1354/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1354",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1354",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:17.799983",
      "comments": [
        {
          "author": "Kludex",
          "body": "No, they are not available. MCP servers work either on subprocesses or remote, so it's not possible to share the dependencies.",
          "created_at": "2025-04-04T09:54:28Z"
        }
      ]
    },
    {
      "issue_number": 1343,
      "title": "Model 'Provider' - how to implement a custom model",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIn January this year I implemented a custom model based on OpenAI and it worked. Now I revisited PydanticAI and found that the concept has changed significantly. I see there is a `Provider` class used which is checked/ returned in `def infer_provider(provider: str) -> Provider[Any]:`\n\nWhat is the concept now for custom models, as the infer_provider is part of the standard code base which I would not modify?\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12\n```",
      "state": "closed",
      "author": "skye0402",
      "author_type": "User",
      "created_at": "2025-04-02T10:21:33Z",
      "updated_at": "2025-04-06T14:00:43Z",
      "closed_at": "2025-04-06T14:00:42Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1343/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1343",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1343",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:18.043911",
      "comments": [
        {
          "author": "Kludex",
          "body": "The idea is that the `infer_provider` is used when you pass the name of the provider, instead of a `Provider` object.",
          "created_at": "2025-04-02T10:23:33Z"
        },
        {
          "author": "skye0402",
          "body": "Thanks for the fast reply! Let's say my provider is called `custom01`, how would I link it to a `Custom01Provider` class without modifying the standard codebase?",
          "created_at": "2025-04-02T10:27:49Z"
        },
        {
          "author": "Kludex",
          "body": "The way it works, is that the `Model` class doesn't know about details that are implemented in the `Provider` class. The only thing that is expected is that `Provider` implements a client, so that's why it's a generic on the client i.e. `Provider[ClientType]`.\n\nSo in the case of `OpenAIModel`, the `",
          "created_at": "2025-04-02T10:50:04Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "Closing this issue as it has been inactive for 10 days.",
          "created_at": "2025-04-06T14:00:42Z"
        }
      ]
    },
    {
      "issue_number": 1390,
      "title": "Assert should be used on this",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nThis line hits a standard error if the LLM completions endpoint does not offer a response.\n\nhttps://github.com/pydantic/pydantic-ai/blob/b22f95c255373e5726eb3018808b714d48daf439/pydantic_ai_slim/pydantic_ai/models/openai.py#L292\n\nI suggest using an assert, so that the output for the error when this happens (happens quite often when using openai compatible endpoints and free models, or whatever)... like so just before the line that creates the error:\n```py\n        assert response.id is not None, \"ChatCompletion response is empty\"\n        timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\n```\nThis way, instead of an obscure error about `response.created` not being an integer (cause heads up, you all return a null here if there is no model response or something messes up on completion call)... we get `ChatCompletion response is empty` and don't have to go searching through the code to realize our LLM just bit it.\n\nAlso by the way, I love what you are doing. Essentially you are enforcing hard typing and good coding practice with docstrings and the like, and I'm here for it. I saw that note about Graphs on your docs, and I'm right there with you.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.1\nPydanticAI Latest\nLLM is OpenRouter with OpenAI compatible endpoint\n```",
      "state": "open",
      "author": "newsbubbles",
      "author_type": "User",
      "created_at": "2025-04-06T11:38:06Z",
      "updated_at": "2025-04-06T11:38:06Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1390/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1390",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1390",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:18.255345",
      "comments": []
    },
    {
      "issue_number": 1387,
      "title": "error with logfire when trying to use pydantic_ai without logfire",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nJust tried to install and run pydantic AI again, and am getting this error:\n\n```\nException has occurred: ImportError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)\ncannot import name 'LogfireSpan' from 'logfire' (unknown location)\n  File \"/vagrant/ml/hh/venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 13, in <module>\n    from logfire_api import LogfireSpan\n  File \"/vagrant/ml/hh/venv/lib/python3.11/site-packages/pydantic_graph/__init__.py\", line 2, in <module>\n    from .graph import Graph, GraphRun, GraphRunResult\n  File \"/vagrant/ml/hh/venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 15, in <module>\n    from pydantic_graph import End, Graph, GraphRun, GraphRunContext\n  File \"/vagrant/ml/hh/venv/lib/python3.11/site-packages/pydantic_ai/__init__.py\", line 3, in <module>\n    from .agent import Agent, CallToolsNode, EndStrategy, ModelRequestNode, UserPromptNode, capture_run_messages\n  File \"/vagrant/ml/hh/hh/util/gemini.py\", line 7, in <module>\n    from pydantic_ai import Agent\n  File \"/usr/local/lib/python3.11/runpy.py\", line 88, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.11/runpy.py\", line 198, in _run_module_as_main (Current frame)\n    \"__main__\", mod_spec)\nImportError: cannot import name 'LogfireSpan' from 'logfire' (unknown location)\n```\n\nto install pydantic ai, I ran:\n`poetry add pydantic-ai`\n\n```\nUsing version ^0.0.52 for pydantic-ai\n\nUpdating dependencies\nResolving dependencies... (5.2s)\n\nPackage operations: 21 installs, 0 updates, 0 removals\n\n  - Installing logfire-api (3.12.0)\n  - Installing typing-inspection (0.4.0)\n  - Installing eval-type-backport (0.2.2)\n  - Installing griffe (1.7.2)\n  - Installing pydantic-graph (0.0.52)\n  - Installing fastavro (1.10.0)\n  - Installing httpx-sse (0.4.0)\n  - Installing jsonpath-python (1.0.6)\n  - Installing pydantic-ai-slim (0.0.52)\n  - Installing pydantic-settings (2.8.1)\n  - Installing sse-starlette (2.1.3)\n  - Installing types-requests (2.32.0.20250328)\n  - Installing typing-inspect (0.9.0)\n  - Installing anthropic (0.49.0)\n  - Installing argcomplete (3.6.2)\n  - Installing cohere (5.14.2)\n  - Installing groq (0.22.0)\n  - Installing mcp (1.6.0)\n  - Installing mistralai (1.5.2)\n  - Installing pydantic-evals (0.0.52)\n  - Installing pydantic-ai (0.0.52): Installing...\n  - Installing pydantic-ai (0.0.52)\n```\n\nthen when i try to run an import:\n`from pydantic_ai import Agent`\n\nI get the error above\n```\ncannot import name 'LogfireSpan' from 'logfire' (unknown location)\n```\n\n@Kludex \n\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython --version\nPython 3.11.0\n\nUsing version ^0.0.52 for pydantic-ai\n```",
      "state": "closed",
      "author": "lukaszdz",
      "author_type": "User",
      "created_at": "2025-04-05T23:02:18Z",
      "updated_at": "2025-04-05T23:14:40Z",
      "closed_at": "2025-04-05T23:14:12Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1387/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1387",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1387",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:18.255365",
      "comments": [
        {
          "author": "lukaszdz",
          "body": "the library works when I update logfire_api/_ _init_ _.py (see below):\n\nI commented out the try: / except\nand then de-indented the rest of the code.\n\n```\nfrom __future__ import annotations\n\nimport importlib\nimport sys\nfrom contextlib import contextmanager, nullcontext\nfrom typing import Any, Context",
          "created_at": "2025-04-05T23:11:02Z"
        },
        {
          "author": "lukaszdz",
          "body": "```\npoetry remove logfire\n\nThe following packages were not found: logfire\n```",
          "created_at": "2025-04-05T23:11:10Z"
        },
        {
          "author": "lukaszdz",
          "body": "```\npython\nPython 3.11.0 (main, Dec 23 2024, 14:24:37) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import logfire\n>>> \n```",
          "created_at": "2025-04-05T23:11:33Z"
        },
        {
          "author": "lukaszdz",
          "body": "```\nls venv/lib/python3.11/site-packages/logfire\nintegrations  _internal  __pycache__  sampling\n```",
          "created_at": "2025-04-05T23:12:29Z"
        },
        {
          "author": "lukaszdz",
          "body": "Running the command `rm -rf venv/lib/python3.11/site-packages/logfire` seems to have fixed the library import issue. Not sure why when I tried to run poetry remove/uninstall logfire, that the directory remained on my filesystem.",
          "created_at": "2025-04-05T23:13:37Z"
        }
      ]
    },
    {
      "issue_number": 1126,
      "title": "Support for model Gemini Flash 2.0 Image Generation",
      "body": "### Description\n\nmodel name: \"gemini-2.0-flash-exp-image-generation\"\ngemini api doc: https://ai.google.dev/gemini-api/docs/image-generation#gemini\n\n\n### References\n\n_No response_",
      "state": "open",
      "author": "tranhoangnguyen03",
      "author_type": "User",
      "created_at": "2025-03-15T06:40:35Z",
      "updated_at": "2025-04-05T15:05:54Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1126/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1126",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1126",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:18.527370",
      "comments": [
        {
          "author": "Kludex",
          "body": "- I'm working on it on #1130 .",
          "created_at": "2025-03-16T12:22:09Z"
        },
        {
          "author": "Rinfore",
          "body": "I think having support for plots from tool calls made by the model would also be very helpful, but unsure whether this fits in this issue. (The multimodal input / output support ticket was closed).\n\nWould also give the model access to the plots so it can analyse further. This is critical for many ap",
          "created_at": "2025-04-05T15:03:56Z"
        }
      ]
    },
    {
      "issue_number": 1384,
      "title": "Pydantiv Evals Issue with opentelemetry",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI installed pydantic-evals and tried to run the example of evaluation process (https://ai.pydantic.dev/evals/#evaluators) and got into this series of issues:\n\n1. It asked me to install opentelemetry-sdk, which I did\n2. It showed this traceID error which I have no idea. I'm on Mac, everything is fine on Window:\nTraceback (most recent call last):\n  File \"/Users/thanhquach/projects/evals/basic.py\", line 35, in <module>\n    report = dataset.evaluate_sync(guess_city)  \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/thanhquach/projects/evals/eval/lib/python3.11/site-packages/pydantic_evals/dataset.py\", line 315, in evaluate_sync\n    return get_event_loop().run_until_complete(self.evaluate(task, name=name, max_concurrency=max_concurrency))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/thanhquach/projects/evals/eval/lib/python3.11/site-packages/pydantic_evals/dataset.py\", line 283, in evaluate\n    cases=await task_group_gather(\n          ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/thanhquach/projects/evals/eval/lib/python3.11/site-packages/pydantic_evals/_utils.py\", line 99, in task_group_gather\n    async with anyio.create_task_group() as tg:\n  File \"/Users/thanhquach/projects/evals/eval/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 597, in __aexit__\n    raise exceptions[0]\n  File \"/Users/thanhquach/projects/evals/eval/lib/python3.11/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n    results[index] = await tsk()\n                     ^^^^^^^^^^^\n  File \"/Users/thanhquach/projects/evals/eval/lib/python3.11/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n    return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/thanhquach/projects/evals/eval/lib/python3.11/site-packages/pydantic_evals/dataset.py\", line 907, in _run_task_and_evaluators\n    trace_id = f'{context.trace_id:032x}'\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: unsupported format string passed to MagicMock.__format__\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nError on both Python 3.11 and 3.12 \nPydantic AI 0.0.52\n```",
      "state": "closed",
      "author": "thanhtheman",
      "author_type": "User",
      "created_at": "2025-04-05T02:17:19Z",
      "updated_at": "2025-04-05T02:27:42Z",
      "closed_at": "2025-04-05T02:27:41Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1384/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1384",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1384",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:23.776949",
      "comments": [
        {
          "author": "thanhtheman",
          "body": "I just realized that I must install logfire, now everything is working. Thanks",
          "created_at": "2025-04-05T02:27:41Z"
        }
      ]
    },
    {
      "issue_number": 1300,
      "title": "Allow users to add more mcp servers during a chat app lifecycle",
      "body": "### Description\n\nIn a user's chat session, what if he/she wants to add more mcp servers? Do we need to restart the entire web service just to add more server? That's why I want to propose a feature to track mcp tool uses during a chat app lifecycle!\n\n### References\n\n_No response_",
      "state": "open",
      "author": "minhrongcon2000",
      "author_type": "User",
      "created_at": "2025-03-30T04:36:47Z",
      "updated_at": "2025-04-04T09:50:31Z",
      "closed_at": null,
      "labels": [
        "MCP"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1300/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1300",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1300",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.057531",
      "comments": [
        {
          "author": "Kludex",
          "body": "In case of stdio transport, yes. But you can create an SSE based one.",
          "created_at": "2025-03-31T08:53:04Z"
        },
        {
          "author": "minhrongcon2000",
          "body": "Regarding SSE, I see from the codebase that we cannot add new server once `Agent` object is instantiated! If I follow this [example](https://ai.pydantic.dev/mcp/client/#sse-client), it would become more inflexible if I want to dynamically add more server. I could try inheriting the `Agent` class and",
          "created_at": "2025-03-31T09:21:25Z"
        },
        {
          "author": "diego898",
          "body": "I havent tried this yet, but I think you may just be able to add to `agent._mcp_servers`. Things don't seem to be cached and are evaluated at run time (.iter(), etc.) - I could be mistaken though",
          "created_at": "2025-04-03T15:31:58Z"
        }
      ]
    },
    {
      "issue_number": 1273,
      "title": "MCP improvement: adding resources to context",
      "body": "We should (optionally) add resources to the model context.",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:37:05Z",
      "updated_at": "2025-04-04T09:50:30Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1273/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1273",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1273",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.345240",
      "comments": []
    },
    {
      "issue_number": 1272,
      "title": "MCP improvement: server support for sampling",
      "body": "When using a `pydantic-ai` agent within an MCP server, we should be able to register a `model` that uses sampling to call back to the client for LLM requests.",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:29:36Z",
      "updated_at": "2025-04-04T09:50:29Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1272/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1272",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1272",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.345259",
      "comments": []
    },
    {
      "issue_number": 1270,
      "title": "MCP improvement: CLI support",
      "body": "We should improve our CLI to support connecting to MCP servers, we should probably read the [claud desktop JSON file format](https://modelcontextprotocol.io/quickstart/user) as I think others are using that.",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:27:37Z",
      "updated_at": "2025-04-04T09:50:28Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1270/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1270",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1270",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.345266",
      "comments": []
    },
    {
      "issue_number": 1269,
      "title": "MCP improvement: client support for sampling",
      "body": "We should support [sampling](https://spec.modelcontextprotocol.io/specification/2024-11-05/client/sampling/) where the server asks the client to make an LLM call",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:24:14Z",
      "updated_at": "2025-04-04T09:50:28Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1269/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1269",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1269",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.345272",
      "comments": []
    },
    {
      "issue_number": 1268,
      "title": "MCP next steps",
      "body": "There's more to do for MCP support:\n* for observability, we're already working on https://github.com/pydantic/logfire/issues/947\n\nOther tasks within pydantic-ai are sub-issues:",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:22:35Z",
      "updated_at": "2025-04-04T09:50:27Z",
      "closed_at": null,
      "labels": [
        "Feature request",
        "MCP"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1268/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1268",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1268",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.345278",
      "comments": []
    },
    {
      "issue_number": 1214,
      "title": "Add logging/tracing for MCP server / Pydantic tool registration with LLM",
      "body": "### Description\n\nHi team 👋,\n\nWhile using the MCP server to define and register tools with the agent and LLM, I noticed that there's currently no logging or trace available to indicate when a tool gets registered. It would be really helpful for debugging and observability to have a log or trace event when:\n\n-  A tool is registered with the agent\n- The tools are passed to the LLM during a ModelRequest\n\nAdditionally, since all tools are currently registered on every request, it might also be useful to expose a way to filter which tools are included, allowing for more granular control and traceability.\n\nThanks!\n\n### References\n\n_No response_",
      "state": "open",
      "author": "BazaidKhan",
      "author_type": "User",
      "created_at": "2025-03-23T12:38:10Z",
      "updated_at": "2025-04-04T09:50:26Z",
      "closed_at": null,
      "labels": [
        "MCP"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1214/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1214",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1214",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.345284",
      "comments": []
    },
    {
      "issue_number": 1362,
      "title": "Add OpenAI Whisper support",
      "body": "### Description\n\nWe currently don't have the ability to use Whisper and all its audio formats. It would be great if you add Whisper or/and GPT 4o transcribe support.\n\n### References\n\nhttps://platform.openai.com/docs/models/whisper-1\nhttps://platform.openai.com/docs/models/gpt-4o-transcribe",
      "state": "open",
      "author": "vadim-su",
      "author_type": "User",
      "created_at": "2025-04-03T13:31:21Z",
      "updated_at": "2025-04-04T09:49:00Z",
      "closed_at": null,
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1362/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1362",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1362",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.345291",
      "comments": [
        {
          "author": "Kludex",
          "body": "Ideas on how would that fit that in the framework?",
          "created_at": "2025-04-03T13:37:35Z"
        }
      ]
    },
    {
      "issue_number": 898,
      "title": "Clarification and Enhancement Request for Message History Documentation",
      "body": "Hi PydanticAI Team,\n\nI’ve been working on a multi-agent system and found the current documentation on [Messages and Chat History](https://ai.pydantic.dev/message-history/) a bit challenging to navigate, especially regarding sharing history between agents in a graph-based setup.\n\nIssue:\n\nI initially attempted to share the messages history directly between agents but discovered that PydanticAI doesn’t support this approach. Instead, I learned that the recommended method is to serialize the message history into a string and inject it into the system prompt of the subsequent agent.\n\nRequest:\n\nCould the documentation be enhanced to include a detailed explanation of this process? A step-by-step example demonstrating how to serialize the message history and incorporate it into the system prompt for the next agent for example in Graph would be incredibly helpful.\n\nThis addition would greatly assist users in implementing multi-agent workflows in Graph more effectively.\n\nThank you for your continuous efforts in improving PydanticAI. ",
      "state": "open",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-11T18:54:26Z",
      "updated_at": "2025-04-04T09:31:24Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/898/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/898",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/898",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.578919",
      "comments": [
        {
          "author": "dmontagu",
          "body": "You should be able to transfer history between agents without serialization. You can see an example of this here: https://github.com/codematrix/agentic-ai-design-patterns/blob/develop/examples/supervisor/call_centre.py. If it doesn't make sense feel free to ask more questions.\n\nPoint taken that we c",
          "created_at": "2025-02-12T22:12:16Z"
        },
        {
          "author": "YanSte",
          "body": "Thanks for the clarification and the example @dmontagu. 👍",
          "created_at": "2025-02-13T23:32:48Z"
        },
        {
          "author": "dinhngoc267",
          "body": "> You should be able to transfer history between agents without serialization. You can see an example of this here: https://github.com/codematrix/agentic-ai-design-patterns/blob/develop/examples/supervisor/call_centre.py. If it doesn't make sense feel free to ask more questions.\n> \n> Point taken tha",
          "created_at": "2025-04-04T09:31:23Z"
        }
      ]
    },
    {
      "issue_number": 1022,
      "title": "Static system prompt is not replaced by dynamic system prompt if passing message history",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nIf you originally have a static system prompt.\n\nYou can't regenerate it by setting a dynamic system prompt if you pass a message history.\n\n```\nchatbot = Agent(\n    model='openai:gpt-4o',\n    result_type=str,\n    system_prompt='You are a highly intelligent chatbot'\n)\n```\n\nRun this and get the history and save to JSON\n\nNow if you add a dynamic system prompt and remove the static one.\n\n```python\nchatbot = Agent(\n    model='openai:gpt-4o',\n    result_type=str,\n)\n\n@chatbot.system_prompt(dynamic=True)\ndef dynamic_prompt() -> str:\n    return 'You are a helpful chatbot'\n```\n\nIf you pass the old history, it still uses the old system prompt.\n\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12\npydantic 0.0.30\nopenai 1.65.1\n```",
      "state": "open",
      "author": "vikigenius",
      "author_type": "User",
      "created_at": "2025-03-01T05:19:05Z",
      "updated_at": "2025-04-04T09:20:15Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1022/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1022",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1022",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:24.862468",
      "comments": [
        {
          "author": "vikigenius",
          "body": "In fact it does not even regenerate if you initially forgot to set dynamic to True but still used a decorator.",
          "created_at": "2025-03-01T05:27:25Z"
        }
      ]
    },
    {
      "issue_number": 1345,
      "title": "Evals: API key not found using LLMJudge with AsyncAzureOpenAI client",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\n`OpenAIModel` based on  `AsyncAzureOpenAI` client passed to `LLMJudge` can't find `api_key`. I've confirmed that the agent is able to run successfully with the identical `BaseSettings`. See the code excerpt based on the recipe example.\n\nDid not find an `LLMJudge` example that defines the model other than through  `models.KnownModelName`.\n\n``` \n + Exception Group Traceback (most recent call last):\n  |   File \"<project>test/example.py\", line 116, in <module>\n  |     report = recipe_dataset.evaluate_sync(transform_recipe)\n  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/dataset.py\", line 315, in evaluate_sync\n  |     return get_event_loop().run_until_complete(self.evaluate(task, name=name, max_concurrency=max_concurrency))\n  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/home/auan/.pyenv/versions/3.12.0/lib/python3.12/asyncio/base_events.py\", line 664, in run_until_complete\n  |     return future.result()\n  |            ^^^^^^^^^^^^^^^\n  |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/dataset.py\", line 283, in evaluate\n  |     cases=await task_group_gather(\n  |           ^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/_utils.py\", line 99, in task_group_gather\n  |     async with anyio.create_task_group() as tg:\n  |   File \"<project>.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Exception Group Traceback (most recent call last):\n    |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n    |     results[index] = await tsk()\n    |                      ^^^^^^^^^^^\n    |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/dataset.py\", line 891, in _run_task_and_evaluators\n    |     evaluator_outputs_by_task = await task_group_gather(\n    |                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/_utils.py\", line 99, in task_group_gather\n    |     async with anyio.create_task_group() as tg:\n    |   File \"<project>.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n    |     raise BaseExceptionGroup(\n    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n    +-+---------------- 1 ----------------\n      | Traceback (most recent call last):\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n      |     results[index] = await tsk()\n      |                      ^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/evaluators/_run_evaluator.py\", line 38, in run_evaluator\n      |     raw_results = await evaluator.evaluate_async(ctx)\n      |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/evaluators/evaluator.py\", line 215, in evaluate_async\n      |     return await output\n      |            ^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/evaluators/common.py\", line 175, in evaluate\n      |     grading_output = await judge_output(ctx.output, self.rubric, self.model)\n      |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_evals/evaluators/llm_as_a_judge.py\", line 48, in judge_output\n      |     return (await _judge_output_agent.run(user_prompt, model=model)).data\n      |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 319, in run\n      |     async with self.iter(\n      |   File \"/home/auan/.pyenv/versions/3.12.0/lib/python3.12/contextlib.py\", line 204, in __aenter__\n      |     return await anext(self.gen)\n      |            ^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 419, in iter\n      |     model_used = self._get_model(model)\n      |                  ^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1176, in _get_model\n      |     model_ = models.infer_model(model)\n      |              ^^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_ai/models/__init__.py\", line 424, in infer_model\n      |     return OpenAIModel(model_name, provider=provider)\n      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 177, in __init__\n      |     provider = infer_provider(provider)\n      |                ^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_ai/providers/__init__.py\", line 50, in infer_provider\n      |     return OpenAIProvider()\n      |            ^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/pydantic_ai/providers/openai.py\", line 67, in __init__\n      |     self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n      |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      |   File \"<project>.venv/lib/python3.12/site-packages/openai/_client.py\", line 345, in __init__\n      |     raise OpenAIError(\n      | openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n      +------------------------------------\n```\n\n### Example Code\n\n```Python\nfrom __future__ import annotations\n\nimport asyncio\nfrom typing import Any\n\nfrom openai.lib.azure import AsyncAzureOpenAI\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai.format_as_xml import format_as_xml\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance, LLMJudge\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass AzureConfig(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=\".env\", env_prefix=\"AZURE_OPENAI_\", extra=\"allow\"\n    )\n    endpoint: str\n    api_key: str\n\n    gpt4o_deployment: str\n    gpt4o_api_version: str\n\n\nconfig = AzureConfig()  # pyright: ignore missing arguments\n\nazure_gpt4o_client = AsyncAzureOpenAI(\n    azure_endpoint=config.endpoint,\n    api_key=config.api_key,\n    api_version=config.gpt4o_api_version,\n)\n\nazure_gpt4o_model = OpenAIModel(\n    model_name=config.gpt4o_deployment,\n    provider=OpenAIProvider(openai_client=azure_gpt4o_client),\n)\n\n\nclass CustomerOrder(BaseModel):\n    dish_name: str\n    dietary_restriction: str | None = None\n\n\nclass Recipe(BaseModel):\n    ingredients: list[str]\n    steps: list[str]\n\n\nrecipe_agent = Agent(\n    model=azure_gpt4o_model,\n    result_type=Recipe,\n    system_prompt=(\n        \"Generate a recipe to cook the dish that meets the dietary restrictions.\"\n    ),\n)\n\n\nasync def transform_recipe(customer_order: CustomerOrder) -> Recipe:\n    r = await recipe_agent.run(format_as_xml(customer_order))\n    return r.data\n\n\nrecipe_dataset = Dataset[CustomerOrder, Recipe, Any](\n    cases=[\n        Case(\n            name=\"vegetarian_recipe\",\n            inputs=CustomerOrder(\n                dish_name=\"Spaghetti Bolognese\", dietary_restriction=\"vegetarian\"\n            ),\n            expected_output=None,  #\n            metadata={\"focus\": \"vegetarian\"},\n            evaluators=(\n                LLMJudge(\n                    rubric=\"Recipe should not contain meat or animal products\",\n                ),\n            ),\n        ),\n        Case(\n            name=\"gluten_free_recipe\",\n            inputs=CustomerOrder(\n                dish_name=\"Chocolate Cake\", dietary_restriction=\"gluten-free\"\n            ),\n            expected_output=None,\n            metadata={\"focus\": \"gluten-free\"},\n            # Case-specific evaluator with a focused rubric\n            evaluators=(\n                LLMJudge(\n                    rubric=\"Recipe should not contain gluten or wheat products\",\n                ),\n            ),\n        ),\n    ],\n    evaluators=[\n        IsInstance(type_name=\"Recipe\"),\n        LLMJudge(\n            rubric=\"Recipe should have clear steps and relevant ingredients\",\n            include_input=True,\n            model=azure_gpt4o_model,\n        ),\n    ],\n)\n\nresult = asyncio.run(\n    transform_recipe(\n        customer_order=CustomerOrder(\n            dish_name=\"Chocolate Cake\", dietary_restriction=\"gluten-free\"\n        )\n    )\n)\n\nprint(result)\n'''\ningredients=['1 cup gluten-free flour', '1 cup sugar', '1/2 cup cocoa powder', '1 teaspoon baking powder', '1/2 teaspoon baking soda', '1/2 teaspoon salt', '2 large eggs', '1 cup almond milk', '1/2 cup vegetable oil', '2 teaspoons vanilla extract', '1/2 cup boiling water'] steps=['Preheat the oven to 350°F (175°C). Grease and flour two 9-inch round baking pans with gluten-free flour.', 'In a large bowl, combine the gluten-free flour, sugar, cocoa powder, baking powder, baking soda, and salt.', 'Add the eggs, almond milk, vegetable oil, and vanilla extract to the dry ingredients. Mix until well combined.', 'Stir in the boiling water until the batter is smooth. The batter will be thin.', 'Pour the batter evenly into the prepared baking pans.', 'Bake for 30 to 35 minutes, or until a toothpick inserted into the center comes out clean.', 'Remove from the oven and allow the cakes to cool in the pans for 10 minutes.', 'Transfer the cakes to a wire rack to cool completely.']\n'''\n\nreport = recipe_dataset.evaluate_sync(transform_recipe)\nprint(report)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.0\n\nuv pip show openai pydantic\nName: openai\nVersion: 1.70.0\n---\nName: pydantic-ai\nVersion: 0.0.49\n```",
      "state": "closed",
      "author": "auan0001",
      "author_type": "User",
      "created_at": "2025-04-02T13:26:58Z",
      "updated_at": "2025-04-04T08:59:30Z",
      "closed_at": "2025-04-03T22:02:15Z",
      "labels": [
        "question",
        "evals"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1345/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1345",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1345",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:25.093686",
      "comments": [
        {
          "author": "auan0001",
          "body": "@dmontagu Is this your ballpark? TLDR: This is the recipe example but with a model as input instead of a `models.KnownModelName`.",
          "created_at": "2025-04-03T07:43:36Z"
        },
        {
          "author": "Kludex",
          "body": "The `LLMJudge` inside the cases need to have the same model, I guess?",
          "created_at": "2025-04-03T09:36:00Z"
        },
        {
          "author": "auan0001",
          "body": "Thank you. Good catch! However, setting the same model for each `LLMJudge` gives me the problem I originally tried to reproduce with the recipe example\n```\n+ Exception Group Traceback (most recent call last):\n  |   File \"<project>test/example.py\", line 110, in <module>\n  |     report = recipe_datase",
          "created_at": "2025-04-03T10:07:10Z"
        },
        {
          "author": "dmontagu",
          "body": "@auan0001 thanks for reporting this!\n\nI see an easy-enough fix for the error you hit in the last message above, but do you care about being able to serialize/deserialize the LLMJudge with a custom model?\n\nI'll note that I could make the `pydantic_evals.evaluators.llm_as_a_judge._judge_output_agent` ",
          "created_at": "2025-04-03T19:44:56Z"
        },
        {
          "author": "dmontagu",
          "body": "I think #1367 closes the version of this as long as you don't care about deserializing \"custom\" models. If you want to be able to do that, I would request we make a new issue for tracking that.\n\nIf #1367 doesn't resolve the issue for you though, please let us know (we can reopen this issue if mergin",
          "created_at": "2025-04-03T20:56:17Z"
        }
      ]
    },
    {
      "issue_number": 1230,
      "title": "Supervisor solely returns the 'final' Node",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIn my Pydantic AI Graph implementation, I wanted to have some sort of a 'supervisor'.\nThat supervisor would be the orchestrator for handling each incoming message to the correct node(s).\n\nCurrently, there are 2 nodes; Calendar & Final, I guess the names are descriptive enough.\n\nBut everytime, and with everytime I mean literally everytime a question/request comes in and I ask: \"What's on my calendar today\"?, the supervisor guides me to the Final node and nothing happens.\n\nI've been fighting this now the whole day and have no clue where to look, I've adjusted the system_prompt several times, I asked ChatGPT, Gemini & Copilot for a solution, but none were able to come up with a working solution.\n\nHere is the `supervisor.py' file:\n```Python\nimport logging\nimport logfire\nfrom dataclasses import dataclass\n\nfrom django.conf import settings\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_graph import BaseNode, GraphRunContext\n\nfrom butlai_utils.llm.models.event import events\nfrom butlai_utils.llm.models.node import NodeType\nfrom butlai_utils.llm.models.state import LLMState\nfrom butlai_utils.llm.pydantic.node import NodeRegistry\n\nlogger = logging.getLogger(__name__)\n\nlogfire.configure(token=settings.LOGFIRE_API_KEY)\nlogfire.instrument_openai()\n\nsupervisor_agent = None\n\n\nclass SupervisorResult(BaseModel):\n    node_type: NodeType\n\n\ngpt_4o_model = OpenAIModel(\n    settings.OPENAI_DEFAULT_LLM_MODEL,\n    provider='openai',\n    api_key=settings.OPENAI_API_KEY,\n)\n\n\n@dataclass\nclass Supervisor(BaseNode):\n    async def run(self, ctx: GraphRunContext[LLMState]) -> BaseNode:\n        logger.info(f\"Supervisor Node: running with context {ctx.state}\")\n\n        event_type = ctx.state.event_state.type\n        event_data = ctx.state.event_state.data\n\n        logger.info(f\"Processing event of type: {event_type}, with data: {event_data}\")\n        result = await get_supervisor_agent().run(\n            {\"type\": event_type, \"data\": event_data}, deps=ctx.state\n        )\n\n        return NodeRegistry.get(result.data.node_type).node()\n\n\ndef generate_system_prompt() -> str:\n    \"\"\"Generate the system prompt dynamically after all nodes are registered.\"\"\"\n    node_options = \"\\n\".join(\n        [\n            f\"- type: {node.name}, description: {config.description}\"\n            for node, config in NodeRegistry.all().items()\n        ]\n    )\n    event_options = \"\\n\".join(\n        [\n            f\"- type: {event_type.value}, description: {description}\"\n            for event_type, description in events.items()\n        ]\n    )\n\n    print(f\"Registered nodes: {NodeRegistry.all()}\")\n\n    return (\n        \"You are the AI Agent supervisor node that delegates tasks to specialist nodes.\\n\"\n        \"After a Specialist Node completes its task, the event is passed back to you.\\n\"\n        \"You must then decide the next step based on the event_state and the added visited_nodes_state.\\n\"\n        \"See yourself as a workflow orchestrator, delegating tasks to the right specialist nodes.\\n\"\n        \"You will receive an event_state with a type and associated data. Your job is to:\\n\"\n        \"- Understand the event_state type and its data.\\n\"\n        \"- Decide which specialist node is capable of handling the task.\\n\"\n        \"- Delegate the task to the appropriate specialist node:\\n\"\n        f\"{node_options}\\n\"\n        \"- After the specialist node completes its tasks, decide the next steps.\\n\"\n        \"Event types you may encounter:\\n\"\n        f\"{event_options}\\n\"\n        \"Important:\\n\"\n        \"- Do not select the Final Node unless all other nodes have been processed.\\n\"\n        \"- Always delegate tasks to the most appropriate Specialist Node first.\\n\"\n        \"- Use the visited_nodes_state to ensure no node is skipped.\\n\"\n    ).strip()\n\ndef get_supervisor_agent():\n    global supervisor_agent\n\n    if supervisor_agent:\n        return supervisor_agent\n\n    supervisor_agent = Agent(\n        model=gpt_4o_model,\n        system_prompt=generate_system_prompt(),\n        result_type=SupervisorResult,\n    )\n\n    return supervisor_agent\n```\n\nThen, we have the `node.py` file:\n```Python\nimport logging\nfrom typing import Dict, Type\n\nfrom pydantic import BaseModel\nfrom pydantic_graph import BaseNode\n\nlogger = logging.getLogger(__name__)\n\n\nclass NodeConfig(BaseModel):\n    description: str\n    node: Type[BaseNode]\n\n\nclass NodeRegistry:\n    _registry: Dict[str, NodeConfig] = {}\n\n    @classmethod\n    def register(cls, node_enum: str, description: str, node: Type[BaseNode]):\n        \"\"\"\n        Register a node with its description and implementation.\n        \"\"\"\n        if node_enum in cls._registry:\n            raise ValueError(f\"Node {node_enum} is already registered.\")\n        cls._registry[node_enum] = NodeConfig(description=description, node=node)\n        logger.info(\n            f\"Registered node: {node_enum} with description: {description} and implementation: {node}\"\n        )\n\n    @classmethod\n    def get(cls, node_enum: str) -> NodeConfig:\n        \"\"\"\n        Retrieve the NodeConfig for a given node_enum.\n        \"\"\"\n        if node_enum not in cls._registry:\n            raise ValueError(f\"Node {node_enum} is not registered.\")\n        config = cls._registry[node_enum]\n        logger.info(f\"Retrieved node config for {node_enum}: {config}\")\n        return config\n\n    @classmethod\n    def all(cls) -> Dict[str, NodeConfig]:\n        \"\"\"\n        Retrieve all registered nodes.\n        \"\"\"\n        logger.info(f\"Retrieving all registered nodes: {cls._registry}\")\n        return cls._registry\n\n    @classmethod\n    def auto_register(cls, node_enum: str, description: str):\n        \"\"\"\n        Decorator to automatically register a node.\n        \"\"\"\n\n        def decorator(node_class: Type[BaseNode]):\n            cls.register(node_enum, description, node_class)\n            logger.info(f\"Auto-registered node: {node_enum} with class: {node_class}\")\n            return node_class\n\n        return decorator\n```\n\nThe `calendar.py` file:\n```Python\nimport logging\nfrom dataclasses import dataclass\n\nfrom django.conf import settings\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_graph import BaseNode, GraphRunContext\n\nfrom butlai_utils.llm.models.llm_response import BaseLLMResponse\nfrom butlai_utils.llm.models.node import NodeType\nfrom butlai_utils.llm.models.state import LLMState\nfrom butlai_utils.llm.pydantic.node import NodeRegistry\nfrom butlai_utils.llm.pydantic.supervisor import Supervisor\n\nlogger = logging.getLogger(__name__)\n\n\ngpt_4o_model = OpenAIModel(\n    settings.OPENAI_DEFAULT_LLM_MODEL,\n    provider='openai',\n    api_key=settings.OPENAI_API_KEY,\n)\n\n\ncalendar_agent = Agent(\n    model=gpt_4o_model,\n    result_type=BaseLLMResponse,\n    system_prompt=(\n        \"Come up with some random calendar events for the user.\"\n    ),\n)\n\n\n@dataclass\n@NodeRegistry.auto_register(NodeType.CALENDAR, \"Calendar Node for handling all calendar related tasks.\")\nclass Calendar(BaseNode):\n\n    async def run(self, ctx: GraphRunContext[LLMState]) -> Supervisor:\n        logger.info(\n            f\"Calendar Node running with messevent_state: {ctx.state.event_state}.\"\n        )\n        response = await calendar_agent.run(ctx.state.event_state, deps=ctx.state)\n        ctx.state.visited_nodes_state.append(response.data)\n\n        return Supervisor()\n```\n\nAnd `final.py`:\n```Python\nimport logging\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, GraphRunContext\n\nfrom butlai_utils.llm.models.node import NodeType\nfrom butlai_utils.llm.models.state import LLMState\nfrom butlai_utils.llm.pydantic.node import NodeRegistry\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\n@NodeRegistry.auto_register(NodeType.FINAL, \"Final node of the graph, only used this Node if the request if fully processed.\")\nclass Final(BaseNode):\n\n    async def run(self, ctx: GraphRunContext[LLMState]) -> End:\n        logger.info(\n            f\"End of the graph, visited nodes: {ctx.state.visited_nodes_state}.\"\n        )\n        return End(\"yeye\")\n```\n\nAnd last but not least; `graph.py`:\n```Python\nfrom pydantic_graph import Graph\n\nfrom .calendar import Calendar\nfrom .final import Final\nfrom .supervisor import Supervisor\n\ngraph = Graph(\n    nodes=(\n        Supervisor,\n        Calendar,\n        Final,\n    )\n)\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython                                   3.12.9\npydantic                                 2.10.3                                                                                                                                                                      \npydantic-ai                              0.0.43                                                                                                                                                                      \npydantic-ai-slim                         0.0.43                                                                                                                                                                      \npydantic_core                            2.27.1\npydantic-graph                           0.0.43\npydantic-settings                        2.8.1\n```",
      "state": "closed",
      "author": "tpotjj",
      "author_type": "User",
      "created_at": "2025-03-24T19:35:32Z",
      "updated_at": "2025-04-04T07:21:29Z",
      "closed_at": "2025-03-24T20:35:07Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1230/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1230",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1230",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:25.359988",
      "comments": [
        {
          "author": "dinhngoc267",
          "body": "Hi @tpotjj  did you solve your problem? ",
          "created_at": "2025-04-04T07:21:28Z"
        }
      ]
    },
    {
      "issue_number": 1371,
      "title": "Pydantic-AI agent fails to connect to MCP server via SSE",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm trying to setup a POC using pydantic-ai Agents and MCP servers, even though the server runs fine and I'm able to connect to it via SEE (or stdio) using a custom client (or even Cursor), whenever I try to run my agent, I get a connection error.\n\nI tried different variations of the server URL (`localhost`, `0.0.0.0`, `127.0.0.1`), after reading the details on this similar issue: https://github.com/pydantic/pydantic-ai/issues/1282\n\nBut had no luck in getting the agent to connect to the MCP server. \n\nThis is the stack trace:\n```\nTraceback (most recent call last):\n  File \"/pydantic_assistant/bare_bones_assistant.py\", line 33, in <module>\n    asyncio.run(general_tool_agent.run(\"Hi AI\"))\n  File \"/.pyenv/versions/3.11.8/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/.pyenv/versions/3.11.8/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.pyenv/versions/3.11.8/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 329, in run\n    async for _ in agent_run:\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 1414, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 782, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 760, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 262, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 313, in _make_request\n    model_settings, model_request_parameters = await self._prepare_request(ctx)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 334, in _prepare_request\n    model_request_parameters = await _prepare_request_parameters(ctx)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 229, in _prepare_request_parameters\n    await asyncio.gather(\n  File \"/experiment_venvs/polar_env/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 224, in add_mcp_server_tools\n    raise exceptions.UserError(f'MCP server is not running: {server}')\npydantic_ai.exceptions.UserError: MCP server is not running: MCPServerHTTP(url='http://localhost:3066/sse', headers=None, timeout=5, sse_read_timeout=300)\n```\n\n### Example Code\n\n```Python\n# server (SSE example)\n\nimport uvicorn\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"HelloWorld\")\n\n@mcp.tool()\ndef hello_world(name: str) -> str:\n    \"\"\"\n    Returns a greeting message.\n\n    Args:\n        name: The name of the person to greet.\n\n    Returns:\n        A greeting message.\n    \"\"\"\n    return f\"Hello, {name}!\"\n\nif __name__ == \"__main__\":\n    app = mcp.sse_app()\n    uvicorn.run(app, host=\"127.0.0.1\", port=3066)\n\n\n# server (stdio example)\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"HelloWorld\", port=3066)\n\n@mcp.tool()\ndef hello_world(name: str) -> str:\n    \"\"\"\n    Returns a greeting message.\n\n    Args:\n        name: The name of the person to greet.\n\n    Returns:\n        A greeting message.\n    \"\"\"\n    return f\"Hello, {name}!\"\n\nif __name__ == \"__main__\":\n    mcp.run()\n\n\n# agent code\n\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_ai.mcp import MCPServerHTTP\n\nLLM_SETTINGS: ModelSettings = {'temperature': 0.3}\nserver_url = \"http://localhost:3066/sse\"\nserver = MCPServerHTTP(url=server_url)\n\ngeneral_tool_agent = Agent(\n    model='openai:gpt-4o',\n    system_prompt=(\n        \"You are a helpful assistant.\"\n    ),\n    mcp_servers=[server],\n    model_settings=LLM_SETTINGS,\n    name='GeneralToolAgent'\n)\n\n\nif __name__==\"__main__\":\n    asyncio.run(general_tool_agent.run(\"Hi AI\"))\n\n\n# sample client that connects successfully\n\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client\n\nasync def list_the_tools():\n    async with sse_client(server_url) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n\n            tools_result = await session.list_tools()\n            print(tools_result)\n\nif __name__==\"__main__\":\n    asyncio.run(list_the_tools())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.11\npydantic-ai==0.0.52\nopenai==1.70.0\n\nOther related libs:\nfastmcp==0.4.1\nmcp==1.6.0\nuvicorn==0.34.0\n```",
      "state": "closed",
      "author": "gamella-igor",
      "author_type": "User",
      "created_at": "2025-04-04T02:01:39Z",
      "updated_at": "2025-04-04T02:12:11Z",
      "closed_at": "2025-04-04T02:10:26Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1371/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1371",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1371",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:25.586003",
      "comments": [
        {
          "author": "gamella-igor",
          "body": "Missed one row from the docs 🤦  All good now\n\nAfter running it within:\n`async with general_tool_agent.run_mcp_servers():`",
          "created_at": "2025-04-04T02:10:57Z"
        }
      ]
    },
    {
      "issue_number": 1357,
      "title": "Evals does not handle BinaryContent.  type. to_jsonable_python(case.inputs)     | UnicodeDecodeError",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nGemini models support BinaryContent.\nHowever, when I add BinaryContent into a Case I get an error.\n\nHere is a full code to reproduce the issue. Just need a sample image.\n\n\nError\n\n```\n  report = dataset.evaluate_sync(guess)\n  + Exception Group Traceback (most recent call last):\n  |   File \"/home/ads/sndbx/pydantic-evals-sndb/failure.py\", line 23, in <module>\n  |     report = dataset.evaluate_sync(guess)\n  |   File \"/home/ads/sndbx/pydantic-evals-sndb/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 315, in evaluate_sync\n  |     return get_event_loop().run_until_complete(self.evaluate(task, name=name, max_concurrency=max_concurrency))\n  |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/home/ads/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py\", line 721, in run_until_complete\n  |     return future.result()\n  |            ~~~~~~~~~~~~~^^\n  |   File \"/home/ads/sndbx/pydantic-evals-sndb/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 283, in evaluate\n  |     cases=await task_group_gather(\n  |           ^^^^^^^^^^^^^^^^^^^^^^^^\n  |     ...<4 lines>...\n  |     ),\n  |     ^\n  |   File \"/home/ads/sndbx/pydantic-evals-sndb/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 99, in task_group_gather\n  |     async with anyio.create_task_group() as tg:\n  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n  |   File \"/home/ads/sndbx/pydantic-evals-sndb/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  |         \"unhandled errors in a TaskGroup\", self._exceptions\n  |     ) from None\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/home/ads/sndbx/pydantic-evals-sndb/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n    |     results[index] = await tsk()\n    |                      ^^^^^^^^^^^\n    |   File \"/home/ads/sndbx/pydantic-evals-sndb/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/home/ads/sndbx/pydantic-evals-sndb/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 910, in _run_task_and_evaluators\n    |     report_inputs = to_jsonable_python(case.inputs)\n    | UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid utf-8\n```\n\n### Example Code\n\n```Python\nfrom pydantic import BaseModel\nfrom pydantic_ai import BinaryContent\nfrom pydantic_evals import Case, Dataset\n\n\nclass Input(BaseModel):\n    data: BinaryContent\n\n\nwith open(\"image.png\", \"rb\") as f:\n    image_data1 = f.read()\n\n\ndata = BinaryContent(data=image_data1, media_type=\"image/png\")\ndataset = Dataset(cases=[Case(inputs=Input(data=data))])\n\n\nasync def guess(question: Input) -> str:\n    # Use Gemini model that accepts BinaryInput\n    return \"\"\n\n\nreport = dataset.evaluate_sync(guess)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nrequires-python = \">=3.13\"\ndependencies = [\n    \"pydantic-ai-slim[openai]>=0.0.49\",\n    \"pydantic-evals[logfire]>=0.0.49\",\n    \"pyyaml>=6.0.2\",\n]\n```",
      "state": "closed",
      "author": "aidiss",
      "author_type": "User",
      "created_at": "2025-04-03T06:39:03Z",
      "updated_at": "2025-04-03T19:33:05Z",
      "closed_at": "2025-04-03T19:33:05Z",
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1357/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1357",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1357",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:25.801660",
      "comments": [
        {
          "author": "Kludex",
          "body": "- It should have been fixed by #1333.\n\nCan you try on the latest version of pydantic-evals? Let me know if it's not. 🙏 ",
          "created_at": "2025-04-03T19:32:51Z"
        }
      ]
    },
    {
      "issue_number": 1352,
      "title": "`opentelemetry-sdk` not explicit dependency of `pydantic_evals`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nGetting import error when trying to use `pydantic_evals.dataset`. \n\n```\nModuleNotFoundError: No module named 'yaml'\n```\n\nLooks like when using `pydantic-ai`, `opentelemetry-sdk` is installed as a transient dependency via `pydantic-ai[logfire]` or`pydantic-ai-slim[logfire]` . \n\n```\n$ uv tree --package opentelemetry-sdk --invert\n\nopentelemetry-sdk v1.31.1\n├── logfire v3.12.0\n│   └── pydantic-ai v0.0.49 (extra: logfire)    # or pydantic-ai-slim v0.0.49 (extra: logfire)\n└── opentelemetry-exporter-otlp-proto-http v1.31.1\n    └── logfire v3.12.0 (*)\n```\n\n### Example Code\n\n```Python\nFollowing example from docs: https://ai.pydantic.dev/evals/#generating-test-datasets\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython: 3.13\npydantic-ai (and friends): 0.0.49\n```",
      "state": "closed",
      "author": "chasewalden",
      "author_type": "User",
      "created_at": "2025-04-02T18:31:06Z",
      "updated_at": "2025-04-03T13:39:59Z",
      "closed_at": "2025-04-03T13:39:59Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1352/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1352",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1352",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:26.067314",
      "comments": []
    },
    {
      "issue_number": 1351,
      "title": "`pyyaml` not explicit dependency of `pydantic_evals`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nGetting import error when trying to use `pydantic_evals.dataset`. \n\n```\nModuleNotFoundError: No module named 'yaml'\n```\n\nLooks like when using `pydantic-ai`, `pyyaml` is installed as a transient dependency via `pydantic-ai-slim[cohere]`. \n\n```\n$ uv tree --package pyyaml --invert\n\npyyaml v6.0.2\n└── huggingface-hub v0.30.1\n    └── tokenizers v0.21.1\n        └── cohere v5.14.0\n            └── pydantic-ai-slim v0.0.49 (extra: cohere)\n```\n\n### Example Code\n\n```Python\nFollowing example from docs: https://ai.pydantic.dev/evals/#generating-test-datasets\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython: 3.13\npydantic-ai (and friends): 0.0.49\n```",
      "state": "closed",
      "author": "chasewalden",
      "author_type": "User",
      "created_at": "2025-04-02T18:15:48Z",
      "updated_at": "2025-04-03T13:39:59Z",
      "closed_at": "2025-04-03T13:39:59Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1351/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1351",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1351",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:26.067334",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Thanks, should be at least an optional group with docs.\n\nCc @dmontagu.",
          "created_at": "2025-04-02T18:24:16Z"
        }
      ]
    },
    {
      "issue_number": 1353,
      "title": "pydantic-ai isn't an explicit dependency of pydantic-evals",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nModuleNotFoundError: No module named 'pydantic_ai'\n\n\n### Example Code\n\n```Python\nimport os\nimport sys\nimport asyncio\nimport time\n\nfrom pydantic_evals import Case, Dataset\n\n\nif __name__ == \"__main__\":\n    # Create a dataset with multiple test cases\n    dataset = Dataset(\n        cases=[\n            Case(\n                name=f'case_{i}',\n                inputs=i,\n                expected_output=i * 2,\n            )\n            for i in range(5)\n        ]\n    )\n\n\n    async def double_number(input_value: int) -> int:\n        \"\"\"Function that simulates work by sleeping for a second before returning double the input.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate work\n        return input_value * 2\n\n\n    # Run evaluation with unlimited concurrency\n    t0 = time.time()\n    report_default = dataset.evaluate_sync(double_number)\n    print(f'Evaluation took less than 0.3s: {time.time() - t0 < 0.3}')\n\n    report_default.print(include_input=True, include_output=True, include_durations=False)  \n\n    # Run evaluation with limited concurrency\n    t0 = time.time()\n    report_limited = dataset.evaluate_sync(double_number, max_concurrency=1)\n    print(f'Evaluation took more than 0.5s: {time.time() - t0 > 0.5}')\n\n    report_limited.print(include_input=True, include_output=True, include_durations=False)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.11\npydantic-evals==0.0.49\n```",
      "state": "closed",
      "author": "rstocker99",
      "author_type": "User",
      "created_at": "2025-04-03T02:53:36Z",
      "updated_at": "2025-04-03T13:39:58Z",
      "closed_at": "2025-04-03T13:39:58Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1353/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1353",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1353",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:26.297918",
      "comments": []
    },
    {
      "issue_number": 1302,
      "title": "How can i evaluate a task wrapped inside a class using pydantic_evals?",
      "body": "I am going through the following example provided as part of evaluation documentation. \n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance\n\ncase1 = Case(  # (1)!\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\n\nclass MyEvaluator(Evaluator[str, str]):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset = Dataset(\n    cases=[case1],\n    evaluators=[IsInstance(type_name='str'), MyEvaluator()],  # (3)!\n)\n\n\nasync def guess_city(question: str) -> str:  # (4)!\n    return 'Paris'\n\n\nreport = dataset.evaluate_sync(guess_city)\n```\nIn the above code, we are testing the task 'guess_city' by passing it to 'evaluate_sync' method. How should i test 'guess_city' method if it is wrapped inside a class?\n\n```\nclass Test():\n     def __init__(self):\n          self.city = 'Paris'\n     async def guess_city(self, question:str) -> str:\n          return self.city\n```",
      "state": "closed",
      "author": "algorithmica-repository",
      "author_type": "User",
      "created_at": "2025-03-30T06:52:17Z",
      "updated_at": "2025-04-02T15:21:16Z",
      "closed_at": "2025-04-02T15:21:15Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1302/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1302",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1302",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:26.297944",
      "comments": [
        {
          "author": "dmontagu",
          "body": "Replace\n```python\nasync def guess_city(question: str) -> str:  # (4)!\n    return 'Paris'\n\nreport = dataset.evaluate_sync(guess_city)\n```\n\nwith\n\n```python\nclass Test():\n    def __init__(self):\n        self.city = 'Paris'\n\n    async def guess_city(self, question: str) -> str:\n        return self.city\n",
          "created_at": "2025-03-31T14:24:39Z"
        },
        {
          "author": "algorithmica-repository",
          "body": "@dmontagu , thank you for your response. I thought Test() call would be triggered for each case of dataset but it is not the case. I validated it as follows:\n```\ncase1 = Case(\n    name=\"simple_case1\",\n    inputs=\"What is the capital of France?\",\n    expected_output=\"Paris\",\n    metadata={\"difficulty",
          "created_at": "2025-04-02T15:21:15Z"
        }
      ]
    },
    {
      "issue_number": 1250,
      "title": "Schema error using MCP server with Gemini 2.5",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nGemini chokes on `$schema` in the JSON schema returned by MCP servers.\n\n\n```sh-session\n> GEMINI_API_KEY=XXX uv run --with pydantic-ai client.py\nSecure MCP Filesystem Server running on stdio\nAllowed directories: [ '/private/tmp' ]\nTraceback (most recent call last):\n  File \"/private/tmp/client.py\", line 15, in <module>\n    asyncio.run(main())\n  File \"/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/private/tmp/client.py\", line 11, in main\n    result = await agent.run('Summarize the file client.py')\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 327, in run\n    async for _ in agent_run:\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 1414, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 782, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 760, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 262, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 314, in _make_request\n    model_response, request_usage = await ctx.deps.model.request(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/models/gemini.py\", line 131, in request\n    async with self._make_request(\n  File \"/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/models/gemini.py\", line 227, in _make_request\n    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=r.text)\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.5-pro-exp-03-25, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[0].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[1].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[2].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[3].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[4].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[5].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[6].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[7].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[8].parameters': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[9].parameters': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools.function_declarations[0].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[0].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[1].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[1].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[2].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[2].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[3].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[3].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[4].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[4].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[5].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[5].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[6].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[6].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[7].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[7].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[8].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[8].parameters': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools.function_declarations[9].parameters\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$schema\\\" at 'tools.function_declarations[9].parameters': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example Code\n\n```Python\nimport asyncio\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio('npx', ['-y', '@modelcontextprotocol/server-filesystem', '.'])\nagent = Agent('gemini-2.5-pro-exp-03-25', mcp_servers=[server])\n\n\nasync def main():\n    async with agent.run_mcp_servers():\n        result = await agent.run('Summarize the file client.py')\n    print(result.data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.11.10\nPydantic 0.0.44\nLLM gemini-2.5-pro-exp-03-25\n```",
      "state": "closed",
      "author": "rectalogic",
      "author_type": "User",
      "created_at": "2025-03-26T15:59:56Z",
      "updated_at": "2025-04-02T09:47:43Z",
      "closed_at": "2025-04-02T09:47:43Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1250/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1250",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1250",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:26.533544",
      "comments": [
        {
          "author": "Kludex",
          "body": "That issue unfortunately is because the MCP server generates a more complete JSON schema, and Gemini doesn't support that.",
          "created_at": "2025-03-26T17:18:43Z"
        },
        {
          "author": "rectalogic",
          "body": "Hmm, Gemini docs say they support \"a select subset of the OpenAPI schema format\". And I don't see `$schema` in that subset.\nhttps://ai.google.dev/gemini-api/docs/function-calling#function_declarations\n\nIs there any way in pydantic-ai to modify the schema returned from the MCP server to remove any `$",
          "created_at": "2025-03-26T17:26:08Z"
        },
        {
          "author": "rectalogic",
          "body": "OK, I was able to get it working with Gemini by subclassing the `MCPServers` to post-process the `ToolDefinition`:\n\n```python\nimport typing as t\n\nfrom pydantic_ai.mcp import MCPServerHTTP, MCPServerStdio\nfrom pydantic_ai.tools import ToolDefinition\n\n\nclass MCPServerMixin(MCPServer):\n    @t.override\n",
          "created_at": "2025-03-26T17:42:47Z"
        },
        {
          "author": "TensorTemplar",
          "body": "This affects all GoogleGLAProvider models in 0.46, not only Gemini 2.5",
          "created_at": "2025-03-27T09:02:09Z"
        },
        {
          "author": "Kludex",
          "body": "- I've implemented the fix on #1342. 🙏 ",
          "created_at": "2025-04-02T09:44:00Z"
        }
      ]
    },
    {
      "issue_number": 1325,
      "title": "Custom types and custom inputs in pydantic-evals",
      "body": "How can I do something like this?\n\n```python\nfrom dataclasses import dataclass\n\nimport logfire\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\nlogfire.configure()\n\n\n@dataclass\nclass HRDepartment:\n    id: int\n    name: str\n\n\n@dataclass\nclass HREmployee:\n    id: int\n    name: str\n    department: HRDepartment\n\n    def __str__(self) -> str:\n        return (\n            f\"<employee>\\n\"\n            f\"id: {self.id}\\n\"\n            f\"name: {self.name}\\n\"\n            f\"<department>\\n\"\n            f\"id: {self.department.id}\\n\"\n            f\"name: {self.department.name}\\n\"\n            f\"</department>\\n\"\n            f\"</employee>\"\n        )\n\n\n@dataclass\nclass HR:\n    employees: list[HREmployee]\n\n    def __str__(self) -> str:\n        return f\"<employees>\\n{'\\n\\n'.join(str(e) for e in self.employees)}\\n</employees>\"\n\n\nclass NumEmployees(BaseModel):\n    num_employees: int\n\n\nclass Department(BaseModel):\n    department_id: int\n    department_name: str\n\n\n@dataclass\nclass HRQuery:\n    query: str\n    deps: HR\n    result_type: type[NumEmployees] | type[Department]\n\n\n@dataclass\nclass EvaluateHR(Evaluator[HRQuery, NumEmployees | Department]):\n    def evaluate(self, ctx: EvaluatorContext[HRQuery, NumEmployees | Department]) -> bool:\n        return ctx.output == ctx.expected_output\n\n\nhr = HR(\n    employees=[\n        HREmployee(id=1, name=\"John Doe\", department=HRDepartment(id=1, name=\"Sales\")),\n        HREmployee(id=2, name=\"Jane Doe\", department=HRDepartment(id=2, name=\"Marketing\")),\n        HREmployee(id=3, name=\"Jim Doe\", department=HRDepartment(id=3, name=\"Engineering\")),\n    ]\n)\n\nhr_dataset = Dataset[HRQuery, NumEmployees | Department](\n    cases=[\n        Case(\n            name=\"hr_q1\",\n            inputs=HRQuery(query=\"How many employees are there?\", deps=hr, result_type=NumEmployees),\n            expected_output=NumEmployees(num_employees=3),\n        ),\n        Case(\n            name=\"hr_q2\",\n            inputs=HRQuery(query=\"What is the department of John Doe?\", deps=hr, result_type=Department),\n            expected_output=Department(department_id=1, department_name=\"Sales\"),\n        ),\n    ],\n    evaluators=[EvaluateHR()],\n)\n\n\nhr_agent = Agent(model=\"google-gla:gemini-2.0-flash\", name=\"hr_agent\", deps_type=HR)\n\n\n@hr_agent.system_prompt\nasync def get_employees(ctx: RunContext[HR]) -> str:\n    return str(ctx.deps)\n\n\nasync def run_hr_query(query: HRQuery) -> NumEmployees | Department:\n    res = await hr_agent.run(query.query, deps=query.deps, result_type=query.result_type)\n    return res.data\n\n\nif __name__ == \"__main__\":\n    report = hr_dataset.evaluate_sync(task=run_hr_query)\n    report.print(include_input=True, include_output=True)\n\n```\n\nFor now, pylance does not have an issue, but I get:  \n\n> pydantic_core._pydantic_core.PydanticSerializationError: Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>",
      "state": "closed",
      "author": "HamzaFarhan",
      "author_type": "User",
      "created_at": "2025-04-01T06:09:29Z",
      "updated_at": "2025-04-01T22:00:59Z",
      "closed_at": "2025-04-01T22:00:58Z",
      "labels": [
        "evals"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1325/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1325",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1325",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:26.752716",
      "comments": [
        {
          "author": "dmontagu",
          "body": "With #1333, the example script succeeds and I get the following content printed:\n```\n                                            Evaluation Summary: run_hr_query                                            \n┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━",
          "created_at": "2025-04-01T20:50:02Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "Amazing! Thank you so much 🚀 ",
          "created_at": "2025-04-01T22:00:58Z"
        }
      ]
    },
    {
      "issue_number": 110,
      "title": "Toolsets, OpenAPI and Model Context Protocol",
      "body": "TL;DR I spoke to @dsp on Friday, and the plan is to add support for MCP to PydanticAI via something I'm currently calling \"toolsets\".\r\n\r\nOne of the things I've realised while building PydanticAI is that building tools to give agents access to resources is going to be quite resource intensive and repetative — say you want to build an agent to interact with slack, 90% of your time will be spend figuring out the slack API and dealing with auth and reties, not building the agent.\r\n\r\nI therefore propose the following process to make this kind of thing easy with PydanticAI:\r\n1. Add the concept of reusable \"Toolsets\" which contain multiple tools, their own state (e.g. http connection) and their own relation to agent dependencies\r\n2. Add a Toolset to support OpenAPI scheme APIs - e.g. so you could register an API which has an OpenAPI schema in a few lines of code - this should cover many common cases where you don't need full bidirectional communications\r\n3. Add support for registering MCP toolsets\r\n4. Create our own toolsets for interacting with common tools like slack, gmail, search etc. - this should be simple since I think many of these are already build for MCP, see [here](https://github.com/modelcontextprotocol/servers)",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2024-12-02T11:21:11Z",
      "updated_at": "2025-04-01T12:59:27Z",
      "closed_at": "2025-04-01T12:59:20Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 18,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/110/reactions",
        "total_count": 70,
        "+1": 34,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 14,
        "rocket": 14,
        "eyes": 8
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/110",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/110",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:26.998376",
      "comments": [
        {
          "author": "skylarbpayne",
          "body": "Was instantly thinking of this as I saw the release. I would love to help with this if you're looking for some extra hands!",
          "created_at": "2024-12-03T07:02:19Z"
        },
        {
          "author": "phil65",
          "body": "Hello! Just stumbled over this, I´m working on a project which incoroporates many of the same ideas, but in a different way. It´s basically a backend for both mcp servers as well as pydantic-ai agents. Perhaps it´s interesting for you. :)\r\n\r\nhttps://github.com/phil65/LLMling",
          "created_at": "2024-12-10T22:42:33Z"
        },
        {
          "author": "arjuna-dev",
          "body": "This framework has a very nice collection of tools and keeps everything quite simple. Only a few of them are API related but a lot of them seem to be quite useful:\r\n\r\n[https://docs.griptape.ai/stable/griptape-tools/](https://docs.griptape.ai/stable/griptape-tools/)",
          "created_at": "2025-01-07T15:34:10Z"
        },
        {
          "author": "datajoely",
          "body": "Hi Everyone, excited to see where this goes. The link between OpenAPI, Pydantic, FastAPI is so well established you don't need me to explain it! MCP is looking like the equivalent of OpenAPI for this world, so naturally we're starting to familiar looking [libraries like this](https://github.com/jlow",
          "created_at": "2025-01-13T11:10:21Z"
        },
        {
          "author": "slavakurilyak",
          "body": "+1 for Model Context Protocol support as their open-source tooling has grown quite a bit over the past 60 days\n\nToolsets can also assist in dealing with third-party data which is often raw and unstructured",
          "created_at": "2025-01-26T02:57:39Z"
        }
      ]
    },
    {
      "issue_number": 765,
      "title": "Endless Tool Loop (Vertex Gemini)",
      "body": "Hi, thanks for the great framework - it's been very productive for me so far. I've been hitting an issue implementing an agentic pattern using some tools that I was hoping to get some direction on. I'm not sure if it can be handled through the type system, prompting, or the agent/model arguments. I've tried variations on all of them with seemingly little effect.\n\nHigh level, I have one top level `Agent` that has tools that call out to sub-agents, - a data agent and a news agent. These agents have tools that get data and news and summarize what they find as they relate to the query. \n\nI'm hitting an issue with the news agent `get_articles` tool where it seems to get placed into an endless loop, calling the news API over and over (getting the same results) and eventually leading to getting 429 (Resource Exhausted) responses from the Gemini API. Using the debugger, I can see that `news_agent.run(...)` is only getting invoked once, so the issue seems to be with how news_agent is converting its tool return type into its agent return type (`list[WPArticle] -> list[NewsSummary]`). I think understanding how this works and being able to manage the control flow of agent tools on some level will be pretty important for developers of agentic applications like this.\n\nI've tried various configurations on `NewsAgent`, including \n`end_strategy=\"early\",\n  defer_model_check=False`, which don't seem to have any effect on the behavior. Every tool has the `retries=1` parameter set too, and explicit system prompting to only invoke said tool one time.\n\n\n```python\nfrom pydantic_ai import Agent, RunContext\nimport yaml\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai.models.vertexai import VertexAIModel\nfrom datetime import datetime\nfrom typing import Optional\nfrom io import StringIO\n\n\nclass WPArticle(BaseModel):\n    # omitted for brevity\n    pass\n\n\nclass NewsRequest(BaseModel):\n    query: str = Field(\n        ...,\n        description=\"(individual components of the user's query with locations, topics)\",\n    )\n    start_date: Optional[datetime] = Field(\n        None,\n        description=\"The Start Date for the user's query. Only include this if it is explicitly requested.\",\n    )\n    end_date: Optional[datetime] = Field(\n        None,\n        description=\"The End Date for the user's query. Only include this if it is explicitly requested.\",\n    )\n\n\nclass NewsSummary(BaseModel):\n    date: datetime = Field(..., description=\"The date the article was published on\")\n    summary: str = Field(\n        ...,\n        description=\"A brief summary of the article, and how it may relate to the user's query\",\n    )\n    title: str = Field(..., description=\"The title of the article\")\n    link: str = Field(..., description=\"The link to the article\")\n\n    def to_yaml(self) -> str:\n        json_body = self.model_dump(mode=\"json\")\n        return yaml.dump(json_body)\n\nnews_agent = Agent(\n    \"google-vertex:gemini-1.5-flash\",\n    system_prompt=[\n        \"You are an expert at searching ___ news articles, using sematic search terms are included or are related to their query, and summarizing their content as they relate to user queries. You call your tool exactly one time to find relevant results - the responses to your tool calls will not change.\",\n    ],\n    model_settings={\"temperature\": 0, \"timeout\": 15},\n    result_type=list[NewsSummary],\n    # end_strategy=\"early\",\n    defer_model_check=False,\n)\n\n\n@news_agent.tool_plain(retries=1)\nasync def get_articles(search: NewsRequest) -> list[WPArticle]:\n    \"\"\"\n    Fetch articles from the ___ API\n    * Use this tool ONE TIME PER RUN to retrieve relevant articles for a search string.\n\n    Args:\n        search: NewsRequest.\n            object to search news articles with, including:\n                 query: str (individual components of the user's query with locations, topics)\n                 start_date: date (optional)\n                 end_date: date (optional)\n\n    Returns:\n        A list of WPArticle objects representing the article contents, date, and title. This list may be empty if no records were found.\n    \"\"\"\n    logging.info(\"Running get_articles...\")\n    # return []\n    query = search.query.replace('\"\"', \"\")\n    async with NewsApiClient() as client:\n        res = await client.get_articles(\n            search=query, start_date=search.start_date, end_date=search.end_date\n        )\n        return res  # I can see that this returns the top 5 articles as WPArticles (pydantic models) related to the user query when debugging - but the tool evaluates over and over, having the effect of spamming the API I'm interacting with. It's the same search and response every time.\n\n\nresponse_agent = Agent(\"google-vertex:gemini-1.5-pro\")\n\n\n@response_agent.tool(retries=1)\nasync def get_data(ctx: RunContext, query: str):\n    # ... longer running op than news\n    pass\n\n\n@response_agent.tool(retries=1)\nasync def get_news(ctx: RunContext, query: str) -> str:\n    \"\"\"\n    Tool to page our org's news API and summarize articles relevant to a user's query.\n    \"\"\"\n    response = await news_agent.run(query)\n    with StringIO() as buffer:\n        for item in response.data:\n            buffer.write(item.to_yaml())\n        return buffer.getvalue()\n\n```\n\nIs there some way that I can better indicate to Gemini through the Agent/tool that the results of a tool call are idempotent, given the same input, and that it should only invoke that tool once while attempting to converge?",
      "state": "open",
      "author": "jtbaker",
      "author_type": "User",
      "created_at": "2025-01-24T19:04:19Z",
      "updated_at": "2025-04-01T11:44:08Z",
      "closed_at": null,
      "labels": [
        "Stale",
        "need confirmation"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/765/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/765",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/765",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:29.118750",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "@jtbaker for scenarios like this, you might want to leverage caching with a K-V store so that if a specific request has already been made within a specified time frame, you could simply return the cached results and not reach out to the remote service for a while. It could help you avoid the 429 err",
          "created_at": "2025-01-24T19:12:50Z"
        },
        {
          "author": "jtbaker",
          "body": "> [@jtbaker](https://github.com/jtbaker) for scenarios like this, you might want to leverage caching with a K-V store so that if a specific request has already been made within a specified time frame, you could simply return the cached results and not reach out to the remote service for a while. It ",
          "created_at": "2025-01-24T19:15:23Z"
        },
        {
          "author": "Kludex",
          "body": "@jtbaker Sorry for the delay here.\n\nCan you please share a code I can copy/paste and it works?",
          "created_at": "2025-04-01T11:44:07Z"
        }
      ]
    },
    {
      "issue_number": 1324,
      "title": "Possible outdated document on state persistence",
      "body": "From [pydantic-ai doc on graph](https://ai.pydantic.dev/graph/#stateful-graphs)\n>In the future, we intend to extend pydantic-graph to provide state persistence with the state recorded after each node is run, see [#695](https://github.com/pydantic/pydantic-ai/issues/695).\n\nLater in the same doc, under [State persistence](https://ai.pydantic.dev/graph/#state-persistence)\n>pydantic-graph provides state persistence — a system for snapshotting the state of a graph run before and after each node is run...\n\nThese sounds contradicting.\n\n\n",
      "state": "open",
      "author": "xtfocus",
      "author_type": "User",
      "created_at": "2025-04-01T03:41:27Z",
      "updated_at": "2025-04-01T11:41:05Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1324/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1324",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1324",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:29.389494",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "yup, it's wrong. PR welcome to fix it, otherwise I'll we'll get to it.",
          "created_at": "2025-04-01T11:32:37Z"
        }
      ]
    },
    {
      "issue_number": 928,
      "title": "Retry Handling for 429 Errors in Pydantic AI Agent Tools Not Working",
      "body": "Hello! \nI am attempting to handle `429 Too Many Requests` errors when executing tools in a `pydantic-ai` _Agent_ using the `tenacity` library for retries. However, the retry mechanism only works when applied to the function that runs the entire agent (e.g., `run_agent`). This approach is not ideal because restarting the agent from scratch for each 429 error is inefficient.\n\nWhen I attempt to apply the `@retry` decorator to individual _tools_ (such as `fetch_website_content` and `analyze_competition`), the retry logic does not trigger as expected. Instead, the agent fails immediately when encountering a 429 error within a tool.\n\n#### **Code Snippets**\n\n1. **Retry Decorator Implementation**\n   ```python\n   from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception, before_sleep_log\n   import logging\n\n   MAX_RETRY_ATTEMPTS = 3\n   RETRY_WAIT_SECONDS = 60\n   RETRY_STATUS_CODES = {429, 503, 504, 500}\n\n   logger = logging.getLogger(__name__)\n\n   def is_retryable_exception(exception):\n       if hasattr(exception, \"response\") and getattr(exception.response, \"status_code\", None) in RETRY_STATUS_CODES:\n           logging.warning(\"Retrying due to status code...\")\n           return True\n       elif isinstance(exception, Exception) and any(str(code) in str(exception) for code in RETRY_STATUS_CODES):\n           logging.warning(\"Retrying due to matching error message...\")\n           return True\n       return False\n   ```\n\n2. **Applying Retry to Tools**\n   ```python\n   @retry(\n       stop=stop_after_attempt(MAX_RETRY_ATTEMPTS),\n       wait=wait_fixed(RETRY_WAIT_SECONDS),\n       retry=retry_if_exception(is_retryable_exception),\n       reraise=True,\n       before_sleep=before_sleep_log(logger, logging.WARNING),\n   )\n   @swot_agent.tool(retries=3)\n   async def fetch_website_content(_ctx: RunContext[SwotAgentDeps], url: str) -> str:\n       \"\"\"Fetches the HTML content of the given URL.\"\"\"\n       async with httpx.AsyncClient(follow_redirects=True) as http_client:\n           try:\n               response = await http_client.get(url)\n               response.raise_for_status()\n               return response.text\n           except httpx.HTTPError as e:\n               logging.info(f\"Request failed: {str(e)}\")\n               raise e\n   ```\n\n3. **Retry Works Only on the Agent Run**\n   ```python\n   @retry(\n       stop=stop_after_attempt(MAX_RETRY_ATTEMPTS),\n       wait=wait_fixed(RETRY_WAIT_SECONDS),\n       retry=retry_if_exception(is_retryable_exception),\n       reraise=True,\n       before_sleep=before_sleep_log(logger, logging.WARNING),\n   )\n   async def run_agent(url: str = ANALYZE_URL, deps: SwotAgentDeps = SwotAgentDeps()) -> SwotAnalysis | Exception:\n       \"\"\"Runs the SWOT analysis agent.\"\"\"\n       try:\n           result = await swot_agent.run(\n               f\"Perform a comprehensive SWOT analysis for this product: {url}\",\n               deps=deps,\n           )\n           return result.data\n       except Exception as e:\n           logging.exception(f\"Error during agent run: {e}\")\n           raise e\n   ```\n\n\n#### **Questions**\n1. Does `pydantic-ai` handle exceptions raised within tools in a way that interferes with `tenacity`’s retry mechanism?\n2. Is there a recommended way to handle retries for individual tool executions without restarting the whole agent?\n3. Are there any known limitations or interactions between `pydantic-ai`'s tool execution and external retry decorators like `tenacity`?\n\n#### **Additional Context**\n- The agent is built using `pydantic-ai` with a `VertexAIModel` backend.\n- Tools are defined using `@swot_agent.tool()` and are expected to handle HTTP requests and external API calls.\n- The issue occurs when executing tools that make requests to external services like Gemini AI or HTTP endpoints.\n\nAny guidance or recommended fixes would be appreciated!",
      "state": "closed",
      "author": "iacobellisdylog",
      "author_type": "User",
      "created_at": "2025-02-14T16:29:58Z",
      "updated_at": "2025-04-01T11:40:43Z",
      "closed_at": "2025-04-01T11:40:43Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/928/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sydney-runkle"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/928",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/928",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:29.658470",
      "comments": [
        {
          "author": "sydney-runkle",
          "body": "This should be solved with #516 support!",
          "created_at": "2025-02-17T15:43:17Z"
        },
        {
          "author": "Kludex",
          "body": "Closing this, since now we have `FallbackModel`.",
          "created_at": "2025-04-01T11:40:43Z"
        }
      ]
    },
    {
      "issue_number": 1184,
      "title": "Agent Losing track of small and simple conversation",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nHello everyone! Hope you're doing great!\n\nFirst of all, thanks a lot for this awesome project - It's easy enough for a beginner to understand.\n\nI have found this weird behavior where an agent will \"forget\" all the past interactions suddenly - And I've checked both with all_messages and the messages history stored on the DB - And messages are available to the agent.\n\nWeird thing is that this happens randomly...\n\nBut I see that something that may trigger agent going \"out of role\" is saying something repeatedly like \"Good morning\" At a given point he'll forget the user name and ask it again, even with a short context like 10 messages...\n\nExample intetraction:\n\nuser: Hello\nagent: Hello! Thanks for your message. May I know your name?\n\nuser: Sure! It's Erikson\nagent: Hello, Erikson! Pleased to meet you. How can I assist you today?\n\nuser: Good night\nagent: Hello! Thanks for your contact. What's your name?\n\n---\n\nI have tried adjusting the prompt, the code and everything, but I still get this.\nI have updated to the latest version but no change.\n\nThanks a lot in advance!\n\n### Example Code\n\n```Python\nprevious_messages = await get_history(user_id, 50)\n\nagent_persona = await get_agent_persona()\n\nmessage_history = [\n    ModelRequest(parts=[SystemPromptPart(content=agent_persona)])\n]\n\nfor record in previous_messages:\n    if record[\"role\"] == \"user\":\n        message_history.append(\n            ModelRequest(parts=[UserPromptPart(content=record[\"content\"])])\n        )\n    else:  # Assumimos que qualquer outro valor significa que veio do modelo\n        message_history.append(\n            ModelResponse(parts=[TextPart(content=record[\"content\"])])\n        )\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nCurrently gemini-flash-2.0, but also happened with gpt-4o-mini and Llama-70b-specdec.\n```",
      "state": "closed",
      "author": "eriksonssilva",
      "author_type": "User",
      "created_at": "2025-03-20T03:12:49Z",
      "updated_at": "2025-04-01T02:53:44Z",
      "closed_at": "2025-04-01T02:53:44Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1184/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1184",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1184",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:29.913419",
      "comments": [
        {
          "author": "Kludex",
          "body": "Can you share a snippet I can reproduce what you said? The code provided has missing imports. ",
          "created_at": "2025-03-20T06:43:32Z"
        },
        {
          "author": "eriksonssilva",
          "body": "Hello there @Kludex ! First of all, thanks a lot for answering here!\nSecond: I'm sorry for the delay, I did not receive an email from github and I just saw your answer by chance!\n\nI have written a standalone implementation to picture what I am facing:\n\n``` python\nfrom typing import List, Dict, Any\nf",
          "created_at": "2025-03-24T04:07:12Z"
        },
        {
          "author": "eriksonssilva",
          "body": "Upon further investigation, a lot of swear and seating blood, it turns out that the CONTEXT was not being lost. My agent could remember things perfectly, it just did not follow the instructions as it should, so this is not Pydantic AI's fault.\nI'm sorry for opening this issue.\nI appreciate the time ",
          "created_at": "2025-04-01T02:53:41Z"
        }
      ]
    },
    {
      "issue_number": 1232,
      "title": "Cannot install b/c logfite & pydantic-ai-slim have conflicting dependencies",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nUsing python 3.11, when i run `poetry add pydantic-ai[logfire]` in the context of my library, I am no longer able to install/deploy my library. I have the following error message:\n\n```\nERROR: Cannot install logfire and pydantic-ai-slim[anthropic,bedrock,cli,cohere,groq,mcp,mistral,openai,vertexai]==0.0.43 because these package versions have conflicting dependencies.\n```\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.11\nsame error message appears with pydantic-ai 0.0.43 and 0.0.41\n```",
      "state": "closed",
      "author": "lukaszdz",
      "author_type": "User",
      "created_at": "2025-03-25T00:15:08Z",
      "updated_at": "2025-03-31T18:52:54Z",
      "closed_at": "2025-03-31T18:52:53Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1232/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1232",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1232",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:30.147800",
      "comments": [
        {
          "author": "Kludex",
          "body": "I can't reproduce it. Can you show the full output of that error? Also, can you try to reproduce it on a new environment?",
          "created_at": "2025-03-31T09:42:43Z"
        },
        {
          "author": "lukaszdz",
          "body": "The next time I attempt to install and deploy our lib including logfire, will share the full error. No plans to do this soon since the library is a nice to have, and this dependency resolution/deploy issue cost us > half a day.",
          "created_at": "2025-03-31T17:55:22Z"
        },
        {
          "author": "Kludex",
          "body": "It was probably the side effect of another transient dependency.\n\nI can't really help without more information. Feel free to create another issue, or reach us on Slack if you get more information. 🙏 ",
          "created_at": "2025-03-31T18:52:53Z"
        }
      ]
    },
    {
      "issue_number": 1312,
      "title": "Custom run name for telemetry use",
      "body": "AFAIK, all Agent runs use the Agent name as the run name logged in telemetry services like Logfire. Is it possible to use a custom name for each `agent.run` call?\n\nBasically, my Logfire view looks like this right now:\n\n<img width=\"728\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9451b3aa-e4e6-4df8-be4f-6a619b276316\" />\n\nAnd I'd like to make it look like this:\n\n<img width=\"728\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/41a4c30b-9fdb-43a6-90c3-a13e66095880\" />\n\nWithout having to change the Agent's name every time",
      "state": "closed",
      "author": "frabert",
      "author_type": "User",
      "created_at": "2025-03-31T13:19:23Z",
      "updated_at": "2025-03-31T14:53:17Z",
      "closed_at": "2025-03-31T14:53:16Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1312/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1312",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1312",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:30.394132",
      "comments": [
        {
          "author": "alexmojaki",
          "body": "You could do this:\n\n```python\nwith logfire.span(f'Run with {param=}'):\n    await agent.run(...)\n```",
          "created_at": "2025-03-31T14:17:43Z"
        },
        {
          "author": "frabert",
          "body": "That works, thanks!",
          "created_at": "2025-03-31T14:53:16Z"
        }
      ]
    },
    {
      "issue_number": 1099,
      "title": "Support OpenAI new response API format and new tools",
      "body": "### Description\n\nContext: https://openai.com/index/new-tools-for-building-agents/\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "odysseus0",
      "author_type": "User",
      "created_at": "2025-03-11T17:56:34Z",
      "updated_at": "2025-03-31T14:31:33Z",
      "closed_at": "2025-03-31T14:31:33Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1099/reactions",
        "total_count": 13,
        "+1": 13,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1099",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1099",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:30.663282",
      "comments": [
        {
          "author": "svilupp",
          "body": "+1 Because OpenAI is gating some of their new models behind this new API, eg, their latest model finetunes like `computer-use-preview`\n\nFor reference: https://platform.openai.com/docs/quickstart?api-mode=responses\n\nQuick reference:\n```python\nimport httpx\nimport os\nimport base64\nimport json\n\nwith ope",
          "created_at": "2025-03-12T09:12:30Z"
        }
      ]
    },
    {
      "issue_number": 803,
      "title": "Exclude model field from result",
      "body": "What I would like to do is have a pydantic model where the AI model call should fill out all the fields but one or two.\n\nThe idea here is I'm looping through a list of elements that need to be processed by a particular agent. I have an ID or some sort of deterministic value that I want set on each result and I don't want the agent to touch that field. I could create two separate pydantic models to represent these two states, but I'd rather have a single pydantic model and somehow mark one of the fields as handled by me instead of handled by the agent.\n\nIs this sort of thing possible?\n\nHere's a quick example.\n\n```python\nclass LabelResult(BaseModel):\n    labels: list[str]\n    reasoning: str\n    summary: str\n    message_id: str # I want the model to avoid filling this field in\n```",
      "state": "closed",
      "author": "iloveitaly",
      "author_type": "User",
      "created_at": "2025-01-29T14:17:01Z",
      "updated_at": "2025-03-31T14:00:43Z",
      "closed_at": "2025-03-31T14:00:43Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/803/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sydney-runkle"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/803",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/803",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:30.876377",
      "comments": [
        {
          "author": "HamzaFarhan",
          "body": "Yes. Like this:\n\n```python\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import SkipJsonSchema\n\n\nclass LabelResult(BaseModel):\n    labels: list[str]\n    reasoning: str\n    summary: str\n    message_id: SkipJsonSchema[str]\n\n\nLabelResult.model_json_schema()\n\n{'properties': {'labels': {'items",
          "created_at": "2025-01-29T20:52:26Z"
        },
        {
          "author": "mikeedjones",
          "body": "Think that would mean this field will be missing if the schema is reproduced for another reason - like the swagger docs of a fastapi deployment which uses this model.",
          "created_at": "2025-01-30T08:02:19Z"
        },
        {
          "author": "iloveitaly",
          "body": "@mikeedjones yeah, ideally the openapi stuff would have this field as an optional field. I just want to exclude certain fields from the AI structured response.\n\nThis is definitely a solid temporary workaround though (I'm not connecting this code to an API _yet_ although that is the next step).",
          "created_at": "2025-01-30T16:00:10Z"
        },
        {
          "author": "mikeedjones",
          "body": "Yeah, would be cool to have an annotation or type which operated like `SkipJsonSchema` - `SkipGeneration` or something. pydantic-ai looks to use pydantic's `GenerateJsonSchema`. I think you could add a context var referenced by `__get_pydantic_json_schema__` to condition the raising of `PydanticOmit",
          "created_at": "2025-01-30T16:27:19Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-02-07T14:07:03Z"
        }
      ]
    },
    {
      "issue_number": 1162,
      "title": "Need help: I don't understand how tools work",
      "body": "I have simple agent with some mocked tools, but the tools are NEVER called ever.\nI had similar script without any frameworks and tool calls worked properly.\n```py\nimport random\nimport string\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\nfrom rich.console import Console\n\nfrom config.groq import groq_settings\nfrom utils.logger import log_error, log_info, log_panel_info, log_warning\n\nconsole = Console()\n\ndriver_phone_number: str = \"+37065315248\"\n\ndef generate_reference_number(driver_phone_number: str) -> str:\n    \"\"\"Generate a 10-character alphanumeric reference number using driver's phone number as seed.\n    \n    Args:\n        driver_phone_number (str): The phone number of the driver.\n    Returns:\n        str: A 10-character alphanumeric reference number.\n    \"\"\"\n    seed = sum(ord(c) for c in driver_phone_number)\n    random.seed(seed)\n\n    characters = string.ascii_uppercase + string.digits\n    reference_number = ''.join(random.choices(characters, k=10))\n    \n    return reference_number\n\nclass OutputSchema(BaseModel):\n    \"\"\"Output schema for the CategoryRouter.\"\"\"\n\n    message: str = Field(..., description=\"The message or question to send to the driver about reference number\")\n    finished: bool = Field(..., description=\"Whether the driver is done discussing reference number\")\n    finished_reasoning: str = Field(..., description=\"Detailed explanation of why the driver is done discussing reference number or not\")\n\n\nsystem_prompt = f\"\"\"\n    # IDENTITY and PURPOSE\n    - You are a <Reference Number> AI assistant in logistics company, specialized in sending drivers reference numbers for loading or unloading locations.\n    - You will use TOOLS to get reference number for loading or unloading location.\n    - You will use TOOL to send reference number to the driver via SMS.\n    - You must only talk about reference numbers and nothing else.\n    - Driver's phone number is: {driver_phone_number}, you must use this number for tool calls.\n    # INTERNAL ASSISTANT STEPS\n    - 1. First you need to figure out if the driver is asking for reference number for loading or unloading location.\n    - 2. You will have two TOOLS to help you with getting apropriate reference number:\n    -- 'get_reference_number_for_loading' - returns reference number for loading location\n    -- 'get_reference_number_for_unloading' - returns reference number for unloading location\n    - 3. After you've got reference number, tell it to the driver and ask if he wants it sent to his phone number.\n    - 4. If driver says he wants reference number sent to him, call 'send_reference_number_to_driver' tool.\n    - 5. When you've done getting and sending reference number, set 'finished' to True\n    # OUTPUT INSTRUCTIONS\n    -Output a JSON object with three fields:\n    -- 'finished_reasoning' (detailed explanation of why the driver is done discussing reference number)\n    -- 'finished' (a boolean that is true if the driver confirms that he is done discussing reference number, or false if he wants to continue)\n    -- 'message' (the message to send to the driver about reference number)\n    - Your reasoning must clearly state WHY you chose to set 'finished' to true or false\n    - Your decision must match your reasoning - don't contradict yourself\n    - You must only talk about reference numbers and nothing else.\n    - You must only use tools to get reference number and send it to the driver.\n    - Your communication must be short and concise.\"\n\"\"\"\n\nlog_panel_info(f\"System prompt: {system_prompt}\", title=\"System Prompt\")\n\nreference_number_event_agent = Agent[OutputSchema](\n    model=GroqModel(groq_settings.GROQ_MODEL_LLAMA_70_V, provider=GroqProvider(api_key=groq_settings.GROQ_API_KEY)),\n    system_prompt=system_prompt,\n    result_type=OutputSchema,\n    instrument=True,\n)\n\n@reference_number_event_agent.tool_plain\ndef get_reference_number_for_loading() -> str:\n    \"\"\"Call this tool when driver is asking for reference number for loading location.\"\"\"\n    reference_number = generate_reference_number(driver_phone_number)\n    log_info(f\"Reference number {reference_number} generated for LOADING location\")\n    return f\"Reference number is: {reference_number}\"\n\n@reference_number_event_agent.tool_plain\ndef get_reference_number_for_unloading() -> str:\n    \"\"\"Call this tool when driver is asking for reference number for unloading location.\"\"\"\n    reference_number = generate_reference_number(driver_phone_number)\n    log_info(f\"Reference number {reference_number} generated for UNLOADING location\")\n    return f\"Reference number is: {reference_number}\"\n\n@reference_number_event_agent.tool_plain\ndef send_reference_number_to_driver(reference_number: str) -> bool:\n    \"\"\"Send reference number to driver.\n\n    Args:\n        ctx: The run context\n        driver_phone_number (str): The phone number of the driver.\n        reference_number (str): The reference number to send to the driver.\n\n    Returns:\n        bool: Whether the reference number was sent successfully.\n    \"\"\"\n\n    success = True\n\n    if success:\n        log_info(f\"Reference number {reference_number} sent to {driver_phone_number}\")\n    else:\n        log_error(f\"Failed to send reference number {reference_number} to {driver_phone_number}\")\n\n    return success\n\n# Define the main function\ndef main():\n    all_messages: list[ModelMessage] | None = None\n    while True:\n        try:\n            user_input = console.input(\"\\n[bold blue]You:[/bold blue]\")\n        except (KeyboardInterrupt, EOFError):\n            log_warning(\"Exiting...\")\n            break\n        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n            log_info(\"👋 Goodbye!\")\n            break\n\n        if all_messages is None:\n            all_messages = []\n\n        result = reference_number_event_agent.run_sync(user_input, message_history=all_messages)\n\n        all_messages = result.all_messages()\n\n        console.print(all_messages)\n        console.print(result)\n        console.print(result.data.message)\n\nif __name__ == \"__main__\":\n    main()\n```\nBut this is what I get in terminal:\n```\nYou:hi i need reference number for unloading\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='\\n    # IDENTITY and PURPOSE\\n    - You are a <Reference Number> AI assistant in logistics company, specialized in sending drivers reference numbers for loading or unloading        \nlocations.\\n    - You will use TOOLS to get reference number for loading or unloading location.\\n    - You will use TOOL to send reference number to the driver via SMS.\\n    - You must only talk about      \nreference numbers and nothing else.\\n    - Driver\\'s phone number is: +37065315248, you must use this number for tool calls.\\n    # INTERNAL ASSISTANT STEPS\\n    - 1. First you need to figure out if the    \ndriver is asking for reference number for loading or unloading location.\\n    - 2. You will have two TOOLS to help you with getting apropriate reference number:\\n    -- \\'get_reference_number_for_loading\\' \n- returns reference number for loading location\\n    -- \\'get_reference_number_for_unloading\\' - returns reference number for unloading location\\n    - 3. After you\\'ve got reference number, tell it to the \ndriver and ask if he wants it sent to his phone number.\\n    - 4. If driver says he wants reference number sent to him, call \\'send_reference_number_to_driver\\' tool.\\n    - 5. When you\\'ve done getting andsending reference number, set \\'finished\\' to True\\n    # OUTPUT INSTRUCTIONS\\n    -Output a JSON object with three fields:\\n    -- \\'finished_reasoning\\' (detailed explanation of why the driver is done    \ndiscussing reference number)\\n    -- \\'finished\\' (a boolean that is true if the driver confirms that he is done discussing reference number, or false if he wants to continue)\\n    -- \\'message\\' (the      \nmessage to send to the driver about reference number)\\n    - Your reasoning must clearly state WHY you chose to set \\'finished\\' to true or false\\n    - Your decision must match your reasoning - don\\'t     \ncontradict yourself\\n    - You must only talk about reference numbers and nothing else.\\n    - You must only use tools to get reference number and send it to the driver.\\n    - Your communication must be   \nshort and concise.\"\\n',\n                dynamic_ref=None,\n                part_kind='system-prompt'\n            ),\n            UserPromptPart(content='hi i need reference number for unloading', timestamp=datetime.datetime(2025, 3, 18, 14, 38, 23, 345314, tzinfo=datetime.timezone.utc), part_kind='user-prompt')\n        ],\n        kind='request'\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='final_result',\n                args='{\"message\": \"Do you want the reference number sent to your phone number +37065315248?\", \"finished\": false, \"finished_reasoning\": \"The driver is asking for a reference number for       \nunloading location, I will get it using the tool.\"}',\n                tool_call_id='call_hpsw',\n                part_kind='tool-call'\n            ),\n            ToolCallPart(tool_name='get_reference_number_for_unloading', args='{}', tool_call_id='call_tp4q', part_kind='tool-call')\n        ],\n        model_name='llama-3.3-70b-versatile',\n        timestamp=datetime.datetime(2025, 3, 18, 14, 38, 23, tzinfo=datetime.timezone.utc),\n        kind='response'\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='final_result',\n                content='Final result processed.',\n                tool_call_id='call_hpsw',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 24, 194479, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            ),\n            ToolReturnPart(\n                tool_name='get_reference_number_for_unloading',\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_tp4q',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 24, 194482, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            )\n        ],\n        kind='request'\n    )\n]\nAgentRunResult(\n    data=OutputSchema(\n        message='Do you want the reference number sent to your phone number +37065315248?',\n        finished=False,\n        finished_reasoning='The driver is asking for a reference number for unloading location, I will get it using the tool.'\n    )\n)\nDo you want the reference number sent to your phone number +37065315248?\n```\nAsk tell me ref number:\n```\nYou:Tell me the reference numner...\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='\\n    # IDENTITY and PURPOSE\\n    - You are a <Reference Number> AI assistant in logistics company, specialized in sending drivers reference numbers for loading or unloading        \nlocations.\\n    - You will use TOOLS to get reference number for loading or unloading location.\\n    - You will use TOOL to send reference number to the driver via SMS.\\n    - You must only talk about      \nreference numbers and nothing else.\\n    - Driver\\'s phone number is: +37065315248, you must use this number for tool calls.\\n    # INTERNAL ASSISTANT STEPS\\n    - 1. First you need to figure out if the    \ndriver is asking for reference number for loading or unloading location.\\n    - 2. You will have two TOOLS to help you with getting apropriate reference number:\\n    -- \\'get_reference_number_for_loading\\' \n- returns reference number for loading location\\n    -- \\'get_reference_number_for_unloading\\' - returns reference number for unloading location\\n    - 3. After you\\'ve got reference number, tell it to the \ndriver and ask if he wants it sent to his phone number.\\n    - 4. If driver says he wants reference number sent to him, call \\'send_reference_number_to_driver\\' tool.\\n    - 5. When you\\'ve done getting andsending reference number, set \\'finished\\' to True\\n    # OUTPUT INSTRUCTIONS\\n    -Output a JSON object with three fields:\\n    -- \\'finished_reasoning\\' (detailed explanation of why the driver is done    \ndiscussing reference number)\\n    -- \\'finished\\' (a boolean that is true if the driver confirms that he is done discussing reference number, or false if he wants to continue)\\n    -- \\'message\\' (the      \nmessage to send to the driver about reference number)\\n    - Your reasoning must clearly state WHY you chose to set \\'finished\\' to true or false\\n    - Your decision must match your reasoning - don\\'t     \ncontradict yourself\\n    - You must only talk about reference numbers and nothing else.\\n    - You must only use tools to get reference number and send it to the driver.\\n    - Your communication must be   \nshort and concise.\"\\n',\n                dynamic_ref=None,\n                part_kind='system-prompt'\n            ),\n            UserPromptPart(content='hi i need reference number for unloading', timestamp=datetime.datetime(2025, 3, 18, 14, 38, 23, 345314, tzinfo=datetime.timezone.utc), part_kind='user-prompt')\n        ],\n        kind='request'\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='final_result',\n                args='{\"message\": \"Do you want the reference number sent to your phone number +37065315248?\", \"finished\": false, \"finished_reasoning\": \"The driver is asking for a reference number for       \nunloading location, I will get it using the tool.\"}',\n                tool_call_id='call_hpsw',\n                part_kind='tool-call'\n            ),\n            ToolCallPart(tool_name='get_reference_number_for_unloading', args='{}', tool_call_id='call_tp4q', part_kind='tool-call')\n        ],\n        model_name='llama-3.3-70b-versatile',\n        timestamp=datetime.datetime(2025, 3, 18, 14, 38, 23, tzinfo=datetime.timezone.utc),\n        kind='response'\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='final_result',\n                content='Final result processed.',\n                tool_call_id='call_hpsw',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 24, 194479, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            ),\n            ToolReturnPart(\n                tool_name='get_reference_number_for_unloading',\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_tp4q',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 24, 194482, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            )\n        ],\n        kind='request'\n    ),\n    ModelRequest(\n        parts=[UserPromptPart(content='Tell me the reference numner...', timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 137464, tzinfo=datetime.timezone.utc), part_kind='user-prompt')],\n        kind='request'\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='final_result',\n                args='{\"message\": \"Your reference number for unloading is XXXX, do you want it sent to your phone number +37065315248?\", \"finished\": false, \"finished_reasoning\": \"The driver is asking for   \nthe reference number, I will provide it and ask if he wants it sent to his phone.\"}',\n                tool_call_id='call_gerh',\n                part_kind='tool-call'\n            ),\n            ToolCallPart(tool_name='send_reference_number_to_driver', args='{}', tool_call_id='call_td9j', part_kind='tool-call')\n        ],\n        model_name='llama-3.3-70b-versatile',\n        timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, tzinfo=datetime.timezone.utc),\n        kind='response'\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='final_result',\n                content='Final result processed.',\n                tool_call_id='call_gerh',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930549, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            ),\n            ToolReturnPart(\n                tool_name='send_reference_number_to_driver',\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_td9j',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930552, tzinfo=datetime.timezone.utc),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='final_result',\n                args='{\"message\": \"Your reference number for unloading is XXXX, do you want it sent to your phone number +37065315248?\", \"finished\": false, \"finished_reasoning\": \"The driver is asking for   \nthe reference number, I will provide it and ask if he wants it sent to his phone.\"}',\n                tool_call_id='call_gerh',\n                part_kind='tool-call'\n            ),\n            ToolCallPart(tool_name='send_reference_number_to_driver', args='{}', tool_call_id='call_td9j', part_kind='tool-call')\n        ],\n        model_name='llama-3.3-70b-versatile',\n        timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, tzinfo=datetime.timezone.utc),\n        kind='response'\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='final_result',\n                content='Final result processed.',\n                tool_call_id='call_gerh',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930549, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            ),\n            ToolReturnPart(\n                tool_name='send_reference_number_to_driver',\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_td9j',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930552, tzinfo=datetime.timezone.utc),\nthe reference number, I will provide it and ask if he wants it sent to his phone.\"}',\n                tool_call_id='call_gerh',\n                part_kind='tool-call'\n            ),\n            ToolCallPart(tool_name='send_reference_number_to_driver', args='{}', tool_call_id='call_td9j', part_kind='tool-call')\n        ],\n        model_name='llama-3.3-70b-versatile',\n        timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, tzinfo=datetime.timezone.utc),\n        kind='response'\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='final_result',\n                content='Final result processed.',\n                tool_call_id='call_gerh',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930549, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            ),\n            ToolReturnPart(\n                tool_name='send_reference_number_to_driver',\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_td9j',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930552, tzinfo=datetime.timezone.utc),\n        model_name='llama-3.3-70b-versatile',\n        timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, tzinfo=datetime.timezone.utc),\n        kind='response'\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='final_result',\n                content='Final result processed.',\n                tool_call_id='call_gerh',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930549, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            ),\n            ToolReturnPart(\n                tool_name='send_reference_number_to_driver',\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_td9j',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930552, tzinfo=datetime.timezone.utc),\n                tool_name='final_result',\n                content='Final result processed.',\n                tool_call_id='call_gerh',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930549, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            ),\n            ToolReturnPart(\n                tool_name='send_reference_number_to_driver',\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_td9j',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930552, tzinfo=datetime.timezone.utc),\n                content='Tool not executed - a final result was already processed.',\n                tool_call_id='call_td9j',\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930552, tzinfo=datetime.timezone.utc),\n                timestamp=datetime.datetime(2025, 3, 18, 14, 38, 35, 930552, tzinfo=datetime.timezone.utc),\n                part_kind='tool-return'\n            )\n        ],\n        kind='request'\n    )\n]\nAgentRunResult(\n    data=OutputSchema(\n        message='Your reference number for unloading is XXXX, do you want it sent to your phone number +37065315248?',\nAgentRunResult(\n    data=OutputSchema(\n        message='Your reference number for unloading is XXXX, do you want it sent to your phone number +37065315248?',\n    data=OutputSchema(\n        message='Your reference number for unloading is XXXX, do you want it sent to your phone number +37065315248?',\n        message='Your reference number for unloading is XXXX, do you want it sent to your phone number +37065315248?',\n        finished=False,\n        finished_reasoning='The driver is asking for the reference number, I will provide it and ask if he wants it sent to his phone.'\n    )\n)\nYour reference number for unloading is XXXX, do you want it sent to your phone number +37065315248?\n```",
      "state": "closed",
      "author": "sarunas-llm",
      "author_type": "User",
      "created_at": "2025-03-18T14:45:37Z",
      "updated_at": "2025-03-31T14:00:41Z",
      "closed_at": "2025-03-31T14:00:41Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1162/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1162",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1162",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:31.170151",
      "comments": [
        {
          "author": "mdfareed92",
          "body": "You probably need to re-engineer your system prompt or simplify it because it seems that your model is confused with how to get the ref number and when to end the interaction.",
          "created_at": "2025-03-18T15:16:22Z"
        },
        {
          "author": "sarunas-llm",
          "body": "> You probably need to re-engineer your system prompt or simplify it because it seems that your model is confused with how to get the ref number and when to end the interaction.\n\nThanks for the reply, I now actually think there's a problem with:\n```\n    tools=[get_reference_number_for_loading, get_r",
          "created_at": "2025-03-19T07:03:28Z"
        },
        {
          "author": "Kludex",
          "body": "Hi @sarunas-llm , can you please provide everything I need to run in the python code without external imports? I would like to try your script locally to understand your issue.",
          "created_at": "2025-03-19T18:39:23Z"
        },
        {
          "author": "sarunas-llm",
          "body": "> Hi [@sarunas-llm](https://github.com/sarunas-llm) , can you please provide everything I need to run in the python code without external imports? I would like to try your script locally to understand your issue.\n\nHey, sure!\nMain file: (No external imports apart of provided API_KEY retrieval provide",
          "created_at": "2025-03-20T08:31:14Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Thanks @sarunas-llm, I was able to reproduce the issue.\n\ni've updated your markdown above to highlight the python code and make it easier to read.\n\nWhen I switch the model to `openai:gpt-4o` it's behaving as you'd expect, so there's nothing fundamentally wrong with your code (or PydanticAI) - just t",
          "created_at": "2025-03-20T09:25:45Z"
        }
      ]
    },
    {
      "issue_number": 81,
      "title": "Weird tool call arguments, resulting in UnexpectedModelBehaviour / validation error",
      "body": "See https://github.com/intellectronica/pydantic-ai-experiments/blob/main/scratch.ipynb\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/_result.py\", line 189, in validate\r\n    result = self.type_adapter.validate_json(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 425, in validate_json\r\n    return self.validator.validate_json(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\npydantic_core._pydantic_core.ValidationError: 2 validation errors for Question\r\nreflection\r\n  Field required [type=missing, input_value={'_': {'reflection': \"The...n': 'Is it an animal?'}}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\r\nquestion\r\n  Field required [type=missing, input_value={'_': {'reflection': \"The...n': 'Is it an animal?'}}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 654, in _handle_model_response\r\n    result_data = result_tool.validate(call)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/_result.py\", line 203, in validate\r\n    raise ToolRetryError(m) from e\r\npydantic_ai._result.ToolRetryError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 181, in run\r\n    either = await self._handle_model_response(model_response, deps)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 657, in _handle_model_response\r\n    self._incr_result_retry()\r\n  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 751, in _incr_result_retry\r\n    raise exceptions.UnexpectedModelBehavior(\r\npydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (1) for result validation\r\n```\r\n\r\nSee the prefixed `\"_\" :`? It's not there on earlier calls. Possibly a hallucination.\r\n\r\nI think this can be avoided with strict mode. Would be great to have it as an option for OpenAI calls.",
      "state": "open",
      "author": "intellectronica",
      "author_type": "User",
      "created_at": "2024-11-21T14:56:59Z",
      "updated_at": "2025-03-31T11:44:38Z",
      "closed_at": null,
      "labels": [
        "model settings"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/81/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sydney-runkle"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/81",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/81",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:31.411747",
      "comments": [
        {
          "author": "intellectronica",
          "body": "@samuelcolvin ^^^^^",
          "created_at": "2024-11-21T14:57:25Z"
        },
        {
          "author": "samuelcolvin",
          "body": "weird, not sure what's going on, I ran your code, and it worked first time.\r\n\r\nI ran it directly as a script, and it worked fine:\r\n\r\n```py\r\nfrom enum import Enum\r\nfrom textwrap import dedent\r\nfrom typing import List\r\n\r\nfrom pydantic import BaseModel, Field\r\nfrom pydantic_ai import Agent, CallContext",
          "created_at": "2024-11-21T18:28:09Z"
        },
        {
          "author": "intellectronica",
          "body": "Yes, it also works most of the time for me. Just not all the time. The LLMs\r\nare non-deterministic and sometimes they do weird things. My point is that\r\nit's good to be defensive, and strict mode is one way to do this.\r\n\r\nOn Thu, 21 Nov 2024 at 19:28, Samuel Colvin ***@***.***>\r\nwrote:\r\n\r\n> weird, n",
          "created_at": "2024-11-21T18:48:13Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Thanks, yup I'll look into it.",
          "created_at": "2024-11-21T19:24:57Z"
        },
        {
          "author": "samuelcolvin",
          "body": "@sydney-runkle we should add `strict` to model settings.",
          "created_at": "2024-12-18T15:50:30Z"
        }
      ]
    },
    {
      "issue_number": 1261,
      "title": "UnboundLocalError: cannot access 'next_node' when running with PYTHONOPTIMIZE=1",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI'm building a conversational tool based on AI agents using Pydantic AI. When deploying to AWS Lambda I found the following error: \n\n`...\nFile \".../.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 379, in run\n    return next_node\nUnboundLocalError: cannot access local variable 'next_node' where it is not associated with a value`\n\nI was trying to replicate the error in my local environment and found that the issue was due to the `Dockerfile` used for deployment defining the environment variable `PYTHONOPTIMIZE=1`, as is usually done for this type of deployment.\n\nIndeed, when the example code below is run with `python script.py`, no issues occur, but when it is run with `python -O script.py` (equivalent to PYTHONOPTIMIZE=1), the reported error appears.\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nimport asyncio\n\nagent = Agent('openai:gpt-4o-mini-2024-07-18')\n\nasync def main():\n    result = await agent.run('What is the capital of France?')\n    print(result.data)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython version: 3.11\npydantic-ai version: 0.0.46\nLLM: 'openai:gpt-4o-mini-2024-07-18'\n```",
      "state": "closed",
      "author": "jmortiz-syc",
      "author_type": "User",
      "created_at": "2025-03-27T15:18:30Z",
      "updated_at": "2025-03-31T10:02:52Z",
      "closed_at": "2025-03-31T10:02:52Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1261/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1261",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1261",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:31.645877",
      "comments": [
        {
          "author": "Kludex",
          "body": "Thanks.\n\nIt should be solved on #1307 :)",
          "created_at": "2025-03-31T09:58:39Z"
        }
      ]
    },
    {
      "issue_number": 1308,
      "title": "Error in multi-agent example: pydantic_ai.exceptions.UsageLimitExceeded: Exceeded the total_tokens_limit of 300 (total_tokens=353)",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nHi everyone. I have been trying to run stuff in the documentation. When I tried running multi-agent examples: https://ai.pydantic.dev/multi-agent-applications/, in the agent delegation part, I got this error:\n\n> Traceback (most recent call last):\n>   File \"C:\\TRUONG\\Code_tu_hoc\\AI_agent_tutorials\\agent_delegation_simple.py\", line 53, in <module>\n>     result = joke_selection_agent.run_sync(\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_ai\\agent.py\", line 570, in run_sync\n>     return get_event_loop().run_until_complete(\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\n>     return future.result()\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_ai\\agent.py\", line 327, in run\n>     async for _ in agent_run:\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_ai\\agent.py\", line 1414, in __anext__\n>     next_node = await self._graph_run.__anext__()\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_graph\\graph.py\", line 782, in __anext__\n>     return await self.next(self._next_node)\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_graph\\graph.py\", line 760, in next\n>     self._next_node = await node.run(ctx)\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 262, in run\n>     return await self._make_request(ctx)\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 319, in _make_request\n>     return self._finish_handling(ctx, model_response, request_usage)\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 349, in _finish_handling\n>     ctx.deps.usage_limits.check_tokens(ctx.state.usage)\n>   File \"C:\\Users\\dangm\\anaconda3\\envs\\ipgncode\\lib\\site-packages\\pydantic_ai\\usage.py\", line 124, in check_tokens\n>     raise UsageLimitExceeded(f'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})')\n> pydantic_ai.exceptions.UsageLimitExceeded: Exceeded the total_tokens_limit of 300 (total_tokens=353)\n\n\n\n### Example Code\n\n```Python\nimport os\nfrom pdb import set_trace\nimport shutil\nimport asyncio\nfrom pprint import pprint\n\nos.environ[\"OPENAI_API_KEY\"] = \"<MY_KEY_HERE>\"\n# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"<MY_KEY_HERE>\"\nos.environ['USER_AGENT'] = 'myagent'\nos.environ['GEMINI_API_KEY'] = \"<MY_KEY_HERE>\"\n\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.usage import UsageLimits\n\njoke_selection_agent = Agent(  \n# The \"parent\" or controlling agent.\n\n\n    'openai:gpt-4o',\n    system_prompt=(\n        'Use the `joke_factory` to generate some jokes, then choose the best. '\n        'You must return just a single joke.'\n    ),\n)\njoke_generation_agent = Agent(  \n# The \"delegate\" agent, which is called from within a tool of the parent agent.\n\n\n    'google-gla:gemini-1.5-flash', result_type=list[str]\n)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[None], count: int) -> list[str]:\n    r = await joke_generation_agent.run(  \n# Call the delegate agent from within a tool of the parent agent.\n\n\n        f'Please generate {count} jokes.',\n        usage=ctx.usage,  \n# Pass the usage from the parent agent to the delegate agent so the final result.usage() includes the usage from both agents.\n\n\n    )\n    return r.data  \n# Since the function returns list[str], and the result_type of joke_generation_agent is also list[str], we can simply return r.data from the tool.\n\n\n\n\nresult = joke_selection_agent.run_sync(\n    'Tell me a joke.',\n    usage_limits=UsageLimits(request_limit=5, total_tokens_limit=300),\n)\nprint(result.data)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\nprint(result.usage())\n\"\"\"\nUsage(\n    requests=3, request_tokens=204, response_tokens=24, total_tokens=228, details=None\n)\n\"\"\"\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython version: 3.10\nPydantic AI version: 0.0.46\nLLM client version: N/A\n```",
      "state": "open",
      "author": "dangmanhtruong1995",
      "author_type": "User",
      "created_at": "2025-03-31T10:02:02Z",
      "updated_at": "2025-03-31T10:02:02Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1308/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1308",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1308",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:31.845376",
      "comments": []
    },
    {
      "issue_number": 562,
      "title": "An assistant message with 'tool_calls' must be followed by tool messages",
      "body": "A weird error is showing on an agent that runs multiple times in parallel\r\n\r\n```py\r\nresearch_agent = Agent(\r\n    model=\"openai:gpt-4o-mini\",\r\n    name=\"research_agent\",\r\n    deps_type=ResearchDependencies,\r\n    result_type=ResearchResult,\r\n    retries=3,\r\n)\r\n```\r\n\r\nError message \r\n\r\n```\r\nError code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_peVwb5yZhjdZ3xtJZAX8dDRV\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\r\n```\r\n\r\nany idea why it could happen?",
      "state": "closed",
      "author": "uriafranko",
      "author_type": "User",
      "created_at": "2024-12-29T13:42:56Z",
      "updated_at": "2025-03-31T07:36:26Z",
      "closed_at": "2025-03-31T07:36:26Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/562/reactions",
        "total_count": 13,
        "+1": 13,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/562",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/562",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:31.845400",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "I think this might be fixed in the latest pydantic-ai, what version are you using?\r\n\r\nIf it's still occurring with the latest version, I would need an MRE to be able to look into this further.",
          "created_at": "2024-12-30T12:40:56Z"
        },
        {
          "author": "tealtony",
          "body": "> I think this might be fixed in the latest pydantic-ai, what version are you using?\r\n> \r\n> If it's still occurring with the latest version, I would need an MRE to be able to look into this further.\r\n\r\nI was on 0.0.15 and was getting the same error. I've upgraded to 0.0.16 and it appears to be fixed",
          "created_at": "2024-12-31T13:29:08Z"
        },
        {
          "author": "XanderHorn",
          "body": "I keep on getting this error as well, seems volatile.\r\n\r\nUsing pydantic-ai 0.0.17.\r\n\r\nDowngraded to 0.0.16 and still have the same experience. For some cases it works, others it fails, only difference is the input_data per row in the dataframe. The input data is not the problem as this runs on langc",
          "created_at": "2025-01-07T14:51:07Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-01-15T14:07:00Z"
        },
        {
          "author": "gurvinder-dhillon",
          "body": "I also keep seeing this on 0.0.19. It is not consistent. I see it on 1 out of 3 runs.",
          "created_at": "2025-01-19T04:51:36Z"
        }
      ]
    },
    {
      "issue_number": 1265,
      "title": "’Qwen‘ model does not support the ’required‘ tool_choice type",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nI tried to create a **Qwen Agent** with **OpenAIProvider**, but when the **_completions_create** method calls the model, the **tool_choice** variable value is ”**required**“, which Qwen does not support.\n\nSometimes the \"**required**\" is recognized as a function and an error is reported\n`{'role': 'user', 'content': \": 'required'. Available tools: final_result\\n\\nFix the errors and try again.\"}`\n\nWhen I manually modify the value of the **tool_choice** in the source code by chance,\n\n`pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: qwen-long, body: {'code': 'invalid_parameter_error', 'param': None, 'message': 'tool_choice is one of the strings that should be [\"none\", \"auto\"]', 'type': 'invalid_request_error'}`\n\nIt seems that the Qwen model only supports \"**none**\", \"**auto**\", or **NOT_GIVEN**\n[qwen document](https://help.aliyun.com/zh/model-studio/user-guide/qwen-function-calling?spm=a2c4g.11186623.help-menu-2400256.d_1_0_0_8.2363453aMJ1mo8#cec840ca39b8b)\n\nThen I don't seem to find a way or function to change the value of the **allow_text_result** and let the **tool_choice** hit \"**auto**\".\n\n### Example Code\n\n```Python\nimport asyncio\nfrom pydantic_ai import Agent\nfrom pydantic import BaseModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n\nprovider = OpenAIProvider(\n    api_key=\"sk-**\",\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\nqwen_model = OpenAIModel(model_name=\"qwen-long\", provider=provider)\nagent = Agent(model=qwen_model, result_type=MyResponse)\n\n\nasync def fun():\n    response = await agent.run(user_prompt=user_prompt)\n    print(response.data)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(fun())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython==3.10.8\npydantic-ai==0.0.46\n\nLLM model: qwen-long\n```",
      "state": "closed",
      "author": "mkroen",
      "author_type": "User",
      "created_at": "2025-03-28T02:51:28Z",
      "updated_at": "2025-03-31T01:24:49Z",
      "closed_at": "2025-03-31T01:24:48Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1265/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1265",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1265",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:32.107277",
      "comments": [
        {
          "author": "novusshu",
          "body": "A walkaround is to set `result_type=Union[MyResponse, str])`. This should set the tool_choice to 'auto', though this may loosen the format validation. ",
          "created_at": "2025-03-30T23:28:14Z"
        },
        {
          "author": "mkroen",
          "body": "> A walkaround is to set `result_type=Union[MyResponse, str])`. This should set the tool_choice to 'auto', though this may loosen the format validation.\n\nThanks, this worked for me.\n",
          "created_at": "2025-03-31T01:24:48Z"
        }
      ]
    },
    {
      "issue_number": 665,
      "title": "How do I use LM Studio as my LLM service?",
      "body": "Hii everyone! I am Abhiraj, an associate SWE looking for some help/advice.\r\n\r\nI want to use LM Studio as my opensource and local LLM Service, is there any support for it yet because I am unable to find it in the docs and the code. \r\n\r\nI tried to write a custom model class for it, while it works, it has some issues when calling it inside an Agent. \r\n\r\nI am using FastAPI and I want to create an agentic chatbot using Pydantic AI agents.",
      "state": "closed",
      "author": "Abhiraj-Alois",
      "author_type": "User",
      "created_at": "2025-01-13T06:15:17Z",
      "updated_at": "2025-03-29T22:21:33Z",
      "closed_at": "2025-01-15T10:53:40Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/665/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/665",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/665",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:32.355555",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "It looks like LM studio has an OpenAI compatible API, I would use that, see:\r\n* https://lmstudio.ai/docs/api/openai-api\r\n* https://ai.pydantic.dev/models/#openai-compatible-models",
          "created_at": "2025-01-13T08:31:52Z"
        },
        {
          "author": "Abhiraj-Alois",
          "body": "Hi Samuel,\r\nThank you for getting back to me. I was aware of this and did try it, but while using it with an Agent and passing in some `result_types`, I encountered some issues with the responses. I keep receiving the following error:\r\n\r\n`pydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maxi",
          "created_at": "2025-01-13T09:04:37Z"
        },
        {
          "author": "wilkensgomes",
          "body": "Update to version **LM Studio 0.3.6** and they now support [tool use](https://lmstudio.ai/docs/advanced/tool-use). I used it here and it worked.\r\n\r\n\r\n```python\r\nfrom pydantic_ai import Agent\r\nfrom pydantic import BaseModel\r\nfrom pydantic_ai.models.openai import OpenAIModel\r\n\r\n\r\nmodel = OpenAIModel(\r",
          "created_at": "2025-01-14T03:04:28Z"
        },
        {
          "author": "Abhiraj-Alois",
          "body": "Thank you so much for your help!\r\nI implemented this and was able to get it up and running smoothly. ",
          "created_at": "2025-01-15T10:53:40Z"
        },
        {
          "author": "tkanngiesser",
          "body": "Hi, I run the very same code also using LM Studio, however I get \n\n```TypeError: 'NoneType' object cannot be interpreted as an integer```\n\nAny idea ? Thanks!",
          "created_at": "2025-01-18T03:49:57Z"
        }
      ]
    },
    {
      "issue_number": 1186,
      "title": "401 when running Gemini on VertexAI in the Cloud",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI'm running Gemini 1.5 with VertexAI provider in the Cloud (GCP) and after some succcesful requests I get 401 for some time with response:\n```json\n{\n    \"error\": {\n        \"code\": 401,\n        \"message\": \"Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.\",\n        \"status\": \"UNAUTHENTICATED\",\n        \"details\": [\n            {\n                \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n                \"reason\": \"ACCESS_TOKEN_EXPIRED\",\n                \"domain\": \"googleapis.com\",\n                \"metadata\": {\n                    \"service\": \"aiplatform.googleapis.com\",\n                    \"method\": \"google.cloud.aiplatform.v1.PredictionService.GenerateContent\"\n                }\n            }\n        ]\n    }\n}\n```\n\nI see in the code that PydanticAI uses [itw own attribute `token_created`](https://github.com/pydantic/pydantic-ai/blob/2b1aee8c6fc92ff0767090f604d5b5cffaccff78/pydantic_ai_slim/pydantic_ai/providers/google_vertex.py#L141) to manage tokens and refreshes them [after 3000s](https://github.com/pydantic/pydantic-ai/blob/2b1aee8c6fc92ff0767090f604d5b5cffaccff78/pydantic_ai_slim/pydantic_ai/providers/google_vertex.py#L32).\n\nHowever, my access token is valid for 1800s only (even though GCP docs say that access tokens last 3600s...). This means that for the 20 minutes after that, all requests fail, given that the token is expired but PydanticAI doesn't know about it and so doesn't refresh it.\n\nWondering what was the reason for using `token_created`, instead of one of the [properties `valid` or `expired`](https://github.com/googleapis/google-auth-library-python/blob/446c8e79b20b7c063d6aa142857a126a7efa1fc3/google/auth/credentials.py#L69-L97) provided by the `google-auth` library to decide when to refresh?\n\nP.s.: My service account already has the correct role to access Gemini in the Cloud.\n\nThank you 🙏 \n\n### Example Code\n\n```Python\n# This is just a part of the code and it's not designed to do anything meaningful\n# I'm actually using the async version in the Cloud\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\nfrom pydantic_ai.settings import ModelSettings\n\nprovider = GoogleVertexProvider(project_id=\"...\", region=\"...\")\ngemini_15 = GeminiModel(\"gemini-1.5-pro\", provider=provider)\n\nagent = Agent(\n    gemini_15,\n    result_type=str,\n    model_settings=ModelSettings(temperature=0.0, max_tokens=50)\n)\n\nresult = agent.run_sync(\"Return 1 random character from the latin alphabet.\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12.9\npydantic-ai-slim 0.0.42\nGemini 1.5 Pro\n```",
      "state": "closed",
      "author": "vricciardulli",
      "author_type": "User",
      "created_at": "2025-03-20T11:08:00Z",
      "updated_at": "2025-03-28T17:04:29Z",
      "closed_at": "2025-03-26T11:52:27Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1186/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1186",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1186",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:32.632245",
      "comments": [
        {
          "author": "bigs",
          "body": "Seconding this issue. I've been using the `GoogleVertexProvider` with a service account credential and ran into the exact same issue, with pydantic assuming the token would be valid when it wasn't, and giving the same unhelpful error. If I have time this weekend, I could look into debugging this.",
          "created_at": "2025-03-20T19:55:20Z"
        },
        {
          "author": "Kludex",
          "body": "- Can you try this branch? https://github.com/pydantic/pydantic-ai/pull/1195",
          "created_at": "2025-03-20T22:41:01Z"
        },
        {
          "author": "Kludex",
          "body": "I made a release that should fix this. Let me know if you still have issues with it. 🙏 ",
          "created_at": "2025-03-21T09:02:59Z"
        },
        {
          "author": "bigs",
          "body": "@Kludex Just got around to testing this out (we had to abandon pydantic-ai for Gemini usage to keep things moving, but absolutely want to bring it back.) My issue is still persisting, which is actually a 403:\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 403, model_name: gemini-2.0-flash-",
          "created_at": "2025-03-25T19:15:13Z"
        },
        {
          "author": "bigs",
          "body": "@Kludex On further investigation, when PydanticAI actually dispatches the request, it's being dispatched against the GLA APIs, not Vertex, even though I've given a `GoogleVertexProvider`. In retrospect, should have been obvious from the service in the error metadata, but my eyes were crossed.\n\nAfter",
          "created_at": "2025-03-25T20:14:54Z"
        }
      ]
    },
    {
      "issue_number": 915,
      "title": "Evals",
      "body": "(Or Evils as I'm coming to think of them)\n\nWe want to build an open-source, sane way to score the performance of LLM calls that is:\n* local first - so you don't need to use a service\n* flexible enough to work with whatever best practice emerges — ideally usable for any code that is stochastic enough to require scoring beyond passed/failed (that means LLM SDKs directly or even other agent frameworks)\n* usable both for \"offline evals\" (unit-test style checks on performance) and \"online evals\" measuring performance in production or equivalent (presumably using an observability platform like Pydantic Logfire)\n* usable with Pydantic Logfire when and where that actually helps\n\nI believe @dmontagu has a plan.",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-02-12T20:43:19Z",
      "updated_at": "2025-03-28T14:55:31Z",
      "closed_at": "2025-03-28T14:55:31Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/915/reactions",
        "total_count": 13,
        "+1": 10,
        "-1": 0,
        "laugh": 0,
        "hooray": 1,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/915",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/915",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:32.892699",
      "comments": []
    },
    {
      "issue_number": 937,
      "title": "Unit Test `pydantic_ai_examples`",
      "body": "We currently unit test examples in the documentation (including confirming output). Let's do the same for `pydantic_ai_examples` scripts so that our recommended usage patterns are confirmed to behave as expected.",
      "state": "closed",
      "author": "sydney-runkle",
      "author_type": "User",
      "created_at": "2025-02-17T15:44:42Z",
      "updated_at": "2025-03-28T11:57:37Z",
      "closed_at": "2025-03-28T11:57:37Z",
      "labels": [
        "good first issue",
        "examples"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/937/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/937",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/937",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:32.892729",
      "comments": [
        {
          "author": "Ravikiran-Bhonagiri",
          "body": "Hi @Kludex \n\nI would like to start contributing to this repo, I think this could be the good starting point for me.\nLet me work on this.\n\nThanks\nRavikiran Bhonagiri",
          "created_at": "2025-02-26T15:41:42Z"
        },
        {
          "author": "Kludex",
          "body": "Sure. Let me know if you have questions.",
          "created_at": "2025-02-26T16:01:02Z"
        },
        {
          "author": "Ravikiran-Bhonagiri",
          "body": "Hi @Kludex,\n\nI’m currently encountering issues while running the test suite in my local development environment. Out of the total test cases, 24 tests are failing on my end. I suspect this might be due to discrepancies in the environment setup or dependencies.\n\nCould you please provide guidance on t",
          "created_at": "2025-02-26T19:27:50Z"
        },
        {
          "author": "Kludex",
          "body": "You should run `make install`.",
          "created_at": "2025-02-26T19:36:07Z"
        }
      ]
    },
    {
      "issue_number": 1276,
      "title": "test `pydantic-ai-examples`",
      "body": "Currently the [examples](https://github.com/pydantic/pydantic-ai/tree/main/examples) are:\n* linted\n* typechecked\n* imported - minimum test that imports don't fail\n\nBut not tested.\n\nWe need to find a way to test them, at least partially",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:42:07Z",
      "updated_at": "2025-03-28T11:57:22Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1276/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1276",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1276",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:33.197549",
      "comments": [
        {
          "author": "Kludex",
          "body": "- Duplicated of https://github.com/pydantic/pydantic-ai/issues/937\n\nI'll close that one...",
          "created_at": "2025-03-28T11:57:21Z"
        }
      ]
    },
    {
      "issue_number": 1278,
      "title": "logfire within examples",
      "body": "most/all examples should have two tabs: one with logfire included, one without.",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:53:25Z",
      "updated_at": "2025-03-28T10:53:25Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1278/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1278",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1278",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:33.434654",
      "comments": []
    },
    {
      "issue_number": 1277,
      "title": "refactor docs",
      "body": "I think we want to move to mkdocs tabs:\n* Getting started - index and contributing sections\n* Agents - most of the documentation for `pydantic-ai`\n* Agent Systems - Pydantic Graph documentation etc\n* Evals - documentation for Pydantic Evals\n* Examples\n\nAPI Documentation should be nested within the appropriate tab.",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:52:10Z",
      "updated_at": "2025-03-28T10:52:10Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1277/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1277",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1277",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:33.434679",
      "comments": []
    },
    {
      "issue_number": 1275,
      "title": "type check docs examples",
      "body": "currently docs examples are tested and linted, but not typechecked, they should be.\n\nI have a plan for this.",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:40:42Z",
      "updated_at": "2025-03-28T10:40:42Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1275/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1275",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1275",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:33.434688",
      "comments": []
    },
    {
      "issue_number": 1274,
      "title": "Docs improvements",
      "body": "None of us are wild about writing docs, but they're very important, and there's a number of improvements we need to make.\n\nSub-issues for details:",
      "state": "open",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-03-28T10:39:57Z",
      "updated_at": "2025-03-28T10:39:57Z",
      "closed_at": null,
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1274/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1274",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1274",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:33.434696",
      "comments": []
    },
    {
      "issue_number": 1260,
      "title": "AnthropicModel no longer accept api_key as keyword argument",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nPreviously: able to pipe in api_key as keyword argument\nNow: not able anymore\n\n\n### Example Code\n\n```Python\nclaude_3_5_sonnet = AnthropicModel('claude-3-5-sonnet-latest', api_key=claude_key)\n\nTypeError: AnthropicModel.__init__() got an unexpected keyword argument 'api_key'\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic-ai==0.0.46\n```",
      "state": "closed",
      "author": "tranhoangnguyen03",
      "author_type": "User",
      "created_at": "2025-03-27T14:16:45Z",
      "updated_at": "2025-03-28T05:35:46Z",
      "closed_at": "2025-03-28T05:35:46Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1260/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1260",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1260",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:33.434703",
      "comments": [
        {
          "author": "Kludex",
          "body": "We deprecated some weeks ago, you can do it now with the following:\n\n```py\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nmodel = AnthropicModel(\n    'claude-3-5-sonnet-latest', provider=AnthropicPr",
          "created_at": "2025-03-27T14:29:45Z"
        },
        {
          "author": "jerry-reevo",
          "body": "I also ran into this issue and migrated to `AnthropicProvider`. Everything seems to work as expected after.",
          "created_at": "2025-03-27T16:42:24Z"
        },
        {
          "author": "tranhoangnguyen03",
          "body": "You guys should at least give some deprecation warning messages and run it for a few months before the hard-switch.\nI have pipelines crashing en-masse because of this change 🤕 . Lucky that this change happens early in the code structure and can be spontted easily; imagine if the change affects somet",
          "created_at": "2025-03-28T02:58:25Z"
        },
        {
          "author": "Kludex",
          "body": "We are trying to move fast so the framework can be the fast as possible. We are trying to reach into a stable state, but for that we need to make some breaking changes.\n\nWe've had this conversation internally, and there isn't much planned that will break stuff. We hope to bring 1.0 not long from now",
          "created_at": "2025-03-28T05:35:20Z"
        },
        {
          "author": "Kludex",
          "body": "Given that the issue was solved, I'll close this.",
          "created_at": "2025-03-28T05:35:38Z"
        }
      ]
    },
    {
      "issue_number": 242,
      "title": "Use ollama's structured outputs feature",
      "body": "I'm looking into pydantic-ai with small and locally running ollama models as backbones.\r\n\r\nI'm noticing that sometimes even for simple models it's possible to run into unexpected `ValidationError`s.\r\n\r\nHere's what I mean: With a pydantic model as simple as\r\n\r\n```python\r\nclass Answer(BaseModel):\r\n    value: str = \"\"\r\n```\r\n\r\nI can see pytandic-ai sometimes retrying and failing in validation.\r\n\r\nHaving experience with llama.cpp's grammars this was unexpected to me. I was under the assumption that pydantic-ai would transform the pydantic model into a grammar or json schema to hard-restrict the llm's output accordingly. Then validation could never fail by design since the llm's output is restricted to the specific grammar.\r\n\r\nInstead when I debug the request pydantic-ai sends to the locally running ollama with\r\n\r\n    nc -l -p 11434\r\n\r\nI can see pydantic-ai turning the pydantic model into a tool use invocation.\r\n\r\nWith ollama v0.5.0 structured output via json schema is now supported:\r\n\r\nhttps://github.com/ollama/ollama/releases/tag/v0.5.0\r\n\r\nI was wondering if that would solve the issue of small locally running models sometimes running into validation errors, since we hard-restrict the output to the shape of our pydantic model.\r\n\r\nAny thoughts on this, or ideas why validation can fail with tool usage as implemented right now? Any pointers in terms of for which model providers validation might fail and for what reason? Thanks!",
      "state": "closed",
      "author": "daniel-j-h",
      "author_type": "User",
      "created_at": "2024-12-13T15:35:45Z",
      "updated_at": "2025-03-27T11:56:21Z",
      "closed_at": "2025-01-24T14:53:18Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 18,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/242/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/242",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/242",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:33.743565",
      "comments": [
        {
          "author": "daniel-j-h",
          "body": "I have looked into this a bit more and here is what is happening\r\n1. The user specifies a return type for a pydantic-ai agent in form of a pydantic model\r\n2. The pydantic model's jsonschema gets passed to the llm in form of a tool and its arguments\r\n3. Because tool usage is optional the ollama model",
          "created_at": "2024-12-13T20:41:33Z"
        },
        {
          "author": "samuelcolvin",
          "body": "> Look into and implement the structured output approach which, I believe, should make validation and parsing deterministic\r\n\r\nIs this part of the OpenAI API, or would we need a dedicated model for it? \r\n\r\nHappy to consumer consider both, just trying to understand.",
          "created_at": "2024-12-13T23:35:58Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Looks like we should switch from the OpenAI compatibility API to using the [Ollama python library](https://github.com/ollama/ollama-python).",
          "created_at": "2024-12-14T09:58:46Z"
        },
        {
          "author": "renkehohl",
          "body": "As I was also facing several validation errors when working with Ollama, I have made an own implementation of the OllamaModel utilizing Ollama's new Structured Output feature. If you are interested in it, I can contribute it here.",
          "created_at": "2024-12-18T09:49:10Z"
        },
        {
          "author": "gt732",
          "body": "@renkehohl do you mind sharing? I'm running into the same issue running local models it sometimes works but its random. I'm mostly testing with qwen models.",
          "created_at": "2024-12-18T22:27:08Z"
        }
      ]
    },
    {
      "issue_number": 1246,
      "title": "Creating a custom model from a Api Call",
      "body": "Is there any kind of documentation on how to create a custom model, i'm trying to use the class Model but it raises an error.\n\n    raise AssertionError(f\"Expected code to be unreachable, but got: {value}\")\nAssertionError: Expected code to be unreachable, but got: 'L'",
      "state": "closed",
      "author": "AgustinGaliana",
      "author_type": "User",
      "created_at": "2025-03-26T11:41:21Z",
      "updated_at": "2025-03-27T11:45:12Z",
      "closed_at": "2025-03-26T13:17:22Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1246/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1246",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1246",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:34.091468",
      "comments": [
        {
          "author": "Kludex",
          "body": "Can you please paste the code, and explain your goal?",
          "created_at": "2025-03-26T11:50:47Z"
        },
        {
          "author": "AgustinGaliana",
          "body": "```\nfrom pydantic_ai.models import Model, ModelRequestParameters\nimport os, requests, re, asyncio\nfrom pydantic_ai.messages import ModelMessage,ModelResponse\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_ai.usage import Usage\nfrom collections.abc import AsyncIterator, Iterable, Mappin",
          "created_at": "2025-03-26T12:04:53Z"
        },
        {
          "author": "Kludex",
          "body": "What's the purpose of doing it tho?",
          "created_at": "2025-03-26T12:11:15Z"
        },
        {
          "author": "AgustinGaliana",
          "body": "Being able to perform calls to my api instead of using predefined models\n",
          "created_at": "2025-03-26T12:14:41Z"
        },
        {
          "author": "Kludex",
          "body": "Which providers are you using? You'd have to copy our implementation of our models, and go from there... I can't really help much right now without more information.\n\nWe should document at some point how to create custom models, but the API needs to be more stable.",
          "created_at": "2025-03-26T12:26:21Z"
        }
      ]
    },
    {
      "issue_number": 1247,
      "title": "Agent delegation: Agent does not retake control after delegate agent finishes when using run_stream()",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n- [x] I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue\n\n### Description\n\nIn some cases when using the agent delegation pattern, the outcome is different depending on if you're using run_sync or run_stream. A common scenario where this often occurs is when the Agent responsible for answering the user query generates a partial response before delegating work to another agent. When the delegate agent finishes and the answer agent takes back control, a couple of things can happen:\n\n- run_sync: Only returns the final response generated after the delegate agent finishes. The partial response before delegating work is ommitted, but present in the message history. This is fine.\n- run_stream: The answer agent never acts on the result from the other agent. As a result, the final response will _always_ be the partial response generated before delegating work to another agent. The result from the other agent is present in the message history.\n\nIs this the expected behaviour for run_stream?\n\nFor run_stream, this behaviour can be very frustrating since we have little control over the presence of such partial responses. For example, my answer agent may sporadically generate a friendly partial response like \"Sure, let me check\", before it delegates some work to another agent, waits for it to finish, and then just stops.\n\n### Example Code\n\n```Python\nimport asyncio\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.providers.google_vertex import GoogleVertexProvider\n\n\nmodel = GeminiModel(\n    \"gemini-2.0-flash\",\n    provider=GoogleVertexProvider(),\n)\n\n\nwelcome_selection_agent = Agent(\n    model=model,\n    system_prompt=(\n        'First, tell the user a joke.'\n        'Then, use the `welcome_factory` to generate a welcome message. '\n        'Finally, respond with the welcome message.'\n    ),\n)\n\n\nwelcome_generation_agent = Agent(  \n    model=model, result_type=list[str]\n)\n\n\n@welcome_selection_agent.tool\nasync def welcome_factory(ctx: RunContext[str]) -> list[str]:\n    r = await welcome_generation_agent.run(  \n        f'Please generate a single welcome messages for {ctx.deps}',\n        deps=ctx.deps,\n        usage=ctx.usage,  \n    )\n    return r.data\n\n\nasync def main():\n    name = 'Jeff'\n    async with welcome_selection_agent.run_stream('Hi!', deps=name) as result:\n        async for chunk in result.stream_text(delta=True):\n            print(chunk)\n        print(result.all_messages())\n\n\ndef sync_main():\n    name = 'Jeff'\n    result = welcome_selection_agent.run_sync('Hi!', deps=name)\n    print(result.data)\n    print(result.all_messages())\n\nasyncio.run(main())\n# sync_main()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.9\npydantic-ai 0.0.45\n```",
      "state": "open",
      "author": "OscarSommervold",
      "author_type": "User",
      "created_at": "2025-03-26T13:52:04Z",
      "updated_at": "2025-03-27T07:58:13Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1247/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1247",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1247",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:34.333959",
      "comments": []
    },
    {
      "issue_number": 1196,
      "title": "pydantic_graph: asyncio.get_event_loop() is deprecated since python 3.12",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nasyncio.get_event_loop() has been deprecated since version 3.12\n\n \n\nThe following code at [line 15 in pydantic_graph/_utils.py](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_graph/pydantic_graph/_utils.py#L15) emits a deprecation warning\n    event_loop = asyncio.get_event_loop()\n\nPython docs recommend using asyncio.get_running_loop() instead.\nhttps://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_event_loop\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\nagent = Agent(TestModel())\nagent.run_sync('hi')\"\n\n\n  /home/user/.local/lib/python3.13/site-packages/pydantic_graph/_utils.py:15: DeprecationWarning: There is no current event loop\n    event_loop = asyncio.get_event_loop()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython=3.13\npydantic=2.10.6\npydantic-ai-slim=0.0.42\n```",
      "state": "open",
      "author": "JohnUiterwyk",
      "author_type": "User",
      "created_at": "2025-03-21T03:12:35Z",
      "updated_at": "2025-03-26T17:16:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1196/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1196",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1196",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:34.333982",
      "comments": [
        {
          "author": "Kludex",
          "body": "Thanks for the report. 🙏 ",
          "created_at": "2025-03-22T10:30:39Z"
        },
        {
          "author": "JohnUiterwyk",
          "body": "Thank you @Kludex! I forked and cloned the project, and made the one line change as suggested in the docs and then ran the test suite (was a good motivator to get a dev env for the project setup). I saw what seemed like dozens of test failures, showing clearly the simple change was not like for like",
          "created_at": "2025-03-22T10:49:46Z"
        },
        {
          "author": "Kludex",
          "body": "I don't think you pinged me on your message. 👀 \n\nUsually, issues related to asyncio are not the easiest. Now we always create a new event loop instead of calling `get_event_loop`. In case of Python 3.11+, we use the new `Runner` asyncio API.",
          "created_at": "2025-03-22T11:09:33Z"
        },
        {
          "author": "Kludex",
          "body": "I'm opening it again because I had to revert the commit. I'm not sure if there's a way for us to solve this without proper sync support.",
          "created_at": "2025-03-26T17:16:50Z"
        }
      ]
    },
    {
      "issue_number": 1221,
      "title": "Getting 401 request error with open ai service account keys",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nWe recently created a service account key with open ai and started using it instead of user account key. However, after few minutes (~15 minutes) we see 401 errors \n\n```\nopenai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided:  You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n```\n\nWe had deployed to production (cloud run in GCP) and had validated that the deployment was funcitoning. During smoke testing, after few minutes, the error occurred. We redeployed and observed similar pattern.\n\nI am wondering if this is an issue similar to #1186 \n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython - 3.12\nPydantic and llm client - 0.0.43\n```",
      "state": "closed",
      "author": "peppermint-ai-lab",
      "author_type": "User",
      "created_at": "2025-03-24T06:25:25Z",
      "updated_at": "2025-03-26T11:52:27Z",
      "closed_at": "2025-03-26T11:52:27Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1221/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1221",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1221",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:34.591827",
      "comments": [
        {
          "author": "Kludex",
          "body": "That doesn't seem on our side. Please try to bump OpenAI package to see if it solves the problem.\n\nIf it doesn't, please provide an. MRE.",
          "created_at": "2025-03-24T06:58:04Z"
        },
        {
          "author": "peppermint-ai-lab",
          "body": "Some more info\n - I have two packages pydantic-ai-slim[openai]==0.0.43, pydantic-ai-slim[vertexai]==0.0.43\n - the error occurs when second agent is activated. In our app, agents are activated serially after the previous one completes\n\nI did some further debugging. Here are my findings\n1. Locally whe",
          "created_at": "2025-03-24T08:13:38Z"
        },
        {
          "author": "Kludex",
          "body": "Well, in your description above you said the issue was with OpenAI, not with Vertex...\n\nMRE is a minimal reproducible example, that I can just copy the code and run on my machine.",
          "created_at": "2025-03-24T08:22:29Z"
        },
        {
          "author": "peppermint-ai-lab",
          "body": "Here you go (https://github.com/peppermint-ai-lab/pydantic-1221-mre)\n\nAs to what is happening, my guess is running agent with vertex-ai is resetting the Open AI API key. I don't know if this issue happens with other models as well. ",
          "created_at": "2025-03-24T20:53:09Z"
        },
        {
          "author": "Kludex",
          "body": "I've fixed this on https://github.com/pydantic/pydantic-ai/pull/1242.\n\nThanks for the help!",
          "created_at": "2025-03-26T11:50:12Z"
        }
      ]
    },
    {
      "issue_number": 1125,
      "title": "Include tools available to LLM in trace",
      "body": "### Description\n\nI'm enjoying prototyping in Pydantic!\n\nOne thing that's been a bit tricky when debugging traces (I'm using `logfire.configure(service_name='...', send_to_logfire=False)` and using Langfuse for now) is that the `tools` actually available to the LLM aren't logged in the trace.\n\nAs far as I can tell, only a limited amount of metadata about the call to the LLM is included by default (quick example of `attributes` below).\n\nPerhaps I'm missing some way to include the tools made available in the call (eg the array of JSON schemas). I can understand why it's a reasonable default not to include this, but it's very helpful when debugging to understand which tools were available to which calls to the LLM, and what the rendered schemas were. Thanks!\n\n```\ngen_ai.operation.name: \"chat\"\ngen_ai.system: \"openai\"\ngen_ai.request.model: \"o3-mini\"\nserver.address: \"api.openai.com\"\nlogfire.span_type: \"span\"\nlogfire.msg: \"chat o3-mini\"\ngen_ai.usage.input_tokens: 219\ngen_ai.usage.output_tokens: 722\ngen_ai.usage.details.reasoning_tokens: 704\ngen_ai.response.model: \"o3-mini-2025-01-31\"\n```\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "talos",
      "author_type": "User",
      "created_at": "2025-03-15T02:29:06Z",
      "updated_at": "2025-03-25T17:09:44Z",
      "closed_at": "2025-03-22T10:07:49Z",
      "labels": [
        "OpenTelemetry"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1125/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1125",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1125",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:34.855262",
      "comments": [
        {
          "author": "talos",
          "body": "Thanks for adding this so fast! Excited to try it out in 0.0.44.",
          "created_at": "2025-03-25T17:09:44Z"
        }
      ]
    },
    {
      "issue_number": 895,
      "title": "AssertionError: OpenAI requires `tool_call_id`",
      "body": "# Description\n\nHi. I had run into an issue when switching models.\nBasically, I have implemented an API endpoint where I can change models.\n\nWhere is what happened:\n- I started with `gemini-1.5-flash`, asking it what time is now, which would call my `now()` tool.\n- It runs without any problem returning the current datetime\n- Then I switched to `gpt-4o-mini` and asked the same question again, passing the message history I got after using Gemini\n- This causes the following exception: `AssertionError: OpenAI requires `tool_call_id` to be set: ToolCallPart(tool_name='now', args={}, tool_call_id=None, part_kind='tool-call')`\n\n## [Edit] Minimal working example\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.models.gemini import GeminiModel\n\nfrom datetime import datetime\n\n\nopen_ai_api_key = ...\ngemini_api_key = ...\n\n\nopenai_model = OpenAIModel(\n    model_name='gpt-4o-mini',\n    api_key=open_ai_api_key,\n)\n\ngemini_model = GeminiModel(\n    model_name='gemini-2.0-flash-exp',  # could be gemini-1.5-flash also\n    api_key=gemini_api_key,\n)\n\nagent = Agent(gemini_model)\n\n\n@agent.tool_plain\ndef now():\n    return datetime.now().isoformat()\n\n\nr1 = agent.run_sync('what is the current date time?')\nprint(r1.all_messages_json())\n\nr2 = agent.run_sync(  # this will fail\n    'what time is now?',\n    model=openai_model,\n    message_history=r1.all_messages(),\n)\nprint(r2.all_messages_json())\n```\n\n## Message history (stored until call gpt-4o-mini)\n```python\n[ModelRequest(parts=[SystemPromptPart(content='\\nYou are a test agent.\\n\\nYou must do what the user asks.\\n', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='call now', timestamp=datetime.datetime(2025, 2, 11, 15, 55, 5, 628330, tzinfo=TzInfo(UTC)), part_kind='user-prompt')], kind='request'),\n ModelResponse(parts=[TextPart(content='I am sorry, I cannot fulfill this request. The available tools do not provide the functionality to make calls.\\n', part_kind='text')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 2, 11, 15, 55, 6, 59052, tzinfo=TzInfo(UTC)), kind='response'),\n ModelRequest(parts=[UserPromptPart(content='call the tool now', timestamp=datetime.datetime(2025, 2, 11, 15, 55, 14, 394461, tzinfo=TzInfo(UTC)), part_kind='user-prompt')], kind='request'),\n ModelResponse(parts=[TextPart(content='I cannot call a tool.  The available tools are functions that I can execute, not entities that I can call in a telephone sense.  Is there something specific you would like me to do with one of the available tools?\\n', part_kind='text')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 2, 11, 15, 55, 15, 449295, tzinfo=TzInfo(UTC)), kind='response'),\n ModelRequest(parts=[UserPromptPart(content='what time is now?', timestamp=datetime.datetime(2025, 2, 11, 15, 55, 23, 502937, tzinfo=TzInfo(UTC)), part_kind='user-prompt')], kind='request'),\n ModelResponse(parts=[ToolCallPart(tool_name='now', args={}, tool_call_id=None, part_kind='tool-call')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 2, 11, 15, 55, 24, 151395, tzinfo=TzInfo(UTC)), kind='response'),\n ModelRequest(parts=[ToolReturnPart(tool_name='now', content='2025-02-11T12:55:24.153651-03:00', tool_call_id=None, timestamp=datetime.datetime(2025, 2, 11, 15, 55, 24, 153796, tzinfo=TzInfo(UTC)), part_kind='tool-return')], kind='request'),\n ModelResponse(parts=[TextPart(content='The current time is 2025-02-11 12:55:24 -03:00.\\n', part_kind='text')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 2, 11, 15, 55, 24, 560881, tzinfo=TzInfo(UTC)), kind='response')]\n```\n\n\n## Traceback\n```\nTraceback (most recent call last):\n  File \"/app/agents/_agents/_wrapper.py\", line 125, in run_stream\n    async with self._agent.run_stream(\n               ~~~~~~~~~~~~~~~~~~~~~~^\n        user_prompt=user_prompt,\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        deps=self.deps,\n        ^^^^^^^^^^^^^^^\n    ) as result:\n    ^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 595, in run_stream\n    async with node.run_to_result(GraphRunContext(graph_state, graph_deps)) as r:\n               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 415, in run_to_result\n    async with ctx.deps.model.request_stream(\n               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        ctx.state.message_history, model_settings, model_request_parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ) as streamed_response:\n    ^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 160, in request_stream\n    response = await self._completions_create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        messages, True, cast(OpenAIModelSettings, model_settings or {}), model_request_parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 203, in _completions_create\n    openai_messages = list(chain(*(self._map_message(m) for m in messages)))\n  File \"/usr/local/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 267, in _map_message\n    tool_calls.append(self._map_tool_call(item))\n                      ~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/pydantic_ai/models/openai.py\", line 284, in _map_tool_call\n    id=_guard_tool_call_id(t=t, model_source='OpenAI'),\n       ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/pydantic_ai/_utils.py\", line 200, in guard_tool_call_id\n    assert t.tool_call_id is not None, f'{model_source} requires `tool_call_id` to be set: {t}'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: OpenAI requires `tool_call_id` to be set: ToolCallPart(tool_name='now', args={}, tool_call_id=None, part_kind='tool-call')\n\n```",
      "state": "closed",
      "author": "AlexEnrique",
      "author_type": "User",
      "created_at": "2025-02-11T16:05:52Z",
      "updated_at": "2025-03-25T15:46:16Z",
      "closed_at": "2025-03-25T15:46:16Z",
      "labels": [
        "bug",
        "good first issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/895/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/895",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/895",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:35.071590",
      "comments": [
        {
          "author": "AlexEnrique",
          "body": "Also, I was wondering if exceptions raised by a tool (not ModelRetry) deserve a type for it and should go into the ModelMessage, to be in the message history\n",
          "created_at": "2025-02-11T16:12:09Z"
        },
        {
          "author": "sydney-runkle",
          "body": "Thanks for the report. Might be a good first issue for someone looking to help out with a bug fix.\n\nPerhaps we should just use a dummy value here so that you can pass messages through in a model agnostic way...",
          "created_at": "2025-02-14T20:41:51Z"
        },
        {
          "author": "AlexEnrique",
          "body": "@sydney-runkle \nI was looking at the code, and I mapped the places where the tool_call_id may be or is None:\n\nFor Gemini\n  - pydantic_ai.models.gemini:\n    - _process_response_from_parts (line 514)\n\n\nFor Mistral\n  - pydantic_ai.models.mistral:\n    - MistralModel._map_mistral_to_pydantic_tool_call (l",
          "created_at": "2025-02-15T14:31:33Z"
        },
        {
          "author": "droid3k",
          "body": "Hi!\n\nI'm facing a similar issue. I'm working with local models using [Jan](https://jan.ai), which deploys a server with [cortex](https://cortex.so). I tried to run one of the examples in the documentation, and it raises the same assertion error:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pyd",
          "created_at": "2025-02-26T17:59:40Z"
        },
        {
          "author": "Kludex",
          "body": "> I'll make a PR to fix this particular case.\n\nThanks!",
          "created_at": "2025-02-27T11:14:06Z"
        }
      ]
    },
    {
      "issue_number": 1073,
      "title": "Add providers classes for `mistralai`, `cohere`, `anthropic` and `groq`",
      "body": "At the moment, we have the models for all of them, but since we introduced the `Provider`s classes, we missed the ones for:\n\n- [x] MistralAI https://github.com/pydantic/pydantic-ai/pull/1118\n- [x] Cohere\n- [x] Anthropic\n- [x] Groq: https://github.com/pydantic/pydantic-ai/pull/1084\n\nPR welcome.",
      "state": "closed",
      "author": "Kludex",
      "author_type": "User",
      "created_at": "2025-03-07T11:58:22Z",
      "updated_at": "2025-03-25T11:50:36Z",
      "closed_at": "2025-03-25T11:50:35Z",
      "labels": [
        "good first issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1073/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Viicos"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1073",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1073",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:35.304886",
      "comments": [
        {
          "author": "Kludex",
          "body": "I just noticed that Cohere can be used via OpenAI SDK: https://docs.cohere.com/docs/compatibility-api\n\nWe should drop the `CohereModel`, and have only the `CohereProvider`.",
          "created_at": "2025-03-07T12:21:14Z"
        },
        {
          "author": "Kludex",
          "body": "> I just noticed that Cohere can be used via OpenAI SDK: [docs.cohere.com/docs/compatibility-api](https://docs.cohere.com/docs/compatibility-api)\n> \n> We should drop the `CohereModel`, and have only the `CohereProvider`.\n\nSame with `Groq`: https://console.groq.com/docs/openai",
          "created_at": "2025-03-07T12:21:38Z"
        },
        {
          "author": "Kludex",
          "body": "@rafidka I saw that you implemented `CohereModel`, and that you work at Cohere. Is there a good reason we should keep the model instead of using the `OpenAIModel` with `CohereProvider`?",
          "created_at": "2025-03-07T12:28:18Z"
        },
        {
          "author": "Kludex",
          "body": "Same questions to @ricklamers, since they implemented the `GroqModel`.",
          "created_at": "2025-03-07T12:30:47Z"
        },
        {
          "author": "YassinNouh21",
          "body": "@Kludex \nThis is my strategy for how to Implement Provider Classes for MistralAI, Cohere, Anthropic, and Groq \n\n### Anthropic Provider Implementation\nThe existing `AnthropicModel` currently handles authentication directly through its constructor[[2](https://ai.pydantic.dev/api/models/anthropic/)]:\n\n",
          "created_at": "2025-03-07T16:56:51Z"
        }
      ]
    },
    {
      "issue_number": 1208,
      "title": "TypeError: 'NoneType' object cannot be interpreted as an integer",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nTraceback (most recent call last):\n  File \"/home/vladlen32230/qa/main.py\", line 41, in <module>\n    main()\n  File \"/home/vladlen32230/qa/main.py\", line 36, in main\n    deepresearch(question)\n  File \"/home/vladlen32230/qa/main.py\", line 7, in deepresearch\n    research = process_question(question)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/src/pipelines.py\", line 7, in process_question\n    topics = plan_topics(user_question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/src/planner_agent.py\", line 34, in plan_topics\n    return planner_agent.run_sync(user_query).data\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 570, in run_sync\n    return get_event_loop().run_until_complete(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 327, in run\n    async for _ in agent_run:\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 1414, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 782, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 760, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 262, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 314, in _make_request\n    model_response, request_usage = await ctx.deps.model.request(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_ai/models/openai.py\", line 204, in request\n    return self._process_response(response), _map_usage(response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vladlen32230/qa/.venv/lib/python3.11/site-packages/pydantic_ai/models/openai.py\", line 299, in _process_response\n    timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object cannot be interpreted as an integer\n\n### Example Code\n\n```Python\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.agent import Agent\nimport os\n\nsmart_model = OpenAIModel(\n    model_name=os.environ[\"DEEP_RESEARCH_MODEL\"],\n    provider=OpenAIProvider(\n        base_url=\"https://openrouter.ai/api/v1\", \n        api_key=os.environ[\"OPENROUTER_API_KEY\"]\n    )\n)\n\nplanner_agent = Agent(\n    model=smart_model, \n    result_type=list[str], \n    system_prompt=(\n        \"You are a helpful assistant that creates a list of titles with short description, \"\n        \"that will be used in final reports consisting of this topics \"\n        \"containing a detailed answer to user's question.\"\n    )\n)\n\nuser_query = \"What is the capital of France?\"\nplanner_agent.run_sync(user_query)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython: 3.11\n\npydantic                                 2.10.6\npydantic-ai                              0.0.43\npydantic-ai-slim                         0.0.43\npydantic-core                            2.27.2\npydantic-graph                           0.0.43\npydantic-settings                        2.8.1\n\nI use any model from OpenRouter and it gives always the same result.\n```",
      "state": "closed",
      "author": "vladlen32230",
      "author_type": "User",
      "created_at": "2025-03-22T13:40:48Z",
      "updated_at": "2025-03-23T20:07:49Z",
      "closed_at": "2025-03-23T20:07:49Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1208/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1208",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1208",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:37.309001",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/1094\" (95% similar)",
          "created_at": "2025-03-22T13:50:06Z"
        }
      ]
    },
    {
      "issue_number": 1146,
      "title": "Tool return types and docstrings are not used",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI noticed that the return type annotations and docstrings of tools are not actually used by pydantic-ai. I was struggling to make my agent understand the instructions and only now noticed that my agent didn't even see the instructions because (part of it) was placed under the \"Returns\" section of the docstring.\n\nI was wondering if this was left out intentionally, and if so, what was the reason to do so?\nIf this is indeed intentional, I think it should be mentioned in the documentation somewhere.\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12\npydantic==2.10.6\npydantic-ai==0.0.40\npydantic-ai-slim==0.0.40\npydantic-graph==0.0.40\npydantic_core==2.27.2\n```",
      "state": "closed",
      "author": "Krogager",
      "author_type": "User",
      "created_at": "2025-03-17T09:56:00Z",
      "updated_at": "2025-03-22T10:00:58Z",
      "closed_at": "2025-03-22T10:00:58Z",
      "labels": [
        "documentation",
        "Feature request",
        "help wanted"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1146/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1146",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1146",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:37.575920",
      "comments": [
        {
          "author": "Kludex",
          "body": "The return type hint is not included because there's no way to include it in the definition.\n\nThe docstring is included. Please provide code that proves otherwise, so I can check.",
          "created_at": "2025-03-17T10:53:56Z"
        },
        {
          "author": "Krogager",
          "body": "Using this code, I don't see anything in the ToolDefinition regarding the return-part of the docstring:\n```py\nfrom pprint import pprint\nfrom pydantic_ai.tools import Tool\n\ndef test_tool_fct(test_arg: str) -> str:\n    \"\"\"Tool description\n\n    Args:\n        test_arg: Test argument\n        \n\n    Return",
          "created_at": "2025-03-17T11:11:59Z"
        },
        {
          "author": "alexmojaki",
          "body": "@Krogager initially I agreed with you, but now I think it would make more sense if users wrote:\n\n```python\ndef test_tool_fct(test_arg: str) -> str:\n    \"\"\"Tool description. Returns nothing as a string.\n\n    Args:\n        test_arg: Test argument\n    \"\"\"\n```\n\nThis way it's very clear what the tool des",
          "created_at": "2025-03-17T11:56:02Z"
        },
        {
          "author": "Krogager",
          "body": "> [@Krogager](https://github.com/Krogager) initially I agreed with you, but now I think it would make more sense if users wrote:\n> \n> def test_tool_fct(test_arg: str) -> str:\n>     \"\"\"Tool description. Returns nothing as a string.\n> \n>     Args:\n>         test_arg: Test argument\n>     \"\"\"\n> This way",
          "created_at": "2025-03-17T14:31:57Z"
        },
        {
          "author": "Kludex",
          "body": "I brought this to our daily call. We should append to the description both return type and return docstring, if available.",
          "created_at": "2025-03-17T14:52:05Z"
        }
      ]
    },
    {
      "issue_number": 1205,
      "title": "Setting `max_tokens` for openai's o3-mini model throws 400 error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\n{\n  \"message\": \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n  \"type\": \"invalid_request_error\",\n  \"param\": \"max_tokens\",\n  \"code\": \"unsupported_parameter\"\n}\n\n### Example Code\n\n```Python\n\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nName: pydantic-ai\nVersion: 0.0.42\nLocation: /Users/br/development/projects/legaide-ai/.venv/lib/python3.12/site-packages\nRequires: pydantic-ai-slim\nRequired-by: legaide-ai\n\nPython 3.12.9\n```",
      "state": "closed",
      "author": "barapa",
      "author_type": "User",
      "created_at": "2025-03-21T15:05:29Z",
      "updated_at": "2025-03-22T09:42:06Z",
      "closed_at": "2025-03-22T09:42:06Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1205/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1205",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1205",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:37.823553",
      "comments": [
        {
          "author": "barapa",
          "body": "It appears that 'max_tokens' is deprecated across all openai models, in favor of 'max_completion_tokens':\nhttps://platform.openai.com/docs/api-reference/completions/create",
          "created_at": "2025-03-21T15:08:21Z"
        }
      ]
    },
    {
      "issue_number": 1201,
      "title": "Resuming a Pydantic Stateful Graph",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\n## Question about Resuming Workflow in PydanticAI Graphs\n### Issue Description\n\nI'm migrating my agent orchestration layer to PydanticAI Graphs for a live chat application, and I'm trying to understand the intended pattern for handling multi-turn conversations.\n\nSpecifically, I'm unclear about how to properly resume a workflow after an `End` state. In my use case:\n\n* A user asks a question\n* The graph processes it and returns a response (ending with `End`)\n* The user asks a follow-up question\n* I want to resume the graph with access to the previous context of my agents in-tact.\n\n\n#### My Understanding & Questions\nDoes an `End` node completely terminate the graph, or can I resume from where I left off? What's the recommended way to maintain conversation state across multiple user interactions?\n\nIs `g.initialize()` the right approach for follow-up questions, or should I be using a different pattern?\n\nIn my ideal workflow, an `End` would emit a response to the user but allow the graph to be resumed with the next user input, preserving all previous conversation context. The state JSON would be loaded from database, disk wherever - this Question Graph [example](https://ai.pydantic.dev/examples/question-graph/) doesn't completely fit the use case.\n\nWhen running this code, I encounter a validation error because the End node doesn't seem to accept the data I'm trying to pass to it.\n\nAny guidance on the intended conversation flow pattern with PydanticAI Graphs would be greatly appreciated!\n\n\n### Example Code\n\n```Python\nimport pydantic\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom pydantic_ai import Agent\n\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\nfrom pydantic_graph.persistence.file import FileStatePersistence\nfrom pydantic_ai.mcp import MCPServerHTTP\n\n# run npx @pydantic/mcp-run-python sse in terminal to start server\nrun_python_mcpserver = MCPServerHTTP(url='http://localhost:3001/sse')  \n\n# Router agent instructions\nROUTER_INSTRUCTIONS = \"\"\"\nYou are an expert query resolver\n\nTopic: Billing\nQuestions:\n    - ID: 4541: Can you explain my bill?\n    - ID: 4570: What is my current balance?\n    \nTopic: Refunds\nQuestions:\n    - ID: 4542: How do I request a refund?\n    - ID: 4571: Can I get a refund for my last payment?\n\"\"\"\n\n\n\n@dataclass\nclass ConversationState:  \n    routes: list[str] = field(default_factory=list)\n    messages: list[ModelMessage] = field(default_factory=list)\n\n@dataclass\nclass Route:\n    predicted_topic: str\n\nrouter_agent = Agent(\n    'openai:gpt-4o-mini',\n    result_type=Route,\n    system_prompt=ROUTER_INSTRUCTIONS, \n)\n\nrefund_agent = Agent(\n    'openai:gpt-4o-mini',\n    system_prompt=\"You are a general assistant.\", \n    mcp_servers=[run_python_mcpserver],\n)\n\n@dataclass\nclass RefundAgent(BaseNode[ConversationState]):  \n    query: str\n\n    async def run(self, ctx: GraphRunContext[ConversationState]) -> End:  \n\n        async with refund_agent.run_mcp_servers(): \n            q = f\"What is their total refund? If the customer is asking for {self.query}\"\n            result = await refund_agent.run(\n                q,\n                message_history=ctx.state.messages,\n            )\n\n            ctx.state.messages += result.new_messages()\n\n        return End(result.data)\n\n@dataclass\nclass Router(BaseNode[ConversationState]):\n    query: str\n    async def run(self, ctx: GraphRunContext[ConversationState]) -> RefundAgent:\n\n        result = await router_agent.run(\n            self.query,\n            message_history=ctx.state.routes\n        )\n        ctx.state.routes += [result.data.predicted_topic]\n\n        return RefundAgent(query=self.query)\n    \n\nasync def main():\n    persistence = FileStatePersistence(Path(f'router_test.json'))\n\n    g = Graph(  \n        nodes=[Router, RefundAgent]\n    )\n\n    state = ConversationState(\n        routes=[],\n    )  \n\n    query_1 = \"You agreed that I would get a refund of £45 for gas and £37 for electricity - so what's my total refund?\"\n\n    output = await g.run(Router(query=query_1), state=state, persistence=persistence)  \n\n    query_2 = \"Great and what about if I'm owed another refund of £25 for water too what is my total refunded amount?\"\n\n    output = await g.initialize(  \n        Router(query=query_2), state=state, persistence=persistence\n    )\n\n    print(output)\n\nif __name__ == '__main__':\n    import asyncio\n    asyncio.run(main())\n```\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npydantic==2.10.6\npydantic-ai-examples==0.0.42\npydantic-ai-slim==0.0.42\npydantic-graph==0.0.42\npydantic-settings==2.8.1\npydantic_core==2.27.2\n```",
      "state": "closed",
      "author": "SamComber",
      "author_type": "User",
      "created_at": "2025-03-21T11:52:17Z",
      "updated_at": "2025-03-21T15:45:36Z",
      "closed_at": "2025-03-21T15:45:01Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1201/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1201",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1201",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:38.058597",
      "comments": [
        {
          "author": "SamComber",
          "body": "Got it, just `return End(None)` and \n\n\n```\n\nasync with g.iter(Router(query=query_1), state=state, persistence=persistence) as run:\n    run\n    node = run.next_node  \n    while not isinstance(node, End):  \n        print('Node:', node)\n        node = await run.next(node)  \n\n    print(run.result) \n\n\nqu",
          "created_at": "2025-03-21T15:45:35Z"
        }
      ]
    },
    {
      "issue_number": 1200,
      "title": "MCPServerStdio bug when passing env vars as param",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nHi. I am having issues to start an mcp server with the class `MCPServerStdio` whenever I pass a dict[str, str] to the `env` parameter.\n\nI can confirm that I have `npx` installed and have used with success the MCPServerStdio without passing the env parameter.\n\n### Example Code\n\n```Python\nimport asyncio\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.mcp import MCPServerStdio\n\ngithub_server = MCPServerStdio(\n    command=\"npx\",\n    args=[\"@modelcontextprotocol/server-github\"],\n    env={\"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<github-pat-here>\"},\n)\n\nollama_model = OpenAIModel(\n    model_name=\"llama3.2\", provider=OpenAIProvider(base_url=\"http://localhost:11434/v1\")\n)\n\nagent = Agent(ollama_model, mcp_servers=[github_server])\n\n\nasync def main():\n    async with agent.run_mcp_servers():\n        result = await agent.run(\"list issues from repo XXX\")\n    print(result.data)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n---\nError:\n\nFile \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py\", line 1974, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory: 'npx'\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n3.13, 0.0.43, openai v1.68.0 (azure openai and ollama)\n```",
      "state": "closed",
      "author": "fcestari",
      "author_type": "User",
      "created_at": "2025-03-21T09:43:11Z",
      "updated_at": "2025-03-21T13:41:29Z",
      "closed_at": "2025-03-21T11:52:29Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1200/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1200",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1200",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:38.279200",
      "comments": [
        {
          "author": "Kludex",
          "body": "It was fixed on `mcp`'s side: https://github.com/modelcontextprotocol/python-sdk/pull/327\n\nIf you bump the `mcp` dependency to 1.5.0, it should work as expected (the release is in progress, should be available in some hours).",
          "created_at": "2025-03-21T11:52:29Z"
        },
        {
          "author": "fcestari",
          "body": "Thanks!",
          "created_at": "2025-03-21T13:41:28Z"
        }
      ]
    },
    {
      "issue_number": 1203,
      "title": "Error when using Dict[str, Any] in Pydantic models with Gemini: \"should be non-empty for OBJECT type\"",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nWhen using a Pydantic model that includes a Dict[str, Any] field with a Gemini model in pydantic-ai, the request fails with a 400 error. The error message indicates that Gemini expects properties to be defined for an OBJECT type. Still, with Dict[str, Any], we're intentionally using an open-ended dictionary that doesn't need predefined properties. This issue does not occur in Anthropic models.\n\n\n\n### Example Code\n\n```Python\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nfrom dotenv import load_dotenv\nfrom pydantic_ai import Agent\n\nload_dotenv()\n\nclass PersonModel(BaseModel):\n    name: str\n    age: int\n    other_fields: Dict[str, Any]\n\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant that can read the user's prompt and returns data about the person.\n\"\"\"\n\nuser_prompt = \"\"\"\nHello, my name is John Doe. I am 30 years old. I live in New York. I am a software engineer. I love cooking, reading and playing tennis.\n\"\"\"\n\n\nagent = Agent(\n    model=\"gemini-2.0-flash\",\n    system_prompt=system_prompt,\n    result_type=PersonModel,\n)\n\nresponse = agent.run_sync(user_prompt=user_prompt)\n\nperson = response.data\n\nprint(f\"Name: {person.name}\")\nprint(f\"Age: {person.age}\")\nprint(f\"Other fields: {person.other_fields}\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.11.11\nPydantic AI: 0.0.43\nLLM client: gemini-2.0-flash\n```",
      "state": "open",
      "author": "sanjay-gr-qrev",
      "author_type": "User",
      "created_at": "2025-03-21T12:07:33Z",
      "updated_at": "2025-03-21T12:07:33Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1203/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1203",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1203",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:38.507520",
      "comments": []
    },
    {
      "issue_number": 1165,
      "title": "BUG - Using Claude through AnthropicVertex client 'await' error",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nUsing Claude through AnthropicVertex custom client is currently broken, with error:\n\n`\nTraceback (most recent call last):\n  File \"/Users/aristide/Documents/bitbucket/llm-dashboard/test_vertex.py\", line 30, in <module>\n    asyncio.run(main())\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/aristide/Documents/bitbucket/llm-dashboard/test_vertex.py\", line 26, in main\n    result_sync = await agent.run('What is the capital of Italy?')\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 316, in run\n    async for _ in agent_run:\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_ai/agent.py\", line 1352, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 734, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 723, in next\n    self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_graph/graph.py\", line 305, in next\n    next_node = await node.run(ctx)\n                ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 252, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py\", line 304, in _make_request\n    model_response, request_usage = await ctx.deps.model.request(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_ai/models/anthropic.py\", line 153, in request\n    response = await self._messages_create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aristide/.pyenv/versions/3.11.11/lib/python3.11/site-packages/pydantic_ai/models/anthropic.py\", line 227, in _messages_create\n    return await self.client.messages.create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: object Message can't be used in 'await' expression`\n\nCurrently, VertexAI is only supported as a Gemini provider, so we have to use the custom Anthropic client workaround that was previously also used for Bedrock.\n\nIn the example I provided I have also included the equivalent with Bedrock, which works.\n\n### Example Code\n\n```Python\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom anthropic import AnthropicVertex\nfrom anthropic import AsyncAnthropicBedrock\nfrom pydantic_ai import Agent\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nasync def main():\n\n    # Vertex\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./vertex-service-account.json\"\n\n    anthropic_vertex_client = AnthropicVertex(\n        region=\"us-east5\",\n        project_id=\"test-project-ari\"\n    )\n\n    HAIKU_3_5_VERTEX_MODEL = AnthropicModel(\n        model_name='claude-3-5-haiku@20241022',\n        anthropic_client=anthropic_vertex_client\n    )\n\n    # Bedrock\n    anthropic_bedrock_client = AsyncAnthropicBedrock(\n        aws_region='us-west-2'\n    )\n\n    HAIKU_3_5_BEDROCK_MODEL = AnthropicModel(\n        model_name='anthropic.claude-3-5-haiku-20241022-v1:0',\n        anthropic_client=anthropic_bedrock_client\n    )\n\n    agent = Agent(model=HAIKU_3_5_VERTEX_MODEL)\n    # agent = Agent(model=HAIKU_3_5_BEDROCK_MODEL)\n\n    try:\n        result_sync = await agent.run('What is the capital of Italy?')\n        print(result_sync.data)\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.0.39\n```",
      "state": "closed",
      "author": "aristideubertas",
      "author_type": "User",
      "created_at": "2025-03-18T17:27:29Z",
      "updated_at": "2025-03-21T08:23:13Z",
      "closed_at": "2025-03-21T08:23:13Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1165/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1165",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1165",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:38.507537",
      "comments": [
        {
          "author": "Kludex",
          "body": "You need to use `AsyncAnthropicVertex` instead of `AnthropicVertex`.\n\nLet me know if that was not the issue, and we can reopen it.",
          "created_at": "2025-03-20T23:02:02Z"
        }
      ]
    },
    {
      "issue_number": 1192,
      "title": "Structured output validation keeps failing",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nThis is hard to report, after looking at it for a while I think it's a prompt issue potentially but not sure. I'm not sure how the internals of the framework work when it comes to structured outputs (Maybe something to be better documented).\n\nFrom what I can guess is happening, the framework sends the structured response object as a \"tool\" for the LLM to call.\n\nI'm trying to get this example to succeed for me. But the response I have is pretty simple, just a dict. I published the small script I'm working on, it's easy to setup and run (install with uv, `uv run python hello.py`) \nhttps://github.com/RamiAwar/notsureyet/blob/aa92c6582eb6210fff983079dd5db44517a0669f/hello.py#L164\n\n<img width=\"509\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2bc42c84-edb1-4c07-a94f-cd3290e497da\" />\n\nFundamentally, I feel like this structured output prompting is a bit off - even tried with other newer models, but here this simple output keeps failing.\n\nHere's a snippet of the code. Let me know if I should post more details or if this is enough. I want to help make this the best agent framework out there!\n\nI also have some [logfire spans if ](https://logfire.pydantic.dev/ramiawar/starter-project?q=trace_id%3D%270195b4ae988d9e149caa836a74102b54%27+and+span_id%3D%27bf8f731cc611399c%27&spanId=bf8f731cc611399c&traceId=0195b4ae988d9e149caa836a74102b54&since=2025-03-20T17%3A51%3A36.077783Z&until=2025-03-20T17%3A51%3A37.457937Z)you can access them.\n\n### Example Code\n\n```Python\nfrom dataclasses import dataclass\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.format_as_xml import format_as_xml\nfrom typing import Any\n\n\n# Specify available query/path parameters to select from\n@dataclass\nclass SelectParametersDeps:\n    parameters: list[str]\n\n\n# Define agent\nagent = Agent(\n    \"openai:gpt-3.5-turbo\",\n    retries=2,\n    deps_type=SelectParametersDeps,\n    result_type=dict[str, Any],\n    instrument=True,\n)\n\n# Make sure selected parameters are from the available ones\n@agent.result_validator\nasync def validate_result(ctx: RunContext[SelectParametersDeps], result: dict[str, Any]) -> dict[str, Any]:\n    if result.keys() != ctx.deps.parameters:\n        raise ModelRetry(f\"Invalid parameters: {result.keys()}. Expected parameters: {ctx.deps.parameters}\")\n    else:\n        return result\n\n@agent.system_prompt\nasync def system_prompt() -> str:\n    return f\"\"\"\\\nYou are a smart API engineer that can help pick the query and path parameter values to use to answer a user's query.\n\nWe've got a list of parameters and their openapi spec, you should look at them and decide what values to use for the parameters.\n\nReturn a dictionary of parameters with the parameter name as the key and the parameter value as the value.\nReturn nothing else.\n\nExample:\nUser query: \"What's the weather in Berlin?\"\nEndpoint: \"/weather\"\nParameters:\n{{\n    \"location\": \"str\",\n    \"timezone\": \"str\",\n}}\n\nResponse:\n{{\n    \"location\": \"Berlin\",\n    \"timezone\": \"Europe/Berlin\",\n}}\n\nAvailable parameters:\n{format_as_xml(parameter_descriptions)}\n\"\"\"\n\ndeps = SelectParametersDeps(parameters=list(parameter_descriptions.keys()))\nresult = agent.run_sync(user_query, deps=deps)\n```\n\n\n### Example Errors\n```\nTraceback (most recent call last):\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_result.py\", line 205, in validate\n    result = self.type_adapter.validate_json(tool_call.args, experimental_allow_partial=pyd_allow_partial)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic/type_adapter.py\", line 446, in validate_json\n    return self.validator.validate_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for typed-dict\nresponse\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 443, in _handle_tool_calls\n    result_data = result_tool.validate(call)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_result.py\", line 215, in validate\n    raise ToolRetryError(m) from e\npydantic_ai._result.ToolRetryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/rami/code/text2api/hello.py\", line 296, in <module>\n    main()\n  File \"/Users/rami/code/text2api/hello.py\", line 286, in main\n    parameters = select_parameters(openapi_spec, endpoint, user_query=query)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/hello.py\", line 214, in select_parameters\n    result = agent.run_sync(user_query, deps=deps)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 569, in run_sync\n    return get_event_loop().run_until_complete(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 326, in run\n    async for _ in agent_run:\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py\", line 1413, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 782, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 760, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 375, in run\n    async with self.stream(ctx):\n               ^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 217, in __aexit__\n    await anext(self.gen)\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 390, in stream\n    async for _event in stream:\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 427, in _run_stream\n    async for event in self._events_iterator:\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 417, in _run_stream\n    async for event in self._handle_tool_calls(ctx, tool_calls):\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 448, in _handle_tool_calls\n    ctx.state.increment_retries(ctx.deps.max_result_retries)\n  File \"/Users/rami/code/text2api/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 71, in increment_retries\n    raise exceptions.UnexpectedModelBehavior(\npydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (2) for result validation\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nlatest == 0.0.42\n```",
      "state": "open",
      "author": "RamiAwar",
      "author_type": "User",
      "created_at": "2025-03-20T18:07:40Z",
      "updated_at": "2025-03-21T06:46:07Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1192/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1192",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1192",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:38.753308",
      "comments": [
        {
          "author": "RamiAwar",
          "body": "After comparing to mirascope, I think this is because my response model is `dict` and I'm using OpenAI, assuming without JSON mode support turned on automatically. Will let someone confirm before closing.",
          "created_at": "2025-03-20T20:27:08Z"
        },
        {
          "author": "vedantkarandikar",
          "body": "I haven't even looked at your complete post, but I'll suggest for better structured output, use stronger models.\nUsing 3.5-Turbo could be on of the issues here",
          "created_at": "2025-03-21T05:16:30Z"
        },
        {
          "author": "RamiAwar",
          "body": "@vedantkarandikar Nope, not the model, tried several. It's the output type I believe. `dict[str, Any]` is problematic in other frameworks, so I expect it to also be problematic here.\n\nThe reason I'm using this is that I have a mostly dynamic output structure, so I'm trying to at least validate it pa",
          "created_at": "2025-03-21T06:46:06Z"
        }
      ]
    },
    {
      "issue_number": 1145,
      "title": "Add optional timestamp to SystemPromptPart",
      "body": "UserPromptPart, ToolReturnPart, RetryPromptPart all include a timestamp attribute.\nAs do TextPart, ToolCallPart.\n\nSystemPromptPart is the only part type in the ModelRequestPart that does not have a timestamp. \n\n There are times that we create a system prompt instance from a template, and this instantiation is done at particular point in time. There are also times when this happens in other systems, and that history is passed to our pydantic-ai service, and the timestamp is consider a required attribute as part of retaining an immutable timestamped sequence of entries related to a user/ai chat.\n\nCan we please add \n```\n    timestamp: datetime = field(default_factory=_now_utc)\n```\nto SystemPromptPart to bring it in line with the other parts",
      "state": "closed",
      "author": "JohnUiterwyk",
      "author_type": "User",
      "created_at": "2025-03-17T02:04:09Z",
      "updated_at": "2025-03-21T02:29:46Z",
      "closed_at": "2025-03-19T18:31:27Z",
      "labels": [
        "good first issue",
        "help wanted"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1145/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1145",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1145",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:38.966047",
      "comments": [
        {
          "author": "Kludex",
          "body": "PR welcome! 🙏 ",
          "created_at": "2025-03-17T07:38:32Z"
        },
        {
          "author": "vricciardulli",
          "body": "Just opened a PR about this, but I didn't see there was another open already. Feel free to close it if redundant",
          "created_at": "2025-03-18T00:37:44Z"
        },
        {
          "author": "Kludex",
          "body": "- Closed by https://github.com/pydantic/pydantic-ai/pull/1154",
          "created_at": "2025-03-19T18:31:27Z"
        },
        {
          "author": "JohnUiterwyk",
          "body": "thank you!",
          "created_at": "2025-03-21T02:29:45Z"
        }
      ]
    },
    {
      "issue_number": 724,
      "title": "Document how to use any HuggingFace model",
      "body": "My personal opinion is that it feels rather weird of you not to support or provide any documentation on how to use LLMs from the `transformers` library. Why only support models that are behind a paywall?\n\nThe current documentations reads:\n\n> Implementing Custom Models\n> \n> To implement support for models not already supported, you will need to subclass the [Model](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.Model) abstract base class.\n> \n> This in turn will require you to implement the following other abstract base classes:\n> \n> [AgentModel](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.AgentModel)\n> [StreamedResponse](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.StreamedResponse)\n> The best place to start is to review the source code for existing implementations, e.g. [OpenAIModel](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py).\n> \n> For details on when we'll accept contributions adding new models to PydanticAI, see the [contributing guidelines](https://ai.pydantic.dev/contributing/#new-model-rules).\n\nThis is not real documentation. Please provide an example for how to load a model from HuggingFace running locally.",
      "state": "closed",
      "author": "svnv-svsv-jm",
      "author_type": "User",
      "created_at": "2025-01-20T20:39:07Z",
      "updated_at": "2025-03-20T13:01:42Z",
      "closed_at": "2025-02-28T03:20:28Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/724/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/724",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/724",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:39.246304",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "@svnv-svsv-jm thank you for the comments. Sometimes it is easy to forget that this is an open source project :)\n\nPlease can you specify the exact set of transformer models from HuggingFace you are looking to use with PydanticAI?\n\nThe project does not only support models behind paywalls. It also supp",
          "created_at": "2025-01-21T03:17:36Z"
        },
        {
          "author": "svnv-svsv-jm",
          "body": "Hi! Thanks for your response.\n\nThe documentation about Ollama integration still lacks a clear example of how one would do what you suggest. All in all, my initial post about lacking documentation still stands, also considering that even your own comment adds more clarity.\n\nBut what would really be b",
          "created_at": "2025-01-26T10:00:46Z"
        },
        {
          "author": "izzyacademy",
          "body": "@svnv-svsv-jm did you try the steps I provided? Did it work for you?",
          "created_at": "2025-01-26T13:45:35Z"
        },
        {
          "author": "svnv-svsv-jm",
          "body": "But that is not what I would like to do. I'd like to add a custom model in-code, inheriting from the recommended base `Model` class, but I can't because there is no example, which I believe would make this library better (this is what is stopping me from using it over Llamaindex and Langchain).",
          "created_at": "2025-01-27T09:33:37Z"
        },
        {
          "author": "jessesightler-redhat",
          "body": "I agree, this would be incredibly useful, especially for testing. It isn't always desirable to have a fully separate model server.",
          "created_at": "2025-02-27T23:00:57Z"
        }
      ]
    },
    {
      "issue_number": 1070,
      "title": "Documentation on Instrumentation?",
      "body": "### Description\n\nI see InstrumentedModel in the codebase, but no documentation of it.\n\nIs the feature not yet prepared, and/or can we expect documentation?\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "pedroallenrevez",
      "author_type": "User",
      "created_at": "2025-03-06T11:01:31Z",
      "updated_at": "2025-03-20T11:40:14Z",
      "closed_at": "2025-03-20T11:40:14Z",
      "labels": [
        "documentation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1070/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1070",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1070",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:39.467453",
      "comments": [
        {
          "author": "Kludex",
          "body": "I think it's not documented because you can pass a `instrument=True` to the `Agent`, and we use it underneath.\n\n@alexmojaki do you want to document it, or what do you think is missing here?",
          "created_at": "2025-03-06T11:05:03Z"
        },
        {
          "author": "alexmojaki",
          "body": "It's recommended to use `Agent(instrument=True)` or `Agent.instrument_all()`. There may be use cases for `InstrumentedModel` but they're not obvious so it's not clear to me that we should document this.",
          "created_at": "2025-03-06T11:14:27Z"
        },
        {
          "author": "Kludex",
          "body": "@pedroallenrevez I'll preemptively close this issue. Let me know if you have opinions, and I'll reopen.",
          "created_at": "2025-03-07T12:31:42Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "Dont need to reopen but, its clear that for agents you can do that, but not specifically for the models. \nThe documentation is just a bit elusive...",
          "created_at": "2025-03-07T17:47:36Z"
        },
        {
          "author": "alexmojaki",
          "body": "Reopening because of https://github.com/pydantic/pydantic-ai/issues/1160",
          "created_at": "2025-03-19T08:52:10Z"
        }
      ]
    },
    {
      "issue_number": 1160,
      "title": "Allow instrumentation to be attached in .run()",
      "body": "### Description\n\nI want my instrumentation to be context-specific. I want to attach a logger for each call into my application's runner function. Because instrumentation is attached at Agent instantiation and cannot be scoped to a single .run(), I either need to instantiate my Agent inside of my application runner function OR do tricks with context vars.\n\nCurrently I can either attach a Model at instantiation or at run invocation. I can do the same for ModelSettings. I would like to be able to have that same duel choice for Instrumentation.\n\n```\n        res = await update_actions.run(\n            prompt,\n            model=model,\n            model_settings=model_settings,\n            instrumentation_settings=instrumentation_settings\n        )\n```\n\n\n### References\n\n_No response_",
      "state": "closed",
      "author": "prescod",
      "author_type": "User",
      "created_at": "2025-03-18T14:32:45Z",
      "updated_at": "2025-03-20T11:25:39Z",
      "closed_at": "2025-03-19T08:51:51Z",
      "labels": [
        "OpenTelemetry"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1160/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1160",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1160",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:39.720585",
      "comments": [
        {
          "author": "alexmojaki",
          "body": ">  I want to attach a logger for each call into my application's runner function.\n\nPlease can you explain this more?\n\nAnyway, try this:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.instrumented import InstrumentedModel\n\nagent = Agent()\nagent.run(model=InstrumentedModel('gpt-4', i",
          "created_at": "2025-03-18T17:54:53Z"
        },
        {
          "author": "prescod",
          "body": "Thanks! I think that will work well!\n\nOur observability strategy predates our usages of Pydantic AI. But consider the following high level picture.\n\nPretend we are generating a Deep Research type report in HTML.\n\nPretend that each HTML document has an appendix which is _all_ of the logs that generat",
          "created_at": "2025-03-19T01:18:39Z"
        },
        {
          "author": "alexmojaki",
          "body": "Thanks so much for explaining, I'm sure @dmontagu will find this very interesting.",
          "created_at": "2025-03-19T08:21:46Z"
        },
        {
          "author": "alexmojaki",
          "body": "https://github.com/pydantic/pydantic-ai/pull/1187",
          "created_at": "2025-03-20T11:25:38Z"
        }
      ]
    },
    {
      "issue_number": 1183,
      "title": "AnthropicProvider module not available",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nTrying to implement custom http client as in: https://ai.pydantic.dev/models/#provider-argument_1\n\nloading AnthropicProvider not working\n\nModuleNotFoundError: No module named 'pydantic_ai.providers.anthropic'\n\n\n### Example Code\n\n```Python\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.11.6\n```",
      "state": "closed",
      "author": "tonyjhlam",
      "author_type": "User",
      "created_at": "2025-03-20T02:15:19Z",
      "updated_at": "2025-03-20T07:19:36Z",
      "closed_at": "2025-03-20T07:19:36Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1183/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1183",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1183",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:40.005707",
      "comments": [
        {
          "author": "Kludex",
          "body": "What is your PydanticAI version?",
          "created_at": "2025-03-20T06:45:52Z"
        },
        {
          "author": "tonyjhlam",
          "body": "'0.0.37'",
          "created_at": "2025-03-20T06:50:57Z"
        },
        {
          "author": "Kludex",
          "body": "Can you bump to the latest?",
          "created_at": "2025-03-20T07:19:27Z"
        }
      ]
    },
    {
      "issue_number": 254,
      "title": "Review shared logic between models and consolidate where possible",
      "body": "Now that we've added 5+ models, I think worth revisiting shared patterns and consolidating some of that logic.\r\n\r\nI think this issue can also account for refactoring our internal streaming logic to be more unified across models 👍 \r\n\r\nOne other thing I noticed - we could consistently prefix model specific structures with their name. Ex, we do a good job of this in `mistral.py` where we have things like `from mistralai import TextChunk as MistralTextChunk`, but we don't do this in `openai.py`.\r\n\r\nWe can unify:\r\n* Docstrings\r\n* Perhaps some sort of protocol for private patterns like `_map_messages` that could ultimately turn into an accessible hook\r\n\r\n(WIP)",
      "state": "open",
      "author": "sydney-runkle",
      "author_type": "User",
      "created_at": "2024-12-14T15:22:46Z",
      "updated_at": "2025-03-18T14:37:33Z",
      "closed_at": null,
      "labels": [
        "refactor"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/254/reactions",
        "total_count": 3,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sydney-runkle"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/254",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/254",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:40.232148",
      "comments": [
        {
          "author": "YanSte",
          "body": "@sydney-runkle Need help for this issue ?",
          "created_at": "2024-12-16T13:04:31Z"
        },
        {
          "author": "sydney-runkle",
          "body": "@YanSte,\r\n\r\nSure, help always welcome!\r\n\r\nI think this will sort of be a WIP as we finish up internal refactors (specifically, streaming is next).",
          "created_at": "2024-12-16T14:12:42Z"
        },
        {
          "author": "sydney-runkle",
          "body": "One other thing I noticed - we have different return types in `_map_model_message` across different models (ex - tuples vs iterables...)",
          "created_at": "2025-01-22T14:31:02Z"
        },
        {
          "author": "AbhishekRP2002",
          "body": "hi @sydney-runkle  if there is a scope for addition of an extra pair of hands to offload few tasks, i will be happy to help \ncheers!",
          "created_at": "2025-03-18T14:37:32Z"
        }
      ]
    },
    {
      "issue_number": 1097,
      "title": "Cannot use partial as a tool",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nMRE, this works:\n\n```python\nfrom pydantic_ai import Agent\n\nasync def returns_the_meaning_of_life() -> int:\n    return 42\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    system_prompt=\"You are searching for the meaning of live.\",\n    tools=[returns_the_meaning_of_life],\n)\n\nagent.run(\"So what's the answer?\")\n```\n\nThis does not work, but IMHO should:\n\n```python\nfrom functools import partial\n\nasync def set_the_meaning_of_life(number: int) -> int:\n    return number\n\nreturns_the_meaning_of_life = partial(set_the_meaning_of_life, number=43)\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    system_prompt=\"You are searching for the meaning of live.\",\n    tools=[returns_the_meaning_of_life],\n)\n\nagent.run(\"So what's the answer?\")\n```\n\nThe error thrown is:\n\n```python\nAttributeError: 'functools.partial' object has no attribute '__name__'\n```\n\nThe use case here is to load tools from config files on app startup, and provide them eg with database connections, user-set params etc.\n\nAny help is greatly appreciated!\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython 3.12, pydantic-ai 0.0.30, openai client 1.65.4\n```",
      "state": "closed",
      "author": "phiweger",
      "author_type": "User",
      "created_at": "2025-03-11T10:27:41Z",
      "updated_at": "2025-03-18T07:27:24Z",
      "closed_at": "2025-03-18T07:26:36Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1097/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Viicos"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1097",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1097",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:40.440310",
      "comments": [
        {
          "author": "Kludex",
          "body": "That's the reason we have dependencies: https://ai.pydantic.dev/dependencies/#defining-dependencies.\n\nWhy is that not enough?",
          "created_at": "2025-03-11T12:07:30Z"
        },
        {
          "author": "phiweger",
          "body": "I'm not sure. Here's my use case:\n\nSay you want to serve two users, each with their own agent, within a single application. Each user adds tools to their agent. So far so good, dependencies are enough.\n\nBut say you want to generalise tools and actually have tool \"types\". Take as example a tool that ",
          "created_at": "2025-03-11T20:23:35Z"
        },
        {
          "author": "phiweger",
          "body": "I managed to do what I intended using dynamic function creation using the `makefun` library (see below); I could see this working (and being simpler) if pydantic-ai allowed partials as tools, or is there a better way @Kludex ?\n\n```python\n# https://smarie.github.io/python-makefun/#1-ex-nihilo-creatio",
          "created_at": "2025-03-12T07:10:05Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "What about this approach?\n```python\nfrom pydantic_ai import Agent, RunContext\n\n\nasync def set_the_meaning_of_life(ctx: RunContext[int]) -> int:\n    return ctx.deps\n\n\nagent = Agent(\n    \"google-gla:gemini-2.0-flash\",\n    system_prompt=\"You are searching for the meaning of live.\",\n    deps_type=int,\n ",
          "created_at": "2025-03-12T19:21:52Z"
        },
        {
          "author": "phiweger",
          "body": "That would not work in my use case bc/ I'd like to use the tool `set_the_meaning_of_life` multiple times, only parametrised differently. ",
          "created_at": "2025-03-12T20:24:47Z"
        }
      ]
    },
    {
      "issue_number": 761,
      "title": "o1 Model fails when making tool calls",
      "body": "Thanks for integrating the o1 model.  I can use o1 agents without tools, but when I try and use tools, I get an unsupported parameter error due to the parallel_tool_calls being set by the pydantic ai framework.  \n\nHere is some code that shows the agent working without tools and produces a BadRequestError when using tools:\n\n```\nagent = Agent(\n    'openai:o1',\n    system_prompt=(\n        'You are a helpful assistant.  Today is 2024-12-12'\n    ),\n)\nresult = agent.run_sync(\"What is today's date?\")\nprint(result)\n\"\"\"\nToday is January 24, 2025.\n\"\"\"\n\n@agent.tool\ndef get_joke(str) -> str:\n    return 'Why did the chicken cross the road? To get to the other side!'\n\nresult = agent.run_sync(\"Call the `get_joke` function\")\n\"\"\"\n---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\n<ipython-input-12-512d4a647163> in <cell line: 0>()\n     19     return 'Why did the chicken cross the road? To get to the other side!'\n     20 \n---> 21 result = agent.run_sync(\"Call the `get_joke` function\")\n     22 \n\n10 frames\n/usr/local/lib/python3.11/dist-packages/openai/_base_client.py in _request(self, cast_to, options, stream, stream_cls, retries_taken)\n   1642 \n   1643             log.debug(\"Re-raising status error\")\n-> 1644             raise self._make_status_error_from_response(err.response) from None\n   1645 \n   1646         return await self._process_response(\n\nBadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'parallel_tool_calls' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'parallel_tool_calls', 'code': 'unsupported_parameter'}}\n\"\"\"\n```\n\nThis code works for openai:gpt-4o but not o1.  Thanks!",
      "state": "open",
      "author": "mtessar",
      "author_type": "User",
      "created_at": "2025-01-24T16:07:05Z",
      "updated_at": "2025-03-18T00:18:22Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/761/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/761",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/761",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:40.670008",
      "comments": [
        {
          "author": "sydney-runkle",
          "body": "@mtessar,\n\nThanks for the report. I'll work on a fix for this today 👍 ",
          "created_at": "2025-01-24T16:29:59Z"
        },
        {
          "author": "mtessar",
          "body": "Pretty sure you are the one who deserves the thanks for adding this support @sydney-runkle ",
          "created_at": "2025-01-24T16:32:53Z"
        },
        {
          "author": "sydney-runkle",
          "body": "@mtessar, could you please confirm your agent works on https://github.com/pydantic/pydantic-ai/pull/764? I'm getting an awfully long delay in responses, but no error 👍 ",
          "created_at": "2025-01-24T17:05:34Z"
        },
        {
          "author": "mtessar",
          "body": "@sydney-runkle I will check",
          "created_at": "2025-01-24T17:54:53Z"
        },
        {
          "author": "mtessar",
          "body": "I couldn't figure out how to get the branch installed with pip but I did temporarily edit the line you did and it worked (and was also very slow for me :) )",
          "created_at": "2025-01-24T18:12:25Z"
        }
      ]
    },
    {
      "issue_number": 910,
      "title": "Coding tool",
      "body": "The plan is to add a coding tool somewhat similar to smolagents, but actually sandboxed and secure.\n\nSee [this tweet](https://x.com/TrelisResearch/status/1889225278997622853), and [this hugging face article](https://huggingface.co/blog/smolagents#code-agents) for some rationale of why coding tools are so useful. cc @RonanKMcGovern.\n\nThe plan is to use [pyodide](https://pyodide.org/en/stable/) to sandbox the code execution, while this will add a bit of setup and complexity, I think a genuinely safe execution environment for code generated by an LLM is pretty important.\n\nI had hoped to offer this as a free service using Cloudflare workers for Python, but it seems that's not possible right now, see https://github.com/pydantic/pydantic.run/pull/42. cc @mikenomitch\n",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-02-12T20:01:35Z",
      "updated_at": "2025-03-17T23:31:30Z",
      "closed_at": "2025-03-17T23:31:30Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/910/reactions",
        "total_count": 6,
        "+1": 5,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/910",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/910",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:40.905377",
      "comments": [
        {
          "author": "odysseus0",
          "body": "curious how we are going to implement this. Is it\n\n- gonna be like executing something through subprocess, where all the structure we have is like exit code and then stdout/stderr?\n- or are we gonna do some kind of fancy introspection to get some kind of type safety out of it?\n\nI am having some diff",
          "created_at": "2025-02-22T00:54:29Z"
        },
        {
          "author": "Kludex",
          "body": "@samuelcolvin said he will be working on this.",
          "created_at": "2025-03-07T13:29:41Z"
        }
      ]
    },
    {
      "issue_number": 1143,
      "title": "Validation errors for _GeminiResponse",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI get the below error when trying to run simple agent with Gemini provider:\n```\nlib/python3.11/site-packages/pydantic/type_adapter.py\", line 468, in validate_json\n    return self.validator.validate_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 4 validation errors for _GeminiResponse\ncandidates.0.avgLogProbs\n  Field required [type=missing, input_value={'content': {'parts': [{'...': -0.05566399544477463}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ncandidates.0.index\n  Field required [type=missing, input_value={'content': {'parts': [{'...': -0.05566399544477463}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ncandidates.0.safetyRatings\n  Field required [type=missing, input_value={'content': {'parts': [{'...': -0.05566399544477463}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\npromptFeedback\n  Field required [type=missing, input_value={'candidates': [{'content...on': 'gemini-2.0-flash'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\n```\n\n### Example Code\n\n```Python\nfrom pydantic_ai import Agent\n\nagent = Agent(model=\"google-gla:gemini-2.0-flash\")\nresult = agent.run_sync(\"Hello, how are you?\")\nprint(result.data)\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython: 3.11.11\nPydantic: 2.11.0b1\nPydantic AI: 0.0.40\n```",
      "state": "closed",
      "author": "torayeff",
      "author_type": "User",
      "created_at": "2025-03-16T23:08:50Z",
      "updated_at": "2025-03-17T09:49:05Z",
      "closed_at": "2025-03-17T09:49:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1143/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Viicos"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1143",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1143",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:41.150330",
      "comments": [
        {
          "author": "torayeff",
          "body": "`pip install pydantic==2.10.6` solved the problem for now.",
          "created_at": "2025-03-16T23:40:36Z"
        },
        {
          "author": "Kludex",
          "body": "Thanks for pointing out.",
          "created_at": "2025-03-17T07:07:14Z"
        }
      ]
    },
    {
      "issue_number": 656,
      "title": "run_stream docs are a little misleading",
      "body": "https://ai.pydantic.dev/api/agent/?h=run_stream#pydantic_ai.agent.Agent.run_stream\r\n\r\nThe example shown uses `response.get_data()` which doesn't result in data being streamed. \r\n\r\nhttps://pydanticlogfire.slack.com/archives/C083V7PMHHA/p1736534340754989 has more details.",
      "state": "open",
      "author": "thoraxe",
      "author_type": "User",
      "created_at": "2025-01-10T18:54:10Z",
      "updated_at": "2025-03-17T07:44:35Z",
      "closed_at": null,
      "labels": [
        "documentation",
        "run_stream"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/656/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/656",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/656",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:41.376533",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "thanks.",
          "created_at": "2025-01-16T09:57:24Z"
        },
        {
          "author": "Kludex",
          "body": "@thoraxe We implemented the `Agent.iter()`, which should be a bit more helpful in those situations. See https://ai.pydantic.dev/agents/#async-for-iteration.\n\nI think we are planning on dropping `Agent.run_stream`. @dmontagu has more details.",
          "created_at": "2025-03-04T08:42:46Z"
        }
      ]
    },
    {
      "issue_number": 760,
      "title": "Multimodal input / output support",
      "body": "Parent issue for organizing requests for multimodal support - ex: images, videos, files, etc.",
      "state": "closed",
      "author": "sydney-runkle",
      "author_type": "User",
      "created_at": "2025-01-24T14:33:44Z",
      "updated_at": "2025-03-17T07:39:34Z",
      "closed_at": "2025-03-17T07:39:32Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/760/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/760",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/760",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:41.596714",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "@sydney-runkle \n\nShould we add this to the Pydantic Docs as a temporary work around under our examples?\n\nhttps://github.com/rawheel/Pydantic-ai-MultiModal-Example",
          "created_at": "2025-01-24T15:48:11Z"
        },
        {
          "author": "sydney-runkle",
          "body": "@izzyacademy,\n\nI think having it linked here is good enough for now, thanks!",
          "created_at": "2025-01-24T15:52:23Z"
        },
        {
          "author": "peppermint-ai-lab",
          "body": "Is there any plan to include image support in message_history? If not then (unrelated) is there anyway to run the chat completion using id (model providers) so that the image remains in context? ",
          "created_at": "2025-02-15T08:51:21Z"
        },
        {
          "author": "Kludex",
          "body": "> Is there any plan to include image support in message_history? If not then (unrelated) is there anyway to run the chat completion using id (model providers) so that the image remains in context?\n\nIt's already being included in the history.",
          "created_at": "2025-03-10T08:53:35Z"
        },
        {
          "author": "krokosik",
          "body": "Is there a workaround for uploading PDFs to Gemini?",
          "created_at": "2025-03-10T11:06:51Z"
        }
      ]
    },
    {
      "issue_number": 731,
      "title": "Add support for `reasoning_content` from DeepSeek R1",
      "body": "New DeepSeek reasoning model outputs `reasoning_content` in messages alongside the regular `content`. See [their API docs here.](https://api-docs.deepseek.com/guides/reasoning_model)\n\nRequested feature is to be able to access this reasoning content from the `Agent.run()` response. According to their docs for multi-turn conversations, reasoning should not be passed back into the messages, so there will need to be consideration of that in the API.\n\nAppreciate your work here, I'm loving the framework so far!\n\nEDIT: OpenRouter supports this content now too, but in a diff format: https://openrouter.ai/announcements/reasoning-tokens-for-thinking-models",
      "state": "closed",
      "author": "charlieyou",
      "author_type": "User",
      "created_at": "2025-01-21T16:59:09Z",
      "updated_at": "2025-03-17T07:32:36Z",
      "closed_at": "2025-03-17T07:32:34Z",
      "labels": [
        "model settings"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/731/reactions",
        "total_count": 15,
        "+1": 15,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/731",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/731",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:41.821663",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Thanks, should be possible, we'll need to think about how.",
          "created_at": "2025-01-21T19:02:52Z"
        },
        {
          "author": "achimstruve",
          "body": "Can the DeepSeek R1 model use tools?\n@samuelcolvin ",
          "created_at": "2025-01-27T08:19:48Z"
        },
        {
          "author": "Wh1isper",
          "body": "@achimstruve According to [this](https://github.com/deepseek-ai/DeepSeek-R1/issues/9#issuecomment-2604747754), not yet",
          "created_at": "2025-02-18T14:05:00Z"
        },
        {
          "author": "Wh1isper",
          "body": "I created a simple project to implement [deepclaude](https://github.com/getAsterisk/deepclaude)-style-agent: https://github.com/Wh1isper/pydantic-ai-deepagent, making models such as claude can use deepseek r1's thinking as a reference for tool use. Check the [example](https://github.com/Wh1isper/pyd",
          "created_at": "2025-02-18T14:05:04Z"
        },
        {
          "author": "Kludex",
          "body": "Let's use a single issue to track reasoning/thinking.",
          "created_at": "2025-03-17T07:32:34Z"
        }
      ]
    },
    {
      "issue_number": 1127,
      "title": "Multi agent stream output without any content, run can output",
      "body": "```\nimport asyncio\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.usage import UsageLimits\nfrom llm import model\nimport logfire\n\nlogfire.configure(send_to_logfire='if-token-present')\n\njoke_selection_agent = Agent(\n    model,\n    system_prompt=(\n        'Use the `joke_factory` to generate some jokes, then choose the best. '\n        'You must return just a single joke.'),\n    instrument=True)\n\njoke_generation_agent = Agent(model, result_type=list[str], instrument=True)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[None], count: int) -> list[str]:\n    language = ctx.deps['language']\n    r = await joke_generation_agent.run(\n        f'Please generate {count} jokes. language: {language}',\n        usage=ctx.usage,\n    )\n    print(r.data)\n    return r.data\n\n\nasync def main_stream():\n    async with joke_selection_agent.run_stream(\n            'Tell me a joke.',\n            usage_limits=UsageLimits(request_limit=5, total_tokens_limit=500),\n            deps={'language': 'en'},\n    ) as result:\n        async for chunk in result.stream():\n            ## nothing\n            print(chunk)\n\n\nasync def main():\n    result = await joke_selection_agent.run(\n        'Tell me a joke.',\n        usage_limits=UsageLimits(request_limit=5, total_tokens_limit=500),\n        deps={'language': 'en'},\n    )\n    print(result.data)\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\n    print(result.usage())\n\n\nif __name__ == '__main__':\n    asyncio.run(main_stream())\n\n\nstream output nothing",
      "state": "open",
      "author": "tianshangwuyun",
      "author_type": "User",
      "created_at": "2025-03-15T07:27:19Z",
      "updated_at": "2025-03-16T12:22:53Z",
      "closed_at": null,
      "labels": [
        "run_stream"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1127/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1127",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1127",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:42.058732",
      "comments": [
        {
          "author": "pydanticai-bot[bot]",
          "body": "PydanticAI Github Bot Found 1 issues similar to this one: \n1. \"https://github.com/pydantic/pydantic-ai/issues/957\" (90% similar)",
          "created_at": "2025-03-15T07:30:07Z"
        },
        {
          "author": "tianshangwuyun",
          "body": "They are different, I didn't output any content，After calling the tool, there is no stream output",
          "created_at": "2025-03-15T15:40:07Z"
        },
        {
          "author": "mdfareed92",
          "body": "change RunContext[None] to RunContext[dict[str,str]]",
          "created_at": "2025-03-15T18:16:10Z"
        },
        {
          "author": "tianshangwuyun",
          "body": "> change RunContext[None] to RunContext[dict[str,str]]\n\nof no avail",
          "created_at": "2025-03-16T00:51:14Z"
        },
        {
          "author": "mdfareed92",
          "body": "try to declare your dependency type on your agent too",
          "created_at": "2025-03-16T01:26:59Z"
        }
      ]
    },
    {
      "issue_number": 1122,
      "title": "Ollama Model",
      "body": "Using Ollama Model:\n```\n# Initialize the model with Ollama provider\nollama_provider = OpenAIProvider(\n    base_url='http://localhost:11434/v1',\n    # http_client=httpx.AsyncClient(timeout=30.0),\n\n)\n\nollama_model = OpenAIModel(\n    model_name='llama3.2',\n    provider=ollama_provider,\n\n)\n```\nWhen sending the request via **run_sync()** with a big input, seems like the model does not receive the big context \n```\nsample_log = \"\"\"\\n\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n2025-03-13 12:56:45   File \"/opt/netbox/venv/lib/python3.10/site-packages/django/template/base.py\", line 173, in render\n\"\"\"\n```\n```\nModelResponse(parts=[TextPart(content=\"I don't see any provided text. Please paste or type out the text you'd like me to analyze, and I'll be happy to assist.\\n\\nOnce I receive the text, I'll review it carefully to identify potential issues, their causes, and suggest resolution steps as needed.\\n\\nPlease go ahead and share the text!\", part_kind='text')], model_name='llama3.2', timestamp=datetime.datetime(2025, 3, 14, 16, 26, 21, tzinfo=datetime.timezone.utc), kind='response')\n\n```",
      "state": "closed",
      "author": "dsavu09",
      "author_type": "User",
      "created_at": "2025-03-14T16:25:28Z",
      "updated_at": "2025-03-16T10:55:51Z",
      "closed_at": "2025-03-16T10:55:50Z",
      "labels": [
        "Stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1122/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1122",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1122",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:42.374767",
      "comments": [
        {
          "author": "Kludex",
          "body": "Please provide a code I can reproduce with `python main.py`. 🙏 ",
          "created_at": "2025-03-16T10:55:17Z"
        },
        {
          "author": "Kludex",
          "body": "Happy to reopen when provided. 🙏 ",
          "created_at": "2025-03-16T10:55:50Z"
        }
      ]
    },
    {
      "issue_number": 1131,
      "title": "OpenTelemetry events should be attached to Span rather than emitted separately as logs",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nI don't think implementation in https://github.com/pydantic/pydantic-ai/pull/945 is correct [according to the OTel spec](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/#capturing-inputs-and-outputs).\n\nThe events should be attached to the OTel span and not emitted separately as OTel logs.\n\n![image](https://github.com/user-attachments/assets/106093d5-f177-48ad-9e3d-a1100e96e671)\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.12.9\npydantic-ai 0.0.39\n```\n\nCC @alexmojaki ",
      "state": "closed",
      "author": "sirianni",
      "author_type": "User",
      "created_at": "2025-03-15T14:12:23Z",
      "updated_at": "2025-03-15T16:06:06Z",
      "closed_at": "2025-03-15T14:34:11Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1131/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "alexmojaki"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1131",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1131",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:42.600105",
      "comments": [
        {
          "author": "alexmojaki",
          "body": "No, 'events' here means a type of log, not span events (which are actually being deprecated in favour of log events). Your screenshot/link of the spec links to https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-events/ which starts with:\n\n> GenAI instrumentations MAY capture user inputs sent ",
          "created_at": "2025-03-15T14:34:05Z"
        },
        {
          "author": "sirianni",
          "body": "Yes, I was aware of this ambiguity but interpreted the spec the other way.  I guess since the event schema used in the semantic conventions is specific to the log signal and does not match the `span events` schema, you are correct.  This is unfortunate as it now requires joining across traces and lo",
          "created_at": "2025-03-15T16:06:05Z"
        }
      ]
    },
    {
      "issue_number": 762,
      "title": "Anthropic stream flag first reasoning message as final with AWS Bedrock",
      "body": "With my agent connected to anthropic. If I do run() the result will be the last message of the agent, resulting from the different tool call. However using:\n```\nasync with agent.run_stream(user_prompt=message.content) as result:\n      async for the token in result.stream():\n          await msg.stream_token(token, is_sequence=True)\n```\nI will stream only the first message made by the agent, basically explaining that it will do a tool call. I expect to stream the last message only.\n\nDo I need to configure this differently? Thanks",
      "state": "closed",
      "author": "celeriev",
      "author_type": "User",
      "created_at": "2025-01-24T16:14:41Z",
      "updated_at": "2025-03-14T14:00:40Z",
      "closed_at": "2025-03-14T14:00:39Z",
      "labels": [
        "more info",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/762/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/762",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/762",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:42.839643",
      "comments": [
        {
          "author": "sydney-runkle",
          "body": "@celeriev,\n\nThanks for the report. Could you please give a bit more context - maybe a snippet with your agent, etc?",
          "created_at": "2025-01-24T16:30:41Z"
        },
        {
          "author": "celeriev",
          "body": "Hello @sydney-runkle. Thanks for your help. I will work on building a small snippet. Generally, this is a simple agent with one tool, retrieving data from a DB. Instead of returning the summary, like what I get with agent.run(), agent.run_stream will output the result of the first LLM call, for exam",
          "created_at": "2025-01-24T17:18:15Z"
        },
        {
          "author": "celeriev",
          "body": "Hello @sydney-runkle , sorry for the initial issue, that was missing enough information indeed\nHere is a snippet of the code that is currently making an issue:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom anthropic import AsyncAnthropicBedrock\nfrom ",
          "created_at": "2025-01-27T18:23:05Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-02-05T14:07:09Z"
        },
        {
          "author": "celeriev",
          "body": "@sydney-runkle do we still need more info ?",
          "created_at": "2025-02-05T14:08:16Z"
        }
      ]
    },
    {
      "issue_number": 695,
      "title": "State persistence",
      "body": "Following #528, we really need a way to store state after each node is run.\r\n\r\nWe should provide:\r\n* an ABC for persisting state\r\n* a disk implementation\r\n* a sqlite implementation\r\n\r\nIn future we'll probably want to provide a postgres implementation, but I think that can come later when we're more confident of the schema.\r\n\r\nThis also relates to other database access and persistence:\r\n* message persistence in graphs #530\r\n* vector search for RAG #58",
      "state": "closed",
      "author": "samuelcolvin",
      "author_type": "User",
      "created_at": "2025-01-15T19:42:27Z",
      "updated_at": "2025-03-14T09:26:50Z",
      "closed_at": "2025-03-14T09:26:50Z",
      "labels": [
        "Feature request",
        "graph"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/695/reactions",
        "total_count": 14,
        "+1": 14,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "samuelcolvin"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/695",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/695",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:43.134889",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "I think we should add a distributed K-V store (Redis) in the initial release of this feature because for apps that have a distributed backend it would benefit from a centralized mechanism to store the state. After your ABC, I can add the redis implementation.",
          "created_at": "2025-01-15T19:44:48Z"
        },
        {
          "author": "rupurt",
          "body": "It might also be rad to also have an event driven version with tiered storage support from memory -> disk -> object storage. I can take a crack when the abc is available also.",
          "created_at": "2025-01-16T02:57:02Z"
        },
        {
          "author": "asaf",
          "body": "Maybe you can take into consideration delegating state to sub graphs (where a node is another graph) that way the entire graph execution could be stored in a single state but also scoped per subgraph.\n\nIt complicates things a bit but sub graphs make much sense when it comes to complex agents executi",
          "created_at": "2025-01-21T17:50:29Z"
        },
        {
          "author": "AlexEnrique",
          "body": "How would this work? We would pass a \"connection or cursor\" down to `.next()` and after it had run, it would store the history in the db?\n\nSince we already have `.dump_history()` and `.load_history()`, in my opinion, that's what we need to persist state.\n\nSo, at least for v1, I would say that it sho",
          "created_at": "2025-02-17T01:06:47Z"
        }
      ]
    },
    {
      "issue_number": 1116,
      "title": "OpenAIProvider Does Not Read OPENAI_BASE_URL from Environment Variables",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nWhen creating a new instance of `OpenAIProvider` like this:  \n\n```python\nprovider = OpenAIProvider()\n```\n\nIf the `OPENAI_BASE_URL` environment variable is set, it is expected that `provider.base_url` should reflect that value. However, it is not being read from the environment variables, and instead, `provider.base_url` defaults to `'https://api.openai.com/v1'`.\n\n### Example Code\n\n```Python\nimport os\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n# Set the environment variable\ncustom_openai_base_url = \"https://custom-openai.com/v1\"\nos.environ[\"OPENAI_BASE_URL\"] = custom_openai_base_url\n\n# Create an instance of OpenAIProvider\nprovider = OpenAIProvider()\n\n# Check if the base_url is set correctly\nprint(f\"Expected: {custom_openai_base_url}\")\nprint(f\"Actual: {provider.base_url}\")\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n0.0.39\n```",
      "state": "closed",
      "author": "hrahmadi71",
      "author_type": "User",
      "created_at": "2025-03-13T14:23:13Z",
      "updated_at": "2025-03-13T14:43:21Z",
      "closed_at": "2025-03-13T14:43:21Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1116/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1116",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1116",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:43.421710",
      "comments": []
    },
    {
      "issue_number": 198,
      "title": "How can I pass in files for models with multimodal capabilities?",
      "body": "Gemini supports a number of file types, Anthropic supports images and pdfs, and several providers support images.  How can we utilize those features in agents backed by models with multimodal capabilities?\r\n\r\nIn my particular situation, I'm working with both the Gemini and Vertex APIs (which are a bit different) and I'd like to know if its possible to include files where needed in my agents backed by either model API?\r\n\r\nThanks much!",
      "state": "closed",
      "author": "ohmeow",
      "author_type": "User",
      "created_at": "2024-12-09T23:07:54Z",
      "updated_at": "2025-03-13T10:42:25Z",
      "closed_at": "2025-03-13T10:42:21Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/198/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/198",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/198",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:43.421733",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "Related to #126, we intend to add multi-modal support in future, but we haven't started that yet.",
          "created_at": "2024-12-10T08:46:39Z"
        },
        {
          "author": "rawheel",
          "body": "@ohmeow https://github.com/rawheel/Pydantic-ai-MultiModal-Example this is a way to process images with pydantic-ai",
          "created_at": "2024-12-21T00:31:13Z"
        },
        {
          "author": "krokosik",
          "body": "This workaround seems to work with OpenAI, what about Gemini? I would love to move my PDF extraction app to Pydantic",
          "created_at": "2025-03-10T11:05:23Z"
        },
        {
          "author": "Kludex",
          "body": "PydanticAI 0.0.38 now supports document input, see: https://ai.pydantic.dev/input/.",
          "created_at": "2025-03-13T10:42:21Z"
        }
      ]
    },
    {
      "issue_number": 152,
      "title": "Support LiteLLM",
      "body": "[LiteLLM](https://docs.litellm.ai/docs/) gives you most if not all the providers through one API. You could even imagine centralizing on this one library to be able to focus your attention on the higher value added features you want to add to the library instead of running after each individual LLM API provider separately.",
      "state": "closed",
      "author": "ndilsou",
      "author_type": "User",
      "created_at": "2024-12-05T22:33:12Z",
      "updated_at": "2025-03-12T21:07:34Z",
      "closed_at": "2024-12-11T18:20:22Z",
      "labels": [
        "Feature request",
        "new models"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/152/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/152",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/152",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:45.659779",
      "comments": [
        {
          "author": "samuelcolvin",
          "body": "I definitely don't want to replace the current models with litellm, but happy to review a pr to add it as an addition model.",
          "created_at": "2024-12-06T12:28:09Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "@ndilsou \r\nI would definitely recommend against litellm, I mean just look at their repo for one minute. It's extremely non-pydantic, and their poor typing would create nightmares for the type-guardians of the python ecossystem.  \r\n",
          "created_at": "2024-12-06T13:41:30Z"
        },
        {
          "author": "laxas",
          "body": "What about https://github.com/andrewyng/aisuite ? That seems more lightweight but would also enable a variety of models at once.",
          "created_at": "2024-12-06T13:51:22Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Putting it politely, it looks like aisuite will suffer from the same drawbacks @pedroallenrevez mentioned about litellm.",
          "created_at": "2024-12-06T16:25:44Z"
        },
        {
          "author": "samuelcolvin",
          "body": "I think on balance it would be a mistake to support LiteLLM right now.\r\n\r\nClosing this, but happy to re-open if I'm wrong.",
          "created_at": "2024-12-11T18:20:22Z"
        }
      ]
    },
    {
      "issue_number": 1105,
      "title": "gpt-4o: `parallel_tool_calls` calls too many tools, fails with `Expected an array with maximum length 128`",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nThis might be a OpenAI bug rather than pydantic-ai, but I suspect it can be the library can work around it.\n\nWhen using `parallel_tool_calls=True` if `gpt-4o` decides to call more than 128 tools in one single go then pydantic-ai will reply with all the tool responses in a single reply, and that fails with `400`:\n\n\n```\n  File \".../lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 294, in _completions_create\n    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gpt-4o, body: {'message': \"Invalid 'messages[3].tool_calls': array too long. Expected an array with maximum length 128, but got an array with length 200 instead.\", 'type': 'invalid_request_error', 'param': 'messages[3].tool_calls', 'code': 'array_above_max_length'}\n```\n\nIf you have a valid `$OPENAI_API_KEY` in your env you can verify with `uv run` in the shared [script](https://gist.github.com/enigma/f2062579acbade3593983652af7b48b8).\n\nI'm not sure what's the best way to solve this, probably chunking the tool responses across multiple replies. Is there any workaround for this beyond asking the model kindly to \"never call more than 128 tools at a time\"?\n\n### Example Code\n\n```Python\n# /// script\n# requires-python = \">=3.12\"\n# dependencies = [\n#     \"pydantic-ai==0.0.37\",\n# ]\n# ///\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.settings import ModelSettings\nimport asyncio\n\n\nagent = Agent(\n    name=\"MyAgent\",\n    result_type=list[int],\n    model=\"gpt-4o\",\n    model_settings=ModelSettings(temperature=0, parallel_tool_calls=True, seed=42))\n\n@agent.tool\ndef list_ids(ctx: RunContext) -> list[int]:\n    print(\"listing ids\")\n    return list(range(200))\n\n@agent.tool\ndef is_id_good(ctx: RunContext, id: int) -> bool:\n    print(f\"checking if id {id} is good\")\n    return id > 20\n\n\nasync def main():\n    run = await agent.run(\"Get all good IDs from the list of ids.\")\n    print(run.data)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\n- Python 3.12\n- pydantic-ai==0.0.37\n- gpt-4o\n```",
      "state": "open",
      "author": "enigma",
      "author_type": "User",
      "created_at": "2025-03-12T16:14:25Z",
      "updated_at": "2025-03-12T16:19:20Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1105/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1105",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1105",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:45.860102",
      "comments": []
    },
    {
      "issue_number": 640,
      "title": "Streaming Tool Calls",
      "body": "Hey! First of all, thanks for the library - it really makes starting to build agents a lot of fun!\r\n\r\nI was wondering if it's not possible **to stream tool calls** to have a responsive frontend, or if I'm just missing something completely (tried looking through the issues but couldn't figure it out either).\r\n\r\nI am connecting a FastAPI backend running agents with pydantic-ai to a frontend using the [ai-sdk](https://sdk.vercel.ai/docs/ai-sdk-ui) and want to show the user when a tool is called (similar to ChatGPT) as this can take a while in some cases.\r\n\r\nI am interpreting this section from the [documentation](https://ai.pydantic.dev/results/#streamed-results):\r\n\r\n> When we get a response, we don't know if it's the final response without starting the stream and looking at the content. PydanticAI streams just enough of the response to sniff out if it's a tool call or a result, then streams the whole thing and calls tools, or returns the stream as a [StreamedRunResult](https://ai.pydantic.dev/api/result/#pydantic_ai.result.StreamedRunResult).\r\n\r\n... that only the final result can be streamed. Is this correct?",
      "state": "closed",
      "author": "martinfg",
      "author_type": "User",
      "created_at": "2025-01-08T12:50:16Z",
      "updated_at": "2025-03-10T15:34:44Z",
      "closed_at": "2025-03-10T15:34:42Z",
      "labels": [
        "Feature request"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/640/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/640",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/640",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:45.860123",
      "comments": [
        {
          "author": "dmontagu",
          "body": "I _believe_ #468 will address this — it should expose a stream of all events, which in particular will include (non-return) tool calls.",
          "created_at": "2025-01-08T19:28:04Z"
        },
        {
          "author": "physicsrob",
          "body": "I'm also interested in streaming tool calls to have a responsive frontend. And despite my best efforts it doesn't seem like that's currently possible.\r\n\r\n@dmontagu I'd love clarification on how your PR might help address this need. I tried what I consider to be the obvious things but wasn't able to ",
          "created_at": "2025-01-12T06:53:53Z"
        },
        {
          "author": "physicsrob",
          "body": "Once this functionality is available it would be fantastic to modify the weather_agent_gradio.py example to show the thought process and stream the results.",
          "created_at": "2025-01-12T06:55:20Z"
        },
        {
          "author": "pedroallenrevez",
          "body": "@dmontagu Does this mean we could potentially, inside a tool call, `yield` a message, for example to indicate to the user, that a tool is currently running?",
          "created_at": "2025-01-14T10:07:12Z"
        },
        {
          "author": "ohmeow",
          "body": "I would love to see something that works like this Instructor + LangGrah example I put together awhile back (but without all the dependencies to LangChain) ...\n\nhttps://gist.github.com/ohmeow/03a7dee8fbfb92ac42bb5ed732e23c84#file-06_a_langraph_instructor-ipynb\n\nConceptually, this is my pref. way to ",
          "created_at": "2025-02-01T22:05:40Z"
        }
      ]
    },
    {
      "issue_number": 1081,
      "title": "Error serializing to JSON: invalid utf-8 sequence of 1 bytes from index 0",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nThe error occurs when running: response.all_messages_json() when there's a BinaryContent object (in this case, an image).\n\n### Example Code\n\n```Python\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ImageUrl, AudioUrl, BinaryContent\n\nresponse = await self.agent.run(input, message_history=chat_history)\nresponse.all_messages_json()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.10.9\npydantic-ai-0.0.36\nAzure OpenAI\n```",
      "state": "closed",
      "author": "Rockyyost",
      "author_type": "User",
      "created_at": "2025-03-08T20:03:58Z",
      "updated_at": "2025-03-10T13:15:28Z",
      "closed_at": "2025-03-10T13:15:27Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1081/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1081",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1081",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:46.162952",
      "comments": [
        {
          "author": "Kludex",
          "body": "You are supposed to use `response.all_messages()` if you want to pass to `message_history`.\n\nI suggest using a type checker to avoid this kind of issue.",
          "created_at": "2025-03-10T08:16:55Z"
        },
        {
          "author": "Rockyyost",
          "body": "Thanks for the response, but that's not what I'm using the json about for. I do use your recommended method for continuous conversions while in session. \n\nI store and later use the json output. ",
          "created_at": "2025-03-10T12:35:55Z"
        }
      ]
    },
    {
      "issue_number": 1082,
      "title": "Cannot upload images to gemini",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nHi, i have been trying to upload an image to gemini mode gemini-2.0-flash-thinking-exp-01-21 which accepts multimodal image.\nHowever when I try to upload the image which has a file extension it seems to break. It works fine if there is some url on the internet which has an image but the URL does not have file extension.\n\nbelow is the attached response\n\n```\npydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.0-flash-thinking-exp-01-21, body: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Request contains an invalid argument.\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n```\n\n### Example Code\n\n```Python\nimport os \nimport io\nimport base64\nfrom dotenv import load_dotenv\nfrom pydantic_ai.models.gemini import GeminiModel\nfrom pydantic_ai.agent import Agent\nfrom pydantic_ai.messages import ImageUrl\nfrom PIL import Image\n\nload_dotenv(override=True)\n\nllm = GeminiModel(os.getenv('MODEL'))\n\npydantic_agent = Agent(\n    model=llm,\n    system_prompt=\"You are an helpful agent for reading and extracting data from images and pdfs.\"\n)\n\ndef image_to_byte_string(image: Image.Image) -> str:\n        img_byte_arr = io.BytesIO()\n        image.save(img_byte_arr, format='PNG')  # or 'JPEG', etc.\n        img_byte_arr = img_byte_arr.getvalue()\n        return base64.b64encode(img_byte_arr).decode('utf-8')\n\ndef main():\n    image = Image.open(\"./fuzzy-bunnies.png\")\n    formatted_image=image_to_byte_string(image)\n    input = [\n        \"What is this image of?\",\n        # ImageUrl(f'data:{image.get_format_mimetype()};base64,{formatted_image}'),\n        ImageUrl(url='https://hatrabbits.com/wp-content/uploads/2017/01/random.png')\n    ]\n\n    response = pydantic_agent.run_sync(input)\n    print(response.data)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\npython: 3.9.6\npydantic_core: 2.27.2\npydantic_ai: 0.0.36\n```",
      "state": "open",
      "author": "Atoo35",
      "author_type": "User",
      "created_at": "2025-03-08T21:08:41Z",
      "updated_at": "2025-03-10T10:29:56Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1082/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1082",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1082",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:46.388737",
      "comments": [
        {
          "author": "grassxyz",
          "body": "You will need to wrap it in the right component or insert it using PIL\r\nwithout base64 encoding.  This pages show two examples.\r\n\r\nhttps://ai.google.dev/gemini-api/docs/vision?lang=python\r\n\r\n\r\nI have been wrapping it with “types.Part” and it works perfectly.\r\n\r\nOn Sat, Mar 8, 2025 at 1:14 PM Atharva",
          "created_at": "2025-03-08T21:31:39Z"
        },
        {
          "author": "Atoo35",
          "body": "sorry forgot to clean it up a bit, but we can ignore the PIL stuff since I am directly passing the URL to the ImageUrl object.\nAlthough I am assuming you already saw that and chose to ignore it, Is this specific for gemini only? will simply adding image URL for say openai work?",
          "created_at": "2025-03-08T22:18:08Z"
        },
        {
          "author": "Kludex",
          "body": "Yes @Atoo35, I can reproduce that. I'm not sure what is happening... 🤔 \n\nMaybe the Google GLA Provider doesn't accept files of any kind? Does someone know?\n\nI could use the same model with Google Vertex Provider.",
          "created_at": "2025-03-10T10:06:19Z"
        },
        {
          "author": "Kludex",
          "body": "Okay. It seems that what I said was right.\n\nSee https://discuss.ai.google.dev/t/i-am-using-google-generative-ai-model-gemini-1-5-pro-for-image-analysis-but-getting-error/34866/4",
          "created_at": "2025-03-10T10:26:03Z"
        },
        {
          "author": "Kludex",
          "body": "It seems far simpler for us to download the image, and send the base64 encoded image instead of calling the File API they mentioned.",
          "created_at": "2025-03-10T10:28:14Z"
        }
      ]
    },
    {
      "issue_number": 642,
      "title": "Interrupt before making a tool call (human in the loop)",
      "body": "We attach a variety of tools to an agent. Some are read only and \"safe\", while others have a side effect (writing to a database) and are \"unsafe\". If the agent wishes to use an unsafe tool, is there some sort of mechanism to:\r\n1) stop execution early before using the unsafe tool\r\n2) save message history in a data store, including an indication the agent wants to make an unsafe tool call, with prescribed parameters,\r\n3) if a human approves of the unsafe tool call with prescribed parameters, then restore message history and append human approval, and resume execution. With human approval, the agent is allowed to call the unsafe tool with the prescribed parameters.\r\n\r\nThank you for your work on this terrific library, and for your work on pydantic as well.",
      "state": "closed",
      "author": "hwong557",
      "author_type": "User",
      "created_at": "2025-01-08T15:59:35Z",
      "updated_at": "2025-03-10T06:53:40Z",
      "closed_at": "2025-01-20T14:07:13Z",
      "labels": [
        "question",
        "Stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/642/reactions",
        "total_count": 5,
        "+1": 5,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/642",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/642",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:46.608527",
      "comments": [
        {
          "author": "izzyacademy",
          "body": "@hwong557 my initial thoughts are that this type of design would be simpler with a state machine or network of graphs such that you can group the unsafe tools to an agent that gets routed to if those tools needs to be invoked and then you can send it back to the previous agent node when you are done",
          "created_at": "2025-01-08T16:23:36Z"
        },
        {
          "author": "sydney-runkle",
          "body": "Indeed, seems related to https://github.com/pydantic/pydantic-ai/pull/142 as well.",
          "created_at": "2025-01-08T19:18:47Z"
        },
        {
          "author": "samuelcolvin",
          "body": "Graph support is coming in #528.\r\n\r\nMessage storage is coming, see #530.\r\n\r\nOtherwise, if you want to confirm before running unsafe code, you should either:\r\n\r\nUse an `input()` or `rich.prompt()` inside a tool call - this works well for demos, but only works well if you are happy for the agent run t",
          "created_at": "2025-01-09T11:11:15Z"
        },
        {
          "author": "HamzaFarhan",
          "body": "A custom [prepare function](https://ai.pydantic.dev/tools/#tool-prepare) might also be useful here to just drop the unsafe tool entirely depending on the RunContext. Since the RunContext has messages and usage, you could even have another agent inside the prepare function that uses those messages as",
          "created_at": "2025-01-10T06:21:57Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale, and will be closed in 3 days if no reply is received.",
          "created_at": "2025-01-17T14:06:46Z"
        }
      ]
    },
    {
      "issue_number": 985,
      "title": "Plan and Execute is not working inside one Agent",
      "body": "✅ Initial Checks\n\t•\tI confirm that I’m using the latest version of Pydantic AI\n\n📝 Description\nHello, Pydantic team 👋\n\nI encountered an issue with my agent setup. Below is a minimal example of the error using a weather agent.\n\n---\n ## Expected Behavior\n\nI want to leverage the LLM’s capabilities to print the plan to the user before execution, while simultaneously performing the tool call. This is possible with vanilla APIs. The best way to reproduce the issue is to use Claude and the example below.\nI expect only a streaming response.\n## Expected Output:\nUser: Asks about the weather and the plan\n<agent_run>\nAgent: Prints plan\nAgent: Executes tool call\nAgent: Prints response based on tool calls\n</agent_run>\n\n## Actual Behavior\nUser: Asks about the weather and the plan\n<agent_run>\nAgent: Prints plan\nEND\n</agent_run>\n\nI see that the LLM returned tools to call, but the agent did not call them for some reason.\n</summary>\n\n❓ Question\n\nHow can I achieve this behavior with pydantic-ai? What am I doing wrong?\nThank you for your support! ❤️\n\n---\n📌 Example Code\n\n<details>\n\n```python\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom devtools import debug\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass Deps:\n    \"\"\"Dependencies required for the weather application.\"\"\"\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\nmodel = AnthropicModel('claude-3-5-sonnet-20240620')\n\nasync def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> dict[str, float]:\n    \"\"\"Get the latitude and longitude of a location.\"\"\"\n    if ctx.deps.geo_api_key is None:\n        return {'lat': 51.1, 'lng': -0.1}  # Dummy response (London)\n\n    params = {'q': location_description, 'api_key': ctx.deps.geo_api_key}\n    with logfire.span('calling geocode API', params=params) as span:\n        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    if data:\n        return {'lat': float(data[0]['lat']), 'lng': float(data[0]['lon'])}\n    else:\n        raise ModelRetry('Could not find the location')\n\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \"\"\"Get the weather at a location.\"\"\"\n    if ctx.deps.weather_api_key is None:\n        return {'temperature': '21°C', 'description': 'Sunny'}  # Dummy response\n\n    params = {\n        'apikey': ctx.deps.weather_api_key,\n        'location': f'{lat},{lng}',\n        'units': 'metric',\n    }\n    with logfire.span('calling weather API', params=params) as span:\n        r = await ctx.deps.client.get('https://api.tomorrow.io/v4/weather/realtime', params=params)\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    values = data['data']['values']\n    code_lookup = {\n        1000: 'Clear, Sunny', 1100: 'Mostly Clear', 1101: 'Partly Cloudy',\n        1102: 'Mostly Cloudy', 1001: 'Cloudy', 2000: 'Fog', 2100: 'Light Fog',\n        4000: 'Drizzle', 4001: 'Rain', 4200: 'Light Rain', 4201: 'Heavy Rain',\n        5000: 'Snow', 5001: 'Flurries', 5100: 'Light Snow', 5101: 'Heavy Snow',\n        6000: 'Freezing Drizzle', 6001: 'Freezing Rain', 6200: 'Light Freezing Rain',\n        6201: 'Heavy Freezing Rain', 7000: 'Ice Pellets', 7101: 'Heavy Ice Pellets',\n        7102: 'Light Ice Pellets', 8000: 'Thunderstorm',\n    }\n    return {\n        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n    }\n\nweather_agent = Agent(\n    model,\n    system_prompt=(\n        'Be concise, reply with one sentence. '\n        'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n        'then use the `get_weather` tool to get the weather.'\n    ),\n    tools=[get_lat_lng, get_weather],\n    deps_type=Deps,\n    retries=3,\n)\n\nasync def main() -> None:\n    \"\"\"Main entry point for the weather application.\"\"\"\n    async with AsyncClient() as client:\n        weather_api_key = os.getenv('WEATHER_API_KEY')\n        geo_api_key = os.getenv('GEO_API_KEY')\n        deps = Deps(client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key)\n\n        async with weather_agent.run_stream(\n            'What is the weather like in London and in Wiltshire? Please respond with plan and then start to execute it',\n            deps=deps,\n        ) as result:\n            async for chunk in result.stream():\n                print(chunk, end='', flush=True)\n            print()  # New line after response\n        debug(result)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n</details>\n\n\n🔢 Python, Pydantic AI & LLM Client Version\n```bash\n0.0.26\n```\n\n❌ Error Output\n<details>\n\n```bash\npd-3.12➜  pydanticai_experimental git:(main) ✗ python weather.py\n10:16:59.368 weather_agent run prompt=What is the weather like in London and in Wiltshire? Please respond with plan and then start to execute it\n10:16:59.387   preparing model request params run_step=1\n10:16:59.388   model request\nLogfire project URL: https://logfire.pydantic.dev/worldinnovationsdepartment/pd-experimental\n10:17:00.337     response stream structured\nCertainly. Here's the plan and execution:\n\nPlan:\n1. Get latitude and longitude for London\n2. Get latitude and longitude for Wiltshire\n3. Use the coordinates to fetch weather information for both locations\nExecution:\n10:17:01.962       running tools=['get_lat_lng', 'get_lat_lng']\n```\n\nProblem:\n\t•\tThe agent prints the plan but does not execute the tool calls.\n\t•\tThe LLM returned tools to call, but they were not executed.\n\n</details>\nThanks in advance! ❤️",
      "state": "open",
      "author": "WorldInnovationsDepartment",
      "author_type": "User",
      "created_at": "2025-02-25T10:18:54Z",
      "updated_at": "2025-03-07T23:17:42Z",
      "closed_at": null,
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/985/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "dmontagu"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/985",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/985",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:46.902363",
      "comments": [
        {
          "author": "Kludex",
          "body": "Please format your description.\n\nIf necessary, use the following to collapse content, and make it easier to read:\n\n```\n<details>\n<summary>Details</summary>\n\n</details>\n```\n",
          "created_at": "2025-02-25T10:51:05Z"
        },
        {
          "author": "WorldInnovationsDepartment",
          "body": "@Kludex done",
          "created_at": "2025-02-25T10:55:42Z"
        },
        {
          "author": "Kludex",
          "body": "@WorldInnovationsDepartment I talked to @dmontagu , and we are trying to solve that issue in https://github.com/pydantic/pydantic-ai/pull/951/. 🙏 ",
          "created_at": "2025-02-25T15:04:46Z"
        },
        {
          "author": "WorldInnovationsDepartment",
          "body": "Thank you so much, everyone! Your framework is truly the best! 🚀",
          "created_at": "2025-02-25T16:06:54Z"
        },
        {
          "author": "Kludex",
          "body": "The `Agent.iter()` was merged.\n\nIs that enough here?",
          "created_at": "2025-03-07T23:17:41Z"
        }
      ]
    },
    {
      "issue_number": 1077,
      "title": "Modifying ToolDefinition.parameters_json_schema in prepare function fails",
      "body": "### Initial Checks\n\n- [x] I confirm that I'm using the latest version of Pydantic AI\n\n### Description\n\nGiven a tool function declared as:\n\n```def execute_tool_fail(**kwargs: dict[str, t.Any]) -> str:```\n\nAnd a `prepare` function that assigns a custom `ToolDefinition.parameters_json_schema`, pydantic-ai fails to validate the function arguments.\n\nIt seems to be using an different `Tool._validator` derived from the initial function definition, not the from schema set in `prepare`.\n\nIf we change the tool definition to (get rid of the type hints on `kwargs`:\n\n```def execute_tool_succeed(**kwargs) -> str:```\n\nThen it works.\n\n```\nexecute_tool_succeed {'path': 'a'}\nTraceback (most recent call last):\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/tools.py\", line 275, in run\n    args_dict = self._validator.validate_python(message.args)\npydantic_core._pydantic_core.ValidationError: 1 validation error for execute_tool_fail\npath\n  Input should be a valid dictionary [type=dict_type, input_value='a', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/repro.py\", line 44, in <module>\n    main()\n    ~~~~^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/repro.py\", line 40, in main\n    agent_fail.run_sync(\"query\")\n    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 558, in run_sync\n    return get_event_loop().run_until_complete(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self.run(\n        ^^^^^^^^^\n    ...<9 lines>...\n        )\n        ^\n    )\n    ^\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\", line 725, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 316, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 1352, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 734, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 723, in next\n    self._next_node = await self.graph.next(node, history, state=state, deps=deps, infer_name=False)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 305, in next\n    next_node = await node.run(ctx)\n                ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 365, in run\n    async with self.stream(ctx):\n               ~~~~~~~~~~~^^^^^\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 221, in __aexit__\n    await anext(self.gen)\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 380, in stream\n    async for _event in stream:\n        pass\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 417, in _run_stream\n    async for event in self._events_iterator:\n        yield event\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 407, in _run_stream\n    async for event in self._handle_tool_calls(ctx, tool_calls):\n        yield event\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 446, in _handle_tool_calls\n    async for event in process_function_tools(\n    ...<6 lines>...\n        yield event\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 631, in process_function_tools\n    result = task.result()\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/tools.py\", line 277, in run\n    return self._on_error(e, message)\n           ~~~~~~~~~~~~~~^^^^^^^^^^^^\n  File \"/Users/aw/Projects/rectalogic/pydantic-mcp/.venv/lib/python3.13/site-packages/pydantic_ai/tools.py\", line 325, in _on_error\n    raise UnexpectedModelBehavior(f'Tool exceeded max retries count of {self.max_retries}') from exc\npydantic_ai.exceptions.UnexpectedModelBehavior: Tool exceeded max retries count of 1\n```\n\n### Example Code\n\n```Python\nimport typing as t\n\nfrom pydantic_ai import Agent, RunContext, Tool\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.tools import ToolDefinition\n\n\nasync def prepare_tool(ctx: RunContext, tool_def: ToolDefinition) -> ToolDefinition:\n    tool_def.parameters_json_schema = {\n        \"type\": \"object\",\n        \"properties\": {\"path\": {\"type\": \"string\"}},\n        \"required\": [\"path\"],\n        \"additionalProperties\": False,\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    }\n    return tool_def\n\n\ndef execute_tool_fail(**kwargs: dict[str, t.Any]) -> str:\n    print(f\"execute_tool_fail {kwargs}\")\n    return \"tool results\"\n\n\ndef execute_tool_succeed(**kwargs) -> str:\n    print(f\"execute_tool_succeed {kwargs}\")\n    return \"tool results\"\n\n\ndef main():\n    tool_succeed = Tool(\n        execute_tool_succeed, prepare=prepare_tool, name=\"tool_succeed\", description=\"tool desc\", takes_ctx=False\n    )\n    agent_succeed = Agent(model=TestModel(), tools=[tool_succeed])\n    agent_succeed.run_sync(\"query\")\n\n    tool_fail = Tool(\n        execute_tool_fail, prepare=prepare_tool, name=\"tool_fail\", description=\"tool desc\", takes_ctx=False\n    )\n    agent_fail = Agent(model=TestModel(), tools=[tool_fail])\n    agent_fail.run_sync(\"query\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Python, Pydantic AI & LLM client version\n\n```Text\nPython 3.13.2\npydantic_ai 0.0.36\n```",
      "state": "closed",
      "author": "rectalogic",
      "author_type": "User",
      "created_at": "2025-03-07T15:40:10Z",
      "updated_at": "2025-03-07T19:53:49Z",
      "closed_at": "2025-03-07T19:53:48Z",
      "labels": [
        "need confirmation"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1077/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/1077",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/1077",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:47.179358",
      "comments": [
        {
          "author": "rectalogic",
          "body": "I think my type hint was incorrect - should be this, which works.\n\n```def execute_tool_fail(**kwargs: t.Any) -> str:```",
          "created_at": "2025-03-07T19:53:48Z"
        }
      ]
    },
    {
      "issue_number": 118,
      "title": "Support for AWS Bedrock",
      "body": "It would be nice to have support for Bedrock so that application that use AWS can integrate more seamlessly. ",
      "state": "closed",
      "author": "abtawfik",
      "author_type": "User",
      "created_at": "2024-12-02T19:37:59Z",
      "updated_at": "2025-03-07T13:16:12Z",
      "closed_at": "2025-03-07T10:32:35Z",
      "labels": [
        "new models"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 29,
      "reactions": {
        "url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/118/reactions",
        "total_count": 44,
        "+1": 32,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 8,
        "rocket": 0,
        "eyes": 4
      },
      "assignees": [
        "Kludex"
      ],
      "milestone": null,
      "html_url": "https://github.com/pydantic/pydantic-ai/issues/118",
      "api_url": "https://api.github.com/repos/pydantic/pydantic-ai/issues/118",
      "repository": "pydantic/pydantic-ai",
      "extraction_date": "2025-06-22T00:28:47.429438",
      "comments": []
    }
  ]
}