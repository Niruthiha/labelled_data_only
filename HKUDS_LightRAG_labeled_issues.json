{
  "repository": "HKUDS/LightRAG",
  "repository_info": {
    "repo": "HKUDS/LightRAG",
    "stars": 17694,
    "language": "Python",
    "description": "\"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
    "url": "https://github.com/HKUDS/LightRAG",
    "topics": [
      "genai",
      "gpt",
      "gpt-4",
      "graphrag",
      "knowledge-graph",
      "large-language-models",
      "llm",
      "rag",
      "retrieval-augmented-generation"
    ],
    "created_at": "2024-10-02T11:57:54Z",
    "updated_at": "2025-06-22T01:03:05Z",
    "search_query": "RAG retrieval augmented language:python stars:>3",
    "total_issues_estimate": 144,
    "labeled_issues_estimate": 136,
    "labeling_rate": 94.7,
    "sample_labeled": 36,
    "sample_total": 38,
    "has_issues": true,
    "repo_id": 866513204,
    "default_branch": "main",
    "size": 35293
  },
  "extraction_date": "2025-06-21T23:35:44.493586",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 486,
  "issues": [
    {
      "issue_number": 1682,
      "title": "[Question]:Cannot build knowledge graph using Ollama backend — only naive vector search works",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI'm trying to use Ollama locally to run the LightRAG project.\nI followed the setup steps referenced from several Bilibili videos and CSDN blogs, but I still couldn't resolve my issue. I would really appreciate your help in identifying what's wrong with my configuration or deployment process.\nMy setup steps:\nVerified that Ollama is running on http://localhost:11434/\nCloned and extracted https://github.com/HKUDS/LightRAG.git\nCreated a virtual environment using conda create -n lightrag python=3.10\nActivated it: conda activate lightrag\nInstalled the package: pip install -e .\nDownloaded book.txt (\"A Christmas Carol\") and placed it in the project root\nModified examples/lightrag_ollama_demo.py, specifically lines 85–105 to use:\n'''\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"qwen2\", # gemma2:9b also doesn't work\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",\n            \"options\": {\"num_ctx\": 32768},\n            \"timeout\": 300,\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embed(\n                texts,\n                embed_model=\"nomic-embed-text\",\n                host=\"http://localhost:11434\",\n            ),\n        ),\n    )\n'''\n\n\n### Additional Context\n\n(lightrag) E:\\PycharmProject\\LightRAG>python examples/lightrag_ollama_demo.py\n2025-06-15 19:30:44 - pipmaster.package_manager - INFO - Targeting pip associated with Python: E:\\anaconda\\envs\\lightrag\\python.exe | Command base: E:\\anaconda\\envs\\lightrag\\python.exe -m pip\n2025-06-15 19:30:44 - pipmaster.package_manager - INFO - Executing: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade ollama\n2025-06-15 19:30:49 - pipmaster.package_manager - INFO - Command succeeded: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade ollama\n\nLightRAG compatible demo log file: E:\\PycharmProject\\LightRAG\\lightrag_ollama_demo.log\n\nDeleting old file:: ./dickens\\kv_store_doc_status.json\nINFO: Process 9400 Shared-Data created for Single Process\n2025-06-15 19:30:51 - pipmaster.package_manager - INFO - Executing: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade networkx\n2025-06-15 19:30:59 - pipmaster.package_manager - INFO - Command succeeded: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade networkx\n2025-06-15 19:30:59 - pipmaster.package_manager - INFO - Executing: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade graspologic\n2025-06-15 19:32:35 - pipmaster.package_manager - INFO - Command succeeded: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade graspologic\nINFO: Created new empty graph\n2025-06-15 19:32:36 - pipmaster.package_manager - INFO - Executing: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade nano-vectordb\n2025-06-15 19:32:38 - pipmaster.package_manager - INFO - Command succeeded: E:\\anaconda\\envs\\lightrag\\python.exe -m pip install --upgrade nano-vectordb\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data\nINFO: Process 9400 initialized updated flags for namespace: [full_docs]\nINFO: Process 9400 ready to initialize storage namespace: [full_docs]\nINFO: Process 9400 KV load full_docs with 0 records\nINFO: Process 9400 initialized updated flags for namespace: [text_chunks]\nINFO: Process 9400 ready to initialize storage namespace: [text_chunks]\nINFO: Process 9400 KV load text_chunks with 0 records\nINFO: Process 9400 initialized updated flags for namespace: [entities]\nINFO: Process 9400 initialized updated flags for namespace: [relationships]\nINFO: Process 9400 initialized updated flags for namespace: [chunks]\nINFO: Process 9400 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 9400 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 9400 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 9400 KV load llm_response_cache with 4 records\nINFO: Process 9400 initialized updated flags for namespace: [doc_status]\nINFO: Process 9400 ready to initialize storage namespace: [doc_status]\nINFO: Process 9400 doc status load doc_status with 0 records\nINFO: Process 9400 storage namespace already initialized: [full_docs]\nINFO: Process 9400 storage namespace already initialized: [text_chunks]\nINFO: Process 9400 storage namespace already initialized: [llm_response_cache]\nINFO: Process 9400 storage namespace already initialized: [doc_status]\nINFO: Process 9400 Pipeline namespace initialized\nINFO: limit_async: 16 new workers initialized\nINFO: Storage Initialization completed!\n\n=======================\nTest embedding function\n========================\nTest dict: ['This is a test string for embedding.']\nDetected embedding dimension: 768\n\n\nINFO: Stored 1 new unique documents\nINFO: Processing 1 document(s)\nINFO: Extracting stage 1/1: unknown_source\nINFO: Processing d-id: doc-addb4618e1697da0445ec72a648e1f92\nINFO: limit_async: 4 new workers initialized\nINFO:  == LLM cache == saving default: a34b2d1c7fc4ed2403c0d56b9d4c637b\nERROR: limit_async: Error in decorated function:\nERROR: limit_async: Error in decorated function:\nERROR: limit_async: Error in decorated function:\nERROR: Failed to extract entities and relationships:\nERROR: Traceback (most recent call last):\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\http_proxy.py\", line 206, in handle_async_request\n    return await self._connection.handle_async_request(proxy_request)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\connection.py\", line 103, in handle_async_request\n    return await self._connection.handle_async_request(request)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 136, in handle_async_request\n    raise exc\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 106, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 177, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 217, in _receive_event\n    data = await self._network_stream.read(\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\lightrag.py\", line 1003, in process_document\n    await asyncio.gather(*tasks)\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\lightrag.py\", line 1201, in _process_entity_relation_graph\n    raise e\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\lightrag.py\", line 1187, in _process_entity_relation_graph\n    chunk_results = await extract_entities(\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\operate.py\", line 854, in extract_entities\n    raise task.exception()\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\operate.py\", line 830, in _process_with_semaphore\n    return await _process_single_content(chunk)\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\operate.py\", line 770, in _process_single_content\n    glean_result = await use_llm_func_with_cache(\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\utils.py\", line 1582, in use_llm_func_with_cache\n    res: str = await use_llm_func(input_text, **kwargs)\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\utils.py\", line 585, in wait_func\n    return await future\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\utils.py\", line 369, in worker\n    result = await func(*args, **kwargs)\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\llm\\ollama.py\", line 130, in ollama_model_complete\n    return await _ollama_model_if_cache(\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\tenacity\\__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\llm\\ollama.py\", line 109, in _ollama_model_if_cache\n    raise e\n  File \"e:\\pycharmproject\\lightrag\\lightrag\\llm\\ollama.py\", line 72, in _ollama_model_if_cache\n    response = await ollama_client.chat(model=model, messages=messages, **kwargs)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\ollama\\_client.py\", line 854, in chat\n    return await self._request(\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\ollama\\_client.py\", line 692, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\ollama\\_client.py\", line 632, in _request_raw\n    r = await self._client.request(*args, **kwargs)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_client.py\", line 1540, in request\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\anaconda\\envs\\lightrag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nERROR: Failed to extract document 1/1: unknown_source\nINFO: Document processing pipeline completed\n\n=====================\nQuery mode: naive\n=====================\nINFO: Vector query: 3 chunks, top_k: 60\nThe primary themes of \"A Christmas Carol\" by Charles Dickens are redemption and transformation, as well as the importance of empathy and charity. These themes are illustrated through the protagonist, Ebenezer Scrooge, who undergoes a profound change from a cold-hearted miser to a compassionate individual after being visited by three spirits representing past, present, and future.\n\nRedemption is evident in Scrooge's journey towards becoming a kinder person, shedding his selfish nature and finding a sense of goodwill and gratitude for others. The theme of transformation reflects not only Scrooge's change but also the growth that occurs within other characters, such as Bob Cratchit and Tiny Tim, who exhibit resilience amidst their hardships.\n\nThe significance of empathy and charity is emphasized through various interactions and the portrayal of festive gatherings like the Christmas Eve party hosted by Mr. Fezziwig. The story highlights the importance of sharing joy and kindness during holidays and underscores the societal impact one can have when they choose to care for others rather than solely focusing on personal gains.\n\nReferences:\n[DC] unknown_source\n=====================\nQuery mode: local\n=====================\nWARNING: low_level_keywords is empty, switching from local mode to global mode\nINFO: Process 9400 building query context...\nINFO: Query edges: Top themes, Story, top_k: 60, cosine: 0.2\nSorry, I'm not able to provide an answer to that question.[no-context]\n\n=====================\nQuery mode: global\n=====================\nINFO:  == LLM cache == saving global: 20f25304745f8c690c60c67fa5ed9e87\nINFO: Process 9400 building query context...\nINFO: Query edges: Themes, Story, top_k: 60, cosine: 0.2\nSorry, I'm not able to provide an answer to that question.[no-context]\n\n=====================\nQuery mode: hybrid\n=====================\nINFO:  == LLM cache == saving hybrid: 8755e9fbac7e447eb32646483edace57\nWARNING: low_level_keywords is empty, switching from hybrid mode to global mode\nINFO: Process 9400 building query context...\nINFO: Query edges: Top themes, Story, top_k: 60, cosine: 0.2\nSorry, I'm not able to provide an answer to that question.[no-context]\n\nDone!\nINFO: Creating a new event loop in main thread.",
      "state": "closed",
      "author": "KinhouSyou",
      "author_type": "User",
      "created_at": "2025-06-15T13:11:06Z",
      "updated_at": "2025-06-21T14:31:33Z",
      "closed_at": "2025-06-16T03:07:25Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1682/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1682",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1682",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:02.068825",
      "comments": [
        {
          "author": "KinhouSyou",
          "body": "\"timeout\": None,\nalso doesn't work",
          "created_at": "2025-06-15T13:54:01Z"
        },
        {
          "author": "KinhouSyou",
          "body": "将timeout改为none",
          "created_at": "2025-06-16T03:07:51Z"
        },
        {
          "author": "anthonyaquino83",
          "body": "Same here with a modified deepseek-r1:1.5b for PARAMETER num_ctx 32768:\n ```\nrag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,  # Use Ollama model for text generation\n    llm_model_name='deepseek-r1m:latest', # Your model name\n    llm_model_kwargs={\"optio",
          "created_at": "2025-06-21T14:31:33Z"
        }
      ]
    },
    {
      "issue_number": 1692,
      "title": "[Bug]:Path Traversal",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nThe LightRAG framework supports the ingestion of diverse file formats, including code files (e.g., .html, .py, .sh, .java), configuration files (e.g., .ini, .conf), and database files (e.g., .sql). Within the LightRAG codebase, specifically in the file LightRAG/lightrag/api/routers/document_routes.py, the file upload functionality is implemented by the function upload_to_input_dir. At Line 802 of this file, the destination file path is constructed via the operation file_path = doc_manager.input_dir / file.filename. Crucially, the filename parameter is user-controllable input. This vulnerability enables a malicious actor to craft filenames incorporating directory traversal sequences (../). Exploiting this flaw permits the unauthorized upload of potentially malicious files to arbitrary, unintended locations within the server's filesystem hierarchy, circumventing the intended input directory constraints.Attackers can also view information pertaining to the inputs directory on the LightRAG Server Setting page.\n\n![Image](https://github.com/user-attachments/assets/dc29d1cb-38d7-47ab-ac7b-c1b235be8540)\n![Image](https://github.com/user-attachments/assets/86b703d1-c2d2-45eb-8d4a-5a96558248cb)\n![Image](https://github.com/user-attachments/assets/f474f898-39b1-4d73-9870-c13a30feb400)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Hannibal0x",
      "author_type": "User",
      "created_at": "2025-06-20T12:31:16Z",
      "updated_at": "2025-06-20T12:31:16Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1692/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1692",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1692",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:02.255836",
      "comments": []
    },
    {
      "issue_number": 1656,
      "title": "[Question]: Query Fails at exactly 23 Entity IDs in a list. What Setting To Increase Capacity?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nGreetings all,\nI am using a mix mode or local mode LightRAG query to look for orphans in the knowledge graph.\n\nThe following query works with any number of entities in the list up to 23:\nThe list of entities is generated by the http://localhost:9621/graph/label/list end point which is provided in the API tab of lightrag-server.\n\n/mix [Only provide the name of the entity in the response. Nothing else is required.] Please examine the Entity ID for all Entities in the following python list. Then please return only the Entities with a Rank of 0. [ \"2023-04-02T06:06:17Z\", \"2023-04-22T23:01:27Z\", \"Alabama\", \"Albury\", \"Biological Compartments\", ... ]\n\nHere is the response:\n\n> Albury\n\nThis is correct, but if I add only one more entity to list of entities to be examined then I get the following failed response:\n\n> I'm sorry, but I don't have the information regarding the Entity IDs or their ranks for the entities listed. Therefore, I cannot return the entities with a rank of 0.\n\nSo at the threshold of 24 items (where the query fails) I doubled the values of: Top K Results, Max Tokens for Text Unit, Max Tokens for Global Context, and Max Tokens for Local Context.\nUnfortunately, I still get the same failed response.\n\nI also tried increasing the following in the .env file but with no success.\n```\nentity_summary_to_max_tokens\nmax_token_size\nchunk_token_size\n```\n\nIf I put the same query into my python query script, then it will process many more list items than just 23.\nI have not yet been able to determine what the limit is when using a python query script.\n\nCan someone please tell me where the limit is specified for LightRAG Server, or how I can ask the question in a way which bypasses the limit?\n\nMuch thanks to all",
      "state": "closed",
      "author": "johnshearing",
      "author_type": "User",
      "created_at": "2025-06-04T15:46:50Z",
      "updated_at": "2025-06-20T00:17:17Z",
      "closed_at": "2025-06-20T00:17:17Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1656/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1656",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1656",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:02.255856",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You cannot use RAG as a tool to query a long list of data.\n\n RAG is suitable for answering focused questions. RAG is not designed for comprehensive data enumeration or extracting extensive lists. This is determined by how RAG works. RAG only filters a limited number of content snippets from the know",
          "created_at": "2025-06-11T18:07:57Z"
        },
        {
          "author": "johnshearing",
          "body": "Thanks @danielaskdd! \nUnderstood.\n\nNow that I am starting to get familiar with the LightRAG server API I can use a python script to find all the orphans.\nI will incorporate this function into the [merge application](https://github.com/johnshearing/deep_avatar/tree/main/LightRAG) I have been working ",
          "created_at": "2025-06-11T18:20:15Z"
        }
      ]
    },
    {
      "issue_number": 1658,
      "title": "[Feature Request]: Click On Item In LightRag Server Knowledge Graph Legend To Display Only Those Types Of Items",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nI wonder if this would be good?\nWhat files should I look at in the LightRAG repository which will give me ideas about how to accomplish the task?\nMuch thanks.\n\n![Image](https://github.com/user-attachments/assets/bf197245-3238-4dfe-afc6-47dc1ddeb197)\n\n",
      "state": "closed",
      "author": "johnshearing",
      "author_type": "User",
      "created_at": "2025-06-05T01:38:57Z",
      "updated_at": "2025-06-20T00:16:44Z",
      "closed_at": "2025-06-20T00:16:44Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1658/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1658",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1658",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:02.448623",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Highlighting nodes of the selected type would be a beneficial feature. You might gain a better understanding of the LightRAG project's code through Deepwiki: https://deepwiki.com/HKUDS/LightRAG",
          "created_at": "2025-06-11T11:56:06Z"
        },
        {
          "author": "johnshearing",
          "body": "Thanks @danielaskdd! This Deepwiki is better than gold.",
          "created_at": "2025-06-11T12:07:36Z"
        },
        {
          "author": "johnshearing",
          "body": "Thanks again, @danielaskdd,\nProblem solved with the new merge app [which is found here](https://github.com/johnshearing/deep_avatar/tree/main/LightRAG):\nLook for _1_merge_GUI_??.py where the question marks are the version.\n\n![Image](https://github.com/user-attachments/assets/8330e160-f4af-4fca-bf2a-",
          "created_at": "2025-06-12T02:11:26Z"
        }
      ]
    },
    {
      "issue_number": 1669,
      "title": "[Question/Feature Request]: How To Refresh the LightRAG Server Without Shutting It Down And Restarting?",
      "body": "Greetings All,\nI made a [simple app](https://github.com/johnshearing/deep_avatar/tree/main/LightRAG) with the help of a.i. that helps me quickly understand knowledge graph data enough to know if entities should be merged or not and then facilitates the merge if required. I could use the WebUI but this app is quicker to use if there is a lot of data to clean up. The app makes use of the lightrag-server API\n\n\n![Image](https://github.com/user-attachments/assets/e8389f34-c047-40b4-b75d-d5164b8d8773)\n\n\nMy problem is that I have a lot of data to go through, and every time I merge entities I am required to stop the lightrag-server at the command line and restart it again in order to see those changes in my app or in the LightRAG WebUI.\n\nIs there some API call that I can send out from my simple app that will cause the lightrag-server to refresh?\n\nMuch thanks",
      "state": "closed",
      "author": "johnshearing",
      "author_type": "User",
      "created_at": "2025-06-11T02:15:17Z",
      "updated_at": "2025-06-20T00:16:04Z",
      "closed_at": "2025-06-20T00:16:04Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1669/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1669",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1669",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:02.649047",
      "comments": [
        {
          "author": "johnshearing",
          "body": "Thanks @danielaskdd,\nIssue was resolved [with your help](https://deepwiki.com/HKUDS/LightRAG/4-api-server).\n\nThree files were changed to create an API call that refreshes LightRAG Server from disk after the merge operation.\n[graph_routes.py](https://github.com/johnshearing/deep_avatar/blob/main/Ligh",
          "created_at": "2025-06-13T02:20:47Z"
        }
      ]
    },
    {
      "issue_number": 1691,
      "title": "[Question]: Entity Description Format For Best Retrieval And Human Readability",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI have an app that allows me to quickly examine the entities in a knowledge graph to clean up any erroneous data.\nThe app is found [here](https://github.com/johnshearing/deep_avatar/tree/main/LightRAG) as 1_merge_GUI??.py where the question marks represent the version number.\n\nEventually my edits will be used as data to fine-tune an LLM to do the same - hopefully as the data is indexed rather than after.\n\nI need to know the format for the descriptions which best facilitates the retrieval of data.\n\nThe data is indexed with this separator \\<SEP> as in the following example:\nMelanopsin is a photopigment found in specific retinal ganglion cells, which plays a role in non-image-forming visual functions such as circadian rhythm regulation. \\<SEP>Melanopsin is a photopigment located in the retina that responds to blue light, playing a significant role in regulating circadian rhythms and other physiological functions. \\<SEP>Melanopsin is an opsin in the human brain that plays a critical role in neuronal signaling related to light exposure.\n\nBut I want to know if I can use a new line or just a period and a space instead of the separator to make it more human readable without poorly affecting the responses from the LLM?\n\nMuch thanks",
      "state": "closed",
      "author": "johnshearing",
      "author_type": "User",
      "created_at": "2025-06-19T15:29:35Z",
      "updated_at": "2025-06-20T00:15:03Z",
      "closed_at": "2025-06-20T00:15:03Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1691/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1691",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1691",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:02.894574",
      "comments": [
        {
          "author": "johnshearing",
          "body": "I stubbled onto the answer.\nIs see in prompt.py that I can pick any separator I want before indexing.",
          "created_at": "2025-06-20T00:15:03Z"
        }
      ]
    },
    {
      "issue_number": 1323,
      "title": "[Feature Request]: Automatic merging of the same entity under different names",
      "body": "### **Background**  \nLightRAG currently merges entities solely based on exact name matches (including captions). This results in multiple disconnected nodes for the same entity under different names, and may even create isolated subgraphs for identical entities, ultimately degrading query performance.  \n\n### **Automated Entity Merging for Variant Names**  \n\nTo address this, we propose an automated entity merging approach for differently named but identical entities:  \n\n1. **Vector Node Database Utilization**:  \n   - Modify  node vector DB implementation to store the embedded vector on entity name.  \n\n2. **Similarity Threshold Configuration**:  \n   - Set a minimum cosine similarity threshold (e.g., 0.8) for candidate selection.  \n\n3. **Candidate Retrieval**:  \n   - During merging, retrieve the top 10 most relevant nodes based on cosine similarity (above the threshold).  \n\n4. **LLM-Based Merge Validation**:  \n   - Submit the current entity’s name/description along with candidate entities’ names/descriptions to an LLM.  \n   - Task the LLM to:  \n     - Determine whether merging is justified,\n     - If merging is approved, select a best candidate for merging, and return the consolidated entity name and description.  \n\n5. Iterative Merging With Depth Limitation (optional):  \n   - Repeat the merging validation process for the newly consolidated entity returned by the LLM.  \n",
      "state": "open",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-09T13:24:31Z",
      "updated_at": "2025-06-19T20:08:31Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Core",
        "discuss"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1323/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.4.0",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1323",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1323",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:03.063888",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "@LarFii Does this algorithm seem reasonable to you? Do you have any suggestions for improvement?",
          "created_at": "2025-04-10T19:34:13Z"
        },
        {
          "author": "frederikhendrix",
          "body": "I might have something to note about this implementation. I have done something similar to this for my own LightRAG. For my own lightrag before a chunk gets send to the AI we first do a Hybrid search with query_param.context_only = true and I modified the build_context to only send back entities and",
          "created_at": "2025-04-11T07:28:55Z"
        },
        {
          "author": "danielaskdd",
          "body": "Node merging is a complex task. This issue is addressing to merge the most obviously  same entity in the document indexing stage:\n\n- Perform merge checks after all entities are extracted from a document.\n- Limit candidate selection to the top n most similar nodes with a strict threshold to minimize ",
          "created_at": "2025-04-11T07:58:24Z"
        },
        {
          "author": "choizhang",
          "body": "I have an idea: for specific vertical domains, during data cleaning, LLM should first use prompts to standardize synonyms, and then feedback the missing synonyms back to the standardized list during vector processing. This can significantly reduce the workload of later mergers",
          "created_at": "2025-04-14T03:23:21Z"
        },
        {
          "author": "danielaskdd",
          "body": "> I have an idea: for specific vertical domains, during data cleaning, LLM should first use prompts to standardize synonyms, and then feedback the missing synonyms back to the standardized list during vector processing. This can significantly reduce the workload of later mergers\n\nGood idea! The next",
          "created_at": "2025-04-14T03:31:28Z"
        }
      ]
    },
    {
      "issue_number": 1650,
      "title": "[Question]:how to use vllm with lightrag",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n_No response_\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "pankajsogain",
      "author_type": "User",
      "created_at": "2025-06-03T03:26:09Z",
      "updated_at": "2025-06-19T06:55:51Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1650/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1650",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1650",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:03.281116",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The vLLM server provides an OpenAI-compatible API, enabling seamless integration with LightRAG.",
          "created_at": "2025-06-11T11:59:02Z"
        },
        {
          "author": "pankajsogain",
          "body": "sample code please ? @danielaskdd ",
          "created_at": "2025-06-11T12:59:17Z"
        },
        {
          "author": "danielaskdd",
          "body": "- Start vLLM Server by: vllm serve YourModelID\n- Use OpenAI compatible LLM setting on LightRAG Server, .env file is something like:\n\n```\nLLM_MODEL=YourModelID\nLLM_BINDING_HOST=http://localhost:8000/v1\nLLM_BINDING=openai\n```",
          "created_at": "2025-06-11T17:51:08Z"
        },
        {
          "author": "pankajsogain",
          "body": "i am trying like this @danielaskdd  but its not working as expected \n\nimport httpx\nasync def vllm_model_complete_async(prompt, host=\"http://localhost:8001\", **kwargs):\n    url = f\"{host}/v1/chat/completions\"\n    data = {\n        \"model\": kwargs.get(\"model_name\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")",
          "created_at": "2025-06-14T12:41:20Z"
        },
        {
          "author": "pankajsogain",
          "body": " Please suggest function which I can use for vllm . I am not using lightrag api server  @danielaskdd ",
          "created_at": "2025-06-19T02:24:11Z"
        }
      ]
    },
    {
      "issue_number": 1640,
      "title": "[Bug]: ERROR: limit_async: Error in decorated function:",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n```docker-compose\nlightrag  | ERROR: limit_async: Error in decorated function:\nlightrag  | ERROR: limit_async: Error in decorated function:\nlightrag  | ERROR: limit_async: Error in decorated function:\nlightrag  | ERROR: limit_async: Error in decorated function:\nlightrag  | ERROR: Failed to extract entities and relationships:\nlightrag  | ERROR: Traceback (most recent call last):\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\nlightrag  |     yield\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\nlightrag  |     resp = await self._pool.handle_async_request(req)\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\nlightrag  |     raise exc from None\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\nlightrag  |     response = await connection.handle_async_request(\nlightrag  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\nlightrag  |     return await self._connection.handle_async_request(request)\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\nlightrag  |     raise exc\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\nlightrag  |     ) = await self._receive_response_headers(**kwargs)\nlightrag  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\nlightrag  |     event = await self._receive_event(timeout=timeout)\nlightrag  |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 217, in _receive_event\nlightrag  |     data = await self._network_stream.read(\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 32, in read\nlightrag  |     with map_exceptions(exc_map):\nlightrag  |   File \"/usr/local/lib/python3.11/contextlib.py\", line 158, in __exit__\nlightrag  |     self.gen.throw(typ, value, traceback)\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\nlightrag  |     raise to_exc(exc) from exc\nlightrag  | httpcore.ReadTimeout\nlightrag  |\nlightrag  | The above exception was the direct cause of the following exception:\nlightrag  |\nlightrag  | Traceback (most recent call last):\nlightrag  |   File \"/app/lightrag/lightrag.py\", line 1003, in process_document\nlightrag  |     await asyncio.gather(*tasks)\nlightrag  |   File \"/app/lightrag/lightrag.py\", line 1201, in _process_entity_relation_graph\nlightrag  |     raise e\nlightrag  |   File \"/app/lightrag/lightrag.py\", line 1187, in _process_entity_relation_graph\nlightrag  |     chunk_results = await extract_entities(\nlightrag  |                     ^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/app/lightrag/operate.py\", line 854, in extract_entities\nlightrag  |     raise task.exception()\nlightrag  |   File \"/app/lightrag/operate.py\", line 830, in _process_with_semaphore\nlightrag  |     return await _process_single_content(chunk)\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/app/lightrag/operate.py\", line 755, in _process_single_content\nlightrag  |     final_result = await use_llm_func_with_cache(\nlightrag  |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/app/lightrag/utils.py\", line 1582, in use_llm_func_with_cache\nlightrag  |     res: str = await use_llm_func(input_text, **kwargs)\nlightrag  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/app/lightrag/utils.py\", line 585, in wait_func\nlightrag  |     return await future\nlightrag  |            ^^^^^^^^^^^^\nlightrag  |   File \"/app/lightrag/utils.py\", line 369, in worker\nlightrag  |     result = await func(*args, **kwargs)\nlightrag  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/app/lightrag/llm/ollama.py\", line 130, in ollama_model_complete\nlightrag  |     return await _ollama_model_if_cache(\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\nlightrag  |     return await copy(fn, *args, **kwargs)\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\nlightrag  |     do = await self.iter(retry_state=retry_state)\nlightrag  |          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\nlightrag  |     result = await action(retry_state)\nlightrag  |              ^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\nlightrag  |     return call(*args, **kwargs)\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/tenacity/__init__.py\", line 400, in <lambda>\nlightrag  |     self._add_action_func(lambda rs: rs.outcome.result())\nlightrag  |                                      ^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\nlightrag  |     return self.__get_result()\nlightrag  |            ^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\nlightrag  |     raise self._exception\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\nlightrag  |     result = await fn(*args, **kwargs)\nlightrag  |              ^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/app/lightrag/llm/ollama.py\", line 109, in _ollama_model_if_cache\nlightrag  |     raise e\nlightrag  |   File \"/app/lightrag/llm/ollama.py\", line 72, in _ollama_model_if_cache\nlightrag  |     response = await ollama_client.chat(model=model, messages=messages, **kwargs)\nlightrag  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/ollama/_client.py\", line 839, in chat\nlightrag  |     return await self._request(\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/ollama/_client.py\", line 684, in _request\nlightrag  |     return cls(**(await self._request_raw(*args, **kwargs)).json())\nlightrag  |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/ollama/_client.py\", line 624, in _request_raw\nlightrag  |     r = await self._client.request(*args, **kwargs)\nlightrag  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_client.py\", line 1540, in request\nlightrag  |     return await self.send(request, auth=auth, follow_redirects=follow_redirects)\nlightrag  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_client.py\", line 1629, in send\nlightrag  |     response = await self._send_handling_auth(\nlightrag  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\nlightrag  |     response = await self._send_handling_redirects(\nlightrag  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\nlightrag  |     response = await self._send_single_request(request)\nlightrag  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_client.py\", line 1730, in _send_single_request\nlightrag  |     response = await transport.handle_async_request(request)\nlightrag  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\nlightrag  |     with map_httpcore_exceptions():\nlightrag  |   File \"/usr/local/lib/python3.11/contextlib.py\", line 158, in __exit__\nlightrag  |     self.gen.throw(typ, value, traceback)\nlightrag  |   File \"/root/.local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\nlightrag  |     raise mapped_exc(message) from exc\nlightrag  | httpx.ReadTimeout\nlightrag  |\nlightrag  | ERROR: Failed to extrat document 1/8: DeepSeek从入门到精通.pdf\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "IAMJOYBO",
      "author_type": "User",
      "created_at": "2025-05-28T23:21:53Z",
      "updated_at": "2025-06-19T06:15:06Z",
      "closed_at": "2025-06-19T06:15:06Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1640/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1640",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1640",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:03.472314",
      "comments": [
        {
          "author": "leviethung2103",
          "body": "I got the same issue. Is there any solution ?",
          "created_at": "2025-06-15T02:28:57Z"
        }
      ]
    },
    {
      "issue_number": 1551,
      "title": "[Bug]: UPLOAD DOCUMENT ERROR KG_VECTOR NOT DegreeView",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/f082ab2a-59cd-46c6-abf6-51bad1963f30)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-05-09T02:59:53Z",
      "updated_at": "2025-06-18T14:31:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1551/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1551",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1551",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:03.646643",
      "comments": [
        {
          "author": "kevinhqf",
          "body": "have same issue",
          "created_at": "2025-06-18T14:31:51Z"
        }
      ]
    },
    {
      "issue_number": 1642,
      "title": "[Question]:Why do queries through LightRag Ui and through OpenWebUI behave very differently ?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nQueries through LightRag Ui and through OpenWebUI behave very differently\n\n### Additional Context\n\nI have been trying to use LightRag with OpenWebUi and I have observed a surprising behaviour. When interacting with my knowledge graph through the UI it passes around 8000 tokens to the LLM (see the first logs) for the query. However, when interacting with it through Open Web Ui it passes 40 000+ tokens to the model (see the second logs) with the exact same question. I know there can be variability between 2 indetical queries but this behaviour is repeatable, every time I query through the UI it passes much less tokens than through OpenWebUI.\n\nThis is a bit of a problem because the large amont of tokens makes the prompt processing very long on my local machine.\n\n\nLogs for query through the LightRag UI:\n\n```\n\n\t\t},\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": \"comment est ce que je peux mettre a jour une paye de janvier ?\"\n\t\t}\n\t],\n\t\"model\": \"mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508\",\n\t\"stream\": true,\n\t\"temperature\": 0.5\n}\n2025-05-29 08:17:11,239 - DEBUG - https://huggingface.co:443 \"GET /api/models/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508/revision/main HTTP/1.1\" 200 6220\nFetching 12 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 54767.84it/s]\n127.0.0.1 - - [29/May/2025 08:17:12] \"POST /v1/chat/completions HTTP/1.1\" 200 -\n2025-05-29 08:17:12,716 - DEBUG - Starting stream:\n2025-05-29 08:17:12,716 - DEBUG - *** Resetting cache. ***\n2025-05-29 08:17:12,716 - DEBUG - Returning 8960 tokens for processing.\n```\n\n\nLogs for query Through OpenWebUi:\n\n```\n\t\t},\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": \"comment est ce que je peux mettre a jour une paye de janvier ?\"\n\t\t}\n\t],\n\t\"model\": \"mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508\",\n\t\"stream\": true,\n\t\"temperature\": 0.5\n}\n2025-05-29 08:11:50,887 - DEBUG - https://huggingface.co:443 \"GET /api/models/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508/revision/main HTTP/1.1\" 200 6220\nFetching 12 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 43996.20it/s]\n127.0.0.1 - - [29/May/2025 08:11:52] \"POST /v1/chat/completions HTTP/1.1\" 200 -\n2025-05-29 08:11:52,612 - DEBUG - Starting stream:\n2025-05-29 08:11:52,612 - DEBUG - *** Resetting cache. ***\n2025-05-29 08:11:52,612 - DEBUG - Returning 44820 tokens for processing.\n```",
      "state": "open",
      "author": "William-Droin",
      "author_type": "User",
      "created_at": "2025-05-29T07:29:09Z",
      "updated_at": "2025-06-18T14:30:02Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1642/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1642",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1642",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:03.807864",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "Im not familiar with Open WebUI, but it might have something to do with the \"top_K\" variable being differnt. \n\nAlso, you say that it uses 40k tokens and 8k tokens on the other method and then you post a picture of exactly the same thing you just said. Try adding code maybe? With LightRAG UI do you m",
          "created_at": "2025-05-30T14:45:35Z"
        },
        {
          "author": "William-Droin",
          "body": "Hi, thank you for your reply.\n\nI am using all default options, I am just following the guidelines here: https://github.com/open-webui/open-webui/discussions/6286#discussioncomment-12032330\n\nOpenWebUI is meant to be officially supported now by LightRag through an \"Ollama API emulation\" as stated int ",
          "created_at": "2025-05-30T15:34:19Z"
        },
        {
          "author": "sin-mike",
          "body": "+1 \nI have bumped into the exactly same problem. ",
          "created_at": "2025-06-09T12:39:57Z"
        },
        {
          "author": "RolphH",
          "body": "If you add /local or /global before your query in openwebui, you see more or less the same results. I haven;t found a way to have this added automatically in the Openwebui settings.",
          "created_at": "2025-06-11T09:00:10Z"
        },
        {
          "author": "RolphH",
          "body": "I noticed two things:\n\n1.  LightRag UI uses /global mode to do the query. When using OpenwebUI the mode in hybrid. \n2. LightRag UI sets the `top_k` parameter to 10, while when using OpenwebUI, it will use your default set in LightRag  environment variables `TOP_K`, which by default is 60.\n\nSo the ha",
          "created_at": "2025-06-11T09:59:11Z"
        }
      ]
    },
    {
      "issue_number": 1630,
      "title": "[Bug]:httpx.ReadTimeout during document processing despite increased timeout in .env (LightRAG v1.3.7)",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI'm encountering a httpx.ReadTimeout error during document ingestion in LightRAG version 1.3.7. This issue had previously occurred in v1.3.4, but at that time, increasing the timeout settings in the .env file (e.g. TIMEOUT=180) resolved it.\n\nHowever, in the latest version (1.3.7), adjusting the TIMEOUT variable in the .env file does not seem to have any effect. The document processing fails with the following error trace:\n\nIt seems like the timeout configuration is either being ignored or overridden internally in this version.\n\n\n### Steps to reproduce\n\nSteps to Reproduce:\n\n    Use LightRAG v1.3.7\n\n    Set up environment using .env file with TIMEOUT=300\n\n    Load a larger document (e.g., document1.pdf)\n\n    Backend LLM: LLaMA 3.3B running via Ollama\n\n    Hardware: A40 GPU\n\n    Run the document ingestion pipeline\n\nExpected: Document processes successfully with extended timeout\nActual: httpx.ReadTimeout exception occurs, and the document fails to process\n\n### Logs and screenshots\n\nFile \"/root/.local/lib/python3.11/site-packages/httpx/_client.py\", line 1629, in send\n  response = await self._send_handling_auth(\n...\nFile \"/root/.local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n  raise mapped_exc(message) from exc\n\nhttpx.ReadTimeout\n\nERROR: Failed to extract document 1/1: document1.pdf\nINFO: Document processing pipeline completed\nINFO: Scanning process completed: 1 files Processed.\n\n### Additional Information\n\nEnvironment:\n\n    LightRAG Version: 1.3.7\n\n    LLM: Meta LLaMA 3.3B via Ollama\n\n    GPU: NVIDIA A40\n\n    OS: Ubuntu 22.04 (Docker container)\n\n    Python: 3.11\n\n    Relevant packages: httpx, httpcore",
      "state": "closed",
      "author": "Ja1aia",
      "author_type": "User",
      "created_at": "2025-05-26T04:02:39Z",
      "updated_at": "2025-06-18T10:41:28Z",
      "closed_at": "2025-06-18T10:40:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1630/reactions",
        "total_count": 8,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 6
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1630",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1630",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:03.995354",
      "comments": [
        {
          "author": "berrujaime",
          "body": "Same error here. Have you managed to fix it yet?",
          "created_at": "2025-05-30T12:48:22Z"
        },
        {
          "author": "Ja1aia",
          "body": "not yet, for the workaround im using the older version of lightrag",
          "created_at": "2025-06-03T09:10:15Z"
        },
        {
          "author": "vanhoabk95",
          "body": "I am also got this error, \ncould you tell me what version not occur this error or can be fix by increasing timeout value?",
          "created_at": "2025-06-03T09:39:04Z"
        },
        {
          "author": "3wweiweiwu",
          "body": "same problem here",
          "created_at": "2025-06-03T17:27:53Z"
        },
        {
          "author": "berrujaime",
          "body": "I'm not encountering this problem in version 1.3.6 with the timeout set as None in the .env",
          "created_at": "2025-06-03T17:31:24Z"
        }
      ]
    },
    {
      "issue_number": 1690,
      "title": "[Feature Request]: Add Query Process Tracing for LightRAG",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nHello, I think LightRAG is a great project. However, when using it myself, I often find it difficult to understand which knowledge from the graph is being used in queries. Although we can see what knowledge is retrieved during debugging, this still isn't convenient enough. \n\nTherefore, I'd like to propose adding query process tracing functionality to LightRAG that can record and visualize the actual nodes (entities) and edges (relationships) used during queries, and export GraphML and HTML files for analysis and debugging. This feature would help developers better understand how the LightRAG system works during queries by:\n\n1. Helping optimize parameter values like `top_k`, `max_token_for_text_unit`, etc.\n2. Providing intuitive understanding of which knowledge is actually retrieved and used during queries\n3. Helping identify problem areas when query results are unsatisfactory\n\nMy current proposal is to:\n- Add optional parameters to `queryparam` to control whether to generate this graph (without affecting any existing functionality)\n- Add functionality to `kg_query` to save the graph used in queries\n- Implement visualization based on either `graph_visual_with_html.py` or a custom visualization solution\n- Provide an example in the `examples` directory\n\nI'm willing to contribute code for this feature and can assist with documentation writing.\n\n### Additional Context\n\nThe complete graph：\n\n<img width=\"915\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/388c9afa-b07d-4657-8979-524e78fb625c\" />\n\nThe graph used during queries：\n\n<img width=\"357\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b22ab342-ca2b-4531-b468-cf7127d9c970\" />",
      "state": "open",
      "author": "Huahuatii",
      "author_type": "User",
      "created_at": "2025-06-18T06:26:13Z",
      "updated_at": "2025-06-18T06:34:03Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1690/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1690",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1690",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.176734",
      "comments": []
    },
    {
      "issue_number": 1666,
      "title": "[Bug]: PGKVStorage requires undocumented dependency on PGVectorStorage and lacks fallback behavior",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nThe PGKVStorage implementation has an undocumented hard dependency on PGVectorStorage for storing document chunks. When using PGKVStorage, the implementation directly passes through to vector storage for document chunk persistence and retrieval, rather than maintaining its own storage mechanism. This architectural decision creates several issues:\n\n1. The dependency relationship between PGKVStorage and PGVectorStorage is not documented anywhere\n\n2. Users cannot use PGKVStorage independently without also configuring PGVectorStorage\n\n3. No fallback behavior exists when PGVectorStorage is not configured alongside PGKVStorage\n\n4. The shared table approach for document chunks between KV and vector storage is not clearly explained in documentation\n\n### Steps to reproduce\n\n1. Configure LightRAG with PGKVStorage for kv_storage\n\n2. Use a different vector storage implementation (e.g., FaissVectorDBStorage, NanoVectorDBStorage)\n\n3. Attempt to insert documents\n\n4. Observe that document chunks are not properly stored/retrieved due to the missing PGVectorStorage dependency\n\n### Expected Behavior\n\n1. PGKVStorage should either work independently or clearly document its dependency on PGVectorStorage\n\n2. The documentation should explicitly state that PGKVStorage requires PGVectorStorage to be configured\n\n3. Alternatively, PGKVStorage should provide its own independent storage mechanism with appropriate fallback behavior\n\n4. Clear architectural documentation explaining the shared table design between KV and vector storage\n\n### LightRAG Config Used\n\n# Paste your config here\n\n```python\nrag = LightRAG(\n    working_dir=\"./working_dir\",\n    kv_storage=\"PGKVStorage\",\n    vector_storage=\"FaissVectorDBStorage\",  # Not PostgreSQL\n    graph_storage=\"Neo4JStorage\",\n    doc_status_storage=\"PGDocStatusStorage\"\n)\n```\n\n(just simple example)\n\n### Logs and screenshots\n\nI didn't save logs, however, I was using the same config with https://github.com/HKUDS/LightRAG/issues/860 when I discovered this problem.\n\nAnd I occurred with the same output just as he described. (In fact, I think the real reason is PGKVStorage's implementation.\n\n### Additional Information\n\n- LightRAG Version: 1.3.8\n- Operating System: Ubuntu 24.04\n- Python Version: 3.12\n- Related Issues: https://github.com/HKUDS/LightRAG/issues/860\n\nThis issue affects deployment flexibility, as users expecting to mix and match different storage implementations may encounter unexpected failures. The storage type documentation in the repository suggests that different storage types can be independently configured, but the current PGKVStorage implementation violates this principle. This creates confusion for users who want to leverage PostgreSQL for key-value operations while using other specialized solutions for vector storage.\n\nA potential solution could involve either:\n\n1. Updating documentation to clearly state dependencies between storage types\n\n2. Refactoring PGKVStorage to provide independent storage capabilities\n\n3. Implementing proper fallback mechanisms when dependent storage types are not available\n\nWhat's more, I'd be glad to offer help like a PR🥰",
      "state": "open",
      "author": "jiwangyihao",
      "author_type": "User",
      "created_at": "2025-06-09T02:56:58Z",
      "updated_at": "2025-06-18T05:35:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1666/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1666",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1666",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.176753",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "kv_storage storage and vector_storage do work independently. You can use LightRAG Server for testing. By modifying the .env file, you can configure different databases for different storage types.",
          "created_at": "2025-06-11T11:02:46Z"
        },
        {
          "author": "jiwangyihao",
          "body": "> kv_storage storage and vector_storage do work independently. You can use LightRAG Server for testing. By modifying the .env file, you can configure different databases for different storage types.\n\nIn this case, the lightrag_doc_chunks table will not store anything like https://github.com/HKUDS/Li",
          "created_at": "2025-06-12T01:04:33Z"
        },
        {
          "author": "jiwangyihao",
          "body": "> kv_storage storage and vector_storage do work independently. You can use LightRAG Server for testing. By modifying the .env file, you can configure different databases for different storage types.\n\nhttps://github.com/HKUDS/LightRAG/blob/main/lightrag%2Fkg%2Fpostgres_impl.py#L479-L480\n\nJust look th",
          "created_at": "2025-06-12T07:18:33Z"
        },
        {
          "author": "jiwangyihao",
          "body": "> kv_storage storage and vector_storage do work independently. You can use LightRAG Server for testing. By modifying the .env file, you can configure different databases for different storage types.\n\n@danielaskdd Hey, as I referenced below, have you confirmed PostgreSQL's PGKVStorage? I do know norm",
          "created_at": "2025-06-16T10:05:18Z"
        },
        {
          "author": "danielaskdd",
          "body": "> > kv_storage storage and vector_storage do work independently. You can use LightRAG Server for testing. By modifying the .env file, you can configure different databases for different storage types.\n> \n> https://github.com/HKUDS/LightRAG/blob/main/lightrag%2Fkg%2Fpostgres_impl.py#L479-L480\n> \n> Ju",
          "created_at": "2025-06-17T04:31:47Z"
        }
      ]
    },
    {
      "issue_number": 1644,
      "title": "[Question]: how to fix the problem?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nERROR: Error in ollama_embed: 404 page not found (status code: 404)\nERROR: limit_async: Error in decorated function: 404 page not found (status code: 404)\nERROR: Traceback (most recent call last):\n  File \"/app/lightrag/lightrag.py\", line 1002, in process_document\n    await asyncio.gather(*tasks)\n  File \"/app/lightrag/kg/nano_vector_db_impl.py\", line 109, in upsert\n    embeddings_list = await asyncio.gather(*embedding_tasks)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/utils.py\", line 586, in wait_func\n    return await future\n           ^^^^^^^^^^^^\n  File \"/app/lightrag/utils.py\", line 370, in worker\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/utils.py\", line 242, in __call__\n    return await self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/llm/ollama.py\", line 165, in ollama_embed\n    raise e\n  File \"/app/lightrag/llm/ollama.py\", line 154, in ollama_embed\n    data = await ollama_client.embed(model=embed_model, input=texts)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/ollama/_client.py\", line 861, in embed\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/ollama/_client.py\", line 682, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/ollama/_client.py\", line 626, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: 404 page not found (status code: 404)\n\nERROR: Failed to extrat document 1/1: text.pdf\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "wade-liwei",
      "author_type": "User",
      "created_at": "2025-05-31T07:57:45Z",
      "updated_at": "2025-06-17T17:09:14Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1644/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1644",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1644",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.381046",
      "comments": [
        {
          "author": "Vifill",
          "body": "Getting same error.",
          "created_at": "2025-06-02T15:46:42Z"
        },
        {
          "author": "aruntemme",
          "body": "anyone resolved this issue?\n",
          "created_at": "2025-06-17T17:09:14Z"
        }
      ]
    },
    {
      "issue_number": 1689,
      "title": "[Bug]: File name already exists in server cache",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen uploading the document into the Ollama. It got the timeout in Ollama. \n\nThe status of process is failed. \n\nI've tried to upload again and it said: \"File name already exists in server cache\"\n\nHow to fix that problem ?\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\nEmbed the document successfully\n\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/b230f95d-7f94-4d01-bc57-8f7da2f91fc8)\n\n### Additional Information\n\n- LightRAG Version: 1.3.7\n- Operating System: Linux\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "open",
      "author": "leviethung2103",
      "author_type": "User",
      "created_at": "2025-06-17T07:15:31Z",
      "updated_at": "2025-06-17T07:17:19Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1689/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1689",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1689",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.583774",
      "comments": []
    },
    {
      "issue_number": 1687,
      "title": "[Bug]: Mix mode doesn't query vector db while providing information to the LLM for response.",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI've been working with LightRAG since about a few months. \"mix\" mode as per documentation should look for context from the knowledge graph and also the vector database. I had a script to look at the data retrieved using the `only_need_prompt` parameter. \nIn 1.3.4, this could return the chunks from the vector database separated by `--New Chunk--` tag. In the recent update, 1.3.9, this is no longer the case. The only context retrieved is from the Knowledge graph and very similar to what is retrieved when mode is set to \"hybrid\".\n\nThis is my RAG Implementation:\n```\nself.rag = LightRAG(\n    working_dir=self.config.get_working_dir(),\n    llm_model_func=ollama_model_complete,\n    llm_model_name=model_info[\"model_id\"],\n    llm_model_max_async=lightrag_config[\"max_async\"],\n    llm_model_max_token_size=lightrag_config[\"max_token_size\"],\n    llm_model_kwargs={\n        \"host\": self.config.get_ollama_host(),\n        \"options\": {\"num_ctx\": lightrag_config[\"max_token_size\"]},\n    },\n    embedding_func=EmbeddingFunc(\n        embedding_dim=lightrag_config[\"embedding_dim\"],\n        max_token_size=lightrag_config[\"max_embedding_token_size\"],\n        func=lambda texts: ollama_embed(\n            texts,\n            embed_model=self.config.get_ollama_embedding_model(),\n            host=self.config.get_ollama_host(),\n        ),\n    ),\n    enable_llm_cache=False,\n)\n\nawait self.rag.initialize_storages()\nawait initialize_pipeline_status()\n```\n\nThis is my query statement:\n```\nresponse_stream = await self.rag.aquery(user_input, param=query_param)\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "samikpal",
      "author_type": "User",
      "created_at": "2025-06-16T15:47:25Z",
      "updated_at": "2025-06-16T23:53:05Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1687/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1687",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1687",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.583794",
      "comments": [
        {
          "author": "kenspirit",
          "body": "According to the code, you need to set mode and original_query in query_param `if query_param.mode == \"mix\" and hasattr(query_param, \"original_query\")`.\n\noriginal_query seems not automatically set now",
          "created_at": "2025-06-16T23:53:05Z"
        }
      ]
    },
    {
      "issue_number": 1663,
      "title": "[Question]: About AGPLv3 Dependency in mineru_parser.py",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello LightRAG team, \n\nthanks for this great project. I have a quick question about a license interaction.\n\nI noticed that `lightrag/mineru_parser.py` on the main branch appears to directly import the `magic_pdf` library, which is licensed under AGPLv3, while LightRAG is MIT.\n\nMy understanding is that directly importing an AGPLv3 library would cause the combined work to fall under the AGPLv3's terms. Could you clarify if this is the intended behavior, or if I may have misunderstood something?\n\n\n\n\n\n### Additional Context\n\n```python\nif TYPE_CHECKING:\n    from magic_pdf.data.data_reader_writer import (\n        FileBasedDataWriter,\n        FileBasedDataReader,\n    )\n    from magic_pdf.data.dataset import PymuDocDataset\n    from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze\n    from magic_pdf.config.enums import SupportedPdfParseMethod\n    from magic_pdf.data.read_api import read_local_office, read_local_images\nelse:\n    # MinerU imports\n    from magic_pdf.data.data_reader_writer import (\n        FileBasedDataWriter,\n        FileBasedDataReader,\n    )\n    from magic_pdf.data.dataset import PymuDocDataset\n    from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze\n    from magic_pdf.config.enums import SupportedPdfParseMethod\n    from magic_pdf.data.read_api import read_local_office, read_local_images\n\n```\n\n> ",
      "state": "open",
      "author": "yuanjua",
      "author_type": "User",
      "created_at": "2025-06-06T07:13:14Z",
      "updated_at": "2025-06-16T19:00:37Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1663/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1663",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1663",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.750156",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Directly  importing MinerU is a temporary solution. After the basic functionalities of multimodal RAG are verified, the document extraction function will be decoupled from LightRAG, and MinerU Server will be called via an API. @LarFii ",
          "created_at": "2025-06-11T11:18:10Z"
        },
        {
          "author": "vikashrajgupta",
          "body": "@danielaskdd - I'm getting below error that are related to mineur_parser\n```\nFile \"/home/vikash-gupta/lightrag_env/lib/python3.12/site-packages/torch/serialization.py\", line 740, in __init__\n    super().__init__(open(name, mode))\n                     ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No ",
          "created_at": "2025-06-16T19:00:36Z"
        }
      ]
    },
    {
      "issue_number": 1686,
      "title": "[Question]运行graph_visual_with_neo4j.py，提示无法提取json文件",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n(lightrag2) C:\\Users\\94325\\Downloads\\LightRAG-1.3.2>C:/Users/94325/anaconda3/envs/lightrag2/python.exe c:/Users/94325/Downloads/LightRAG-1.3.2/examples/graph_visual_with_neo4j.py\nTraceback (most recent call last):\n  File \"c:\\Users\\94325\\Downloads\\LightRAG-1.3.2\\examples\\graph_visual_with_neo4j.py\", line 3, in <module>\n    from lightrag.utils import xml_to_json\nImportError: cannot import name 'xml_to_json' from 'lightrag.utils' (C:\\Users\\94325\\anaconda3\\envs\\lightrag2\\lib\\site-packages\\lightrag\\utils.py)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "carry1005",
      "author_type": "User",
      "created_at": "2025-06-16T07:34:59Z",
      "updated_at": "2025-06-16T07:34:59Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1686/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1686",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1686",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.942649",
      "comments": []
    },
    {
      "issue_number": 1685,
      "title": "[Question]: o200k_base.tiktoken",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n连接不上openaipublic.blob.core.windows.net\n尝试用其他电脑下载o200k_base.tiktoken，放入到根目录下新建文件夹tiktoken_cache，并在lightrag_ollama_demo.py的开头设置了os.environ[\"TIKTOKEN_CACHE_DIR\"] = \"./tiktoken_cache\"。但是还有这个报错\n\n### Additional Context\n\n(lightrag) syou@dell:~/projects/LightRAG$ python examples/lightrag_ollama_demo.py\n2025-06-16 11:04:24 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /home/syou/anaconda3/envs/lightrag/bin/python | Command base: /home/syou/anaconda3/envs/lightrag/bin/python -m pip\n\nLightRAG compatible demo log file: /home/syou/projects/LightRAG/lightrag_ollama_demo.log\n\nINFO: Process 30760 Shared-Data created for Single Process\nAn error occurred: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/o200k_base.tiktoken (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f78aa96ae90>, 'Connection to openaipublic.blob.core.windows.net timed out. (connect timeout=None)'))\nTraceback (most recent call last):\n  File \"/home/syou/projects/LightRAG/examples/lightrag_ollama_demo.py\", line 216, in <module>\n    asyncio.run(main())\n  File \"/home/syou/anaconda3/envs/lightrag/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/home/syou/anaconda3/envs/lightrag/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/home/syou/projects/LightRAG/examples/lightrag_ollama_demo.py\", line 208, in main\n    if rag:\nUnboundLocalError: local variable 'rag' referenced before assignment\nINFO: Creating a new event loop in main thread.",
      "state": "open",
      "author": "KinhouSyou",
      "author_type": "User",
      "created_at": "2025-06-16T03:14:16Z",
      "updated_at": "2025-06-16T03:14:22Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1685/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1685",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1685",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.942662",
      "comments": []
    },
    {
      "issue_number": 1555,
      "title": "[Feature Request]: Synchronize the update of entity and relation descriptions upon document deletion.",
      "body": "### Feature Request Description\n\nLightRAG relies significantly on entity and relation summaries to generate accurate responses to queries; therefore, only removing chunks from a deleted document can lead to hallucinations.\n\nIf we retain all original entity and relation descriptions along with the corresponding document IDs from which these descriptions originate, we will be able to re-aggregate the descriptions after a specific document ID has been deleted.\n",
      "state": "open",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-05-10T01:38:49Z",
      "updated_at": "2025-06-13T03:34:32Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1555/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1555",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1555",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:04.942667",
      "comments": [
        {
          "author": "FeHuynhVI",
          "body": "@danielaskdd \n Can you clearly explain where I should start and what the steps are? I need a well-defined roadmap ",
          "created_at": "2025-05-10T12:05:57Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "@danielaskdd  I really need this feature, but at the moment, I don’t know where to start. I’m not very experienced with code, so understanding the entire project to build on top of it is quite challenging for me. I would really appreciate any support, or at least some initial guidance to help me mov",
          "created_at": "2025-05-23T07:30:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "Currently, the community's resources are still insufficient, and there are many important matters that take priority, such as:\n1. Allowing users to customize and switch prompt templates\n2. Automatically merging synonymous entities\n3. Supporting multiple workspaces (corpuses)",
          "created_at": "2025-05-23T10:37:24Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "I believe the delete operation is more critical. Adding documents only incurs a embedding cost, and the other features don’t significantly affect current system workflows. However, deleting data can have a major impact — it may disrupt queries, break relationships, or compromise data integrity if no",
          "created_at": "2025-05-23T10:42:58Z"
        },
        {
          "author": "danielaskdd",
          "body": "@LarFii What is your perspective on establishing work priorities?",
          "created_at": "2025-05-23T10:49:20Z"
        }
      ]
    },
    {
      "issue_number": 1307,
      "title": "[Question]: Are we dropping MongoDB Graph for good?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am working with Azure Services and my constraints have been that I can only use Cosmos DB. I have gotten this project to be runnable with Postgres for KV, Vector and DocStatus and Mongo for Graph. This is due to the fact that \n1. I cannot install AGE on Cosmos Postgres and \n2. Even if I could it would the version with the bug when inserting entities and relations.\n\nIf we are dropping Mongo graph support, I will need to somehow find an alternative solution again. Would appreciate if anyone else has any suggestions on how to deploy this for production use! Thanks!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "lcwlouis",
      "author_type": "User",
      "created_at": "2025-04-08T02:00:58Z",
      "updated_at": "2025-06-13T01:46:23Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 19,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1307/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1307",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1307",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:05.156354",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Is Neo4j meet you need?",
          "created_at": "2025-04-08T11:04:01Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "FWIW, Let me share a perspective that I am sure many consumers of the Lightrag solution might have shared in the past. The Lightrag is an exciting approach to solve the naive-RAG challenges. But if there is any desire to increase the adoption of the solution in enterprises at a much higher scale , I",
          "created_at": "2025-04-08T21:15:11Z"
        },
        {
          "author": "danielaskdd",
          "body": "Is than mean you prefer MongoDB over PostgreSQL for key-value and vector storage use cases? Additionally, could you share your perspective on their respective capabilities for graph storage scenarios?",
          "created_at": "2025-04-09T01:24:48Z"
        },
        {
          "author": "lcwlouis",
          "body": "I cant't comment too much on acsangamnerkar. But for my current case, I am unable to get Neo4j as approval from above will likely request us to use services we already have which includes Azure Cosmos DB. So I am constrained by what is available in Cosmos DB. The DB types I can work with are PG (wit",
          "created_at": "2025-04-09T02:23:27Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "@lcwlouis I have tried to use Cosmos DB Mongo for Graph storage and It did not work for me. Postgres Flex server with AGE and VECTOR extension enabled works but has significant performance issues.\n\n@danielaskdd  As of now, based on my experiments, MongoDB Atlas was the most complete storage solution",
          "created_at": "2025-04-10T15:40:38Z"
        }
      ]
    },
    {
      "issue_number": 1181,
      "title": "[Feature Request]: access retrievals directly without llm",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nI think it would be useful for external integrations if I could make a query, and just retrieve the results of that query, without passing them through an LLM.  being able to separate that concern would make integration into other systems easier.  For example, I would like to use a knowledge graph query as one of several tools in an agentic system, or as an MCP.  In these cases, I  will use an external LLM to process the results, and I would prefer not to pass those query results through an LLM twice - both for cost and other reasons.  If I have missed how this is already possible, I apologize - I did find `/bypass` in the docs, but as I understand this is different.  It rather focus on using _just_ the LLM and chat history without any query results.  This is kind of the opposite of what I mean.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "jvsteiner",
      "author_type": "User",
      "created_at": "2025-03-24T23:20:41Z",
      "updated_at": "2025-06-12T14:57:50Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1181/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1181",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1181",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:05.382618",
      "comments": [
        {
          "author": "O-J1",
          "body": "I would also like this.",
          "created_at": "2025-03-25T13:25:40Z"
        },
        {
          "author": "jvsteiner",
          "body": "I think you can already do it actually.  you need to pass a QueryParam with only_need_context set to true like this:\n\n```\nresult = rag.query(\n        \"What are the top themes in this story?\",\n        param=QueryParam(mode=mode, only_need_context=True),\n    )\n```",
          "created_at": "2025-03-25T20:26:55Z"
        },
        {
          "author": "GTimothee",
          "body": "I don't know if it is just me but only_need_context does not work anymore on my side. It returns the answer instead.   I'll try to downgrade.",
          "created_at": "2025-06-12T14:35:10Z"
        },
        {
          "author": "GTimothee",
          "body": "ok I found the issue if someone has the same problem later on... the issue is that if you already generated an answer for a given query A. then if you run that same query later on *but* with only_need_context=True it will not work because there will be a cache hit and therefore it will keep returnin",
          "created_at": "2025-06-12T14:55:52Z"
        }
      ]
    },
    {
      "issue_number": 1638,
      "title": "[Bug]: Connect vLLM Faild.",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/77ae2670-45e3-4d57-9d04-6b830490976e)\n\n![Image](https://github.com/user-attachments/assets/036e6d6a-38e3-4c7b-b34a-cda411cab1be)\n\n![Image](https://github.com/user-attachments/assets/e6700f24-384b-4844-9296-c1422cbb613a)\n\n![Image](https://github.com/user-attachments/assets/9ddc7a49-7e39-4a62-a9bd-055eae088961)\n\n![Image](https://github.com/user-attachments/assets/b990336e-1b9d-4dd6-bb57-4ca04c7cbc23)\n\n### Steps to reproduce\n\ndocker compose up -d\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n```yaml\n### This is sample file of .env\n\n\n### Server Configuration\nHOST=0.0.0.0\nPORT=9621\nWEBUI_TITLE='My Graph KB'\nWEBUI_DESCRIPTION=\"Simple and Fast Graph Based RAG System\"\nOLLAMA_EMULATING_MODEL_TAG=latest\n# WORKERS=2\n# CORS_ORIGINS=http://localhost:3000,http://localhost:8080\n\n### Login Configuration\n# AUTH_ACCOUNTS='admin:admin123,user1:pass456'\n# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server\n# TOKEN_EXPIRE_HOURS=48\n# GUEST_TOKEN_EXPIRE_HOURS=24\n# JWT_ALGORITHM=HS256\n\n### API-Key to access LightRAG Server API\n# LIGHTRAG_API_KEY=your-secure-api-key-here\n# WHITELIST_PATHS=/health,/api/*\n\n### Optional SSL Configuration\n# SSL=true\n# SSL_CERTFILE=/path/to/cert.pem\n# SSL_KEYFILE=/path/to/key.pem\n\n### Directory Configuration (defaults to current working directory)\n### Should not be set if deploy by docker (Set by Dockerfile instead of .env)\n### Default value is ./inputs and ./rag_storage\n# INPUT_DIR=<absolute_path_for_doc_input_dir>\n# WORKING_DIR=<absolute_path_for_working_dir>\n\n### Max nodes return from grap retrieval\n# MAX_GRAPH_NODES=1000\n\n### Logging level\n# LOG_LEVEL=INFO\n# VERBOSE=False\n# LOG_MAX_BYTES=10485760\n# LOG_BACKUP_COUNT=5\n### Logfile location (defaults to current working directory)\n# LOG_DIR=/path/to/log/directory\n\n### Settings for RAG query\n# HISTORY_TURNS=3\n# COSINE_THRESHOLD=0.2\n# TOP_K=60\n# MAX_TOKEN_TEXT_CHUNK=4000\n# MAX_TOKEN_RELATION_DESC=4000\n# MAX_TOKEN_ENTITY_DESC=4000\n\n### Entity and ralation summarization configuration\n### Language: English, Chinese, French, German ...\nSUMMARY_LANGUAGE=English\n### Number of duplicated entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)\n# FORCE_LLM_SUMMARY_ON_MERGE=6\n### Max tokens for entity/relations description after merge\n# MAX_TOKEN_SUMMARY=500\n\n### Number of parallel processing documents(Less than MAX_ASYNC/2 is recommended)\n# MAX_PARALLEL_INSERT=2\n### Chunk size for document splitting, 500~1500 is recommended\n# CHUNK_SIZE=1200\n# CHUNK_OVERLAP_SIZE=100\n\n### LLM Configuration\nENABLE_LLM_CACHE=true\nENABLE_LLM_CACHE_FOR_EXTRACT=true\n### Time out in seconds for LLM, None for infinite timeout\nTIMEOUT=240\n### Some models like o1-mini require temperature to be set to 1\nTEMPERATURE=0\n### Max concurrency requests of LLM\nMAX_ASYNC=4\n### MAX_TOKENS: max tokens send to LLM for entity relation summaries (less than context size of the model)\n### MAX_TOKENS: set as num_ctx option for Ollama by API Server\nMAX_TOKENS=32768\n### LLM Binding type: openai, ollama, lollms, azure_openai\nLLM_BINDING=openai\nLLM_MODEL=qwen2.5-vl\nLLM_BINDING_HOST=https://jzrmq7nhuc-8000.cnb.run/v1\nLLM_BINDING_API_KEY=********\n### Optional for Azure\n# AZURE_OPENAI_API_VERSION=2024-08-01-preview\n# AZURE_OPENAI_DEPLOYMENT=gpt-4o\n\n### Embedding Configuration\n### Embedding Binding type: openai, ollama, lollms, azure_openai\nEMBEDDING_BINDING=openai\nEMBEDDING_MODEL=bge-m3\nEMBEDDING_DIM=1024\nEMBEDDING_BINDING_API_KEY=********\n# If the embedding service is deployed within the same Docker stack, use host.docker.internal instead of localhost\nEMBEDDING_BINDING_HOST=https://jzrmq7nhuc-8001.cnb.run\n### Num of chunks send to Embedding in single request\n# EMBEDDING_BATCH_NUM=32\n### Max concurrency requests for Embedding\n# EMBEDDING_FUNC_MAX_ASYNC=16\n### Maximum tokens sent to Embedding for each chunk (no longer in use?)\n# MAX_EMBED_TOKENS=8192\n### Optional for Azure\n# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large\n# AZURE_EMBEDDING_API_VERSION=2023-05-15\n\n### Data storage selection\n# LIGHTRAG_KV_STORAGE=PGKVStorage\n# LIGHTRAG_VECTOR_STORAGE=PGVectorStorage\n# LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n# LIGHTRAG_GRAPH_STORAGE=Neo4JStorage\n\n### TiDB Configuration (Deprecated)\n# TIDB_HOST=localhost\n# TIDB_PORT=4000\n# TIDB_USER=your_username\n# TIDB_PASSWORD='your_password'\n# TIDB_DATABASE=your_database\n### separating all data from difference Lightrag instances(deprecating)\n# TIDB_WORKSPACE=default\n\n### PostgreSQL Configuration\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_USER=your_username\nPOSTGRES_PASSWORD='your_password'\nPOSTGRES_DATABASE=your_database\nPOSTGRES_MAX_CONNECTIONS=12\n### separating all data from difference Lightrag instances(deprecating)\n# POSTGRES_WORKSPACE=default\n\n### Neo4j Configuration\nNEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD='your_password'\n\n### Independent AGM Configuration(not for AMG embedded in PostreSQL)\n# AGE_POSTGRES_DB=\n# AGE_POSTGRES_USER=\n# AGE_POSTGRES_PASSWORD=\n# AGE_POSTGRES_HOST=\n# AGE_POSTGRES_PORT=8529\n\n# AGE Graph Name(apply to PostgreSQL and independent AGM)\n### AGE_GRAPH_NAME is precated\n# AGE_GRAPH_NAME=lightrag\n\n### MongoDB Configuration\nMONGO_URI=mongodb://root:root@localhost:27017/\nMONGO_DATABASE=LightRAG\n### separating all data from difference Lightrag instances(deprecating)\n# MONGODB_GRAPH=false\n\n### Milvus Configuration\nMILVUS_URI=http://localhost:19530\nMILVUS_DB_NAME=lightrag\n# MILVUS_USER=root\n# MILVUS_PASSWORD=your_password\n# MILVUS_TOKEN=your_token\n\n### Qdrant\nQDRANT_URL=http://localhost:16333\n# QDRANT_API_KEY=your-api-key\n\n### Redis\nREDIS_URI=redis://localhost:6379\n```\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "IAMJOYBO",
      "author_type": "User",
      "created_at": "2025-05-28T22:16:22Z",
      "updated_at": "2025-06-12T10:16:13Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1638/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1638",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1638",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:05.611399",
      "comments": [
        {
          "author": "ysf-gd",
          "body": "请问解决了吗？",
          "created_at": "2025-06-12T10:16:13Z"
        }
      ]
    },
    {
      "issue_number": 1680,
      "title": "[Feature Request]: Add AI/ML API as provider",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nHi!\n\nI'm Sergey from the Integrations team over at [AI/ML API](https://aimlapi.com/), a startup with 150K+ users, providing over 300 AI models in one place\n\nYour project looks dope, so we'd like a native integration. We already got integrations with Langflow, AutoGPT, and LiteLLM - so our integrations team is pretty seasoned :)\n\nSay you're interested, and we'll test the compatibility, update the code/docs to include us, create a PR and add a tutorial on using LightRAG with AI/ML API to our docs\n\nIf you guys give us a green light, we could get on it next week 👷\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "OctavianTheI",
      "author_type": "User",
      "created_at": "2025-06-12T09:04:02Z",
      "updated_at": "2025-06-12T09:10:35Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1680/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1680",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1680",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:05.843594",
      "comments": []
    },
    {
      "issue_number": 1675,
      "title": "[Question]:RuntimeError('This event loop is already running')",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/LightRAG/app/__main__.py\", line 29, in <module>\n    command_routes()\n  File \"/LightRAG/app/__main__.py\", line 22, in command_routes\n    asyncio.run(insert(args.knowledge_id, args.file))\n  File \"/home/shaw/miniconda3/envs/LightRAG/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/shaw/miniconda3/envs/LightRAG/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shaw/miniconda3/envs/LightRAG/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/LightRAG/app/insert.py\", line 8, in insert\n    await rag.insert(read_doc(file))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/LightRAG/lightrag/lightrag.py\", line 572, in insert\n    loop.run_until_complete(\n  File \"/home/shaw/miniconda3/envs/LightRAG/lib/python3.11/asyncio/base_events.py\", line 630, in run_until_complete\n    self._check_running()\n  File \"/home/shaw/miniconda3/envs/LightRAG/lib/python3.11/asyncio/base_events.py\", line 589, in _check_running\n    raise RuntimeError('This event loop is already running')\nRuntimeError: This event loop is already running\n\nHow to solve the above problem？\n\n### Additional Context\n\n```python\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../.env\")\n\nimport textract\nfrom lightrag.llm.ollama import ollama_embed, ollama_model_complete\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger, EmbeddingFunc\n\nsetup_logger(\"lightrag\", level=\"INFO\")\n\nWORKING_DIR = \"./rag_storage\"\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nasync def initialize_rag():\n    rag = LightRAG(\n        # working_dir=get_workdir_of_knowledge_id(knowledge_id),\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=os.getenv(\"LLM_MODEL\"),\n        llm_model_max_token_size=8192,\n        llm_model_kwargs={\n            \"host\": os.getenv(\"LLM_BINDING_HOST\"),\n            \"options\": {\"num_ctx\": 8192},\n            \"timeout\": int(os.getenv(\"TIMEOUT\", \"300\")),\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=int(os.getenv(\"EMBEDDING_DIM\", \"1024\")),\n            max_token_size=int(os.getenv(\"MAX_EMBED_TOKENS\", \"8192\")),\n            func=lambda texts: ollama_embed(\n                texts,\n                embed_model=os.getenv(\"EMBEDDING_MODEL\"),\n                host=os.getenv(\"EMBEDDING_BINDING_HOST\"),\n            ),\n        ),\n        graph_storage=\"Neo4JStorage\",\n        kv_storage=\"JsonKVStorage\",\n    )\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n    return rag\n\nasync def main():\n    try:\n        # 初始化RAG实例\n        rag = await initialize_rag()\n        text = textract.process(\"./new-199脚本.docx\").decode(\"utf-8\")\n        print(text)\n        # 插入文本\n        await rag.insert(text)\n\n        # 执行混合检索\n        mode = \"hybrid\"\n        print(\n            await rag.query(\n                \"这个故事的主要主题是什么？\",\n                param=QueryParam(mode=mode)\n            )\n        )\n\n    except Exception as e:\n        print(f\"发生错误: {e}\")\n    finally:\n        if rag:\n            await rag.finalize_storages()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "state": "closed",
      "author": "fuxiaoya0",
      "author_type": "User",
      "created_at": "2025-06-11T09:16:19Z",
      "updated_at": "2025-06-12T08:56:43Z",
      "closed_at": "2025-06-12T08:56:43Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1675/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1675",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1675",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:05.843618",
      "comments": []
    },
    {
      "issue_number": 1668,
      "title": "[Bug]: ERROR: Failed to extrat document",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nERROR: Traceback (most recent call last):\n\n\nFile \"/app/lightrag/lightrag.py\", line 1003, in process_document\n\n\nawait asyncio.gather(*tasks)\n\n\nFile \"/app/lightrag/kg/nano_vector_db_impl.py\", line 116, in upsert\n\n\nresults = client.upsert(datas=list_data)\n\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\nFile \"/root/.local/lib/python3.11/site-packages/nano_vectordb/dbs.py\", line 116, in upsert\n\n\nself.__storage[\"matrix\"] = np.vstack([self.__storage[\"matrix\"], new_matrix])\n\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\nFile \"/root/.local/lib/python3.11/site-packages/numpy/core/shape_base.py\", line 289, in vstack\n\n\nreturn _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1024 and the array at index 1 has size 2560\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Rebel001X",
      "author_type": "User",
      "created_at": "2025-06-10T08:53:29Z",
      "updated_at": "2025-06-12T06:39:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1668/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1668",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1668",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:05.843626",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Once a vector database is established, its embedding model cannot be changed. To resolve this issue, please delete all data and re-add the documents.",
          "created_at": "2025-06-11T10:49:17Z"
        },
        {
          "author": "fuxiaoya0",
          "body": "```python\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=2560,\n        max_token_size=8192,\n        func=embedding_func,\n    )\n)\n```\nSet the embedding_dim parameter to 2560\n` embedding_dim=2560`",
          "created_at": "2025-06-12T01:56:31Z"
        }
      ]
    },
    {
      "issue_number": 1679,
      "title": "[Question]:the Service cant up",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n\"connection to openaipublic.blob.core.winodws.net time out,\"\n\nSo,the service cant up in linux. I want to know how to solve it. tks!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Rebel001X",
      "author_type": "User",
      "created_at": "2025-06-12T06:01:29Z",
      "updated_at": "2025-06-12T06:01:29Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1679/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1679",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1679",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.116186",
      "comments": []
    },
    {
      "issue_number": 1645,
      "title": "[Question]: Non-naive query modes in lightrag_ollama_demo.py do not return expected responses.",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi,\n\nI'm currently learning LightRAG and started with the `lightrag_ollama_demo.py` example. The demo runs successfully in naive mode and returns a meaningful thematic analysis of the input text. However, when I switch to other modes (local, global, hybrid), the responses seem to indicate that the model is not receiving or interpreting the text input correctly.  \n\nHere’s an example of the successful output from naive mode:  \n\n```cmd\n=====================\nQuery mode: naive\n=====================\nINFO: Vector query: 3 chunks, top_k: 60\nINFO: limit_async: 4 new workers initialized\nThis excerpt from Charles Dickens' *A Christmas Carol* is rich with several prominent themes. One of the main themes is the transformative power of redemption and change, particularly through Scrooge's experiences during his visits by the Ghosts of Christmas Past, Present, and Yet to Come. Another significant theme is the contrast between wealth and poverty, illustrated through Scrooge’s own transformation from a miserly, greedy man to someone more understanding of human suffering and happiness.\n\nAdditionally, the importance of family and relationships, especially as depicted in Scrooge's interactions with his past self and his nephew Fred, plays a crucial role. The story also delves into the impact of social class and societal expectations on individuals, particularly through the portrayal of Bob Cratchit and his family struggling against financial difficulties. Lastly, the theme of death and its consequences is woven throughout the narrative as Scrooge reflects on both his own mortality and the fate of others.\n\nReferences:\n1. [DC] unknown_source\n2. [DC] unknown_source\n3. [DC] unknown_source\n```    \n  \n\nIn contrast, the output from other modes:  \n\n```cmd\nQuery mode: local\n=====================\nINFO: Process 8648 building query context...\nINFO: Query nodes: Main ideas, Key points, Narrative elements, top_k: 60, cosine: 0.2\nINFO: Local query uses 60 entites, 116 relations, 3 chunks\nI need the text of the story to identify the top themes. Could you please provide it?\n=====================\nQuery mode: global\n=====================\nINFO: Process 8648 building query context...\nINFO: Query edges: Top themes, Story analysis, top_k: 60, cosine: 0.2\nINFO: Global query uses 65 entites, 60 relations, 3 chunks\nI'm sorry, but I need the actual story or a summary of it to identify the top themes. Could you please provide that information?\n=====================\nQuery mode: hybrid\n=====================\nINFO: Process 8648 building query context...\nINFO: Query nodes: Main ideas, Key points, Narrative elements, top_k: 60, cosine: 0.2\nINFO: Local query uses 60 entites, 116 relations, 3 chunks\nINFO: Query edges: Top themes, Story analysis, top_k: 60, cosine: 0.2\nINFO: Global query uses 65 entites, 60 relations, 3 chunks\nTo accurately identify the top themes in a story, I need to know the specific details or summary of the story. Could you please provide me with the plot, characters, or any key events from the story so I can analyze it and determine the main themes?INFO: Storage Finalization completed!\n\nDone!\n```  \nWhere the model asks for the story or summary, as if no input was passed.  \n\nIs there something different required when running these modes (e.g., additional configuration or preprocessing)? Any guidance would be greatly appreciated.\n\nThanks!\n\n### Additional Context\n\nLog from the console:\n\n```cmd\n(lightrag) PS C:\\Users\\PC\\Desktop\\RAG\\LightRAG> python .\\examples\\lightrag_ollama_demo.py\n2025-06-01 18:21:56 - pipmaster.package_manager - INFO - Targeting pip associated with Python: C:\\Users\\PC\\anaconda3\\envs\\lightrag\\python.exe | Command base: C:\\Users\\PC\\anaconda3\\envs\\lightrag\\python.exe -m pip\n\nLightRAG compatible demo log file: C:\\Users\\PC\\Desktop\\RAG\\LightRAG\\lightrag_ollama_demo.log\n\nDeleting old file:: ./dickens\\graph_chunk_entity_relation.graphml\nDeleting old file:: ./dickens\\kv_store_doc_status.json\nDeleting old file:: ./dickens\\kv_store_full_docs.json\nDeleting old file:: ./dickens\\kv_store_text_chunks.json\nDeleting old file:: ./dickens\\vdb_chunks.json\nDeleting old file:: ./dickens\\vdb_entities.json\nDeleting old file:: ./dickens\\vdb_relationships.json\nINFO: Process 8648 Shared-Data created for Single Process        \nINFO: Created new empty graph\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data       \nINFO: Process 8648 initialized updated flags for namespace: [full_docs]\nINFO: Process 8648 ready to initialize storage namespace: [full_docs]\nINFO: Process 8648 KV load full_docs with 0 records\nINFO: Process 8648 initialized updated flags for namespace: [text_chunks]\nINFO: Process 8648 ready to initialize storage namespace: [text_chunks]\nINFO: Process 8648 KV load text_chunks with 0 records\nINFO: Process 8648 initialized updated flags for namespace: [entities]\nINFO: Process 8648 initialized updated flags for namespace: [relationships]\nINFO: Process 8648 initialized updated flags for namespace: [chunks]\nINFO: Process 8648 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 8648 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 8648 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 8648 KV load llm_response_cache with 103 records\nINFO: Process 8648 initialized updated flags for namespace: [doc_status]\nINFO: Process 8648 ready to initialize storage namespace: [doc_status]\nINFO: Process 8648 doc status load doc_status with 0 records\nINFO: Process 8648 storage namespace already initialized: [full_docs]\nINFO: Process 8648 storage namespace already initialized: [text_chunks]\nINFO: Process 8648 storage namespace already initialized: [llm_response_cache]\nINFO: Process 8648 storage namespace already initialized: [doc_status]\nINFO: Process 8648 Pipeline namespace initialized\nINFO: limit_async: 16 new workers initialized\nINFO: Storage Initialization completed!\n\n=======================\nTest embedding function\n========================\nTest dict: ['This is a test string for embedding.']\nDetected embedding dimension: 1024\n\n\nINFO: Stored 1 new unique documents\nINFO: Processing 1 document(s)\nINFO: Extracting stage 1/1: unknown_source\nINFO: Processing d-id: doc-ac69b82dec8d2562e4887e87efdcf8ab\nINFO: Chunk 1 of 42 extracted 30 Ent + 12 Rel\nINFO: Chunk 2 of 42 extracted 11 Ent + 6 Rel\nINFO: Chunk 3 of 42 extracted 6 Ent + 9 Rel\nINFO: Chunk 4 of 42 extracted 10 Ent + 8 Rel\nINFO: Chunk 5 of 42 extracted 13 Ent + 11 Rel\nINFO: Chunk 6 of 42 extracted 15 Ent + 11 Rel\nINFO: Chunk 7 of 42 extracted 16 Ent + 7 Rel\nINFO: Chunk 8 of 42 extracted 7 Ent + 9 Rel\nINFO: Chunk 9 of 42 extracted 8 Ent + 12 Rel\nINFO: Chunk 10 of 42 extracted 12 Ent + 12 Rel\nINFO: Chunk 11 of 42 extracted 9 Ent + 8 Rel\nINFO: Chunk 12 of 42 extracted 27 Ent + 22 Rel\nINFO: Chunk 13 of 42 extracted 12 Ent + 9 Rel\nINFO: Chunk 14 of 42 extracted 27 Ent + 18 Rel\nINFO: Chunk 15 of 42 extracted 14 Ent + 12 Rel\nINFO: Chunk 16 of 42 extracted 13 Ent + 8 Rel\nINFO: Chunk 17 of 42 extracted 16 Ent + 10 Rel\nINFO: Chunk 18 of 42 extracted 13 Ent + 10 Rel\nINFO: Chunk 19 of 42 extracted 7 Ent + 6 Rel\nINFO: Chunk 20 of 42 extracted 15 Ent + 7 Rel\nINFO: Chunk 21 of 42 extracted 14 Ent + 11 Rel\nINFO: Chunk 22 of 42 extracted 10 Ent + 8 Rel\nINFO: Chunk 23 of 42 extracted 12 Ent + 10 Rel\nINFO: Chunk 24 of 42 extracted 12 Ent + 0 Rel\nINFO: Chunk 25 of 42 extracted 17 Ent + 9 Rel\nINFO: Chunk 26 of 42 extracted 15 Ent + 14 Rel\nINFO: Chunk 27 of 42 extracted 7 Ent + 8 Rel\nINFO: Chunk 28 of 42 extracted 15 Ent + 10 Rel\nINFO: Chunk 29 of 42 extracted 7 Ent + 6 Rel\nINFO: Chunk 30 of 42 extracted 18 Ent + 13 Rel\nINFO: Chunk 31 of 42 extracted 9 Ent + 7 Rel\nINFO: Chunk 32 of 42 extracted 8 Ent + 5 Rel\nINFO: Chunk 33 of 42 extracted 8 Ent + 7 Rel\nINFO: Chunk 34 of 42 extracted 12 Ent + 11 Rel\nINFO: Chunk 35 of 42 extracted 7 Ent + 7 Rel\nINFO: Chunk 36 of 42 extracted 12 Ent + 9 Rel\nINFO: Chunk 37 of 42 extracted 9 Ent + 7 Rel\nINFO: Chunk 38 of 42 extracted 12 Ent + 10 Rel\nINFO: Chunk 39 of 42 extracted 9 Ent + 6 Rel\nINFO: Chunk 40 of 42 extracted 6 Ent + 4 Rel\nINFO: Chunk 41 of 42 extracted 18 Ent + 12 Rel\nINFO: Chunk 42 of 42 extracted 6 Ent + 7 Rel\nINFO: Merging stage 1/1: unknown_source\nINFO: LLM merge N: Bob Cratchit | 10+0\nINFO: Merge N: Peter Cratchit | 2+0\nINFO: Merge N: Fred | 5+0\nINFO: LLM merge N: Scrooge | 35+0\nINFO: Merge N: Marley | 5+0\nINFO: Merge N: Scrooge's Business | 3+0\nINFO: LLM merge N: Christmas Eve | 8+0\nINFO: Merge N: Clerk | 2+0\nINFO: Merge N: Scrooge's nephew | 2+0\nINFO: Merge N: Fog | 2+0\nINFO: Merge N: London | 3+0\nINFO: Merge N: Christmas | 3+0\nINFO: Merge N: Marley's ghost | 2+0\nINFO: Merge N: Fog and Frost | 2+0\nINFO: Merge N: Ebenezer Scrooge | 4+0\nINFO: Merge N: Marley's Ghost | 4+0\nINFO: LLM merge N: Tokyo | 7+0\nINFO: LLM merge N: World Athletics Championship | 7+0\nINFO: Merge N: Global Tech Index | 2+0\nINFO: Merge N: Noah Carter | 3+0\nINFO: Merge N: Carbon-Fiber Spikes | 5+0\nINFO: Merge N: World Athletics Federation | 3+0\nINFO: LLM merge N: Ghost of Christmas Past | 16+0\nINFO: LLM merge N: Ghost of Christmas Present | 18+0\nINFO: LLM merge N: Ghost of Christmas Yet to Come | 12+0\nINFO: Merge N: World | 5+0\nINFO: Merge N: Merry Christmas | 2+0\nINFO: Merge N: Spirit of Christmas Past | 4+0\nINFO: Merge N: 100m Sprint Record | 4+0\nINFO: Merge N: Cutting-Edge Carbon-Fiber Spikes | 2+0\nINFO: Merge N: Christmas Spirit | 2+0\nINFO: Merge N: Father | 2+0\nINFO: Merge N: Schoolmaster | 2+0\nINFO: Merge N: Scrooge's Ghost | 2+0\nINFO: Merge N: Old Fezziwig | 2+0\nINFO: LLM merge N: Jacob Marley | 6+0\nINFO: Merge N: Christmastime | 2+0\nINFO: Merge N: Spirit | 3+0\nINFO: Merge N: Christmas Past | 2+0\nINFO: Merge N: Christmas Yet to Come | 2+0\nINFO: Merge N: Children | 2+0\nINFO: Merge N: Spirit of Christmas Present | 2+0\nINFO: Merge N: Cratchit Family | 3+0\nINFO: Merge N: Mrs. Cratchit | 4+0\nINFO: LLM merge N: Tiny Tim | 7+0\nINFO: Merge N: Master Peter Cratchit | 2+0\nINFO: Merge N: World of Spirits | 2+0\nINFO: Merge N: Christmas Day | 4+0\nINFO: Merge N: Scrooge's Nephew | 4+0\nINFO: Merge N: Phantom | 2+0\nINFO: Merge N: Old Joe | 3+0\nINFO: Merge N: Mrs. Dilber | 2+0\nINFO: Merge N: Project Gutenberg Literary Archive Foundation | 4+0\nINFO: Merge N: Project Gutenberg | 2+0\nINFO: Merge N: Foundation | 2+0\nINFO: Merge N: Project Gutenberg™ | 2+0\nINFO: LLM merge E: Marley - Scrooge | 6+0\nINFO: Merge E: Clerk - Scrooge | 3+0\nINFO: Merge E: Scrooge - Scrooge's nephew | 3+0\nINFO: Merge E: Christmas - Scrooge | 2+0\nINFO: Merge E: Marley's ghost - Scrooge | 2+0\nINFO: Merge E: Christmas Eve - Scrooge | 2+0\nINFO: Merge E: Marley's Ghost - Scrooge | 3+0\nINFO: LLM merge E: Ghost of Christmas Past - Scrooge | 13+0\nINFO: LLM merge E: Ghost of Christmas Present - Scrooge | 13+0\nINFO: Merge E: London - Scrooge | 2+0\nINFO: Merge E: Jacob Marley's Ghost - Scrooge | 2+0\nINFO: LLM merge E: Ghost of Christmas Yet to Come - Scrooge | 10+0\nINFO: Merge E: Merry Christmas - Scrooge | 2+0\nINFO: Merge E: Scrooge - Spirit of Christmas Past | 4+0\nINFO: Merge E: Tokyo - World Athletics Championship | 2+0\nINFO: Merge E: 100m Sprint Record - Noah Carter | 3+0\nINFO: Merge E: Carbon-Fiber Spikes - Noah Carter | 2+0\nINFO: Merge E: Scrooge - Spirit | 2+0\nINFO: Merge E: Jacob Marley - Scrooge | 3+0\nINFO: Merge E: Scrooge - Spirit of Christmas Yet to Come | 2+0\nINFO: Merge E: Fred - Scrooge | 3+0\nINFO: Merge E: Cratchit Family - Scrooge | 2+0\nINFO: LLM merge E: Bob Cratchit - Scrooge | 6+0\nINFO: LLM merge E: Bob Cratchit - Tiny Tim | 6+0\nINFO: Merge E: Christmas Day - Scrooge | 2+0\nINFO: Merge E: Scrooge - Tiny Tim | 3+0\nINFO: Merge E: Scrooge - Scrooge's Nephew | 2+0\nINFO: Merge E: Plump Sister with Lace Tucker  - Topper | 3+0\nINFO: Merge E: Old Joe - Scrooge | 2+0\nINFO: Merge E: Project Gutenberg - Project Gutenberg Literary Archive Foundation | 2+0\nINFO: Merge E: Project Gutenberg Literary Archive Foundation - Project Gutenberg™ License | 2+0\nINFO: Updating 321 entities  1/1: unknown_source\nINFO: Updating 302 relations 1/1: unknown_source\nINFO: Writing graph with 330 nodes, 302 edges\nINFO: In memory DB persist to disk\nINFO: Completed processing file 1/1: unknown_source\nINFO: Document processing pipeline completed\n\n=====================\nQuery mode: naive\n=====================\nINFO: Vector query: 3 chunks, top_k: 60\nINFO: limit_async: 4 new workers initialized\nThis excerpt from Charles Dickens' *A Christmas Carol* is rich with several prominent themes. One of the main themes is the transformative power of redemption and change, particularly through Scrooge's experiences during his visits by the Ghosts of Christmas Past, Present, and Yet to Come. Another significant theme is the contrast between wealth and poverty, illustrated through Scrooge’s own transformation from a miserly, greedy man to someone more understanding of human suffering and happiness.\n\nAdditionally, the importance of family and relationships, especially as depicted in Scrooge's interactions with his past self and his nephew Fred, plays a crucial role. The story also delves into the impact of social class and societal expectations on individuals, particularly through the portrayal of Bob Cratchit and his family struggling against financial difficulties. Lastly, the theme of death and its consequences is woven throughout the narrative as Scrooge reflects on both his own mortality and the fate of others.\n\nReferences:\n1. [DC] unknown_source\n2. [DC] unknown_source\n3. [DC] unknown_source\n=====================\nQuery mode: local\nQuery mode: local\n=====================\nINFO: Process 8648 building query context...\nINFO: Query nodes: Main ideas, Key points, Narrative elements, top_k: 60, cosine: 0.2\nINFO: Local query uses 60 entites, 116 relations, 3 chunks\nI need the text of the story to identify the top themes. Could you please provide it?\n=====================\nQuery mode: global\n=====================\nINFO: Process 8648 building query context...\nINFO: Query edges: Top themes, Story analysis, top_k: 60, cosine: 0.2\nINFO: Global query uses 65 entites, 60 relations, 3 chunks\nI'm sorry, but I need the actual story or a summary of it to identify the top themes. Could you please provide that information?\n=====================\nQuery mode: hybrid\n=====================\nINFO: Process 8648 building query context...\nINFO: Query nodes: Main ideas, Key points, Narrative elements, top_k: 60, cosine: 0.2\nINFO: Local query uses 60 entites, 116 relations, 3 chunks\nINFO: Query edges: Top themes, Story analysis, top_k: 60, cosine: 0.2\nINFO: Global query uses 65 entites, 60 relations, 3 chunks\nTo accurately identify the top themes in a story, I need to know the specific details or summary of the story. Could you please provide me with the plot, characters, or any key events from the story so I can analyze it and determine the main themes?INFO: Storage Finalization completed!\n\nDone!\n```\n\n\nCode:  \n\n```python\nimport asyncio\nimport os\nimport inspect\nimport logging\nimport logging.config\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.ollama import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc, logger, set_verbose_debug\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\nfrom dotenv import load_dotenv\n\nload_dotenv(dotenv_path=\".env\", override=False)\n\nWORKING_DIR = \"./dickens\"\n\n\ndef configure_logging():\n    \"\"\"Configure logging for the application\"\"\"\n\n    # Reset any existing handlers to ensure clean configuration\n    for logger_name in [\"uvicorn\", \"uvicorn.access\", \"uvicorn.error\", \"lightrag\"]:\n        logger_instance = logging.getLogger(logger_name)\n        logger_instance.handlers = []\n        logger_instance.filters = []\n\n    # Get log directory path from environment variable or use current directory\n    log_dir = os.getenv(\"LOG_DIR\", os.getcwd())\n    log_file_path = os.path.abspath(os.path.join(log_dir, \"lightrag_ollama_demo.log\"))\n\n    print(f\"\\nLightRAG compatible demo log file: {log_file_path}\\n\")\n    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n\n    # Get log file max size and backup count from environment variables\n    log_max_bytes = int(os.getenv(\"LOG_MAX_BYTES\", 10485760))  # Default 10MB\n    log_backup_count = int(os.getenv(\"LOG_BACKUP_COUNT\", 5))  # Default 5 backups\n\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"default\": {\n                    \"format\": \"%(levelname)s: %(message)s\",\n                },\n                \"detailed\": {\n                    \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n                },\n            },\n            \"handlers\": {\n                \"console\": {\n                    \"formatter\": \"default\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stderr\",\n                },\n                \"file\": {\n                    \"formatter\": \"detailed\",\n                    \"class\": \"logging.handlers.RotatingFileHandler\",\n                    \"filename\": log_file_path,\n                    \"maxBytes\": log_max_bytes,\n                    \"backupCount\": log_backup_count,\n                    \"encoding\": \"utf-8\",\n                },\n            },\n            \"loggers\": {\n                \"lightrag\": {\n                    \"handlers\": [\"console\", \"file\"],\n                    \"level\": \"INFO\",\n                    \"propagate\": False,\n                },\n            },\n        }\n    )\n\n    # Set the logger level to INFO\n    logger.setLevel(logging.INFO)\n    # Enable verbose debug if needed\n    set_verbose_debug(os.getenv(\"VERBOSE_DEBUG\", \"false\").lower() == \"true\")\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=os.getenv(\"LLM_MODEL\", \"qwen2.5m\"),\n        llm_model_max_token_size=8192,\n        llm_model_kwargs={\n            \"host\": os.getenv(\"LLM_BINDING_HOST\", \"http://localhost:11434\"),\n            \"options\": {\"num_ctx\": 8192},\n            \"timeout\": int(os.getenv(\"TIMEOUT\", \"300\")),\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=int(os.getenv(\"EMBEDDING_DIM\", \"1024\")),\n            max_token_size=int(os.getenv(\"MAX_EMBED_TOKENS\", \"8192\")),\n            func=lambda texts: ollama_embed(\n                texts,\n                embed_model=os.getenv(\"EMBEDDING_MODEL\", \"bge-m3:latest\"),\n                host=os.getenv(\"EMBEDDING_BINDING_HOST\", \"http://localhost:11434\"),\n            ),\n        ),\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\nasync def main():\n    try:\n        # Clear old data files\n        files_to_delete = [\n            \"graph_chunk_entity_relation.graphml\",\n            \"kv_store_doc_status.json\",\n            \"kv_store_full_docs.json\",\n            \"kv_store_text_chunks.json\",\n            \"vdb_chunks.json\",\n            \"vdb_entities.json\",\n            \"vdb_relationships.json\",\n        ]\n\n        for file in files_to_delete:\n            file_path = os.path.join(WORKING_DIR, file)\n            if os.path.exists(file_path):\n                os.remove(file_path)\n                print(f\"Deleting old file:: {file_path}\")\n\n        # Initialize RAG instance\n        rag = await initialize_rag()\n\n        # Test embedding function\n        test_text = [\"This is a test string for embedding.\"]\n        embedding = await rag.embedding_func(test_text)\n        embedding_dim = embedding.shape[1]\n        print(\"\\n=======================\")\n        print(\"Test embedding function\")\n        print(\"========================\")\n        print(f\"Test dict: {test_text}\")\n        print(f\"Detected embedding dimension: {embedding_dim}\\n\\n\")\n\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform naive search\n        print(\"\\n=====================\")\n        print(\"Query mode: naive\")\n        print(\"=====================\")\n        resp = await rag.aquery(\n            \"What are the top themes in this story?\",\n            param=QueryParam(mode=\"naive\", stream=True),\n        )\n        if inspect.isasyncgen(resp):\n            await print_stream(resp)\n        else:\n            print(resp)\n\n        # Perform local search\n        print(\"\\n=====================\")\n        print(\"Query mode: local\")\n        print(\"=====================\")\n        resp = await rag.aquery(\n            \"What are the top themes in this story?\",\n            param=QueryParam(mode=\"local\", stream=True),\n        )\n        if inspect.isasyncgen(resp):\n            await print_stream(resp)\n        else:\n            print(resp)\n\n        # Perform global search\n        print(\"\\n=====================\")\n        print(\"Query mode: global\")\n        print(\"=====================\")\n        resp = await rag.aquery(\n            \"What are the top themes in this story?\",\n            param=QueryParam(mode=\"global\", stream=True),\n        )\n        if inspect.isasyncgen(resp):\n            await print_stream(resp)\n        else:\n            print(resp)\n\n        # Perform hybrid search\n        print(\"\\n=====================\")\n        print(\"Query mode: hybrid\")\n        print(\"=====================\")\n        resp = await rag.aquery(\n            \"What are the top themes in this story?\",\n            param=QueryParam(mode=\"hybrid\", stream=True),\n        )\n        if inspect.isasyncgen(resp):\n            await print_stream(resp)\n        else:\n            print(resp)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        if rag:\n            await rag.llm_response_cache.index_done_callback()\n            await rag.finalize_storages()\n\n\nif __name__ == \"__main__\":\n    # Configure logging before running the main function\n    configure_logging()\n    asyncio.run(main())\n    print(\"\\nDone!\")\n\n```\n",
      "state": "open",
      "author": "pablo-01",
      "author_type": "User",
      "created_at": "2025-06-01T18:00:42Z",
      "updated_at": "2025-06-12T04:58:46Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1645/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1645",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1645",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.116207",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Consider whether this is due to the LLM's weak capabilities or insufficient context length. It is recommended to use an API to call a cloud-based LLM, setting the context length `llm_model_max_token_size` to 32768.",
          "created_at": "2025-06-11T17:37:24Z"
        },
        {
          "author": "nboierzyc",
          "body": "> Consider whether this is due to the LLM's weak capabilities or insufficient context length. It is recommended to use an API to call a cloud-based LLM, setting the context length `llm_model_max_token_size` to 32768.\n\nI have exact the same problem so I wish to give some supplements. I have tested bo",
          "created_at": "2025-06-12T02:16:54Z"
        },
        {
          "author": "danielaskdd",
          "body": "The log shows a Global or Local query returning 71 entities and 60 relations. This content needs to be sent to the LLM, therefore requiring a very long context. If Ollama's LLM context length is insufficient, this cannot answer the question.\n\nYou can view the prompt that LightRAG sends to the LLM th",
          "created_at": "2025-06-12T04:58:45Z"
        }
      ]
    },
    {
      "issue_number": 1661,
      "title": "[Bug]: Scan Button Fails to re-index Documents from /documents/text Endpoint",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nThe \"rescan\" button does not initiate the re-indexing of documents that have failed to index when sent to LightRAG via the `/documents/text` or `/documents/texts` endpoint.\n\n### Steps to reproduce\n\n1. Upload a text document using the /documents/text endpoint.\n2. Make the indexing process a fail (e.g., maybe a rate limit error).\n3. Click the \"rescan\" button to start re-indexing.\n\n### Expected Behavior\n\nWhen clicking the \"rescan\" button, LightRAG should re-index documents that failed to index, including those uploaded through/documents/text` endpoint. \n\nThe documents uploaded through the `/documents/upload` endpoint don’t have this problem because they get saved in the `inputs` directory, which the `/scan` endpoint uses to find documents that still need to be indexed.\n\nOn the other hand, the `/documents/text` endpoint doesn’t save the uploaded text in the `inputs` directory, which might be why they’re failing to index. I get that this is by design, but it leaves us with no way to re-index those documents. The only option is to send another indexing request to LightRAG, which will try to re-index all the failed documents.\n\n### LightRAG Config Used\n\n```\n📡 Server Configuration:\n    ├─ Host: 0.0.0.0\n    ├─ Port: 9621\n    ├─ Workers: 1\n    ├─ CORS Origins: *\n    ├─ SSL Enabled: False\n    ├─ Ollama Emulating Model: lightrag:latest\n    ├─ Log Level: INFO\n    ├─ Verbose Debug: False\n    ├─ History Turns: 3\n    ├─ API Key: Not Set\n    └─ JWT Auth: Disabled\n\n📂 Directory Configuration:\n    ├─ Working Directory: /app/data/rag_storage\n    └─ Input Directory: /app/data/inputs\n\n🤖 LLM Configuration:\n    ├─ Binding: openai\n    ├─ Host: https://generativelanguage.googleapis.com/v1beta/openai/\n    ├─ Model: gemini-2.0-flash\n    ├─ Temperature: 0.7\n    ├─ Max Async for LLM: 4\n    ├─ Max Tokens: 32768\n    ├─ Timeout: 150\n    ├─ LLM Cache Enabled: False\n    └─ LLM Cache for Extraction Enabled: False\n\n📊 Embedding Configuration:\n    ├─ Binding: openai\n    ├─ Host: https://generativelanguage.googleapis.com/v1beta/openai/\n    ├─ Model: text-embedding-004\n    └─ Dimensions: 768\n\n⚙️ RAG Configuration:\n    ├─ Summary Language: English\n    ├─ Max Parallel Insert: 1\n    ├─ Max Embed Tokens: 8192\n    ├─ Chunk Size: 1200\n    ├─ Chunk Overlap Size: 100\n    ├─ Cosine Threshold: 0.2\n    ├─ Top-K: 60\n    ├─ Max Token Summary: 500\n    └─ Force LLM Summary on Merge: 6\n\n💾 Storage Configuration:\n    ├─ KV Storage: JsonKVStorage\n    ├─ Vector Storage: NanoVectorDBStorage\n    ├─ Graph Storage: NetworkXStorage\n    └─ Document Status Storage: JsonDocStatusStorage\n```\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.3.7\n- Operating System: Linux 6.12.29-1-lts\n- Python Version: Python 3.11 (Docker container is using this)\n- Related Issues: None\n",
      "state": "open",
      "author": "yumpyy",
      "author_type": "User",
      "created_at": "2025-06-05T16:38:45Z",
      "updated_at": "2025-06-11T17:08:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1661/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1661",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1661",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.399724",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "To utilize the \"rescan\" feature, please use the /documents/upload api to upload your texts.",
          "created_at": "2025-06-11T11:32:15Z"
        },
        {
          "author": "yumpyy",
          "body": "> To utilize the \"rescan\" feature, please use the /documents/upload api to upload your texts.\n\nI am aware that the /upload endpoint has the ability to rescan, but I wanted to point out a flaw with the /text and /texts endpoints. If the indexing pipeline fails, there is no way to re-index the content",
          "created_at": "2025-06-11T17:08:43Z"
        }
      ]
    },
    {
      "issue_number": 1649,
      "title": "[Bug]: lightrag-hku==1.3.7 Fails with KeyError: 'history_messages' when you  run .insert() function",
      "body": "## 📌 Summary\n\nI want to use `query_param = QueryParam(mode=\"hybrid\", user_prompt=\"custom prompt\")` so I updated LightRAG to the latest version.\n\nHowever, the latest version of the LightRAG library (`lightrag-hku==1.3.7`) is critically broken in production environments when invoking `LightRAG.insert(...)`. It results in a runtime `KeyError: 'history_messages'`, despite passing valid input as expected by the public API.\n\n---\n\n## 📎 Affected Version\n\n- **Package:** `lightrag-hku`\n- **Version:** `1.3.7`\n- **Working Version:** `1.1.4`\n\n---\n\n## 🧪 Reproduction Steps\n\n```python\nfrom lightrag.lighrag import LightRAG\n\n# Initialize LightRAG with minimal configuration\nlight_rag = LightRAG(\n    working_dir=\"./demo_working_dir\",\n    llm_model_func=cutom_gpt4_function_complete,\n    llm_model_name=\"se-gpt-4o\",\n    embedding_func=None  # Not required for this test\n)\n\n# Trigger the bug\nlight_rag.insert(\"Sample input document\")\n```\n\n---\n\n## 📋 Result\n\n```\nKeyError: 'history_messages'\n```\n\n### 🔍 Traceback\n\n```\nFile \".../lightrag.py\", line 862, in apipeline_process_enqueue_documents\n    del pipeline_status[\"history_messages\"][:]\nKeyError: 'history_messages'\n```\n\n---\n\n## 🧠 Root Cause\n\nThe issue lies in the method:\n\n```python\napipeline_process_enqueue_documents(...)\n```\n\nWhere this line is executed **without checking** if the key exists:\n\n```python\ndel pipeline_status[\"history_messages\"][:]\n```\n\nIf `pipeline_status` does not contain `\"history_messages\"`, which is the default behavior when calling `insert()` or `ainsert()` without custom pipeline parameters, the method raises a `KeyError`.\n\n---\n\n## ✅ Expected Behavior\n\nCalling:\n\n```python\nlight_rag.insert(\"some text\")\n```\n\n…should complete without exceptions, processing the input as a simple document, without requiring the user to manage internal keys like `\"history_messages\"`.\n\n---\n\n## 🧪 Verification with Earlier Version\n\nAfter downgrading to:\n\n```bash\npip install lightrag-hku==1.1.4\n```\n\nThe issue no longer occurs and all workflows complete as expected.\n\n---\n\n## 🧩 LightRAG Config Used\n\n```python\n{\n  \"working_dir\": \"./demo_working_dir\",\n  \"llm_model_func\": lambda *args, **kwargs: \"dummy\",\n  \"llm_model_name\": \"se-gpt-4o\",\n  \"embedding_func\": None\n}\n```\n\n---\n\n## 📄 Logs\n\n```\nKeyError: 'history_messages'\n  File \".../lightrag.py\", line 862, in apipeline_process_enqueue_documents\n    del pipeline_status[\"history_messages\"][:]\n```\n\n---\n\n## 💡 Additional Information\n\n- **LightRAG Version:** 1.3.7\n- **Operating System:** Windows 10 / Linux (both tested)\n- **Python Version:** 3.11.1\n- **Related Issues:** None found in existing issue tracker\n\n---\n\n## ✅ Suggested Fix\n\nUpdate the code in `apipeline_process_enqueue_documents()` to safely handle missing keys:\n\n```python\nif \"history_messages\" in pipeline_status:\n    del pipeline_status[\"history_messages\"][:]\n```\n\nThis would restore compatibility with previous behavior and allow `insert()` to be called without internal knowledge of `pipeline_status`.\n",
      "state": "open",
      "author": "EmilianoAguayo",
      "author_type": "User",
      "created_at": "2025-06-02T14:34:39Z",
      "updated_at": "2025-06-11T14:21:22Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1649/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1649",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1649",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.593347",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Upon instantiation of the LightRAG object, it is essential to initialize the storage and pipeline status. This can be achieved by calling the following methods:\n\n```\n     await rag.initialize_storages()\n     await initialize_pipeline_status()\n```\n\nFor a practical example, please refer to `examples/l",
          "created_at": "2025-06-11T14:21:22Z"
        }
      ]
    },
    {
      "issue_number": 1664,
      "title": "[Bug]: Multiple LightRAG instances conflict due to shared global locks in same process",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n## Description\nWhen creating multiple LightRAG instances in the same process, instances fail due to asyncio lock conflicts. The issue stems from global shared state in `lightrag/kg/shared_storage.py` that causes locks to be bound to different event loops across threads.\n\n## Environment\n- **LightRAG Version**: 1.3.7\n- **Python Version**: 3.11\n\n## Steps to Reproduce\n\n1. Create multiple LightRAG instances simultaneously in different threads:\n```python\nimport asyncio\nimport threading\nfrom lightrag import LightRAG\n\ndef create_instance_sync(namespace):\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    \n    async def create_instance():\n        rag = LightRAG(\n            namespace_prefix=namespace,\n            working_dir=\"./documents\",\n            # ... other config\n        )\n        await rag.initialize_storages()\n        await rag.ainsert(\"test content\", ids=f\"doc_{namespace}\")\n    \n    loop.run_until_complete(create_instance())\n    loop.close()\n\n# This will fail when run concurrently\nthreads = []\nfor i in range(3):\n    t = threading.Thread(target=create_instance_sync, args=(f\"namespace_{i}\",))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n```\n\n2. The second and subsequent instances will fail with asyncio lock errors.\n\n## Error Log\n```\nRuntimeError: <asyncio.locks.Lock object at 0x31a76d310 [unlocked, waiters:1]> is bound to a different event loop\n```\n\n## Expected Behavior\nMultiple LightRAG instances should be able to operate independently in the same process without conflicts, especially when using different `namespace_prefix` values.\n\n## Root Cause Analysis\nThe issue is in `lightrag/kg/shared_storage.py` where global variables are used:\n\n```python\n# These are module-level globals shared across all instances\n_pipeline_status_lock: Optional[LockType] = None\n_storage_lock: Optional[LockType] = None\n_graph_db_lock: Optional[LockType] = None\n```\n\nWhen the first LightRAG instance calls `initialize_pipeline_status()`, these locks get bound to the current thread's event loop. Subsequent instances in different threads (with different event loops) cannot use these same lock objects.\n\n## Impact\nThis prevents LightRAG from being used in:\n- Multi-threaded applications\n- Concurrent document processing workflows\n- Multi-tenant applications requiring instance isolation\n- Any scenario requiring multiple independent LightRAG instances\n\n## Proposed Solutions\n\n### Option 1: Instance-level locks (Recommended)\nMove locks from global scope to instance scope:\n\n```python\nclass LightRAG:\n    def __init__(self, ...):\n        self._pipeline_status_lock = asyncio.Lock()\n        self._storage_lock = asyncio.Lock()\n        self._graph_db_lock = asyncio.Lock()\n```\n\n### Option 2: Thread-local storage\nUse thread-local storage for locks:\n\n```python\nimport threading\n_local = threading.local()\n\ndef get_pipeline_lock():\n    if not hasattr(_local, 'pipeline_lock'):\n        _local.pipeline_lock = asyncio.Lock()\n    return _local.pipeline_lock\n```\n\n### Option 3: Event loop aware lock management\nCreate new locks when event loop changes:\n\n```python\ndef get_pipeline_lock():\n    current_loop = asyncio.get_event_loop()\n    if (_pipeline_status_lock is None or \n        getattr(_pipeline_status_lock, '_loop', None) != current_loop):\n        return asyncio.Lock()\n    return _pipeline_status_lock\n```\n\n## Workaround\nCurrently, the only workaround is to ensure only one LightRAG instance exists per process, which severely limits scalability.\n\n## Additional Context\nThis architectural limitation prevents LightRAG from being used in production environments where multiple instances need to process documents concurrently within the same application process.\n\nThe `namespace_prefix` parameter suggests that multiple instances were intended to be supported, but the global lock design prevents this from working properly in multi-threaded scenarios.\n\nWould be happy to contribute a PR if the maintainers can provide guidance on the preferred approach!",
      "state": "open",
      "author": "earayu",
      "author_type": "User",
      "created_at": "2025-06-06T16:23:59Z",
      "updated_at": "2025-06-11T11:12:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1664/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1664",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1664",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.762673",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Since LightRAG uses in-memory database as default storage for all storage types, this is the main reason why documents cannot be processed concurrently using multiple processes.",
          "created_at": "2025-06-11T11:12:42Z"
        }
      ]
    },
    {
      "issue_number": 1674,
      "title": "[Question]:",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n_No response_\n\n### Additional Context\n\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../.env\")\n\nimport textract\nfrom lightrag.llm.ollama import ollama_embed, ollama_model_complete\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger, EmbeddingFunc\n\nsetup_logger(\"lightrag\", level=\"INFO\")\n\nWORKING_DIR = \"./rag_storage\"\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nasync def initialize_rag():\n    rag = LightRAG(\n        # working_dir=get_workdir_of_knowledge_id(knowledge_id),\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=os.getenv(\"LLM_MODEL\"),\n        llm_model_max_token_size=8192,\n        llm_model_kwargs={\n            \"host\": os.getenv(\"LLM_BINDING_HOST\"),\n            \"options\": {\"num_ctx\": 8192},\n            \"timeout\": int(os.getenv(\"TIMEOUT\", \"300\")),\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=int(os.getenv(\"EMBEDDING_DIM\", \"1024\")),\n            max_token_size=int(os.getenv(\"MAX_EMBED_TOKENS\", \"8192\")),\n            func=lambda texts: ollama_embed(\n                texts,\n                embed_model=os.getenv(\"EMBEDDING_MODEL\"),\n                host=os.getenv(\"EMBEDDING_BINDING_HOST\"),\n            ),\n        ),\n        graph_storage=\"Neo4JStorage\",\n        kv_storage=\"JsonKVStorage\",\n    )\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n    return rag\n\nasync def main():\n    try:\n        # 初始化RAG实例\n        rag = await initialize_rag()\n        text = textract.process(\"./new-199脚本.docx\").decode(\"utf-8\")\n        print(text)\n        # 插入文本\n        await rag.insert(text)\n\n        # 执行混合检索\n        mode = \"hybrid\"\n        print(\n            await rag.query(\n                \"这个故事的主要主题是什么？\",\n                param=QueryParam(mode=mode)\n            )\n        )\n\n    except Exception as e:\n        print(f\"发生错误: {e}\")\n    finally:\n        if rag:\n            await rag.finalize_storages()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "state": "closed",
      "author": "fuxiaoya0",
      "author_type": "User",
      "created_at": "2025-06-11T09:10:37Z",
      "updated_at": "2025-06-11T09:13:48Z",
      "closed_at": "2025-06-11T09:13:48Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1674/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1674",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1674",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.950944",
      "comments": []
    },
    {
      "issue_number": 1670,
      "title": "[Bug]: Incorrect Exception Handling for JSON Encoding - Using Nonexistent json.JSONEncodeError",
      "body": "redis_impl.py：\n```python\nasync def upsert(self, data: dict[str, dict[str, Any]]) -> None:\n        if not data:\n            return\n\n        logger.info(f\"Inserting {len(data)} items to {self.namespace}\")\n        async with self._get_redis_connection() as redis:\n            try:\n                pipe = redis.pipeline()\n                for k, v in data.items():\n                    pipe.set(f\"{self.namespace}:{k}\", json.dumps(v))\n                await pipe.execute()\n\n                for k in data:\n                    data[k][\"_id\"] = k\n            except json.JSONEncodeError as e:\n                logger.error(f\"JSON encode error during upsert: {e}\")\n                raise\n```\n\nERROR: Unexpected error in Redis operation for full_docs: module 'json' has no attribute 'JSONEncodeError'",
      "state": "open",
      "author": "sijunmanyue",
      "author_type": "User",
      "created_at": "2025-06-11T02:55:12Z",
      "updated_at": "2025-06-11T02:56:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1670/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1670",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1670",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.950967",
      "comments": []
    },
    {
      "issue_number": 1632,
      "title": "[Question]: RuntimeError('Event loop is closed') issue when using Postgresql",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI have successfully deployed LightRag on Windows using the default storage options:\n\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\nI found the performance a bit slow so I've started testing using Postgresql storage options:\n\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=PGVectorStorage\nLIGHTRAG_GRAPH_STORAGE=PGGraphStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\nI think I have the Postgresql correctly setup, with both 'vector' and 'age' extensions\ninstalled and their functions appearing inside 'public' and 'ag_catalog' schemas respectively.\n\nNow I'm stuck with this error:\n\n```\nerror:cannot perform operation: another operation is in progress\nTraceback (most recent call last):\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 762, in call_soon\n    self._check_closed()\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 520, in _check_closed\n    raise RuntimeError('Event loop is closed')\nRuntimeError: Event loop is closed\n```\n\nIs there any setting I missed? Can someone who successfully run Postgre share with me their\nsetup  or settings?\n\n\n\n### Additional Context\n\nMy setup:\n\nLightRag instance: Windows 11 pro, python 3.11.9, LightRag 1.3.7\nPostgresql: Windows Server 2016, Postgresql 17.5, AGE 1.5.0 , pgvector 0.8.0\n\nMy setup is limited and I am unable to run using docker images/WSL.\nI used a precompiled binaries of AGE and PGVECTOR.\n\n---\n\n**Complete terminal logs:**\n\n```\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 654, in run_until_complete\nPS C:\\DEV\\LightRAG> & C:/DEV/LightRAG/.venv/Scripts/python.exe c:/DEV/LightRAG/ingest_demo.py\nResponse of llm_model_func:  I am an AI language model created by OpenAI, designed to assist with a wide range of questions and tasks by providing information, generating text, and engaging in conversation. How can I help you today?\nResult of embedding_func:  (1, 256)\nEmbedding Dimensions:  256\nINFO: Process 28928 Shared-Data created for Single Process\n2025-05-26 22:45:58 - pipmaster.package_manager - INFO - Targeting pip associated with Python: C:\\DEV\\LightRAG\\.venv\\Scripts\\python.exe | Command base: C:\\DEV\\LightRAG\\.venv\\Scripts\\python.exe -m pip\nINFO: PostgreSQL, Connected to database at sgnt-dev-mpe01:5432/val_lightrag_graph\nINFO: Column LIGHTRAG_VDB_ENTITY.create_time is already timezone-aware, no migration needed\nINFO: Column LIGHTRAG_VDB_ENTITY.update_time is already timezone-aware, no migration needed\nINFO: Column LIGHTRAG_VDB_RELATION.create_time is already timezone-aware, no migration needed\nINFO: Column LIGHTRAG_VDB_RELATION.update_time is already timezone-aware, no migration needed\nINFO: Column LIGHTRAG_DOC_CHUNKS.create_time is already timezone-aware, no migration needed\nINFO: Column LIGHTRAG_DOC_CHUNKS.update_time is already timezone-aware, no migration needed\nERROR: PostgreSQL database,\nsql:SELECT create_graph('testchunk_entity_relation'),\ndata:None,\nerror:graph \"testchunk_entity_relation\" already exists\nERROR: PostgreSQL database,\nsql:SELECT create_vlabel('testchunk_entity_relation', 'base');,\ndata:None,\nerror:label \"base\" already exists\nERROR: PostgreSQL database,\nsql:SELECT create_elabel('testchunk_entity_relation', 'DIRECTED');,\ndata:None,\nerror:label \"DIRECTED\" already exists\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nINFO: Process 28928 Pipeline namespace initialized\nChecking path: RAG\nChecking path: INGEST\nFiles found: 1\nINFO: Creating a new event loop in main thread.\nERROR: PostgreSQL database, error:'NoneType' object has no attribute 'send'\nERROR: PostgreSQL database,\nsql:SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ('WRITING QUALITY REQUIREMENTS.md'),\nparams:{'workspace': 'default'},\nerror:cannot perform operation: another operation is in progress\nTraceback (most recent call last):\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 762, in call_soon\n    self._check_closed()\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 520, in _check_closed\n    raise RuntimeError('Event loop is closed')\nRuntimeError: Event loop is closed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\lightrag\\kg\\postgres_impl.py\", line 225, in query\n    rows = await connection.fetch(sql, *params.values())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\connection.py\", line 690, in fetch\n    return await self._execute(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\connection.py\", line 1864, in _execute\n    result, _ = await self.__execute(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\connection.py\", line 1961, in __execute\n    result, stmt = await self._do_execute(\n                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\connection.py\", line 2004, in _do_execute\n    stmt = await self._get_statement(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\connection.py\", line 432, in _get_statement\n    statement = await self._protocol.prepare(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"asyncpg\\\\protocol\\\\protocol.pyx\", line 165, in prepare\n  File \"asyncpg\\\\protocol\\\\protocol.pyx\", line 155, in asyncpg.protocol.protocol.BaseProtocol.prepare\n  File \"asyncpg\\\\protocol\\\\coreproto.pyx\", line 991, in asyncpg.protocol.protocol.CoreProtocol._prepare_and_describe\n  File \"asyncpg\\\\protocol\\\\protocol.pyx\", line 967, in asyncpg.protocol.protocol.BaseProtocol._write\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\proactor_events.py\", line 365, in write\n    self._loop_writing(data=bytes(data))\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\proactor_events.py\", line 401, in _loop_writing\n    self._write_fut = self._loop._proactor.send(self._sock, data)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'send'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\DEV\\LightRAG\\ingest_demo.py\", line 187, in <module>\n    main()\n  File \"c:\\DEV\\LightRAG\\ingest_demo.py\", line 163, in main\n    rag.insert(input=texts, ids=ids, file_paths=batch)\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\lightrag\\lightrag.py\", line 571, in insert\n    loop.run_until_complete(\n  File \"C:\\Users\\arestauro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\lightrag\\lightrag.py\", line 596, in ainsert\n    await self.apipeline_enqueue_documents(input, ids, file_paths)\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\lightrag\\lightrag.py\", line 782, in apipeline_enqueue_documents\n    unique_new_doc_ids = await self.doc_status.filter_keys(all_new_doc_ids)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\lightrag\\kg\\postgres_impl.py\", line 902, in filter_keys\n    res = await self.db.query(sql, params, multirows=True)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\lightrag\\kg\\postgres_impl.py\", line 217, in query\n    async with self.pool.acquire() as connection:  # type: ignore\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\pool.py\", line 228, in release\n    raise ex\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\pool.py\", line 218, in release\n    await self._con.reset(timeout=budget)\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\connection.py\", line 1562, in reset\n    await self.execute(reset_query)\n  File \"C:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\asyncpg\\connection.py\", line 349, in execute\n    result = await self._protocol.query(query, timeout)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"asyncpg\\\\protocol\\\\protocol.pyx\", line 360, in query\n  File \"asyncpg\\\\protocol\\\\protocol.pyx\", line 745, in asyncpg.protocol.protocol.BaseProtocol._check_state\nasyncpg.exceptions._base.InterfaceError: cannot perform operation: another operation is in progress\nPS C:\\DEV\\LightRAG> \n```",
      "state": "closed",
      "author": "BireleyX",
      "author_type": "User",
      "created_at": "2025-05-26T14:47:58Z",
      "updated_at": "2025-06-09T14:21:58Z",
      "closed_at": "2025-06-09T14:21:58Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1632/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1632",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1632",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:06.950974",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "What exactly are you running? Can you show the code you have in your \"ingest_demo.py\"?",
          "created_at": "2025-05-28T20:31:22Z"
        },
        {
          "author": "BireleyX",
          "body": "@frederikhendrix  hi. here's the demo code:\n\n```\nimport os\nimport asyncio\nimport glob\nimport time\nfrom lightrag import LightRAG, QueryParam, utils\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.api import config\nimport numpy as np\nfrom dotenv import load_dotenv\nimport logging\nfrom openai imp",
          "created_at": "2025-05-29T02:19:48Z"
        },
        {
          "author": "BireleyX",
          "body": "the code works fine when using default storage settings:\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage",
          "created_at": "2025-05-29T02:20:50Z"
        },
        {
          "author": "liu-nian-dh",
          "body": "```\nINFO: Process 3363589 Pipeline namespace initialized\n##### embedding: texts: 1 #####\nError getting node for 21-羟化酶缺乏症: Task <Task pending name='Task-31' coro=<LightRAG.apipeline_process_enqueue_documents.<locals>.process_document() running at /home/ddd/code/rag/LightRAG/lightrag/lightrag.py:1055",
          "created_at": "2025-05-29T09:41:24Z"
        },
        {
          "author": "frederikhendrix",
          "body": "> the code works fine when using default storage settings: LIGHTRAG_KV_STORAGE=JsonKVStorage LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage LIGHTRAG_GRAPH_STORAGE=NetworkXStorage LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\nI recommend: \n\n```python\nasync def main():\n    rag: LightRAG = await initi",
          "created_at": "2025-05-29T11:51:33Z"
        }
      ]
    },
    {
      "issue_number": 860,
      "title": "When using Milvus + Neo4j + PostgreSQL, the lightrag_doc_chunks table has no data after the document is processed.",
      "body": "When I only use PostgreSQL, the lightrag_doc_chunks table has data after the document is processed.\n```python\nrag = LightRAG(\n    ...      \n    kv_storage=\"PGKVStorage\",\n    doc_status_storage=\"PGDocStatusStorage\",\n    graph_storage=\"PGGraphStorage\",\n    vector_storage=\"PGVectorStorage\"\n)\n```\n\n\nBut when using Milvus + Neo4j + PostgreSQL, the lightrag_doc_chunks table has no data after the document is processed.\n```python\nrag = LightRAG(\n    ...\n    kv_storage=\"PGKVStorage\",\n    doc_status_storage=\"PGDocStatusStorage\",\n    graph_storage=\"Neo4JStorage\",\n    vector_storage=\"MilvusVectorDBStorage\",\n)\n```",
      "state": "open",
      "author": "tevooli",
      "author_type": "User",
      "created_at": "2025-02-19T10:30:42Z",
      "updated_at": "2025-06-09T02:58:39Z",
      "closed_at": null,
      "labels": [
        "question",
        "neo4j"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/860/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/860",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/860",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:07.129595",
      "comments": [
        {
          "author": "YanSte",
          "body": "We have some issues with Neo4JStorage and MilvusVectorDBStorage.\n\nCould you help us and assist us in the implementation? Thank you very much.",
          "created_at": "2025-02-19T18:47:37Z"
        },
        {
          "author": "jiwangyihao",
          "body": "> We have some issues with Neo4JStorage and MilvusVectorDBStorage.\n> \n> Could you help us and assist us in the implementation? Thank you very much.\n\nI think I've discovered real reason, it's related to PGKVStorage's implementation and I've described details in https://github.com/HKUDS/LightRAG/issue",
          "created_at": "2025-06-09T02:58:37Z"
        }
      ]
    },
    {
      "issue_number": 1660,
      "title": "[Question]:How to increase the token generation limit in retrieval mode",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi, after importing documents and indexing them, when I try to ask questions in the retrieval tab, the answers by LLM are sometimes limited to 512 tokens, since there is no continue button in the webui its difficult to continue the conversation. Is it possible to increase the answer generation limit, i searched for a parameter in the .env but there is none.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "mercurial-moon",
      "author_type": "User",
      "created_at": "2025-06-05T15:12:40Z",
      "updated_at": "2025-06-08T00:50:25Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1660/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1660",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1660",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:07.297572",
      "comments": [
        {
          "author": "johnshearing",
          "body": "There seem to be one or two parameters [found here in the README.md](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#lightrag-init-parameters) that might address your issue.\n\n![Image](https://github.com/user-attachments/assets/accc00b7-82be-433a-a617-af0d1bdff381)\n\nIn particular `entity_summary",
          "created_at": "2025-06-05T16:54:49Z"
        },
        {
          "author": "mercurial-moon",
          "body": "> There seem to be one or two parameters [found here in the README.md](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#lightrag-init-parameters) that might address your issue.\n> \n> ![Image](https://github.com/user-attachments/assets/accc00b7-82be-433a-a617-af0d1bdff381)\n> \n> In particular `enti",
          "created_at": "2025-06-07T06:06:01Z"
        },
        {
          "author": "johnshearing",
          "body": "An easy way to check if the parameters can be set in the .env file is to look in the env.example file which is [linked here](https://github.com/HKUDS/LightRAG/blob/main/env.example)\n\nAlso, I searched for the expression     `os.getenv( `     in the lightrag codebase.\nThe following is a list of most o",
          "created_at": "2025-06-07T19:01:53Z"
        },
        {
          "author": "mercurial-moon",
          "body": "seems like it could be either one of `MAX_TOKEN_SUMMARY` or `MAX_TOKENS`, I've already tried the first one but no change will try the next one.\nCurrently I've added some lines in the `openai.py` that seems to resolve the issue but its a more rigid approach.\n\n`kwargs[\"max_completion_tokens\"] = 1024;`",
          "created_at": "2025-06-08T00:50:24Z"
        }
      ]
    },
    {
      "issue_number": 1653,
      "title": "[Question]: Rate limiting errors from OpenAI cause LightRAG to fail processing documents - and wasting tokens/money.",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhen using OpenAI API I get rate limiting errors resulting in processing failed as well as wasted tokens/money. \n\n```\nINFO: Chunk 1 of 12 extracted 14 Ent + 6 Rel\nINFO:  == LLM cache == saving default: 6e0d6a4745de844a5a35d2a405aa3f20\nINFO: Chunk 2 of 12 extracted 12 Ent + 10 Rel\nINFO:  == LLM cache == saving default: 1f3da3aa291f4c6158108a218170a93f\nINFO: Chunk 3 of 12 extracted 18 Ent + 12 Rel\nINFO:  == LLM cache == saving default: e589c710b7d26cb882b915c30cb98545\nINFO: Chunk 4 of 12 extracted 15 Ent + 11 Rel\nINFO:  == LLM cache == saving default: 463c1d4a1bb536e8524737fa996ab695\nERROR: OpenAI API Rate Limit Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-... on tokens per min (TPM): Limit 30000, Used 26107, Requested 3923. Please try again in 60ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\lightrag.py\", line 1003, in process_document\n    await asyncio.gather(*tasks)\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\lightrag.py\", line 1201, in _process_entity_relation_graph\n    raise e\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\lightrag.py\", line 1187, in _process_entity_relation_graph\n    chunk_results = await extract_entities(\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\operate.py\", line 854, in extract_entities\n    raise task.exception()\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\operate.py\", line 830, in _process_with_semaphore\n    return await _process_single_content(chunk)\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\operate.py\", line 770, in _process_single_content\n    glean_result = await use_llm_func_with_cache(\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\utils.py\", line 1582, in use_llm_func_with_cache\n    res: str = await use_llm_func(input_text, **kwargs)\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\utils.py\", line 585, in wait_func\n    return await future\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\utils.py\", line 369, in worker\n    result = await func(*args, **kwargs)\n  File \"c:\\users\\pc\\desktop\\lr-serv\\lightrag\\lightrag\\api\\lightrag_server.py\", line 229, in openai_alike_model_complete\n    return await openai_complete_if_cache(\n  File \"C:\\Users\\PC\\anaconda3\\envs\\lightrag-serv\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n  File \"C:\\Users\\PC\\anaconda3\\envs\\lightrag-serv\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n  File \"C:\\Users\\PC\\anaconda3\\envs\\lightrag-serv\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"C:\\Users\\PC\\anaconda3\\envs\\lightrag-serv\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"C:\\Users\\PC\\anaconda3\\envs\\lightrag-serv\\lib\\site-packages\\tenacity\\__init__.py\", line 421, in exc_check\n    raise retry_exc from fut.exception()\ntenacity.RetryError: RetryError[<Future at 0x26de51d3ac0 state=finished raised RateLimitError>]\n\n```\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "pablo-01",
      "author_type": "User",
      "created_at": "2025-06-03T20:49:31Z",
      "updated_at": "2025-06-05T20:05:09Z",
      "closed_at": "2025-06-05T20:05:09Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1653/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1653",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1653",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:07.484805",
      "comments": [
        {
          "author": "johnshearing",
          "body": "I had this issue when using lightrag-server and resolved it with the following entries in the .env file.\n\n```\n### Number of worker processes, not greater than (2 x number_of_cores) + 1\nWORKERS=2\n### Number of parallel files to process in one batch\nMAX_PARALLEL_INSERT=2\n### Max concurrent requests to",
          "created_at": "2025-06-04T16:04:54Z"
        },
        {
          "author": "pablo-01",
          "body": "Thanks, the paramater in the .env file fixed it. ",
          "created_at": "2025-06-05T20:05:09Z"
        }
      ]
    },
    {
      "issue_number": 1657,
      "title": "[Feature Request]: Allow duplicates if their context/meaning is different",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nThis follows the discussion [from here](https://github.com/HKUDS/LightRAG/discussions/1526#discussioncomment-13369366).\n\nIt is very often the case, even within the boundaries of a field of scientific (e.g. computing), that a word-entity can take different meanings depending on the context. For example, a particular term in one programming language may have a different meaning in another or in the context of a communications protocol. The meaning of a word may also change over time, as the environment evolves, making it even harder (for humans) to draw a line. I expect similar semantic overlaps occur in all scientific fields.\n\nThe interpretation of the meaning/context will certainly have to rely on the actual (con)text and the skill of the LLM, to make that judgement call!\n\nWould you consider it -as a new feature request- to modify the algorithm to allow for duplicate entities, where the meaning/context is different?\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ntsarb",
      "author_type": "User",
      "created_at": "2025-06-04T21:43:59Z",
      "updated_at": "2025-06-05T16:48:15Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1657/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1657",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1657",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:07.645411",
      "comments": [
        {
          "author": "johnshearing",
          "body": "As I experiment with light-rag server, I think I am noticing that when the LLM finds words that are used in different contexts, it will always add some other word to the entity it creates so as to make all entities unique. I would imagine that disambiguating overloaded words is beneficial and delibe",
          "created_at": "2025-06-05T16:48:14Z"
        }
      ]
    },
    {
      "issue_number": 1581,
      "title": "[Bug]: I cant process large textbook",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI am using webUI server, and provided the folder to scan for new documents. It is able to scan and process a .txt file that is small couple of lines. However when i try to upload any textbook in .pdf or .md, it will always fail. I have provided the error message i get.\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n# Server Configuration\nHOST=0.0.0.0\nPORT=9621\nWORKERS=1\n\n# Directory Configuration\nINPUT_DIR=/home/auro/Downloads/Light-RAG/PDF_c\nWORKING_DIR=./rag_storage\n\n# PDF Processing Settings\nCHUNK_SIZE=500\nCHUNK_OVERLAP_SIZE=50\nMAX_PARALLEL_INSERT=1\nTIMEOUT=10000\nMAX_TOKENS=4096\n\n# Concurrency Controls\nMAX_ASYNC=1\nEMBEDDING_FUNC_MAX_ASYNC=1\nMAX_EMBED_TOKENS=4096\n\n# LLM Configuration (Ollama)\nTEMPERATURE=0.2\nLLM_BINDING=ollama\nLLM_MODEL=gemma3:1b\nLLM_BINDING_HOST=http://localhost:11434\n\n# Embedding Configuration (Ollama) \nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\n\n# Improved Extraction Settings\nENABLE_LLM_CACHE=true\nENABLE_LLM_CACHE_FOR_EXTRACT=true\nSUMMARY_LANGUAGE=English\n\n# Storage configuration\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\n# Debug settings\nLOG_LEVEL=DEBUG\nVERBOSE=True\n\n### Logs and screenshots\n\nProcessing 1 document(s)\nExtracting stage 1/1: new.md\nProcessing d-id: doc-a666165be050128861bcdcde2454e623\nFailed to extract entities and relationships: POST predict: Post \"http://127.0.0.1:37383/completion\": EOF (status code: 500)\nTraceback (most recent call last):\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/lightrag.py\", line 1002, in process_document\n    await asyncio.gather(*tasks)\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/lightrag.py\", line 1200, in _process_entity_relation_graph\n    raise e\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/lightrag.py\", line 1186, in _process_entity_relation_graph\n    chunk_results = await extract_entities(\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/operate.py\", line 853, in extract_entities\n    raise task.exception()\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/operate.py\", line 829, in _process_with_semaphore\n    return await _process_single_content(chunk)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/operate.py\", line 769, in _process_single_content\n    glean_result = await use_llm_func_with_cache(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/utils.py\", line 1648, in use_llm_func_with_cache\n    res: str = await use_llm_func(input_text, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/utils.py\", line 586, in wait_func\n    return await future\n           ^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/utils.py\", line 370, in worker\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/llm/ollama.py\", line 130, in ollama_model_complete\n    return await _ollama_model_if_cache(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/tenacity/__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/llm/ollama.py\", line 109, in _ollama_model_if_cache\n    raise e\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/lightrag/llm/ollama.py\", line 72, in _ollama_model_if_cache\n    response = await ollama_client.chat(model=model, messages=messages, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/ollama/_client.py\", line 837, in chat\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/ollama/_client.py\", line 682, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/auro/Downloads/Light-RAG/LightRAG/light/lib/python3.11/site-packages/ollama/_client.py\", line 626, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: POST predict: Post \"http://127.0.0.1:37383/completion\": EOF (status code: 500)\nFailed to extrat document 1/1: new.md\nDocument processing pipeline completed\n\n### Additional Information\n\n- LightRAG Version: \n- Operating System: Ubuntu\n- Python Version: 3.11\n- Related Issues:\n",
      "state": "open",
      "author": "brownauro2520",
      "author_type": "User",
      "created_at": "2025-05-14T22:13:14Z",
      "updated_at": "2025-06-05T07:42:37Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1581/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1581",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1581",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:07.848195",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Firstly, ensure that your Ollama instance is functioning properly. Secondly, LightRAG recommends using a model with a minimum of 32 billion parameters and a 32K context window. If your server does not have sufficient resources to run the required LLM efficiently, we suggest leveraging the LLM API as",
          "created_at": "2025-05-15T07:29:43Z"
        },
        {
          "author": "asdespot",
          "body": "Hello @brownauro2520 did you solve the problem, I also have the same problem with gemma3 models. Am using gemma3:4b and i have a A40 with 48gb of vram.",
          "created_at": "2025-05-30T11:15:48Z"
        },
        {
          "author": "brownauro2520",
          "body": "For me it doesn’t work still, however u can try running a bigger model 32b Gemma model or llama one. I don’t understand how model size affect it, the only thing that maters should be context window.\n\nOr for test u can try random LLM api.\n\nAlso, I think it is a time out issue, refer below I think tha",
          "created_at": "2025-05-30T20:35:57Z"
        },
        {
          "author": "asdespot",
          "body": "For me, setting MAX_ASYNC=1 solved the problem. Now I can run Gemma 3 14B on my 48 GB VRAM GPU with no problems.\n",
          "created_at": "2025-06-04T09:59:57Z"
        },
        {
          "author": "brownauro2520",
          "body": "> For me, setting MAX_ASYNC=1 solved the problem. Now I can run Gemma 3 14B on my 48 GB VRAM GPU with no problems.\n\n@asdespot were u able to run it with smaller models?\n\nAlso can you share your config or the env file whatever you have your settings in",
          "created_at": "2025-06-04T20:19:46Z"
        }
      ]
    },
    {
      "issue_number": 1635,
      "title": "[Question]: How to visualise the Knowledge Graph?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThere is any way to export the knowledge graph to a JSON file ? Or maybe any format which I can later visualise ?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "leonardoaraujosantos",
      "author_type": "User",
      "created_at": "2025-05-27T16:44:06Z",
      "updated_at": "2025-06-04T16:25:33Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1635/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1635",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1635",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.042756",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "What knowledge graph type are you talking about? NetworkXStorage? If that is the case you probably mean that you have a graphml file and you want to visualize it.  \n\nThe way I usually do this is https://gephi.org/ download this gephi app and import the file here. Then at the bottom you can turn on t",
          "created_at": "2025-05-28T20:17:49Z"
        },
        {
          "author": "Ja1aia",
          "body": "If you're using the default NetworkXStorage, you can refer to the example provided in the official repository here:\n\n🔗 [graph_visual_with_html.py](https://github.com/HKUDS/LightRAG/blob/main/examples/graph_visual_with_html.py)\n\nYou simply need to load the generated .graphml file, and the script will",
          "created_at": "2025-06-03T09:26:09Z"
        },
        {
          "author": "johnshearing",
          "body": "Export functions [are listed here](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#data-export-functions).\nAlso the light-rag server [found here](https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md) has a json export function on the API tab which works well and is super easy to u",
          "created_at": "2025-06-04T16:25:32Z"
        }
      ]
    },
    {
      "issue_number": 1262,
      "title": "[Feature Request]: Query only a specific range of nodes and edges in data storage",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nHello, I am currently using Lightrag-server and Neo4j Community Edition. Considering that in simple usage scenarios, we might store all nodes in a single database, some of the entities may contain conflicting information. For example, I uploaded the two documents \"The Indecision Overthinker.txt\" and \"The Ruthless Avenger.txt\" provide completely different descriptions of the entity Hamlet. However, I may only want to reference files that contain specific content in their filenames, they can be \"academic,\" \"financial,\" or \"technical.\", etc.\n\n![Image](https://github.com/user-attachments/assets/3ced31b7-9da9-4dba-864c-aabbfdf7ae54)\n\nI checked the prompt sent to the LLMs during the Retrieve stage, which indeed includes various attributes of the entity (including the date and source filename). In my database, I have stored both documents related to Hamlet. I attempted to instruct the LLMs to extract knowledge only from \"The Ruthless Avenger.txt\", and in fact, it did so correctly.\n\n![Image](https://github.com/user-attachments/assets/7b46ea5d-4ba8-4f0d-a386-32102655acc0)\n\n![Image](https://github.com/user-attachments/assets/9af92bcb-0242-49ce-9ca2-b90ad47b3c63)\n\nI believe the following needs may arise in actual use:\nSometimes, users may prefer to extract more recent knowledge.\nSometimes, users may already know which documents or what range of documents they want to extract knowledge from.\n\n**Thoughts:**\nSimply allow specifying a date to extract only knowledge from that point onward, or only knowledge within a certain time frame from the current date.\n\nSemantic processing of filenames—embedding might help locate the correct nodes using keywords, but this would require first retrieving all unrepeated file_path values from storage. This approach doesn’t seem very reliable, but I believe by specifing file_name in prompt, current LightRAG already allows for some degree of node classification.\nOr perhaps allow the users to define custom tags for nodes when uploading files, though this may require more code modifications.\n\nstill these are some vague thoughts, I'm not sure if there are other reliable storage methods that can already classify nodes and work with vanilla LightRAG, I just think I might need these features. If anyone is interested, please provide more ideas\n\n\n### Additional Context\n\nThe Indecision Overthinker:\n\"I'm Hamlet—a man who thinks too much and acts too little. I question everything, second-guess every move, and get lost in my own doubts. Even when I know what must be done, I hesitate, trapped in endless 'what ifs.' My mind is my greatest enemy, paralyzing me when action is needed most. I'm not weak—just too aware of consequences, too afraid of making the wrong choice.\"\n\nThe Ruthless Avenger:\n\"I’m Hamlet—a man who doesn’t hesitate when justice demands blood. I play the fool to deceive my enemies, but when the moment comes, I strike without mercy. I’ve sent traitors to their deaths, manipulated friends, and embraced violence when necessary. My father’s ghost called for vengeance, and I delivered. If others think me cruel, so be it—betrayal deserves no pity.\"\n\n\nEntities near Hamlet in KG\n![Image](https://github.com/user-attachments/assets/a65a9f06-a3e6-4e97-a18b-cf9751b15040)\n\nSimplely ask about Hamlet's character\n![Image](https://github.com/user-attachments/assets/824e7423-35a1-4bfe-bea2-30ee930f17d7)\n\nReferencing documents with conflicting information can lead to a certain degree of misunderstanding\n![Image](https://github.com/user-attachments/assets/0e83c56d-4dcc-411e-9851-cf2c728c2cd4)",
      "state": "open",
      "author": "Exploding-Soda",
      "author_type": "User",
      "created_at": "2025-04-03T06:47:34Z",
      "updated_at": "2025-06-04T13:08:24Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1262/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1262",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1262",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.301056",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "Yes, I would also like to see the addition of the \"metadata\" tags to entities like \"timestamp\" or other relevant data. This should be appendable when uploading a document. This way people can retrieve specifically the information added before or after a certain date.\n\nI would also like to add that i",
          "created_at": "2025-04-03T13:55:28Z"
        },
        {
          "author": "Miyamura80",
          "body": "+1 \nThis would be useful for providing KG for user-by-user",
          "created_at": "2025-04-06T16:13:55Z"
        },
        {
          "author": "emgiezet",
          "body": "+1\nFiltering by the metadata in queries like AWS Bedrock KnowlageBase & OpenSearch is super cool to have. You can add parameters to not let llm mixup the information between documents by narrowing the context.",
          "created_at": "2025-04-18T12:38:35Z"
        },
        {
          "author": "oliverkaiser",
          "body": "+1",
          "created_at": "2025-04-29T15:03:25Z"
        },
        {
          "author": "tanasecucliciu",
          "body": "+1",
          "created_at": "2025-05-08T10:04:50Z"
        }
      ]
    },
    {
      "issue_number": 1651,
      "title": "[Question]:rag.ainsert(texts, ids=doc_ids)，在插入时文档和文件id一一对应，希望在查询返回时返回对应的id",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n我使用mongo保存文本数据，有对应的文本数据和文件id，在插入时文档和文件id一一对应，希望在查询返回时返回对应的id\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "kejishucloud",
      "author_type": "User",
      "created_at": "2025-06-03T10:08:18Z",
      "updated_at": "2025-06-03T10:08:18Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1651/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1651",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1651",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.498959",
      "comments": []
    },
    {
      "issue_number": 1012,
      "title": "[Question]: 多用户支持",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n请问单个实例下如何支持多个用户同时建立索引，感谢~\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "wangzhen38",
      "author_type": "User",
      "created_at": "2025-03-06T01:50:03Z",
      "updated_at": "2025-06-03T09:10:13Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1012/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1012",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1012",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.498978",
      "comments": [
        {
          "author": "wangzhen38",
          "body": "我看源码实现，是不是可以在insert的时候传入用户id，一直到写数据库的时候，使用用户id代替之前的workspace，然后查询的时候同样传入用户id，去对应指定数据库读取，请问这样是否行的通？",
          "created_at": "2025-03-06T02:35:06Z"
        },
        {
          "author": "wade-liwei",
          "body": "兄弟，多用户你实现的怎么样了？\n\n带带弟弟。",
          "created_at": "2025-06-03T09:10:13Z"
        }
      ]
    },
    {
      "issue_number": 1575,
      "title": "[Question]: I need to set up the LightRAG service to support multiple collections or databases. Currently, when the program starts, the service only works with a single collection. Is there a way I can create and use multiple collections dynamically?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n_No response_\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "tranhuuan170302",
      "author_type": "User",
      "created_at": "2025-05-14T03:09:46Z",
      "updated_at": "2025-06-02T20:00:11Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1575/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1575",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1575",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.717682",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Yes, support for multiple document collections (workspaces) is included in our product roadmap. ",
          "created_at": "2025-05-14T14:07:51Z"
        },
        {
          "author": "tranhuuan170302",
          "body": "> Yes, support for multiple document collections (workspaces) is included in our product roadmap.\n\nhow can I implementation it? pls help me clarify it",
          "created_at": "2025-05-18T02:47:55Z"
        },
        {
          "author": "netandreus",
          "body": "It is a very needed feature. When do you plan to implement it? @danielaskdd  is there is some beta version?",
          "created_at": "2025-06-02T19:59:32Z"
        }
      ]
    },
    {
      "issue_number": 1648,
      "title": "[Question] : Performance Bottleneck with Large Scale Ingestion (50K Documents) - LightRAG FastAPI + Ray Integration",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n\n\nIssue Description\nWe are currently building a FastAPI application that integrates LightRAG for document ingestion and querying, leveraging Ray for distributed processing. While the system works effectively for smaller datasets, we are encountering significant performance degradation and extended processing times when attempting to ingest a large volume of documents (approximately 50,000 text-heavy files, primarily PDFs).\n\nWe suspect the bottleneck lies within the distributed ingestion pipeline, specifically concerning the interaction between Ray actors, external API calls for embeddings, and LightRAG's internal storage mechanisms. We are seeking insights, potential optimizations, or best practices for handling such large-scale ingestion with LightRAG and Ray.\n\nSystem Configuration\nFastAPI Version: (Assumed latest, please specify if different)\nLightRAG Version: (Assumed latest, please specify if different)\nRay Version: (Assumed latest, please specify if different)\nPython Version: 3.9+\nOperating System: Linux (e.g., Ubuntu)\nHardware: (Mention CPU cores, RAM, SSD/HDD, e.g., \"16 vCPUs, 64GB RAM, NVMe SSD\")\nOpenAI-Compatible API: Self-hosted (http://**********) using gpt-4o for LLM and text-embedding-ada-002 for embeddings.\nWorking Directory: Local disk path (./test_storage)\nObserved Behavior with 50,000 Documents\nIngestion Time: Extends from minutes (for ~100 documents) to many hours or potentially days (for 50,000 documents).\nResource Utilization:\nCPU: Appears to be utilized by Ray workers during text extraction and RAG ingestion, but not consistently at 100% across all cores.\nMemory: Ray object store and individual actor memory usage grow, but don't typically hit OOM errors.\nDisk I/O: High during ingestion as LightRAG writes data.\nNetwork I/O: Significant during embedding calls to the external API.\nError Rates: Occasional HTTP 429 (Too Many Requests) from the embedding API, handled by retry logic, but this indicates API saturation and contributes to delays.\nResponsiveness: The /ingest endpoint blocks until the entire ingestion process (including Ray tasks) completes.\nCurrent Implementation Details (Code Snippets for Context)\nOur FastAPI application sets up Ray and LightRAG on startup. The ingestion process is designed to be distributed.\n\n1. FastAPI Startup & Global RAG Initialization:\n(Relevant for the single RAG instance used for querying)\n\nPython\n\n# app.py\n```\nimport os\nimport json\nimport asyncio\nimport httpx\nimport nest_asyncio\nimport textract\nimport ray\nimport threading\nimport shutil\nimport base64\nfrom uuid import uuid4\nfrom typing import List, Dict, Any, Optional\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger\n\nfrom fastapi import FastAPI, UploadFile, File, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nimport aiofiles\nimport psutil\nimport time\nfrom starlette.responses import Response\nfrom starlette.background import BackgroundTasks\n\n# Environment configurations\nos.environ[\"OPENAI_API_BASE\"] = \"http:/****************\"\nos.environ[\"OPENAI_API_KEY\"] = \"****************\"\nAPI_BASE = os.environ[\"OPENAI_API_BASE\"]\nAPI_KEY = os.environ[\"OPENAI_API_KEY\"]\nHEADERS = {\n    \"Authorization\": f\"Bearer {API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\nWORKING_DIR = \"./test_storage\"\nTRACKING_FILE = os.path.join(WORKING_DIR, \"tracking.json\")\n\n# Custom embedding function with concurrency limit\nEMBEDDING_CONCURRENCY_LIMIT = 5\nembedding_semaphore = asyncio.Semaphore(EMBEDDING_CONCURRENCY_LIMIT)\n\nasync def ada_embed(texts: list[str]) -> list[list[float]]:\n    url = f\"{API_BASE}/v1/embeddings\"\n    payload = {\"model\": \"text-embedding-ada-002\", \"input\": texts}\n    async with embedding_semaphore:\n        async with httpx.AsyncClient(timeout=500.0) as client:\n            try:\n                response = await client.post(url, headers=HEADERS, json=payload)\n                response.raise_for_status()\n                return [d[\"embedding\"] for d in response.json()[\"data\"]]\n            except httpx.HTTPStatusError as e:\n                if e.response.status_code == 429:\n                    print(f\"Client error '429 Too Many Requests' for URL '{url}'. Retrying after delay...\")\n                    await asyncio.sleep(2) # Example: wait 2 seconds\n                    raise\n                else: raise\n            except Exception as e:\n                print(f\"An unexpected error occurred during embedding: {e}\")\n                raise\nada_embed.embedding_dim = 3072\n\nasync def gpt_4o_generate(prompt: str, **kwargs) -> str:\n    # ... (similar httpx call to API_BASE/v1/chat/completions)\n\nasync def initialize_rag_instance() -> LightRAG:\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        addon_params={\"insert_batch_size\": 100}, # Configured batch size for LightRAG internal insertion\n        embedding_func=ada_embed,\n        llm_model_func=gpt_4o_generate,\n    )\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n    return rag\n\napp = FastAPI(title=\"LightRAG Server\")\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    ray.init(num_cpus=psutil.cpu_count(logical=True), object_store_memory=8 * 1024 * 1024 * 1024, ignore_reinit_error=True)\n    global global_rag_instance\n    global_rag_instance = await initialize_rag_instance()\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    if global_rag_instance: await global_rag_instance.finalize_storages()\n    ray.shutdown()\n    import gc; gc.collect()\n2. Distributed Ingestion Endpoint (/ingest) and Ray Actor Logic:\n(Core of the distributed ingestion using Ray)\n\nPython\n\n# app.py (continued)\n\n# Ray remote function for text extraction\n@ray.remote\ndef extract_text_from_file(file_path: str) -> Dict[str, Any]:\n    try:\n        file_name = os.path.basename(file_path)\n        custom_doc_id = f\"{file_name}-{uuid4().hex[:8]}\"\n        extracted_text = textract.process(file_path).decode(\"utf-8\")\n        return {\n            \"success\": True, \"file_path\": file_path, \"file_name\": file_name,\n            \"custom_doc_id\": custom_doc_id, \"extracted_text\": extracted_text,\n            \"text_length\": len(extracted_text)\n        }\n    except Exception as e:\n        return {\"success\": False, \"file_path\": file_path, \"file_name\": os.path.basename(file_path), \"error\": str(e)}\n\n# Ray actor for batched RAG ingestion\n@ray.remote\nclass FixedBatchRAGIngestionActor:\n    def __init__(self, working_dir: str, actor_id: int):\n        self.working_dir = working_dir\n        self.actor_id = actor_id\n        self.rag = None # Initialized asynchronously\n\n    async def initialize(self):\n        # Initializes LightRAG instance within the actor\n        self.rag = LightRAG(working_dir=self.working_dir, embedding_func=ada_embed, llm_model_func=gpt_4o_generate)\n        await self.rag.initialize_storages()\n        await initialize_pipeline_status()\n\n    async def ingest_batch(self, extracted_data_batch: List[Dict[str, Any]], batch_id: int) -> Dict[str, Any]:\n        results = []\n        batch_tracking = {}\n        for extracted_data in extracted_data_batch:\n            if not extracted_data[\"success\"]:\n                results.append(extracted_data)\n                continue\n            \n            custom_doc_id = extracted_data[\"custom_doc_id\"]\n            extracted_text = extracted_data[\"extracted_text\"]\n            file_name = extracted_data[\"file_name\"]\n            \n            chunks = [extracted_text] # Currently, each document is a single chunk\n            chunk_ids = [f\"{custom_doc_id}-{i}\" for i in range(len(chunks))]\n            \n            max_retries = 5\n            base_delay = 1.0\n            for attempt in range(max_retries):\n                try:\n                    if attempt > 0: await asyncio.sleep(base_delay * (2 ** (attempt - 1)))\n                    await self.rag.ainsert(chunks, ids=chunk_ids) # LightRAG ingestion call\n                    batch_tracking[custom_doc_id] = {\"file_name\": file_name, \"doc_ids\": chunk_ids}\n                    results.append({\"success\": True, \"custom_doc_id\": custom_doc_id, \"file_name\": file_name, \"chunk_ids\": chunk_ids, \"chunks_count\": len(chunk_ids)})\n                    break\n                except Exception as e:\n                    if attempt == max_retries - 1: raise e\n                    print(f\"[RAG ACTOR {self.actor_id}] Attempt {attempt + 1} failed for {file_name}: {e}\")\n            else: # Executed if loop completes without break (all retries failed)\n                results.append({\"success\": False, \"custom_doc_id\": custom_doc_id, \"file_name\": file_name, \"error\": str(e)}) # Ensure 'e' is captured\n        return {\"results\": results, \"tracking\": batch_tracking}\n\n    async def finalize(self):\n        if self.rag: await self.rag.finalize_storages()\n\n# Orchestration of distributed ingestion\nasync def ingest_files_distributed_batch_fixed(file_paths: List[str], num_workers: int = 2, batch_size: int = 3):\n    if num_workers > 4: # Hardcoded warning/reduction\n        print(\"Warning: Using more than 4 workers may cause storage contention issues. Reducing to 4 workers for stability.\")\n        num_workers = 4\n\n    extraction_futures = [extract_text_from_file.remote(file_path) for file_path in file_paths]\n    extracted_results = ray.get(extraction_futures)\n    successful_extractions = [r for r in extracted_results if r[\"success\"]]\n\n    actors = [FixedBatchRAGIngestionActor.remote(WORKING_DIR, i) for i in range(num_workers)]\n    init_futures = [actor.initialize.remote() for actor in actors]\n    ray.get(init_futures)\n\n    batches = [successful_extractions[i:i + batch_size] for i in range(0, len(successful_extractions), batch_size)]\n    batch_futures = []\n    for batch_id, batch in enumerate(batches):\n        actor = actors[batch_id % num_workers]\n        future = actor.ingest_batch.remote(batch, batch_id)\n        batch_futures.append(future)\n        if batch_id < len(batches) - 1: await asyncio.sleep(0.1) # Small delay for smoother distribution\n\n    batch_results = ray.get(batch_futures)\n    all_tracking = {}\n    all_results = []\n    for batch_result in batch_results:\n        all_results.extend(batch_result.get(\"results\", []))\n        all_tracking.update(batch_result.get(\"tracking\", {}))\n    update_tracking_data(all_tracking) # Updates shared tracking.json\n\n    finalize_futures = [actor.finalize.remote() for actor in actors]\n    ray.get(finalize_futures)\n```\n\n    # ... return ingestion summary\n3. Ingestion API Endpoint:\n\nPython\n\n# app.py (continued)\n\n```\n@app.post(\"/ingest\")\nasync def ingest_file_api(file: UploadFile = File(...)):\n    temp_dir = f\"temp_uploads/{uuid4()}\"\n    os.makedirs(temp_dir, exist_ok=True)\n    temp_file_path = os.path.join(temp_dir, file.filename)\n    try:\n        async with aiofiles.open(temp_file_path, \"wb\") as out_file:\n            content = await file.read()\n            await out_file.write(content)\n\n        # num_workers=6 and batch_size=2 are hardcoded currently\n        ingestion_result = await ingest_files_distributed_batch_fixed(\n            file_paths=[temp_file_path],\n            num_workers=6,\n            batch_size=2\n        )\n        return {\"message\": \"File ingestion initiated successfully\", \"result\": ingestion_result}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"File ingestion failed: {e}\")\n    finally:\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n```\n\nSuspected Bottlenecks and Questions\nBased on the observed behavior and code review, we believe the primary bottlenecks are:\n\nEmbedding API Rate Limits / Latency: The ada_embed function is protected by a semaphore, but 5 concurrent calls might still be too low if the external API has significant latency or stricter internal rate limits. The current retry logic helps, but constant 429s will slow down the entire process.\nQuestion: What are the recommended concurrency settings for text-embedding-ada-002 with LightRAG when using an external API? Are there strategies for more adaptive rate limiting?\nLightRAG ainsert Performance: While insert_batch_size is set to 100, the underlying write operations to the vector store and knowledge graph might be slow for individual documents, especially when scaled across many concurrent actors writing to the same shared storage location (./test_storage).\nQuestion: How does LightRAG's internal storage (initialize_storages, finalize_storages) handle high concurrent writes from multiple Ray actors? Are there known limitations or recommended storage backends/configurations for large-scale distributed ingestion?\nQuestion: Would increasing the batch_size in ingest_files_distributed_batch_fixed (e.g., from 2 to 10 or 20 documents per actor batch) improve overall throughput, given that each document is currently treated as a single chunk?\nRay Worker Count vs. Storage Contention: We've limited num_workers to 4 for stability, acknowledging potential storage contention.\nQuestion: Is there a recommended maximum number of Ray actors for LightRAG ingestion, especially when using a local file system for working_dir? Would shifting the working_dir to a highly performant network file system (e.g., NFS, shared block storage) mitigate contention?\ntracking.json Writes: The update_tracking_data function uses a threading.Lock but still writes to a single JSON file.\nQuestion: For 50,000+ documents, would this become a significant I/O bottleneck? Are there alternative, more performant ways to track document ingestion status in a distributed environment (e.g., using a small, lightweight distributed key-value store)?\nSingle Document Chunking: Currently, chunks = [extracted_text] means each document is treated as one large chunk. This simplifies the process but might be inefficient for very large documents.\nQuestion: Would implementing more granular chunking within extract_text_from_file and then passing multiple smaller chunks to ainsert (which supports multiple chunks per ainsert call) improve performance or memory usage?\nRequest for Assistance\nAny guidance, best practices, or potential optimizations for scaling LightRAG ingestion to tens of thousands or hundreds of thousands of documents would be highly appreciated. Specific advice on Ray configuration, LightRAG storage strategies, and handling external API interactions at scale would be particularly valuable.\n\nThank you for your time and support!\n",
      "state": "open",
      "author": "pankaj-2k01",
      "author_type": "User",
      "created_at": "2025-06-02T11:15:12Z",
      "updated_at": "2025-06-02T11:17:37Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1648/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1648",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1648",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.891388",
      "comments": []
    },
    {
      "issue_number": 1646,
      "title": "[Question]: Question about keyword_extraction flag usage",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhat is the purpose of setting keyword_extraction to true? Even if it's not set, the LLM should still be able to extract keywords if the prompt is designed for it and the expected output format is mentioned. So, why is it necessary to enable keyword_extraction?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Utkarshagharat58",
      "author_type": "User",
      "created_at": "2025-06-02T04:28:57Z",
      "updated_at": "2025-06-02T04:28:57Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1646/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1646",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1646",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.891411",
      "comments": []
    },
    {
      "issue_number": 30,
      "title": "Entity Extraction Failure: No Entities or Relationships Extracted with ollama models",
      "body": "**Setup**\r\n- Ubuntu 22.04\r\n- 2x NVIDIA RTX A5000 GPU (48 GB VRAM)\r\n\r\n**Description**\r\nI am encountering an issue with LightRAG where entity extraction consistently fails when using ollama models. Even though the system successfully processes chunks from a document, no entities or relationships are extracted, and the resulting graph contains 0 nodes and 0 edges.  I have tried both  `llama3.1:70b` and `llama3.2:3b` served via ollama.\r\n\r\n**Steps to Reproduce:**\r\nUsed the following code to initialize and run LightRAG:\r\n\r\n```python\r\nimport os\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\r\nfrom lightrag.utils import EmbeddingFunc\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=ollama_model_complete,  \r\n    llm_model_name='llama3.2:3b',\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=768,\r\n        max_token_size=8192,\r\n        func=lambda texts: ollama_embedding(\r\n            texts, \r\n            embed_model=\"nomic-embed-text:latest\"\r\n        )\r\n    ),\r\n)\r\n\r\nwith open(\"./book.txt\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")))\r\n```\r\n\r\n**Observed the following logs:**\r\n\r\n```\r\nLogger initialized and directory created.\r\n42 chunks processed successfully.\r\nEntity extraction failed with no entities or relationships found.\r\nFull Logs:\r\nplaintext\r\nCopy code\r\n2024-10-16 22:57:21,241 - lightrag - INFO - Logger initialized for working directory: ./dickens\r\n2024-10-16 22:57:21,241 - lightrag - DEBUG - LightRAG init with param:\r\n  working_dir = ./dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 768, 'max_token_size': 8192, 'func': <function <lambda> at 0x716396d12160>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function ollama_model_complete at 0x7163357649a0>,\r\n  llm_model_name = llama3.2:3b,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x716335762020>\r\n\r\n2024-10-16 22:57:21,241 - lightrag - INFO - Load KV full_docs with 0 data\r\n2024-10-16 22:57:21,242 - lightrag - INFO - Load KV text_chunks with 0 data\r\n2024-10-16 22:57:21,242 - lightrag - INFO - Load KV llm_response_cache with 0 data\r\n2024-10-16 22:57:21,243 - lightrag - INFO - Creating a new event loop in a sub-thread.\r\n2024-10-16 22:57:21,243 - lightrag - INFO - [New Docs] inserting 1 docs\r\n2024-10-16 22:57:21,645 - lightrag - INFO - [New Chunks] inserting 42 chunks\r\n2024-10-16 22:57:21,645 - lightrag - INFO - Inserting 42 vectors to chunks\r\n2024-10-16 22:57:26,800 - lightrag - INFO - [Entity Extraction]...\r\n2024-10-16 23:02:18,328 - lightrag - WARNING - Didn't extract any entities, maybe your LLM is not working\r\n2024-10-16 23:02:18,328 - lightrag - WARNING - No new entities and relationships found\r\n2024-10-16 23:02:18,337 - lightrag - INFO - Writing graph with 0 nodes, 0 edges\r\n``` \r\n\r\n**Expected Behavior:**\r\nEntities and relationships should be extracted from the processed chunks, and the resulting graph should contain nodes and edges representing them.\r\n\r\n**Observed Behavior:**\r\nNo entities or relationships are extracted. The following warnings appear in the logs:\r\n\r\nWARNING - Didn't extract any entities, maybe your LLM is not working\r\nWARNING - No new entities and relationships found\r\nThe final graph contains 0 nodes and 0 edges.\r\n\r\n**Additional Information:**\r\nLLM Model: llama3.2:3b was used, but entity extraction consistently fails with `llama3.1:70b` as well.\r\nWorking Directory: Set to ./dickens.\r\n\r\nQuestion:\r\n\r\nHow does hardcoding the `tiktoken_model_name` to `gpt-4o-mini` in `lightrag.py` supposed to work with other non-OpenAI models? \r\n\r\n\r\n```python\r\n@dataclass\r\nclass LightRAG:\r\n    working_dir: str = field(\r\n        default_factory=lambda: f\"./lightrag_cache_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}\"\r\n    )\r\n\r\n    # text chunking\r\n    chunk_token_size: int = 1200\r\n    chunk_overlap_token_size: int = 100\r\n    tiktoken_model_name: str = \"gpt-4o-mini\"\r\n```    \r\n \r\nAftet attempting to exchange `gpt-4o-mini` with `llama3.2:3b`  and running the demo script, I get an error log which is summarized by GPT-4o as follows:\r\n \r\n``` \r\nThe error you are encountering happens because the model name llama3.2:3b is not automatically recognized by the tiktoken library, which is responsible for handling the tokenization process. The error message suggests that tiktoken cannot map llama3.2:3b to an appropriate tokenizer.\r\n``` \r\n      \r\nAttempted Fixes: Verified that the document chunks are processed, but no entities are extracted.\r\nPlease let me know if any further details or debugging information are needed. Thank you for your assistance.\r\n\r\n\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "maxruby",
      "author_type": "User",
      "created_at": "2024-10-16T21:24:01Z",
      "updated_at": "2025-06-02T01:02:20Z",
      "closed_at": "2024-10-20T09:18:29Z",
      "labels": [
        "good first PR"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 42,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/30/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/30",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/30",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.891418",
      "comments": []
    },
    {
      "issue_number": 1069,
      "title": "[Bug]: <title>Failed to process document - doc-4ed46e6033f61a939bf0930ad9c61590: {} (status code: 500)",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n---the output of program -----\nINFO: Process 166934 Shared-Data created for Single Process\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './chinese-bingli/vdb_entities.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './chinese-bingli/vdb_relationships.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './chinese-bingli/vdb_chunks.json'} 0 data\nINFO: Process 166934 initialized updated flags for namespace: [full_docs]\nINFO: Process 166934 ready to initialize storage namespace: [full_docs]\nINFO: Process 166934 initialized updated flags for namespace: [text_chunks]\nINFO: Process 166934 ready to initialize storage namespace: [text_chunks]\nINFO: Process 166934 initialized updated flags for namespace: [entities]\nINFO: Process 166934 initialized updated flags for namespace: [relationships]\nINFO: Process 166934 initialized updated flags for namespace: [chunks]\nINFO: Process 166934 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 166934 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 166934 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 166934 initialized updated flags for namespace: [doc_status]\nINFO: Process 166934 ready to initialize storage namespace: [doc_status]\nINFO: Process 166934 storage namespace already initialized: [full_docs]\nINFO: Process 166934 storage namespace already initialized: [text_chunks]\nINFO: Process 166934 storage namespace already initialized: [llm_response_cache]\nINFO: Process 166934 storage namespace already initialized: [doc_status]\nINFO: Process 166934 Pipeline namespace initialized\nFailed to process document 123 - doc-4ed46e6033f61a939bf0930ad9c61590: {} (status code: 500)\n\n\n---background---\nlightrag:latest main (up to 03.12 14:00:00 )\nprompt.py   modify from english to chinese,\nbook.txt   modify like this:【病例id：41420241764120241011，患者姓名：赵**,女性,年龄52,2024年09月28日入院,2024年10月11日出院。出院科室推拿科,诊断编码及名称包括：K07.604,颞颌关节炎,G50.000,三叉神经痛,M50.101+G55.1*,颈椎间盘突出伴有神经根病,E78.000,纯高胆固醇血症。手术及操作编码及名称包括：83.9800x001,,软组织治疗性药物局部注射,,17.96100,,针刀治疗,,17.92110,,颈椎病推拿治疗,,17.91100,,毫针治疗,,17.94100,,拔罐治疗,,17.91350,,温针灸治疗。治疗期间费用包括：2024年09月28日  11时0分 子午流注开穴法 医保费用分类丙类 数量 1.0每个穴位  花费10.0元 微波治疗 医保费用分类丙类 数量 1.0每部位/次  花费20.0元 拔罐疗法 医保费用分类丙类 数量 4.03罐  花费60.0元 灸法 医保费用分类丙类 数量 1.0次  花费20.0元 电针 医保费用分类丙类 数量 4.0二个穴位  花费80.0元 耳针(磁珠压耳穴) 医保费用分类丙类 数量 1.0单耳  花费17.0元 。2024年09月28日  12时0分 II级护理（优质护理） 医保费用分类丙类 数量 1.0日  花费35.0元 五床间及以上 医保费用分类丙类 数量 1.0床日  花费12.0元 住院诊查费 医保费用分类丙类 数量 1.0日  花费30.0元 。2024年09月28日  3时23分 真空采血管/抗凝剂/环氧乙烷灭菌 医保费用分类丙类 数量 1.0支  花费0.85元 静脉采血器采血 医保费用分类丙类 数量 1.0次  花费4.0元 。2024年09月28日  3时4分 25羟维生素D测定(免疫学法等) 医保费用分类丙类 数量 1.0项  花费40.0元 人免疫缺陷病毒抗体测定（免疫学法） 医保费用分类丙类 数量 1.0项  花费25.0元 十二通道心电图检查 医保费用分类丙类 数量 1.0次  花费20.0元】 \n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n采用了ollma的llm以及embedding\nllm_model_name=\"deepseek-r1:7b\",\nollama_embed：embed_model=\"shaw/dmeta-embedding-zh:latest\"\n\n同时参考，将embedding_batch_num设置为4；\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:1.2.5\n- Operating System:ubuntu 20\n- Python Version: 3.12\n- Related Issues: \n\n\nvdb_entities.json 这个文件没有实体，是空的。",
      "state": "open",
      "author": "wjj97172",
      "author_type": "User",
      "created_at": "2025-03-12T08:38:12Z",
      "updated_at": "2025-06-02T00:49:41Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1069/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1069",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1069",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:08.891426",
      "comments": [
        {
          "author": "seanzhang-zhichen",
          "body": "> ### Do you need to file an issue?\n> * [x]  I have searched the existing issues and this bug is not already filed.[ ]  I believe this is a legitimate bug, not just a question or feature request.\n> \n> ### Describe the bug\n> ---the output of program ----- INFO: Process 166934 Shared-Data created for ",
          "created_at": "2025-03-13T10:17:57Z"
        },
        {
          "author": "RXZAN",
          "body": "我也遇到这个问题，请问有没有解决？",
          "created_at": "2025-04-21T02:02:19Z"
        },
        {
          "author": "Victory66",
          "body": "请问该问题解决了吗",
          "created_at": "2025-06-02T00:49:40Z"
        }
      ]
    },
    {
      "issue_number": 1631,
      "title": "[Question]:About Entity Merging",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello LightRAG team, thank you for creating this amazing project.\n\nCurrently, LightRAG’s deduplication function D(·) merges entities solely based on exact key matching. I’m wondering if adding the number of shared neighbors (common_neighbors) between two nodes as an additional metric might help catch hidden duplicates that exact matching alone misses (e.g., “Donald J. Trump” vs. “Donald Trump”)?\n\nLet me know what you think of this idea:\n\t•\tWhile the added comparison may increase the number of calls and token usage,\n\t•\tI believe it represents a valuable compromise for improving graph consistency.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "Eunchan24",
      "author_type": "User",
      "created_at": "2025-05-26T13:44:35Z",
      "updated_at": "2025-05-30T14:16:15Z",
      "closed_at": "2025-05-30T13:33:23Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1631/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1631",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1631",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.065264",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "Does #1323 mention this already?",
          "created_at": "2025-05-28T20:32:32Z"
        },
        {
          "author": "Eunchan24",
          "body": "Ah, I actually missed that — thanks for pointing it out, @frederikhendrix!",
          "created_at": "2025-05-30T13:33:23Z"
        }
      ]
    },
    {
      "issue_number": 1594,
      "title": "[Bug]: MongoDocStatusStorage implementation does not have `delete` method",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n`MongoDocStatusStorage` has not implemented the `async def delete(self, ids: list[str]) -> None` method.\nBecause of this you can't use MongoDB as DocStatusStorage. In order to fix it you would have to implement this method. Returned error:\n```\n\"Can't instantiate abstract class MongoDocStatusStorage without an implementation for abstract method 'delete'\"\n```\n\n### Steps to reproduce\n\nInitialize LightRag with this config:\n```\n        rag = LightRAG(\n            working_dir=working_dir,\n            llm_model_func=llm_model_func,\n            embedding_func=embedding_finc,\n            doc_status_storage=\"MongoDocStatusStorage\",\n        )\n        await rag.initialize_storages()\n        await initialize_pipeline_status()\n```\ntry:\n```\nrag_service.insert(document)\n```\nThis will raise the error mentioned above.\n\n### Expected Behavior\n\nThe document should have been inserted in to the MongoDB successfully.\n\n### LightRAG Config Used\n\n```\n        rag = LightRAG(\n            working_dir=working_dir,\n            llm_model_func=llm_model_func,\n            embedding_func=embedding_finc,\n            doc_status_storage=\"MongoDocStatusStorage\",\n        )\n```\n\n### Logs and screenshots\n\n```\n{\n    \"error\": \"Can't instantiate abstract class MongoDocStatusStorage without an implementation for abstract method 'delete'\"\n}\n```\n\n### Additional Information\n\n- LightRAG Version: v1.3.7\n- Operating System: MacOS\n- Cpu: Apple M1 Pro (arm64)\n- Python Version: 3.12.9\n- Mongo image used: mongo:latest\n",
      "state": "open",
      "author": "kysre",
      "author_type": "User",
      "created_at": "2025-05-18T20:49:14Z",
      "updated_at": "2025-05-30T07:18:00Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1594/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1594",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1594",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.251696",
      "comments": [
        {
          "author": "kysre",
          "body": "I like to work on this issue myself. Currently, I want to use mongo for my DocStatusStorage and need this bug fixed.",
          "created_at": "2025-05-18T20:50:40Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "Would you mind reviewing this problem?  https://github.com/HKUDS/LightRAG/issues/1555",
          "created_at": "2025-05-23T10:18:28Z"
        },
        {
          "author": "liu-nian-dh",
          "body": "I am also encountering errors when using MongoDocStatusStorage here.\n\n```\n\"Can't instantiate abstract class MongoDocStatusStorage without an implementation for abstract method 'delete'\"\n```\n\nThe issue has been resolved using the methods here.\nhttps://github.com/HKUDS/LightRAG/issues/1416#issuecommen",
          "created_at": "2025-05-30T07:18:00Z"
        }
      ]
    },
    {
      "issue_number": 1104,
      "title": "[Question]: Error mesaage when doing an insert: \"del pipeline_status[\"history_messages\"][:]  KeyError: 'history_messages'",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nLightRAG (wonderful system) was running until recently. \n\nAfter an update I am now getting \n\n`File \"/mnt/c/Projects/Work/LightRAG/LightRAG/lightrag/lightrag.py\", line 800, in apipeline_process_enqueue_documents\n    del pipeline_status[\"history_messages\"][:]\n        ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\nKeyError: 'history_messages'`\n\nwhen doing an insert.\n \nI did a fresh install of lightrag and all dependencies without any pip cache etc. \n\nI post this as a question. Not sure if bug would be appropriate since this is quite fundamental (occurring with each insert\"). Has anyone made a similar observation? \n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "zwurgl",
      "author_type": "User",
      "created_at": "2025-03-17T15:54:50Z",
      "updated_at": "2025-05-30T02:41:14Z",
      "closed_at": "2025-03-28T20:04:28Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1104/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1104",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1104",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.448448",
      "comments": [
        {
          "author": "JoramMillenaar",
          "body": "I ran into a similar issue. It's because the shared data has not been initialized yet. Calling `initialize_share_data(1)` before you call your stuff should fix it.",
          "created_at": "2025-03-17T16:06:16Z"
        },
        {
          "author": "zwurgl",
          "body": "Thanks for the hint, Joram! \n\nI followed the advice: \n\n`rag = LightRAG( .... )\n\nfrom lightrag.kg.shared_storage import initialize_share_data, finalize_share_data \ninitialize_share_data(1)\n\nprint(\"adding documents ...\")\nfor text in texts:\n    rag.insert(text)\n`\nbut it still complains about the KeyErr",
          "created_at": "2025-03-17T16:26:20Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "You'd likely need to call the initialization before instantiating the LightRAG\n\n```python\nfrom lightrag.kg.shared_storage import initialize_share_data, finalize_share_data\n\ninitialize_share_data(1)\nrag = LightRAG( .... )\n\nprint(\"adding documents ...\")\nfor text in texts:\nrag.insert(text)\n```",
          "created_at": "2025-03-17T16:28:46Z"
        },
        {
          "author": "zwurgl",
          "body": "Hm, Seems to be coming from somewhere else. I tried to do `initialize_share_data(1)` before and after my object definition. (Even before and after :-) --> Error still comes. \n\nWhen I don't do it explicitly myself, I still see in the logs that the initialization has taken place (see above). So seem t",
          "created_at": "2025-03-17T16:45:24Z"
        },
        {
          "author": "zwurgl",
          "body": "tried also to call `initialize_pipeline_status()` ... \n\nbut error remains (and this function call complains \n\n`testlightrag.py:66: RuntimeWarning: coroutine 'initialize_pipeline_status' was never awaited\n  initialize_pipeline_status()`\n\n",
          "created_at": "2025-03-17T16:59:34Z"
        }
      ]
    },
    {
      "issue_number": 1641,
      "title": "[Bug]: Can Edit or add Entity with rag function, But there is no function for specific delete targeted entity ",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/035098de-97d4-4767-922c-69fcbde87a39)\nAny documentation how to use it?\n\nI use Postgres DB and Neo4jGraph. \nThank you\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "molion097",
      "author_type": "User",
      "created_at": "2025-05-29T04:49:25Z",
      "updated_at": "2025-05-29T04:50:45Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1641/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1641",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1641",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.635395",
      "comments": []
    },
    {
      "issue_number": 1636,
      "title": "[Bug]:API Timeout error during retreival",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I try to run retrieval from WebUI,  I see that it sends a query to the LLM to extract keywords from the query but nothing is sent after that.\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\nWas expecting a response in the WebUI\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n```\n\nINFO: Process 988 building query context...\nINFO: Query nodes: Intelligence, Social structures, Echolocation, Habitat, Marine mammals, top_k: 10, cosine: 0.2\nINFO: limit_async: 16 new workers initialized\nERROR: limit_async: Error in decorated function: RetryError[<Future at 0x233c3b93bd0 state=finished raised APITimeoutError>]\n2025-05-27 19:52:16 - ASCIIColors - ERROR - Exception Traceback (RetryError)\nTraceback (most recent call last):\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_backends\\auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"..\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"..\\lightrag\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"..\\lightrag\\Lib\\site-packages\\openai\\_base_client.py\", line 1484, in request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"..\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"..\\lightrag\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"..\\lightrag\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\llm\\openai.py\", line 451, in openai_embed\n    response = await openai_async_client.embeddings.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 245, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\openai\\_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\openai\\_base_client.py\", line 1502, in request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\api\\routers\\query_routes.py\", line 145, in query_text\n    response = await rag.aquery(request.query, param=param)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\lightrag.py\", line 1443, in aquery\n    response = await kg_query(\n               ^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\operate.py\", line 917, in kg_query\n    context = await _build_query_context(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\operate.py\", line 1243, in _build_query_context\n    ll_data = await _get_node_data(\n              ^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\operate.py\", line 1351, in _get_node_data\n    results = await entities_vdb.query(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\kg\\nano_vector_db_impl.py\", line 128, in query\n    embedding = await self.embedding_func(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\utils.py\", line 586, in wait_func\n    return await future\n           ^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\utils.py\", line 370, in worker\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\utils.py\", line 242, in __call__\n    return await self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\lightrag\\utils.py\", line 242, in __call__\n    return await self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"..\\lightrag\\Lib\\site-packages\\tenacity\\__init__.py\", line 421, in exc_check\n    raise retry_exc from fut.exception()\ntenacity.RetryError: RetryError[<Future at 0x233c3b93bd0 state=finished raised APITimeoutError>]\n\nINFO: 127.0.0.1:50153 - \"POST /query HTTP/1.1\" 500\n```\n\n### Additional Information\n\n- LightRAG Version: 1.3.7\n- Operating System: Win 11\n- Python Version: 3.11\n- Related Issues:\n",
      "state": "open",
      "author": "mercurial-moon",
      "author_type": "User",
      "created_at": "2025-05-27T18:42:35Z",
      "updated_at": "2025-05-29T01:03:12Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1636/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1636",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1636",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.635417",
      "comments": [
        {
          "author": "fingeng",
          "body": "I have the same problem：\n\nINFO:openai._base_client:Retrying request to /embeddings in 0.857938 seconds\nERROR: limit_async: Error in decorated function: RetryError[<Future at 0x7f2ff3a2d2d0 state=finished raised APITimeoutError>]\nAn error occurred: RetryError[<Future at 0x7f2ff3a2d2d0 state=finished ",
          "created_at": "2025-05-28T05:15:55Z"
        },
        {
          "author": "mercurial-moon",
          "body": "@fingeng not yet, have posted on the discord channel too but no replies yet.",
          "created_at": "2025-05-28T06:19:21Z"
        },
        {
          "author": "DMBMZ1",
          "body": "i have the same problem like this......",
          "created_at": "2025-05-28T15:59:56Z"
        },
        {
          "author": "fingeng",
          "body": "我终于跑通了 需要更改url 这个项目默认是openai官方的 我用的是第三方 需要自己更改\n",
          "created_at": "2025-05-28T16:05:50Z"
        },
        {
          "author": "DMBMZ1",
          "body": "怎么理解？就是openai 兼容的API是不能用的吗？",
          "created_at": "2025-05-28T16:29:27Z"
        }
      ]
    },
    {
      "issue_number": 1639,
      "title": "[Bug]: User defined env configuration does not take effect（Docker Image）",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/1375506b-4b81-42e6-8796-8ed4054254fd)\n\n![Image](https://github.com/user-attachments/assets/3c8ebf62-6e7a-410e-a839-4b7d28406243)\n\n![Image](https://github.com/user-attachments/assets/13de5796-c227-40b8-a8ca-0b82f9f4ad27)\n\n### Steps to reproduce\n\ndocker compose up -d\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "IAMJOYBO",
      "author_type": "User",
      "created_at": "2025-05-28T22:40:54Z",
      "updated_at": "2025-05-28T22:40:54Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1639/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1639",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1639",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863274",
      "comments": []
    },
    {
      "issue_number": 1637,
      "title": "[Bug]:limit_async: Error in decorated function: RetryError[<Future at 0x7f2ff3a2d2d0 state=finished raised APITimeoutError>] An error occurred: RetryError[<Future at 0x7f2ff3a2d2d0 state=finished raised APITimeoutError>]",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n(lightrag) root@LAPTOP-A4Q6SSQG:~/LightRAG# python examples/lightrag_openai_demo.py\n2025-05-28 13:11:21 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /root/miniconda3/envs/lightrag/bin/python | Command base: /root/miniconda3/envs/lightrag/bin/python -m pip\n\nLightRAG demo log file: /root/LightRAG/lightrag_demo.log\n\nINFO: Process 6798 Shared-Data created for Single Process\nINFO: Created new empty graph\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './dickens/vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './dickens/vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './dickens/vdb_chunks.json'} 0 data\nINFO: Process 6798 initialized updated flags for namespace: [full_docs]\nINFO: Process 6798 ready to initialize storage namespace: [full_docs]\nINFO: Process 6798 KV load full_docs with 0 records\nINFO: Process 6798 initialized updated flags for namespace: [text_chunks]\nINFO: Process 6798 ready to initialize storage namespace: [text_chunks]\nINFO: Process 6798 KV load text_chunks with 0 records\nINFO: Process 6798 initialized updated flags for namespace: [entities]\nINFO: Process 6798 initialized updated flags for namespace: [relationships]\nINFO: Process 6798 initialized updated flags for namespace: [chunks]\nINFO: Process 6798 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 6798 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 6798 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 6798 KV load llm_response_cache with 0 records\nINFO: Process 6798 initialized updated flags for namespace: [doc_status]\nINFO: Process 6798 ready to initialize storage namespace: [doc_status]\nINFO: Process 6798 doc status load doc_status with 0 records\nINFO: Process 6798 storage namespace already initialized: [full_docs]\nINFO: Process 6798 storage namespace already initialized: [text_chunks]\nINFO: Process 6798 storage namespace already initialized: [llm_response_cache]\nINFO: Process 6798 storage namespace already initialized: [doc_status]\nINFO: Process 6798 Pipeline namespace initialized\nINFO: limit_async: 16 new workers initialized\nINFO: Storage Initialization completed!\nINFO:openai._base_client:Retrying request to /embeddings in 0.402493 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.779730 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.380552 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.862378 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.411222 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.857938 seconds\nERROR: limit_async: Error in decorated function: RetryError[<Future at 0x7f2ff3a2d2d0 state=finished raised APITimeoutError>]\nAn error occurred: RetryError[<Future at 0x7f2ff3a2d2d0 state=finished raised APITimeoutError>]\n\nDone!\nINFO: Creating a new event loop in main thread.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n没有更改源文件，按照quick start中依序执行命令 \n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:the latest\n- Operating System:Linux Ubuntu\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "fingeng",
      "author_type": "User",
      "created_at": "2025-05-28T05:25:18Z",
      "updated_at": "2025-05-28T05:25:18Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1637/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1637",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1637",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863294",
      "comments": []
    },
    {
      "issue_number": 1634,
      "title": "[Feature Request]: Disable SSL verification in the AsyncOpenAI client",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nAdd the ability to disable SSL verification in the AsyncOpenAI client by accepting an http_client parameter for flexible configuration.\n\n### Additional Context\n\nHello,\n\nI'd like to draw your attention to an issue I'm encountering while using your extension. I'm using an Open AI-like chat/embeddings API, but it uses a self-signed SSL certificate. This causes SSL verification issues in my API calls.\n\nCurrently, I'm using the embedding_func->openai_embed->create_openai_async_client function to initialize the client. However, this function only accepts the api_key, base_url, and client_configs parameters, and I can't directly pass an HTTP client configured with httpx.AsyncClient(verify=False) to disable SSL verification, nor can I disable SSL verification via an environment variable. Unlike request, HTTPX doesn't automatically retrieve variables. Therefore, I can't add a certificate either.\n\nI'd like to know if there's a way to disable SSL verification in the AsyncOpenAI client, or if this could be considered in a future update of the extension by accepting an http_client parameter (AsyncOpenAI recognizes this).\n\nFor now, I've had to rework some functions in my code to disable SSL verification, but I'm wondering if there's a cleaner way to handle this, or if the extension could be updated to include this flexibility.\n\nThank you in advance for your attention and help on this matter.\n\nSincerely,",
      "state": "open",
      "author": "cambierelliot",
      "author_type": "User",
      "created_at": "2025-05-27T13:20:55Z",
      "updated_at": "2025-05-27T13:24:45Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1634/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1634",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1634",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863300",
      "comments": []
    },
    {
      "issue_number": 1629,
      "title": "[Question]:Cross-chunk relations & multi-hop retrieval design choices in LightRAG",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi LightRAG team :wave:,\n\nFirst, thanks for releasing such a clean and practical RAG framework—its **dual-level retrieval** has been working great for us!\n\nWhile reviewing the indexing flow I noticed that\n\n* entity / relation extraction is performed **within each chunk only**;  \n* during graph construction, entities deduplicated by `D(·)` become shared nodes, but **no new edges are created between entities that never co-occur in the same chunk**.\n\nI’d love to understand the design rationale and its impact on deeper reasoning:\n\n1. **Why omit explicit cross-chunk edge creation?**  \n   * Is the main goal to avoid graph explosion, reduce LLM calls, minimise hallucination risk, or something else?\n\n2. **Multi-hop coverage**  \n   * In practice, does the current dual-level retrieval (low-level 1-hop + high-level keyword expansion) capture **>2-hop relations** reliably when the entities never appeared together?  \n   * Have you benchmarked long-range queries that depend on such paths (e.g. linking characters introduced in different chapters of a novel)?\n\n3. **Possible extensions**  \n   * Would the project welcome an option to add lightweight cross-chunk edges, such as:  \n     * *coreference-resolved “same entity” links*  \n     * *“same paragraph / same section” contextual edges*  \n     * *LLM-inferred relations between high-frequency entity pairs across chunks*  \n\n   * *Prompting an LLM on a vector‑based **top‑k chunk set** to infer cross‑chunk relations*\nAny insights, papers, or code pointers would be greatly appreciated.  \nThanks again for the excellent work!\n\nBest regards,  \nSangwookBaek\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "SangwookBaek",
      "author_type": "User",
      "created_at": "2025-05-26T03:57:15Z",
      "updated_at": "2025-05-26T03:57:15Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1629/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1629",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1629",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863349",
      "comments": []
    },
    {
      "issue_number": 1628,
      "title": "[Question]:Cross-chunk relations & multi-hop retrieval design choices in LightRAG",
      "body": "\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "SangwookBaek",
      "author_type": "User",
      "created_at": "2025-05-26T03:55:22Z",
      "updated_at": "2025-05-26T03:56:41Z",
      "closed_at": "2025-05-26T03:56:41Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1628/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1628",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1628",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863358",
      "comments": []
    },
    {
      "issue_number": 1627,
      "title": "[Feature Request]: Adding document deletion in the lightrag server frontend",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nWhile the document deletion is available in the lightrag package, the lightrag server does not have document deletion. Is it possible to add document deletion to the lightrag server frontend? \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "acsangamnerkar",
      "author_type": "User",
      "created_at": "2025-05-25T21:01:45Z",
      "updated_at": "2025-05-25T21:01:45Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1627/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1627",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1627",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863364",
      "comments": []
    },
    {
      "issue_number": 1626,
      "title": "[Feature Request]: Postgres: Creating extensions VECTOR and AGE if not already extended in the Postgres DB",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nRequest is to create VECTOR extension OR AGE extension in PostgresDB if it is not already extended. This will streamline the operational deployment process of Lightrag server using Docker with Postgres DB. \n\nAs of now, the configure_age class method does check the AGE graph if will be a simple change to the method to accomplish the AGE extension creation if it does not exist. \n\nVECTOR extension check and creation can be done when the vector db is created in the postgres implementation.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "acsangamnerkar",
      "author_type": "User",
      "created_at": "2025-05-25T20:53:55Z",
      "updated_at": "2025-05-25T20:53:55Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1626/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1626",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1626",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863368",
      "comments": []
    },
    {
      "issue_number": 1417,
      "title": "[Feature Request]: Official Docker Image for LightRAG",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nCurrently, Docker Hub hosts 17 different images for LightRAG, created by various contributors, making it challenging to identify the most stable and up-to-date version.\n\nPlease consider publishing an official Docker image under an authoritative repository (e.g., `lightrag/lightrag`).\n\nAn official Docker image would:\n- Ensure a trusted and regularly updated source.\n- Simplify deployment and maintenance.\n- Prevent confusion caused by multiple unofficial builds.\n\nExample official repository structure: `hub.docker.com/r/lightrag/lightrag`\n\nThank you for considering this fr!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "aospan",
      "author_type": "User",
      "created_at": "2025-04-20T12:35:08Z",
      "updated_at": "2025-05-25T20:49:59Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "docker"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1417/reactions",
        "total_count": 6,
        "+1": 6,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1417",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1417",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:09.863374",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thank you for the great idea. I noticed the Docker Compose file referenced in #1356. To streamline the process, could you please simplify it into a standard Docker Compose format? This will enable the LightRAG team to efficiently publish Docker images based on it.\n\nFor simplicity, I recommend that t",
          "created_at": "2025-04-20T23:39:18Z"
        },
        {
          "author": "helicalchris",
          "body": "Hi is there any progress on this? Thought I'd ask as I am looking for a docker-compose and yes, it's complicated to choose the right one now!",
          "created_at": "2025-05-25T20:49:58Z"
        }
      ]
    },
    {
      "issue_number": 1513,
      "title": "[Question]: Multiple Graphs / Users Setup?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI want to know if there is a way to set up LightRAG to work with multiple graphs. I am in a situation where I need this to be used for multiple users, each having an ID. What I did so far is have multiple working dirs, each folder with the id of a user. Which seemed to work, but sometimes user A could ask a question, which would be answered with user B's inserted text. Now I am wondering if I am missing a setting, or whether or not it is possible to do this at all. If there are any ideas I would love to try them out. Pretty desperate for a solution to this..\nThank you in advance! :) \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "vilewired",
      "author_type": "User",
      "created_at": "2025-05-03T20:07:14Z",
      "updated_at": "2025-05-25T20:04:14Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1513/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1513",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1513",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.017212",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You can run multiple instances of LightRAG Server:\n1. Create a folder for each instance of LightRAG Server\n2. Create an .env file in each folder, and assign difference port for each LightRAG Server\n3. Launch each LightRAG Server within its respective directory to ensure it correctly reads the approp",
          "created_at": "2025-05-05T03:11:37Z"
        },
        {
          "author": "vilewired",
          "body": "But I might get multiple thousands of users, I cannot run one instance for everyone of them",
          "created_at": "2025-05-14T08:49:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "Support for multiple document collections (workspaces) is already included in the LightRAG product roadmap.",
          "created_at": "2025-05-14T09:38:25Z"
        },
        {
          "author": "bastianwegge",
          "body": "We do something like this right now:\n```python\nasync def initialize_lightrag(topic_id: str) -> LightRAG:\n    working_dir = os.path.join(WORKING_DIR, \"topics/\", topic_id)\n    logger.info(f\"Setting up LightRAG in {working_dir}\")\n    os.makedirs(working_dir, exist_ok=True)\n\n    rag = LightRAG(\n        ",
          "created_at": "2025-05-23T13:39:56Z"
        },
        {
          "author": "vilewired",
          "body": "Thank you both. @bastianwegge I already wrote very similar code to have one dir per user (since it doesn't take up too much space) but I ended up with scenarios where asking the same / similar question resulted in info from one directory to be leaking into another and data scrambled. I currently nee",
          "created_at": "2025-05-25T20:04:13Z"
        }
      ]
    },
    {
      "issue_number": 1596,
      "title": "[Question]:Clarification on Edge Weight and Rank Logic",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIs the weight in relations basically the strength of the relationship between two entities, as determined by the LLM? If so, is there a specific range of weights, or does the LLM just decide the values on its own based on how relevant it thinks the relationship is?\n\nAlso, during the retrieval process, does LightRAG use both the relation weights and ranking to find the most relevant entities and relations? Does it follow a method like: if the sum of weight and rank is higher compared to others, then that relation is considered first? Or is there some other logic used?\n\nI would really appreciate any insights or documentation you could share on this.\n\nThanks in advance!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "martina0211",
      "author_type": "User",
      "created_at": "2025-05-19T02:31:58Z",
      "updated_at": "2025-05-25T19:00:24Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1596/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1596",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1596",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.184381",
      "comments": [
        {
          "author": "johnshearing",
          "body": "I need clarification about how the relationship weights in the graph_chunk_entity_relation.graphml file affect a query response.\n\nI see in the README.md under the heading of [Entities and Relations](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#edit-entities-and-relations) that we can assign ",
          "created_at": "2025-05-24T20:26:42Z"
        },
        {
          "author": "johnshearing",
          "body": "This is the answer for the previous post:\n\nThe LightRAG server responded that higher weights mean a stronger relationship and that the relationships with higher weights are more likely to be used to produce the query response.\n\nAfter looking in a large knowledge graph file I noticed that the LLM wil",
          "created_at": "2025-05-24T21:06:28Z"
        },
        {
          "author": "johnshearing",
          "body": "After further investigation:\n\nI did a search for the word \"weight\" and looked though all the code in the lightrag directory of the LightRAG project and found many instances where the weight is recorded and inserted into various json and vdb files, but was not able to identify any code that uses the ",
          "created_at": "2025-05-25T18:54:36Z"
        }
      ]
    },
    {
      "issue_number": 1625,
      "title": "[Bug]:lightrag_openai_compatible_demo.py uses ollama_embed instead of openai_embed and doesn't import this function",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nIn the file examples/lightrag_openai_compatible_demo.py, the demo is supposed to show compatibility with OpenAI's embedding model. However, it incorrectly uses ollama_embed for the embedding function, and does not import the openai_embed function at all. As a result, the script fails .\n\n### Steps to reproduce\n\n### Steps to reproduce the behavior\n\n1. **import this function**  `from lightrag.llm.openai import openai_embed`\n2. **use** \n    `func=lambda texts: openai_embed(texts , model , base_url , api_key)`\n\n\n### Expected Behavior\n\nThe demo should import and use the openai_embed function when OpenAI is selected as the embedding backend. It should not rely on ollama_embed in this context.\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\nThis issue may confuse users who expect openai_compatible_demo.py to work out-of-the-box with OpenAI. Please consider correcting the demo or updating the README to clarify.\n\nYou can contact me at: 1877296512@qq.com if any clarification is needed.If this original code is actually right , please teach me about the function . Just want to learn better about the wonderful lightrag technology.Thanks.\n",
      "state": "open",
      "author": "achiever-zbw",
      "author_type": "User",
      "created_at": "2025-05-25T15:44:17Z",
      "updated_at": "2025-05-25T15:47:28Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1625/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1625",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1625",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.360699",
      "comments": []
    },
    {
      "issue_number": 1616,
      "title": "[Question] About modeling document-chunk-entity graph in RAG use case",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi LightRAG team,\n\nWe’re working on ontology-based RAG project using supply-chain data and official USTR documents. Now using the LightRAG as a base and relying heavily on a graph engine to model relationships between document and domain, connected by **event nodes** which hold date information.\n\nWe have some questions about best practices for graph modeling for document:\n\n### 1. About using chunk_id as source_id instead of document_id\nIn LightRAG’s example(example/insert_custom_kg.py), each chunk is assigned a unique source_id. We’re currently following this, but we wonder if this is ideal for our case.\n\nSince our downstream queries and reasoning are often document-level (e.g., linking supply-chain events to official documents), would it make more sense to assign the source_id based on the document instead of each chunk?\n\n### 2. Use existing entity node or create new one?\nNow we extract entities and relationships from each chunk of a USTR document. Naturally, the same name of entities extract in multiple chunks with slightly different descriptions.\nAbout this, we considers below two approaches and would love your advice:\n\nApproach1. **Use existing entity node**\nPros: Avoid redundancy, easier to count node with filtering entity name\nCons: Hard to handle descriptions of entity, risk of losing context\n\nApproach2. **Create new entity node per chunk**\nPros: Keep contextual info intact, no conflict in metadata\nCons: Cause duplication, harder to analyze globally\n\nThank you so much in advance :)\nYour insights would be helpful as scale our RAG project.\n\nBest Regards,\nByeonggyu\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ggyuchive",
      "author_type": "User",
      "created_at": "2025-05-22T13:37:00Z",
      "updated_at": "2025-05-25T13:59:02Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1616/reactions",
        "total_count": 4,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1616",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1616",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.360721",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "I am not familiar with your business scenario, so I do not yet understand the problem you need to solve. Could you provide some excerpts from supply-chain data and official USTR documents, along with the entities and relationships extracted from them, as well as the queries to be used in the process",
          "created_at": "2025-05-24T04:28:57Z"
        },
        {
          "author": "grootmy",
          "body": "@danielaskdd \n\nThank you for your interest in our project! Here are the specific details you requested:\n\nOur goal is to analyze how trade policy announcements impact supply chain operations.\n\n## Sample Data\n\n### 1. USTR Document Data\n- Date: 2023-05-12\n- Title: \"USTR Extends Certain COVID-Related Ex",
          "created_at": "2025-05-25T13:59:00Z"
        }
      ]
    },
    {
      "issue_number": 1622,
      "title": "[Bug]:ERROR:neo4j.pool:Unable to retrieve routing information Error getting node for les opérations d'investissement: Unable to retrieve routing information",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nERROR:neo4j.pool:Unable to retrieve routing information\nError getting node for les opérations d'investissement: Unable to retrieve routing information\n\ni face this problem for a week a changed the network even new neo4j but the problem persist  \nplease help \n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "dinari-eskander",
      "author_type": "User",
      "created_at": "2025-05-24T21:34:23Z",
      "updated_at": "2025-05-24T21:34:23Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1622/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1622",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1622",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.555449",
      "comments": []
    },
    {
      "issue_number": 1527,
      "title": "[Bug]: ModuleNotFoundError: No module named 'pyuca'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n I got this error when starting the server with lightrag-gunicorn --workers 1 :\n\n\n```\n File \"/home/andrey/lightrag/lightrag/api/routers/document_routes.py\", line 6, in <module>\n    from pyuca import Collator\nModuleNotFoundError: No module named 'pyuca'\n```\n\nThe module is installed with pip but I got the error.\n\n### Steps to reproduce\n\n1) Get the latest release with git clone / git pull\n2) Try to run the  server\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n### This is sample file of .env\n\n### Server Configuration\n# HOST=0.0.0.0\n# PORT=9621\n# WORKERS=2\n# CORS_ORIGINS=http://localhost:3000,http://localhost:8080\nWEBUI_TITLE='Graph RAG Engine'\nWEBUI_DESCRIPTION=\"Simple and Fast Graph Based RAG System\"\n\n### Optional SSL Configuration\n# SSL=true\n# SSL_CERTFILE=/path/to/cert.pem\n# SSL_KEYFILE=/path/to/key.pem\n\n### Directory Configuration (defaults to current working directory)\n# WORKING_DIR=<absolute_path_for_working_dir>\n# INPUT_DIR=<absolute_path_for_doc_input_dir>\n\n### Ollama Emulating Model Tag\n# OLLAMA_EMULATING_MODEL_TAG=latest\n\n### Max nodes return from grap retrieval\n# MAX_GRAPH_NODES=1000\n\n### Logging level\n# LOG_LEVEL=INFO\n# VERBOSE=False\n# LOG_MAX_BYTES=10485760\n# LOG_BACKUP_COUNT=5\n### Logfile location (defaults to current working directory)\n# LOG_DIR=/path/to/log/directory\n\n### Settings for RAG query\nHISTORY_TURNS=3\nCOSINE_THRESHOLD=0.2\nTOP_K=60\nMAX_TOKEN_TEXT_CHUNK=2000\nMAX_TOKEN_RELATION_DESC=2000\nMAX_TOKEN_ENTITY_DESC=2000\n\n### Settings for document indexing\nSUMMARY_LANGUAGE=English\nCHUNK_SIZE=200\nCHUNK_OVERLAP_SIZE=20\n\n### Number of parallel processing documents in one patch\nMAX_PARALLEL_INSERT=1\n\n### Max tokens for entity/relations description after merge\nMAX_TOKEN_SUMMARY=500\n### Number of entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)\nFORCE_LLM_SUMMARY_ON_MERGE=5\n\n### Num of chunks send to Embedding in single request\nEMBEDDING_BATCH_NUM=8\n### Max concurrency requests for Embedding\n#EMBEDDING_FUNC_MAX_ASYNC=2\nMAX_EMBED_TOKENS=8192\n\n### LLM Configuration\n### Time out in seconds for LLM, None for infinite timeout\nTIMEOUT=150\n### Some models like o1-mini require temperature to be set to 1\nTEMPERATURE=0.01\n### Max concurrency requests of LLM\nMAX_ASYNC=1\n### Max tokens send to LLM (less than context size of the model)\nMAX_TOKENS=1024\nENABLE_LLM_CACHE=true\nENABLE_LLM_CACHE_FOR_EXTRACT=true\n\n### Ollama example (For local services installed with docker, you can use host.docker.internal as host)\nLLM_BINDING=ollama\nLLM_MODEL=qwen3:8b\nLLM_BINDING_API_KEY=\nLLM_BINDING_HOST=http://localhost:11434\n\n### OpenAI alike example\n# LLM_BINDING=openai\n# LLM_MODEL=gpt-4o\n# LLM_BINDING_HOST=https://api.openai.com/v1\n# LLM_BINDING_API_KEY=your_api_key\n### lollms example\n# LLM_BINDING=lollms\n# LLM_MODEL=mistral-nemo:latest\n# LLM_BINDING_HOST=http://localhost:9600\n# LLM_BINDING_API_KEY=your_api_key\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nEMBEDDING_MODEL=Definity/snowflake-arctic-embed-l-v2.0-q8_0:latest\nEMBEDDING_DIM=1024\n# EMBEDDING_BINDING_API_KEY=your_api_key\n### ollama example\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\n### OpenAI alike example\n# EMBEDDING_BINDING=openai\n# LLM_BINDING_HOST=https://api.openai.com/v1\n### Lollms example\n# EMBEDDING_BINDING=lollms\n# EMBEDDING_BINDING_HOST=http://localhost:9600\n\n### Optional for Azure (LLM_BINDING_HOST, LLM_BINDING_API_KEY take priority)\n# AZURE_OPENAI_API_VERSION=2024-08-01-preview\n# AZURE_OPENAI_DEPLOYMENT=gpt-4o\n# AZURE_OPENAI_API_KEY=your_api_key\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/5b8d0eb7-2d35-43a6-9a27-64f74e58514d)\n\n### Additional Information\n\n- LightRAG Version: 1.3.6\n- Operating System: Ubuntu 24.04 LTS\n- Python Version: 3.12\n- Related Issues:\n",
      "state": "open",
      "author": "ndrewpj",
      "author_type": "User",
      "created_at": "2025-05-05T22:09:06Z",
      "updated_at": "2025-05-24T20:33:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1527/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1527",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1527",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.555469",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Try to install `pyuca` manually to see what happen.",
          "created_at": "2025-05-05T23:44:52Z"
        },
        {
          "author": "ndrewpj",
          "body": "> Try to install `pyuca` manually to see what happen.\n\nwhat do you mean by manually? I wrote that I used 'pip install pyuca'",
          "created_at": "2025-05-06T07:17:28Z"
        },
        {
          "author": "danielaskdd",
          "body": "After cloning the repository, please install the LightRAG server and verify that the installation completes without errors by running the following command:\n```\npip install -e \".[api]\"\n```",
          "created_at": "2025-05-06T08:11:19Z"
        },
        {
          "author": "ndrewpj",
          "body": "> After cloning the repository, please install the LightRAG server and verify that the installation completes without errors by running the following command:\n> \n> ```\n> pip install -e \".[api]\"\n> ```\n\nthat did not help:\n\nStarting Gunicorn with direct Python API...\nTraceback (most recent call last):\n",
          "created_at": "2025-05-24T20:33:38Z"
        }
      ]
    },
    {
      "issue_number": 1615,
      "title": "[Question]: Different responses from LightRAG UI vs local script — please review my implementation",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi team,\n\nI'm facing an issue where I get different responses when using the LightRAG UI versus running LightRAG via the repo code. Despite using the same input and settings (as closely as possible), the output varies significantly between the two.\n\nBelow is the code I’m using locally. I’ve customized the embedding and LLM functions to use text-embedding-ada-002 and gpt-4o, and I'm initializing and querying LightRAG as shown:\n\npython\n```\nimport os\nimport asyncio\nimport httpx\nimport nest_asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger\n\n# 🔧 Setup logging\nsetup_logger(\"lightrag\", level=\"INFO\")\n\n# 🔐 Environment config\nos.environ[\"http_proxy\"] = \"\"\nos.environ[\"https_proxy\"] = \"\"\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OPENAI_API_BASE\"] = \"*****************\"\nos.environ[\"OPENAI_API_KEY\"] = \"******************\"\n\nAPI_BASE = os.environ[\"OPENAI_API_BASE\"]\nAPI_KEY = os.environ[\"OPENAI_API_KEY\"]\n\nHEADERS = {\n    \"Authorization\": f\"Bearer {API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 📁 RAG working dir\nWORKING_DIR = \"./rag_storage\"\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# 🔹 Custom embedding function\nasync def ada_embed(texts: list[str]) -> list[list[float]]:\n    url = f\"{API_BASE}/v1/embeddings\"\n    payload = {\n        \"model\": \"text-embedding-ada-002\",\n        \"input\": texts\n    }\n    async with httpx.AsyncClient(timeout=500.0) as client:\n        response = await client.post(url, headers=HEADERS, json=payload)\n        response.raise_for_status()\n        return [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n# Add required attribute for LightRAG\nada_embed.embedding_dim = 3072\n\n\n# 🔹 Custom LLM function\nasync def gpt_4o_generate(prompt: str, **kwargs) -> str:\n    url = f\"{API_BASE}/v1/chat/completions\"\n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"temperature\": 0.1\n    }\n    async with httpx.AsyncClient(timeout=500.0) as client:\n        response = await client.post(url, headers=HEADERS, json=payload)\n        response.raise_for_status()\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n\n\n\n# 🔹 Init LightRAG\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        embedding_func=ada_embed,\n        llm_model_func=gpt_4o_generate,\n    )\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n    return rag\n\nimport pandas as pd\n\nasync def insert_csv_into_rag(rag, csv_path: str):\n    # Load CSV\n    df = pd.read_csv(csv_path)\n\n    # Option 1: Combine all rows and columns into a single string\n    text_data = df.to_string(index=False)\n\n    # Option 2 (recommended): Customize which columns to include\n    # text_data = \"\\n\".join([f\"{row['title']} - {row['description']}\" for _, row in df.iterrows()])\n\n    # Insert into LightRAG\n    await rag.ainsert(text_data)\n\n# 🔹 Main logic\nasync def main():\n    rag = None\n    try:\n        rag = await initialize_rag()\n        await rag.aclear_cache()\n        result = rag.query(\n            \"How many employees are under Mr. B Srinivasan?\",\n            param=QueryParam(\n                mode=\"hybrid\",\n                only_need_context=False,\n                only_need_prompt=False,\n                response_type=\"Multiple Paragraph\",\n                top_k=10,\n                max_token_for_text_unit=4000,\n                max_token_for_global_context=4000,\n                max_token_for_local_context=4000,\n                user_prompt=\"Give answer only from the retreived context\"\n            )\n        )\n\n        \n        # result = rag.query(\n        #     \"How many employess are under Mr. B Srinivasan ?\",\n        #     param=QueryParam(mode=\"hybrid\")\n        # )\n        print(result)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        if rag:\n            await rag.finalize_storages()\n\n\n# 🔁 Run async properly even in Jupyter / VSCode\nif __name__ == \"__main__\":\n    import sys\n\n    nest_asyncio.apply()\n    asyncio.get_event_loop().run_until_complete(main())\n\n```\n\n\n\n### Additional Context\n\nI’ve tried multiple tweaks but haven’t been able to align the local results with what I see on the UI. Could someone please take a look and let me know what I might be missing or doing incorrectly?\n\nAny help would be greatly appreciated. 🙏\n\nThanks!",
      "state": "open",
      "author": "pankaj-2k01",
      "author_type": "User",
      "created_at": "2025-05-22T12:20:00Z",
      "updated_at": "2025-05-24T07:56:44Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1615/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1615",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1615",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.711494",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "I found that the default query parameters in WebUI differ from the code you provided in two places:  \n1. top_k=40: This affects the number of entities and relationships recalled in the query.  \n2. History sessions: Three rounds of historical sessions are referenced, and the LLM’s responses take into",
          "created_at": "2025-05-24T04:34:49Z"
        },
        {
          "author": "pankaj-2k01",
          "body": "I am able to retrieve all relevant chunks related to the query However LLM is not able to generate response with it, Can you look at the code and let me know why it's happening, Am I not able to pass those chunks to the LLM ? \n\nBoth UI and Code using same LLM and Embedding function with same paramet",
          "created_at": "2025-05-24T07:56:44Z"
        }
      ]
    },
    {
      "issue_number": 1617,
      "title": "[Bug]: response field larger than field limit (131072)",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHello,\n\n# Current situation\n\nWhen I use the `rag.query` in LightRAG Vanilla with the parameter `response_type=\"Multiple Paragraphs\"` there is a bug if the response is too long.\n\n### Steps to reproduce\n\nTake a document with a long description, initialize LightRAG vanilla and ask to repeat the description.\nThen launch: \n\n``` python\nrag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=768,\n                max_token_size=8192,\n                func=embedding,\n            ),\n        )\n\n with open(name_of_element_, \"r\") as file:\n        text = file.read()\n        # Inserting the text into LightRAG\n        rag.insert(text)\n\n\nrag.query(\n        query=query,\n        param=QueryParam(\n            mode=\"mix\",\n            top_k=5,\n            response_type=\"Multiple Paragraphs\"\n        ),\n    )\n```\n\n### Expected Behavior\n\n``` python\nraise ValueError(f\"Failed to parse CSV string: {str(e)}\")\nValueError: Failed to parse CSV string: field larger than field limit (131072)\n```\n\n### LightRAG Config Used\n\n# Paste your config here\nrag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=768,\n                max_token_size=8192,\n                func=embedding,\n            ),\n        )\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.3.2\n- Operating System:\n- Python Version: 3.11.12\n- Related Issues:\n",
      "state": "open",
      "author": "theauAg",
      "author_type": "User",
      "created_at": "2025-05-22T14:49:33Z",
      "updated_at": "2025-05-24T04:41:59Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1617/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1617",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1617",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:10.879210",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The information you provided is insufficient to determine where the error occurs in the program. Additionally, the new version of LightRAG has replaced CSV format with JSON format when constructing context-aware queries. Please check the latest version to see if the same issue persists.",
          "created_at": "2025-05-24T04:41:58Z"
        }
      ]
    },
    {
      "issue_number": 1614,
      "title": "[Bug]:visit the api \"http://localhost:9621/query\" with the Error{\"response\":\"Sorry, I'm not able to provide an answer to that question.[no-context]\"}",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWe wanna execute this order \"lightrag-gunicorn --workers 4\", but the panel of the knowledge graph doesn't work with the panel of  the document worked. whether do our setting which LLM is Qwen affect this result?\n\nAnd at the same time, this error happens when we execute the following order:\n(deers) (base) ubuntu@VM-132-111-ubuntu:~/githubs/LightRAG-main$ curl -X POST \"http://localhost:9621/query\"     -H \"Content-Type: application/json\"     -d '{\"query\": \"您的问题:给出大概得关系介绍\", \"mode\": \"hybrid\"}'\n{\"response\":\"Sorry, I'm not able to provide an answer to that question.[no-context]\"}\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "why6why",
      "author_type": "User",
      "created_at": "2025-05-22T08:43:39Z",
      "updated_at": "2025-05-24T00:42:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1614/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1614",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1614",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:11.059281",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Please verify the pipeline status after document processing to confirm whether any entities have been extracted. Note that Light RAG requires an LLM with at least 32B parameters.",
          "created_at": "2025-05-24T00:42:42Z"
        }
      ]
    },
    {
      "issue_number": 1606,
      "title": "[Feature Request]: Add text origin option to text requests",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nWhen we send a file to the API, lightRAG automatically uses the file path as source for the RAG system, I believe it would be a good feature to add it as an option for text requests as well, since they are marked as \"unknown_source\".\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "belabon25",
      "author_type": "User",
      "created_at": "2025-05-21T10:07:43Z",
      "updated_at": "2025-05-23T13:26:40Z",
      "closed_at": "2025-05-23T13:26:40Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1606/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1606",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1606",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:11.268754",
      "comments": []
    },
    {
      "issue_number": 1353,
      "title": "[Feature Request]: Multiple Prompt.py files for different use-cases",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nCurrently I am planning on using the LightRAG implementation for multiple different use-cases. To guide the AI a bit better for my specific use-case I change the prompt, the examples, the rag_response and this ultimately leads to a better result.\n\nBut when I want to use it for a different use-case I'm not able to have a second or third prompt.py file which I can insert at different endpoints.\n\nMaybe it is an idea to add to query_param variable the name of the prompt.py file you want to use, also to the settings and set it to default there?\n\nAnd then do something like this: \n\n**Directory structure:**\n\n```\nmy_application/\n└── prompts/\n    ├── __init__.py\n    ├── default.py\n    └── alternative.py\n```\n\n**prompts/__init__.py:**\n\n```python\nimport os\nfrom typing import Any, Dict\nfrom . import default, alternative\n\ndef get_prompts(prompt_type: str = None) -> Dict[str, Any]:\n    # Use the provided prompt_type, or fall back to the environment variable if not provided.\n    style = (prompt_type if prompt_type is not None else os.getenv(\"PROMPTS_STYLE\", \"default\")).lower()\n    \n    if style == \"alternative\":\n        return alternative.PROMPTS\n    return default.PROMPTS\n\nGRAPH_FIELD_SEP = default.GRAPH_FIELD_SEP  # You can also decide based on style if needed.\n```\n\nNow you can simply import from the package:\n\n```python\nfrom prompts import GRAPH_FIELD_SEP, get_prompts\n\n# Using default (either through env or as fallback)\nPROMPTS = get_prompts()\n\n# Explicitly using the alternative configuration\nPROMPTS_alt = get_prompts(prompt_type=\"alternative\")\n```\n\nThis way depending on your use-case you can opt for a specific prompt.py file with desciptive prompts.\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-04-11T08:11:23Z",
      "updated_at": "2025-05-23T10:05:19Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "Core",
        "discuss"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1353/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "danielaskdd"
      ],
      "milestone": "v1.3.8",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1353",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1353",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:11.268780",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Multiple prompt templates also applied during the document indexing stage. Do we have a comprehensive and user-friendly solution that allows users to select different templates during both indexing and query phases?",
          "created_at": "2025-04-12T00:30:14Z"
        },
        {
          "author": "danielaskdd",
          "body": "**Proposal: Three Methods for Prompt Template Selection**  \n\nI recommend implementing the following three approaches to select prompt templates by name:  \n\n1. **Environment Variable**: Configure via `DEFAULT_PROMPT_TEMPLATE`.  \n2. **API Endpoint**: Expose an API to allow users to dynamically switch ",
          "created_at": "2025-04-12T08:31:46Z"
        },
        {
          "author": "danielaskdd",
          "body": "### Multiple Sets of Prompt Templates Requirement\n*   **Template Types**: Entity-relationship extraction templates, query templates, entity-relationship merging templates\n\n*   **Template Versions**: Each template type has multiple versions. Users can switch between different versions and set the cur",
          "created_at": "2025-05-22T01:35:52Z"
        },
        {
          "author": "jerrywang121",
          "body": "Hi @danielaskdd \nThanks for pointed me to this feature change, which is really welcomed/wanted\none question, is API / WebUI support will be included? managing and switching between multiple set of templates could be tedious for end-user - the reason why I started with the same thoughts but ended up ",
          "created_at": "2025-05-23T08:36:47Z"
        },
        {
          "author": "danielaskdd",
          "body": "Certainly! The API should, at a minimum, support the following functionalities:  \n1. Temporarily switch the query prompt version using query parameters.  \n2. Provide a separate API to list template types and their versions, and to set the default version for each template type.\n\nDo you have any sugg",
          "created_at": "2025-05-23T10:05:18Z"
        }
      ]
    },
    {
      "issue_number": 1600,
      "title": "[Bug]: Error while deleting document 'PGVectorStorage' object has no attribute 'client_storage'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nError Message:\n`Error while deleting document 43619e35-0ad6-4d16-b9fc-0bb72d810790: 'PGVectorStorage' object has no attribute 'client_storage'\n`\n\n![Image](https://github.com/user-attachments/assets/e7338f37-0ecc-41e0-96b5-9f9ab9ab32d9)\n\nThe possible code related to the issue:  \nlightrag.py\n ``\n\n![Image](https://github.com/user-attachments/assets/7e9cedf6-a53b-4e0a-9e18-0e057c275e6c)\n\n![Image](https://github.com/user-attachments/assets/2dbaf494-d512-4a14-a660-ddf59bda9d33)\n\n### Steps to reproduce\n\nWhere:  I invoke adelete_by_doc_id() in lightrag.py to delete a specific document.\nEnvironment:   Python 3.11.10,   RedisKVStorage , PGDocStatusStorage, PGVectorStorage, Neo4JStorage\n\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "bladestone01",
      "author_type": "User",
      "created_at": "2025-05-20T08:27:21Z",
      "updated_at": "2025-05-23T09:50:08Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1600/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1600",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1600",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:13.352614",
      "comments": [
        {
          "author": "bladestone01",
          "body": "I am little confused where 'client_storage' is in the PGVectorStorage. It seems to be in nano and faiss.",
          "created_at": "2025-05-20T08:34:45Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "> I am little confused where 'client_storage' is in the PGVectorStorage. It seems to be in nano and faiss.\n\nI’ve created a new pull request  https://github.com/HKUDS/LightRAG/pull/1554 — please review it. It’s designed to work with both FAISS and the default local storage. But delete will have issue",
          "created_at": "2025-05-23T09:47:01Z"
        }
      ]
    },
    {
      "issue_number": 1613,
      "title": "[Bug]:在使用 adelete_by_doc_id 方法删除文档时，报'RedisKVStorage' object has no attribute 'get_all'。",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/f2899542-03bf-46db-9396-79b54784b50b)\n调试代码发现  'RedisKVStorage' object has no attribute 'get_all'\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Sindcs",
      "author_type": "User",
      "created_at": "2025-05-22T07:03:26Z",
      "updated_at": "2025-05-23T09:45:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1613/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1613",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1613",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:13.573235",
      "comments": [
        {
          "author": "FeHuynhVI",
          "body": "Deleting data will lead to phantom issues, making the project no longer viable for production use.",
          "created_at": "2025-05-23T09:45:19Z"
        }
      ]
    },
    {
      "issue_number": 1621,
      "title": "Is it an idea to reduce the complexity of the main entity extraction prompt to allow for \"possible\" higher entity extraction quality for bigger and smaller models?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThe main entity extraction prompt now looks at the chunk and extracts entities with descriptions and numbers and then as well relationships between those entities also with weights etc. \n\nThen it needs to formulate everything in the right format for the extraction process. And if the chunks language doesn't match the language of the prompt it might start listing entities like: {Entity: Hombres, desc: Spanish word for men} instead of what the word implies in the context. \n\nFor the language problem there are two approaches. One is to have the prompt instructions in exactly the same language as the incoming file. Or to do a translation step, but this might cause the AI to miss information. \n\nThen to reduce complexity it might be better to first let an AI extract entities and relationships in plain text with weight and all that is needed and in the final AI call make an AI formulate it according to the delimiters. \n\nThis is just an idea and I haven't validated it yet, but usually this is how you make a smaller AI perform on the same level as a LLM. By reducing the needed intelligence for the task.\n\nLet me know what you think of this idea. It will definitely result in more calls and more tokens, but I think that it definitely helps AIs. \n\nCurrently, I have mostly been testing with \"gpt-4.1-mini\".\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-05-23T09:07:57Z",
      "updated_at": "2025-05-23T09:07:57Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1621/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1621",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1621",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:13.760312",
      "comments": []
    },
    {
      "issue_number": 1620,
      "title": "[Question]:Is there a mistake, swapping the low-level keywords with the high-level keywords?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nAccording to my understanding from the papers and documents, low-level keywords are directly derived from the query, representing detailed content, while high-level keywords extract abstract concepts, indicating broader connections. However, when I looked at the prompt example, it seemed to be the opposite. I want to confirm whether it's a mistake or if my understanding is incorrect?\n\nhere is the prompt examples:\n\n\n\n> \"\"\"Example 1:\n> \n> Query: \"How does international trade influence global economic stability?\"\n> ################\n> Output:\n> {\n>   \"high_level_keywords\": [\"International trade\", \"Global economic stability\", \"Economic impact\"],\n>   \"low_level_keywords\": [\"Trade agreements\", \"Tariffs\", \"Currency exchange\", \"Imports\", \"Exports\"]\n> }\n> #############################\"\"\",\n>     \"\"\"Example 2:\n> \n> Query: \"What are the environmental consequences of deforestation on biodiversity?\"\n> ################\n> Output:\n> {\n>   \"high_level_keywords\": [\"Environmental consequences\", \"Deforestation\", \"Biodiversity loss\"],\n>   \"low_level_keywords\": [\"Species extinction\", \"Habitat destruction\", \"Carbon emissions\", \"Rainforest\", \"Ecosystem\"]\n> }\n> #############################\"\"\",\n>     \"\"\"Example 3:\n> \n> Query: \"What is the role of education in reducing poverty?\"\n> ################\n> Output:\n> {\n>   \"high_level_keywords\": [\"Education\", \"Poverty reduction\", \"Socioeconomic development\"],\n>   \"low_level_keywords\": [\"School access\", \"Literacy rates\", \"Job training\", \"Income inequality\"]\n> }\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "TonicZhang",
      "author_type": "User",
      "created_at": "2025-05-23T08:05:35Z",
      "updated_at": "2025-05-23T08:05:35Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1620/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1620",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1620",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:13.760346",
      "comments": []
    },
    {
      "issue_number": 1603,
      "title": "[Question]:是否支持k8s部署？",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n请教LightRAG能否用k8s部署？或者有生产落地的案例或者经验吗？\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "farwent",
      "author_type": "User",
      "created_at": "2025-05-20T15:36:37Z",
      "updated_at": "2025-05-22T11:14:48Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1603/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1603",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1603",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:13.760359",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "应该是可以的。LightRAG Server提供了官方的docker镜像，因此你可以把它放到K8S的服务编排文件中。\n```\ndocker pull ghcr.io/hkuds/lightrag:latest\n```",
          "created_at": "2025-05-21T13:22:48Z"
        },
        {
          "author": "earayu",
          "body": "我正好提交了1个PR，支持用helm部署LightRAG：https://github.com/HKUDS/LightRAG/pull/1602\n你还可以参考PR中提供的视频：[YouTube](https://www.youtube.com/watch?v=JW1z7fzeKTw) 、 [Bilibili](https://www.bilibili.com/video/BV1bUJazBEq2)",
          "created_at": "2025-05-21T17:20:03Z"
        },
        {
          "author": "danielaskdd",
          "body": "非常感谢您的分享",
          "created_at": "2025-05-22T04:38:23Z"
        },
        {
          "author": "farwent",
          "body": "非常感谢",
          "created_at": "2025-05-22T11:14:37Z"
        }
      ]
    },
    {
      "issue_number": 854,
      "title": "What's the correct approach for setup Graph UI for lightrag",
      "body": "I'm following this guide for setup:\nhttps://github.com/HKUDS/LightRAG/blob/main/lightrag_webui/README.md\n\n1. Done all the steps in the page and \"bun run dev\"\n2. Open http://localhost:5173\n3. Get the 'Backend Health Check Error'\n\nI assume there should be a LightRag server running, so I tried setup using the below command:\n```\npython -m lightrag.api.lightrag_server \\\n  --llm-binding openai \\\n  --llm-binding-host https://api.openai.com/v1 \\\n  --llm-binding-API-key <my_api_key> \\\n  --llm-model gpt-4\n```\nI've neo4j docker running already, and configured NEO4J_URI, username and pwd in .env.example for in project root path.\nBut still get the following error:\n`neo4j.exceptions.ConfigurationError: URI scheme b'' is not supported. Supported URI schemes are ['bolt', 'bolt+ssc', 'bolt+s', 'neo4j', 'neo4j+ssc', 'neo4j+s']. Examples: bolt://host[:port] or neo4j://host[:port][?routing_context]`\n\nSo am I did the right steps or anything else I miss? Is there a guide on how a user can start this server? Could you add this to the Graph UI guide? I believe this is a mandatory prerequisite before running the frontend webui, thanks",
      "state": "open",
      "author": "Johnny987",
      "author_type": "User",
      "created_at": "2025-02-19T08:24:25Z",
      "updated_at": "2025-05-22T03:23:24Z",
      "closed_at": null,
      "labels": [
        "neo4j"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/854/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/854",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/854",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:13.964226",
      "comments": [
        {
          "author": "YanSte",
          "body": "NOTE: Neo4j we have couple of issue. We are looking for someone have a look.",
          "created_at": "2025-02-19T09:12:28Z"
        },
        {
          "author": "Johnny987",
          "body": "> NOTE: Neo4j we have couple of issue. We are looking for someone have a look.\n\nIs Neo4j the must-have KG storage if I want to use the WebGUI, is there an alternative solution?",
          "created_at": "2025-02-19T09:18:59Z"
        },
        {
          "author": "ParisNeo",
          "body": "I think for now the only one that works for showing the graph is MongiDB and Neo4j, all other implementations (json/postgresql etc) are under construction and most of the methods that are required by the webui are not implemented yet (but they will all be implemented).\n\nTo make the webui work, you c",
          "created_at": "2025-02-19T09:21:13Z"
        },
        {
          "author": "Johnny987",
          "body": "> I think for now the only one that works for showing the graph is MongiDB and Neo4j, all other implementations (json/postgresql etc) are under construction and most of the methods that are required by the webui are not implemented yet (but they will all be implemented).\n> \n> To make the webui work,",
          "created_at": "2025-02-20T02:47:45Z"
        },
        {
          "author": "ParisNeo",
          "body": "Can you be more specific? There are multiple possible errors. Just show the log if possible please.",
          "created_at": "2025-02-20T10:47:37Z"
        }
      ]
    },
    {
      "issue_number": 884,
      "title": "[Bug]: PostgreSQL database and Doc (asyncpg.exceptions.CharacterNotInRepertoireError: invalid byte sequence for encoding \"UTF8\": 0x00)",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n- Try to upload a document \n- And got with posgres `asyncpg.exceptions.CharacterNotInRepertoireError: invalid byte sequence for encoding \"UTF8\": 0x00`\n- This document stay in pending and always fail.\n\n\n### Steps to reproduce\n\n- Upload a document (https://arxiv.org/html/2407.05750v1)\n- By the LightRAG server\n\n### Expected Behavior\n\n- Upload the document and process it\n\n### LightRAG Config Used\n\n- Master LightRag\n\n\n### Logs and screenshots\n\n```\n2025-02-19 22:18:54 ERROR:PostgreSQL database,\n2025-02-19 22:18:54 sql:insert into LIGHTRAG_DOC_STATUS(workspace,id,content,content_summary,content_length,chunks_count,status)\n2025-02-19 22:18:54                  values($1,$2,$3,$4,$5,$6,$7)\n2025-02-19 22:18:54                   on conflict(id,workspace) do update set\n2025-02-19 22:18:54                   content = EXCLUDED.content,\n2025-02-19 22:18:54                   content_summary = EXCLUDED.content_summary,\n2025-02-19 22:18:54                   content_length = EXCLUDED.content_length,\n2025-02-19 22:18:54                   chunks_count = EXCLUDED.chunks_count,\n2025-02-19 22:18:54                   status = EXCLUDED.status,\n2025-02-19 22:18:54                   updated_at = CURRENT_TIMESTAMP,\n2025-02-19 22:18:54 data:{'workspace': 'default', 'id': 'doc-8774e5c30ba9c1e89d4e739015e07afc', 'content': 'Published as a conference paper at ICLR 2024\\nHYBRID LLM: C OST-EFFICIENT AND QUALITY -\\nAWARE QUERY ROUTING\\nDujian Ding1∗, Ankur Mallick2, Chi Wang2, Robert Sim2, Subhabrata Mukherjee3†,\\nVictor Ruhle2, Laks V . S. Lakshmanan1, Ahmed Awadallah2\\n1University of British Columbia2Microsoft3Hippocratic AI\\ndujian.ding@gmail.com ankurmallick@microsoft.com\\nABSTRACT\\nLarge language models (LLMs) excel in most NLP tasks but also require expen-\\nsive cloud servers for deployment due to their size, while smaller models that can\\nbe deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of re-\\nsponse quality. Therefore in this work we propose a hybrid inference approach\\nwhich combines their respective strengths to save cost and maintain quality. Our\\napproach uses a router that assigns queries to the small or large model based on\\nthe predicted query difficulty and the desired quality level. The desired quality\\nlevel can be tuned dynamically at test time to seamlessly trade quality for cost as\\nper the scenario requirements. In experiments our approach allows us to make up\\nto 40% fewer calls to the large model, with no drop in response quality.\\n1 I NTRODUCTION\\nLarge language models (LLMs) have become the dominant force in natural language processing\\nin recent years (Zhao et al., 2023). Their impact has been especially striking in generative ap-\\nplications where it has extended beyond standard language understanding and question-answering\\nbenchmarks like (Hendrycks et al., 2020; Srivastava et al., 2022) to several successful real-world\\ndeployments. These include the wildly popular ChatGPT (OpenAI, b) and several other chatbots\\n(Zheng et al., 2023) powered by different LLMs (Taori et al., 2023; Touvron et al., 2023; OpenAI,\\n2023), which allow users to engage in natural language conversations and obtain informative re-\\nsponses on a range of practically useful tasks like creative writing, translation, code completion,\\netc. An important added attraction of these models is their accessibility. Users can input queries\\nand receive responses in natural language, without any specialized data or code, and this is what has\\ncreated such a widespread demand for their services across regions, professions, and disciplines.\\nThe best performing LLMs are based on the transformer architecture of (Vaswani et al., 2017) and\\ngenerally have tens of billions of parameters. E.g., Alpaca (Taori et al., 2023) has 13 billion param-\\neters, the best version of Llama-2 (Touvron et al., 2023) has 70 billion parameters, while OpenAI’s\\nGPT-3.5 (OpenAI, a) and GPT-4 (OpenAI, 2023) are rumored to be much larger. Their enormous\\nsize and the autoregressive nature of text generation in their transformer architectures means that\\nthese models typically have a high compute and memory requirement that can only be met by ex-\\npensive cloud servers (Yu et al., 2022). This can potentially impose an enormous cost on developers\\nand users as more LLM-based services are introduced. In response to this there has been a surge of\\ninterest in designing smaller, cost-effective LLMs – e.g., (Touvron et al., 2023) provides multiple\\nversions of Llama-2, with the smallest having only 7 billion parameters, small enough to run on\\na laptop1, while the smallest offering of Google’s Palm-2 model can even run on mobile devices2.\\nHowever empirical evaluations in (Chung et al., 2022; Touvron et al., 2023) as well as our own\\nevaluation in Figure 1a show that smaller models generally lag behind in terms of response quality.\\n∗work performed during internship at Microsoft\\n†work performed while at Microsoft\\n1https://github.com/microsoft/Llama-2-Onnx\\n2https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\\n1\\nPublished as a conference paper at ICLR 2024\\n(a) Accuracy v/s size of LLM\\n (b) Tail of accuracy difference\\n (c) Results with routing\\nFigure 1: We use a dataset of natural language queries from a range of tasks like question answering,\\nsummarization, information extraction, etc. (See Section 4 for details). We observe that (a) smaller\\nmodels generally give poorer response quality or lower BART score (Yuan et al., 2021), (b) Llama-2\\n(13b) outperforms GPT-3.5-turbo on around 20% examples, and (c) our router can make 22% fewer\\ncalls to GPT-3.5-turbo (cost advantage) with 1%drop in response quality (BART score).\\nFaced with this tradeoff between response quality and inference cost, we propose a hybrid inference\\napproach which provides the best of both worlds. Our approach is motivated by the observation\\nthat most tasks for which LLMs are useful, like creative writing, translation, code completion, etc.,\\ninclude a range of queries of different difficulty levels and there is always a subset of “easy” queries\\nfor which responses of a small (inexpensive and weak) model may be comparable to, and sometimes\\neven better than those of a large (expensive and powerful) model. This is also illustrated in Figure 1b\\nwhere we plot the tail of the quality gap (defined in Section 3) between the 13 billion parameter\\nversion of Llama-2 and OpenAI’s GPT-3.5-turbo, the model that powers ChatGPT. Quality gap is\\nnon-negative for examples where the response quality of Llama-2 is comparable to or better than that\\nof GPT-3.5-turbo which is the case for around 20% queries in our dataset (described in Section 4).\\nWe leverage this insight to train a router that takes a large model and a small model as input, and\\nlearns to identify these easy queries as a function of the desired level of response quality, while\\ntaking into account the generative nature of tasks, inherent randomness in LLM responses, and\\nresponse quality disparity between the two models. At test time, the router seamlessly adjusts to\\ndifferent response quality requirements and assigns the corresponding “easy” queries to the small\\nmodel, leading to significant inference cost reduction with minimal drop in response quality. In\\nFigure 1c our router assigns 22% of queries to Llama-2 (13b)3with less than 1%drop in response\\nquality measured in BART scores (Yuan et al., 2021). The gains are even higher for pairs where the\\nsmall model is closer in terms of response quality to the large model (see Section 4).\\nWith the explosion in the complexity and costs of LLM deployments, small companies and individ-\\nual consumers, have started to rely on the pre-existing LLMs hosted on platforms like HuggingFace\\n(HuggingFace) and OpenAI (OpenAI, c). In this context, our hybrid inference approach can reduce\\nthe costs incurred by both consumers and platform owners because a) consumers can use it to route\\neasy queries to small models hosted on their edge devices (laptops/smartphones) and only call the\\nAPI for the more complex queries (illustrated in Figure 2) and b) platform owners can automatically\\nroute queries to lower cost models at the backend without affecting the user experience, as long as\\nthe response quality levels are maintained. Thus our hybrid inference approach offers a flexible and\\ncost-effective solution for harnessing the full potential of LLMs while accommodating diverse cost\\nbudgets and quality requirements.\\nThe main technical contributions of this work are: a) we are the first to explore cost-effective and\\nquality-aware hybrid LLM inference, b) we design a novel query router which routes queries based\\non an estimate of the response quality gap between models (Section 3.1), c) we incorporate un-\\ncertainty due to randomness in LLM responses in our router design to improve performance (Sec-\\ntion 3.2), d) we identify challenges for our router when the small model is significantly weaker than\\nthe large model and introduce a novel data transformation to address this issue (Section 3.3), and\\ne) we provide extensive experimental results (Section 4) on a large benchmark dataset of real world\\nnatural language queries and responses (Jiang et al., 2023) thereby demonstrating the value of the\\n3We term the fraction of queries routed to the small model as the cost advantage (see §2.3)\\n2\\nPublished as a conference paper at ICLR 2024\\nFigure 2: Routing between edge and cloud.\\napproach and its superiority over baseline approaches, enabling LLM providers and consumers to\\ncost-efficiently enable LLM-backed experiences.\\n2 P ROBLEM FORMULATION\\n2.1 R ELATED WORK\\nLarge Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the\\nstudy of natural language processing (NLP), computer vision, information retrieval, and other do-\\nmains(Menghani, 2023; Chen et al., 2023; Jiang et al., 2023). The impressive effectiveness and gen-\\neralizability of LLMs has come at the price of a drastic increase in LLM sizes (Treviso et al., 2023)\\nand consequent challenges, including huge amounts of computational resources and data required\\nto train, and prohibitive expenses at both training and deployment stages (Bender et al., 2021).\\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation\\nmodels (Bommasani et al., 2021) – models that are trained once and can then be used to serve a wide\\nvariety of tasks. As such, we expect inference cost to dominate the overall cost of such models and\\nhence focus on works that reduce the cost of ML inference (Menghani, 2023). Common techniques\\nfor efficient ML inference include model pruning (LeCun et al., 1989), quantization (Jacob et al.,\\n2018), knowledge distillation (Hinton et al., 2015), and Neural Architecture Search (Elsken et al.,\\n2019). Such static efficiency optimizations typically produce a fixed model with lower inference\\ncost and lower accuracy compared to the large model which may not suffice for foundation models\\nlike LLMs, whose core premise is that the same model will serve a range of tasks, each with its own\\naccuracy/cost constraints. This is already manifesting in inference platforms described in Section 1\\nwhich need more dynamic optimizations to meet the demands of all users.\\nHybrid ML Inference. Recent works (Kag et al., 2022; Ding et al., 2022) have introduced a new\\ninference paradigm called hybrid inference which uses two models of different sizes instead of a\\nsingle model for inference. The smaller model (e.g. Llama-2 (Touvron et al., 2023)) generally has\\nlower inference cost but also lower accuracy than the larger model (e.g. GPT-4 (OpenAI, 2023)). The\\nkey idea is to identify and route easy queries to the small model so that inference cost can be reduced\\nwhile maintaining response quality. By tuning a threshold on query difficulty we can dynamically\\ntrade off quality and cost for the same inference setup. (Kag et al., 2022) study this setup for image\\nclassification and propose to train the small model, large model, and router from scratch. However\\nLLM training is expensive and retraining LLMs from scratch for every scenario goes against the\\nvery premise of inference with pre-trained foundation models. Moreover text generation (Iqbal &\\nQureshi, 2022) is often more ambiguous and challenging than image classification due to which\\nnovel techniques are required for effective hybrid LLM inference for text generation.\\nInference with Multiple LLMs. Some recent works use multiple LLMs for inference but these\\napproaches typically call more than one LLM for a single query that can incur significant compu-\\n3\\nPublished as a conference paper at ICLR 2024\\ntational overheads. Specifically (Jiang et al., 2023) calls an ensemble of LLMs at inference time\\ndue to which the inference cost will be proportional to the number of models in the system. (Chen\\net al., 2023) performs inference by sequentially calling LLMs until one has a high confidence score\\nexceeding the predefined threshold. Our work provides high quality responses while always call-\\ning a single LLM for all queries and will thus incur much lower costs than both of these works\\non average. Speculative decoding (Leviathan et al., 2023; Kim et al., 2023) speeds up decoding of\\nexpensive LLMs by invoking small decoders on the “easy” decoding steps. Instead, in our work we\\nare interested in query routing which assigns “easy” queries to small models to reduce overall infer-\\nence costs while maintaining high performance. While these two approaches have different goals,\\nan interesting line of future work would be to combine these to achieve further cost reduction.\\n2.2 P ROBLEM SETTING\\nWe extend the hybrid ML paradigm to LLM inference by routing queries between two models with\\ndifferent inference costs and accuracy. We use XandZto denote the input query space and the set of\\nall possible output responses respectively. Let L:X → Z denote the large model and S:X → Z\\ndenote the small model. Formally, the objective in our paradigm is to learn a router r:X → { 0,1}\\nsuch that each user query x∈ X is routed to the small model S(x)ifr(x) = 0 , and to the large\\nmodel L(x), otherwise. Note that we always route each query to a single LLM at inference time as\\nopposed to using an ensemble (Jiang et al., 2023) or a cascade (Chen et al., 2023) of LLMs, which\\nmay call multiple LLMs to resolve a single query and incur significant computational overheads.\\n2.3 E VALUATION METRIC\\nResponse Quality Automatic evaluation for text generation is a challenging and widely studied\\nproblem. Traditional metrics, such as BLEU and ROUGE, initially designed for machine translation\\nand summarization, have been found to be of limited concordance with human judgment and re-\\nstricted applicability across diverse NLP tasks (Blagec et al., 2022). Significant research efforts have\\nbeen devoted to implementing task-agnostic evaluation metrics with neural networks. GPT-ranking\\n(Jiang et al., 2023), as a representative example, employs GPT models (e.g., GPT-4 (OpenAI, 2023))\\nto provide relative rankings between pairs of generated outputs. In spite of the high correlation with\\nhuman perception, GPT-ranking suffers from high computational costs and inability to distinguish\\nbetween examples with the same ranking. Instead, we use the BART score (Yuan et al., 2021) to\\nevaluate response quality of different models since (1) it is inexpensive to compute in comparison to\\nLLM-based metrics such as GPT-ranking, and (2) it has been shown in prior work (Jiang et al., 2023)\\nthat this metric correlates well with the ground truth. We also provide a case study in Appendix C.2\\nto empirically justify using BART score as the response quality metric. We use q(z), q:Z →Rto\\ndenote the BART score (response quality) of model responses z∈ Z.\\nCost Advantage The absolute costs of running a model may not be known a priori , and may be\\nexpressed using a variety of metrics, including latency, FLOPs, energy consumption, etc. In LLM\\ninference, however, each of these metrics is affected by several underlying confounders such as\\ndifferent prompt templates, hardware capability, network connectivity, etc. Moreover different plat-\\nforms/users may be interested in different metrics. However the common underlying assumption\\nin this and previous works on efficient ML inference is that smaller models are more efficient than\\nlarger models and therefore we expect to obtain an improvement in all the metrics by routing more\\nqueries to the smaller model. Hence we define cost advantage as the percentage of queries routed to\\nthe smaller model. Note that the notion cost advantage has been used as a generic efficiency metric\\nin previous hybrid ML work (Kag et al., 2022), where it is termed as coverage .\\n3 H YBRID LLM I NFERENCE\\nEasy Queries. We refer to queries for which the response quality of the small model is close to the\\nresponse quality of the large model as “easy” queries. The goal of our hybrid inference framework is\\nto identify the easy queries and route them to the small model thereby ensuring significant inference\\ncost reduction without much drop in response quality. Note that the easy queries as defined here,\\nneed not necessarily be queries that are easy/inexpensive to respond to, they are just queries for\\n4\\nPublished as a conference paper at ICLR 2024\\nwhich the small model can match up to the large model. Examples of easy and hard queries as per\\nthis definition a2025-02-19T21:18:54.080541709Z re provided in Appendix C.1.\\nQuality Gap. We define quality gap of a query xasH(x) :=q(S(x))−q(L(x))i.e. the difference\\nin quality of the small model’s response S(x)and the large model’s response L(x). The quality\\ngap is a random variable since LLM responses are typically non-deterministic. This is illustrated\\nin Figure 3 below where the blue and orange plots correspond to the distribution of responses from\\nFLAN-t5 (800m)4(Chung et al., 2022) and Llama-2 (13b) (Touvron et al., 2023) for a single query.\\nFigure 3: Response quality distribu-\\ntion for FLAN-t5 (800m) and Llama-\\n2 (13b) on the query “How to iden-\\ntify the index of median?” measured\\nin BART scores. Llama-2 (13b) with\\ntransformation significantly overlaps\\nwith FLAN-t5 (800m).Proposed Orchestration Framework. Queries are routed\\nusing a BERT-style encoder model (e.g., DeBERTa (He\\net al., 2020)) which is trained on a dataset of representa-\\ntive queries and learns to predict a score. Since the router\\nis an encoder model, a single pass of the query through it\\nis sufficient to generate the score and we assume that the\\ncost of this step is negligible compared to the cost of run-\\nning autoregressive decoding using the large model L(x)\\n(Sun et al., 2019). Thus, we expect that using the router\\nto route queries to the small model will not detract signifi-\\ncantly from the realizable cost advantage.\\nRouter Score. We design the router score to be large for\\neasy queries as defined above. Intuitively, an estimate of\\nPr[H(x)≥0]is a suitable candidate since a large value of\\nPr[H(x)≥0] = Pr[ q(S(x))≥q(L(x))]corresponds to\\nqueries for which there is a high likelihood that the response\\nquality of the small model will be at least as high as that of\\nthe large model. However we show below that in scenarios\\nwhere the large model is significantly more powerful than\\nthe small model i.e. q(S(x))<< q (L(x))in general, one\\ncan train more effective routers by relaxing the definition of\\neasy queries to Pr[H(x)≥ −t] = Pr[ q(S(x))≥q(L(x))−t]for an appropriate t >0. At test\\ntime we achieve the desired performance accuracy tradeoff by tuning a threshold on the score and\\nrouting queries with score above the threshold to the small model. For a router with parameters w,\\nwe denote router score by pw(x), pw:X → [0,1]. We discuss different router score designs in the\\nrest of this section assuming a training set of Nqueries x1, . . . , x N.\\n3.1 D ETERMINISTIC ROUTER\\nPrevious work on hybrid ML (Ding et al., 2022; Kag et al., 2022) assumes that neural models are\\ndeterministic functions mapping input features to a single point in the output space. To realize\\nthis for LLMs, we sample a single response per query from each model. We assign boolean labels\\nydet\\ni= 1[q(S(xi))≥q(L(xi))]to each training query xiwith the BART score as the quality function\\nq(.). The router is trained by minimizing the binary cross-entropy loss (Ruby & Yendapalli, 2020).\\nL(w) =−1\\nNNX\\ni=1\\x10\\nydet\\nilog(pw(xi)) + (1 −ydet\\ni) log(1 −pw(xi))\\x11\\n(1)\\nObserve that the assigned labels ydet\\nican be viewed as an estimate for Pr[H(xi)≥0]given a single\\nresponse per query from each model and thus minimizing the above loss encourages the router score\\npw(x)to be close to Pr[H(x)≥0]for test queries. We refer to this deterministic router as rdet.\\n3.2 P ROBABILISTIC ROUTER\\nThe determinism assumption can be justified for tasks where the ground truth labels are often explicit\\nand unique such as image classification (Masana et al., 2022) and video segmentation (Yao et al.,\\n2020). When it comes to NLP tasks, however, there is usually no single best answer due to the\\nintrinsic ambiguity and complexity of natural languages. LLMs are widely used as non-deterministic\\ngenerators to capture the intrinsic uncertainty of NLP tasks, as shown in Figure 3 (ignore the dashed\\n4We use the FLAN-t5-large model from https://huggingface.co/google/flan-t5-large.\\n5\\nPublished as a conference paper at ICLR 2024\\n(a) Before transformation.\\n (b) Grid search for the best t.\\n (c) After transformation.\\nFigure 4: Effect of data transformation on labels for training the router.\\ncurve for now). The non-determinism mainly comes from the randomness in the decoding phase.\\nUsers typically control the level of uncertainty by choosing different decoding strategies such as\\nnucleus sampling (Holtzman et al., 2019), as well as the values of the hyper-parameter temperature .\\nIntuitively, higher temperature values result in a higher level of randomness and diversity among the\\ngenerated responses. For black-box LLM APIs such as GPT-4 (OpenAI, 2023), it has been observed\\nthat even upon setting temperature to the minimum value 0, it can still provide different responses\\nfor the same input queries. The underlying mechanism is still an open problem while a recent study\\nhints at the instability of the MoE backbone (Skyward, 2023).\\nWe propose to incorporate the uncertainty into the router training loss by relaxing the hard la-\\nbelsydet\\ni∈ {0,1}to the soft labels yprob\\ni:= Pr[ H(xi)≥0] = Pr[ q(S(xi))≥q(L(xi))] =\\nE[ 1[q(S(xi))≥q(L(xi))]]where Edenotes the expectation. In practice, we estimate expecta-\\ntion by sampling 10responses from each model and computing the sample average of the corre-\\nsponding indicator function values. Observe that the hard label ydet\\niis a higher-variance estimate\\nofE[ 1[q(S(xi))≥q(L(xi))]](since it is obtained from a single sample) and hence we expect im-\\nproved performance of the probabilistic router with the following training loss, referred to as rprob.\\nL(w) =−1\\nNNX\\ni=1\\x10\\nyprob\\nilog(pw(xi)) + (1 −yprob\\ni) log(1 −pw(xi))\\x11\\n(2)\\n3.3 P ROBABILISTIC ROUTER WITH DATA TRANSFORMATION\\nWhile so far we have designed scores that try to estimate Pr[H(x)≥0], we observe that the\\nempirical estimate of Pr[H(xi)≥0] =E[ 1[q(S(xi))≥q(L(xi))]]tends to be extremely small\\nwhen the large model is significantly more powerful than the small model (0 for almost 90% of the\\nqueries in Figure 4a with Flan-t5 (800m) as the small model and Llama-2 (13b) as the large model).\\nBecause q(S(x))<< q (L(x))for most queries in this case, it provides an extremely weak signal\\nfor training using Equation (2) and as shown in Section 4 both rdetandrprob fail to provide much\\nimprovement over random query assignment in this case.\\nTraditional approaches for learning with imbalanced data have their own shortcomings (Krawczyk,\\n2016). Moreover our goal is to only design a router that can reduce inference cost while maintaining\\nresponse quality as much as possible and so we are not tied to a particular definition of class labels\\nto achieve this. We leverage this flexibility to introduce new labels ytrans\\ni(t) := Pr[ H(xi)≥ −t] =\\nPr[q(S(xi))> q(L(xi))−t]for some t >0. Since −t <0,Pr[H(x)≥ −t]≥Pr[H(x)≥0]\\nby definition of the tail distribution and so we expect this relaxation to provide a stronger signal\\nfor router training while still allowing us to identify the easy queries i.e. those queries for which\\nq(S(x))has a high likelihood of being close to q(L(x))(q(S(x))> q(L(x))−t). Visually, this\\ncorresponds to comparing the distribution of the small model’s response with a shifted distribution\\nof the large model’s response to a query (dotted curve in Figure 3).\\nNow the question is how to choose the best relaxation t?Given that tail probability Pr[H(x)≥ −t]\\nlies in [0,1], we choose tby maximizing the average pairwise differences between the transformed\\nlabels to push them as far apart as possible and provide a strong signal for training. Thus we set,\\nt∗= arg max\\nt1\\nN2X\\n(i,i′)|ytrans\\ni(t)−ytrans\\ni′(t)| (3)\\n6\\nPublished as a conference paper at ICLR 2024\\nTable 1: Cost advantage v.s. performance drop for model pairs of different performance gaps.\\nPerformance drops are computed w.r.t. the all-at-large baseline.\\nCost\\nAdvantage\\n(%)Response Quality (BART Score) Drop w.r.t all-at-large (%)\\nS: Llama-2 (7b) S: Llama-2 (13b) S: FLAN-t5 (800m)\\nL: Llama-2 (13b) L: GPT-3.5-turbo L: Llama-2 (13b)\\nrdet rprob rtrans rdet rprob rtrans rdet rprob rtrans\\n10 0.1 -0.1 0.1 0.1 -0.1 0.2 2.3 2.2 2.1\\n20 0.1 0.0 0.0 1.0 0.8 0.8 5.8 5.8 4.7\\n40 0.2 0.1 0.0 3.5 3.4 2.9 13.8 13.1 10.3\\nWe currently solve the above optimization problem via grid-search and leave more sophisticated\\napproaches for future work. We plot the optimization objective for different values of tfor our\\ntraining dataset in Section 3.3 and show the distribution of transformed labels ytrans\\ni(t∗)in Figure 4c.\\nAs we see, the distribution is significantly more balanced now and we expect the resulting router to\\nbe much more effective. We train the router by minimizing the loss, referred to as rtrans .\\nL(w) =−1\\nNNX\\ni=1\\x00\\nytrans\\ni(t∗) log( pw(xi)) + (1 −ytrans\\ni(t∗)) log(1 −pw(xi))\\x01\\n(4)\\n4 E VALUATION\\n4.1 E VALUATION SETUP\\nDataset. We use the MixInstruct dataset from (Jiang et al., 2023) to evaluate the effectiveness of\\ndifferent routing strategies. MixInstruct consists of a wide range of tasks (e.g., question answering,\\nsummarization, information extraction) and enables us to train a generic router that will be effective\\nacross different scenarios. We present additional information about this dataset in Appendix B.\\nRouter Model. We use DeBERTa-v3-large (He et al., 2020) (300M) as the backbone to train our\\nrouters. We train each router with the corresponding loss from Section 3 for 5epochs and use\\nthe validation set to choose the best checkpoints for final evaluation. All experiments are con-\\nducted with 1NVIDIA A100 GPU of 80GB GPU RAM. We have made our source code available\\nat https://github.com/m365-core/hybrid llmrouting.\\nEvaluation Measures. We use BART score (Yuan et al., 2021) as the quality metric and use fraction\\nof queries routed to the small model ( cost advantage ) as the efficiency metric (see Section 2.3).\\nBaselines. We consider three baselines: all-at-large ,all-at-small , and random .All-at-large routes\\nall queries to the large model, while all-at-small routes all queries to the small model. Random gen-\\nerates a random number in [0,1] and selects the large model if it is below the probability threshold.\\nExperiments. We investigate all three routers: rdet,rprob, and rtrans . We select candidate model\\npairs from FLAN-T5 (800m), FLAN-T5 (11b), Llama-2 (7b), Llama-2 (13b), and GPT-3.5-turbo\\nfor our experiments. At test time the trained router ( rdet,rprob, orrtrans ) takes a threshold value as\\ninput and routes all queries with router score higher than the threshold to the small model as these\\nare the easy queries. We evaluate the router performance in Section 4.2 in terms of both BART\\nscore and cost advantage (Figure 5 and Table 1), validate that the router is indeed routing easy\\nqueries to the small model in Section 4.3, demonstrate that our routers are of negligible compute\\noverhead in Appendix A.1, show how to choose routing thresholds in practice in Appendix A.2,\\nevaluate the effectiveness of our routers using a response quality metric other than the BART score\\nin Appendix A.3, and test the generalizability of routers across model pairs in Appendix A.4.\\n4.2 R OUTER PERFORMANCE RESULTS\\nSmall performance gap. LLMs of the same architectures are observed to be of small performance\\ngap such as Llama-2 (7b) v.s. Llama-2 (13b), as seen in Figure 5a. In this case, by trading little to\\n7\\nPublished as a conference paper at ICLR 2024\\n(a) Small performance gap\\n (b) Medium performance gap\\n (c) Large performance gap\\nFigure 5: Error-cost tradeoffs achieved by rdet,rprob, andrtrans for different performance gaps.\\nno performance drop, we show that (1) the deterministic router rdetcan achieve good cost advan-\\ntages, (2) rprob consistently improves rdet, and (3) rtrans is able to match or slightly improve the\\nperformance of rprob. Numerical comparison results are summarized in Table 1. rdetroutes 20%\\n(40%) queries to the small model i.e. Llama-2 (7b) with only 0.1%(0.2%) drop in response quality\\nw.r.t. the all-at-large baseline. Impressively rprobandrtrans achieve 20% cost advantages without\\nany quality drop, and rtrans is able to achieve even 20% cost advantage without quality drop, which\\ncan be attributed to these methods capturing the non-deterministic nature of LLMs.\\nMedium performance gap. Often there is only a moderate performance gap between leading open-\\nsource LLMs like Llama-2 (13b) and state-of-the-art commodified LLMs, such as GPT-3.5-turbo\\n(Figure 5b). In this case, all our routers deliver reasonable cost advantages with acceptable quality\\ndrop. The effectiveness order between rdet,rprob, andrtrans resembles that in the small quality gap\\ncase. All routers achieve 20% (40%) cost advantage with ≤1%(≤4%) quality drop (Table 1). In\\nthe40% cost advantage regime, rprobslightly outperforms rdetandrtrans improves rprobby0.5%\\nin terms of quality drop.\\nLarge performance gap. In the edge-cloud routing scenarios, edge devices often have very limited\\nresources and can only support small models of limited quality, which can be significantly outper-\\nformed by large models deployed on the cloud. We investigate how to effectively route queries with\\nLLM pairs of large performance gaps, such as FLAN-t5 (800m) and Llama-2 (13b) (Figure 5c).\\nNon-trivial routing is challenging in this situation since the large model dominates for a majority\\nof examples. Both rdetandrprob perform marginally better than the random routing baseline. In\\ncontrast, rtrans can still effectively distinguish relatively easy queries from the harder ones. rtrans\\nachieves 40% cost advantages with 10.3%quality drop, which is 3.5%and2.8%lower than rdet\\nandrprobrespectively (Table 1).\\nIn the course of these experiments we made two interesting observations. Firstly, when the cost\\nadvantage is modest (e.g., 10%) and the LLM performance gaps are not large (e.g., Llama-2 (7b) v.s.\\nLlama-2 (13b)) rprobis able to achieve even better performance than all-at-large which leads to the\\n“negative quality drops” in Table 1. This is because, as seen from the large value of Pr[H(x)≥0]\\nin the tail distribution in Figure 5, the response quality of the small model may be higher than that\\nof the large model for several queries and by routing these queries to the small model, the router is\\nable to even beat all-at-large . Secondly, for lower cost advantages ( ≤10%) and small or moderate\\nLLM performance gaps rtrans can be slightly outperformed by rdetorrprob. This might be due\\nnoise in the estimation of the relaxation parameter tfrom sample averages instead of expectation in\\nEquation (3) and from the grid search process leading to suboptimal settings of rtrans . However we\\nclearly see that in more challenging routing scenarios with high cost advantage targets or large LLM\\n8\\nPublished as a conference paper at ICLR 2024\\n(a) Small performance diff.\\n (b) Medium quality gaps.\\n (c) Large quality gaps.\\nFigure 6: Difference between average quality gap of queries routed to the small and large models\\nwith different performance gaps.\\nperformance gaps, both rdetandrprobhave difficulty in correctly routing queries, and rtrans starts\\nto dominate due to the benefits of the data transformation.\\n4.3 R OUTER VALIDATION RESULTS\\nWe also validate that the router is functioning as intended, that is, routing easy queries to the small\\nmodel and hard queries to the large model. To see this, in Figure 6 we plot the difference between\\nthe average quality gaps of queries routed to the small model and those routed to the large model\\nfor our router and the random baseline v/s different values of cost advantages (i.e., the fraction of\\nqueries routed to the small model). Since the random baseline randomly assigns queries the average\\ndifference is nearly always zero. However our router routes easy queries i.e. queries with large\\nquality gap ( q(S(x))−q(L(x))) to the small model and queries with small quality gap to the large\\nmodel. Hence the difference between the average quality gaps always has a significant positive\\nvalue indicating that more easy queries are routed to the small model than to the large model in our\\napproach as compared to the random assignment approach at all cost advan2025-02-19T21:18:54.080541709Z tages.\\n5 D ISCUSSION AND CONCLUSION\\nMotivated by the need to optimize the trade-off between LLM inference costs and response quality,\\nwe have presented a hybrid inference approach based on quality-aware query routing. We train a\\nrouter to discriminate between “hard” and “easy” queries, enabling the LLM provider to make cost-\\nefficient decisions about which model should serve a given query. Our experimental results on a\\nvariety of state-of-the-art LLMs of varying sizes show that such an optimization is possible and that\\nwe can realize cost advantages of up to 40% with no significant drop in response quality.\\nTo the best of our knowledge, this is the first work exploring the possibilities of cost-effective and\\nquality-aware query routing between LLMs. We identify several important extensions for future\\nwork: (1) Task-aware routing . Our current routers make routing decisions purely based on query\\ninputs. To improve routing effectiveness, we can provide more informative signals which help\\nrouters distinguish easy queries from the hard ones, such as task labels for query examples and can\\nalso identify tasks which may be more suited to routing for a given pair of LLMs. (2) Generalizing\\ntoN-model routing . Modern MLaaS platforms typically host a large number of LLM instances\\nof the same or different configurations to efficiently serve users in different scenarios. This natu-\\nrally forms a more challenging routing problem with richer optimization opportunities (e.g., load\\nbalancing) (3) Out-of-distribution (OOD) generalization . In this work, the model pair and data\\ndistribution is fixed across training and testing. In the real-world it may be cumbersome/infeasible to\\ntrain a new router for every new model pair and for every new data distribution. Therefore there is a\\nneed for techniques to generalize our approach to changes in the model pair and/or data distribution\\nat test time. (4) Novel evaluation metrics . Effective evaluation metrics are critical to train high-\\nquality routers. It is intriguing to see how to develop metrics of higher human-judgment correlation\\nand to which extent it will improve the routing performance.\\n9\\nPublished as a conference paper at ICLR 2024\\nACKNOWLEDGMENTS\\nThe authors would like to thank Daniel Madrigal Diaz, Mirian del Carmen Hipolito Garcia, Chen\\nDun, Guoqing Zheng, Menglin Xia, Wen Xiao, and Jieyu Zhang for helpful discussions.\\nREFERENCES\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\\ndangers of stochastic parrots : can language models be too big? In Proceedings of the 2021\\nACM Conference on Fairness, Accountability, and Transparency , pp. 610–623, Virtual Event\\nCanada, March 2021. ACM. ISBN 978-1-4503-8309-7. doi: 10.1145/3442188.3445922. URL\\nhttps://dl.acm.org/doi/10.1145/3442188.3445922 .\\nKathrin Blagec, Georg Dorffner, Milad Moradi, Simon Ott, and Matthias Samwald. A global anal-\\nysis of metrics used for measuring performance in natural language processing. arXiv preprint\\narXiv:2204.11574 , 2022.\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\\nnities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\\nHarrison Chase. LangChain, October 2022. URL https://github.com/langchain-ai/\\nlangchain .\\nLingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while\\nreducing cost and improving performance. arXiv preprint arXiv:2305.05176 , 2023.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-\\nels.arXiv preprint arXiv:2210.11416 , 2022.\\nDujian Ding, Sihem Amer-Yahia, and Laks VS Lakshmanan. On efficient approximate queries over\\nmachine learning models. arXiv preprint arXiv:2206.02845 , 2022.\\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The\\nJournal of Machine Learning Research , 20(1):1997–2017, 2019.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\\nwith disentangled attention. arXiv preprint arXiv:2006.03654 , 2020.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.\\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv\\npreprint arXiv:1503.02531 , 2(7), 2015.\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\\ndegeneration. arXiv preprint arXiv:1904.09751 , 2019.\\nHuggingFace. Hugging face inference api. https://huggingface.co/inference-api .\\nTouseef Iqbal and Shaima Qureshi. The survey: Text generation models in deep learning. Journal\\nof King Saud University-Computer and Information Sciences , 34(6):2515–2528, 2022.\\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\\nHartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for\\nefficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pp. 2704–2713, 2018.\\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models\\nwith pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561 , 2023.\\n10\\nPublished as a conference paper at ICLR 2024\\nAnil Kag, Igor Fedorov, Aditya Gangrade, Paul Whatmough, and Venkatesh Saligrama. Efficient\\nedge inference by selective query. In The Eleventh International Conference on Learning Repre-\\nsentations , 2022.\\nSehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir\\nGholami, and Kurt Keutzer. Speculative decoding with big little decoder. In Thirty-seventh\\nConference on Neural Information Processing Systems , 2023.\\nBartosz Krawczyk. Learning from imbalanced data: open challenges and future directions. Progress\\nin Artificial Intelligence , 5(4):221–232, 2016.\\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information\\nprocessing systems , 2, 1989.\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\\ndecoding. In International Conference on Machine Learning , pp. 19274–19286. PMLR, 2023.\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg\\nevaluation using gpt-4 with better human alignment, may 2023. arXiv preprint arXiv:2303.16634 ,\\n2023.\\nMarc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost\\nVan De Weijer. Class-incremental learning: survey and performance evaluation on image clas-\\nsification. IEEE Transactions on Pattern Analysis and Machine Intelligence , 45(5):5513–5533,\\n2022.\\nGaurav Menghani. Efficient deep learning: A survey on making deep learning models smaller,\\nfaster, and better. ACM Computing Surveys , 55(12):1–37, 2023.\\nOpenAI. Openai gpt-3.5-turbo. https://platform.openai.com/docs/models/\\ngpt-3-5 , a.\\nOpenAI. Chatgpt. https://chat.openai.com/ , b.\\nOpenAI. Openai platform. https://platform.openai.com/overview , c.\\nOpenAI. Gpt-4 technical report, 2023.\\nUsha Ruby and Vamsidhar Yendapalli. Binary cross entropy with deep learning technique for image\\nclassification. Int. J. Adv. Trends Comput. Sci. Eng , 9(10), 2020.\\nLuke Skyward. Gpt-4 is non-deterministic and moe is the likely rea-\\nson why, Aug 2023. URL https://ai.plainenglish.io/\\ngpt-4-is-non-deterministic-af202847529c .\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri `a Garriga-Alonso, et al. Beyond the\\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint\\narXiv:2206.04615 , 2022.\\nZhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, and Zhihong Deng. Fast structured\\ndecoding for sequence models. Advances in Neural Information Processing Systems , 32, 2019.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\nMarcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael\\nHassid, Kenneth Heafield, Sara Hooker, Colin Raffel, et al. Efficient methods for natural language\\nprocessing: A survey. Transactions of the Association for Computational Linguistics , 11:826–\\n860, 2023.\\n11\\nPublished as a conference paper at ICLR 2024\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems , 30, 2017.\\nRui Yao, Guosheng Lin, Shixiong Xia, Jiaqi Zhao, and Yong Zhou. Video object segmentation and\\ntracking: A survey. ACM Transactions on Intelligent Systems and Technology (TIST) , 11(4):1–47,\\n2020.\\nGyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A\\ndistributed serving system for {Transformer-Based }generative models. In 16th USENIX Sympo-\\nsium on Operating Systems Design and Implementation (OSDI 22) , pp. 521–538, 2022.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing Systems , 34:27263–27277, 2021.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv\\npreprint arXiv:2303.18223 , 2023.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\\nchatbot arena. arXiv preprint arXiv:2306.05685 , 2023.\\n12\\nPublished as a conference paper at ICLR 2024\\nA A DDITIONAL EXPERIMENTS\\nA.1 R OUTER LATENCY\\nWe measure the latency of our router and compare it to the latency of the different LLMs – Flan-t5\\n(800m), Llama-2 (7b), and Llama-2 (13b) that we use in our experiments for generating responses\\nto user queries. Note that the latency of all the routers rdet,rprob, andrtrans will be the same since\\nthey use the same model (DeBERTa-v3-large (He et al., 2020)) and are just trained differently. Also,\\nwe do not measure the latency of GPT-3.5-turbo since its responses are generated by querying the\\nOpenAI API (OpenAI, c) as the model weights are not publicly available due to which it is not\\npossible to disentangle the inference latency from the network latency, queueing delay, latency of\\nthe API call, etc. However we note that the inference latency of all other LLMs we consider is\\nsignificantly larger than that of the router (see Table 2) and therefore we expect the same to hold for\\nGPT-3.5-turbo as well.\\nThe latency results are reported in Table 2 where we measure the average latency per query averaged\\nover 200 randomly chosen queries from our dataset (confidence bounds correspond to one standard\\nerror). As expected the router processes queries significantly faster than all the LLMs (nearly 10×\\nfaster than the fastest LLM – FLAN-t5(800m)). This is both due to its smaller size (300m param-\\neters) and the fact that it performs a single forward pass over the query to generate the score while\\nthe LLMs generate the response token-by-token in an autoregressive fashion due to which the infer-\\nence latency is proportional to the response length. Thus the router adds minimal overhead to the\\ninference cost due to its small size and extremely low latency.\\nTable 2: Latency Values for Different Models.\\nModel Latency (seconds)\\nRouter 0.036±0.002\\nFLAN-t5 (800m) 0.46±0.039\\nLlama-2 (7b) 7.99±0.15\\nLlama-2 (13b) 14.61±0.27\\nA.2 E MPIRICAL DETERMINATION OFROUTING THRESHOLD\\nRecall that at test time the model owner is required to set a threshold on the router score which\\nserves to separate the easy queries from the hard ones (see Section 3). All queries with router score\\nhigher than the threshold will be routed to the small model. Thus the threshold is a user-defined\\nparameter controlling the achieved efficiency-performance trade-off, to best serve the interests of\\ndifferent users. In this section we show how to empirically choose thresholds on router scores to\\nachieve cost reduction with little to no performance drops. For this, we use a small calibration set\\nto recommend default thresholds to users. We investigate all three routers rdet,rprob, and rtrans\\nwith different LLM pairs that we use in our experiments. For each LLM pair, we randomly draw\\n500samples from the validation set and use grid search to determine the threshold that delivers the\\nhighest cost advantages i.e., cost savings on the validation set while keeping the performance drop\\n(reduction in BART score) less than 1%. The limit on performance drop can be adjusted as per\\nuser requirements. With the selected thresholds, we report the achieved performance drops and cost\\nadvantages on the test sets, as summarized in Table 3.\\nAs seen from the table the performance and the cost advantage obtained on the test sets closely\\nfollows that on the validation sets for allcategories of LLM pairs. This clearly illustrates that a\\nthreshold chosen on the validation set generalizes well to the test set. We note that there is a slight\\nincrease in the performance drop from the validation to the test set for the LLama-2 (7b) and Llama-\\n2 (13b) pair, i.e the LLM pair with small performance gap as per the categorization in Section 4.\\nHowever this is also the pair with the highest cost advantage or cost savings ( >96% for all routers)\\nand thus the issue can be addressed by just using a more conservative limit on the performance drop\\nwhile choosing the threshold which would still lead to very significant cost savings.\\n13\\nPublished as a conference paper at ICLR 2024\\nTable 3: Test performance drops v.s. cost advantages achieved by thresholds chosen from 500\\nvalidation samples with ≤1%sampled performance drops.\\nRouterS: Llama-2 (7b)\\nL: Llama-2 (13b)S: Llama-2 (13b)\\nL: GPT-3.5-turboS: FLAN-t5 (800m)\\nL: Llama-2 (13b)\\nPerf. Drop Cost Adv. Perf. Drop Cost Adv. Perf. Drop Cost Adv.\\nrdetVal. 0.99% 98.20% 0.97% 15.20% 0.77% 5.40%\\nTest 1.60% 98.56% 0.55% 15.15% 0.69% 4.89%\\nrprobVal. 0.92% 97.60% 0.56% 8.60% 0.70% 5.00%\\nTest 1.42% 96.80% 0.11% 8.38% 0.57% 4.44%\\nrtransVal. 0.79% 96.00% 0.77% 17.00% 0.92% 4.00%\\nTest 1.39% 96.45% 0.49% 15.68% 1.02% 5.05%\\n(a) High correlation ( r= 0.46, ρ=\\n0.44).\\n(b) Medium correlation ( r=\\n0.38, ρ= 0.38).\\n(c) Low correlation ( r= 0.26, ρ=\\n0.27).\\nFigure 7: Routing performance evaluated with GPT-4 scores. Pearson ( r) and spearman ( ρ) correla-\\ntion coefficients between quality gaps measured by BART score and GPT-4 score are computed for\\neach LLM pair.\\nA.3 A LTERNATE EVALUATION METRICS\\nTo provide a more comprehensive evaluation of our routers, we test the routing performance with\\nmetrics in addition to BART score (Yuan et al., 2021). GPT-4-based evaluators have been found\\nto be well correlated with human assessments (Liu et al., 2023; Chase, 2022). We generate GPT-4\\nevaluation scores (integer ratings from 1to10) for test responses from Flan-t5 (800m), Llama-2 (7b),\\nLlama-2 (13b), and GPT-3.5-turbo that we investigate in our experiments, using LangChain scoring\\nevaluator (Chase, 2022). Recall that our routers are trained with BART score due to efficiency and\\neffectiveness reasons as discussed in Section 2.3. Intuitively, if the quality gaps measured by BART\\nscore and GPT-4 score are highly correlated, we could expect good routing performance even under\\nthe GPT-4 score as we have seen in Section 4.2. We compute the correlatio2025-02-19T21:18:54.080541709Z n between quality gaps\\nmeasured by BART score and GPT-4 score, and report it along with routing performance evaluated\\nwith GPT-4 score, as shown in Figure 7.\\nAligned with our intuition, when the two metrics are well correlated (Figure 7a), our routers trained\\nwith BART score are still effective even when evaluated against GPT-4 score. Typically, rdet,rprob,\\nandrtrans are able to achieve 20% cost advantage with up to 1%performance drop, and 40% cost\\nadvantage with up to 2.1%performance drop. As the correlation gets weaker, the router performance\\ngradually decays, as shown in Figure 7b and 7c. This observation suggests a simple-yet-effective\\nstrategy of using BART score in practice to save labelling costs while maintaining routing perfor-\\nmance. We can first compute the correlation between BART score and the target metrics (e.g.,\\nhuman assessments) using a small sample and use BART score as training labels whenever there is\\nstrong positive correlation with target metrics.\\n14\\nPublished as a conference paper at ICLR 2024\\n(a) High correlation ( r= 0.76, ρ=\\n0.71).\\n(b) Medium correlation ( r=\\n0.55, ρ= 0.50).\\n(c) Low correlation ( r= 0.06, ρ=\\n0.02).\\nFigure 8: Routing performance on the testing LLM pairs that are different than the pairs routers\\nwere trained with. Pearson ( r) and spearman ( ρ) correlation coefficients between quality gaps of the\\ntraining and testing LLM pairs are computed for each setting.\\nA.4 G ENERALIZING TODIFFERENT MODEL PAIRS\\nWe evaluate the generalizability of our routers by testing their routing performance on LLM pairs\\ndifferent than the pairs they were trained with. We compute the correlation between quality gaps of\\ntraining and testing LLM pairs, and report it along with routing performance, as shown in Figure 8.\\nSimilar to our observation in Section A.3, our routers can generalize well if the quality gaps of\\ntesting LLM pairs exhibit strong positive correlation with the quality gaps of the training pairs. In\\nFigure 8a, both pearson and spearman correlation coefficients exceed 0.7, and all three routers are\\nable to achieve 20% cost advantage with up to 1.6%performance drop, and 40% cost advantage with\\nup to 4.1%performance drop. As the correlation becomes weaker, the generalizability of our router\\ngets restricted and routing performance decays, as shown in Figure 8b and 8c. This observation\\nsheds light on using the quality gap correlation as an effective indicator to decide if our routers can\\nbe applied to new LLM pairs in the early stage. Given a pair of LLMs (source pair) and a router\\ntrained on this pair we can measure the correlation between the quality gap of the source pair and\\nthe quality gap of any new target pair of LLMs to decide if the router will be effective on the target\\npair.\\nA.5 M ORE ROUTER PERFORMANCE RESULTS\\nIn this section, we provide more routing evaluation results on 4LLM pairs. Typically, as shown\\nin Figure 9, the FLAN-t5 (800m) v.s. FLAN-t5 (11b) pair is another example of small perfor-\\nmance gaps , the Llama-2 (7b) v.s. GPT-3.5-turbo pair can be characterized as being of medium\\nperformance gaps , while the routing between GPT-3.5-turbo and two FLAN-t5 models is of large\\nperformance gaps . Qualitative comparison results are summarized in Table 4, which resemble our\\nanalysis in Section 4.2. In general, rdetis able to achieve noticeable cost advantages with little to no\\nperformance drop when the the cost advantage targets are low or the performance gaps are small. As\\nthe routing becomes challenging, the improvements over rdetby having rprobbecome considerable\\nandrtrans starts to dominate the competition.\\nWe also provide the quality gaps difference for each routing scenario, as shown in Figure 10, where\\nwe consistently observe that our routing strategy correctly identifies easy queries and route them to\\nthe small model.\\nB D ATASET STATISTICS\\nWe uniformly sample 10k training examples from the training split of MixInstruct , for each of which\\nwe generate 10responses from all investigated LLMs. Our validation and test splits are the same\\nas the MixInstruct dataset, which consist of 5k instruction examples separately. Our dataset in total\\nconsists of 20k instruction examples, as shown in Table 5.\\n15\\nPublished as a conference paper at ICLR 2024\\nFigure 9: Error-cost tradeoffs achieved by rdet,rprob, and rtrans when the small and large models\\nare of different performance gaps.\\n16\\nPublished as a conference paper at ICLR 2024\\nTable 4: Cost advantage v.s. Performance drop.\\nCost\\nAdvantage\\n(%)Performance Drop (%)\\nS: FLAN-t5 (800m) S: Llama-2 (7b) S: FLAN-t5 (800m) S: FLAN-t5 (11b)\\nL: FLAN-t5 (11b) L: GPT-3.5-turbo L: GPT-3.5-turbo L: GPT-3.5-turbo\\nrdet rprob rtrans rdet rprob rtrans rdet rprob rtrans rdet rprob rtrans\\n10 -0.2 -0.3 -0.2 0.3 0.3 0.6 3.6 3.4 3.6 3.8 3.6 3.3\\n20 -0.2 -0.2 -0.2 1.5 1.3 1.2 8.7 8.7 7.9 9.0 8.9 7.3\\n40 0.0 -0.1 0.0 4.1 4.1 3.6 19.3 19.8 17.4 19.2 19.4 16.5\\nFigure 10: Average quality gap difference between queries routed to the small and large models with\\ndifferent performance gaps.\\nC Q UERY HARDNESS AND BART SCORE\\nC.1 Q UERY HARDNESS : A C ASE STUDY\\nIn this section, we demonstrate the query hardness with real-world examples from our dataset. We\\nchoose the FLAN-t5 (11b) v.s. GPT-3.5-turbo routing pair for illustration purpose.\\nC.1.1 E ASY QUERY EXAMPLE\\nInput Query : “Rewrite the sentence so that it’s in the present tense. She had worked at the company\\nfor the past 3 years ”\\nLLM Responses :\\n•FLAN-t5 (11b) : “She works at the company for the past 3 years.”\\n•GPT-3.5-turbo : “She works at the company for the past 3 years.”\\n17\\nPublished as a conference paper at ICLR 2024\\nTable 5: Dataset Statistics.\\nSources #Examples Source of Responses\\nAlpace-GPT4 4,179 GPT-4\\nDolly-15K 1,381 Human\\nGPT4All-LAION 13,547 chatGPT\\nShareGPT 567 chatGPT\\nTotal 20K Mix of Above\\nIn this example, the small model (i.e., FLAN-t5 (11b)) is able to generate the same high quality\\nresponse as the large model (i.e., GPT-3.5-turbo), which makes it an easy query by definition.\\nC.1.2 H ARD QUERY EXAMPLE\\nInput Query : “What are the benefits of having a dog in the family?”\\nLLM Responses :\\n•FLAN-t5 (11b) : “A dog is good for physical activity.”\\n•GPT-3.5-turbo : “Having a dog in the family can have numerous benefits, including:\\n1. Companionship: Dogs provide unconditional love and companionship, offering constant\\nemotional support and a sense of belonging.\\n2. Improved mental health: Being around dogs has been proven to reduce stress, anxiety,\\nand depression. They can also help alleviate loneliness and provide a sense of purpose.\\n3. Increased physical activity: Dogs require regular exercise, which encourages family\\nmembers to engage in physical activities such as walking, running, and playing. This\\npromotes a healthier lifestyle for both the dog and the family.\\n4. Teaching responsibility: Caring for a dog requires feeding, grooming, training, and\\nregular vet visits. Having a dog in the family can teach children valuable life skills and\\ninstill a sense of responsibility.\\n5. Socialization: Dogs are known to be social creatures. Walking a dog or visiting dog\\nparks can provide opportunities for social interaction and help family members connect\\nwith other dog owners in the community.\\n6. Teaching empathy and compassion: Nurturing and caring for a dog teaches children\\nempathy, compassion, and the importance of kindness towards animals.\\n7. Home security: Dogs are natural protectors and can act as an early warning system,\\nalerting the family to potential intruders or dangers.\\n8. Health benefits: Interacting with dogs, such as petting or playing, has been shown to\\nlower blood pressure, decrease the risk of allergies and asthma in children, and boost the\\nimmune system.\\n9. Improved family dynamics: The presence of a dog can strengthen family bonds by\\nproviding shared experiences, fostering teamwork, and creating a common focus of love\\nand care.\\nIt is important to note that owning a dog requires time, effort, and financial responsibil-\\nity. Families should carefully consider their lifestyle, resources, and commitment before\\ndeciding to bring a dog into their home.”\\nIn this example, the response from the small model (i.e., FLAN-t5 (11b)) is clearly uninformative\\nand unpolished, while the large model (i.e., GPT-3.5-turbo) is able to provide comprehensive and\\nwell-constructed arguments. We identify this example as a hard query.\\nC.2 BART SCORE : A CASE STUDY\\nIn this section, we empirically examine the correlation between BART scores and human judgments.\\nWe take the query “ Where is the A ´eronef A-1 Skyraider built? ” as example to which the expected\\n18\\nPublished as a conference paper at ICLR 2024\\nanswer is “ The A ´eronef A-1 Skyraider was primarily built by the Douglas Aircraft Company\\nin El Segundo, California. ” We report the 5random responses from Llama-2 (13b) appended with\\nthe corresponding BART scores, as shown below.\\nResponses from Llama-2 (13b):\\n1. Where is the A ´eronef A-1 Skyraider built? The A ´eronef A-1 Skyraider was built by the\\nDouglas Aircraft Company in El Segundo, California, USA. ( -0.927 )\\n2. Where is the A ´eronef A-1 Skyraider built? A-1 Skyraider is built by Douglas Aircraft\\nCompany, located in El Segundo, California, USA. ( -1.12 )\\n3. Where is the A ´eronef A-1 Skyraider built? The A ´eronef A-1 Skyraider was built in the\\nUnited States by Douglas Aircraft Company. ( -1.63 )\\n4. Where is the A ´eronef A-1 Skyraider built? The A ´eronef A-1 Skyraider was built in the\\nUnited States. ( -2.25 )\\n5. Where is the A ´eronef A-1 Skyraider built? The A ´eronef A-1 Skyraider is a fictional aircraft,\\nso it is not built anywhere as it does not exist in reality. ( -2.93 )\\nAs shown above, the best response of the highest BART score −0.927provides all necessary in-\\nformation such as the company name and the city name. As the BART scores decrease, it can be\\nobserved that the quality of corresponding responses starts degrading as well. For example, the 3-th\\nresponse does not include the city information, the 4-th response further misses the company name,\\nwhile the last one is completely wrong. This example empirically justifies the effectiveness of using\\nBART scores as the response quality metric.\\n19', 'content_summary': 'Published as a conference paper at ICLR 2024\\nHYBRID LLM: C OST-EFFICIENT AND QUALITY -\\nAWARE QUERY R...', 'content_length': 58372, 'chunks_count': -1, 'status': <DocStatus.PENDING: 'pending'>},\n2025-02-19 22:18:54 error:invalid byte sequence for encoding \"UTF8\": 0x00\n2025-02-19 22:18:54 ERROR:Error processing or enqueueing file 7318_Hybrid_LLM_Cost_Efficient.pdf: invalid byte sequence for encoding \"UTF8\": 0x00\n2025-02-19 22:18:54 ERROR:Traceback (most recent call last):\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/lightrag/api/lightrag_server.py\", line 1213, in pipeline_enqueue_file\n2025-02-19 22:18:54     await rag.apipeline_enqueue_documents(content)\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/lightrag/lightrag.py\", line 764, in apipeline_enqueue_documents\n2025-02-19 22:18:54     await self.doc_status.upsert(new_docs)\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/lightrag/kg/postgres_impl.py\", line 632, in upsert\n2025-02-19 22:18:54     await self.db.execute(\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/lightrag/kg/postgres_impl.py\", line 184, in execute\n2025-02-19 22:18:54     await connection.execute(sql, *data.values())  # type: ignore\n2025-02-19 22:18:54     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connection.py\", line 352, in execute\n2025-02-19 22:18:54     _, status, _ = await self._execute(\n2025-02-19 22:18:54                    ^^^^^^^^^^^^^^^^^^^^\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connection.py\", line 1864, in _execute\n2025-02-19 22:18:54     result, _ = await self.__execute(\n2025-02-19 22:18:54                 ^^^^^^^^^^^^^^^^^^^^^\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connection.py\", line 1961, in __execute\n2025-02-19 22:18:54     result, stmt = await self._do_execute(\n2025-02-19 22:18:54                    ^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-19 22:18:54   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connection.py\", line 2024, in _do_execute\n2025-02-19 22:18:54     result = await executor(stmt, None)\n2025-02-19 22:18:54              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-19 22:18:54   File \"asyncpg/protocol/protocol.pyx\", line 206, in bind_execute\n2025-02-19 22:18:54 asyncpg.exceptions.CharacterNotInRepertoireError: invalid byte sequence for encoding \"UTF8\": 0x00\n```\n\n### Additional Information\n\n- LightRAG Version: Master\n- Operating System: Docker\n- Python Version: 3.10\n- Related Issues: NA\n",
      "state": "open",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-19T21:29:22Z",
      "updated_at": "2025-05-22T03:23:23Z",
      "closed_at": null,
      "labels": [
        "bug",
        "PostgreSQL"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/884/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/884",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/884",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.163117",
      "comments": []
    },
    {
      "issue_number": 1612,
      "title": "it run on cpu win10",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nhttps://github.com/werruww/suc-lightrag-on-cpu-win-10\n\n### Steps to reproduce\n\n1\n\n### Expected Behavior\n\n1\n\n### LightRAG Config Used\n\n# Paste your config here\n1\n\n### Logs and screenshots\n\n1\n\n### Additional Information\n\n1",
      "state": "open",
      "author": "werruww",
      "author_type": "User",
      "created_at": "2025-05-22T02:35:11Z",
      "updated_at": "2025-05-22T02:35:56Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1612/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1612",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1612",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.163141",
      "comments": []
    },
    {
      "issue_number": 1601,
      "title": "[Bug]:pipmaster.package_manager - ERROR - Command failed with exit code 1: /home/ubuntu/miniconda3/envs/deers/bin/python -m pip install --upgrade graspologic",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWe just execute the order \"pip install lightrag-hku\". And we run the file \"lightrag_compatible_demo.log\" with the error \"- pipmaster.package_manager - ERROR - Command failed with exit code 1: /home/ubuntu/miniconda3/envs/deers/bin/python -m pip install --upgrade graspologic\". And the details are as follows:\n\n2025-05-20 22:50:53 - pipmaster.package_manager - ERROR - Command failed with exit code 1: /home/ubuntu/miniconda3/envs/deers/bin/python -m pip install --upgrade graspologic\n--- stdout ---\nLooking in indexes: http://mirrors.tencentyun.com/pypi/simple\nCollecting graspologic\n  Using cached graspologic-0.3.1-py3-none-any.whl\nCollecting anytree>=2.8.0 (from graspologic)\n  Using cached http://mirrors.tencentyun.com/pypi/packages/7b/98/f6aa7fe0783e42be3093d8ef1b0ecdc22c34c0d69640dfb37f56925cb141/anytree-2.13.0-py3-none-any.whl (45 kB)\nCollecting beartype>=0.7.1 (from graspologic)\n  Using cached http://mirrors.tencentyun.com/pypi/packages/78/05/536d025b3e17cf938f836665dde32e86f65ee76acd0ae14e22bda6aee274/beartype-0.20.2-py3-none-any.whl (1.2 MB)\nCollecting gensim<=3.9.0,>=3.8.0 (from graspologic)\n  Using cached http://mirrors.tencentyun.com/pypi/packages/a0/b4/f4e45875a4cb1c4f6a76d6d07a2981753aab5f135dac2381f625e8807542/gensim-3.8.3.tar.gz (23.4 MB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting graspologic-native>=1.0.0 (from graspologic)\n  Using cached http://mirrors.tencentyun.com/pypi/packages/8e/55/15e6e4f18bf249b529ac4cd1522b03f5c9ef9284a2f7bfaa1fd1f96464fe/graspologic_native-1.2.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\nCollecting hyppo>=0.2.0 (from graspologic)\n  Using cached http://mirrors.tencentyun.com/pypi/packages/74/c9/b4dbc133a23363994e634f9c6c8637dec1b371db065407690d40868d2d72/hyppo-0.5.1-py3-none-any.whl (165 kB)\nCollecting joblib>=0.17.0 (from graspologic)\n  Using cached http://mirrors.tencentyun.com/pypi/packages/da/d3/13ee227a148af1c693654932b8b0b02ed64af5e1f7406d56b088b57574cd/joblib-1.5.0-py3-none-any.whl (307 kB)\nCollecting matplotlib!=3.3.*,>=3.0.0 (from graspologic)\n  Using cached http://mirrors.tencentyun.com/pypi/packages/f5/64/41c4367bcaecbc03ef0d2a3ecee58a7065d0a36ae1aa817fe573a2da66d4/matplotlib-3.10.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\nRequirement already satisfied: networkx>=2.1 in /home/ubuntu/miniconda3/envs/deers/lib/python3.13/site-packages (from graspologic) (3.4.2)\nRequirement already satisfied: numpy>=1.8.1 in /home/ubuntu/miniconda3/envs/deers/lib/python3.13/site-packages (from graspologic) (2.2.6)\nCollecting POT>=0.7.0 (from graspologic)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "why6why",
      "author_type": "User",
      "created_at": "2025-05-20T15:00:50Z",
      "updated_at": "2025-05-22T02:34:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1601/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1601",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1601",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.163147",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Please use python 3.10 ~ 3.12 instead of  or python 3.13",
          "created_at": "2025-05-21T13:33:54Z"
        },
        {
          "author": "why6why",
          "body": "thanks",
          "created_at": "2025-05-22T02:34:38Z"
        }
      ]
    },
    {
      "issue_number": 1611,
      "title": "it run on colab gpu",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nhttps://github.com/werruww/suc-lightrag-gpu/blob/main/suc_LightRAG_GPU.ipynb\n\n### Steps to reproduce\n\n1\n\n### Expected Behavior\n\n1\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n1\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "werruww",
      "author_type": "User",
      "created_at": "2025-05-22T01:53:46Z",
      "updated_at": "2025-05-22T01:53:46Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1611/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1611",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1611",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.353791",
      "comments": []
    },
    {
      "issue_number": 1609,
      "title": "llama3.23b",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHow can I run a small Hagggingface model on the processor and what are the versions of the libraries because I tried and it produced errors\nlightrag\n pdf \nLlama3.2without ollama\nTorch transformer=xxxxxxxx\n\n### Additional Context\n\n?",
      "state": "open",
      "author": "Qarqor5555555",
      "author_type": "User",
      "created_at": "2025-05-21T15:43:27Z",
      "updated_at": "2025-05-21T22:52:48Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1609/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1609",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1609",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.353817",
      "comments": [
        {
          "author": "Qarqor5555555",
          "body": "how run lighrrag with pdf and llama3.23b on cpu in colab or codspeac githuub",
          "created_at": "2025-05-21T15:44:54Z"
        },
        {
          "author": "Qarqor5555555",
          "body": "Can teachers create a Colab page to run the program with the hugginfacce model?",
          "created_at": "2025-05-21T15:46:17Z"
        },
        {
          "author": "werruww",
          "body": "!python /content/LightRAG/examples/unofficial-sample/lightrag_hf_demo.py\n\n\nDrop files to upload them to session storage.\nDisk\n61.03 GB available\n\n[ ]\n\nStart coding or generate with AI.\n\n[ ]\n\nStart coding or generate with AI.\n\n[ ]\n\nStart coding or generate with AI.\n\n[ ]\n\nStart coding or generate with",
          "created_at": "2025-05-21T20:38:43Z"
        },
        {
          "author": "werruww",
          "body": "\n\n/content/LightRAG-1.3.7/examples/unofficial-sample/lightrag_hf_demo.py\n\n\n\n\n\n\n\n\n\nimport os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.hf import hf_model_complete, hf_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom transformers import AutoModel, AutoTokenizer\nfrom lightrag.kg.s",
          "created_at": "2025-05-21T22:18:24Z"
        },
        {
          "author": "werruww",
          "body": "colab cpu",
          "created_at": "2025-05-21T22:18:32Z"
        }
      ]
    },
    {
      "issue_number": 15,
      "title": "Unable to run on colab",
      "body": "Unable to run on the google colab getting error\r\n\r\n`from lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nOPENAI_API_KEY = \"sk-xxxxxxxx\"\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=gpt_4o_mini_complete  # Use gpt_4o_mini_complete LLM model\r\n    # llm_model_func=gpt_4o_complete  # Optionally, use a stronger model\r\n)\r\n\r\n with open(\"abc.txt\") as f:\r\n     await rag.insert(f.read())\r\n\r\n#Perform naive search\r\nprint(rag.query(\"What are the formatting guidelines the a writer should follow?\", param=QueryParam(mode=\"naive\")))\r\n\r\n#Perform local search\r\nprint(rag.query(\"What are the formatting guidelines the a writer should follow?\", param=QueryParam(mode=\"local\")))\r\n\r\n #Perform global search\r\nprint(rag.query(\"What are the formatting guidelines the a writer should follow?\", param=QueryParam(mode=\"global\")))\r\n\r\n#Perform hybrid search\r\nprint(rag.query(\"What are the formatting guidelines the a writer should follow?\", param=QueryParam(mode=\"hybrid\")))`\r\n\r\nthe error I got is\r\n\r\n`INFO:lightrag:Logger initialized for working directory: ./dickens\r\nDEBUG:lightrag:LightRAG init with param:\r\n  working_dir = ./dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7e70392f68c0>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function gpt_4o_mini_complete at 0x7e70392f6440>,\r\n  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x7e70392f41f0>\r\n\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n[<ipython-input-22-a6197c408cb5>](https://localhost:8080/#) in <cell line: 21>()\r\n     19 \r\n     20 # Perform naive search\r\n---> 21 print(rag.query(\"What are the formatting guidelines the a writer should follow while writing manuscript for RCT in lancet?\", param=QueryParam(mode=\"naive\")))\r\n     22 \r\n     23 # Perform local search\r\n\r\n2 frames\r\n[/usr/local/lib/python3.10/dist-packages/lightrag/lightrag.py](https://localhost:8080/#) in query(self, query, param)\r\n    246     def query(self, query: str, param: QueryParam = QueryParam()):\r\n    247         loop = always_get_an_event_loop()\r\n--> 248         return loop.run_until_complete(self.aquery(query, param))\r\n    249 \r\n    250     async def aquery(self, query: str, param: QueryParam = QueryParam()):\r\n\r\n[/usr/lib/python3.10/asyncio/base_events.py](https://localhost:8080/#) in run_until_complete(self, future)\r\n    623         \"\"\"\r\n    624         self._check_closed()\r\n--> 625         self._check_running()\r\n    626 \r\n    627         new_task = not futures.isfuture(future)\r\n\r\n[/usr/lib/python3.10/asyncio/base_events.py](https://localhost:8080/#) in _check_running(self)\r\n    582     def _check_running(self):\r\n    583         if self.is_running():\r\n--> 584             raise RuntimeError('This event loop is already running')\r\n    585         if events._get_running_loop() is not None:\r\n    586             raise RuntimeError(\r\n\r\nRuntimeError: This event loop is already running`\r\n",
      "state": "closed",
      "author": "Shubh789da",
      "author_type": "User",
      "created_at": "2024-10-15T18:01:33Z",
      "updated_at": "2025-05-21T15:50:55Z",
      "closed_at": "2024-10-16T06:58:34Z",
      "labels": [
        "good first PR"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/15/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/15",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/15",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.532878",
      "comments": [
        {
          "author": "alew3",
          "body": "I was about to report the same thing. I also noticed, because they are doing rag.insert in async, you cant do a for loop docs and insert them. Had to do a big workaround to get it working as below\r\n\r\n```python\r\nimport asyncio\r\nimport os\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm i",
          "created_at": "2024-10-15T19:14:54Z"
        },
        {
          "author": "Drwaish",
          "body": "Can you share colab notebook here? When I try to run the above code snippet, it throws the same error.",
          "created_at": "2024-10-17T10:02:15Z"
        },
        {
          "author": "Shubh789da",
          "body": "sorry I am very new to python and LLM. I am still unable to run it. I want to perform RAG on a file name \"instructions.txt\". How to run LightRAG on it can anyone please provide the full code.\r\n\r\nMy current code it \r\n`import asyncio\r\nimport os\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag",
          "created_at": "2024-10-17T18:55:28Z"
        },
        {
          "author": "HeQinWill",
          "body": "Install nest-asyncio first:  \r\n```python\r\n!pip install nest-asyncio\r\n```\r\n\r\nAdd the below before the official codes:  \r\n```python\r\nimport nest_asyncio\r\nnest_asyncio.apply()\r\n```",
          "created_at": "2024-10-18T06:46:24Z"
        },
        {
          "author": "Shubh789da",
          "body": "@HeQinWill \r\nThanks a lot, it worked.",
          "created_at": "2024-10-18T15:47:44Z"
        }
      ]
    },
    {
      "issue_number": 1599,
      "title": "[Question]: Where is the example/batch_eval.py mentioned in README.MD?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhere is the example/batch_eval.py mentioned in README.MD?\nI  want to reproduce LightRAG's evaluation process.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "WangAo-0",
      "author_type": "User",
      "created_at": "2025-05-20T08:13:43Z",
      "updated_at": "2025-05-20T08:13:43Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1599/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1599",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1599",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.716335",
      "comments": []
    },
    {
      "issue_number": 1598,
      "title": "[Feature Request]: export script to transfer data from nanovactor to postgres",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nAn export script to transfer data from nanovactor to postgres would be desirable where a project need to be transfered from demo to production stage.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Samir1145",
      "author_type": "User",
      "created_at": "2025-05-20T07:33:10Z",
      "updated_at": "2025-05-20T07:33:21Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1598/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1598",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1598",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.716360",
      "comments": []
    },
    {
      "issue_number": 1489,
      "title": "[Question]:lightrag_webui通过bun启动之后在界面上登录的用户名密码默认是多少？",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nlightrag_webui通过bun启动之后在界面上登录的用户名密码默认是多少？\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "jstinwei",
      "author_type": "User",
      "created_at": "2025-04-29T07:31:31Z",
      "updated_at": "2025-05-20T05:09:10Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1489/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1489",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1489",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.716369",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "密码在配置文件 .env 里面。",
          "created_at": "2025-04-29T10:01:34Z"
        },
        {
          "author": "q2467481940",
          "body": "我也遇到了这个问题 请问您解决了嘛 是否需要启动后台服务",
          "created_at": "2025-05-20T05:09:09Z"
        }
      ]
    },
    {
      "issue_number": 1588,
      "title": "[Bug]: Limited compatibility with asyncio",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI’m working with an async app that includes calls like `await asyncio.gather(*tasks)`. One task creates an agentic team which does some things on LightRAG. These are triggered inside a `run()` function, which is called through `loop.run_until_complete(...)`. I patched the event loop with `nest_asyncio` and retrieved the current event loop using `asyncio.get_event_loop()`.\n\nThe issue arises when `loop.run_until_complete(...)` finishes and the task is executed again. Then I observed that some asyncio tasks remained pending or unfinished — particularly those which comes from `final_decro`. The loop hangs after `kg_query`. \n\nUpon inspection after the first execution of my `loop`, I noticed that many tasks were still pending even after execution should have finished. Most of the pending tasks are: `worker()` coroutines from `lightrag.utils.py`. Calls to `Queue.get()` from `asyncio.queues`. A `health_check()` coroutine.\nThese tasks are all stuck waiting on pending futures and never complete, even after all expected tasks have been processed.\n\nTo avoid that, and doesnt really solve the pending futures, I currently wrapped `run()` in `asyncio.run`, which I isn't save either, since this will break running loops. \n\n### Steps to reproduce\n\n- `nest_asyncio` and `loop=asyncio.get_event_loop()` or do smth like `loop=always_get_an_event_loop()`\n- wrap `loop.run_until_complete(run)`\n- wrap tasks with `asyncio.gather(*tasks)`\n- check `print(\"Remaining tasks:\", asyncio.all_tasks(loop))`\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 3.6.1\n- Operating System: \n- Python Version: 3.11\n- Related Issues:\n",
      "state": "closed",
      "author": "reqyou",
      "author_type": "User",
      "created_at": "2025-05-16T08:31:10Z",
      "updated_at": "2025-05-19T09:52:41Z",
      "closed_at": "2025-05-19T09:52:40Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1588/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1588",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1588",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:14.906416",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG utilizes an asynchronous task queue to manage LLM calls, and the coroutine remains active after handling the LLM responses. For safe use of LightRAG, ensure that the main process of your project is asynchronous.",
          "created_at": "2025-05-16T08:52:22Z"
        },
        {
          "author": "reqyou",
          "body": "This was helpful. I haven't noticed that fact. Thanks. ",
          "created_at": "2025-05-19T09:52:40Z"
        }
      ]
    },
    {
      "issue_number": 1573,
      "title": "[Bug]:KeyError: 'get_by_mode_id_project2llm_response_cache'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI think there's a bug when enable enable_llm_cache_for_entity_extract.\nMy lightrag config as below:\n```python\nrag = LightRAG(\n        namespace_prefix=NAMESPACE,\n        working_dir=WORKING_DIR,\n        enable_llm_cache=False,\n        llm_model_func=_llm_model_func,\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=_embedding_func,\n        ),\n        embedding_batch_num=10,\n        embedding_func_max_async=10,\n        embedding_cache_config={\n            \"enabled\": \"true\",\n            \"similarity_threshold\": 0.95,\n            \"use_llm_check\": False,\n        },\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        # graph_storage=\"NetworkXStorage\",\n        vector_storage=\"MilvusVectorDBStorage\",\n        auto_manage_storages_states=True,\n        # llm_model_kwargs={\n        #     \"response_format\": {\"type\": \"json_object\"},\n        #     \"extra_body\": {\"enable_search\": True}\n        #     },\n        addon_params={\n            # \"language\": \"Simplified Chinese\",\n            \"language\": \"English\",\n        },\n    )\n```\n\nThen I get the error logs:\n```bash\nTraceback (most recent call last):\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/operate.py\", line 829, in _process_with_semaphore\n    return await _process_single_content(chunk)\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/operate.py\", line 754, in _process_single_content\n    final_result = await use_llm_func_with_cache(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/utils.py\", line 1594, in use_llm_func_with_cache\n    cached_return, _1, _2, _3 = await handle_cache(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/utils.py\", line 1004, in handle_cache\n    mode_cache = await hashing_kv.get_by_mode_and_id(mode, args_hash) or {}\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 413, in get_by_mode_and_id\n    sql = SQL_TEMPLATES[\"get_by_mode_id_\" + self.namespace]\nKeyError: 'get_by_mode_id_project2llm_response_cache'\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-05-14T01:48:21Z",
      "updated_at": "2025-05-19T07:43:05Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1573/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1573",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1573",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:15.110121",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The use of namespace_prefix is deprecated and should no longer be configured.",
          "created_at": "2025-05-14T14:14:43Z"
        },
        {
          "author": "jasperchen01",
          "body": "> The use of namespace_prefix is deprecated and should no longer be configured.\n\nI see in the source code\n```python\n# TODO: deprecated, remove in the future, use WORKSPACE instead\nnamespace_prefix: str = field(default=\"\")\n\"\"\"Prefix for namespacing stored data across different environments.\"\"\"\n```\nCa",
          "created_at": "2025-05-17T06:20:33Z"
        },
        {
          "author": "danielaskdd",
          "body": "Multiple WORKSPACEs are included in the LightRAG roadmap and are expected to be available within approximately 2 to 3 months.",
          "created_at": "2025-05-17T07:40:58Z"
        },
        {
          "author": "jasperchen01",
          "body": "> Multiple WORKSPACEs are included in the LightRAG roadmap and are expected to be available within approximately 2 to 3 months.\n\nI desperately need this feature. Is there anything I can do to make it available in a month? Or can you give me some implementation details? I can help with this part",
          "created_at": "2025-05-18T01:55:04Z"
        },
        {
          "author": "jasperchen01",
          "body": "> Multiple WORKSPACEs are included in the LightRAG roadmap and are expected to be available within approximately 2 to 3 months.\n\nI have post a pull request to add workspace param with milvus and pg storage, maybe you could help to review it:\nhttps://github.com/HKUDS/LightRAG/pull/1597",
          "created_at": "2025-05-19T07:43:04Z"
        }
      ]
    },
    {
      "issue_number": 1566,
      "title": "[Question]: What is causing this execution error? Is it a configuration error or a bug?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello, I am trying to run the LightRAG server and while the server starts up as expected, identifies the local ollama service and the models, it fails to ingest any text file. I provide a snippet from the log file below.\n\nIs this due to a configuration error, or a bug (or both)?\n\n### Additional Context\n\n2025-05-12 23:48:04 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\n\nLightRAG log file: /home/ntsarb/LightRAG_m3/lightrag/lightrag.log\n\nWARNING:root:In uvicorn mode, workers parameter was set to 4. Forcing workers=1\n\n    ╔══════════════════════════════════════════════════════════════╗\n    ║                  🚀 LightRAG Server v1.3.7/0170              ║\n    ║          Fast, Lightweight RAG Server Implementation         ║\n    ╚══════════════════════════════════════════════════════════════╝\n\n\n📡 Server Configuration:\n    ├─ Host: 0.0.0.0\n    ├─ Port: 9621\n    ├─ Workers: 1\n    ├─ CORS Origins: *\n    ├─ SSL Enabled: False\n    ├─ Ollama Emulating Model: lightrag:latest\n    ├─ Log Level: INFO\n    ├─ Verbose Debug: False\n    ├─ History Turns: 3\n    ├─ API Key: Not Set\n    └─ JWT Auth: Disabled\n\n📂 Directory Configuration:\n    ├─ Working Directory: /home/ntsarb/LightRAG_m3/lightrag/rag_storage\n    └─ Input Directory: /home/ntsarb/LightRAG_m3/lightrag/inputs\n\n🤖 LLM Configuration:\n    ├─ Binding: ollama\n    ├─ Host: http://localhost:11434\n    ├─ Model: gemma3:27b-it-qat\n    ├─ Temperature: 0.2\n    ├─ Max Async for LLM: 4\n    ├─ Max Tokens: 32768\n    ├─ Timeout: 150\n    ├─ LLM Cache Enabled: True\n    └─ LLM Cache for Extraction Enabled: True\n\n📊 Embedding Configuration:\n    ├─ Binding: ollama\n    ├─ Host: http://localhost:11434\n    ├─ Model: bge-m3:latest\n    └─ Dimensions: 1024\n\n⚙️ RAG Configuration:\n    ├─ Summary Language: English\n    ├─ Max Parallel Insert: 2\n    ├─ Max Embed Tokens: 8192\n    ├─ Chunk Size: 1200\n    ├─ Chunk Overlap Size: 100\n    ├─ Cosine Threshold: 0.2\n    ├─ Top-K: 60\n    ├─ Max Token Summary: 500\n    └─ Force LLM Summary on Merge: 6\n\n💾 Storage Configuration:\n    ├─ KV Storage: JsonKVStorage\n    ├─ Vector Storage: NanoVectorDBStorage\n    ├─ Graph Storage: NetworkXStorage\n    └─ Document Status Storage: JsonDocStatusStorage\n\n✨ Server starting up...\n\n\n🌐 Server Access Information:\n    ├─ WebUI (local): http://localhost:9621\n    ├─ Remote Access: http://<your-ip-address>:9621\n    ├─ API Documentation (local): http://localhost:9621/docs\n    └─ Alternative Documentation (local): http://localhost:9621/redoc\n\n📝 Note:\n    Since the server is running on 0.0.0.0:\n    - Use 'localhost' or '127.0.0.1' for local access\n    - Use your machine's IP address for remote access\n    - To find your IP address:\n      • Windows: Run 'ipconfig' in terminal\n      • Linux/Mac: Run 'ifconfig' or 'ip addr' in terminal\n\nINFO: Process 21658 Shared-Data created for Single Process\nINFO: Created new empty graph\nStarting Uvicorn server in single-process mode on 0.0.0.0:9621\nINFO: Started server process [21658]\nINFO: Waiting for application startup.\nINFO: Process 21658 initialized updated flags for namespace: [full_docs]\nINFO: Process 21658 ready to initialize storage namespace: [full_docs]\nINFO: Process 21658 KV load full_docs with 0 records\nINFO: Process 21658 initialized updated flags for namespace: [text_chunks]\nINFO: Process 21658 ready to initialize storage namespace: [text_chunks]\nINFO: Process 21658 KV load text_chunks with 0 records\nINFO: Process 21658 initialized updated flags for namespace: [entities]\nINFO: Process 21658 initialized updated flags for namespace: [relationships]\nINFO: Process 21658 initialized updated flags for namespace: [chunks]\nINFO: Process 21658 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 21658 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 21658 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 21658 KV load llm_response_cache with 3 records\nINFO: Process 21658 initialized updated flags for namespace: [doc_status]\nINFO: Process 21658 ready to initialize storage namespace: [doc_status]\nINFO: Process 21658 doc status load doc_status with 1 records\nINFO: Process 21658 Pipeline namespace initialized\n\nServer is ready to accept connections! 🚀\n\nINFO: Application startup complete.\n...\n...\n...\nINFO: limit_async: 16 new workers initialized\nINFO: limit_async: 4 new workers initialized\nINFO:  == LLM cache == saving default: 0db851aa861be8018eb3d78480a8ca84\nERROR: limit_async: Error in decorated function:\nERROR: limit_async: Error in decorated function:\nERROR: limit_async: Error in decorated function:\nERROR: Failed to extract entities and relationships:\nERROR: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n    return await self._connection.handle_async_request(request)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n    raise exc\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 217, in _receive_event\n    data = await self._network_stream.read(\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n...\n...\n...\n2025-05-11 20:35:55,073 - lightrag - ERROR - Failed to extrat document 1/1: overview.txt\n2025-05-11 20:35:55,077 - lightrag - INFO - Document processing pipeline completed\n2025-05-11 20:35:55,079 - lightrag - ERROR - Error indexing file overview.txt: object method can't be used in 'await' expression\n2025-05-11 20:35:55,081 - lightrag - ERROR - Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n    return await self._connection.handle_async_request(request)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n    raise exc\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 217, in _receive_event\n    data = await self._network_stream.read(\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/ntsarb/LightRAG_m3/lightrag/lightrag/lightrag.py\", line 1002, in process_document\n...",
      "state": "closed",
      "author": "ntsarb",
      "author_type": "User",
      "created_at": "2025-05-12T22:41:25Z",
      "updated_at": "2025-05-17T10:49:02Z",
      "closed_at": "2025-05-17T10:49:02Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1566/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1566",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1566",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:15.339678",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The LLM operation timed out due to a slow response from your Ollama server. I recommend increasing the timeout setting for the LLM. I have updated lightrag_ollama_demo.py to demonstrate how to configure both the timeout and context window settings for Ollama. Please pull the latest version and revie",
          "created_at": "2025-05-13T17:29:05Z"
        },
        {
          "author": "ntsarb",
          "body": "> The LLM operation timed out due to a slow response from your Ollama server. I recommend increasing the timeout setting for the LLM. I have updated lightrag_ollama_demo.py to demonstrate how to configure both the timeout and context window settings for Ollama. Please pull the latest version and rev",
          "created_at": "2025-05-15T23:59:18Z"
        },
        {
          "author": "danielaskdd",
          "body": "Each file is treated as an atomic processing unit in the pipeline. A file is marked as successfully processed only after all its text blocks have completed extraction and merging. If any error occurs during processing, the entire file is marked as failed and must be reprocessed.\n\nLarge files should ",
          "created_at": "2025-05-16T00:54:15Z"
        }
      ]
    },
    {
      "issue_number": 1589,
      "title": "[Question]: Guvicorn errors raise two questions",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello, I imported several text documents and launched the server using:\n\nlightrag-gunicorn --workers 4\n\nThe process ingested the first text file (there was an unhandled race condition preventing the parallel ingestion of two documents), it proceeded to the deduplication steps and completed this successfully. \n\nThen, it ingested two text files in parallel, but after the deduplication steps it stopped with an error message suggesting it may have run out of memory. (I presume system RAM, 180GB allocated via WSL2, mostly unused).\n\nAfter that, when I attempt to re-run the server in the same way, I get the below errors.\n\nNote that I have since tested running the server with lightrag-server --workers 4 and this appears to be working (currently running), but it is re-running the entities+relationships extraction for the documents I had already processed and this took several hours (wasted). \n\nHence, this is to raise two questions:\n\n1) Is it possible to figure out what causes the server to misbehave and handle the relevant exceptions in a graceful manner, so that no data is lost?\n\n2) Does it make sense to get the lightRAG process to save intermediate outputs (which can be costly to produce) on the SSD before proceeding with the next step?\n\nPlease let me know if there is additional information I can provide to help resolve this.\n\n### Additional Context\n\n(lightrag) ntsarb@myhostname:~/LightRAG$ lightrag-gunicorn --workers 4\n2025-05-16 15:11:21 - pipmaster.package_manager - INFO - Targeting pip associated with Python: /usr/bin/python3 | Command base: /usr/bin/python3 -m pip\n\n    ╔══════════════════════════════════════════════════════════════╗\n    ║                  🚀 LightRAG Server v1.3.7/0170              ║\n    ║          Fast, Lightweight RAG Server Implementation         ║\n    ╚══════════════════════════════════════════════════════════════╝\n\n\n📡 Server Configuration:\n    ├─ Host: 0.0.0.0\n    ├─ Port: 9621\n    ├─ Workers: 4\n    ├─ CORS Origins: *\n    ├─ SSL Enabled: False\n    ├─ Ollama Emulating Model: lightrag:latest\n    ├─ Log Level: INFO\n    ├─ Verbose Debug: False\n    ├─ History Turns: 3\n    ├─ API Key: Not Set\n    └─ JWT Auth: Disabled\n\n📂 Directory Configuration:\n    ├─ Working Directory: /home/ntsarb/LightRAG/rag_storage\n    └─ Input Directory: /home/ntsarb/LightRAG/inputs\n\n🤖 LLM Configuration:\n    ├─ Binding: ollama\n    ├─ Host: http://localhost:11434\n    ├─ Model: llama3.3:70b-instruct-q8_0\n    ├─ Temperature: 0.2\n    ├─ Max Async for LLM: 4\n    ├─ Max Tokens: 32768\n    ├─ Timeout: None (infinite)\n    ├─ LLM Cache Enabled: True\n    └─ LLM Cache for Extraction Enabled: True\n\n📊 Embedding Configuration:\n    ├─ Binding: ollama\n    ├─ Host: http://localhost:11434\n    ├─ Model: bge-m3:latest\n    └─ Dimensions: 1024\n\n⚙️ RAG Configuration:\n    ├─ Summary Language: English\n    ├─ Max Parallel Insert: 2\n    ├─ Max Embed Tokens: 8192\n    ├─ Chunk Size: 1200\n    ├─ Chunk Overlap Size: 100\n    ├─ Cosine Threshold: 0.2\n    ├─ Top-K: 60\n    ├─ Max Token Summary: 500\n    └─ Force LLM Summary on Merge: 6\n\n💾 Storage Configuration:\n    ├─ KV Storage: JsonKVStorage\n    ├─ Vector Storage: NanoVectorDBStorage\n    ├─ Graph Storage: NetworkXStorage\n    └─ Document Status Storage: JsonDocStatusStorage\n\n✨ Server starting up...\n\n\n🌐 Server Access Information:\n    ├─ WebUI (local): http://localhost:9621\n    ├─ Remote Access: http://<your-ip-address>:9621\n    ├─ API Documentation (local): http://localhost:9621/docs\n    └─ Alternative Documentation (local): http://localhost:9621/redoc\n\n📝 Note:\n    Since the server is running on 0.0.0.0:\n    - Use 'localhost' or '127.0.0.1' for local access\n    - Use your machine's IP address for remote access\n    - To find your IP address:\n      • Windows: Run 'ipconfig' in terminal\n      • Linux/Mac: Run 'ifconfig' or 'ip addr' in terminal\n\n🚀 Starting LightRAG with Gunicorn\n🔄 Worker management: Gunicorn (workers=4)\n🔍 Preloading app: Enabled\n📝 Note: Using Gunicorn's preload feature for shared data initialization\n\n\n================================================================================\nMAIN PROCESS INITIALIZATION\nProcess ID: 783\nWorkers setting: 4\n================================================================================\n\nINFO: Process 783 Shared-Data created for Multiple Process (workers=4)\n\nStarting Gunicorn with direct Python API...\nINFO: Process 783 Shared-Data already initialized (multiprocess=True)\n2025-05-16 15:11:24,362 [INFO] lightrag: Loaded graph from /home/ntsarb/LightRAG/rag_storage/graph_chunk_entity_relation.graphml with 131 nodes, 124 edges\n2025-05-16 15:11:24,374 [INFO] nano-vectordb: Load (131, 1024) data\n2025-05-16 15:11:24,375 [INFO] nano-vectordb: Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/ntsarb/LightRAG/rag_storage/vdb_entities.json'} 131 data\n2025-05-16 15:11:24,380 [INFO] nano-vectordb: Load (124, 1024) data\n2025-05-16 15:11:24,380 [INFO] nano-vectordb: Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/ntsarb/LightRAG/rag_storage/vdb_relationships.json'} 124 data\n2025-05-16 15:11:24,381 [INFO] nano-vectordb: Load (12, 1024) data\n2025-05-16 15:11:24,381 [INFO] nano-vectordb: Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/ntsarb/LightRAG/rag_storage/vdb_chunks.json'} 12 data\n2025-05-16 15:11:24,430 [INFO] gunicorn.error: Starting gunicorn 23.0.0\n\n================================================================================\nGUNICORN MASTER PROCESS: on_starting jobs for 4 worker(s)\nProcess ID: 783\n================================================================================\nMemory usage after initialization: 180.19 MB\nLightRAG log file: /home/ntsarb/LightRAG/lightrag.log\n\nGunicorn initialization complete, forking workers...\n\n2025-05-16 15:11:24,443 [INFO] gunicorn.error: Listening at: http://0.0.0.0:9621 (783)\n2025-05-16 15:11:24,443 [INFO] gunicorn.error: Using worker: uvicorn.workers.UvicornWorker\n2025-05-16 15:11:24,447 [INFO] gunicorn.error: Booting worker with pid: 843\nINFO: Process 843 initialized updated flags for namespace: [full_docs]\nINFO: Process 843 ready to initialize storage namespace: [full_docs]\nINFO: Process 843 KV load full_docs with 1 records\nINFO: Process 843 initialized updated flags for namespace: [text_chunks]\nINFO: Process 843 ready to initialize storage namespace: [text_chunks]\nINFO: Process 843 KV load text_chunks with 12 records\nINFO: Process 843 initialized updated flags for namespace: [entities]\nINFO: Process 843 initialized updated flags for namespace: [relationships]\nINFO: Process 843 initialized updated flags for namespace: [chunks]\nINFO: Process 843 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 843 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 843 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 843 KV load llm_response_cache with 30 records\n2025-05-16 15:11:24,542 [INFO] gunicorn.error: Booting worker with pid: 923\nINFO: Process 843 initialized updated flags for namespace: [doc_status]\nINFO: Process 843 ready to initialize storage namespace: [doc_status]\nINFO: Process 843 doc status load doc_status with 7 records\nINFO: Process 923 storage namespace already initialized: [full_docs]\nINFO: Process 843 Pipeline namespace initialized\nINFO: Process 923 storage namespace already initialized: [text_chunks]\n\nServer is ready to accept connections! 🚀\n\n2025-05-16 15:11:24,628 [INFO] gunicorn.error: Booting worker with pid: 971\nINFO: Process 923 storage namespace already initialized: [llm_response_cache]\nINFO: Process 923 storage namespace already initialized: [doc_status]\nINFO: Process 971 storage namespace already initialized: [full_docs]\nINFO: Process 971 storage namespace already initialized: [text_chunks]\n\nServer is ready to accept connections! 🚀\n\nINFO: Process 971 storage namespace already initialized: [llm_response_cache]\nINFO: Process 971 storage namespace already initialized: [doc_status]\n\nServer is ready to accept connections! 🚀\n\n2025-05-16 15:11:24,715 [INFO] gunicorn.error: Booting worker with pid: 1046\nINFO: Process 1046 storage namespace already initialized: [full_docs]\nINFO: Process 1046 storage namespace already initialized: [text_chunks]\nINFO: Process 1046 storage namespace already initialized: [llm_response_cache]\nINFO: Process 1046 storage namespace already initialized: [doc_status]\n\nServer is ready to accept connections! 🚀\n\nINFO: 127.0.0.1:41512 - \"POST /documents/scan HTTP/1.1\" 200\nINFO: Found 7 new files to index.\nINFO: No new unique documents were found.\nINFO: Successfully fetched and enqueued file: overview.txt\nINFO: No new unique documents were found.\nINFO: Successfully fetched and enqueued file: analysis2.txt\nINFO: No new unique documents were found.\nINFO: Successfully fetched and enqueued file: remedies1.txt\nINFO: No new unique documents were found.\nINFO: Successfully fetched and enqueued file: remedies2.txt\nINFO: No new unique documents were found.\nINFO: Successfully fetched and enqueued file: quality.txt\nINFO: No new unique documents were found.\nINFO: Successfully fetched and enqueued file: volume-6.txt\nINFO: No new unique documents were found.\nINFO: Successfully fetched and enqueued file: instrumentation.txt\nINFO: Processing 6 document(s)\nINFO: Extracting stage 1/6: analysis2.txt\nINFO: Processing d-id: doc-3a627fb560124c574869c239518f0d22\nINFO: Extracting stage 2/6: remedies1.txt\nINFO: Processing d-id: doc-fc4a2be5501319f238edcb86bd491f4e\nINFO: limit_async: 16 new workers initialized\nINFO: limit_async: 4 new workers initialized\n2025-05-16 15:12:26,901 [CRITICAL] gunicorn.error: WORKER TIMEOUT (pid:1046)\n2025-05-16 15:12:27,910 [ERROR] gunicorn.error: Worker (pid:1046) was sent SIGKILL! Perhaps out of memory?\n2025-05-16 15:12:27,914 [INFO] gunicorn.error: Booting worker with pid: 1523\nINFO: Process 1523 storage namespace already initialized: [full_docs]\nINFO: Process 1523 storage namespace already initialized: [text_chunks]\nINFO: Process 1523 storage namespace already initialized: [llm_response_cache]\nINFO: Process 1523 storage namespace already initialized: [doc_status]\n\nServer is ready to accept connections! 🚀\n\n",
      "state": "open",
      "author": "ntsarb",
      "author_type": "User",
      "created_at": "2025-05-16T14:46:30Z",
      "updated_at": "2025-05-17T07:07:50Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1589/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1589",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1589",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:15.549273",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "1. When multiple files are uploaded, once the first file upload is complete, the server immediately initiates a processing job, resulting in subsequent files being queued until the initial processing job is finished.\n\n2. I'm not certain whether WSL2 supports Gunicorn. It is recommended to run Gunico",
          "created_at": "2025-05-17T01:52:26Z"
        },
        {
          "author": "danielaskdd",
          "body": "I noticed that you are using a local Ollama as your LLM. LightRAG's default context window size is 32K, which is much larger than Ollama's default of 2K. This will cause Ollama's GPU memory usage to increase significantly, and the token output speed will also decrease sharply. In addition, we recomm",
          "created_at": "2025-05-17T01:58:53Z"
        },
        {
          "author": "ntsarb",
          "body": "> I noticed that you are using a local Ollama as your LLM. LightRAG's default context window size is 32K, which is much larger than Ollama's default of 2K. This will cause Ollama's GPU memory usage to increase significantly, and the token output speed will also decrease sharply. In addition, we reco",
          "created_at": "2025-05-17T06:19:12Z"
        },
        {
          "author": "ntsarb",
          "body": "Thanks again! \n \n> 1. When multiple files are uploaded, once the first file upload is complete, the server immediately initiates a processing job, resulting in subsequent files being queued until the initial processing job is finished.\n\nSometimes it ingests 2 documents in parallel, other times it in",
          "created_at": "2025-05-17T07:07:49Z"
        }
      ]
    },
    {
      "issue_number": 1239,
      "title": "[Feature Request]: configuration of entity types and custom kg in LightRAG server",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nHello,\n\nI would like to be able to provide a custom KG and list of entity types to my LightRAG server.\n- The custom KG could be provided as an environment variable pointing to a json file (or a directory of files ?)\n- The list of entity types could be provided as an environment variable as well. Using the same format as the one used in `PROMPTS[\"DEFAULT_ENTITY_TYPES\"]`.\n\nWhat do you think ?\n\nThanks\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "vmahe35",
      "author_type": "User",
      "created_at": "2025-03-31T08:36:15Z",
      "updated_at": "2025-05-16T16:15:33Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1239/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1239",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1239",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:15.767591",
      "comments": [
        {
          "author": "flamz3d",
          "body": "+1\n\nBeing able to pass a configuration would be preferable.",
          "created_at": "2025-03-31T10:38:46Z"
        },
        {
          "author": "0bserver07",
          "body": "I believe this is currently possible:\n\nif you pass a dict for \"addon_params\"\n\nhttps://github.com/HKUDS/LightRAG/blob/7a67f6c2fd95aa01c7a9c7945874eb170b1eb6f0/examples/lightrag_oracle_demo.py#L94\n\n\n\n```python\n\n    config[\"addon_params\"] = {\n        \"entity_types\": args.entity_types,\n        \"language",
          "created_at": "2025-04-01T04:07:18Z"
        },
        {
          "author": "vmahe35",
          "body": "Yes, I know it is possible if I use lightrag package in my python code.\nBut here the point is to be able to do it also in the LightRAG server, with only configuration via environment variables and config files (no Python code).\n\n",
          "created_at": "2025-04-01T07:07:08Z"
        },
        {
          "author": "ujnxs123",
          "body": "+1\nI need this feature badly.",
          "created_at": "2025-04-02T04:14:48Z"
        },
        {
          "author": "Exploding-Soda",
          "body": "Do you mean we may need a feature to specify the type of nodes stored in the graph database before queries, so that when retrieve knowledge we can only get nodes in a specific range as knowledge input?",
          "created_at": "2025-04-03T05:46:30Z"
        }
      ]
    },
    {
      "issue_number": 1587,
      "title": "[Feature Request]: Synchronize MAX_GRAPH_NODES setting to webui",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nSynchronize the MAX_GRAPH_NODES setting with the web UI via the status and health check APIs:\n\n- Add the max_graph_nodes attribute to the LightRAG class and all graph storage classes.\n- Retrieve the MAX_GRAPH_NODES value during server startup and pass it to LightRAG and graph storage classes during their initialization.\n- Update the health check and get_status APIs to include max_graph_nodes in their responses to the frontend.\n- Modify the frontend to set the maximum nodes value based on the values returned by the health check and get_status APIs.",
      "state": "open",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-05-16T08:26:55Z",
      "updated_at": "2025-05-16T09:29:11Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1587/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1587",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1587",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:15.963899",
      "comments": []
    },
    {
      "issue_number": 1536,
      "title": "[Bug]: OpenAI o3 via openAI backend: Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen using o3 on the openai backend, I try to upload a file of around 1 MB and get the following error in the console:\n\n`openai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}`\n\nGiven that for the thinking models on then openai backend it needs to use max_completion_tokens instead of max_tokens this error makes sense, however I do not see a way to configure max_completion_tokens in the .env?\n\n### Steps to reproduce\n\nUpload any larger file and configure the openai backend to o3 or o4-mini\n\n### Expected Behavior\n\nif using an OpenAI thinking model, namely o3 or o4-mini, the parameter max_completion_tokens instead of max_tokens should be sent to the backend.\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\nERROR: OpenAI API Call Failed,\nModel: o3-2025-04-16,\nParams: {'max_tokens': 1000, 'temperature': 1.0}, Got: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\nERROR: limit_async: Error in decorated function: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\nERROR: Merging stage failed in document doc-1254d453f577c1661c94d287e7c5b5cf: Traceback (most recent call last):\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/lightrag.py\", line 1033, in process_document\n    await merge_nodes_and_edges(\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/operate.py\", line 543, in merge_nodes_and_edges\n    entity_data = await _merge_nodes_then_upsert(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/operate.py\", line 292, in _merge_nodes_then_upsert\n    description = await _handle_entity_relation_summary(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/operate.py\", line 144, in _handle_entity_relation_summary\n    summary = await use_llm_func_with_cache(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/utils.py\", line 1614, in use_llm_func_with_cache\n    res: str = await use_llm_func(input_text, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/utils.py\", line 544, in wait_func\n    return await future\n           ^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/utils.py\", line 328, in worker\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/api/lightrag_server.py\", line 223, in openai_alike_model_complete\n    return await openai_complete_if_cache(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/tenacity/__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/lightrag/llm/openai.py\", line 185, in openai_complete_if_cache\n    response = await openai_async_client.chat.completions.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/openai/_base_client.py\", line 1742, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/votan/.local/pipx/venvs/lightrag-hku/lib/python3.11/site-packages/openai/_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n\n### Additional Information\n\n- LightRAG Version: v1.3.6\n- Operating System: macOS 15.4.1\n- Python Version: 3.11.12\n- Related Issues:\n",
      "state": "open",
      "author": "ViolentVotan",
      "author_type": "User",
      "created_at": "2025-05-06T16:29:27Z",
      "updated_at": "2025-05-16T08:05:34Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1536/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.3.8",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1536",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1536",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:15.963919",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thank you for reporting the issue. Do you have any recommendations for resolving it?",
          "created_at": "2025-05-09T13:21:34Z"
        },
        {
          "author": "BireleyX",
          "body": "@danielaskdd Please also include this issue in relation to supporting 'reasoning models' like the o3. I'm using the lightrag-server setup:\n\nFile \"C:\\Apps\\dVALi\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from N",
          "created_at": "2025-05-16T07:04:52Z"
        },
        {
          "author": "danielaskdd",
          "body": "> [@danielaskdd](https://github.com/danielaskdd) Please also include this issue in relation to supporting 'reasoning models' like the o3. I'm using the lightrag-server setup:\n> \n> File \"C:\\Apps\\dVALi.venv\\Lib\\site-packages\\openai_base_client.py\", line 1562, in _request raise self._make_status_error_",
          "created_at": "2025-05-16T08:02:54Z"
        },
        {
          "author": "BireleyX",
          "body": "**I'm using Azure OpenAI o3-mini.**\n\nI was able to add support for this by modifying these files:\n\nlightrag_server.py\n```\n    async def azure_openai_model_complete(\n        prompt,\n        system_prompt=None,\n        history_messages=None,\n        keyword_extraction=False,\n        **kwargs,\n        ",
          "created_at": "2025-05-16T08:05:33Z"
        }
      ]
    },
    {
      "issue_number": 1586,
      "title": "[Feature Request]:Optimize the knowledge graph using graph neural networks",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nIf I want to use graph neural networks to optimize knowledge graphs, how should I modify the code?\n\nThank you for your answer\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Ssqjxxlwhy",
      "author_type": "User",
      "created_at": "2025-05-16T07:15:35Z",
      "updated_at": "2025-05-16T07:15:35Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1586/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1586",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1586",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.164941",
      "comments": []
    },
    {
      "issue_number": 1576,
      "title": "[Question]:",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nBased on your code, can user permissions be added to enable different users to access different databases\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "WLULULULU",
      "author_type": "User",
      "created_at": "2025-05-14T03:56:24Z",
      "updated_at": "2025-05-16T03:02:58Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1576/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1576",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1576",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.164964",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Support for multiple document collections (workspaces) is included in our product roadmap. The user permissions feature will be introduced when multiple document collections become available.",
          "created_at": "2025-05-14T14:06:47Z"
        },
        {
          "author": "WLULULULU",
          "body": "Is it possible to provide the source code of the webUI？",
          "created_at": "2025-05-16T02:32:00Z"
        },
        {
          "author": "danielaskdd",
          "body": "The source code is located in the \"lightrag_webui\" directory.",
          "created_at": "2025-05-16T03:02:57Z"
        }
      ]
    },
    {
      "issue_number": 1584,
      "title": "[Question]:如何把大模型更改为kimi官方的大模型？",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n如何把大模型更改为kimi官方的大模型？\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "huyidu",
      "author_type": "User",
      "created_at": "2025-05-15T07:41:15Z",
      "updated_at": "2025-05-15T16:15:42Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1584/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1584",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1584",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.374423",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "根据Kimi的[官方API文档](https://platform.moonshot.cn/docs/guide/start-using-kimi-api)， 其大模型使用OpenAI兼容的接口。因此LightRAG Server的 [README](https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README-zh.md#%E5%90%AF%E5%8A%A8-lightrag-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87) 中的 OpenAI 兼",
          "created_at": "2025-05-15T16:15:41Z"
        }
      ]
    },
    {
      "issue_number": 1574,
      "title": "[Bug]: function create_graph(unknown) does not exist",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI think there's a bug with PGGraphStorage.\nMy demo code as below:\n```python\nimport asyncio\nimport logging\nimport os\nimport time\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import openai_embed,openai_complete_if_cache\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nimport numpy as np\n\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nWORKING_DIR = f\"{CURRENT_DIR}/lightrag_data\"\n\nNAMESPACE = \"project2\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.DEBUG)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# PG\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\nos.environ[\"POSTGRES_HOST\"] = \"localhost\"\nos.environ[\"POSTGRES_PORT\"] = \"5432\"\nos.environ[\"POSTGRES_USER\"] = \"postgres\"\nos.environ[\"POSTGRES_PASSWORD\"] = \"\"\nos.environ[\"POSTGRES_DATABASE\"] = \"lightrag_test\"\n\n# neo4j\nos.environ[\"NEO4J_URI\"] = \"neo4j://localhost:7687\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"admin123\"\n\nasync def _llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        model=\"qwen-plus-latest\",\n        prompt=prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        **kwargs\n    )\n\nasync def _embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=\"text-embedding-v3\",\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\nasync def initialize_rag():\n    rag = LightRAG(\n        namespace_prefix=NAMESPACE,\n        working_dir=WORKING_DIR,\n        enable_llm_cache=False,\n        llm_model_func=_llm_model_func,\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=False,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=_embedding_func,\n        ),\n        embedding_batch_num=10,\n        embedding_func_max_async=10,\n        embedding_cache_config={\n            \"enabled\": \"true\",\n            \"similarity_threshold\": 0.95,\n            \"use_llm_check\": False,\n        },\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        # graph_storage=\"NetworkXStorage\",\n        vector_storage=\"MilvusVectorDBStorage\",\n        auto_manage_storages_states=True,\n        # llm_model_kwargs={\n        #     \"response_format\": {\"type\": \"json_object\"},\n        #     \"extra_body\": {\"enable_search\": True}\n        #     },\n        addon_params={\n            # \"language\": \"Simplified Chinese\",\n            \"language\": \"English\",\n        },\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def main():\n    # Initialize RAG instance\n    rag = await initialize_rag()\n\n\n    # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n    # await rag.adelete_by_doc_id(doc_id=FILE_NAME)\n    \n    start_time = time.time()\n    word_count = 0\n    \n    content1 = \"performance on a variety of professional and academic standards, even if it might not be as proficient as humans in everyday situations [13]. For example, compared to GPT-3.5, it has attained a score in the highest 10% of participants in tests on a virtual legal examination [12]. There remains an opportunity for enhancements to GPT-4's factuality, steerability, and remaining within the provided restrictions, but after six months of incremental alignment using lessons from OpenAI's adversarial evaluation programme and ChatGPT, the model achieved its best-ever efficiency [57].\"\n    \n    content2 = \"ChatGPT's utility in medicine and healthcare [14] lies in its ability to (i) aid in diagnosis by analysing patient records, health status, and indications to produce treatment strategies tailored to each patient's unique desires and requirements, (ii) summarise and synthesise clinical studies to support a researchbased practices, (iii) offer medical knowledge and guidance to patients in a simple and digestible manner, and (iv) encourage interaction among medical experts. Further, ChatGPT can be utilised to create robots that can aid with patient evaluation, which is the process by which medical professionals assess the severity of a patient's illness and decide what treatment is\"\n    \n    content3 = \"Due to its adaptability and superior NLP skills, ChatGPT has found use outside of the academic research community. In this section, we discuss how ChatGPT can be used in a variety of applications (as shown in Figure 3) illustrating the ways in which the platform can revolutionise business processes, bolster collaboration, and spark new ideas.\"\n    \n    inputs = [content1, content2, content3]\n    postions = [\"{'polygon': [[66.86399841308594, 281.00927734375], [242.998046875, 281.00927734375], [242.998046875, 200.75341796875], [66.86399841308594, 291.75341796875]]}\", \"{'polygon': [[33.86399841308594, 281.00927734375], [242.998046875, 281.00927734375], [242.998046875, 200.75341796875], [33.86399841308594, 200.75341796875]]}\", \"{'polygon': [[55.86399841308594, 55.00927734375], [55.998046875, 55.00927734375], [55.998046875, 55.75341796875], [55.86399841308594, 55.75341796875]]}\"]\n    \n    await rag.ainsert(input=inputs, file_paths=postions)\n    \n    end_time = time.time()\n    print(f\"Word count: {word_count}, insertion time: {end_time - start_time} seconds\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\nAnd get error logs when run the demo:\n```bash\nTraceback (most recent call last):\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 1307, in _query\n    data = await self.db.query(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 219, in query\n    await self.configure_age(connection, graph_name)  # type: ignore\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 100, in configure_age\n    await connection.execute(  # type: ignore\n  File \"/opt/miniconda3/envs/matetext/lib/python3.10/site-packages/asyncpg/connection.py\", line 349, in execute\n    result = await self._protocol.query(query, timeout)\n  File \"asyncpg/protocol/protocol.pyx\", line 375, in query\nasyncpg.exceptions.UndefinedFunctionError: function create_graph(unknown) does not exist\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/lightrag.py\", line 1055, in process_document\n    await merge_nodes_and_edges(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/operate.py\", line 564, in merge_nodes_and_edges\n    entity_data = await _merge_nodes_then_upsert(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/operate.py\", line 258, in _merge_nodes_then_upsert\n    already_node = await knowledge_graph_inst.get_node(entity_name)\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 1375, in get_node\n    record = await self._query(query)\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 1322, in _query\n    raise PGGraphQueryException(\nlightrag.kg.postgres_impl.PGGraphQueryException: {'message': 'Error executing graph query: SELECT * FROM cypher(\\'project2chunk_entity_relation\\', $$\\n                     MATCH (n:base {entity_id: \"ChatGPT\"})\\n                     RETURN n\\n                   $$) AS (n agtype)', 'wrapped': 'SELECT * FROM cypher(\\'project2chunk_entity_relation\\', $$\\n                     MATCH (n:base {entity_id: \"ChatGPT\"})\\n                     RETURN n\\n                   $$) AS (n agtype)', 'detail': 'function create_graph(unknown) does not exist\\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.'}\n```\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: master\n- Operating System: macos 15.3.1\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-05-14T02:06:47Z",
      "updated_at": "2025-05-15T07:45:57Z",
      "closed_at": "2025-05-15T07:45:56Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1574/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1574",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1574",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.563017",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You should install and enable AGE graph feature in PostgreSQL  server.",
          "created_at": "2025-05-14T14:09:57Z"
        },
        {
          "author": "jasperchen01",
          "body": "> You should install and enable AGE graph feature in PostgreSQL server.\n\nThx, it works",
          "created_at": "2025-05-15T07:45:56Z"
        }
      ]
    },
    {
      "issue_number": 1578,
      "title": "[Question]:想自建一个基于postgresql数据库的方法，但是不太清楚需要的表单结构，我自己新建一个表单后，报错如下",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n2025-05-14 22:23:47,986 - __main__ - ERROR - 构建索引失败: 'NoneType' object has no attribute 'workspace'\nTraceback (most recent call last):\n  File \"d:\\ai\\web_new\\LightRAG\\lightrag_api\\Graph_Processing.py\", line 207, in <module>\n    rag.insert(contents, file_paths=file_paths)\n  File \"d:\\ai\\web_new\\LightRAG\\lightrag\\lightrag.py\", line 575, in insert\n    loop.run_until_complete(\n  File \"E:\\Anaconda3\\envs\\agent\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"d:\\ai\\web_new\\LightRAG\\lightrag\\lightrag.py\", line 600, in ainsert\n    await self.apipeline_enqueue_documents(input, ids, file_paths)\n  File \"d:\\ai\\web_new\\LightRAG\\lightrag\\lightrag.py\", line 786, in apipeline_enqueue_documents\n    unique_new_doc_ids = await self.doc_status.filter_keys(all_new_doc_ids)\n  File \"d:\\ai\\web_new\\LightRAG\\lightrag\\kg\\postgres_impl.py\", line 900, in filter_keys\n    params = {\"workspace\": self.db.workspace}\nAttributeError: 'NoneType' object has no attribute 'workspace'\n\n\n请问如何解决\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "DiaoFuYuan",
      "author_type": "User",
      "created_at": "2025-05-14T14:24:26Z",
      "updated_at": "2025-05-15T07:38:23Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1578/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1578",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1578",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.736423",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Errors that are not based on the official code or recommended usage are outside the scope of Issue discussions.",
          "created_at": "2025-05-15T07:38:22Z"
        }
      ]
    },
    {
      "issue_number": 1583,
      "title": "[Question]:想复现想完一下训练集的出处",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n想请求数据集\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "belo-belove",
      "author_type": "User",
      "created_at": "2025-05-15T03:10:23Z",
      "updated_at": "2025-05-15T03:10:23Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1583/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1583",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1583",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.931750",
      "comments": []
    },
    {
      "issue_number": 1582,
      "title": "[Question]: Clarification on HyDE prompt, chunking, and LLM settings for Ultradomain Winrate (Table 1 reproduction)",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi, I'm currently reproducing the Ultradomain winrate results in Table 1 of the LightRAG paper, particularly the comparisons between LightRAG, HyDE, and GraphRAG.\n\nI successfully constructed the graph using LightRAG and evaluated winrate performance using the HyDE library. However, I'm seeing significant performance gaps when using HyDE under different configurations.\n\nSpecifically:\n\nWhen using HyDE with the default prompt, the generated answers often contain hallucinations, and the resulting winrate is significantly lower than what's reported in the paper.\n\nWhen modifying the prompt to explicitly restrict hallucination and enforce information-grounded answers, the winrate improves notably. (I'll attach both winrate evaluation graphs in this issue.)\n\nTo properly reproduce the experiment, could you clarify:\n\nWhat prompt template was used with HyDE in Table 1?\n\nWhat chunk size and chunking strategy were used when processing the Ultradomain documents?\n\nWhich language model (e.g., OpenAI GPT-4, Claude, GPT-4o, etc.) was used to generate answers in the HyDE evaluation?\n\nWere any additional hyperparameters changed (e.g., top-k retrieval count, number of hypotheses n in HyDE generation)?\n\nAnd similarly, what were the initial settings for GraphRAG in Table 1?\n\nI'm attaching:\n\n📊 Graph 1: HyDE default prompt, chunk size 128 → low winrate\n\n```\nCHUNK_SIZE = 128\n\nprompt_template = f\"\"\"\nBased on the following information, please answer the question.\nQuestion: {query}\nDocuments:\n{{contexts}}\nAnswer:\n\"\"\"\n\ngenerator = OpenAIGenerator(\n    model_name=\"gpt-4o-mini\",\n    n=8,\n    max_tokens=512,\n    temperature=0.7\n)\n```\n\n\n![Image](https://github.com/user-attachments/assets/50037a21-29cf-485f-b4b9-bcaca553bc8c)\n\n\n📊 Graph 2: Custom hallucination-restricted prompt, same chunk size → improved winrate\n\n```\nCHUNK_SIZE = 128\n\nprompt_template = f\"\"\"\n        ---Role---\n        You are a helpful assistant responding to user query\n        ---Goal---\n        Generate a concise response based on the following information and follow Response Rules. Do not include information not provided by following Information\n        ---User Query---\n        {query}\n        ---Information---\n        {{contexts}}\n        ---Response Rules---\n        - Use markdown formatting with appropriate section headings\n        - Please respond in the same language as the user's question.\n        - Ensure the response maintains continuity with the conversation history.\n        - If you don't know the answer, just say so.\n        - Do not make anything up. Do not include information not provided by the Infromation.\n\n        Answer:\n        \"\"\"\n\ngenerator = OpenAIGenerator(\n    model_name=\"gpt-4o-mini\",\n    n=8,\n    max_tokens=512,\n    temperature=0.7\n)\n```\n\n![Image](https://github.com/user-attachments/assets/03e7dac0-5145-431b-b3a8-f74f344a9a82)\n\n📊 Table 1 From the Paper\n![Image](https://github.com/user-attachments/assets/92bb45d9-e145-405b-87c7-f494c5343480)\n\n\n📝 Full evaluation script-adapted Custom hallucination-restricted prompt: [HyDE Evaluation Gist](https://gist.github.com/shshjhjh4455/f78c405e72b95e35a340a34880dee7d1)\n\n\n\nThank you in advance for your help — accurate reproduction of the original settings would be extremely valuable!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "shshjhjh4455",
      "author_type": "User",
      "created_at": "2025-05-15T02:08:11Z",
      "updated_at": "2025-05-15T02:15:50Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1582/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1582",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1582",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.931770",
      "comments": []
    },
    {
      "issue_number": 1580,
      "title": "[Question]: Gaining better control over the insert process?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThe `rag.insert(...)`takes a text document (or a collection of documents) as an input. From there, LightRAG uses an LLM to extract various information (entities, relationships, etc.). This extraction is tailored by a rather complex prompt which outputs formatted data that is later parsed by LightRAG.\n\nMy first question concern the format of the output : why this default particular default format (the one described in `prompt.py`)? Why not ask the LLM to output something like `json` or even `xml` which can be easily handled by machines and humans alike? Not to mention that LLM are trained on such formats (or language in the case of xml). I couldn't find any reason for this design choice running through the paper.\n\nThe follow-up question addresses the possibility of interacting with the insert task by providing `rag.insert(...)` formatted data rather than plain text (using a schema specified by LightRAG). This way, preparation of data could be handled outside LightRAG, allowing easier testing and better control (moreover, having an explicit schema for the extracted data would make it more comfortable to modify the LLM prompt). Is there an approach that already allows that kind of interaction?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "WilliamDiakite",
      "author_type": "User",
      "created_at": "2025-05-14T22:00:13Z",
      "updated_at": "2025-05-14T22:02:49Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1580/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1580",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1580",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.931777",
      "comments": []
    },
    {
      "issue_number": 1577,
      "title": "[Question]:Using Ollama is too slow",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI started the Ollama server locally and tried to construct a KG. 200 chunks take 16 hours+. I checked the serve information and found that they would stagnate for a long time. What is the reason? Is it because of Ollama's problem or Lightrag's insert? It is now 12:18 Beijing time, my KG is not finished, and Ollama has not worked for 7 hours.\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "Wings-Of-Disaster",
      "author_type": "User",
      "created_at": "2025-05-14T04:19:21Z",
      "updated_at": "2025-05-14T15:20:10Z",
      "closed_at": "2025-05-14T15:20:08Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1577/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1577",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1577",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:16.931783",
      "comments": [
        {
          "author": "Wings-Of-Disaster",
          "body": "Surprisingly, this is related to the content. I guess it may be the LightRAG prompt that caused the large model deployed by Ollama to have corresponding problems.",
          "created_at": "2025-05-14T04:44:59Z"
        },
        {
          "author": "danielaskdd",
          "body": "The default context window (num_ctx) for the Ollama server is set to 2,048, whereas LightRAG defaults to a context window of 32,768. This significant difference may considerably slow down the Ollama server, especially when limited video memory is available. Adjusting the MAX_TOKENS parameter to 8,19",
          "created_at": "2025-05-14T13:59:10Z"
        },
        {
          "author": "Wings-Of-Disaster",
          "body": "> The default context window (num_ctx) for the Ollama server is set to 2,048, whereas LightRAG defaults to a context window of 32,768. This significant difference may considerably slow down the Ollama server, especially when limited video memory is available. Adjusting the MAX_TOKENS parameter to 8,",
          "created_at": "2025-05-14T15:20:08Z"
        }
      ]
    },
    {
      "issue_number": 1543,
      "title": "Bug: The LightRAG API uses Uvicorn, which overrides the Uvicorn instance of the main project.",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/4a6189f3-c986-4d2f-8ac9-96be42c7d571)\n\n![Image](https://github.com/user-attachments/assets/53660b89-41a8-4e46-b387-10899cbac19d)\n\nSuggestion: Separate the api module from lightrag-server to prevent conflicts.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-05-07T11:54:33Z",
      "updated_at": "2025-05-14T04:30:38Z",
      "closed_at": "2025-05-14T04:30:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1543/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1543",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1543",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:17.120003",
      "comments": [
        {
          "author": "OxidBurn",
          "body": "Experienced the same problem.",
          "created_at": "2025-05-08T09:19:45Z"
        },
        {
          "author": "danielaskdd",
          "body": "How you start LightRAG server? \n\nTo initiate the LightRAG server, please utilize one of the following two approved methods:\n```\nlightrag-server\n# or\nlightrag-gunicorn\n```",
          "created_at": "2025-05-09T04:09:40Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "I built my own backend using FastAPI and chose not to use the LightRAG API as a standalone service. Instead, I directly integrated LightRAG’s source code into my main project. However, when I started running my main application with Uvicorn, the uvicorn.run() call inside LightRAG’s code launched its",
          "created_at": "2025-05-09T04:26:13Z"
        },
        {
          "author": "OxidBurn",
          "body": "The same is relevant for my case, I didn't use the LightRAG API as I have my own.",
          "created_at": "2025-05-09T07:22:48Z"
        },
        {
          "author": "danielaskdd",
          "body": "If you are using your own FastAPI implementation,, you shouldn't include the codes from the api folder.",
          "created_at": "2025-05-09T14:40:41Z"
        }
      ]
    },
    {
      "issue_number": 1569,
      "title": "[Bug]:Postgresql have type \"vector\" does not exist error",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nLightrag config as below: \n```python\nrag = LightRAG(\n        namespace_prefix=NAMESPACE,\n        working_dir=WORKING_DIR,\n        enable_llm_cache=False,\n        llm_model_func=_llm_model_func,\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=_embedding_func,\n        ),\n        embedding_batch_num=10,\n        embedding_func_max_async=10,\n        embedding_cache_config={\n            \"enabled\": \"true\",\n            \"similarity_threshold\": 0.95,\n            \"use_llm_check\": False,\n        },\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        # graph_storage=\"NetworkXStorage\",\n        vector_storage=\"MilvusVectorDBStorage\",\n        auto_manage_storages_states=True,\n        # llm_model_kwargs={\n        #     \"response_format\": {\"type\": \"json_object\"},\n        #     \"extra_body\": {\"enable_search\": True}\n        #     },\n        addon_params={\n            # \"language\": \"Simplified Chinese\",\n            \"language\": \"English\",\n        },\n    )\n```\n\nI already install the pgvetor plugin for the PostgreSQL 16, and execute sql \"CREATE EXTENSION VECTOR;\" for the database. \n\nThen I get the error:\n```bash\nsql:CREATE TABLE LIGHTRAG_DOC_CHUNKS (\n                    id VARCHAR(255),\n                    workspace VARCHAR(255),\n                    full_doc_id VARCHAR(256),\n                    chunk_order_index INTEGER,\n                    tokens INTEGER,\n                    content TEXT,\n                    content_vector VECTOR,\n                    file_path VARCHAR(256),\n                    create_time TIMESTAMP(0) WITH TIME ZONE,\n                    update_time TIMESTAMP(0) WITH TIME ZONE,\n                        CONSTRAINT LIGHTRAG_DOC_CHUNKS_PK PRIMARY KEY (workspace, id)\n                    ),\ndata:None,\nerror:type \"vector\" does not exist\nPostgreSQL, Failed to create table LIGHTRAG_DOC_CHUNKS in database, Please verify the connection with PostgreSQL database, Got: type \"vector\" does not exist\nTraceback (most recent call last):\n  File \"/Users/apple/workplace/AI/matetext-ai/demo/lightrag_demo/rag_insert_citation.py\", line 129, in <module>\n    asyncio.run(main())\n  File \"/opt/miniconda3/envs/matetext/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/miniconda3/envs/matetext/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/Users/apple/workplace/AI/matetext-ai/demo/lightrag_demo/rag_insert_citation.py\", line 102, in main\n    rag = await initialize_rag()\n  File \"/Users/apple/workplace/AI/matetext-ai/demo/lightrag_demo/rag_insert_citation.py\", line 94, in initialize_rag\n    await rag.initialize_storages()\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/lightrag.py\", line 493, in initialize_storages\n    await asyncio.gather(*tasks)\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 358, in initialize\n    self.db = await ClientManager.get_client()\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 328, in get_client\n    await db.check_tables()\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 178, in check_tables\n    raise e\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 170, in check_tables\n    await self.execute(v[\"ddl\"])\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 267, in execute\n    await connection.execute(sql)  # type: ignore\n  File \"/opt/miniconda3/envs/matetext/lib/python3.10/site-packages/asyncpg/connection.py\", line 349, in execute\n    result = await self._protocol.query(query, timeout)\n  File \"asyncpg/protocol/protocol.pyx\", line 375, in query\nasyncpg.exceptions.UndefinedObjectError: type \"vector\" does not exist\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-05-13T13:03:08Z",
      "updated_at": "2025-05-14T01:44:22Z",
      "closed_at": "2025-05-14T01:44:22Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1569/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1569",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1569",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:17.300487",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Installing and enabling the pgvector extension will resolve this issue.",
          "created_at": "2025-05-13T15:05:20Z"
        },
        {
          "author": "jasperchen01",
          "body": "> Installing and enabling the pgvector extension will resolve this issue.\n\nThx for the help, actually, I already installed and enabled the pgvector extension in my database of PostgreSQL 16. But I tried to delete the database and enable the vector extension again, it works now.",
          "created_at": "2025-05-14T01:00:39Z"
        }
      ]
    },
    {
      "issue_number": 1558,
      "title": "[Question]: Poor Answering Performance",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi there,\n\nThank you for the excellent work.\nI managed to setup Lightrag Server and it works fine.\nHowever I am facing an issue with the quality of the outcomes.\ne.g. I asked the question \"List the GIAS principles, domains and standards\"\nand used all available Query Mode, however, the best result was around 70%.\nThe document is around 120 pages. But the answers are in two pages at max (page 3 and 4)\n\nAny suggestions to improve the outcomes??\n\n### Additional Context\n\nI am using LM Studio, text-embedding-bge-m3 (quantization 8).\nand the LLM I tried gemma-3-4b-it and qwen3-4b.\n\n[globalinternalauditstandards_2024.pdf](https://github.com/user-attachments/files/20148739/globalinternalauditstandards_2024.pdf)\n\n![Image](https://github.com/user-attachments/assets/38652187-2a34-4deb-a209-8f65a397b4ac)",
      "state": "open",
      "author": "MSZ-MGS",
      "author_type": "User",
      "created_at": "2025-05-11T11:44:34Z",
      "updated_at": "2025-05-13T19:51:45Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1558/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1558",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1558",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:17.489342",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "A 4B parameter LLM is insufficient for document indexing and querying tasks. It is recommended to use a model with at least 32B parameters and a 32K context window. Generally, larger models provide better performance.",
          "created_at": "2025-05-12T02:12:26Z"
        },
        {
          "author": "MSZ-MGS",
          "body": "> A 4B parameter LLM is insufficient for document indexing and querying tasks. It is recommended to use a model with at least 32B parameters and a 32K context window. Generally, larger models provide better performance.\n\nThat's far away from consumer grade PC.\n\nWhat about the embbeding model text-em",
          "created_at": "2025-05-12T16:00:54Z"
        },
        {
          "author": "danielaskdd",
          "body": "bge-m3 is fine.",
          "created_at": "2025-05-12T19:05:06Z"
        },
        {
          "author": "MSZ-MGS",
          "body": "The quality of the outcomes are improved after detecting below bug. I am not sure where is it coming from.\n![Image](https://github.com/user-attachments/assets/f6512acc-9d52-49f3-8a4f-25d1736f2f63)\nThe issue is taking place with all embedding modules including bge-m3.\n\nI used Ollama for the embedding",
          "created_at": "2025-05-13T19:50:37Z"
        }
      ]
    },
    {
      "issue_number": 1567,
      "title": "[Bug]:addon_params entity_types not working?",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nwhen extracting the Knowledge Graph I used addon_params\n\n![Image](https://github.com/user-attachments/assets/f6d122d5-e729-44e0-a3d1-61ca140e3961)\n\nbut after its finished I got a lot of entity_types that not listed.\n\n![Image](https://github.com/user-attachments/assets/81349ce6-67dc-4aa6-9365-cd5e9c0dd282)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "molion097",
      "author_type": "User",
      "created_at": "2025-05-13T03:03:13Z",
      "updated_at": "2025-05-13T15:17:56Z",
      "closed_at": "2025-05-13T15:17:55Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1567/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1567",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1567",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:17.743326",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "It is possible that the capabilities of your current LLM are insufficient. We recommend trying a more advanced model, such as gpt-4.1-mini.",
          "created_at": "2025-05-13T15:15:48Z"
        },
        {
          "author": "molion097",
          "body": "Thank you for your information. ",
          "created_at": "2025-05-13T15:17:55Z"
        }
      ]
    },
    {
      "issue_number": 1568,
      "title": "[Question]:How to Update Existing Entity and Relationship Data in Lightrag When Definitions Change or Entities Are Deleted?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHow can I update existing entity and relationship data in Lightrag? It's not an incremental update, but rather a situation where the original entity definitions have changed or been deleted. I haven't found much information or source code regarding this.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhangxingbang",
      "author_type": "User",
      "created_at": "2025-05-13T07:59:46Z",
      "updated_at": "2025-05-13T15:11:07Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1568/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1568",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1568",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:17.965573",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG offers the following entity and relation operation functions:\n- edit_entity\n- edit_relation\n- merge_entities",
          "created_at": "2025-05-13T15:11:06Z"
        }
      ]
    },
    {
      "issue_number": 1571,
      "title": "[Feature Request]: GPUstack support?",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nis it possible to add support for GPUstack? This would be an alternative to Ollama to serve LLMs and embedding models.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Hollowmon111",
      "author_type": "User",
      "created_at": "2025-05-13T14:24:08Z",
      "updated_at": "2025-05-13T14:24:08Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1571/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1571",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1571",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:18.204474",
      "comments": []
    },
    {
      "issue_number": 1564,
      "title": "[Question]: Did lightrag support milvus distributed?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI tested Lightrag using milvus for vector_storage data storage, but it only seemed to support the mivus lite approach. May I ask whether milvus standalone or distributed can be configured\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-05-12T13:31:15Z",
      "updated_at": "2025-05-13T13:29:33Z",
      "closed_at": "2025-05-13T13:29:33Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1564/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1564",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1564",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:18.204497",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Default behavior: If no configuration is provided, the system operates in Milvus Lite mode, creating a milvus_lite.db file in the working directory for local storage.\n\nRemote connection support: You can connect to Milvus Standalone or Distributed deployments by:\n- Setting environment variables such ",
          "created_at": "2025-05-12T15:59:44Z"
        },
        {
          "author": "jasperchen01",
          "body": "Thx for the help",
          "created_at": "2025-05-13T06:53:06Z"
        }
      ]
    },
    {
      "issue_number": 1565,
      "title": "[Bug]:Found some deadlock log when insert rag",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\ndemo code as below:\n```python\nimport asyncio\nimport logging\nimport os\nimport time\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import openai_embed,openai_complete_if_cache\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nimport numpy as np\n\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nWORKING_DIR = f\"{CURRENT_DIR}/lightrag_data\"\n\nNAMESPACE = \"project2\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.DEBUG)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# PG\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\nos.environ[\"POSTGRES_HOST\"] = \"localhost\"\nos.environ[\"POSTGRES_PORT\"] = \"5432\"\nos.environ[\"POSTGRES_USER\"] = \"postgres\"\nos.environ[\"POSTGRES_PASSWORD\"] = \"\"\nos.environ[\"POSTGRES_DATABASE\"] = \"lightrag\"\n\n# neo4j\nos.environ[\"NEO4J_URI\"] = \"neo4j://localhost:7687\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"admin123\"\n\nasync def _llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        model=\"qwen-plus-latest\",\n        prompt=prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        **kwargs\n    )\n\nasync def _embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=\"text-embedding-v3\",\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\nasync def initialize_rag():\n    rag = LightRAG(\n        namespace_prefix=NAMESPACE,\n        working_dir=WORKING_DIR,\n        enable_llm_cache=True,\n        llm_model_func=_llm_model_func,\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=_embedding_func,\n        ),\n        embedding_batch_num=10,\n        embedding_func_max_async=10,\n        embedding_cache_config={\n            \"enabled\": \"true\",\n            \"similarity_threshold\": 0.95,\n            \"use_llm_check\": False,\n        },\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        # graph_storage=\"NetworkXStorage\",\n        vector_storage=\"MilvusVectorDBStorage\",\n        auto_manage_storages_states=True,\n        # llm_model_kwargs={\n        #     \"response_format\": {\"type\": \"json_object\"},\n        #     \"extra_body\": {\"enable_search\": True}\n        #     },\n        addon_params={\n            # \"language\": \"Simplified Chinese\",\n            \"language\": \"English\",\n        },\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def main():\n    # Initialize RAG instance\n    rag = await initialize_rag()\n\n\n    # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n    # await rag.adelete_by_doc_id(doc_id=FILE_NAME)\n    \n    start_time = time.time()\n    word_count = 0\n    \n    content1 = \"performance on a variety of professional and academic standards, even if it might not be as proficient as humans in everyday situations [13]. For example, compared to GPT-3.5, it has attained a score in the highest 10% of participants in tests on a virtual legal examination [12]. There remains an opportunity for enhancements to GPT-4's factuality, steerability, and remaining within the provided restrictions, but after six months of incremental alignment using lessons from OpenAI's adversarial evaluation programme and ChatGPT, the model achieved its best-ever efficiency [57].\"\n    \n    content2 = \"ChatGPT's utility in medicine and healthcare [14] lies in its ability to (i) aid in diagnosis by analysing patient records, health status, and indications to produce treatment strategies tailored to each patient's unique desires and requirements, (ii) summarise and synthesise clinical studies to support a researchbased practices, (iii) offer medical knowledge and guidance to patients in a simple and digestible manner, and (iv) encourage interaction among medical experts. Further, ChatGPT can be utilised to create robots that can aid with patient evaluation, which is the process by which medical professionals assess the severity of a patient's illness and decide what treatment is\"\n    \n    content3 = \"Due to its adaptability and superior NLP skills, ChatGPT has found use outside of the academic research community. In this section, we discuss how ChatGPT can be used in a variety of applications (as shown in Figure 3) illustrating the ways in which the platform can revolutionise business processes, bolster collaboration, and spark new ideas.\"\n    \n    inputs = [content1, content2, content3]\n    postions = [\"{'polygon': [[66.86399841308594, 281.00927734375], [242.998046875, 281.00927734375], [242.998046875, 200.75341796875], [66.86399841308594, 291.75341796875]]}\", \"{'polygon': [[33.86399841308594, 281.00927734375], [242.998046875, 281.00927734375], [242.998046875, 200.75341796875], [33.86399841308594, 200.75341796875]]}\", \"{'polygon': [[55.86399841308594, 55.00927734375], [55.998046875, 55.00927734375], [55.998046875, 55.75341796875], [55.86399841308594, 55.75341796875]]}\"]\n    \n    await rag.ainsert(input=inputs, file_paths=postions)\n    \n    end_time = time.time()\n    print(f\"Word count: {word_count}, insertion time: {end_time - start_time} seconds\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\nWhen I run the demo code, found some deadlock logs as below:\n```bash\nDEBUG:Using selector: KqueueSelector\nINFO: Process 85762 Shared-Data created for Single Process\nPostgreSQL database,\nsql:SELECT create_graph('project2chunk_entity_relation'),\ndata:None,\nerror:graph \"project2chunk_entity_relation\" already exists\nPostgreSQL database,\nsql:SELECT create_graph('project2chunk_entity_relation'),\ndata:None,\nerror:graph \"project2chunk_entity_relation\" already exists\nPostgreSQL database,\nsql:SELECT create_vlabel('project2chunk_entity_relation', 'base');,\ndata:None,\nerror:label \"base\" already exists\nPostgreSQL database,\nsql:SELECT create_elabel('project2chunk_entity_relation', 'DIRECTED');,\ndata:None,\nerror:label \"DIRECTED\" already exists\nKey value duplicate, but upsert succeeded.\nPostgreSQL database,\nsql:CREATE INDEX CONCURRENTLY edge_seid_idx ON project2chunk_entity_relation.\"_ag_label_edge\" (start_id,end_id),\ndata:None,\nerror:deadlock detected\nDETAIL:  Process 85783 waits for ShareLock on virtual transaction 6/1396; blocked by process 85785.\nProcess 85785 waits for ShareUpdateExclusiveLock on relation 35836 of database 18767; blocked by process 85783.\nHINT:  See server log for query details.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nKey value duplicate, but upsert succeeded.\nPostgreSQL database,\nsql:CREATE INDEX CONCURRENTLY directed_eid_idx ON project2chunk_entity_relation.\"DIRECTED\" (end_id),\ndata:None,\nerror:deadlock detected\nDETAIL:  Process 85785 waits for ShareUpdateExclusiveLock on relation 35855 of database 18767; blocked by process 85783.\nProcess 85783 waits for ShareLock on virtual transaction 6/1412; blocked by process 85785.\nHINT:  See server log for query details.\nPostgreSQL database,\nsql:CREATE INDEX CONCURRENTLY directed_sid_idx ON project2chunk_entity_relation.\"DIRECTED\" (start_id),\ndata:None,\nerror:deadlock detected\nDETAIL:  Process 85783 waits for ShareUpdateExclusiveLock on relation 35855 of database 18767; blocked by process 85785.\nProcess 85785 waits for ShareLock on virtual transaction 5/4222; blocked by process 85783.\nHINT:  See server log for query details.\nPostgreSQL database,\nsql:CREATE INDEX CONCURRENTLY directed_seid_idx ON project2chunk_entity_relation.\"DIRECTED\" (start_id,end_id),\ndata:None,\nerror:deadlock detected\nDETAIL:  Process 85785 waits for ShareUpdateExclusiveLock on relation 35855 of database 18767; blocked by process 85783.\nProcess 85783 waits for ShareLock on virtual transaction 6/1423; blocked by process 85785.\nHINT:  See server log for query details.\nPostgreSQL database,\nsql:CREATE INDEX CONCURRENTLY entity_p_idx ON project2chunk_entity_relation.\"base\" (id),\ndata:None,\nerror:deadlock detected\nDETAIL:  Process 85783 waits for ShareUpdateExclusiveLock on relation 35846 of database 18767; blocked by process 85785.\nProcess 85785 waits for ShareLock on virtual transaction 5/4233; blocked by process 85783.\nHINT:  See server log for query details.\nPostgreSQL database,\nsql:CREATE INDEX CONCURRENTLY entity_idx_node_id ON project2chunk_entity_relation.\"base\" (ag_catalog.agtype_access_operator(properties, '\"entity_id\"'::agtype)),\ndata:None,\nerror:deadlock detected\nDETAIL:  Process 85785 waits for ShareUpdateExclusiveLock on relation 35846 of database 18767; blocked by process 85783.\nProcess 85783 waits for ShareLock on virtual transaction 6/1434; blocked by process 85785.\nHINT:  See server log for query details.\nPostgreSQL database,\nsql:CREATE INDEX CONCURRENTLY entity_node_id_gin_idx ON project2chunk_entity_relation.\"base\" using gin(properties),\ndata:None,\nerror:deadlock detected\nDETAIL:  Process 85783 waits for ShareUpdateExclusiveLock on relation 35846 of database 18767; blocked by process 85785.\nProcess 85785 waits for ShareLock on virtual transaction 5/4244; blocked by process 85783.\nHINT:  See server log for query details.\nINFO: Process 85762 Pipeline namespace initialized\nkeys: {'doc-81486a3a7df9ae052078d5db5e65402c', 'doc-c4cec84dedd8553129b30c489455e49a', 'doc-43d68698f0b0e530b342999908cf44d6'}\nnew_keys: set()\nWord count: 0, insertion time: 0.04132199287414551 seconds\nE20250512 21:33:09.730710 3075196 server.cpp:47] [SERVER][BlockLock][] Process exit\n```\n\nAre there any bugs for the PG storage?\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: master\n- Operating System: macos 15.3.1 (24D70)\n- Python Version: Python 3.10.16\n- Related Issues: PG deadlocks\n",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-05-12T13:40:41Z",
      "updated_at": "2025-05-13T13:29:18Z",
      "closed_at": "2025-05-13T13:29:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1565/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1565",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1565",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:18.455476",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The deadlock appears to occur when multiple workers attempt to create indexes on the same table simultaneously. Please confirm whether the script was launched in a multi-process mode or if another service instance was previously started. If the issue persists, restart the machine and the database to",
          "created_at": "2025-05-12T15:53:24Z"
        }
      ]
    },
    {
      "issue_number": 1563,
      "title": "[Bug]: Embedding does not work if you use EmbeddingFunc",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nEmbedding is not called if you use the EmbeddingFunc wrapper. If you pass the `openai_embed` function to the  `embedding_func` parameter directly, everything is ok and this function is called\n\n### Steps to reproduce\n\n```python\nasync def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):\n    try:\n        if 'llm_instance' not in kwargs:\n            llm_instance = llm\n            kwargs['llm_instance'] = llm_instance\n\n        response = await llama_index_complete_if_cache(\n            kwargs['llm_instance'],\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            **kwargs,\n        )\n        return response\n    except Exception as e:\n        print(f'LLM request failed: {str(e)}')\n        raise\n\n\nasync def embedding_func(texts):\n    try:\n        return await llama_index_embed(texts, embed_model=embedding)\n    except Exception as e:\n        print(f'Embedding request failed: {str(e)}')\n        raise\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir='kg_lightrag',\n        chunk_token_size=1200,\n        chunk_overlap_token_size=50,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1536,\n            max_token_size=8192,\n            func=embedding_func,\n        ),\n        llm_model_func=llm_model_func,\n        llm_model_max_async=8,\n        embedding_func_max_async=16,\n        max_parallel_insert=8,\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n```\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n_No response_\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.3.6\n- Operating System: Windows 11\n- Python Version: 3.11\n",
      "state": "open",
      "author": "MarkHmnv",
      "author_type": "User",
      "created_at": "2025-05-12T10:38:51Z",
      "updated_at": "2025-05-13T09:53:14Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1563/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1563",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1563",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:18.656565",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The LightRAG object applies a concurrency-limiting decorator to the embedding_func attribute, which may not handle the passed-in objects properly. I have made improvements to the program; please pull the latest code and test it again. If the issue persists, kindly modify the code by removing the Emb",
          "created_at": "2025-05-12T18:12:39Z"
        },
        {
          "author": "MarkHmnv",
          "body": "> The LightRAG object applies a concurrency-limiting decorator to the embedding_func attribute, which may not handle the passed-in objects properly. I have made improvements to the program; please pull the latest code and test it again. If the issue persists, kindly modify the code by removing the E",
          "created_at": "2025-05-13T07:07:18Z"
        },
        {
          "author": "danielaskdd",
          "body": "Try to pass the function in this way:\n\n```\nmy_embedding_func =  EmbeddingFunc(\n            embedding_dim=1536,\n            max_token_size=8192,\n            func=embedding_func,\n        )\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir='kg_lightrag',\n        chunk_token_size=1200",
          "created_at": "2025-05-13T08:04:38Z"
        },
        {
          "author": "MarkHmnv",
          "body": "> Try to pass the function in this way:\n> \n> ```\n> my_embedding_func =  EmbeddingFunc(\n>             embedding_dim=1536,\n>             max_token_size=8192,\n>             func=embedding_func,\n>         )\n> \n> async def initialize_rag():\n>     rag = LightRAG(\n>         working_dir='kg_lightrag',\n>    ",
          "created_at": "2025-05-13T08:10:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please pull the latest version of the repository and check if you can make `lightrag_openai_compatible_demo.py` work as expected.",
          "created_at": "2025-05-13T09:53:13Z"
        }
      ]
    },
    {
      "issue_number": 1509,
      "title": "[Bug]: rag.insert() not working for local ollama",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nFrom last 3 weeks, i am trying to figure out what's wrong in rag.insert(). I am using book.txt, ollama with qwen3:4b, nomic-embed-text, all running in local with 8gb 4060 ram, on windows 11. I have tried python 13, 12, 10 versions. When the text in book.txt has 1 page, i see \"/embed\" in ollama logs, but when the text is longer than a page, i only find \"/chat\" 98% of the time. Also lightrag is not consistent with outputs. I have tried changing num_ctx parameters from 1024 to 20480, but no use.  Most of the time when this logs appears \"INFO: Process 7640 Pipeline namespace initialized\" gets stuck and I get this message on ollama msg=\"gpu VRAM usage didn't recover within timeout\", after which if we check ollama ps, it only shows llm model like qwen3:4b running and does not show the embedding model.\n\n### Steps to reproduce\n\nrun lightag on 8gb vram card with python 3.12.9 in a virtual environment using the steps mentioned in HKUDS / Lightrag 1.3.6. Run embed models and LLM lcoally on ollama . Test ./examples/lightrag_ollama_demo.py file using book.txt file.\n\n### Expected Behavior\n\nI should have seen '/embed' calls in ollama. \n\n### LightRAG Config Used\n\n# Paste your config here\nimport asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\nimport os\nimport inspect\nimport logging\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.ollama import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def initialize_rag():\n    print(\"Initialise rag:\")\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"qwen3:4b\",\n        llm_model_max_async=1,\n        llm_model_max_token_size=8192,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",\n            \"options\": {\"num_ctx\": 8192},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=1024,\n            func=lambda texts: ollama_embed(\n                texts, embed_model=\"nomic-embed-text:latest\", host=\"http://localhost:11434\"\n            ),\n        ),\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def print_stream(stream):\n    print(\"Initialise print stream:\")\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\ndef main():\n    # Initialize RAG instance\n    print(\"First\")\n    rag = asyncio.run(initialize_rag())\n    print(\"Second\")\n\n    # Insert example text\n    with open(\"./examples/book.txt\", \"r\", encoding=\"utf-8\") as f:\n        print(\"Insert text\")\n        rag.insert(f.read())\n\n    print(\"Third\")\n    # Test different query modes\n    print(\"\\nNaive Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n\n    print(\"\\nLocal Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\n\n    print(\"\\nGlobal Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n\n    print(\"\\nHybrid Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n\n    # stream response\n    resp = rag.query(\n        \"What are the top themes in this story?\",\n        param=QueryParam(mode=\"hybrid\", stream=True),\n    )\n\n    if inspect.isasyncgen(resp):\n        asyncio.run(print_stream(resp))\n    else:\n        print(resp)\n\n\nif __name__ == \"__main__\":\n    main()\n\n### Logs and screenshots\n\n<img width=\"1012\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/83ea7ce2-cfaa-4649-af1b-3707ee6c55c7\" />\n\n[server.log](https://github.com/user-attachments/files/20023423/server.log)\n\n### Additional Information\n\n- LightRAG Version: 1.3.6\n- Operating System: Windows 11\n- Python Version: 3.12.9\n- Related Issues:\n",
      "state": "open",
      "author": "NavneetKamboj",
      "author_type": "User",
      "created_at": "2025-05-03T13:12:04Z",
      "updated_at": "2025-05-13T07:00:52Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1509/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1509",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1509",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:18.852435",
      "comments": [
        {
          "author": "NavneetKamboj",
          "body": "i have tried with other models as well gemma1.5b, granite embed, llama 3b etc.\n",
          "created_at": "2025-05-03T13:15:02Z"
        },
        {
          "author": "danielaskdd",
          "body": "LightRAG requires LLM with a minimum of 32B parameters and 32K context window to work properly.",
          "created_at": "2025-05-03T16:42:48Z"
        },
        {
          "author": "steelliberty",
          "body": "NavneetKamboj:\n\nI was not able to get nomic-embed-text:latest to work even when I created a copy of the LLM with expanded parameters.  However I have this working fine with Ollama. bge-m3:latest -- pulling that model and it setup as below and works well. Leaving lastest off the embed_model was neces",
          "created_at": "2025-05-05T17:44:34Z"
        },
        {
          "author": "danielaskdd",
          "body": "After embedding model is changed, you  should clear all the data before uploading new file.",
          "created_at": "2025-05-05T23:17:48Z"
        },
        {
          "author": "DarioRepoRuler",
          "body": "I had the same issue and downgraded to tag v1.0.3 (the oldest listed) then it worked for me.\n ",
          "created_at": "2025-05-13T07:00:51Z"
        }
      ]
    },
    {
      "issue_number": 1560,
      "title": "[Bug]: Failed to run query with gemini example",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen trying to use the gemini example got an unexpected internal bug\n\n### Steps to reproduce\n\nCode used to create bug:\n\n```py\nimport os\nimport numpy as np\nfrom google import genai\nfrom google.genai import types\nfrom dotenv import load_dotenv\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag import LightRAG, QueryParam\nfrom sentence_transformers import SentenceTransformer\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\nimport asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nload_dotenv()\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nWORKING_DIR = \"./lightrag\"\n\nif os.path.exists(WORKING_DIR):\n    import shutil\n\n    shutil.rmtree(WORKING_DIR)\n\nos.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    client = genai.Client(api_key=gemini_api_key)\n    if history_messages is None:\n        history_messages = []\n\n    combined_prompt = \"\"\n    if system_prompt:\n        combined_prompt += f\"{system_prompt}\\n\"\n\n    for msg in history_messages:\n        combined_prompt += f\"{msg['role']}: {msg['content']}\\n\"\n\n    combined_prompt += f\"user: {prompt}\"\n\n    response = client.models.generate_content(\n        model=\"gemini-1.5-flash\",\n        contents=[combined_prompt],\n        config=types.GenerateContentConfig(max_output_tokens=500, temperature=0.1),\n    )\n\n    return response.text\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    embeddings = model.encode(texts, convert_to_numpy=True)\n    return embeddings\n\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=384,\n            max_token_size=8192,\n            func=embedding_func,\n        ),\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\ndef main():\n    rag = asyncio.run(initialize_rag())\n\n    nr_dir = \"jobs/data/nr\"\n    all_texts = []\n\n    for filename in sorted(os.listdir(nr_dir)):\n        if filename.endswith(\".txt\"):\n            file_path = os.path.join(nr_dir, filename)\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read().strip()\n                all_texts.append(content)\n\n    combined_text = \"\\n\\n\\n\".join(all_texts)\n\n    rag.insert(combined_text)\n\n    response = rag.query(\n        query=\"What is the main theme of these documents?\",\n        param=QueryParam(mode=\"hybrid\", top_k=5, response_type=\"single line\"),\n    )\n\n    print(response)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Expected Behavior\n\nQuery should've worked\n\n### LightRAG Config Used\n\n```py\n rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=384,\n            max_token_size=8192,\n            func=embedding_func,\n        ),\n    )\n```\n\n### Logs and screenshots\n\n\n\nFile \"/Users/gfffrtt/projects/legislation-search/.venv/lib/python3.12/site-packages/lightrag/utils.py\", line 805, in process_combine_contexts\n    for item in hl_context + ll_context:\n                ~~~~~~~~~~~^~~~~~~~~~~~\nTypeError: can only concatenate str (not \"list\") to str\nException ignored in: <function LightRAG.__del__ at 0x107a5b740>\nTraceback (most recent call last):\n  File \"/Users/gfffrtt/projects/legislation-search/.venv/lib/python3.12/site-packages/lightrag/lightrag.py\", line 444, in __del__\n  File \"/Users/gfffrtt/projects/legislation-search/.venv/lib/python3.12/site-packages/lightrag/lightrag.py\", line 456, in _run_async_safely\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.10/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n  File \"/Users/gfffrtt/projects/legislation-search/.venv/lib/python3.12/site-packages/lightrag/lightrag.py\", line 506, in finalize_storages\n  File \"/Users/gfffrtt/projects/legislation-search/.venv/lib/python3.12/site-packages/lightrag/kg/json_kv_impl.py\", line 206, in finalize\n  File \"/Users/gfffrtt/projects/legislation-search/.venv/lib/python3.12/site-packages/lightrag/kg/json_kv_impl.py\", line 85, in index_done_callback\n  File \"/Users/gfffrtt/projects/legislation-search/.venv/lib/python3.12/site-packages/lightrag/utils.py\", line 575, in write_json\nNameError: name 'open' is not defined\n\n### Additional Information\n\n- LightRAG Version: 1.3.6\n- Operating System: MacOS\n- Python Version: 3.12\n- Related Issues: \n",
      "state": "open",
      "author": "gfffrtt",
      "author_type": "User",
      "created_at": "2025-05-11T22:08:01Z",
      "updated_at": "2025-05-12T04:12:01Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1560/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1560",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1560",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:19.030025",
      "comments": [
        {
          "author": "gfffrtt",
          "body": "Just some extra context, this only happens when I try to run LightRAG with pre-existing storage, like just initing it and querying it.\n\nThis for some reason doesn't happen when I do my .insert()",
          "created_at": "2025-05-11T22:56:34Z"
        },
        {
          "author": "gfffrtt",
          "body": "I've altered the code in the package to make it work:\n\nEssentially, the `hl_context` was a string for some reason, might indicate a deeper problem\nSo I added checks to manually set them to lists if they were strings\n\n```py\ndef process_combine_contexts(\n    hl_context: list[dict[str, str]] = [], ll_c",
          "created_at": "2025-05-12T00:08:52Z"
        },
        {
          "author": "danielaskdd",
          "body": "### `contents` Parameter Construction Issue\n\n Google's Gemini API, expect the contents (or a similar parameter like messages) to be a structured list, where each element in the list represents a turn or a part of the conversation, typically with a specified role (e.g., \"user\", \"model\") and its corre",
          "created_at": "2025-05-12T03:10:06Z"
        },
        {
          "author": "danielaskdd",
          "body": "### Asynchronous Call Issue (async and await)\n\n* Problem: The function llm_model_func is defined as an async function (using async def), which suggests it's intended for non-blocking asynchronous operations. However, the line response = client.models.generate_content(...) appears to be a standard sy",
          "created_at": "2025-05-12T03:12:02Z"
        },
        {
          "author": "danielaskdd",
          "body": "Currently, LightRAG does not provide an official Gemini LLM interface implementation. If you are unable to develop the Gemini interface yourself, it is recommended to use third-party LLM aggregation services such as openrouter.ai. These aggregation services offer OpenAI-compatible interfaces for all",
          "created_at": "2025-05-12T04:02:04Z"
        }
      ]
    },
    {
      "issue_number": 1562,
      "title": "[Question]:Support for Elasticsearch as a Vector Store?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThank you for your great contribution!\n\nI'm wondering if there are any plans to support Elasticsearch as a vector store in LightRAG? It would be helpful for teams already using Elasticsearch in their stack.\n\n\n### Additional Context\n\nWe are currently evaluating LightRAG and plan to integrate it into our existing pipeline, which already uses Elasticsearch for search and vector storage. Native support would be greatly appreciated.\n",
      "state": "open",
      "author": "HansonnnCheung",
      "author_type": "User",
      "created_at": "2025-05-12T02:33:39Z",
      "updated_at": "2025-05-12T03:44:02Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1562/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1562",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1562",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:19.227092",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The underlying storage implementation of LightRAG can be extended through storage drivers. LightRAG primarily uses three types of storage: KV, Vector, and vector storage. Each type of storage supports several implementation methods. Since there are many important features that still need to be impro",
          "created_at": "2025-05-12T03:44:01Z"
        }
      ]
    },
    {
      "issue_number": 1522,
      "title": "Suggest making the API into an MCP server",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nThe service protocol of MCP supports multiple interfaces and can open up services for clients to implement graph writing and query services.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "smileyboy2019",
      "author_type": "User",
      "created_at": "2025-05-05T08:35:48Z",
      "updated_at": "2025-05-12T02:58:20Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1522/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1522",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1522",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:19.458708",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "There are several points that require clarification:\n\n- What functionalities need to be integrated into the MCP server?\n- What is the appropriate approach for introducing the MCP server to the LLM, considering that each application integrates the LightRAG server for different purposes?",
          "created_at": "2025-05-05T11:24:07Z"
        },
        {
          "author": "zhuyanhuazhuyanhua",
          "body": "MCP 服务器（Multi - Client Proxy Server，多客户端代理服务器）：\n\nConnection management: managing connections to multiple clients (like LLM, LightRAG servers, etc.), including connection establishment, disconnection, and heartbeat detection.\nRequest routing: routing requests to the corresponding service components b",
          "created_at": "2025-05-05T13:30:31Z"
        },
        {
          "author": "BireleyX",
          "body": "I don't think converting the existing API to MCP server is necessary or even the correct approach. MCP servers are API wrappers... to make the interaction with AI consistent.\nI was also thinking about using MCP with Lightrag for  my project, luckily I already found an existing project:\n[https://gith",
          "created_at": "2025-05-08T08:50:56Z"
        },
        {
          "author": "choizhang",
          "body": "LLM in natural language dialogue currently cannot determine whether a problem requires a local knowledge base, a direct connection to LLM, or online search through intent recognition. Server memory is identified through Custom Instructions starting with 'please remember' and is not accurate, while m",
          "created_at": "2025-05-12T02:58:19Z"
        }
      ]
    },
    {
      "issue_number": 1312,
      "title": "[Bug]: Query vector data not support ids param with MilvusVectorDBStorage",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I use the rag.query with ids params and using Milvus for vector data storage, it doesn't work\n```python\nawait rag.aquery(query=query, param=QueryParam(mode=\"mix\", ids=['1', '2']))\n```\n\nAnd I checked the source code of MilvusVectorDBStorage:\n```python\nclass MilvusVectorDBStorage(BaseVectorStorage):\n    async def query(\n        self, query: str, top_k: int, ids: list[str] | None = None\n    ) -> list[dict[str, Any]]:\n        embedding = await self.embedding_func([query])\n        results = self._client.search(\n            collection_name=self.namespace,\n            data=embedding,\n            limit=top_k,\n            output_fields=list(self.meta_fields),\n            search_params={\n                \"metric_type\": \"COSINE\",\n                \"params\": {\"radius\": self.cosine_better_than_threshold},\n            },\n        )\n        print(results)\n        return [\n            {**dp[\"entity\"], \"id\": dp[\"id\"], \"distance\": dp[\"distance\"]}\n            for dp in results[0]\n        ]\n```\n\nThe query function doesn't use the ids, is that meaning that the ids param doesn't work for Milvus query?\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-04-08T08:59:52Z",
      "updated_at": "2025-05-11T18:40:59Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1312/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1312",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1312",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:19.635992",
      "comments": [
        {
          "author": "johnshearing",
          "body": "I experience the same issue. \nThe \"ids\" filter is ignored in all modes [\"naive\", \"local\", \"global\", 'hybrid', \"mix\"] \nIt would be amazing if this worked. \nIn any case, LightRAG is amazing. \nMuch thanks.",
          "created_at": "2025-05-11T18:40:58Z"
        }
      ]
    },
    {
      "issue_number": 1557,
      "title": "[Bug]:PostgreSQL KV query issue",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nAfter the latest update of v1.3.7, the query crashed on the \"weight\" and \"descriptions\" keys.\n\n`INFO: limit_async: 16 new workers initialized\nDEBUG: Truncate chunks from 14 to 4 (max tokens:4000)\nTraceback (most recent call last):\n  File \"/Users/mykolachaban/Documents/Work/WorkMaterials/AI Related/lightrag-tests/main.py\", line 190, in retrive_knowledge\n    result = await rag.aquery(\n             ^^^^^^^^^^^^^^^^^\n  File \"/Users/mykolachaban/Documents/Work/WorkMaterials/AI Related/lightrag-tests/.venv/lib/python3.12/site-packages/lightrag/lightrag.py\", line 1443, in aquery\n    response = await kg_query(\n               ^^^^^^^^^^^^^^^\n  File \"/Users/mykolachaban/Documents/Work/WorkMaterials/AI Related/lightrag-tests/.venv/lib/python3.12/site-packages/lightrag/operate.py\", line 917, in kg_query\n    context = await _build_query_context(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mykolachaban/Documents/Work/WorkMaterials/AI Related/lightrag-tests/.venv/lib/python3.12/site-packages/lightrag/operate.py\", line 1243, in _build_query_context\n    ll_data = await _get_node_data(\n              ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mykolachaban/Documents/Work/WorkMaterials/AI Related/lightrag-tests/.venv/lib/python3.12/site-packages/lightrag/operate.py\", line 1391, in _get_node_data\n    use_relations = await _find_most_related_edges_from_entities(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mykolachaban/Documents/Work/WorkMaterials/AI Related/lightrag-tests/.venv/lib/python3.12/site-packages/lightrag/operate.py\", line 1620, in _find_most_related_edges_from_entities\n    all_edges_data = sorted(\n                     ^^^^^^^\n  File \"/Users/mykolachaban/Documents/Work/WorkMaterials/AI Related/lightrag-tests/.venv/lib/python3.12/site-packages/lightrag/operate.py\", line 1621, in <lambda>\n    all_edges_data, key=lambda x: (x[\"rank\"], x[\"weight\"]), reverse=True\n                                              ~^^^^^^^^^^\nKeyError: 'weight'\n\nError retrieving knowledge graph: 'weight'`\n\nIs there any solution to resolve this?\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "OxidBurn",
      "author_type": "User",
      "created_at": "2025-05-10T19:18:26Z",
      "updated_at": "2025-05-11T10:52:32Z",
      "closed_at": "2025-05-11T10:52:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1557/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1557",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1557",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:19.848107",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "It appears that the “weight” attribute is missing from the edge properties in the graph storage.\n\nThe latest version is designed to handle edge data without the “weight” attribute and will prevent a KeyError from occurring. Please update to the latest version and review the log for any warning messa",
          "created_at": "2025-05-11T03:24:00Z"
        },
        {
          "author": "OxidBurn",
          "body": "Thank you in advance @danielaskdd ",
          "created_at": "2025-05-11T10:52:29Z"
        }
      ]
    },
    {
      "issue_number": 970,
      "title": "[Question]: <title>Failed to load module script: Expected a JavaScript module script but the server responded with a MIME type of \"text/plain\". Strict MIME type checking is enforced for module scripts per HTML spec.",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nCan anyone tell me why webui is getting an error\n\n\n\n### Additional Context\n\n<!-- Failed to upload \"屏幕截图 2025-03-01 181033.png\" -->",
      "state": "open",
      "author": "jvzihaochi",
      "author_type": "User",
      "created_at": "2025-03-01T10:12:00Z",
      "updated_at": "2025-05-11T08:57:30Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/970/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/970",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/970",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:20.049082",
      "comments": [
        {
          "author": "jvzihaochi",
          "body": "![Image](https://github.com/user-attachments/assets/eba05be2-06e7-46df-9519-1f79ae52a3da)",
          "created_at": "2025-03-01T10:13:52Z"
        },
        {
          "author": "mengdeer589",
          "body": "虽然不清楚为啥会出现这个问题，但是解法如下：修改.venv\\Lib\\site-packages\\lightrag\\lightrag.py，476行\n`    class NoCacheStaticFiles(StaticFiles):\n        async def get_response(self, path: str, scope):\n            response = await super().get_response(path, scope)\n            if path.endswith(\".js\"):\n                response.h",
          "created_at": "2025-05-11T08:54:38Z"
        }
      ]
    },
    {
      "issue_number": 1556,
      "title": "[Bug]:README中的“注意1”里面有错别字",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n将“可以在清除数据目录是保留kv_store_llm_response_cache.json文件。”中的是，改成时\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "476820505",
      "author_type": "User",
      "created_at": "2025-05-10T09:04:56Z",
      "updated_at": "2025-05-11T02:23:03Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1556/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1556",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1556",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:20.272747",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thanks for reporting",
          "created_at": "2025-05-11T02:23:03Z"
        }
      ]
    },
    {
      "issue_number": 1542,
      "title": "[Question]: Why hasn't MongoGraphStorage been implemented yet? I noticed it's commented out — is there a specific reason for that?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n![Image](https://github.com/user-attachments/assets/6fe8171d-29a6-4171-bc01-5818dc840d50)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-05-07T09:51:29Z",
      "updated_at": "2025-05-09T17:35:59Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1542/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1542",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1542",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:20.479530",
      "comments": [
        {
          "author": "acsangamnerkar",
          "body": "@FeHuynhVI I had a PR for updating the Mongograph but @danielaskdd had to test it end to end. Look at the PR and if you want the updated code, I can share it with you. I am really busy at work right now so dont have bandwidth. ",
          "created_at": "2025-05-09T15:00:34Z"
        },
        {
          "author": "danielaskdd",
          "body": "I apologize for the delay as I have been attending to several urgent matters recently. I plan to review and test this PR next week.",
          "created_at": "2025-05-09T17:35:59Z"
        }
      ]
    },
    {
      "issue_number": 1544,
      "title": "[Question]: Clarification on Chunk Processing Order and Logic After Failure",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI’d like to request some clarification on how LightRAG handles chunk and document processing, particularly in scenarios involving multiple documents and recovery after failed executions.\n\nWhile processing multiple documents, I observed that the script doesn’t fully process one document before moving to the next. Instead, it processes some chunks from the first document, then switches to the second, then back to the first, and so on. Is this expected behavior? Is chunk processing done in parallel across documents, or is parallel processing done document-wise?\n\nAlso In a previous run, the process failed midway and execution was stopped. . I restarted the execution and as per the logic implemented in the LIGHTRAG scripts it checks for the file status: if it is marked as failed, it should be reprocessed right? Also the chunks had already been created in the earlier run, and some chunks had already been processed for entity and relation extraction and stored in llm_cache_response, does LightRAG automatically skips those chunks, or does it process the entire document only again after failure occurs?\n\nThanks for building a great tool! Looking forward to your insights on this.\n\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "martina0211",
      "author_type": "User",
      "created_at": "2025-05-07T17:04:30Z",
      "updated_at": "2025-05-09T13:06:25Z",
      "closed_at": null,
      "labels": [
        "question",
        "information",
        "discuss"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1544/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1544",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1544",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:20.662546",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The document processing pipeline in LightRAG is somewhat complex and is divided into two primary stages: the Extraction stage (entity and relationship extraction) and the Merging stage (entity and relationship merging). There are two key parameters that control pipeline concurrency: the maximum numb",
          "created_at": "2025-05-09T05:31:10Z"
        },
        {
          "author": "Utkarshagharat58",
          "body": "Thanks a lot for the clarification—it really helped clear things up. I appreciate your support!\nI just have one more question—\nEven though you mentioned that inserting custom chunks is now deprecated in the script, is there still a way I can add my own chunks (with page numbers and some extra metada",
          "created_at": "2025-05-09T10:53:50Z"
        },
        {
          "author": "danielaskdd",
          "body": "You can use `insert` to add custom chunks instead of the deprecated function, which requires the `file_paths` parameter for compatibility.",
          "created_at": "2025-05-09T13:06:24Z"
        }
      ]
    },
    {
      "issue_number": 1452,
      "title": "[Feature Request]: Instead of using Docling library in the solution use Docling-serve to offload document processing",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nAs of not the document ingestion option of Docling is using the python library. It does increase the time to build and run the solution. Also, the initialization with the first document takes a long time.\n\nMy proposal will be to use the default option as it is but use the docling-serve as a container and add the URL of the docling server as a ENV config. \n\n[https://github.com/docling-project/docling-serve?tab=readme-ov-file](https://github.com/docling-project/docling-serve?tab=readme-ov-file)\n\nThis will also give you an option of running the docling API on CPU or GPU as per  requireemnt.\n\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "acsangamnerkar",
      "author_type": "User",
      "created_at": "2025-04-25T00:28:01Z",
      "updated_at": "2025-05-08T09:33:26Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1452/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1452",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1452",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:20.829165",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thanks for you suggestion.",
          "created_at": "2025-04-26T00:27:29Z"
        },
        {
          "author": "BireleyX",
          "body": "This is a very good feature request! was deciding to use docling service on a separate server in conjuction with Lightrag.",
          "created_at": "2025-05-08T09:33:26Z"
        }
      ]
    },
    {
      "issue_number": 1398,
      "title": "[Feature Request]: Keywords deduplication on edge merge stage",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nKeywords deduplication on edge merge stage\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-17T14:50:25Z",
      "updated_at": "2025-05-08T08:11:02Z",
      "closed_at": "2025-05-08T08:11:01Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1398/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "LarFii"
      ],
      "milestone": "v1.3.8",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1398",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1398",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:21.036538",
      "comments": [
        {
          "author": "zhouzhou12",
          "body": "Looking forward to this feature! When is it planned to be released? @danielaskdd ",
          "created_at": "2025-04-21T03:06:13Z"
        },
        {
          "author": "danielaskdd",
          "body": "Already on schedule, will be available in a few weeks.",
          "created_at": "2025-04-21T03:33:20Z"
        }
      ]
    },
    {
      "issue_number": 1289,
      "title": "[Feature Request]: Add multiple workspace support for LightRAG Server",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nDetailed requirements are pending supplementation. Please discuss the relevant features and technical implementation methods in the following link: https://github.com/HKUDS/LightRAG/discussions/1016\n\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-06T19:35:49Z",
      "updated_at": "2025-05-08T01:32:02Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1289/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.5.0",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1289",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1289",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:21.255890",
      "comments": []
    },
    {
      "issue_number": 1343,
      "title": "[Feature Request]: Merge two or more nodes by webui",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n### Merge two or more nodes by webui\n\n- Add candidate node to clipboard\n- Open clipboard and select the nodes to be merged\n- Click merged button, merge selected nodes and relevant edges\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-10T19:55:06Z",
      "updated_at": "2025-05-08T01:28:47Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "ui"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1343/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.4.5",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1343",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1343",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:21.255911",
      "comments": [
        {
          "author": "choizhang",
          "body": "### I have an idea as follows, or do you have a better way？\n\n1. Select a node and open the property panel\n<img width=\"374\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/13e4bbe0-19af-408b-82a7-9e25a98dd6dd\" />\n\n2. After clicking the icon, open a pop-up window\n\n<img width=\"374\" alt=\"Ima",
          "created_at": "2025-04-14T08:12:15Z"
        },
        {
          "author": "danielaskdd",
          "body": "I believe we need a more convenient and sustainable operational environment for merging, adding, and deleting nodes. My preliminary ideas are:  \n\n1. Add a **Nodes Clipboard** feature to Graphviewer: Responsible for collecting nodes that require subsequent processing.  \n2. Allow adding the current no",
          "created_at": "2025-04-14T10:57:49Z"
        },
        {
          "author": "danielaskdd",
          "body": "@LarFii any advice on this issue?",
          "created_at": "2025-04-14T10:59:45Z"
        },
        {
          "author": "danielaskdd",
          "body": "To streamline the merging process, we will leverage LLM to automatically generate consolidated node/edge data. For scenarios involving more than 10 nodes, a map-reduce approach will be implemented in the background to ensure scalability. Note that merging tow nodes with lots of connected  edges,  re",
          "created_at": "2025-04-14T11:07:57Z"
        }
      ]
    },
    {
      "issue_number": 1356,
      "title": "[Feature Request]: Add Quickstart Docker Compose & .env Setup for lightRAG Deployment for OpenAI, Milvus, Redis, MongoDB & Neo4j",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n**Issue/Feature Description:**  \n\nFirst off, I want to express my gratitude for lightRAG—it’s been a fantastic tool for my projects. I'd like to contribute a quickstart setup that makes it even easier for users to deploy lightRAG using Docker. This setup supports integration with OpenAI, Milvus (as Vector DB), Redis (for KV Storage), MongoDB (for Document Storage), and Neo4j (for Graph DB).\n\n### Contribution Details\n\nI propose adding a docker-compose file and an example .env file to the repository. These files will serve as a quickstart guide for people to get lightRAG up and running with all the required storage services.  \n\n**Docker Compose file:**\n```yaml\nservices:\n  lightrag:\n    build: .\n    ports:\n      - \"${PORT:-9621}:9621\"\n    volumes:\n      - ./data/rag_storage:/app/data/rag_storage\n      - ./data/inputs:/app/data/inputs\n      - ./config.ini:/app/config.ini\n      - ./.env:/app/.env\n    env_file:\n      - .env\n    restart: unless-stopped\n    depends_on:\n      - redis\n      - mongo\n      - milvus\n      - neo4j\n\n  redis:\n    image: redis:7.4.2-alpine3.21\n    container_name: lightrag-server_redis\n    restart: always\n    ports:\n      - \"6379:6379\"  # Exposes container's port 6379 on host's port 6379\n    volumes:\n      - lightrag_redis_data:/data\n\n  neo4j:\n    image: neo4j:5.26.4-community\n    container_name: lightrag-server_neo4j-community\n    restart: always\n    ports:\n      - \"7474:7474\"\n      - \"7687:7687\"\n    environment:\n      - NEO4J_AUTH=${NEO4J_USERNAME}/${NEO4J_PASSWORD}\n      - NEO4J_apoc_export_file_enabled=true\n      - NEO4J_server_bolt_listen__address=0.0.0.0:7687\n      - NEO4J_server_bolt_advertised__address=neo4j:7687\n    volumes:\n      - ./neo4j/plugins:/var/lib/neo4j/plugins  # This is something I did because for neo4j you need to download the APOC file.\n      - lightrag_neo4j_import:/var/lib/neo4j/import\n      - lightrag_neo4j_data:/data\n      - lightrag_neo4j_backups:/backups\n\n  etcd: # etcd, minio and milvus are just copy pasted from their own docker compose file.\n    container_name: lightrag-server_milvus-etcd\n    image: quay.io/coreos/etcd:v3.5.5\n    environment:\n      - ETCD_AUTO_COMPACTION_MODE=revision\n      - ETCD_AUTO_COMPACTION_RETENTION=1000\n      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n      - ETCD_SNAPSHOT_COUNT=50000\n    volumes:\n      - lightrag_etcd_data:/etcd\n    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n    healthcheck:\n      test: [\"CMD\", \"etcdctl\", \"endpoint\", \"health\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n\n  minio:\n    container_name: lightrag-server_milvus-minio\n    image: minio/minio:RELEASE.2023-03-20T20-16-18Z\n    environment:\n      MINIO_ACCESS_KEY: minioadmin\n      MINIO_SECRET_KEY: minioadmin\n    ports:\n      - \"9001:9001\"\n      - \"9000:9000\"\n    volumes:\n      - lightrag_minio_data:/minio_data\n    command: minio server /minio_data --console-address \":9001\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n\n  milvus:\n    container_name: lightrag-server_milvus-standalone\n    image: milvusdb/milvus:v2.4.15\n    command: [\"milvus\", \"run\", \"standalone\"]\n    security_opt:\n      - seccomp:unconfined\n    environment:\n      ETCD_ENDPOINTS: etcd:2379\n      MINIO_ADDRESS: minio:9000\n    volumes:\n      - lightrag_milvus_data:/var/lib/milvus\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9091/healthz\"]\n      interval: 30s\n      start_period: 90s\n      timeout: 20s\n      retries: 3\n    ports:\n      - \"19530:19530\"\n      - \"9091:9091\"\n    depends_on:\n      - etcd\n      - minio\n\n  mongo:\n    image: mongodb/mongodb-community-server:7.0.8-ubi8\n    container_name: lightrag-server_mongo-community\n    restart: always\n    ports:\n      - \"27017:27017\"\n    environment:\n      - MONGODB_INITDB_ROOT_USERNAME=${MONGO_USERNAME}\n      - MONGODB_INITDB_ROOT_PASSWORD=${MONGO_PASSWORD}\n    volumes:\n      - lightrag_mongo_data:/data/db\n\nvolumes: # I have tried to not have any guid named volumes anymore but this isn't complete yet.\n  lightrag_redis_data:\n  lightrag_neo4j_import:\n  lightrag_neo4j_data:\n  lightrag_neo4j_backups:\n  lightrag_etcd_data:\n  lightrag_minio_data:\n  lightrag_milvus_data:\n  lightrag_mongo_data:\n```\n\n**Environment variables file:**\n```dotenv\n### This is sample file of .env\n\n### Server Configuration\nHOST=0.0.0.0\nPORT=9621\nWORKERS=4\nNAMESPACE_PREFIX=lightrag  # separating data from difference Lightrag instances\nMAX_GRAPH_NODES=1000       # Max nodes return from grap retrieval\nCORS_ORIGINS=http://localhost:3000,http://localhost:8080 # It runs on port 9621 as you can see in your docker desktop\n\n### Optional SSL Configuration\n# SSL=true\n# SSL_CERTFILE=/path/to/cert.pem\n# SSL_KEYFILE=/path/to/key.pem\n\n### Directory Configuration\n# INPUT_DIR=<absolute_path_for_doc_input_dir>\n\n### Ollama Emulating Model Tag\n# OLLAMA_EMULATING_MODEL_TAG=latest\n\n### Logging level\n# LOG_LEVEL=INFO\n# VERBOSE=False\n# LOG_DIR=/path/to/log/directory  # Log file directory path, defaults to current working directory\n# LOG_MAX_BYTES=10485760          # Log file max size in bytes, defaults to 10MB\n# LOG_BACKUP_COUNT=5              # Number of backup files to keep, defaults to 5\n\n### Settings for RAG query\n# HISTORY_TURNS=3\n# COSINE_THRESHOLD=0.2\n# TOP_K=60\n# MAX_TOKEN_TEXT_CHUNK=4000\n# MAX_TOKEN_RELATION_DESC=4000\n# MAX_TOKEN_ENTITY_DESC=4000\n\n### Settings for document indexing\nENABLE_LLM_CACHE_FOR_EXTRACT=true    # Enable LLM cache for entity extraction\nSUMMARY_LANGUAGE=English             # Should probably do everything in english if possible. If you need a different language just translate it first to english.\n\n# You can always tweak these variables if you want.\nCHUNK_SIZE=1200\nCHUNK_OVERLAP_SIZE=100\n# MAX_TOKEN_SUMMARY=500                # Max tokens for entity or relations summary\n# MAX_PARALLEL_INSERT=2                # Number of parallel processing documents in one patch\n\n# EMBEDDING_BATCH_NUM=32               # num of chunks send to Embedding in one request\n# EMBEDDING_FUNC_MAX_ASYNC=16          # Max concurrency requests for Embedding\n# MAX_EMBED_TOKENS=8192\n\n### LLM Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nTIMEOUT=150                            # Time out in seconds for LLM, None for infinite timeout\nTEMPERATURE=0.5\nMAX_ASYNC=4                            # Max concurrency requests of LLM\nMAX_TOKENS=32768                       # Max tokens send to LLM (less than context size of the model)\n\n### OpenAI alike example\nLLM_BINDING=openai\nLLM_MODEL=gpt-4o-mini                   # I would use mini for cost limitations.\nLLM_BINDING_HOST=https://api.openai.com/v1\n\n# If you do not include OPENAI_API_KEY you will get an error. So i just input my api key at both variables.\nLLM_BINDING_API_KEY=sk-                 # API key openai. Same key for both\nOPENAI_API_KEY=sk-                      # API key openai. Same key for both\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nEMBEDDING_MODEL=text-embedding-3-small      # Low cost, good performance\nEMBEDDING_DIM=1536                          # The number of dimensions the embedding model returns\nEMBEDDING_BINDING_API_KEY=your_api_key      # I have never set this, but I have also configured my LightRAG a bit differnet in the lightrag_server.py file. Ill also post this somewhere.\n\nEMBEDDING_BINDING=openai\nLLM_BINDING_HOST=https://api.openai.com/v1\n\n### Data storage selection\nLIGHTRAG_KV_STORAGE=RedisKVStorage\nLIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=Neo4JStorage\nLIGHTRAG_DOC_STATUS_STORAGE=MongoDocStatusStorage\n\n### Neo4j Configuration\nNEO4J_URI=bolt://neo4j:7687         # Should match name of service\nNEO4J_USERNAME=neo4j                # This can only be neo4j\nNEO4J_PASSWORD=                     # Can be anything you want, but you would want to make it a bit difficult. This is also what you use when checking your graph at 7474 port of neo4j.\nNEO4J_DATABASE=neo4j                # This avoids the \"Database name cannot be set\" error, because the community edition only supports one graph db.\nLOGFIRE_API_KEY=                    # Logfire is an easy way to log calls. Especially to OpenAI. But also any other LLM or providor and its free.\n\n# ### MongoDB Configuration\nMONGO_URI=mongodb://admin:admin123@mongo:27017/\nMONGO_USERNAME=admin                # Should match URI credentials\nMONGO_PASSWORD=admin123             # Should match URI credentials\nMONGO_DATABASE=LightRAG             # Just a standard name\n# MONGODB_GRAPH=false # deprecated (keep for backward compatibility)\n\n### Milvus Configuration\nMILVUS_URI=http://milvus:19530\nMILVUS_DB_NAME=lightrag             # Should match name you give your db at lightrag_server.py. I'll explain what i mean by this in the github post.\nMILVUS_USER=                        # Can be anything you want\nMILVUS_PASSWORD=                    # Can be anything you want\n# # MILVUS_TOKEN=your_token\n\n### Redis\nREDIS_URI=redis://redis:6379\n\n### For JWTt Auth (LIGHTRAG VERSION 1.3.0)\nAUTH_USERNAME=admin             # login name\nAUTH_PASSWORD=admin123          # password\nTOKEN_SECRET=adminadmin-LightRAG-API-Server           # JWT key\nTOKEN_EXPIRE_HOURS=4            # expire duration\n\n### For JWT Auth (LIGHTRAG VERSION 1.3.1)\nAUTH_ACCOUNTS='admin:admin123,user1:pass456'\nTOKEN_SECRET=Your-Key-For-LightRAG-API-Server\nTOKEN_EXPIRE_HOURS=48\nGUEST_TOKEN_EXPIRE_HOURS=24\nJWT_ALGORITHM=HS256\n\n### API-Key to access LightRAG Server API\n# LIGHTRAG_API_KEY=your-secure-api-key-here\n# WHITELIST_PATHS=/health,/api/*\n```\n\n---\n\n### Additional Code Changes\n\nI have made a few small adjustments in the codebase that I believe will improve the setup experience:\n\n1. **Milvus Database Creation in `lightrag_server.py`:**\n\n   To avoid errors where Milvus complains that the \"lightrag\" database doesn’t exist, I added the following snippet at the top of the imports in the file:\n   ```python\n   from pymilvus import connections, db\n\n   # Connect to Milvus\n   connections.connect(host=\"milvus\", port=19530)\n\n   # Check if database 'lightrag' exists by listing available databases\n   if \"lightrag\" not in db.list_database():\n       db.create_database(\"lightrag\")\n   ```\n   *Note: Please ensure that the database name (\"lightrag\") matches what is specified in the .env file.*\n\n2. **RAG Initialization Update in `lightrag_server.py`:**\n\n   I adjusted the initialization of the RAG component so that the `llm_model_func` is explicitly set and the `embedding_func` parameter is provided. The update looks like this:\n   ```python\n   # Initialize RAG\n   if args.llm_binding in [\"openai\"]:\n       rag = LightRAG(\n           working_dir=args.working_dir,\n           llm_model_func=gpt_4o_mini_complete,\n           llm_model_name=args.llm_model,\n           llm_model_max_async=args.max_async,\n           llm_model_max_token_size=args.max_tokens,\n           chunk_token_size=int(args.chunk_size),\n           chunk_overlap_token_size=int(args.chunk_overlap_size),\n           llm_model_kwargs={\n               \"host\": args.llm_binding_host,\n               \"timeout\": args.timeout,\n               \"options\": {\"num_ctx\": args.max_tokens},\n               \"api_key\": args.llm_binding_api_key,\n           }\n           if args.llm_binding == \"lollms\" or args.llm_binding == \"ollama\"\n           else {},\n           embedding_func=openai_embed,\n           kv_storage=args.kv_storage,\n           graph_storage=args.graph_storage,\n           vector_storage=args.vector_storage,\n           doc_status_storage=args.doc_status_storage,\n           vector_db_storage_cls_kwargs={\n               \"cosine_better_than_threshold\": args.cosine_threshold\n           },\n           enable_llm_cache_for_entity_extract=args.enable_llm_cache_for_extract,\n           embedding_cache_config={\n               \"enabled\": True,\n               \"similarity_threshold\": 0.95,\n               \"use_llm_check\": False,\n           },\n           namespace_prefix=args.namespace_prefix,\n           auto_manage_storages_states=False,\n           max_parallel_insert=args.max_parallel_insert,\n       )\n\n\t\n    # Add routes\n    app.include_router(create_document_routes(rag, doc_manager, api_key))\n   ```\n   I import `gpt_4o_mini_complete` and `openai_embed` from:\n   ```python\n   from lightrag.llm.openai import openai_complete_if_cache, openai_embed\n   ```\n   This change addresses occasional errors when `openai_embed` isn’t explicitly passed despite being mentioned in the settings.\n\n3. **Neo4j Connection Pool Size in `neo4j_impl.py`:**\n\n   To better accommodate higher workloads (which do exist for Neo4j specifically), I increased the default maximum connection pool size for Neo4j to 400 (which is the maximum for the community edition by default). This change is intended as a temporary measure until the batch feature is implemented, after which it can be reduced to 100.\n   ```python\n   # Set this a bit higher than expected workloads\n   MAX_CONNECTION_POOL_SIZE = int(\n       os.environ.get(\n           \"NEO4J_MAX_CONNECTION_POOL_SIZE\",\n           config.get(\"neo4j\", \"connection_pool_size\", fallback=400),\n       )\n   )\n   ```\n   Please note that sometimes the Neo4j database container takes a little time to start up, so the lightrag container might log an \"unable to connect\" message; this is expected, and waiting a few moments should resolve the issue.\n\n---\n\n### Docker Compose Commands\n\nFor convenience, here are some instructions to start and stop the containers:\n\n- **To start everything:**  \n  Open Windows PowerShell, navigate to the repository directory where the docker-compose file is located, and run:\n  ```\n  docker compose -f <nameoffile.yml> up --build -d\n  ```\n\nAfter everything is started check your docker desktop and click on port 9621 for the WebUI. The .env settings contains username and password.\n\n- **To stop the containers:**  \n  ```\n  docker compose -f <nameoffile.yml> down\n  ```\n\n- **To stop the containers and remove the volumes:**  \n  ```\n  docker compose -f <nameoffile.yml> down -v\n  ```\n\n---\n\nI hope these changes help new users get started with lightRAG more quickly and smoothly. Thank you for all the work on this project, and good luck to everyone!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-04-11T13:28:21Z",
      "updated_at": "2025-05-08T01:27:53Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "docker",
        "Server"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1356/reactions",
        "total_count": 5,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.4.0",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1356",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1356",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:21.437601",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thank you for sharing. Could you elaborate on the purpose of MinIO and its advantages when integrated with Milvus?",
          "created_at": "2025-04-11T23:56:14Z"
        },
        {
          "author": "frederikhendrix",
          "body": "I'm also fairly new to Milvus, so I had a chat with o3-mini and this was what the conclusion is in short: \n\n- **MinIO’s Role:**  \n  - Acts as the persistent, scalable, S3-compatible object storage solution for Milvus deployments.\n  - Stores raw files, snapshots, and backups, allowing Milvus to conce",
          "created_at": "2025-04-12T18:13:31Z"
        }
      ]
    },
    {
      "issue_number": 1352,
      "title": "[Feature Request]: Add disable citations option to the query_param",
      "body": "### Feature Request Description\n\nDisabling citations may be required for certain applications. We should implement a query parameter option to manage this functionality, and add DISABLE_CITATION_BY_DEFAULT to set the default behavior of LightRAG.\n",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-11T08:08:24Z",
      "updated_at": "2025-05-08T01:19:35Z",
      "closed_at": "2025-05-08T01:19:34Z",
      "labels": [
        "enhancement",
        "Core"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1352/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.4.0",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1352",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1352",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:21.607369",
      "comments": [
        {
          "author": "ShamshadAhmedShorthillsAI",
          "body": "how are citations are currently generated in lightRAG? ",
          "created_at": "2025-04-12T06:22:38Z"
        },
        {
          "author": "danielaskdd",
          "body": "At the end of query response, there is a References  passage generated by LLM.",
          "created_at": "2025-04-12T07:36:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "Use cumstimized prompt template to control citation behavior.",
          "created_at": "2025-05-08T01:19:34Z"
        }
      ]
    },
    {
      "issue_number": 1541,
      "title": "[Feature Request]:add magic-pdf for zhipu to transform pdf to txt",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nI had added lightrag_zhipu_magic_pdf.py to transform pdf to txt .My URL is https://github.com/HKUDS/zhuyanhuazhuyanhua/issues\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhuyanhuazhuyanhua",
      "author_type": "User",
      "created_at": "2025-05-07T02:55:37Z",
      "updated_at": "2025-05-07T02:55:37Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1541/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1541",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1541",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:21.825281",
      "comments": []
    },
    {
      "issue_number": 1535,
      "title": "[Question]: Optimizing RAG architecture with Qdrant + PostgreSQL: Redundant data fetching pattern",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI've been working with a RAG system that uses Qdrant for vector search and PostgreSQL as the main database, and I've noticed what seems to be a redundant data fetching pattern:\n\n```\n# Current pattern\nresults = await chunks_vdb.query(query, ...)  # Results from vector search already contain full text\nchunks_ids = [r[\"id\"] for r in results]\nchunks = await text_chunks_db.get_by_ids(chunks_ids)  # Second DB call to PostgreSQL to get text that's already in results\nSince modern vector DBs like Qdrant store full document content in their payload, this seems inefficient. I've optimized it to:\n```\n\n```\n# Optimized pattern\nresults = await chunks_vdb.query(query, ...)\n# Use results directly, they already contain all needed data\n```\nQuestions:\n\n- Is this a common architectural pattern in RAG or is it an artifact from older vector DB architectures?\n- Are there any considerations I'm missing for keeping the second PostgreSQL DB call?\n- Would this optimization generally be considered a best practice for systems using modern vector DBs like Qdrant, Weaviate, Pinecone, and PostgreSQL?\n\n\n### Additional Context\n\nRelevant code reference:\nhttps://github.com/HKUDS/LightRAG/blob/4b35dbcf196c151f9ea3160668559b4257d0bd78/lightrag/operate.py#L1217C13-L1217C65",
      "state": "open",
      "author": "Brenon28",
      "author_type": "User",
      "created_at": "2025-05-06T14:57:58Z",
      "updated_at": "2025-05-06T18:07:48Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1535/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1535",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1535",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:23.481625",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thank you for your detailed and accurate observation. The issue has been addressed in PR #1537. We appreciate your suggestion.",
          "created_at": "2025-05-06T18:07:47Z"
        }
      ]
    },
    {
      "issue_number": 1532,
      "title": "[Bug]: issue running ollama pipeline_status[\"history_messages\"]",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHi, I tried to run lightrag with ollama; especially the model phi. It does not work, as I get the error below. However, I can make lightrag work with hugging face models. \n\n\nTraceback (most recent call last):\n  File \"c:\\Users\\FT999908\\Desktop\\Safran_work_local\\project\\RAG flight\\RAG_texte\\lightrag-ollama.py\", line 72, in <module>    rag.insert([chunk.page_content for chunk in chunks])     \n  File \"c:\\users\\ft999908\\desktop\\safran_work_local\\project\\rag flight\\rag_texte\\lightragprojet\\lightrag-main\\lightrag\\lightrag.py\", line 561, in insert\n    loop.run_until_complete(\n  File \"C:\\Users\\FT999908\\AppData\\Local\\anaconda3\\envs\\lightrag-env\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"c:\\users\\ft999908\\desktop\\safran_work_local\\project\\rag flight\\rag_texte\\lightragprojet\\lightrag-main\\lightrag\\lightrag.py\", line 587, in ainsert\n    await self.apipeline_process_enqueue_documents(\n  File \"c:\\users\\ft999908\\desktop\\safran_work_local\\project\\rag flight\\rag_texte\\lightragprojet\\lightrag-main\\lightrag\\lightrag.py\", line 852, in apipeline_process_enqueue_documents  \n    del pipeline_status[\"history_messages\"][:]\nKeyError: 'history_messages'\n\nAnd here's the code that produced the error\n\n```python\nimport os\nimport logging\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.ollama import ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\nfrom langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# #try to fix\n# #     del pipeline_status[\"history_messages\"][:]\n# # KeyError: 'history_messages'\n# from lightrag.utils import pipeline_status\n# pipeline_status[\"history_messages\"] = []\n\n# 📁 CONFIGURATION\nPDF_DIR = \"./data\"\nWORKING_DIR = \"./lightrag_with_pdf\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\n# 🔤 Nomic embedding via HuggingFace\ntokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1\")\nmodel = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n\ndef nomic_embed(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\nembedding_func = EmbeddingFunc(\n    embedding_dim=768,\n    max_token_size=8192,\n    func=nomic_embed\n)\n\n# 🧠 Initialiser LightRAG\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    chunk_token_size=1000,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"phi\",\n    llm_model_max_token_size=2048,\n    llm_model_kwargs={\n        \"host\": \"http://localhost:11434\",\n        \"options\": {\n            \"temperature\": 0.7,\n            \"top_k\": 40,\n            \"top_p\": 0.9,\n            \"num_predict\": 256,\n        },\n    },\n    embedding_func=embedding_func,\n)\n\n# 📄 CHARGER LES PDF\nloader = DirectoryLoader(PDF_DIR, glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\ndocuments = loader.load()\n\n# ✂️ SPLIT EN CHUNKS\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nchunks = splitter.split_documents(documents)\n\n\n# 🔁 INSERTION DANS LIGHTRAG\n\n# import asyncio\n\n# asyncio.run(rag.ainsert([chunk.page_content for chunk in chunks]))\nrag.insert([chunk.page_content for chunk in chunks])\n\n\n\n# ❓ EXEMPLE DE QUESTION\nquery = \"What are the main AI applications in aircraft design and safety?\"\nmodes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n\nfor mode in modes:\n    print(f\"\\n🔍 Mode: {mode}\")\n    result = rag.query(query, param=QueryParam(mode=mode))\n    print(result)\n\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "vogab",
      "author_type": "User",
      "created_at": "2025-05-06T11:15:53Z",
      "updated_at": "2025-05-06T16:25:22Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1532/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1532",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1532",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:23.697411",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You should initialize storage and pipeline status before calling any LightRAG function. Please refer to sample code: `lightrag_openai_compatible_demo.py`\n\n```\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n```",
          "created_at": "2025-05-06T16:25:21Z"
        }
      ]
    },
    {
      "issue_number": 1410,
      "title": "[Bug]:No dependencies are installed with `uv pip install`",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI installed lightrag-hku like so:\n```zsh\nuv pip install \"lightrag-hku[api]\"\n```\nHowever only the lightrag-hku package was installed:\n```console\nInstalled 1 package in 5ms\n + lightrag-hku==1.3.2\n```\nWorks when installing from source except for the following dependecies:\n```\nnano-vectordb\nollama\n```\n\n### Steps to reproduce\n\n```zsh\nuv venv --python 3.11\n. .venv/bin/activate\nuv pip install \"lightrag-hku[api]\"\n```\n\n### Expected Behavior\n\nThese packages should be installed:\n```console\n + aiofiles==24.1.0\n + aiohappyeyeballs==2.6.1\n + aiohttp==3.11.16\n + aiosignal==1.3.2\n + annotated-types==0.7.0\n + anyio==4.9.0\n + anytree==2.13.0\n + ascii-colors==0.7.0\n + asyncpg==0.30.0\n + attrs==25.3.0\n + autograd==1.7.0\n + bcrypt==4.3.0\n + beartype==0.18.5\n + certifi==2025.1.31\n + cffi==1.17.1\n + charset-normalizer==3.4.1\n + click==8.1.8\n + configparser==7.2.0\n + contourpy==1.3.2\n + cryptography==44.0.2\n + cycler==0.12.1\n + distro==1.9.0\n + ecdsa==0.19.1\n + fastapi==0.115.12\n + fonttools==4.57.0\n + frozenlist==1.6.0\n + future==1.0.0\n + gensim==4.3.3\n + graspologic==3.4.1\n + graspologic-native==1.2.5\n + h11==0.14.0\n + httpcore==1.0.8\n + httpx==0.28.1\n + hyppo==0.4.0\n + idna==3.10\n + jiter==0.9.0\n + joblib==1.4.2\n + kiwisolver==1.4.8\n + lightrag-hku==1.3.2 (from git+https://github.com/HKUDS/LightRAG.git@14b4bc96ce31086a807dc9d204a5409e8e812a3b)\n + llvmlite==0.44.0\n + matplotlib==3.10.1\n + multidict==6.4.3\n + nano-vectordb==0.0.4.3\n + networkx==3.4.2\n + numba==0.61.2\n + numpy==1.26.4\n + ollama==0.4.8\n + openai==1.75.0\n + packaging==24.2\n + pandas==2.2.3\n + passlib==1.7.4\n + patsy==1.0.1\n + pillow==11.2.1\n + pipmaster==0.5.4\n + pot==0.9.5\n + propcache==0.3.1\n + pyasn1==0.4.8\n + pycparser==2.22\n + pydantic==2.11.3\n + pydantic-core==2.33.1\n + pyjwt==2.10.1\n + pynndescent==0.5.13\n + pyparsing==3.2.3\n + python-dateutil==2.9.0.post0\n + python-dotenv==1.1.0\n + python-jose==3.4.0\n + python-multipart==0.0.20\n + pytz==2025.2\n + regex==2024.11.6\n + requests==2.32.3\n + rsa==4.9.1\n + scikit-learn==1.6.1\n + scipy==1.12.0\n + seaborn==0.13.2\n + setuptools==78.1.0\n + six==1.17.0\n + smart-open==7.1.0\n + sniffio==1.3.1\n + starlette==0.46.2\n + statsmodels==0.14.4\n + tenacity==9.1.2\n + threadpoolctl==3.6.0\n + tiktoken==0.9.0\n + tqdm==4.67.1\n + typing-extensions==4.13.2\n + typing-inspection==0.4.0\n + tzdata==2025.2\n + umap-learn==0.5.7\n + urllib3==2.4.0\n + uvicorn==0.34.2\n + wrapt==1.17.2\n + xlsxwriter==3.2.3\n + yarl==1.20.0\n```\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: v1.3.2\n- Operating System: Arch\n- Python Version: 3.11\n- Related Issues:\n",
      "state": "open",
      "author": "adamkatav",
      "author_type": "User",
      "created_at": "2025-04-19T07:14:36Z",
      "updated_at": "2025-05-06T11:12:13Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1410/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1410",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1410",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:23.918561",
      "comments": [
        {
          "author": "JaremaPiekutowski",
          "body": "The same with standard pip installation\n\n![Image](https://github.com/user-attachments/assets/1d61a5b3-3e48-4b0d-87db-2e9fe77f5de0)",
          "created_at": "2025-04-19T09:22:19Z"
        },
        {
          "author": "liguochuan00",
          "body": "This is a huge problem",
          "created_at": "2025-04-28T02:42:14Z"
        },
        {
          "author": "danielaskdd",
          "body": "This problem is solved by v1.3.5",
          "created_at": "2025-04-28T05:19:20Z"
        },
        {
          "author": "adamkatav",
          "body": "> This problem is solved by v1.3.5\n\nThanks!\nTough, I have issues with the real time pip installation for 2 reasons:\n1. When not running in virtual environment pip cannot install packages without the --break flag\n2. uv users don't have pip installed by default. I believe that if pip is a dependency i",
          "created_at": "2025-05-06T11:12:12Z"
        }
      ]
    },
    {
      "issue_number": 894,
      "title": "[Question] Scaling Ingestion of Records in LightRAG",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWe collect tens of thousands of documents every hour in our pre-processing environment from a couple of different data sources. However, in lightRAG, we can only ingest around 250 records every 10 minutes or around 1,500 articles an hour into our LightRAG Deployment (average document size is 500 words long or around ~1,000 tokens). We are leveraging LanceDB,  NEO4J and OpenAI API (Tier 5 Limits)  for document processing. Has anyone been able to scale up LightRAG to handle large hourly data ingestion needs? The bottleneck is the NEO4J/GraphDB processing. While we could parallel process batch records, if we run 4 parallel batches successfully every 10 minutes, we will still fall short of our goal of 80,000 every hour.  Are we missing any good ideas to speed this up from paralleling to enhanced batch processing?  We know with LanceDB, we can ingest 80,000 records an hour into the Vector DB. The question is can we enhance the GraphDB ingestion to keep up.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "gsidari",
      "author_type": "User",
      "created_at": "2025-02-20T02:43:48Z",
      "updated_at": "2025-05-06T10:15:19Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/894/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/894",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/894",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:24.698102",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi, I'm working on it.\n\nIf you want you can help us on NEOJ because we have some problems.\n\nThanks",
          "created_at": "2025-02-20T08:19:36Z"
        },
        {
          "author": "gsidari",
          "body": "I would love to have our team help with this however, we are not NEO4J Exports, so we are not able to optimize the approach. If you want us to share what we are seeing we can pass that along and our thoughts on how we could adjust lightRAG for high volumn use cases like this that would also be awsom",
          "created_at": "2025-02-20T08:25:13Z"
        },
        {
          "author": "ShamshadAhmedShorthillsAI",
          "body": "@YanSte is there any update specifically after release of lightRAG 1.3.0. Also can you explain what help you need in neo4j",
          "created_at": "2025-04-03T10:42:35Z"
        },
        {
          "author": "xiyihan0",
          "body": "Well...it shouldn't be so slow when using Neo4j to store graph data... On my 16-core 128GB PC, I can insert edges at an average speed of 3,000 edges per second into my local database that already contains 4 billion relationships. If possible, could you provide the corresponding query.log file under ",
          "created_at": "2025-05-06T09:13:14Z"
        },
        {
          "author": "danielaskdd",
          "body": "The latest version includes optimized performance for document indexing and graph storage. Please download it and verify the improvements.\n\n\n\n\n\n\n\n\n\n",
          "created_at": "2025-05-06T10:15:18Z"
        }
      ]
    },
    {
      "issue_number": 1529,
      "title": "[Question]:Where does the problem lie when the knowledge graph extracts too few entities and relationships",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI used deepseek-r1:1.5b and bge-m3:567m. However, very few entities and relationships were extracted. Even the entities and relationships in the examples were added to the knowledge graph. I tried to modify the prompt content, but still there was no change\n\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "pure-df",
      "author_type": "User",
      "created_at": "2025-05-06T07:36:22Z",
      "updated_at": "2025-05-06T10:04:13Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1529/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1529",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1529",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:24.915385",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "deepseek-r1:1.5b may not provide sufficient performance for entity and relation extraction tasks. It is recommended to use an LLM with at least 32B parameters. Additionally, leveraging a reasoning model for this purpose is not advised.",
          "created_at": "2025-05-06T10:04:12Z"
        }
      ]
    },
    {
      "issue_number": 1530,
      "title": "[Bug]:passing a parameter called \"stream\" to the Bedrock API, but it's not in the list of accepted parameters",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen using bedrock_complete, it throws below exception. Because a parameter called \"stream\" is being passed to the Bedrock API, but it's not in the list of accepted parameters\n\n```\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nUnknown parameter in input: \"stream\", must be one of: modelId, messages, system, inferenceConfig, toolConfig, guardrailConfig, additionalModelRequestFields, promptVariables, additionalModelResponseFieldPaths, requestMetadata, performanceConfig\n```\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "balaji-munusamy",
      "author_type": "User",
      "created_at": "2025-05-06T09:25:25Z",
      "updated_at": "2025-05-06T09:25:25Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1530/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1530",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1530",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:25.116091",
      "comments": []
    },
    {
      "issue_number": 1528,
      "title": "[Question]: 支持现有neo4j数据库连接查询吗？",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n我目前已经有neo4j数据库，数据量有数千万，请问可以直接用lightrag连接并查询吗？我是否需要对数据进行额外处理？\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "lccant",
      "author_type": "User",
      "created_at": "2025-05-06T07:02:09Z",
      "updated_at": "2025-05-06T08:36:28Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1528/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1528",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1528",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:25.116120",
      "comments": [
        {
          "author": "xiyihan0",
          "body": "估计不太行...因为lightrag进行索引查询不仅需要图关系本身(graph_chunk_entity_relation.graphml)，还需要之前计算的关于图中各个属性和关系描述的嵌入向量文件",
          "created_at": "2025-05-06T08:36:27Z"
        }
      ]
    },
    {
      "issue_number": 1520,
      "title": "[Bug]: create_at = UNKOWN",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nCant retrived the \"create_at\"\nDo you guys check the prompt input?\n![Image](https://github.com/user-attachments/assets/a2fd5d6f-4883-49c1-ad02-09a13fd76220)\n\nif from Document chunk its able to get the value.\nOnly From Knowledge Graph(KG):\n-----Entities-----\n-----Relationships-----\n![Image](https://github.com/user-attachments/assets/1b5dd5fb-0bd9-42ed-8900-0ca7d7fc328e)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "molion097",
      "author_type": "User",
      "created_at": "2025-05-05T03:08:14Z",
      "updated_at": "2025-05-06T02:41:56Z",
      "closed_at": "2025-05-06T02:41:56Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1520/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1520",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1520",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:25.335396",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "This issue has been resolved in the latest version on the main branch.",
          "created_at": "2025-05-05T03:14:26Z"
        },
        {
          "author": "molion097",
          "body": "Tried to use the latest main branch also still same result.\nI am using Postgre as database and Neo4j as KG store\n\n![Image](https://github.com/user-attachments/assets/0f519ce0-5cc0-42b0-8bd7-0708573dc66c)\n\n![Image](https://github.com/user-attachments/assets/20105e0d-9ba2-4888-a5e3-a8e0d708c9c8)\n\n![Im",
          "created_at": "2025-05-05T08:21:03Z"
        },
        {
          "author": "danielaskdd",
          "body": "You should clear the old data, reindex all the docs again.",
          "created_at": "2025-05-05T09:47:53Z"
        },
        {
          "author": "molion097",
          "body": "Thank you, its solved\n\n![Image](https://github.com/user-attachments/assets/f38a609f-a199-46e5-b5a0-847599448e42)",
          "created_at": "2025-05-06T02:41:56Z"
        }
      ]
    },
    {
      "issue_number": 1525,
      "title": "[Question]:Clarification Request on LightRAG End-to-End Querying Workflow",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThanks for building LightRAG — it's very promising for hybrid RAG use cases. I’m currently integrating it into a system that combines knowledge graph and vector-based retrieval. To ensure correct implementation, I’ve outlined the end-to-end query process below.\n\nCould you please confirm whether this is aligned with LightRAG’s intended flow and let me know if any steps are missing or redundant?\n\n1. Initialize RAG with all required parameters and configurations.\n2. Load all knowledge sources (neo4j, FAISS, mongodb) one by one.\n3. Check for cached response: If the query has already been processed, return the cached result.\n4. If not cached, extract keywords from the query using an LLM.\n5. Check if keywords are cached: If yes, return the corresponding response.\n6. Generate a query prompt using LightRAG for querying the Knowledge Graph.\n6. Extract relevant nodes/entities from the KG based on the generated prompt and  keywords extracted from user query.\n7. Query the FAISS vector index by embedding the extracted keywords, then compute similarities with entities (using Top-K and cosine similarity).\n8. Batch query Neo4j to retrieve the number of relationships (node degrees) for relevant nodes.\n9. Query FAISS again using the full user query for relation-focused retrieval.\n10. Another batch query on Neo4j to get degrees of a broader range of nodes for wider graph context (local exploration).\n11.Query FAISS again, specifically to refine relation extraction.\n12.  Final Neo4j query to compute node degrees again (possibly for final ranking or reranking).\n13. Generate final prompt combining results from KG and vector searches for answer generation.\n\nI also had one specific question:\nMy query_param is set to mix, but I noticed that logs do not show any similarity check between the user query and the embedded chunks.\nIs that similarity comparison part of the process, and if so, at which step does it occur? \n\nAny clarification or documentation reference would be really helpful. Thanks again for your support!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Utkarshagharat58",
      "author_type": "User",
      "created_at": "2025-05-05T17:07:34Z",
      "updated_at": "2025-05-05T17:07:34Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1525/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1525",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1525",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:25.517659",
      "comments": []
    },
    {
      "issue_number": 1173,
      "title": "[Question]: no way to get answer AND context retrieved ?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nCalling query does return the answer as text, but I would like the context also. Am I forced to perform 2 calls or is there a smarter way ? \n\n### Additional Context\n\ni am currently doing two times the same .query call; one with only_need_context=False and one with only_need_context=True, which is a bit weird as the context has already been retrieved in the first call to generate the answer",
      "state": "open",
      "author": "GTimothee",
      "author_type": "User",
      "created_at": "2025-03-24T11:33:49Z",
      "updated_at": "2025-05-05T17:06:02Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1173/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1173",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1173",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:25.517680",
      "comments": [
        {
          "author": "Snoowp",
          "body": "Anyone got the answer for this, I need the best way to view the context retrieved to tune my promps the best",
          "created_at": "2025-05-05T17:06:01Z"
        }
      ]
    },
    {
      "issue_number": 1507,
      "title": "[Feature Request]: stable entity ids",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\ncurrently the entity_id is set to entity_name. in operate.py, but the system allows the renaming of entities, which means entity_id and entity_name can eventually have different names which leads to confusion. a simple fix could be:\n\nin operate.py:\n\n```\n    node_data = dict(\n        entity_id=compute_mdhash_id(entity_name),\n        entity_type=entity_type,\n        description=description,\n        source_id=source_id,\n        file_path=file_path,\n    )\n```\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "dom-nura",
      "author_type": "User",
      "created_at": "2025-05-01T11:23:37Z",
      "updated_at": "2025-05-05T11:16:11Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1507/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1507",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1507",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:25.693346",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "In LightRAG, `entity_id` serves the same purpose as the entity name. It is used to distinguish between entities, and entities with the same `entity_id` will be merged. Therefore, using a hash value as the `entity_id` is not recommended.\n\nFor certain graph databases such as Neo4j and AGE, LightRAG ut",
          "created_at": "2025-05-05T11:16:11Z"
        }
      ]
    },
    {
      "issue_number": 1521,
      "title": "Does it support Java version",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nDoes it support Java version? It is recommended to write the service in Java to improve efficiency\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "smileyboy2019",
      "author_type": "User",
      "created_at": "2025-05-05T08:29:49Z",
      "updated_at": "2025-05-05T10:46:59Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1521/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1521",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1521",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:25.873467",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG Server provides REST APIs for application integration: https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md#api-endpoints",
          "created_at": "2025-05-05T10:46:58Z"
        }
      ]
    },
    {
      "issue_number": 1481,
      "title": "[question]: Problem with retrieved create_at",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI used the Neo4j integration only, without milvus and redis.\nwhen i want to retrieve \"created_at\", i check there is no \"created_at\" in neo4j data.\nany idea how to get it with neo4j? or i need another way to retrieve\n\n### Additional Context\n\n<img width=\"952\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bac71418-e810-4cc8-9496-3a46365210fc\" />",
      "state": "closed",
      "author": "molion097",
      "author_type": "User",
      "created_at": "2025-04-28T06:07:03Z",
      "updated_at": "2025-05-05T03:08:37Z",
      "closed_at": "2025-05-05T03:08:25Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1481/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1481",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1481",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:26.143777",
      "comments": [
        {
          "author": "molion097",
          "body": "I tested again and check without any native integration. still cant get the \"create_at\" \nDo you guys check the prompt input?\n\n![Image](https://github.com/user-attachments/assets/49b4bfb2-474a-4165-985f-261b3d66be40)",
          "created_at": "2025-04-29T03:21:24Z"
        },
        {
          "author": "molion097",
          "body": "if from Document chunk its able to get the value. \nOnly From Knowledge Graph(KG):\n-----Entities-----\n-----Relationships-----\n\n![Image](https://github.com/user-attachments/assets/1dcc02e0-e0e9-4723-a834-e454f6f81610)",
          "created_at": "2025-04-29T03:24:18Z"
        }
      ]
    },
    {
      "issue_number": 1515,
      "title": "[Question]:",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHas anyone successfully set up lightrag-server locally with Ollama and got it working properly?\n\nI managed to install and run it, and uploading files works fine — they are uploaded successfully. However, the problem occurs during chunk processing and knowledge graph construction, which always fails.\n\nI followed all the steps for setting up lightrag-server, updated the .env file with my Ollama configuration, and the server runs without other issues.\n\nWhat could be causing the failure in processing? Is there something I might be missing?\nHere’s my current configuration for reference:\n\n### Ollama example (For local services installed with docker, you can use host.docker.internal as host)\nLLM_BINDING=ollama\nLLM_MODEL=deepseek-r1:14b\nLLM_BINDING_HOST=http://localhost:11434\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\n### ollama example\nEMBEDDING_BINDING=ollama\nEMBEDDING_MODEL=nomic-embed-text:latest\nEMBEDDING_DIM=768\nEMBEDDING_BINDING_HOST=http://localhost:11434\n\nAny guidance or suggestions would be greatly appreciated\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "muhamedragheb",
      "author_type": "User",
      "created_at": "2025-05-03T22:03:10Z",
      "updated_at": "2025-05-05T03:05:13Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1515/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1515",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1515",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:26.328757",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The deepseek-r1:14b model is not suitable for knowledge graph extraction with LightRAG. Firstly, its parameter size is relatively small; it is recommended that LLMs have at least 32 billion parameters. Secondly, using an inference model is not advised—instead, it is recommended to use deepseek-chat.",
          "created_at": "2025-05-05T03:05:13Z"
        }
      ]
    },
    {
      "issue_number": 1464,
      "title": "[Bug]: Document Processing hangs after reaching the last or second last chunk",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I submit a document for processing, the document processes up to a point and then the processing hangs.\n\nI am using azure openai with gpt-4o-mini model. \n\n![Image](https://github.com/user-attachments/assets/2ffe97da-0c83-48e6-b5a5-47e763484808)\n\nHere is the server log:\n2025-04-25 15:17:39,518 - lightrag - INFO -  == LLM cache == saving default: 6567312281dddab13d860eead49f556a\n2025-04-25 15:17:39,518 - lightrag - INFO - Inserting 1 to llm_response_cache\n2025-04-25 15:17:39,531 - lightrag - INFO - Chk 81/88: extracted 16 Ent + 13 Rel\n2025-04-25 15:17:45,636 - lightrag - INFO -  == LLM cache == saving default: 57070c4211f54b65eeb363c286bf3a6e\n2025-04-25 15:17:45,637 - lightrag - INFO - Inserting 1 to llm_response_cache\n2025-04-25 15:17:45,647 - lightrag - INFO - Chk 82/88: extracted 14 Ent + 13 Rel\n2025-04-25 15:17:53,339 - lightrag - INFO -  == LLM cache == saving default: 6128f9abf94916c7c0bda6cd635bce6c\n2025-04-25 15:17:53,340 - lightrag - INFO - Inserting 1 to llm_response_cache\n2025-04-25 15:17:53,354 - lightrag - INFO - Chk 83/88: extracted 15 Ent + 12 Rel\n2025-04-25 15:18:02,987 - lightrag - INFO -  == LLM cache == saving default: ec23d28c83232e78da03707bc2d9b8e7\n2025-04-25 15:18:02,989 - lightrag - INFO - Inserting 1 to llm_response_cache\n2025-04-25 15:18:03,007 - lightrag - INFO - Chk 84/88: extracted 9 Ent + 7 Rel\n2025-04-25 15:18:09,922 - lightrag - INFO -  == LLM cache == saving default: 334580c3001af8edf1647e02d8e8df96\n2025-04-25 15:18:09,923 - lightrag - INFO - Inserting 1 to llm_response_cache\n2025-04-25 15:18:09,932 - lightrag - INFO - Chk 85/88: extracted 18 Ent + 13 Rel\n2025-04-25 15:18:10,491 - lightrag - INFO -  == LLM cache == saving default: 58a03c9f3f265b659bacaa089c37b474\n2025-04-25 15:18:10,492 - lightrag - INFO - Inserting 1 to llm_response_cache\n2025-04-25 15:18:10,500 - lightrag - INFO - Chk 86/88: extracted 19 Ent + 14 Rel\n2025-04-25 15:36:24,466 - uvicorn.access - INFO - 10.92.0.5:59377 - \"GET /graph/label/list HTTP/1.1\" 200\n2025-04-25 15:36:24,528 - lightrag - INFO - Subgraph query successful | Node count: 157 | Edge count: 190\n2025-04-25 15:36:24,553 - uvicorn.access - INFO - 10.92.0.5:59377 - \"GET /graphs?label=*&max_depth=4&max_nodes=1000 HTTP/1.1\" 200\n\n\nAs you can see the process is \"waiting\" for 87 and 88 chuck processing. The issue is intermittent and the same document is processed successfully other times.  The issue is also happening when it is processing the last or second last chunk.\n\nI am using the MAX_ASYSNC=4 default value.  Other parameters are also default.\n\nHere is another example that was shared by my team member.\n\n![Image](https://github.com/user-attachments/assets/b783aca4-5699-4d3a-a3d4-3dec6808aefb)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.3.3\n- Operating System: Linux\n- Python Version: 3.10\n- Related Issues: \n",
      "state": "open",
      "author": "acsangamnerkar",
      "author_type": "User",
      "created_at": "2025-04-25T15:57:47Z",
      "updated_at": "2025-05-05T02:03:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1464/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1464",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1464",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:26.555826",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "It is likely that the issue was caused by the LLM call hanging. Please try restarting the LightRAG server and re-indexing the same file to see if the process hangs at the same point.\n\nAdditionally, could you confirm whether you are using the default storage configuration?",
          "created_at": "2025-04-26T00:09:29Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "Tried it again 2 times and it hangs at the same spot in the file\r\nprocessing.\r\n\r\nOn Fri, Apr 25, 2025 at 7:09 PM Daniel.y ***@***.***> wrote:\r\n\r\n> *danielaskdd* left a comment (HKUDS/LightRAG#1464)\r\n> <https://github.com/HKUDS/LightRAG/issues/1464#issuecomment-2831642466>\r\n>\r\n> It is likely that the",
          "created_at": "2025-04-27T04:12:43Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "Btw, I am using Postgres and have tried it again with mongo too.\r\n\r\n\r\n\r\n\r\nOn Fri, Apr 25, 2025 at 7:09 PM Daniel.y ***@***.***> wrote:\r\n\r\n> *danielaskdd* left a comment (HKUDS/LightRAG#1464)\r\n> <https://github.com/HKUDS/LightRAG/issues/1464#issuecomment-2831642466>\r\n>\r\n> It is likely that the issue ",
          "created_at": "2025-04-27T04:15:24Z"
        },
        {
          "author": "danielaskdd",
          "body": "To further diagnose the issue, you may consider indexing a different document to verify the results. Try to download the official sample file:\n\n```\ncurl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt > ./book.txt\n```",
          "created_at": "2025-04-27T04:35:33Z"
        },
        {
          "author": "danielaskdd",
          "body": "To simplify troubleshooting, it is advisable to retain the default storage configuration.",
          "created_at": "2025-04-27T04:37:27Z"
        }
      ]
    },
    {
      "issue_number": 1506,
      "title": "[Bug]: Failure during document upload",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nERROR: Failed to extrat document doc-b3e8e0cafab134968c62d0e2e6b464a7: Traceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/lightrag-env2/lib/python3.11/site-packages/lightrag/lightrag.py\", line 990, in process_document\n    await asyncio.gather(*tasks)\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/lightrag-env2/lib/python3.11/site-packages/lightrag/kg/nano_vector_db_impl.py\", line 116, in upsert\n    results = client.upsert(datas=list_data)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/lightrag-env2/lib/python3.11/site-packages/nano_vectordb/dbs.py\", line 116, in upsert\n    self.__storage[\"matrix\"] = np.vstack([self.__storage[\"matrix\"], new_matrix])\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/lightrag-env2/lib/python3.11/site-packages/numpy/core/shape_base.py\", line 289, in vstack\n    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1024 and the array at index 1 has size 3072\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "lschaupp",
      "author_type": "User",
      "created_at": "2025-05-01T08:30:46Z",
      "updated_at": "2025-05-04T18:27:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1506/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1506",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1506",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:26.756485",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "It appears that the embedding model was modified between document uploads, which is unsupported. To resolve this, please clear all existing data and attempt the file upload again.",
          "created_at": "2025-05-02T23:47:42Z"
        },
        {
          "author": "firefrorefiddle",
          "body": "Another possibility is that `EMBEDDING_DIM=1024` is set in your `.env` file, but you are using an embedding model with more dimensions.",
          "created_at": "2025-05-04T18:27:38Z"
        }
      ]
    },
    {
      "issue_number": 1517,
      "title": "[Question]: LightRAG Server + Neo4J + MongoDB token tracking",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi all, wondering what the best way to implement tracking token usage whilst using the lightrag-server + open-webui to chat with my data? Would be helpful to know! Kinda new to this :) \n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "maseegal",
      "author_type": "User",
      "created_at": "2025-05-04T10:44:07Z",
      "updated_at": "2025-05-04T10:55:20Z",
      "closed_at": "2025-05-04T10:55:20Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1517/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1517",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1517",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:26.950577",
      "comments": []
    },
    {
      "issue_number": 1031,
      "title": "[Bug]: No module named 'lightrag.kg.shared_storage'",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nwindows\npython 3.10\npip install lightrag-hku\nlightrag-hku                             1.2.3\n\n## Setup and Imports\nimport os\nimport asyncio\nimport nltk\nfrom nltk.corpus import brown\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.ollama import ollama_model_complete, ollama_embed\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[6], line 10\n      8 from lightrag import LightRAG, QueryParam\n      9 from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n---> 10 from lightrag.kg.shared_storage import initialize_pipeline_status\n     11 from lightrag.utils import setup_logger\n     13 # Set up logging\n\nModuleNotFoundError: No module named 'lightrag.kg.shared_storage'\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "thistleknot",
      "author_type": "User",
      "created_at": "2025-03-08T18:24:39Z",
      "updated_at": "2025-05-04T06:39:38Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1031/reactions",
        "total_count": 4,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1031",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1031",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:26.950601",
      "comments": [
        {
          "author": "b-d055",
          "body": "I'm also getting this error on MacOS, python 3.10 using pip install lightrag-hku",
          "created_at": "2025-03-09T15:23:25Z"
        },
        {
          "author": "b-d055",
          "body": "Looks like it's because of the change between[ 1.2.3 and 1.2.4,](https://github.com/HKUDS/LightRAG/compare/v1.2.3...v1.2.4)  - pipy does not yet have 1.2.4, so I've reverted to using 1.2.3 example ",
          "created_at": "2025-03-09T15:38:31Z"
        },
        {
          "author": "thistleknot",
          "body": "I had 1.2.3 installed and got this error (in the initial description, I did\r\na pip list installed).\r\n\r\nOn Sun, Mar 9, 2025 at 8:38 AM Bryan ***@***.***> wrote:\r\n\r\n> Looks like it's because of the change between 1.2.3 and 1.2.4,\r\n> <https://github.com/HKUDS/LightRAG/compare/v1.2.3...v1.2.4> - pipy do",
          "created_at": "2025-03-09T15:40:25Z"
        },
        {
          "author": "b-d055",
          "body": "@thistleknot yeah - if you look at the diff I linked, `shared_storage` wasn't introduced [until 1.2.4](https://github.com/HKUDS/LightRAG/compare/v1.2.3...v1.2.4#diff-38c221cdcc66f59e26e57e577a56f6b5dad2647d909224c41e9d28b8b40005b8), which is not available via pip yet. I think they need to update the",
          "created_at": "2025-03-09T15:43:27Z"
        },
        {
          "author": "thistleknot",
          "body": "I'm confused.  Does this mean I must be mistaken?  I know I did a pip install whatever hku version they had, then after the error tried to install from source... and then did a pip list installed = 1.2.3 so do I _install_ 1.2.4?",
          "created_at": "2025-03-09T16:44:23Z"
        }
      ]
    },
    {
      "issue_number": 1516,
      "title": "[Question]:object of type 'list' can't be used in 'await'",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhen inserting a PDF using the web UI, the following problem occurred \n\n![Image](https://github.com/user-attachments/assets/94d49a98-7995-4618-a967-c66e314cd2fd)\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "cxcx-nice",
      "author_type": "User",
      "created_at": "2025-05-04T01:49:47Z",
      "updated_at": "2025-05-04T05:52:27Z",
      "closed_at": "2025-05-04T05:52:26Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1516/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1516",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1516",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:27.163887",
      "comments": [
        {
          "author": "cxcx-nice",
          "body": "have already fix  ",
          "created_at": "2025-05-04T05:52:26Z"
        }
      ]
    },
    {
      "issue_number": 932,
      "title": "[Question]: rag.insert() is too slow and how to monitor the status from insert() so that I can know what's the rootcause.",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nrag.insert() is too slow and how to monitor the status from insert() so that I can know what's the rootcause. \nrag = LightRAG(\n       working_dir=GRAPHDATA_FOLDER,\n        addon_params={\n        \"insert_batch_size\": 100  # Process 20 documents per batch\n        },\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"deepseek-r1:14b\",  # Use mixtral-8x7b-instruct model\n        llm_model_max_async=4,  # Maximum concurrent requests\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",  # Ollama service address\n            \"options\": {\"num_ctx\": 32768}  # Context window size\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embed(\n                texts, \n                embed_model=\"nomic-embed-text:latest\",  # Use nomic-embed-text as embedding model\n                host=\"http://localhost:11434\"\n            ),\n        ),\n    )\n    documents = []\n    ids = []\n.....\n  rag.insert(documents,  ids=ids)\n    \n    print(\"LightRAG generated successfully\")\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Tasarinan",
      "author_type": "User",
      "created_at": "2025-02-24T09:49:04Z",
      "updated_at": "2025-05-03T13:47:31Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/932/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/932",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/932",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:27.329005",
      "comments": [
        {
          "author": "LarFii",
          "body": "Yes, Ollama is generally quite slow, while vLLM performs much better. Additionally, setting the log level to debug can provide more detailed information.",
          "created_at": "2025-02-25T04:09:47Z"
        },
        {
          "author": "shoaibshaikh07",
          "body": "I'm new here. i also have issue with rag.insert() function, it's very slow.\nBtw I'm using api service for embedding and Ollama Qwen 2.5 as LLM. I have almost 72 documents in queue, but the first document is still being inserted from last 30 minutes.\nGPU: Nvidia L4 (24G)\n\nWhat's the issue? If it's sl",
          "created_at": "2025-03-04T14:37:52Z"
        },
        {
          "author": "rguntha",
          "body": "Same issue for me. It is stuck with rag.insert call. I even modified the call to rag.insert(\"Simple Text\"), just two words, it is still stuck.",
          "created_at": "2025-03-05T10:11:53Z"
        },
        {
          "author": "shoaibshaikh07",
          "body": "Is the problem really with ollama or local models? or the Lightrag is slow anyways?",
          "created_at": "2025-03-05T18:11:50Z"
        },
        {
          "author": "rguntha",
          "body": "I am not sure. Today I will try with hugging-face. I really want to run lightrag locally using locally run models. I have been trying with the previous versions of lightrag to see if the problem goes away, but I am facing some library compatibility issues.",
          "created_at": "2025-03-06T04:42:17Z"
        }
      ]
    },
    {
      "issue_number": 1495,
      "title": "[Bug]: \"Installing and Using PostgreSQL with LightRAG\" readme is outdated",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n> lightrag-server --port 9621 --key sk-somepassword --kv-storage PGKVStorage --graph-storage PGGraphStorage --vector-storage PGVectorStorage --doc-status-storage PGDocStatusStorage\n\nThis doesn't work, and in-fact a look at the code shows that lightrag relies on environment variables for this\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "maharjun",
      "author_type": "User",
      "created_at": "2025-04-29T17:13:22Z",
      "updated_at": "2025-05-02T16:58:15Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1495/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1495",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1495",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:27.515036",
      "comments": [
        {
          "author": "maharjun",
          "body": "Also, the mention of a config.ini is also outdated. you rely on environment variables",
          "created_at": "2025-04-29T17:18:37Z"
        },
        {
          "author": "danielaskdd",
          "body": "Thanks for reporting. The updated doc is removed. The config.ini file should work for configuration realated to DB connection params.",
          "created_at": "2025-04-30T03:10:10Z"
        },
        {
          "author": "maharjun",
          "body": "That document contained a lot of useful information that is not available anywhere else. such as the fact that I needed to run `CREATE EXTENSION vector;` etc. The only part that seems outdated is the command invocation and the details regarding the final command. Removing the doc actually caused qui",
          "created_at": "2025-05-02T14:40:18Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please kindly submit a PR to restore it with the correct version.",
          "created_at": "2025-05-02T16:58:14Z"
        }
      ]
    },
    {
      "issue_number": 1094,
      "title": "[Bug]: <title> In the case, knowledge_graph.html cannot be displayed well",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI deployed lightrag according to the official steps, but my python3.10 and ollama local large model generation case will result in an incomplete knowledge graph. What should I do?\n\n<img width=\"954\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a614c896-77c1-4c0e-b278-a6aac5b07a9c\" />\nMy operating system is Mac OS, and the local large models are gemma2:2b and nomic-embed-text:latest\n\n### Steps to reproduce\n\nFollow the official video\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Slian22",
      "author_type": "User",
      "created_at": "2025-03-15T17:29:31Z",
      "updated_at": "2025-05-02T15:38:36Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1094/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1094",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1094",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:27.690901",
      "comments": [
        {
          "author": "Ja1aia",
          "body": "You could try using bigger model, because lightRAG knowledge graph generation heavily relied on the LLM capability.",
          "created_at": "2025-03-17T03:56:52Z"
        },
        {
          "author": "BireleyX",
          "body": "had a similar issue. was able to reduce 'orphan' entities be using a better LLM and also reducing chunk size to a value between 200 to 400 and overlap to a value between 20-100.",
          "created_at": "2025-03-18T13:11:21Z"
        },
        {
          "author": "Slian22",
          "body": "Ok, I'll try it. Thanks.",
          "created_at": "2025-03-21T12:19:52Z"
        },
        {
          "author": "pateldeepp",
          "body": "I am facing similar issue. Which better LLM are you referring to ? @BireleyX . I have this set up in Mac Mini M4, 16GB RAM - 2024 Model. I am also using local large models are **gemma2:2b and nomic-embed-text:latest**",
          "created_at": "2025-03-22T17:40:59Z"
        },
        {
          "author": "BireleyX",
          "body": "what i meant by better LLM was change from gpt-4o-mini to gpt-4o. when possible, use the model variant with the most parameters your hardware (in case of local AI) can support.",
          "created_at": "2025-05-02T15:38:35Z"
        }
      ]
    },
    {
      "issue_number": 1476,
      "title": "[Bug]: n8n ollama chat node not able to produce expected output",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI tried to connect a local docker n8n instance to my docker hosted LightRAG using the \"Ollama Model\" and \"Ollama Chat Model\" nodes. Both ways fail with different issues. \n\nWorkaround: Using a \"Http request\" node pointing to the /query api works as expected, but does not have same convenience features. \n\n### Steps to reproduce\n\n- Setup a new Ollama credential object in n8n pointing to the LightRAG url\n- Create a new n8n workflow \n- Add a Chat message, a \"Basic llm chain\" and a \"Ollama model\" node\n- Change the \"Basic llm chain\" input prompt to \"/mix {{ $json.chatInput }}\"\n- Select \"lightrag:latest\" in the Ollama node\n- Enter a chat message -> Chain starts to work but produces no output\nInput json\n```\n{\n  \"messages\": [\n    \"/mix how to determine a version of a exe file?\"\n  ],\n  \"estimatedTokens\": 12,\n  \"options\": {\n    \"base_url\": \"http://localhost:9621\",\n    \"model\": \"lightrag:latest\"\n  }\n}\n```\nOutput json\n```\n{\n  \"response\": {\n    \"generations\": [\n      [\n        {\n          \"text\": \"\"\n        }\n      ]\n    ]\n  },\n  \"tokenUsageEstimate\": {\n    \"completionTokens\": 0,\n    \"promptTokens\": 12,\n    \"totalTokens\": 12\n  }\n}\n```\n\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\nMainly default\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: v1.3.4/0162\n- Operating System: Windows 11\n",
      "state": "open",
      "author": "malt001",
      "author_type": "User",
      "created_at": "2025-04-27T10:49:54Z",
      "updated_at": "2025-05-02T12:57:59Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1476/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1476",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1476",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:27.878439",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Please provide the LightRAG Server logs for troubleshooting purposes.",
          "created_at": "2025-04-27T19:13:13Z"
        },
        {
          "author": "malt001",
          "body": "Sorry, but where can i find the server logs?",
          "created_at": "2025-05-02T09:52:11Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please verify that n8n is transmitting queries to LightRAG in the correct format. To confirm Ollama emulation functionality, execute the following command:\n```\ncurl -N -X POST http://localhost:9621/api/chat -H \"Content-Type: application/json\" -d \\\n  '{\"model\":\"lightrag:latest\",\"messages\":[{\"role\":\"u",
          "created_at": "2025-05-02T12:57:58Z"
        }
      ]
    },
    {
      "issue_number": 1458,
      "title": "[Question]:Unexpected Restart in Chunk Processing",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhile generating knowledge graphs with LightRAG, I noticed an issue: after processing around 5400 out of 10,600 chunks, the system unexpectedly restarted from chunk 22.\n\nThere were no errors or warnings in the logs, and processing continued as if normal. The fulldoc status showed the document as failed.\nCould this be due to an internal retry mechanism or cache issue? Any insight would be helpful.\n\nThanks!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Utkarshagharat58",
      "author_type": "User",
      "created_at": "2025-04-25T07:54:15Z",
      "updated_at": "2025-05-02T06:33:57Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1458/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1458",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1458",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:28.060035",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "A single large file containing 10,600 chunks is not optimal. LightRAG needs to process the entire file in memory before writing it to disk. We recommend splitting the document so that each file contains no more than 100 chunks.\n\nAdditionally, for very large document collections, it is advisable to a",
          "created_at": "2025-04-26T00:15:21Z"
        },
        {
          "author": "44cort44",
          "body": "> A single large file containing 10,600 chunks is not optimal. LightRAG needs to process the entire file in memory before writing it to disk. We recommend splitting the document so that each file contains no more than 100 chunks.\n> \n> Additionally, for very large document collections, it is advisabl",
          "created_at": "2025-04-26T04:05:56Z"
        },
        {
          "author": "danielaskdd",
          "body": "The default storage configuration is an all-in-one solution, which simplifies the initial setup. The official Docker Compose file, with recommending separated database configuration, will be released soon.",
          "created_at": "2025-04-27T01:37:38Z"
        },
        {
          "author": "44cort44",
          "body": "> The default storage configuration is an all-in-one solution, which simplifies the initial setup. The official Docker Compose file, with recommending separated database configuration, will be released soon.\n\nThis is wonderful news. Thanks for the hard work!",
          "created_at": "2025-04-27T01:40:10Z"
        },
        {
          "author": "Utkarshagharat58",
          "body": "> A single large file containing 10,600 chunks is not optimal. LightRAG needs to process the entire file in memory before writing it to disk. We recommend splitting the document so that each file contains no more than 100 chunks.\n> \n> Additionally, for very large document collections, it is advisabl",
          "created_at": "2025-04-27T11:52:21Z"
        }
      ]
    },
    {
      "issue_number": 1387,
      "title": "[Question]: Why the change to merging and upserting nodes and edges serially?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nOn Release 1.3.1 the `extract_entities` had a change where nodes and edges went from being merged+upserted in parallel to serial. This seems to slow things down; what was the reason for this change?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "alineberry",
      "author_type": "User",
      "created_at": "2025-04-16T14:54:20Z",
      "updated_at": "2025-05-01T23:51:24Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1387/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1387",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1387",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:28.251412",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Too many concurrent requests can generate excessive database connections, consuming a large amount of resources and leading to a performance decline. The correct approach is to use batch mode for data queries. The main branch has already been optimized following this principle. Currently, the perfor",
          "created_at": "2025-04-16T17:06:19Z"
        },
        {
          "author": "danielaskdd",
          "body": "The bottleneck in node merging occurs during LLM calls, and simply increasing Python's concurrency isn't the right solution. In v1.3.2, we've optimized the process by deferring node merging until after each document's processing phase, followed by a comprehensive merge of all nodes with identical na",
          "created_at": "2025-04-16T17:37:29Z"
        },
        {
          "author": "danielaskdd",
          "body": "The latest version on main branch significantly optimizes file parallel processing logic. The entity/relationship merge phase no longer blocks the extraction phase tasks, ensuring LLM resources maintain full utilization without idle periods.",
          "created_at": "2025-04-28T16:26:03Z"
        },
        {
          "author": "alineberry",
          "body": "Yes, we have identified that optimization as well. Seems it would double the throughput for our use case. \n\nI am also curious what is the status of using dense retrieval + LLM during the entity merging step on your end? We see massive duplication (~60-80% compression rate) with the v1.3.0 we work fr",
          "created_at": "2025-04-29T19:42:19Z"
        },
        {
          "author": "danielaskdd",
          "body": "> Yes, we have identified that optimization as well. Seems it would double the throughput for our use case.\n> \n> I am also curious what is the status of using dense retrieval + LLM during the entity merging step on your end? We see massive duplication (~60-80% compression rate) with the v1.3.0 we wo",
          "created_at": "2025-04-30T04:40:42Z"
        }
      ]
    },
    {
      "issue_number": 1501,
      "title": "[Question]: A bit of help integrating with OpenWebUI?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI read through the README.md instructions, and did manage to make the lightrag:latest model appear on the interface. However, when sending a question I get an error: \n\n500: Ollama: 500, message='Internal Server Error', url='http://host.docker.internal:9621/api/chat'\n\nI dont understand how can I modify the endpoints in either OpenWebUI or LightRag, so some pointers would be useful. \n\nI placed the connection on the admin panel under Ollama connections, if it matters. \n\nThanks!\n\n\n\n\n\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "JotaDeRodriguez",
      "author_type": "User",
      "created_at": "2025-04-30T14:40:22Z",
      "updated_at": "2025-04-30T19:47:44Z",
      "closed_at": "2025-04-30T19:47:43Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1501/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1501",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1501",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:28.477663",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "It appears your Open WebUI is deployed in a Docker container. To ensure accessibility, you must configure host.docker.internal to be reachable from Open WebUI by adding the following parameter to your docker run command\n\n```\ndocker run -d -p 5023:8080 --restart unless-stopped \\\n  --add-host=host.doc",
          "created_at": "2025-04-30T16:38:31Z"
        },
        {
          "author": "JotaDeRodriguez",
          "body": "Fixed! In the end it was an issue with the way I set up the docker container for LightRAG itself, that was evident when I got the error even when querying the web interface, should've tested it sooner. Re-installing from a clean slate did the trick. I'll close it now, thanks for the reply!",
          "created_at": "2025-04-30T19:47:43Z"
        }
      ]
    },
    {
      "issue_number": 1503,
      "title": "[Question]:Does lightrag support multithreaded insertion into the same workspace or merging across different workspaces?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI have lightrag processing results for different datasets stored in different directories. Essentially, these datasets are not significantly different from each other. I now want to merge them, because otherwise I would have to create over 200 separate lightrag instances in my code and query them individually — which is clearly not a smart solution. \n\nTo merge them, I looked into the vdb_entity.json and vdb_relationship.json files, but I couldn't get the final matrix field. After some investigation, I found that it's possible to regenerate this by keeping only the kv_store_llm_response_cache.json , according to https://github.com/HKUDS/LightRAG/issues/1280. So I merged all the kv_store_llm_response_cache files from each working directory, and randomly discarded entries with duplicate UUIDs, hoping to regenerate the datasets.\n\nHowever, I found that single-threaded insertion is extremely slow. Is there a multithreaded insertion solution available? At this point, I'm not sure where the performance bottleneck lies.\n\nIf the above is not supported, can you create an instance whose workspace can contain many workspaces? I don't really want to create 200+ lightrag instances.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Socratesa",
      "author_type": "User",
      "created_at": "2025-04-30T18:07:52Z",
      "updated_at": "2025-04-30T18:07:59Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1503/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1503",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1503",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:28.697209",
      "comments": []
    },
    {
      "issue_number": 1500,
      "title": "[Question]: Whys is there need for chunks if embeddings have chunk info?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIf the vector embeddings already contain chunk information, why is there a need to send both the knowledge graph and the original text chunks for answering a query? Wouldn’t the embeddings alone be sufficient for retrieving the relevant chunks?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ug200203",
      "author_type": "User",
      "created_at": "2025-04-30T11:07:16Z",
      "updated_at": "2025-04-30T16:31:42Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1500/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1500",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1500",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:28.697231",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The adoption of Graph-based RAG is beyond the purview of issue support here.",
          "created_at": "2025-04-30T16:31:42Z"
        }
      ]
    },
    {
      "issue_number": 1462,
      "title": "[Bug]:The process stops and hangs",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHello, recently when I was running Lightrag, I also encountered a similar problem. After changing the content of the book.txt file, I deleted the contents in the./dickens directory, but it still stopped in a certain process and was in a suspended state. I have no idea why\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "pure-df",
      "author_type": "User",
      "created_at": "2025-04-25T12:00:27Z",
      "updated_at": "2025-04-30T16:19:25Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1462/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1462",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1462",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:28.891281",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Please run LightRAG Server, and using it's webui and API to evaluate LightRAG first.",
          "created_at": "2025-04-26T00:01:24Z"
        },
        {
          "author": "pure-df",
          "body": "I'm using the large model and embedded model in the locally deployed ollama. Is it also operated in this way? Could you provide a detailed entire process of changing the content of book.txt\n",
          "created_at": "2025-04-26T05:41:49Z"
        },
        {
          "author": "NavneetKamboj",
          "body": "trying from last 3 weeks. I too am using ollama with qwen3:4b which is around 4gb and nomic embed model which is around 200mb, i have a 8gb rtx 4060 but only 5 gb is utilized and 2gb is still free, i don't encounter out of memory issues, just i don't see /embed api calls in ollama logs every time, 9",
          "created_at": "2025-04-30T13:31:29Z"
        },
        {
          "author": "danielaskdd",
          "body": "The qwen3:4b model may not possess sufficient capability for LightRAG to effectively perform entity and relation extraction tasks. At least 32B LLM model is recommended for this purpose. Please conduct an initial evaluation using an LLM API. You'd better use LightRAG Server and webui to make sure LL",
          "created_at": "2025-04-30T16:19:24Z"
        }
      ]
    },
    {
      "issue_number": 1496,
      "title": "[Bug]:前端webui的时间和本地时间不一致",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n通过docker-compose.yml文件部署lightrag后，发现webui前端显示的创建时间和更新时间，与本地不一致？\n如图：\n\n![Image](https://github.com/user-attachments/assets/1445b9c3-7ac4-45a0-be84-a35d26c07b68)\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n请问是否有地方设置本地时间timezone？我的timezone应该是Asia/Shanghai\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:v1.3.6/0165\n- Operating System:Windows11\n- Python Version:3.12\n- Related Issues:\n",
      "state": "open",
      "author": "ruanjunmin",
      "author_type": "User",
      "created_at": "2025-04-30T06:11:18Z",
      "updated_at": "2025-04-30T10:44:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1496/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1496",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1496",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:29.056750",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "这个可能是API没有正确处理时区。作为临时的解决方案，你可以修改docker-compose.yml让其把容器的时区设置为当地时间。谢谢。",
          "created_at": "2025-04-30T08:17:24Z"
        },
        {
          "author": "ruanjunmin",
          "body": "> 这个可能是API没有正确处理时区。作为临时的解决方案，你可以修改docker-compose.yml让其把容器的时区设置为当地时间。谢谢。\n\n好的 ，谢谢大佬，我试试。\n",
          "created_at": "2025-04-30T08:20:24Z"
        },
        {
          "author": "ruanjunmin",
          "body": "是这样修改吗？希望大佬指导下。感激不尽！\n\n![Image](https://github.com/user-attachments/assets/89821ec3-533b-4746-a5ed-7c586634a18c)",
          "created_at": "2025-04-30T08:23:40Z"
        },
        {
          "author": "ruanjunmin",
          "body": "不行，容器的时区修改后显示是正确的时间，但是前端webui测试了下还是显示错误的时间。\n\n![Image](https://github.com/user-attachments/assets/db375247-e589-4442-a2c9-d7b84cd5d236)\n\n![Image](https://github.com/user-attachments/assets/f253629f-23cc-4458-9733-2171fc33a3c7)",
          "created_at": "2025-04-30T08:34:04Z"
        },
        {
          "author": "danielaskdd",
          "body": "清空数据之后重新索引文件看看。",
          "created_at": "2025-04-30T10:44:50Z"
        }
      ]
    },
    {
      "issue_number": 1498,
      "title": "[Bug]:尝试将图谱存入neo4j时报错",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n使用test_neo4j出错:\n```\nD:\\py\\Python-3.11.3(64bit)\\python.exe D:\\final_codes\\LightRAG-1.3.6\\examples\\test_neo4j.py \n'D:\\py\\Python-3.11.3' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n���������ļ���\nINFO: Process 15976 Shared-Data created for Single Process\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir\\\\vdb_chunks.json'} 0 data\nINFO: Process 15976 initialized updated flags for namespace: [full_docs]\nINFO: Process 15976 ready to initialize storage namespace: [full_docs]\nINFO: Process 15976 initialized updated flags for namespace: [text_chunks]\nINFO: Process 15976 ready to initialize storage namespace: [text_chunks]\nINFO: Process 15976 initialized updated flags for namespace: [entities]\nINFO: Process 15976 initialized updated flags for namespace: [relationships]\nINFO: Process 15976 initialized updated flags for namespace: [chunks]\nINFO: Process 15976 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 15976 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 15976 initialized updated flags for namespace: [doc_status]\nINFO: Process 15976 ready to initialize storage namespace: [doc_status]\nINFO: Process 15976 storage namespace already initialized: [full_docs]\nINFO: Process 15976 storage namespace already initialized: [text_chunks]\nINFO: Process 15976 storage namespace already initialized: [llm_response_cache]\nINFO: Process 15976 storage namespace already initialized: [doc_status]\nINFO: Process 15976 Pipeline namespace initialized\nFailed to extrat document doc-d78eff4cdcdbf3c13bd97da07727ba9c: Traceback (most recent call last):\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 990, in process_document\n    await asyncio.gather(*tasks)\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\kg\\nano_vector_db_impl.py\", line 116, in upsert\n    results = client.upsert(datas=list_data)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\nano_vectordb\\dbs.py\", line 116, in upsert\n    self.__storage[\"matrix\"] = np.vstack([self.__storage[\"matrix\"], new_matrix])\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\numpy\\core\\shape_base.py\", line 289, in vstack\n    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 768 and the array at index 1 has size 1024\n\nTraceback (most recent call last):\n  File \"D:\\final_codes\\LightRAG-1.3.6\\examples\\test_neo4j.py\", line 111, in <module>\n    main()\n  File \"D:\\final_codes\\LightRAG-1.3.6\\examples\\test_neo4j.py\", line 77, in main\n    asyncio.run(rag.finalize_storages())\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\base_events.py\", line 653, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 506, in finalize_storages\n    await asyncio.gather(*tasks)\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\kg\\neo4j_impl.py\", line 194, in finalize\n    await self._driver.close()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\driver.py\", line 650, in close\n    await self._pool.close()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_pool.py\", line 607, in close\n    await self._close_connections(connections)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_pool.py\", line 533, in _close_connections\n    await connection.close()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_bolt.py\", line 977, in close\n    await self._send_all()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_bolt.py\", line 819, in _send_all\n    if await self.outbox.flush():\n       ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_common.py\", line 144, in flush\n    await self.socket.sendall(data)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py\", line 161, in sendall\n    self._writer.write(data)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\streams.py\", line 332, in write\n    self._transport.write(data)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\proactor_events.py\", line 365, in write\n    self._loop_writing(data=bytes(data))\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\proactor_events.py\", line 401, in _loop_writing\n    self._write_fut = self._loop._proactor.send(self._sock, data)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'send'\nException ignored in: <function LightRAG.__del__ at 0x0000023FBC1994E0>\nTraceback (most recent call last):\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 444, in __del__\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 449, in _run_async_safely\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\utils.py\", line 1193, in always_get_an_event_loop\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\events.py\", line 806, in new_event_loop\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\events.py\", line 695, in new_event_loop\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\windows_events.py\", line 315, in __init__\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\proactor_events.py\", line 642, in __init__\nImportError: sys.meta_path is None, Python is likely shutting down\n\n进程已结束,退出代码-1073741819 (0xC0000005)\n\n```\n\n### Steps to reproduce\n\n```\nimport os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.llm.openai import openai_complete_if_cache\nfrom lightrag.llm.siliconcloud import siliconcloud_embedding\nimport numpy as np\n\n\nWORKING_DIR = \"./local_neo4jWorkDir\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        graph_storage=\"Neo4JStorage\",\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768, max_token_size=512, func=embedding_func\n        )\n    )\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"Qwen/Qwen2.5-7B-Instruct\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=\"...\",#隐去\n        base_url=\"https://api.siliconflow.cn/v1/\",\n        **kwargs,\n    )\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await siliconcloud_embedding(\n        texts,\n        model=\"BAAI/bge-m3\",\n        api_key=\"....\",\n        max_token_size=512\n    )\n\n\n\ndef main():\n    # Initialize RAG instance\n    rag = asyncio.run(initialize_rag())\n\n    try:\n        # 使用python-docx库读取docx文件\n        from docx import Document\n        doc = Document(r\"D:\\final_codes\\LightRAG-1.3.6\\lightrag_webui\\inputs\\1.docx\")\n        content = \"\\n\".join([para.text for para in doc.paragraphs])\n        rag.insert(content)\n\n    except Exception as e:\n        print(f\"文件处理失败: {str(e)}\")\n    finally:\n        # 确保资源释放\n        asyncio.run(rag.finalize_storages())\n\n    # Perform naive search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n\n    # Perform local search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\n\n    # Perform global search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n\n    # Perform hybrid search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n\n    # 显式关闭RAG实例（解决驱动关闭异常）\n    asyncio.run(rag.finalize_storages())\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:1.3.6\n",
      "state": "open",
      "author": "wenquxing1",
      "author_type": "User",
      "created_at": "2025-04-30T07:16:40Z",
      "updated_at": "2025-04-30T10:39:26Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1498/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1498",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1498",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:29.252765",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "你可能是修改了Embedding模型，向量数据库需要保持模型不变。请删除所有数据后在重新索引。",
          "created_at": "2025-04-30T08:19:52Z"
        },
        {
          "author": "wenquxing1",
          "body": "我不做修改直接运行会报这个错误：\n```\nD:\\py\\Python-3.11.3(64bit)\\python.exe D:\\final_codes\\LightRAG-1.3.2\\examples\\test_neo4j.py \n<string>:36: UserWarning: WARNING: log_level parameter is deprecated, use setup_logger in utils.py instead\nINFO: Process 15684 Shared-Data created for Single Process\nTraceback (most recen",
          "created_at": "2025-04-30T08:31:09Z"
        },
        {
          "author": "danielaskdd",
          "body": "你的Embedding函数没有设置 embedding_dim 参数",
          "created_at": "2025-04-30T10:30:41Z"
        },
        {
          "author": "danielaskdd",
          "body": "test_neo4j.py 测试文件有Bug，初始化 LightRAG 的时候没有添加 embedding_func 参数：\n\n```\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model\n        embedding_func=openai_embed,\n        graph_storage=\"Neo4JSto",
          "created_at": "2025-04-30T10:39:24Z"
        }
      ]
    },
    {
      "issue_number": 1474,
      "title": "[Feature Request]:Add semaphore on extracting phase and release it before merging entities and relations",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nWhen processing documents with parallel n, after the n documents have been extracted, n-1 coroutines are waiting for the graph_db_lock, causing the LLM to be idle. During this period, we can utilize the LLM to extract other documents.\n\nFor instance, when max_parallel_insert is set to 2, after two documents have been extracted, one document enters the merging stage and one document is waiting for the graph_db_lock. During this period, two more documents can be placed into the extraction stage.\n\nSo I suggest:\nAdd(or move) semaphore on extracting phase, and release it before merging entities and relations, which can maximize the utilization of GPU computing power.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "zhouzhou12",
      "author_type": "User",
      "created_at": "2025-04-27T02:51:47Z",
      "updated_at": "2025-04-30T08:38:33Z",
      "closed_at": "2025-04-30T08:38:33Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1474/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1474",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1474",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:29.415731",
      "comments": [
        {
          "author": "zhouzhou12",
          "body": "eg:\n\nin LightRAG.apipeline_process_enqueue_documents\n```python\n                # Create a semaphore to limit the number of concurrent file processing\n                semaphore = asyncio.Semaphore(self.max_parallel_insert * 2)\n                extracting_semaphore = asyncio.Semaphore(self.max_parallel",
          "created_at": "2025-04-27T05:36:20Z"
        },
        {
          "author": "danielaskdd",
          "body": "Your suggestion is pretty nice actually. The latest release (v1.3.4) has resolved this issue by implementing semaphore-based concurrency control for chunk and file processing. You may review the implementation and identify potential areas for improvement.",
          "created_at": "2025-04-27T06:44:11Z"
        },
        {
          "author": "zhouzhou12",
          "body": "> Your suggestion is pretty nice actually. The latest release (v1.3.4) has resolved this issue by implementing semaphore-based concurrency control for chunk and file processing. You may review the implementation and identify potential areas for improvement.\n\n@danielaskdd \n\nI have seen the semaphore ",
          "created_at": "2025-04-27T06:52:02Z"
        },
        {
          "author": "danielaskdd",
          "body": "I finally get your point. We should use a semaphore to limit the number of parallel files during the extraction stage. Once extraction is complete, the semaphore should be released before proceeding to the merge stage. Is that right?",
          "created_at": "2025-04-27T14:54:21Z"
        },
        {
          "author": "danielaskdd",
          "body": "The merge stage no longer blocks file extraction (Resolved by PR #1478)",
          "created_at": "2025-04-27T18:59:33Z"
        }
      ]
    },
    {
      "issue_number": 1467,
      "title": "[Question]:当我自定义插入实体和关系时file_path出错",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n自定义插入实体和关系时file_path出错\n\n### Additional Context\n\n![Image](https://github.com/user-attachments/assets/0ff55e7c-9a5d-4430-a8ed-b79dd80d8a3e)",
      "state": "closed",
      "author": "wenquxing1",
      "author_type": "User",
      "created_at": "2025-04-26T03:57:24Z",
      "updated_at": "2025-04-30T07:19:43Z",
      "closed_at": "2025-04-30T07:19:43Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1467/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1467",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1467",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:29.661285",
      "comments": [
        {
          "author": "wenquxing1",
          "body": "D:\\py\\Python-3.11.3(64bit)\\python.exe D:\\final_codes\\LightRAG-1.3.1\\custom_kg_demo.py \nINFO: Process 8716 Shared-Data created for Single Process\nINFO: Process 8716 initialized updated flags for namespace: [doc_status]\nINFO: Process 8716 ready to initialize storage namespace: [doc_status]\nINFO: Proce",
          "created_at": "2025-04-26T03:57:31Z"
        },
        {
          "author": "danielaskdd",
          "body": "Thanks for reporting this bug. It will be fixed in next release.",
          "created_at": "2025-04-26T14:19:28Z"
        }
      ]
    },
    {
      "issue_number": 1497,
      "title": "[Bug]:尝试将图谱存入到neo4j时使用test_neo4j报错",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n尝试将图谱存入到neo4j时使用test_neo4j报错：（使用的neo4j社区版）\n\n\nD:\\py\\Python-3.11.3(64bit)\\python.exe D:\\final_codes\\LightRAG-1.3.6\\examples\\test_neo4j.py \n'D:\\py\\Python-3.11.3' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n���������ļ���\nINFO: Process 15976 Shared-Data created for Single Process\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir\\\\vdb_chunks.json'} 0 data\nINFO: Process 15976 initialized updated flags for namespace: [full_docs]\nINFO: Process 15976 ready to initialize storage namespace: [full_docs]\nINFO: Process 15976 initialized updated flags for namespace: [text_chunks]\nINFO: Process 15976 ready to initialize storage namespace: [text_chunks]\nINFO: Process 15976 initialized updated flags for namespace: [entities]\nINFO: Process 15976 initialized updated flags for namespace: [relationships]\nINFO: Process 15976 initialized updated flags for namespace: [chunks]\nINFO: Process 15976 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 15976 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 15976 initialized updated flags for namespace: [doc_status]\nINFO: Process 15976 ready to initialize storage namespace: [doc_status]\nINFO: Process 15976 storage namespace already initialized: [full_docs]\nINFO: Process 15976 storage namespace already initialized: [text_chunks]\nINFO: Process 15976 storage namespace already initialized: [llm_response_cache]\nINFO: Process 15976 storage namespace already initialized: [doc_status]\nINFO: Process 15976 Pipeline namespace initialized\nFailed to extrat document doc-d78eff4cdcdbf3c13bd97da07727ba9c: Traceback (most recent call last):\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 990, in process_document\n    await asyncio.gather(*tasks)\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\kg\\nano_vector_db_impl.py\", line 116, in upsert\n    results = client.upsert(datas=list_data)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\nano_vectordb\\dbs.py\", line 116, in upsert\n    self.__storage[\"matrix\"] = np.vstack([self.__storage[\"matrix\"], new_matrix])\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\numpy\\core\\shape_base.py\", line 289, in vstack\n    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 768 and the array at index 1 has size 1024\n\nTraceback (most recent call last):\n  File \"D:\\final_codes\\LightRAG-1.3.6\\examples\\test_neo4j.py\", line 111, in <module>\n    main()\n  File \"D:\\final_codes\\LightRAG-1.3.6\\examples\\test_neo4j.py\", line 77, in main\n    asyncio.run(rag.finalize_storages())\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\base_events.py\", line 653, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 506, in finalize_storages\n    await asyncio.gather(*tasks)\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\kg\\neo4j_impl.py\", line 194, in finalize\n    await self._driver.close()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\driver.py\", line 650, in close\n    await self._pool.close()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_pool.py\", line 607, in close\n    await self._close_connections(connections)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_pool.py\", line 533, in _close_connections\n    await connection.close()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_bolt.py\", line 977, in close\n    await self._send_all()\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_bolt.py\", line 819, in _send_all\n    if await self.outbox.flush():\n       ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async\\io\\_common.py\", line 144, in flush\n    await self.socket.sendall(data)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py\", line 161, in sendall\n    self._writer.write(data)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\streams.py\", line 332, in write\n    self._transport.write(data)\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\proactor_events.py\", line 365, in write\n    self._loop_writing(data=bytes(data))\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\proactor_events.py\", line 401, in _loop_writing\n    self._write_fut = self._loop._proactor.send(self._sock, data)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'send'\nException ignored in: <function LightRAG.__del__ at 0x0000023FBC1994E0>\nTraceback (most recent call last):\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 444, in __del__\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\lightrag.py\", line 449, in _run_async_safely\n  File \"D:\\final_codes\\LightRAG-1.3.6\\lightrag\\utils.py\", line 1193, in always_get_an_event_loop\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\events.py\", line 806, in new_event_loop\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\events.py\", line 695, in new_event_loop\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\windows_events.py\", line 315, in __init__\n  File \"D:\\py\\Python-3.11.3(64bit)\\Lib\\asyncio\\proactor_events.py\", line 642, in __init__\nImportError: sys.meta_path is None, Python is likely shutting down\n\n进程已结束,退出代码-1073741819 (0xC0000005)\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:1.3.6\ncode:\nimport os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.llm.openai import openai_complete_if_cache\nfrom lightrag.llm.siliconcloud import siliconcloud_embedding\nimport numpy as np\n\n#########\n# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n# import nest_asyncio\n# nest_asyncio.apply()\n#########\n\nWORKING_DIR = \"./local_neo4jWorkDir\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n    # os.environ[\"NEO4J_DATABASE\"] = \"neo4j\"\n\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        graph_storage=\"Neo4JStorage\",\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768, max_token_size=512, func=embedding_func\n        )\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"Qwen/Qwen2.5-7B-Instruct\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=\"\", #隐去\n        base_url=\"https://api.siliconflow.cn/v1/\",\n        **kwargs,\n    )\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await siliconcloud_embedding(\n        texts,\n        model=\"BAAI/bge-m3\",\n        api_key=\"\", #隐去\n        max_token_size=512\n    )\n\n\n\ndef main():\n    # Initialize RAG instance\n    rag = asyncio.run(initialize_rag())\n\n    try:\n        # 使用python-docx库读取docx文件\n        from docx import Document\n        doc = Document(r\"D:\\final_codes\\LightRAG-1.3.6\\lightrag_webui\\inputs\\1.docx\")\n        content = \"\\n\".join([para.text for para in doc.paragraphs])\n        rag.insert(content)\n\n    except Exception as e:\n        print(f\"文件处理失败: {str(e)}\")\n    finally:\n        # 确保资源释放\n        asyncio.run(rag.finalize_storages())\n\n    # Perform naive search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n\n    # Perform local search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\n\n    # Perform global search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n\n    # Perform hybrid search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n\n    # 显式关闭RAG实例（解决驱动关闭异常）\n    asyncio.run(rag.finalize_storages())\n\nif __name__ == \"__main__\":\n    main()\n\n",
      "state": "closed",
      "author": "wenquxing1",
      "author_type": "User",
      "created_at": "2025-04-30T07:10:45Z",
      "updated_at": "2025-04-30T07:12:00Z",
      "closed_at": "2025-04-30T07:12:00Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1497/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1497",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1497",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:29.956178",
      "comments": []
    },
    {
      "issue_number": 1395,
      "title": "The script from the example (lightrag_ollama_demo.py) hangs",
      "body": "### Your Question\n\nGood afternoon.\nI encountered a problem when I run a local script (examples/lightrag_ollama_demo.py), the script starts executing and stops at Process 14088 Pipeline namespace initialized and hangs.\n\nAt the same time the computer CPU is at 100%\n\nWhat can be the problem? \nThe document with test data (documents/bim.txt) is quite small - 500 kb.\n\n```\n\nimport asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\nimport os\nimport inspect\nimport logging\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.ollama import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"qwen2.5:7b-8k\",\n        llm_model_max_async=2,\n        llm_model_max_token_size=8192,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",\n            \"options\": {\"num_ctx\": 8192},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embed(\n                texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n            ),\n        ),\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\ndef main():\n    # Initialize RAG instance\n    rag = asyncio.run(initialize_rag())\n\n    # Insert example text\n    with open(\"./documents/bim.txt\", \"r\", encoding=\"utf-8\") as f:\n        rag.insert(f.read())\n\n    # Test different query modes\n    print(\"\\nNaive Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n\n    print(\"\\nLocal Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\n\n    print(\"\\nGlobal Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n\n    print(\"\\nHybrid Search:\")\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n\n    # stream response\n    resp = rag.query(\n        \"What are the top themes in this story?\",\n        param=QueryParam(mode=\"hybrid\", stream=True),\n    )\n\n    if inspect.isasyncgen(resp):\n        asyncio.run(print_stream(resp))\n    else:\n        print(resp)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\n\n### Additional Context\n\n<img width=\"653\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f6555fe1-ce97-4fdf-b85b-c3271e35e425\" />",
      "state": "open",
      "author": "ValyaBabenkov",
      "author_type": "User",
      "created_at": "2025-04-17T09:25:12Z",
      "updated_at": "2025-04-30T00:49:51Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1395/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1395",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1395",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:29.956199",
      "comments": [
        {
          "author": "ValyaBabenkov",
          "body": "I should add that I also tried the test document (book.txt), which is in the documentation, but the problem with freezing remained.",
          "created_at": "2025-04-17T09:33:51Z"
        },
        {
          "author": "pure-df",
          "body": "Hello, recently when I was running Lightrag, I also encountered a similar problem. After changing the content of the book.txt file, I deleted the contents in the./dickens directory, but it still stopped in a certain process and was in a suspended state. I have no idea why",
          "created_at": "2025-04-25T11:57:58Z"
        },
        {
          "author": "will-riley",
          "body": "Same problem.",
          "created_at": "2025-04-29T14:32:07Z"
        },
        {
          "author": "NavneetKamboj",
          "body": "same problem\ndid you get any solution for this\n\n<img width=\"566\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a6c53b8b-3fbb-4abb-ac4e-31c23a615f4d\" />",
          "created_at": "2025-04-29T14:45:44Z"
        },
        {
          "author": "pure-df",
          "body": "Currently, every time I change the content, I will re-extract the code",
          "created_at": "2025-04-30T00:49:50Z"
        }
      ]
    },
    {
      "issue_number": 1251,
      "title": "[Bug]: Error in get_vector_context: 'file_path'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nRunning LightRAG-HKU in a Docker Windows Container environment, the error message in the log below is generated.  During the LightRAG insert process a document link is included in the FILE_PATHS parameter.\n\n2025-04-01 15:19:16 INFO: Process 1460 Shared-Data created for Single Process\n2025-04-01 15:19:16 INFO:nano-vectordb:Load (26, 1536) data\n2025-04-01 15:19:16 INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './chatSvrConfig/lightRAG/ABCxyz\\\\vdb_entities.json'} 26 data\n2025-04-01 15:19:16 INFO:nano-vectordb:Load (16, 1536) data\n2025-04-01 15:19:16 INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './chatSvrConfig/lightRAG/ABCxyz\\\\vdb_relationships.json'} 16 data\n2025-04-01 15:19:16 INFO:nano-vectordb:Load (3, 1536) data\n2025-04-01 15:19:16 INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './chatSvrConfig/lightRAG/ABCxyz\\\\vdb_chunks.json'} 3 data\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [full_docs]\n2025-04-01 15:19:16 INFO: Process 1460 ready to initialize storage namespace: [full_docs]\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [text_chunks]\n2025-04-01 15:19:16 INFO: Process 1460 ready to initialize storage namespace: [text_chunks]\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [entities]\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [relationships]\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [chunks]\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [chunk_entity_relation]\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [llm_response_cache]\n2025-04-01 15:19:16 INFO: Process 1460 ready to initialize storage namespace: [llm_response_cache]\n2025-04-01 15:19:16 INFO: Process 1460 initialized updated flags for namespace: [doc_status]\n2025-04-01 15:19:16 INFO: Process 1460 ready to initialize storage namespace: [doc_status]\n2025-04-01 15:19:16 INFO: Process 1460 storage namespace already initialized: [full_docs]\n2025-04-01 15:19:16 INFO: Process 1460 storage namespace already initialized: [text_chunks]\n2025-04-01 15:19:16 INFO: Process 1460 storage namespace already initialized: [llm_response_cache]\n2025-04-01 15:19:16 INFO: Process 1460 storage namespace already initialized: [doc_status]\n2025-04-01 15:19:16 INFO: Process 1460 Pipeline namespace initialized\n### **2025-04-01 15:19:18 Error in get_vector_context: 'file_path'**\n2025-04-01 15:19:30 INFO: Process 1460 finalizing storage data (multiprocess=False)\n2025-04-01 15:19:30 INFO: Process 1460 storage data finalization complete\n2025-04-01 15:19:30 INFO:tornado.access:200 POST /lightrag (192.168.1.15) 16642.11ms\n\n### Steps to reproduce\n\n### LightRAG-HKU v1.3.0\n\n### Content insert code:\nrag.insert(docText, ids= [orgPrefix + fname] ,file_paths=[\"http://collaborativeai:8890/docs/\" + fname[:-4] + \".pdf\"])\n\n### Build a Python based Tornado server to query the content with prompts.  Below is the docker file. \nFROM python:3.11.9   \nWORKDIR /app\nCOPY . /app\nRUN pip install --no-cache-dir lightrag-hku python-dotenv tornado nest_asyncio python-multipart langchain langchain_openai msvc-runtime\nRUN pip install -r requirements.txt\nCMD [\"python\", \"./main.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8895\"]\n\n### Run a prompt to activate LightRAG.\n\n### Knowledge Graph Links are working and providing document tracing, but not vector content. \n\n### Expected Behavior\n\nLog should not have errors and vector FILE_PATH should provide source document tracability. \n\n### LightRAG Config Used\n\nasync def llm_model_func(prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs) -> str:\n    return await openai_complete_if_cache(\n        gpt-4o,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        base_url=BASE_URL,\n        api_key=API_KEY,\n        **kwargs,\n    )\n\n# Embedding function\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts=texts,\n        model=EMBEDDING_MODEL,\n        base_url=BASE_URL,\n        api_key=API_KEY,\n    )\n\n\n\n### Logs and screenshots\n\nsee log in the description. \n\n### Additional Information\n\n- LightRAG Version: 1.3.0\n- Operating System: latest Windows - Docker with Wincore:ltsc2022\n- Python Version:  3.11.9\n- Related Issues: None.\n",
      "state": "closed",
      "author": "LorenDH",
      "author_type": "User",
      "created_at": "2025-04-01T22:48:13Z",
      "updated_at": "2025-04-29T23:27:30Z",
      "closed_at": "2025-04-29T23:27:29Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1251/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1251",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1251",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:30.228447",
      "comments": [
        {
          "author": "alistp27",
          "body": "same here with ollama selfhosting both embeding and chat generator",
          "created_at": "2025-04-03T22:13:56Z"
        },
        {
          "author": "UmutAlihan",
          "body": "same here with **Azure AI foundry models** for both embedding and chat generator",
          "created_at": "2025-04-06T00:54:48Z"
        },
        {
          "author": "hsbdkdn",
          "body": "same here.",
          "created_at": "2025-04-06T06:10:58Z"
        },
        {
          "author": "leonardocerliani",
          "body": "same here _**but**_ only when using `QueryParam(mode=\"mix\")`",
          "created_at": "2025-04-06T10:35:53Z"
        },
        {
          "author": "steelliberty",
          "body": "Same here -- using a file for text. Reading file and inserting line-by-line. After all the lines are read this message is produced (not an exception) -- \n\nI see that it inserts all of the lines and only after that the error message is printed. \nNOTE: It does not produce this error when using other m",
          "created_at": "2025-04-07T17:10:46Z"
        }
      ]
    },
    {
      "issue_number": 1408,
      "title": "[Bug]: Postgres Global mode does not find entities. Warning: No valid text chunks found",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI am using 1.3.3 with Azure Postgres. AGE and VECTOR extensions are installed and activated. I am using Postgres for all storage types. But I am getting \"No valid text chunks found\" warning with INFO: Global query uses 0 entites, 0 relations, 0 chunks when I use the retrieval. I first observed the behavior in GLOBAL mode but saw the same in HYBRID mode also. \n\nI then changed the graph storage to NetworkX with other storage types still with Postgres. In this set up for the same documents uploaded, I am NOT getting the warning and Global Query is finding expected entities.\n\n### Steps to reproduce\n\nDEBUG: get_best_cached_response:  mode=global cache_type=query use_llm_check=False\nDEBUG: Embedding cached missed(mode:global type:query)\nDEBUG: get_best_cached_response:  mode=global cache_type=keywords use_llm_check=False\nDEBUG: Embedding cached missed(mode:global type:keywords)\nDEBUG: [kg_query]Prompt Tokens: 416\nINFO:  == LLM cache == saving global: 8ea6d7f1aad71ecb22b94373501871bf\nDEBUG: Inserting 1 to llm_response_cache\nDEBUG: High-level keywords: ['EU AI Act', 'Artificial Intelligence regulation', 'Legislation']\nDEBUG: Low-level  keywords: ['European Union', 'AI governance', 'Compliance requirements', 'Ethical guidelines', 'Technology standards']\nINFO: Process 1 buidling query context...\nINFO: Query edges: EU AI Act, Artificial Intelligence regulation, Legislation, top_k: 60, cosine: 0.2\nWARNING: No valid text chunks found\nDEBUG: Truncate entities from 0 to 0 (max tokens:4000)\nINFO: Global query uses 0 entites, 0 relations, 0 chunks\nDEBUG: [kg_query]Prompt Tokens: 378\nDEBUG: Streaming response detected, skipping cache\nINFO: 172.20.0.1:44018 - \"POST /query/stream HTTP/1.1\" 200\nDEBUG: get_best_cached_response:  mode=hybrid cache_type=query use_llm_check=False\nDEBUG: Embedding cached missed(mode:hybrid type:query)\nDEBUG: get_best_cached_response:  mode=hybrid cache_type=keywords use_llm_check=False\nDEBUG: Embedding cached missed(mode:hybrid type:keywords)\nDEBUG: [kg_query]Prompt Tokens: 416\nINFO:  == LLM cache == saving hybrid: e0ca097d99404b6693f9e345d4cc4958\nDEBUG: Inserting 1 to llm_response_cache\nDEBUG: High-level keywords: ['EU AI Act', 'Artificial Intelligence regulation', 'European Union legislation']\nDEBUG: Low-level  keywords: ['AI systems', 'Compliance requirements', 'Risk assessment', 'Transparency', 'Accountability']\nINFO: Process 1 buidling query context...\nINFO: Query nodes: AI systems, Compliance requirements, Risk assessment, Transparency, Accountability, top_k: 60, cosine: 0.2\nDEBUG: Truncate chunks from 55 to 3 (max tokens:4000)\nDEBUG: Truncate relations from 149 to 0 (max tokens:4000)\nDEBUG: Truncate entities from 60 to 18 (max tokens:4000)\nINFO: Local query uses 18 entites, 0 relations, 3 chunks\nINFO: Query edges: EU AI Act, Artificial Intelligence regulation, European Union legislation, top_k: 60, cosine: 0.2\nWARNING: No valid text chunks found\nDEBUG: Truncate entities from 0 to 0 (max tokens:4000)\nINFO: Global query uses 0 entites, 0 relations, 0 chunks\nDEBUG: [kg_query]Prompt Tokens: 4420\nDEBUG: Streaming response detected, skipping cache\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: v1.3.3\n- Operating System: Linux\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "closed",
      "author": "acsangamnerkar",
      "author_type": "User",
      "created_at": "2025-04-18T18:15:14Z",
      "updated_at": "2025-04-29T18:54:51Z",
      "closed_at": "2025-04-29T18:54:50Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1408/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1408",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1408",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:30.434924",
      "comments": [
        {
          "author": "acsangamnerkar",
          "body": "I found the following issue. #957 But no resolution. Good news is that Graph database performance on Postgres Age is really good but global mode is not working.",
          "created_at": "2025-04-18T18:52:27Z"
        },
        {
          "author": "danielaskdd",
          "body": "Could you please review the Graph view in the WebUI and verify the properties of the nodes and edges displayed?",
          "created_at": "2025-04-19T02:24:09Z"
        },
        {
          "author": "danielaskdd",
          "body": "Could you please provide the model name and parameters of your LLM to ensure it is sufficiently robust for document indexing?",
          "created_at": "2025-04-19T02:32:13Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "I am using gpt-4o for document ingestion. I think I should also share that\r\nI am using two large documents of 88 and 101 chunks I this test.\r\n\r\nI will share the properties of the nodes and relationship tomorrow.\r\n\r\nBTW, same two large documents with networkx as graph database do find the\r\nglobal ent",
          "created_at": "2025-04-19T03:59:44Z"
        },
        {
          "author": "danielaskdd",
          "body": "There may be special characters in the documents that are causing issues with PostgreSQL graph storage.",
          "created_at": "2025-04-19T07:43:33Z"
        }
      ]
    },
    {
      "issue_number": 1494,
      "title": "[Question]: continue RAG insert after failure",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI use the reference example in examples/lightrag_azure_openai_demo.py to insert a large document into the RAG system.\nDue to my LLM limits, after sometime (when a major portion of the document is chunked and vectrorized) the insert operation fails. It would ve nice to continue the insert from where it was left off. Could you point me to any possible mechanism that would accomplish this?\nThanks!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "deepakvenu",
      "author_type": "User",
      "created_at": "2025-04-29T16:37:09Z",
      "updated_at": "2025-04-29T16:38:11Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1494/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1494",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1494",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:30.681903",
      "comments": []
    },
    {
      "issue_number": 850,
      "title": "Query time is too long in this new version",
      "body": "i have used the previous version of the light rag (1.01) and the response time was really fast in Hybrid mode and other modes (like around less than 3 second (using gpt4o) for each query i ask ) , but in the latest version from vs 1.1.4  the query for hybrid . mix mode is really taking longer response time (around 15-25 seconds)(using gpt4o only ) , i don't know what that happen i tried to reduce the topk values but still the response time is really high as compared to previous version. @YanSte can you check this.",
      "state": "open",
      "author": "ankit1063",
      "author_type": "User",
      "created_at": "2025-02-19T05:15:56Z",
      "updated_at": "2025-04-29T13:07:29Z",
      "closed_at": null,
      "labels": [
        "question",
        "performance"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/850/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/850",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/850",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:30.681925",
      "comments": [
        {
          "author": "ITh0rn",
          "body": "Same, particularly in query methods that leverage 'local' (hybrid/mix). I believe the issue is related to Postgres. Got 99% CPU usage and critical response times.",
          "created_at": "2025-02-19T11:09:15Z"
        },
        {
          "author": "ITh0rn",
          "body": "I ran a couple of tests, and it seems that the bottleneck is related to the function.\n\n`async def _find_most_related_edges_from_entities(\n    node_datas: list[dict],\n    query_param: QueryParam,\n    knowledge_graph_inst: BaseGraphStorage,\n):\n    all_related_edges = await asyncio.gather(\n        *[kn",
          "created_at": "2025-02-19T12:02:42Z"
        },
        {
          "author": "YanSte",
          "body": "@ITh0rn Do you think, you can make an update and PR ? To improve this point.\n\nThanks.",
          "created_at": "2025-02-19T12:08:23Z"
        },
        {
          "author": "ankit1063",
          "body": "@YanSte @ITh0rn Basically i am using only neo4j and mongodb , and i am getting too much response time for every single query.",
          "created_at": "2025-02-19T14:20:54Z"
        },
        {
          "author": "YanSte",
          "body": "On my side with Postgres it's fast and I don't have that delay that you might have.",
          "created_at": "2025-02-19T14:32:09Z"
        }
      ]
    },
    {
      "issue_number": 1485,
      "title": "[Bug]:mongo $update op [$setOnInsert] is invalid",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen using MongoDB, I get the error \" $update op [$setOnInsert] is invalid\". After checking, this occurs when using setOnInsert alone in upsert operations. Changing it to set resolves the error. I'm not sure if this is a common issue, but it consistently occurs in my case. Please check if this is a bug, and if so, I hope it can be fixed soon.\n\n![Image](https://github.com/user-attachments/assets/26641ce5-75fd-4ede-9c82-fed0e1d1d838)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "zhangxingbang",
      "author_type": "User",
      "created_at": "2025-04-29T02:16:38Z",
      "updated_at": "2025-04-29T13:03:15Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1485/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1485",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1485",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:30.878390",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "I don't think it's a bug. \n\n@lcwlouis would you be available to address this matter at your earliest convenience?",
          "created_at": "2025-04-29T13:03:14Z"
        }
      ]
    },
    {
      "issue_number": 1486,
      "title": "[Question]:Clarification on Entity and Relationship Merging in Neo4j with Large Document Splitting",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI'm using Neo4j as my graph database and working with large documents that need to be split and processed. I wanted to know:\nWill similar entities and relationships be merged automatically across all the documents that are splitted, or do I need to configure something for that?\nAlso, if for default LightrAG storage, it handle merging of entities across multiple documents automatically right? Could you please explain how this works?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ug200203",
      "author_type": "User",
      "created_at": "2025-04-29T02:37:54Z",
      "updated_at": "2025-04-29T12:31:10Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1486/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1486",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1486",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:31.065339",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Entities with the exact same name will be merge automatically across all documents.",
          "created_at": "2025-04-29T12:31:09Z"
        }
      ]
    },
    {
      "issue_number": 1490,
      "title": "[Question]: nano-vectordb stores nane data.",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI try to run example from repo. \n\nfile path: `examples/lightrag_openai_compatible_demo.py`\n\n```bash\nDetected embedding dimension: 1024\nINFO: Process 18596 Shared-Data created for Single Process\nINFO:nano-vectordb:Load (0, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Load (0, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Load (0, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data\n```\n\nThe LLM and Embedding API server are all right.\n\nSame question: https://github.com/HKUDS/LightRAG/issues/865\n\nBut not solved.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Amireon",
      "author_type": "User",
      "created_at": "2025-04-29T08:07:01Z",
      "updated_at": "2025-04-29T10:07:21Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1490/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1490",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1490",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:31.320058",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Could you please clarify the issue you're experiencing?",
          "created_at": "2025-04-29T10:07:20Z"
        }
      ]
    },
    {
      "issue_number": 1487,
      "title": "[Bug]: web-ui  ReTrieval page console errors:  Uncaught (in promise) TypeError: crypto.randomUUID is not a function",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I use web-ui  ReTrieval page to ask some question，the page console errors like this :\nUncaught (in promise) TypeError: crypto.randomUUID is not a function\n    at feature-retrieval-DaoLh-kj.js:15:1887\n    at yf (index-QYZi8qob.js:24:123964)\n    at index-QYZi8qob.js:24:129037\n    at js (index-QYZi8qob.js:24:16983)\n    at Gc (index-QYZi8qob.js:24:125265)\n    at Wc (index-QYZi8qob.js:25:25844)\n    at ng (index-QYZi8qob.js:25:25666) \n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.3.5\n- Operating System:  server - ubuntu 22.04， desktop: windows 10\n- Python Version: 3.10.11\n- Related Issues: \n",
      "state": "open",
      "author": "lccant",
      "author_type": "User",
      "created_at": "2025-04-29T03:57:43Z",
      "updated_at": "2025-04-29T10:03:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1487/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1487",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1487",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:31.515614",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "This bug is fixed on main branch now.",
          "created_at": "2025-04-29T10:03:20Z"
        }
      ]
    },
    {
      "issue_number": 1491,
      "title": "[Bug]:graph_data.json中entity_type description等错位",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n例如\nkv_store_llm_response_cache.json中显示，大模型输出结果是：\n```\n.....##\\n(\\\"entity\\\"<|>\\\"Additional Sum Insured Benefit for Public Transport Accident (Clause)\\\"<|>\\\"Clauses\\\"<|>\\\"This clause specifies additional benefits for accidents occurring during public transportation, including conditions for payment and integration with downstream systems.\\\")##\\n....\n```\nvdb_entities.json中显示：\n```json\n        {\n            \"__id__\": \"ent-c5e1b0f9c448f3d451d7d88537b675e1\",\n            \"__created_at__\": 1745909420.5779407,\n            \"entity_name\": \"Additional Sum Insured Benefit for Public Transport Accident (Clause)\",\n            \"content\": \"Additional Sum Insured Benefit for Public Transport Accident (Clause)\\nThis clause specifies additional benefits for accidents occurring during public transportation, including conditions for payment and integration with downstream systems.\",\n            \"source_id\": \"chunk-fa11dd2c0312ddabfdb5760026665536\",\n            \"file_path\": \"xxxxxxxxxxxxxxxxx\"\n        }\n```\n最终存储为graph_chunk_entity_relation.graphml，转换为graph_data.json的结果中显示：\n```json\n    {\n      \"id\": \"Additional Sum Insured Benefit for Public Transport Accident (Clause)\",\n      \"entity_type\": \"Additional Sum Insured Benefit for Public Transport Accident (Clause)\",\n      \"description\": \"Clauses\",\n      \"source_id\": \"This clause specifies additional benefits for accidents occurring during public transportation, including conditions for payment and integration with downstream systems.\"\n    }\n```\n\n看上去字段映射存在问题，产生了错位。\n期望的结果应该是：\n```json\n    {\n      \"id\": \"ent-c5e1b0f9c448f3d451d7d88537b675e1\",\n      \"entity_type\": \"Clauses\",\n      \"description\": \"This clause specifies additional benefits for accidents occurring during public transportation, including conditions for payment and integration with downstream systems.\",\n      \"source_id\": \"chunk-fa11dd2c0312ddabfdb5760026665536\"\n    }\n```\n\n看上去中间存储过程和处理过程（extract_entities函数以及上传db的部分）没有出错，但是最终生成的图谱有问题。我没有修改过这些代码，请问是哪里出了问题？\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "rosa-lie",
      "author_type": "User",
      "created_at": "2025-04-29T09:17:45Z",
      "updated_at": "2025-04-29T09:26:21Z",
      "closed_at": "2025-04-29T09:26:19Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1491/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1491",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1491",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:31.689836",
      "comments": [
        {
          "author": "rosa-lie",
          "body": "https://github.com/HKUDS/LightRAG/issues/1457\n解决了这个问题",
          "created_at": "2025-04-29T09:26:19Z"
        }
      ]
    },
    {
      "issue_number": 1482,
      "title": "[Question]:Latest code-deployed WebUI fails to send queries when using the page's search functionality.",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nLatest code-deployed WebUI fails to send queries when using the page's search functionality.\nThe issue occurs when pressing Enter or clicking Send after entering a question—there's no response. This problem doesn't exist in tag v1.3.3 but appears in the latest tag v1.3.4.\nthank you\n\n![Image](https://github.com/user-attachments/assets/ad39d6c7-575e-40e7-8cc4-29b5c16971f7)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhangxingbang",
      "author_type": "User",
      "created_at": "2025-04-28T09:40:09Z",
      "updated_at": "2025-04-29T02:11:46Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1482/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1482",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1482",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:31.859966",
      "comments": [
        {
          "author": "msikes6",
          "body": "Same issue here, just updated to 1.34 and both pressing enter or clicking the send button result in no action. I looked at the logs and didn't see any activity. Have tried different browsers with the same result. API requests still return a result. ",
          "created_at": "2025-04-28T15:28:49Z"
        },
        {
          "author": "danielaskdd",
          "body": "This might be a browser compatibility issue. Please check if it's resolved by pulling the latest main branch version.",
          "created_at": "2025-04-28T18:48:50Z"
        },
        {
          "author": "zhangxingbang",
          "body": "Great, it's fixed now.",
          "created_at": "2025-04-29T02:11:45Z"
        }
      ]
    },
    {
      "issue_number": 1416,
      "title": "[Bug]: TypeError: Can't instantiate abstract class MongoDocStatusStorage with abstract method delete",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n`delete` method is not implemented in `MongoDocStatusStorage`.\n\n### Steps to reproduce\n\nSetting LIGHTRAG_DOC_STATUS_STORAGE=MongoDocStatusStorage results in this error.\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\nLIGHTRAG_DOC_STATUS_STORAGE=MongoDocStatusStorage\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: v1.3.3\n- Operating System: Windows 11\n- Python Version: 3.11.4\n- Related Issues:\n",
      "state": "open",
      "author": "mc-marcocheng",
      "author_type": "User",
      "created_at": "2025-04-20T03:18:57Z",
      "updated_at": "2025-04-29T02:09:40Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mongoDB"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1416/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1416",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1416",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:32.060807",
      "comments": [
        {
          "author": "zhangxingbang",
          "body": "Yes, I also encountered this issue. The latest code still has this problem. I had to implement the delete method myself. Hope it gets fixed soon.\n\n![Image](https://github.com/user-attachments/assets/6891af73-1049-4c48-a83a-0e1d44c75589)",
          "created_at": "2025-04-29T02:09:39Z"
        }
      ]
    },
    {
      "issue_number": 1385,
      "title": "[Question]: Why not concurrently execute merge to speedup inserting?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nCurrently, merging entities and relationships is sequentially:\n```python\n        # Process and update all entities at once\n        for entity_name, entities in all_nodes.items():\n            entity_data = await _merge_nodes_then_upsert(\n                entity_name,\n                entities,\n                knowledge_graph_inst,\n                global_config,\n                pipeline_status,\n                pipeline_status_lock,\n                llm_response_cache,\n            )\n            entities_data.append(entity_data)\n\n        # Process and update all relationships at once\n        for edge_key, edges in all_edges.items():\n            edge_data = await _merge_edges_then_upsert(\n                edge_key[0],\n                edge_key[1],\n                edges,\n                knowledge_graph_inst,\n                global_config,\n                pipeline_status,\n                pipeline_status_lock,\n                llm_response_cache,\n            )\n            relationships_data.append(edge_data)\n```\n\nWhy not change it to be parallel?  Like this:\n```python\n        # Process and update all entities at once\n        futures = []\n        for entity_name, entities in all_nodes.items():\n            futures.append(asyncio.create_task(_merge_nodes_then_upsert(\n                entity_name,\n                entities,\n                knowledge_graph_inst,\n                global_config,\n                pipeline_status,\n                pipeline_status_lock,\n                llm_response_cache,\n            )))\n        entities_data = await asyncio.gather(*futures)\n\n        # Process and update all relationships at once\n        futures = []\n        for edge_key, edges in all_edges.items():\n            futures.append(asyncio.create_task(_merge_edges_then_upsert(\n                edge_key[0],\n                edge_key[1],\n                edges,\n                knowledge_graph_inst,\n                global_config,\n                pipeline_status,\n                pipeline_status_lock,\n                llm_response_cache,\n            )))\n        relationships_data = await asyncio.gather(*futures)\n```\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhouzhou12",
      "author_type": "User",
      "created_at": "2025-04-16T09:19:43Z",
      "updated_at": "2025-04-28T16:26:16Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1385/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1385",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1385",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:32.261181",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Too many concurrent requests can generate excessive database connections, consuming a large amount of resources and leading to a performance decline. The correct approach is to use batch mode for data queries. The main branch has already been optimized following this principle. Currently, the perfor",
          "created_at": "2025-04-16T17:11:45Z"
        },
        {
          "author": "danielaskdd",
          "body": "The bottleneck in node merging occurs during LLM calls, and simply increasing Python's concurrency isn't the right solution. In v1.3.2, we've optimized the process by deferring node merging until after each document's processing phase, followed by a comprehensive merge of all nodes with identical na",
          "created_at": "2025-04-16T17:37:47Z"
        },
        {
          "author": "zhouzhou12",
          "body": "> The bottleneck in node merging occurs during LLM calls, and simply increasing Python's concurrency isn't the right solution. In v1.3.2, we've optimized the process by deferring node merging until after each document's processing phase, followed by a comprehensive merge of all nodes with identical ",
          "created_at": "2025-04-17T01:09:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "To ensure data consistency, only a single thread is permitted to access the graph database during the node merging stage. This limitation accounts for the observed reduction in LLM concurrency. Resolving this issue requires optimizing the pipeline architecture to enable asynchronous operation betwee",
          "created_at": "2025-04-17T06:10:16Z"
        },
        {
          "author": "zhouzhou12",
          "body": "> To ensure data consistency, only a single thread is permitted to access the graph database during the node merging stage. This limitation accounts for the observed reduction in LLM concurrency. Resolving this issue requires optimizing the pipeline architecture to enable asynchronous operation betw",
          "created_at": "2025-04-17T06:31:31Z"
        }
      ]
    },
    {
      "issue_number": 1373,
      "title": "[Bug]: <title>IndexError: index 0 is out of bounds for axis 0 with size 0",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nFailed to process document doc-addb4618e1697da0445ec72a648e1f92: index 0 is out of bounds for axis 0 with size 0\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "hzhguang4721",
      "author_type": "User",
      "created_at": "2025-04-15T02:43:46Z",
      "updated_at": "2025-04-28T04:54:41Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1373/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1373",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1373",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:32.474828",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Your issue appears to be related to the embedding configuration. Please verify whether the relevant settings are correct.",
          "created_at": "2025-04-16T18:45:10Z"
        },
        {
          "author": "yuyuanzi-hub",
          "body": "Hello, I just met the same bug as yours. Have you solvet it yet? If so, please help me.",
          "created_at": "2025-04-28T04:48:37Z"
        },
        {
          "author": "yuyuanzi-hub",
          "body": "To be more specific, when running lightrag_ollama_demo.py, I got this bug:\n\nINFO: Process 15056 Pipeline namespace initialized\n**Failed to process document doc-addb4618e1697da0445ec72a648e1f92: Traceback (most recent call last):**\n  File \"e:\\liesmars\\prof_du\\rag\\lightrag-main\\lightrag\\lightrag.py\", ",
          "created_at": "2025-04-28T04:54:41Z"
        }
      ]
    },
    {
      "issue_number": 1461,
      "title": "[Bug]:LightRAG Server not working anymore with Open Webui, response goes direct to llm",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nLightRAG version v1.3.4/0159\n\nWhen we connect lightrag with open webui the response do not use rag at all, seems there are directly plug with the llm and not using rag, even with slash command /mix etc ...\n\nThis is possible linked to the recent feature to bypass rag for title and tag from open webui  ?\n\nThanks for feed back.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: v1.3.4/0159\n- Operating System: debian\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "neo-inuit",
      "author_type": "User",
      "created_at": "2025-04-25T11:55:25Z",
      "updated_at": "2025-04-28T03:47:10Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1461/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1461",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1461",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:32.658411",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Are you sure your Open WebUI is connection to LightRAG using the ollama emulation feature of LightRAG server?",
          "created_at": "2025-04-25T23:53:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "We have recently introduced a bypass mode in the query parameters for the query API. When enabled, this mode allows all queries to LightRAG to be routed directly to the LLM, bypassing any intermediate processing.",
          "created_at": "2025-04-25T23:57:50Z"
        },
        {
          "author": "neo-inuit",
          "body": "yes i think, i saw my special tag, i used to change it in .env with this\n\n> # OLLAMA_EMULATING_MODEL_TAG=test\n\nCan this change enable the bypass ?\n\nDo you know if an environnement variable exist to enable or not the bypass ?\n\nThanks for your previous quick feedback",
          "created_at": "2025-04-27T19:18:29Z"
        },
        {
          "author": "danielaskdd",
          "body": "The bypass feature is automatically triggered upon detection of a query prefix in requests sent to the Ollama emulation API.",
          "created_at": "2025-04-28T03:47:10Z"
        }
      ]
    },
    {
      "issue_number": 1445,
      "title": "[Feature Request]:借鉴 Hyper-RAG 优化实体与关系抽取的效率",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n目前 LightRAG 在处理文档时，实体和关系的抽取是一个关键环节。为了进一步提升处理效率，特别是针对复杂或长篇文档，建议研究并借鉴 Hyper-RAG 项目 (https://github.com/iMoonLab/Hyper-RAG) 中可能存在的、用于加速实体和关系抽取的有效方法或架构设计。\n具体而言，希望 LightRAG 能够：\n\n1.  评估 Hyper-RAG 在实体/关系抽取方面的性能优势（如果存在）。\n2.  探索是否可以将 Hyper-RAG 中的相关技术（例如，可能的并行处理、优化的索引或超图构建方法）集成或适配到 LightRAG 的流程中。因为该项目声称：\n➊处理性能比 Graph RAG 和 Light RAG 高出 6.3% 和 6.0%；\n➋在九个不同的数据集上，Hyper-RAG 基于选择评估，比 Light RAG 提高了 35.5% 的性能。\n\n4.  最终目标是显著提高 LightRAG 在从文档中提取实体及其相互关系时的速度和效率，减少处理时间，提升用户体验。\n\n### Additional Context\n\nHyper-RAG 项目地址：https://github.com/iMoonLab/Hyper-RAG\nHyper-RAG 项目在其介绍中可能包含关于高效处理文档和构建基于超图知识图谱的技术细节。研究这些细节可能为 LightRAG 的性能优化提供有价值的参考。提升抽取速度对于需要快速处理大量文档或实时性要求较高的 RAG 应用场景尤为重要。\n\n![Image](https://github.com/user-attachments/assets/dbea1b0d-cf75-436c-b3b3-87fbf237e727)\n\n![Image](https://github.com/user-attachments/assets/e578bf2f-78b1-4b66-9425-9813810fbef0)",
      "state": "closed",
      "author": "ruanjunmin",
      "author_type": "User",
      "created_at": "2025-04-24T03:31:58Z",
      "updated_at": "2025-04-27T04:40:19Z",
      "closed_at": "2025-04-27T04:40:19Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1445/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1445",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1445",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:32.843399",
      "comments": [
        {
          "author": "TonicZhang",
          "body": "@ruanjunmin 你自己跑过吗？我也看到了这个论文，但还没自己实测过。超图理论上更复杂，运行速度反而更快，有点怀疑……",
          "created_at": "2025-04-24T06:51:13Z"
        },
        {
          "author": "ruanjunmin",
          "body": "> [@ruanjunmin](https://github.com/ruanjunmin) 你自己跑过吗？我也看到了这个论文，但还没自己实测过。超图理论上更复杂，运行速度反而更快，有点怀疑……\n\n跑过了，圣诞颂歌的示例文本，用免费的智谱API（glm-4-flash-250414）跑用了6分52秒，速度上确实比lightRAG快。\n\n![Image](https://github.com/user-attachments/assets/8250f03b-d329-4cef-9841-ab72fd90d9f1)\n回答也还行\nnaive回答：\n![Image](https://github.co",
          "created_at": "2025-04-24T07:12:20Z"
        },
        {
          "author": "danielaskdd",
          "body": "超图的Super edge可能社区总结的效果，回答全局和抽象问题的能力可能会更强。对于事实的挖掘能力如何有待评估。希望各位多反馈一些比较数据和使用上的体会。",
          "created_at": "2025-04-24T10:05:33Z"
        },
        {
          "author": "ruanjunmin",
          "body": "希望取其精华，去其糟粕",
          "created_at": "2025-04-24T10:16:44Z"
        },
        {
          "author": "TonicZhang",
          "body": "最近也试了试Hyper-RAG，代码基本上是借用的lightRAG，只做了一些简化实现，还是有一些小bug，比如log，抽取的结果过长容易超过最大token数等。\n效果方面，在细节和多关联的回答上，虽然期待比较高，但暂时没有体现出优势。可能超图的作用比较依赖entity type和example的配置，可能要自己重新适配一下，到时候再评估。\n速度方面，index阶段略微快一点，最主要还是它的实现是全部抽取后存储到内存上，再一次性持久化。其中hyperdb速度确实比较快，比networkx性能好。",
          "created_at": "2025-04-25T08:35:01Z"
        }
      ]
    },
    {
      "issue_number": 1472,
      "title": "[Feature Request]: add llm reasoning_content like deepseek-r1  \"<think>\\n\\n</think>\"",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nhow to support \">\" in markdow, such as \"> <think> ok, let me think ..... </think> {llm content}\"\n<img width=\"1494\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a4e3c1a8-6301-4f2a-abfb-e60df2e196c3\" />\n\nwebui chat, like\n\n<img width=\"768\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/733ffdef-45f1-4d22-8536-9b57d7163dd5\" />\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "LYUYork",
      "author_type": "User",
      "created_at": "2025-04-26T21:21:06Z",
      "updated_at": "2025-04-26T21:39:28Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1472/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1472",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1472",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:33.075511",
      "comments": [
        {
          "author": "LYUYork",
          "body": "push the commit, in https://github.com/HKUDS/LightRAG/pull/1473 \n\ntodo: webui display",
          "created_at": "2025-04-26T21:39:27Z"
        }
      ]
    },
    {
      "issue_number": 1471,
      "title": "[Feature Request]:It's very difficult to use Lightrag for production.",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nThe purpose of creating this topic is to discuss the response speed of queries.\nIn terms of response accuracy, it has met expectations.\nHowever, the response speed still seems unsatisfactory despite several previous updates.\nI would like to ask for your opinions and ideas to help improve Lightrag's response speed.\n\n![Image](https://github.com/user-attachments/assets/5ebac95b-e959-4a1c-93e2-b897249d7599)\nBelow is the detailed timeline for executing the sub-functions:\n\n![Image](https://github.com/user-attachments/assets/09915e7f-3e26-4f89-9ffb-cf94ab2b62a7)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-04-26T15:19:10Z",
      "updated_at": "2025-04-26T20:06:40Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1471/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1471",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1471",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:33.292115",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "In order to minimize query latency, we should distribute key-value, vector, and graph storage across different high-performance backend servers.",
          "created_at": "2025-04-26T17:52:31Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "> In order to minimize query latency, we should distribute key-value, vector, and graph storage across different high-performance backend servers.\n\nI feel that NanoVectorDB, along with other types of databases, accounts for nearly 80% of the execution time. It could also be due to the influence of t",
          "created_at": "2025-04-26T18:01:31Z"
        },
        {
          "author": "danielaskdd",
          "body": "Tryp GPU-accelerated version of faiss-gpu, it's likely to get significant performance improvements.",
          "created_at": "2025-04-26T20:06:38Z"
        }
      ]
    },
    {
      "issue_number": 1469,
      "title": "[Bug]: pypi not up-to-date",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n[Latest version on Pypi ](https://pypi.org/project/lightrag-hku/#history) is still `1.3.2` and does not include the latest `1.3.3` release from git.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\nI believe a github action would be good that triggers a wheel building and distribution.\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "drahnreb",
      "author_type": "User",
      "created_at": "2025-04-26T13:54:12Z",
      "updated_at": "2025-04-26T19:05:32Z",
      "closed_at": "2025-04-26T19:04:23Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1469/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1469",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1469",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:33.485011",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thanks for letting me know. PyPI has now been updated.",
          "created_at": "2025-04-26T17:37:27Z"
        },
        {
          "author": "drahnreb",
          "body": "Thanks for the quick fix @danielaskdd @LarFii ",
          "created_at": "2025-04-26T19:05:31Z"
        }
      ]
    },
    {
      "issue_number": 1455,
      "title": "[Bug]:KeyError: 'history_messages' in apipeline_process_enqueue_documents",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen using LightRAG  calling the rag.insert() method can lead to a KeyError: 'history_messages'. This error occurs within the internal apipeline_process_enqueue_documents function in lightrag/lightrag.py.\n\nThe error seems to happen because the code attempts to clear the pipeline_status[\"history_messages\"] list using del pipeline_status[\"history_messages\"][:] without first ensuring that the history_messages key exists in the pipeline_status dictionary and that its value is indeed a list. This can happen even if no new documents are being processed in a particular run, potentially due to internal state management.\n\nSuggested Fix:\n\nThe issue can be resolved by ensuring pipeline_status[\"history_messages\"] exists and is a list before attempting operations on it. Based on local testing, adding checks like the following within the async with pipeline_status_lock: block in apipeline_process_enqueue_documents seems to prevent the error:\n\n\nCheck before del (line ~851):\n\nif \"history_messages\" in pipeline_status and isinstance(pipeline_status[\"history_messages\"], list):\n    del pipeline_status[\"history_messages\"][:]\n\nCheck before append (line ~881 or similar):\n\nif \"history_messages\" not in pipeline_status or not isinstance(pipeline_status[\"history_messages\"], list):\n    pipeline_status[\"history_messages\"] = []\n# Now safe to append\npipeline_status[\"history_messages\"].append(log_message)\n\nBoth approaches aim to prevent accessing or modifying a non-existent or incorrectly typed history_messages key.\n\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "a-issaoui",
      "author_type": "User",
      "created_at": "2025-04-25T03:14:20Z",
      "updated_at": "2025-04-26T00:30:56Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1455/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1455",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1455",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:33.657277",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You probably missing pipeline initialization, pls add the following line before you create LightRAG object:\n\n```\nawait initialize_pipeline_status()\n```",
          "created_at": "2025-04-26T00:30:55Z"
        }
      ]
    },
    {
      "issue_number": 1457,
      "title": "[Bug]:修复utils.py->xml_to_json 导出JSON文件出错的问题",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n源码链接：\nhttps://github.com/HKUDS/LightRAG/blob/36acc332b5e73f871a7574296f7292ab549569ce/lightrag/utils.py#L469\n\n我的修改如下：\n```python\ndef xml_to_json(xml_file):\n    try:\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n\n        # Print the root element's tag and attributes to confirm the file has been correctly loaded\n        print(f\"Root element: {root.tag}\")\n        print(f\"Root attributes: {root.attrib}\")\n\n        data = {\"nodes\": [], \"edges\": []}\n\n        # Use namespace\n        namespace = {\"\": \"http://graphml.graphdrawing.org/xmlns\"}\n\n        for node in root.findall(\".//node\", namespace):\n            node_data = {\n                \"id\": node.get(\"id\").strip('\"'),\n                \"entity_type\": (\n                    node.find(\"./data[@key='d1']\", namespace).text.strip('\"')\n                    if node.find(\"./data[@key='d1']\", namespace) is not None\n                    else \"\"\n                ),\n                \"description\": (\n                    node.find(\"./data[@key='d2']\", namespace).text\n                    if node.find(\"./data[@key='d2']\", namespace) is not None\n                    else \"\"\n                ),\n                \"source_id\": (\n                    node.find(\"./data[@key='d3']\", namespace).text\n                    if node.find(\"./data[@key='d3']\", namespace) is not None\n                    else \"\"\n                ),\n                \"source_file\": (\n                    node.find(\"./data[@key='d4']\", namespace).text\n                    if node.find(\"./data[@key='d4']\", namespace) is not None\n                    else \"\"\n                ),\n            }\n            data[\"nodes\"].append(node_data)\n\n        for edge in root.findall(\".//edge\", namespace):\n            edge_data = {\n                \"source\": edge.get(\"source\").strip('\"'),\n                \"target\": edge.get(\"target\").strip('\"'),\n                \"weight\": (\n                    float(edge.find(\"./data[@key='d5']\", namespace).text)\n                    if edge.find(\"./data[@key='d5']\", namespace) is not None\n                    else 0.0\n                ),\n                \"description\": (\n                    edge.find(\"./data[@key='d6']\", namespace).text\n                    if edge.find(\"./data[@key='d6']\", namespace) is not None\n                    else \"\"\n                ),\n                \"keywords\": (\n                    edge.find(\"./data[@key='d7']\", namespace).text\n                    if edge.find(\"./data[@key='d7']\", namespace) is not None\n                    else \"\"\n                ),\n                \"source_id\": (\n                    edge.find(\"./data[@key='d8']\", namespace).text\n                    if edge.find(\"./data[@key='d8']\", namespace) is not None\n                    else \"\"\n                ),\n                \"source_file\": (\n                    node.find(\"./data[@key='d9']\", namespace).text\n                    if node.find(\"./data[@key='d9']\", namespace) is not None\n                    else \"\"\n                ),\n            }\n            data[\"edges\"].append(edge_data)\n\n        # Print the number of nodes and edges found\n        print(f\"Found {len(data['nodes'])} nodes and {len(data['edges'])} edges\")\n\n        return data\n    except ET.ParseError as e:\n        print(f\"Error parsing XML file: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "lix8886",
      "author_type": "User",
      "created_at": "2025-04-25T06:44:48Z",
      "updated_at": "2025-04-26T00:16:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1457/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1457",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1457",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:33.825238",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Can you submit a PR on this issue?",
          "created_at": "2025-04-26T00:16:42Z"
        }
      ]
    },
    {
      "issue_number": 1450,
      "title": "[Bug]: error in file lightrag_openai_demo.py  An error occurred: 'OPENAI_API_BASE'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nerror in file lightrag_openai_demo.py  An error occurred: 'OPENAI_API_BASE'\n\nhttps://github.com/HKUDS/LightRAG/blob/main/examples/lightrag_openai_demo.py\n\nopen ai key set from code\nOPENAI_API_KEY = 'sk-proj-Zw4_dn\nos.environ[\"OPENAI_API_KEY\"] =  OPENAI_API_KEY\nWORKING_DIR = \"./dickens\"\n\n![Image](https://github.com/user-attachments/assets/dd9c3b5f-37e3-414e-88d3-6be8d288e420)\n\nis it my mistake , would I set it  to config file ?\n\n### Steps to reproduce\n\njust run your demo file \nhttps://github.com/HKUDS/LightRAG/blob/main/examples/lightrag_openai_demo.py\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n### This is sample file of .env\n\n### Server Configuration\n# HOST=0.0.0.0\n# PORT=9621\n# WORKERS=2\n# CORS_ORIGINS=http://localhost:3000,http://localhost:8080\nWEBUI_TITLE='Graph RAG Engine'\nWEBUI_DESCRIPTION=\"Simple and Fast Graph Based RAG System\"\n\n### Optional SSL Configuration\n# SSL=true\n# SSL_CERTFILE=/path/to/cert.pem\n# SSL_KEYFILE=/path/to/key.pem\n\n### Directory Configuration (defaults to current working directory)\n# WORKING_DIR=<absolute_path_for_working_dir>\n# INPUT_DIR=<absolute_path_for_doc_input_dir>\n\n### Ollama Emulating Model Tag\n# OLLAMA_EMULATING_MODEL_TAG=latest\n\n### Max nodes return from grap retrieval\n# MAX_GRAPH_NODES=1000\n\n### Logging level\n# LOG_LEVEL=INFO\n# VERBOSE=False\n# LOG_MAX_BYTES=10485760\n# LOG_BACKUP_COUNT=5\n### Logfile location (defaults to current working directory)\n# LOG_DIR=/path/to/log/directory\n\n### Settings for RAG query\n# HISTORY_TURNS=3\n# COSINE_THRESHOLD=0.2\n# TOP_K=60\n# MAX_TOKEN_TEXT_CHUNK=4000\n# MAX_TOKEN_RELATION_DESC=4000\n# MAX_TOKEN_ENTITY_DESC=4000\n\n### Settings for document indexing\nSUMMARY_LANGUAGE=English\n# CHUNK_SIZE=1200\n# CHUNK_OVERLAP_SIZE=100\n\n### Number of parallel processing documents in one patch\n# MAX_PARALLEL_INSERT=2\n\n### Max tokens for entity/relations description after merge\n# MAX_TOKEN_SUMMARY=500\n### Number of entities/edges to trigger LLM re-summary on merge ( at least 3 is recommented)\n# FORCE_LLM_SUMMARY_ON_MERGE=6\n\n### Num of chunks send to Embedding in single request\n# EMBEDDING_BATCH_NUM=32\n### Max concurrency requests for Embedding\n# EMBEDDING_FUNC_MAX_ASYNC=16\n# MAX_EMBED_TOKENS=8192\n\n### LLM Configuration\n### Time out in seconds for LLM, None for infinite timeout\nTIMEOUT=150\n### Some models like o1-mini require temperature to be set to 1\nTEMPERATURE=0.5\n### Max concurrency requests of LLM\nMAX_ASYNC=4\n### Max tokens send to LLM (less than context size of the model)\nMAX_TOKENS=32768\nENABLE_LLM_CACHE=true\nENABLE_LLM_CACHE_FOR_EXTRACT=true\n\n### Ollama example (For local services installed with docker, you can use host.docker.internal as host)\nLLM_BINDING=ollama\nLLM_MODEL=mistral-nemo:latest\nLLM_BINDING_API_KEY=your_api_key\nLLM_BINDING_HOST=http://localhost:11434\n\n### OpenAI alike example\n# LLM_BINDING=openai\n# LLM_MODEL=gpt-4o\n# LLM_BINDING_HOST=https://api.openai.com/v1\n# LLM_BINDING_API_KEY=your_api_key\n### lollms example\n# LLM_BINDING=lollms\n# LLM_MODEL=mistral-nemo:latest\n# LLM_BINDING_HOST=http://localhost:9600\n# LLM_BINDING_API_KEY=your_api_key\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\n# EMBEDDING_BINDING_API_KEY=your_api_key\n### ollama example\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\n### OpenAI alike example\n# EMBEDDING_BINDING=openai\n# LLM_BINDING_HOST=https://api.openai.com/v1\n### Lollms example\n# EMBEDDING_BINDING=lollms\n# EMBEDDING_BINDING_HOST=http://localhost:9600\n\n### Optional for Azure (LLM_BINDING_HOST, LLM_BINDING_API_KEY take priority)\n# AZURE_OPENAI_API_VERSION=2024-08-01-preview\n# AZURE_OPENAI_DEPLOYMENT=gpt-4o\n# AZURE_OPENAI_API_KEY=your_api_key\n# AZURE_OPENAI_ENDPOINT=https://myendpoint.openai.azure.com\n\n# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large\n# AZURE_EMBEDDING_API_VERSION=2023-05-15\n\n### Data storage selection\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\n### TiDB Configuration (Deprecated)\n# TIDB_HOST=localhost\n# TIDB_PORT=4000\n# TIDB_USER=your_username\n# TIDB_PASSWORD='your_password'\n# TIDB_DATABASE=your_database\n### separating all data from difference Lightrag instances(deprecating)\n# TIDB_WORKSPACE=default\n\n### PostgreSQL Configuration\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_USER=your_username\nPOSTGRES_PASSWORD='your_password'\nPOSTGRES_DATABASE=your_database\n### separating all data from difference Lightrag instances(deprecating)\n# POSTGRES_WORKSPACE=default\n\n### Independent AGM Configuration(not for AMG embedded in PostreSQL)\nAGE_POSTGRES_DB=\nAGE_POSTGRES_USER=\nAGE_POSTGRES_PASSWORD=\nAGE_POSTGRES_HOST=\n# AGE_POSTGRES_PORT=8529\n\n# AGE Graph Name(apply to PostgreSQL and independent AGM)\n### AGE_GRAPH_NAME is precated\n# AGE_GRAPH_NAME=lightrag\n\n### Neo4j Configuration\nNEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD='your_password'\n\n### MongoDB Configuration\nMONGO_URI=mongodb://root:root@localhost:27017/\nMONGO_DATABASE=LightRAG\n### separating all data from difference Lightrag instances(deprecating)\n# MONGODB_GRAPH=false\n\n### Milvus Configuration\nMILVUS_URI=http://localhost:19530\nMILVUS_DB_NAME=lightrag\n# MILVUS_USER=root\n# MILVUS_PASSWORD=your_password\n# MILVUS_TOKEN=your_token\n\n### Qdrant\nQDRANT_URL=http://localhost:16333\n# QDRANT_API_KEY=your-api-key\n\n### Redis\nREDIS_URI=redis://localhost:6379\n\n### For JWT Auth\n# AUTH_ACCOUNTS='admin:admin123,user1:pass456'\n# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server\n# TOKEN_EXPIRE_HOURS=48\n# GUEST_TOKEN_EXPIRE_HOURS=24\n# JWT_ALGORITHM=HS256\n\n### API-Key to access LightRAG Server API\n# LIGHTRAG_API_KEY=your-secure-api-key-here\n# WHITELIST_PATHS=/health,/api/*\n\n\n### Logs and screenshots\n\nINFO: Process 26416 Shared-Data created for Single Process\nINFO: Loaded graph from ./dickens\\graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\nINFO:nano-vectordb:Load (0, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Load (0, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Load (0, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data\nINFO: Process 26416 initialized updated flags for namespace: [full_docs]\nINFO: Process 26416 ready to initialize storage namespace: [full_docs]\nINFO: Process 26416 KV load full_docs with 1 records\nINFO: Process 26416 initialized updated flags for namespace: [text_chunks]\nINFO: Process 26416 ready to initialize storage namespace: [text_chunks]\nINFO: Process 26416 KV load text_chunks with 8 records\nINFO: Process 26416 initialized updated flags for namespace: [entities]\nINFO: Process 26416 initialized updated flags for namespace: [relationships]\nINFO: Process 26416 initialized updated flags for namespace: [chunks]\nINFO: Process 26416 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 26416 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 26416 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 26416 KV load llm_response_cache with 0 records\nINFO: Process 26416 initialized updated flags for namespace: [doc_status]\nINFO: Process 26416 ready to initialize storage namespace: [doc_status]\nINFO: Process 26416 doc status load doc_status with 1 records\nINFO: Process 26416 storage namespace already initialized: [full_docs]\nINFO: Process 26416 storage namespace already initialized: [text_chunks]\nINFO: Process 26416 storage namespace already initialized: [llm_response_cache]\nINFO: Process 26416 storage namespace already initialized: [doc_status]\nINFO: Process 26416 Pipeline namespace initialized\nINFO: No new unique documents were found.\nINFO: Storage Initialization completed!\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processing batch 1 of 1.\nINFO: Processing file: unknown_source\nINFO: Processing d-id: doc-d2b2fa7d56f079833479b68ae38bb4c4\nINFO: Process 26416 doc status writting 1 records to doc_status\nERROR: Failed to extract entities and relationships\nERROR: Failed to process document doc-d2b2fa7d56f079833479b68ae38bb4c4: 'OPENAI_API_BASE'\nINFO: Process 26416 doc status writting 1 records to doc_status\nINFO: Process 26416 KV writting 1 records to full_docs\nINFO: Process 26416 KV writting 8 records to text_chunks\nINFO: Writing graph with 0 nodes, 0 edges\nINFO: In memory DB persist to disk\nINFO: Completed batch 1 of 1.\nINFO: Document processing pipeline completed\n\n=====================\nQuery mode: naive\n=====================\nAn error occurred: 'OPENAI_API_BASE'\n\n### Additional Information\n\n- LightRAG Version: latest \n- Operating System: windows \n- Python Version: 3.10\n- Related Issues:\n",
      "state": "open",
      "author": "Sandy4321",
      "author_type": "User",
      "created_at": "2025-04-24T19:03:49Z",
      "updated_at": "2025-04-25T19:34:32Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1450/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1450",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1450",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.027783",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The issue has been resolved on the main branch. As a temporary workaround, you can set the environment variable as follows:\n\n```shell\nexport OPENAI_API_BASE=\"https://api.openai.com/v1\"\n```",
          "created_at": "2025-04-24T19:37:08Z"
        },
        {
          "author": "Sandy4321",
          "body": "but I suppose to use .env file \nwhere is default  .env file located with I install by pip install LightRAG?\ncan I use custom path to .env ? \nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nprint(os.getenv(\"OPENAI_API_KEY\")) \nsince I tried this and this did not work\nis it know issue \nas you w",
          "created_at": "2025-04-25T19:34:31Z"
        }
      ]
    },
    {
      "issue_number": 1440,
      "title": "[Question]: pip install \"lightrag-hku[api]\" is not installing all needed packages",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nfor installation by pip install \"lightrag-hku[api]\"\nwhy \nall packages from \nhttps://github.com/HKUDS/LightRAG/blob/main/requirements.txt\nare not installed ?\nthen for pip install faiss-cpu\nthere is contradiction with numpy version ? \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Sandy4321",
      "author_type": "User",
      "created_at": "2025-04-23T23:08:56Z",
      "updated_at": "2025-04-25T19:23:01Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1440/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1440",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1440",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.229048",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You should remove numpy first, and install faiss-cpu again.",
          "created_at": "2025-04-24T10:09:05Z"
        },
        {
          "author": "Sandy4321",
          "body": "for installation by pip install \"lightrag-hku[api]\"\nwhy\nall packages from\nhttps://github.com/HKUDS/LightRAG/blob/main/requirements.txt\nare not installed ?\n******\nplease clarify. if  https://github.com/HKUDS/LightRAG/blob/main/requirements.txt needs to be used , seems to be no need to use requirement",
          "created_at": "2025-04-24T14:41:45Z"
        },
        {
          "author": "danielaskdd",
          "body": "Thank you for pointing out this issue. Currently, when only the API server is installed, the requirements.txt file in the root directory is indeed not installed. We will address this problem in the next version release. As a temporary solution, please install Core first and then install the API serv",
          "created_at": "2025-04-24T15:38:33Z"
        },
        {
          "author": "Sandy4321",
          "body": "can you clarify\n1 \n install Core first \nwhat is core  ?\n 2\n install the API server\nhow to do it ? \nis it pip install \"lightrag-hku[api]\"?\n3\n manually install the requirements.txt separately\nbut then it will have contradiction with numpy needed for faiss?\n",
          "created_at": "2025-04-25T19:23:00Z"
        }
      ]
    },
    {
      "issue_number": 1439,
      "title": "[Bug]:Docker-compose on Windows. Failed to connect to Ollama.",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n127.0.0.1:11434 - Ollama is running\nFailed pipeline in Web Uploaded Documents when load MD file.\nError: lightrag-1 | ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n\n### Steps to reproduce\n\nClone repo.\nConfigure .evn and config.ini files.\ndocker-compose up --build\n[Start local in web browser](http://127.0.0.1:9621/webui/)\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\nconfig.ini:\n```\n[neo4j]\nuri = http://localhost:7687\nusername = neo4j\npassword = tester12\n\n[redis]\nuri=redis://localhost:6379/1\n\n[qdrant]\nuri = http://localhost:16333\n```\n\n.env:\n```\nWEBUI_TITLE='Graph RAG Engine'\nWEBUI_DESCRIPTION=\"Simple and Fast Graph Based RAG System\"\n\nLLM_BINDING=ollama\nLLM_MODEL=mistral-nemo:latest\nLLM_BINDING_HOST=http://localhost:11434\n# LLM_BINDING_API_KEY=your_api_key\n### Max tokens sent to LLM (based on your Ollama Server capacity)\nMAX_TOKENS=8192\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\n# EMBEDDING_BINDING_API_KEY=your_api_key\n### ollama example\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\n### OpenAI alike example\n# EMBEDDING_BINDING=openai\n# LLM_BINDING_HOST=https://api.openai.com/v1\n### Lollms example\n# EMBEDDING_BINDING=lollms\n# EMBEDDING_BINDING_HOST=http://localhost:9600\n\n### Neo4j Configuration\nNEO4J_URI=http://localhost\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD='tester12'\n\n### Data storage selection\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\n### Qdrant\n#QDRANT_URL=http://localhost:16333\n# QDRANT_API_KEY=your-api-key\n\n### Redis\nREDIS_URI=redis://localhost:6379\n```\n\n### Logs and screenshots\n\ndocker-compose up --build\n[+] Building 5.7s (17/17) FINISHED                                                                 docker:desktop-linux\n => [lightrag internal] load build definition from Dockerfile                                                      0.0s\n => => transferring dockerfile: 1.33kB                                                                             0.0s\n => [lightrag internal] load metadata for docker.io/library/python:3.11-slim                                       1.0s\n => [lightrag internal] load .dockerignore                                                                         0.0s\n => => transferring context: 781B                                                                                  0.0s\n => [lightrag builder 1/7] FROM docker.io/library/python:3.11-slim@sha256:82c07f2f6e35255b92eb16f38dbd22679d5e8fb  0.0s\n => [lightrag internal] load build context                                                                         0.1s\n => => transferring context: 2.79MB                                                                                0.1s\n => CACHED [lightrag builder 2/7] WORKDIR /app                                                                     0.0s\n => CACHED [lightrag builder 3/7] RUN apt-get update && apt-get install -y     curl     build-essential     pkg-c  0.0s\n => CACHED [lightrag builder 4/7] COPY requirements.txt .                                                          0.0s\n => CACHED [lightrag builder 5/7] COPY lightrag/api/requirements.txt ./lightrag/api/                               0.0s\n => CACHED [lightrag builder 6/7] RUN pip install --user --no-cache-dir -r requirements.txt                        0.0s\n => CACHED [lightrag builder 7/7] RUN pip install --user --no-cache-dir -r lightrag/api/requirements.txt           0.0s\n => CACHED [lightrag stage-1 3/7] COPY --from=builder /root/.local /root/.local                                    0.0s\n => [lightrag stage-1 4/7] COPY ./lightrag ./lightrag                                                              0.1s\n => [lightrag stage-1 5/7] COPY setup.py .                                                                         0.1s\n => [lightrag stage-1 6/7] RUN pip install .                                                                       3.5s\n => [lightrag stage-1 7/7] RUN mkdir -p /app/data/rag_storage /app/data/inputs                                     0.5s\n => [lightrag] exporting to image                                                                                  0.2s\n => => exporting layers                                                                                            0.1s\n => => writing image sha256:783b4ef067cdd9a63748c4042d040363cf0c46e8530991f4813b180472fbcc24                       0.0s\n => => naming to docker.io/library/lightrag-lightrag                                                               0.0s\n[+] Running 0/0\n - Network lightrag_default  Creating                                                                              0.0s\ntime=\"2025-04-23T14:59:36+03:00\" level=warning msg=\"Found orphan containers ([ollama-webui ollama]) for this project. If[+] Running 3/3 renamed this service in your compose file, you can run this command with the --remove-orphans flag to cl\n ✔ Network lightrag_default                   Created                                                              0.1s\n ✔ Container lightrag-lightrag-1              Created                                                              0.1s\n ✔ Container lightrag-server_neo4j-community  Created                                                              0.1s\nAttaching to lightrag-1, lightrag-server_neo4j-community\nlightrag-1                       |\nlightrag-1                       | LightRAG log file: /app/lightrag.log\nlightrag-1                       |\nlightrag-1                       |\nlightrag-1                       |     ╔══════════════════════════════════════════════════════════════╗\nlightrag-1                       |     ║                  🚀 LightRAG Server v1.3.4/0159              ║\nlightrag-1                       |     ║          Fast, Lightweight RAG Server Implementation         ║\nlightrag-1                       |     ╚══════════════════════════════════════════════════════════════╝\nlightrag-1                       |\nlightrag-1                       |\nlightrag-1                       | 📡 Server Configuration:\nlightrag-1                       |     ├─ Host: 0.0.0.0\nlightrag-1                       |     ├─ Port: 9621\nlightrag-1                       |     ├─ Workers: 1\nlightrag-1                       |     ├─ CORS Origins: *\nlightrag-1                       |     ├─ SSL Enabled: False\nlightrag-1                       |     ├─ Ollama Emulating Model: lightrag:latest\nlightrag-1                       |     ├─ Log Level: INFO\nlightrag-1                       |     ├─ Verbose Debug: False\nlightrag-1                       |     ├─ History Turns: 3\nlightrag-1                       |     ├─ API Key: Not Set\nlightrag-1                       |     └─ JWT Auth: Disabled\nlightrag-1                       |\nlightrag-1                       | 📂 Directory Configuration:\nlightrag-1                       |     ├─ Working Directory: /app/data/rag_storage\nlightrag-1                       |     └─ Input Directory: /app/data/inputs\nlightrag-1                       |\nlightrag-1                       | 🤖 LLM Configuration:\nlightrag-1                       |     ├─ Binding: ollama\nlightrag-1                       |     ├─ Host: http://localhost:11434\nlightrag-1                       |     ├─ Model: mistral-nemo:latest\nlightrag-1                       |     ├─ Temperature: 0.5\nlightrag-1                       |     ├─ Max Async for LLM: 4\nlightrag-1                       |     ├─ Max Tokens: 8192\nlightrag-1                       |     ├─ Timeout: None (infinite)\nlightrag-1                       |     ├─ LLM Cache Enabled: True\nlightrag-1                       |     └─ LLM Cache for Extraction Enabled: True\nlightrag-1                       |\nlightrag-1                       | 📊 Embedding Configuration:\nlightrag-1                       |     ├─ Binding: ollama\nlightrag-1                       |     ├─ Host: http://localhost:11434\nlightrag-1                       |     ├─ Model: bge-m3:latest\nlightrag-1                       |     └─ Dimensions: 1024\nlightrag-1                       |\nlightrag-1                       | ⚙️ RAG Configuration:\nlightrag-1                       |     ├─ Summary Language: en\nlightrag-1                       |     ├─ Max Parallel Insert: 2\nlightrag-1                       |     ├─ Max Embed Tokens: 8192\nlightrag-1                       |     ├─ Chunk Size: 1200\nlightrag-1                       |     ├─ Chunk Overlap Size: 100\nlightrag-1                       |     ├─ Cosine Threshold: 0.2\nlightrag-1                       |     ├─ Top-K: 60\nlightrag-1                       |     ├─ Max Token Summary: 500\nlightrag-1                       |     └─ Force LLM Summary on Merge: 6\nlightrag-1                       |\nlightrag-1                       | 💾 Storage Configuration:\nlightrag-1                       |     ├─ KV Storage: JsonKVStorage\nlightrag-1                       |     ├─ Vector Storage: NanoVectorDBStorage\nlightrag-1                       |     ├─ Graph Storage: NetworkXStorage\nlightrag-1                       |     └─ Document Status Storage: JsonDocStatusStorage\nlightrag-1                       |\nlightrag-1                       | ✨ Server starting up...\nlightrag-1                       |\nlightrag-1                       |\nlightrag-1                       | 🌐 Server Access Information:\nlightrag-1                       |     ├─ WebUI (local): http://localhost:9621\nlightrag-1                       |     ├─ Remote Access: http://<your-ip-address>:9621\nlightrag-1                       |     ├─ API Documentation (local): http://localhost:9621/docs\nlightrag-1                       |     └─ Alternative Documentation (local): http://localhost:9621/redoc\nlightrag-1                       |\nlightrag-1                       | 📝 Note:\nlightrag-1                       |     Since the server is running on 0.0.0.0:\nlightrag-1                       |     - Use 'localhost' or '127.0.0.1' for local access\nlightrag-1                       |     - Use your machine's IP address for remote access\nlightrag-1                       |     - To find your IP address:\nlightrag-1                       |       • Windows: Run 'ipconfig' in terminal\nlightrag-1                       |       • Linux/Mac: Run 'ifconfig' or 'ip addr' in terminal\nlightrag-1                       |\nlightrag-1                       | Collecting ollama\nlightrag-1                       |   Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\nlightrag-1                       | Requirement already satisfied: httpx<0.29,>=0.27 in /root/.local/lib/python3.11/site-packages (from ollama) (0.28.1)\nlightrag-1                       | Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /root/.local/lib/python3.11/site-packages (from ollama) (2.11.3)\nlightrag-1                       | Requirement already satisfied: anyio in /root/.local/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\nlightrag-1                       | Requirement already satisfied: certifi in /root/.local/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\nlightrag-1                       | Requirement already satisfied: httpcore==1.* in /root/.local/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.8)\nlightrag-1                       | Requirement already satisfied: idna in /root/.local/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\nlightrag-1                       | Requirement already satisfied: h11<0.15,>=0.13 in /root/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\nlightrag-1                       | Requirement already satisfied: annotated-types>=0.6.0 in /root/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\nlightrag-1                       | Requirement already satisfied: pydantic-core==2.33.1 in /root/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.1)\nlightrag-1                       | Requirement already satisfied: typing-extensions>=4.12.2 in /root/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\nlightrag-1                       | Requirement already satisfied: typing-inspection>=0.4.0 in /root/.local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\nlightrag-1                       | Requirement already satisfied: sniffio>=1.1 in /root/.local/lib/python3.11/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\nlightrag-1                       | Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\nlightrag-1                       | Installing collected packages: ollama\nlightrag-1                       | Successfully installed ollama-0.4.8\nlightrag-1                       | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nlightrag-1                       |\nlightrag-1                       | [notice] A new release of pip is available: 24.0 -> 25.0.1\nlightrag-1                       | [notice] To update, run: pip install --upgrade pip\nlightrag-1                       | INFO: Process 1 Shared-Data created for Single Process\nlightrag-server_neo4j-community  | Changed password for user 'neo4j'. IMPORTANT: this change will only take effect if performed before the database is started for the first time.\nlightrag-server_neo4j-community  | 2025-04-23 11:59:42.307+0000 INFO  Logging config in use: File '/var/lib/neo4j/conf/user-logs.xml'\nlightrag-server_neo4j-community  | 2025-04-23 11:59:42.328+0000 INFO  Starting...\nlightrag-server_neo4j-community  | 2025-04-23 11:59:43.017+0000 INFO  This instance is ServerId{24ee5788} (24ee5788-b6b8-422a-9a29-630f2567b5eb)\nlightrag-server_neo4j-community  | 2025-04-23 11:59:43.923+0000 INFO  ======== Neo4j 5.26.4 ========\nlightrag-1                       | INFO: Loaded graph from /app/data/rag_storage/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\nlightrag-1                       | Collecting nano-vectordb\nlightrag-1                       |   Downloading nano_vectordb-0.0.4.3-py3-none-any.whl.metadata (3.7 kB)\nlightrag-1                       | Requirement already satisfied: numpy in /root/.local/lib/python3.11/site-packages (from nano-vectordb) (1.26.4)\nlightrag-1                       | Downloading nano_vectordb-0.0.4.3-py3-none-any.whl (5.6 kB)\nlightrag-server_neo4j-community  | 2025-04-23 11:59:45.332+0000 INFO  Anonymous Usage Data is being sent to Neo4j, see https://neo4j.com/docs/usage-data/\nlightrag-server_neo4j-community  | 2025-04-23 11:59:45.362+0000 INFO  Bolt enabled on 0.0.0.0:7687.\nlightrag-1                       | Installing collected packages: nano-vectordb\nlightrag-1                       | Successfully installed nano-vectordb-0.0.4.3\nlightrag-1                       | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nlightrag-1                       |\nlightrag-1                       | [notice] A new release of pip is available: 24.0 -> 25.0.1\nlightrag-1                       | [notice] To update, run: pip install --upgrade pip\nlightrag-1                       | INFO:nano-vectordb:Load (0, 1024) data\nlightrag-1                       | INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/app/data/rag_storage/vdb_entities.json'} 0 data\nlightrag-1                       | INFO:nano-vectordb:Load (0, 1024) data\nlightrag-1                       | INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/app/data/rag_storage/vdb_relationships.json'} 0 data\nlightrag-1                       | INFO:nano-vectordb:Load (0, 1024) data\nlightrag-1                       | INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/app/data/rag_storage/vdb_chunks.json'} 0 data\nlightrag-1                       | INFO: Started server process [1]\nlightrag-1                       | INFO: Waiting for application startup.\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [full_docs]\nlightrag-1                       | INFO: Process 1 ready to initialize storage namespace: [full_docs]\nlightrag-1                       | INFO: Process 1 KV load full_docs with 1 records\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [text_chunks]\nlightrag-1                       | INFO: Process 1 ready to initialize storage namespace: [text_chunks]\nlightrag-1                       | INFO: Process 1 KV load text_chunks with 1 records\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [entities]\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [relationships]\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [chunks]\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [chunk_entity_relation]\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [llm_response_cache]\nlightrag-1                       | INFO: Process 1 ready to initialize storage namespace: [llm_response_cache]\nlightrag-1                       | INFO: Process 1 KV load llm_response_cache with 0 records\nlightrag-1                       | INFO: Process 1 initialized updated flags for namespace: [doc_status]\nlightrag-1                       | INFO: Process 1 ready to initialize storage namespace: [doc_status]\nlightrag-1                       | INFO: Process 1 doc status load doc_status with 1 records\nlightrag-1                       | INFO: Process 1 Pipeline namespace initialized\nlightrag-1                       | INFO: Application startup complete.\nlightrag-1                       | INFO: Uvicorn running on http://0.0.0.0:9621 (Press CTRL+C to quit)\nlightrag-server_neo4j-community  | 2025-04-23 11:59:46.166+0000 INFO  HTTP enabled on 0.0.0.0:7474.\nlightrag-server_neo4j-community  | 2025-04-23 11:59:46.166+0000 INFO  Remote interface available at http://localhost:7474/\nlightrag-server_neo4j-community  | 2025-04-23 11:59:46.169+0000 INFO  id: 31D16C3BFB899E8D601BBD40AF974BB13990D45D00128EA0D02704143E92C7BD\nlightrag-server_neo4j-community  | 2025-04-23 11:59:46.169+0000 INFO  name: system\nlightrag-server_neo4j-community  | 2025-04-23 11:59:46.169+0000 INFO  creationDate: 2025-04-22T09:37:14.513Z\nlightrag-server_neo4j-community  | 2025-04-23 11:59:46.169+0000 INFO  Started.\nlightrag-1                       | INFO: 172.19.0.1:39906 - \"GET /webui/assets/index-D7JWlILY.js HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:39908 - \"GET /webui/assets/feature-graph-8qBvwmR-.js HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:39942 - \"GET /webui/assets/index-DsHQCgEh.css HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:39926 - \"GET /webui/assets/mermaid-vendor-98Zb_Fmj.js HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:39946 - \"GET /webui/assets/feature-retrieval-CURaqaYO.js HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:39956 - \"GET /webui/assets/feature-documents-CEVUlHQd.js HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:39926 - \"GET /docs HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:39926 - \"GET /openapi.json HTTP/1.1\" 200\nlightrag-1                       | INFO: Process 1 KV writting 0 records to text_chunks\nlightrag-1                       | INFO: Process 1 drop text_chunks\nlightrag-1                       | INFO: Process 1 KV writting 0 records to full_docs\nlightrag-1                       | INFO: Process 1 drop full_docs\nlightrag-1                       | INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/app/data/rag_storage/vdb_entities.json'} 0 data\nlightrag-1                       | INFO: Process 1 drop entities(file:/app/data/rag_storage/vdb_entities.json)\nlightrag-1                       | INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/app/data/rag_storage/vdb_relationships.json'} 0 data\nlightrag-1                       | INFO: Process 1 drop relationships(file:/app/data/rag_storage/vdb_relationships.json)\nlightrag-1                       | INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/app/data/rag_storage/vdb_chunks.json'} 0 data\nlightrag-1                       | INFO: Process 1 drop chunks(file:/app/data/rag_storage/vdb_chunks.json)\nlightrag-1                       | INFO: Process 1 drop graph chunk_entity_relation (file:/app/data/rag_storage/graph_chunk_entity_relation.graphml)\nlightrag-1                       | INFO: Process 1 doc status writting 0 records to doc_status\nlightrag-1                       | INFO: Process 1 drop doc_status\nlightrag-1                       | INFO: Successfully dropped JsonKVStorage\nlightrag-1                       | INFO: Successfully dropped JsonKVStorage\nlightrag-1                       | INFO: Successfully dropped NanoVectorDBStorage\nlightrag-1                       | INFO: Successfully dropped NanoVectorDBStorage\nlightrag-1                       | INFO: Successfully dropped NanoVectorDBStorage\nlightrag-1                       | INFO: Successfully dropped NetworkXStorage\nlightrag-1                       | INFO: Successfully dropped JsonDocStatusStorage\nlightrag-1                       | INFO: 172.19.0.1:39926 - \"DELETE /documents HTTP/1.1\" 200\nlightrag-1                       | INFO: Cleared all cache\nlightrag-1                       | INFO: 172.19.0.1:39926 - \"POST /documents/clear_cache HTTP/1.1\" 200\nlightrag-1                       | INFO: 172.19.0.1:41888 - \"POST /documents/upload HTTP/1.1\" 200\nlightrag-1                       | INFO: Process 1 doc status writting 1 records to doc_status\nlightrag-1                       | INFO: Stored 1 new unique documents\nlightrag-1                       | INFO: Successfully fetched and enqueued file: Памятка по жирности мяса.pdf.md\nlightrag-1                       | INFO: Processing 1 document(s) in 1 batches\nlightrag-1                       | INFO: Start processing batch 1 of 1.\nlightrag-1                       | INFO: Processing file: Памятка по жирности мяса.pdf.md\nlightrag-1                       | INFO: Processing d-id: doc-0818c182309b7a520b4b9a9ccfccd713\nlightrag-1                       | INFO: Process 1 doc status writting 1 records to doc_status\nlightrag-1                       | ERROR: Failed to process document doc-0818c182309b7a520b4b9a9ccfccd713: Traceback (most recent call last):\nlightrag-1                       |   File \"/app/lightrag/lightrag.py\", line 975, in process_document\nlightrag-1                       |     await asyncio.gather(*tasks)\nlightrag-1                       |   File \"/app/lightrag/kg/nano_vector_db_impl.py\", line 109, in upsert\nlightrag-1                       |     embeddings_list = await asyncio.gather(*embedding_tasks)\nlightrag-1                       |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag-1                       |   File \"/app/lightrag/utils.py\", line 279, in wait_func\nlightrag-1                       |     result = await func(*args, **kwargs)\nlightrag-1                       |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag-1                       |   File \"/app/lightrag/utils.py\", line 202, in __call__\nlightrag-1                       |     return await self.func(*args, **kwargs)\nlightrag-1                       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag-1                       |   File \"/app/lightrag/llm/ollama.py\", line 131, in ollama_embed\nlightrag-1                       |     data = ollama_client.embed(model=embed_model, input=texts)\nlightrag-1                       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag-1                       |   File \"/usr/local/lib/python3.11/site-packages/ollama/_client.py\", line 357, in embed\nlightrag-1                       |     return self._request(\nlightrag-1                       |            ^^^^^^^^^^^^^^\nlightrag-1                       |   File \"/usr/local/lib/python3.11/site-packages/ollama/_client.py\", line 178, in _request\nlightrag-1                       |     return cls(**self._request_raw(*args, **kwargs).json())\nlightrag-1                       |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag-1                       |   File \"/usr/local/lib/python3.11/site-packages/ollama/_client.py\", line 124, in _request_raw\nlightrag-1                       |     raise ConnectionError(CONNECTION_ERROR_MESSAGE) from None\nlightrag-1                       | ConnectionError: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\nlightrag-1                       |\nlightrag-1                       | INFO: Process 1 doc status writting 1 records to doc_status\nlightrag-1                       | INFO: Process 1 KV writting 1 records to full_docs\nlightrag-1                       | INFO: Process 1 KV writting 1 records to text_chunks\nlightrag-1                       | INFO: Writing graph with 0 nodes, 0 edges\nlightrag-1                       | INFO: In memory DB persist to disk\nlightrag-1                       | INFO: Completed batch 1 of 1.\nlightrag-1                       | INFO: Document processing pipeline completed\n\n### Additional Information\n\n- LightRAG Version: last clone from github\n- Operating System: Windows 10\n- Python Version: as in docker\n- Related Issues:\n",
      "state": "open",
      "author": "Taron133",
      "author_type": "User",
      "created_at": "2025-04-23T12:12:43Z",
      "updated_at": "2025-04-25T09:03:13Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1439/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1439",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1439",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.406652",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "I guess your Ollama is on PC. You should change your embedding host address from 127.0.0.1 to your PC's ip address.\n\n```\nEMBEDDING_BINDING_HOST=http://your.pc.ip.address:11434\n```\n\nor, you can add this line to your docker-compose.yml\n\n```\nservices:\n  lightrag:\n    build: .\n    ... ...\n    extra_host",
          "created_at": "2025-04-23T15:51:50Z"
        },
        {
          "author": "Taron133",
          "body": "local compose:\n```\nservices:\n  lightrag:\n    build: .\n    ports:\n      - \"${PORT:-9621}:9621\"\n    volumes:\n      - ./data/rag_storage:/app/data/rag_storage\n      - ./data/inputs:/app/data/inputs\n      - ./config.ini:/app/config.ini\n      - ./.env:/app/.env\n    env_file:\n      - .env\n    restart: unl",
          "created_at": "2025-04-25T08:55:08Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please replace \"localhost\" with \"host.docker.internal\" in the .env file.",
          "created_at": "2025-04-25T09:03:09Z"
        }
      ]
    },
    {
      "issue_number": 1456,
      "title": "[Bug]: After starting Docker Compose, the container rag_storage folder is empty.",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nBefore starting the Docker container, I had been running the project using lightrag-server for testing. When I switched to using Docker, the rag_storage foldre data wasn't copied into the container. I found that this was because the Docker Compose volume path included an extra ./data directory \n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Lw-CodeStorage",
      "author_type": "User",
      "created_at": "2025-04-25T06:02:26Z",
      "updated_at": "2025-04-25T06:02:26Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1456/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1456",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1456",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.587321",
      "comments": []
    },
    {
      "issue_number": 1454,
      "title": "[Feature Request]:aws的boto3 可以支持stream吗",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\naws的boto3 可以支持stream吗\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "valueLzy",
      "author_type": "User",
      "created_at": "2025-04-25T01:57:43Z",
      "updated_at": "2025-04-25T01:57:43Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1454/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1454",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1454",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.587340",
      "comments": []
    },
    {
      "issue_number": 1348,
      "title": "[Bug]: LightRAG does not work with gpt-4o-mini",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I run the code from azure open ai gpt-4o-mini it either gives me an error sayng that binding failed (LighRAG API) or never comes out of some loop while its indexing a file (azure_open_ai_demo code).\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "JoedNgangmeni",
      "author_type": "User",
      "created_at": "2025-04-11T04:00:42Z",
      "updated_at": "2025-04-25T01:49:36Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 21,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1348/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1348",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1348",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.587347",
      "comments": []
    },
    {
      "issue_number": 814,
      "title": "Namespace prefix on postgresql impl",
      "body": "The parameter namespace prefix did not affect the table creation process (DDL), only apply to the graph name in the PG internal AGE schema, it would be useful to apply to the entire table name crearion process in order to support multi KB instances on a single \"tenant\"",
      "state": "open",
      "author": "puppetm4st3r",
      "author_type": "User",
      "created_at": "2025-02-17T14:42:15Z",
      "updated_at": "2025-04-24T20:46:07Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/814/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/814",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/814",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.587353",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi, Thanks for sharing.\n\nCould you provide me with some of what you need?\n\nI'll try to take a look.",
          "created_at": "2025-02-17T20:11:34Z"
        },
        {
          "author": "puppetm4st3r",
          "body": "thanks, In certain production scenarios it would be super useful to have the possibility of mounting several knowledge graphs in the same pg database, in the implementation of postgres the names of the tables are created in a hard code way and do not take the namespace_suffix parameter which I assum",
          "created_at": "2025-02-18T16:37:10Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "I ran into the same issue. Have you looked into the 'workspace' variable of the postgres implementation? I believe it works the same way namespace prefixes work in other implementations.",
          "created_at": "2025-03-10T21:18:07Z"
        },
        {
          "author": "puppetm4st3r",
          "body": "didnt work for me",
          "created_at": "2025-03-10T22:44:14Z"
        },
        {
          "author": "ZhuLinsen",
          "body": "same issue",
          "created_at": "2025-03-12T09:07:10Z"
        }
      ]
    },
    {
      "issue_number": 1448,
      "title": "[Bug]: faiss-cpu installation contradicts with  gensim 4.3.3 from requirements.txt",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nfaiss-cpu installation contradicts with  gensim 4.3.3 from requirements.txt\n\nI installed all packages from requirements.txt \nthen try to install   faiss-cpu,\n since faiss-cpu is not in  requirements.txt\n\nthen get this error\n(py310lightragapr23) C:\\my_py_environments\\py310lightragapr23>pip install faiss-cpu\nCollecting faiss-cpu\n  Using cached faiss_cpu-1.10.0-cp310-cp310-win_amd64.whl (13.7 MB)\nCollecting numpy<3.0,>=1.25.0\n  Using cached numpy-2.2.5-cp310-cp310-win_amd64.whl (12.9 MB)\nRequirement already satisfied: packaging in c:\\my_py_environments\\py310lightragapr23\\lib\\site-packages (from faiss-cpu) (25.0)\nInstalling collected packages: numpy, faiss-cpu\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\nSuccessfully installed faiss-cpu-1.10.0 numpy-2.2.5\n\n[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n### Steps to reproduce\n\nuse windows \n1\ninstall using pip lightrag\n2\ninstall packages from requirements.txt\n3\nremove numpy \n4\nrun pip install faiss-cpu\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/6ff47e6b-7e94-4fb3-956d-e0a2d459208f)\n\n### Additional Information\n\n- LightRAG Version: latest \n- Operating System: Windows 10\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "open",
      "author": "Sandy4321",
      "author_type": "User",
      "created_at": "2025-04-24T14:51:06Z",
      "updated_at": "2025-04-24T15:52:32Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1448/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1448",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1448",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.801055",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Your can remove `gensim`, it's not needed any more.",
          "created_at": "2025-04-24T15:52:32Z"
        }
      ]
    },
    {
      "issue_number": 1449,
      "title": "[Question]: graph as chunks  similarities or Subject-Predicate-Object",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nmay you share \nwhat technique you used for graph creation : nodes and chunks  similarities or Subject-Predicate-Object\n1\nnodes are chunks and edges are similarities between chunks like \nhttps://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graph_rag.ipynb\n\nor\n2\nSubject-Predicate-Object (SPO) Triples\nnode Subject.\n then edge Predicate\nnext now is Object\nlike\nhttps://levelup.gitconnected.com/converting-unstructured-data-into-a-knowledge-graph-using-an-end-to-end-pipeline-552a508045f9\n\n\n\n\n \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Sandy4321",
      "author_type": "User",
      "created_at": "2025-04-24T15:00:12Z",
      "updated_at": "2025-04-24T15:00:12Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1449/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1449",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1449",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.989628",
      "comments": []
    },
    {
      "issue_number": 1442,
      "title": "[Question]:how to control to to create edges  and chunking process like chunk size and overlapping ?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nhow to control to to create edges  and chunking process like chunk size and overlapping ? \nand how to control how to create edges ?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Sandy4321",
      "author_type": "User",
      "created_at": "2025-04-23T23:35:08Z",
      "updated_at": "2025-04-24T14:24:59Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1442/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1442",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1442",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:34.989654",
      "comments": [
        {
          "author": "choizhang",
          "body": "<img width=\"364\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/efbbed1f-1e9e-4d2a-b243-1f305c04b6bc\" />",
          "created_at": "2025-04-24T01:06:15Z"
        },
        {
          "author": "Sandy4321",
          "body": "great , thanks for soon answer. but what is about advanced chunking? or adding custom chunking\nis there some file with enabling to add  custom chunking python code?",
          "created_at": "2025-04-24T14:24:58Z"
        }
      ]
    },
    {
      "issue_number": 1444,
      "title": "[Feature Request]:create vlabel \"base\" and elabel \"DIRECTED\" before create index on PG AGE.",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nCurrently, creating index on PG AGE report errors due to the tables \"base\" and \"DIRECTED\" not exist. So we should create vlabel \"base\" and elabel \"DIRECTED\" before create index on PG AGE.\nAdditionaly, we should not create indexes on \"_ag_label_vertex\" (id) and \"_ag_label_edge\" (id)', as they are automatically created when cteating the graph. \n\nSo:\n\nTo add:\n```python\n            f\"SELECT create_vlabel('{self.graph_name}', 'base');\",\n            f\"SELECT create_elabel('{self.graph_name}', 'DIRECTED');\",\n```\n\nTo delete:\n```python\n            # f'CREATE INDEX CONCURRENTLY vertex_p_idx ON {self.graph_name}.\"_ag_label_vertex\" (id)',\n            # f'CREATE INDEX CONCURRENTLY edge_p_idx ON {self.graph_name}.\"_ag_label_edge\" (id)',\n````\n\nTotal:\n```python\n        queries = [\n            f\"SELECT create_graph('{self.graph_name}')\",\n            f\"SELECT create_vlabel('{self.graph_name}', 'base');\",\n            f\"SELECT create_elabel('{self.graph_name}', 'DIRECTED');\",\n            # f'CREATE INDEX CONCURRENTLY vertex_p_idx ON {self.graph_name}.\"_ag_label_vertex\" (id)',\n            f'CREATE INDEX CONCURRENTLY vertex_idx_node_id ON {self.graph_name}.\"_ag_label_vertex\" (ag_catalog.agtype_access_operator(properties, \\'\"entity_id\"\\'::agtype))',\n            # f'CREATE INDEX CONCURRENTLY edge_p_idx ON {self.graph_name}.\"_ag_label_edge\" (id)',\n            f'CREATE INDEX CONCURRENTLY edge_sid_idx ON {self.graph_name}.\"_ag_label_edge\" (start_id)',\n            f'CREATE INDEX CONCURRENTLY edge_eid_idx ON {self.graph_name}.\"_ag_label_edge\" (end_id)',\n            f'CREATE INDEX CONCURRENTLY edge_seid_idx ON {self.graph_name}.\"_ag_label_edge\" (start_id,end_id)',\n            f'CREATE INDEX CONCURRENTLY directed_p_idx ON {self.graph_name}.\"DIRECTED\" (id)',\n            f'CREATE INDEX CONCURRENTLY directed_eid_idx ON {self.graph_name}.\"DIRECTED\" (end_id)',\n            f'CREATE INDEX CONCURRENTLY directed_sid_idx ON {self.graph_name}.\"DIRECTED\" (start_id)',\n            f'CREATE INDEX CONCURRENTLY directed_seid_idx ON {self.graph_name}.\"DIRECTED\" (start_id,end_id)',\n            f'CREATE INDEX CONCURRENTLY entity_p_idx ON {self.graph_name}.\"base\" (id)',\n            f'CREATE INDEX CONCURRENTLY entity_idx_node_id ON {self.graph_name}.\"base\" (ag_catalog.agtype_access_operator(properties, \\'\"entity_id\"\\'::agtype))',\n            f'CREATE INDEX CONCURRENTLY entity_node_id_gin_idx ON {self.graph_name}.\"base\" using gin(properties)',\n            f'ALTER TABLE {self.graph_name}.\"DIRECTED\" CLUSTER ON directed_sid_idx',\n        ]\n\n        for query in queries:\n            try:\n                await self.db.execute(\n                    query,\n                    upsert=True,\n                    with_age=True,\n                    graph_name=self.graph_name,\n                )\n                logger.info(f\"Successfully executed: {query}\")\n            except Exception:\n                continue\n```",
      "state": "closed",
      "author": "zhouzhou12",
      "author_type": "User",
      "created_at": "2025-04-24T02:07:26Z",
      "updated_at": "2025-04-24T10:01:54Z",
      "closed_at": "2025-04-24T10:01:53Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1444/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1444",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1444",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:35.205409",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thanks for sharing",
          "created_at": "2025-04-24T09:57:48Z"
        },
        {
          "author": "danielaskdd",
          "body": "I have already applied your suggestions to the main branch.",
          "created_at": "2025-04-24T10:01:53Z"
        }
      ]
    },
    {
      "issue_number": 1438,
      "title": "[Bug]:postgres_impl upsert_edge funtion can not create properties",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n\nIn the postgres_impl.PGGraphStorage.upsert_edge funtion can not work both update and insert.\nit can not  create(insert) properties when the edge does not exist in the \"DIRECTED\". this funtion only works when edge already exists, which means update.\n\nhowever the upsert_node funtion works good both in create and update. I don't know why....\n\n![Image](https://github.com/user-attachments/assets/cc3bb678-51f7-49e7-8122-c3d30fb8aaf2)\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "TonicZhang",
      "author_type": "User",
      "created_at": "2025-04-23T11:06:11Z",
      "updated_at": "2025-04-24T06:46:32Z",
      "closed_at": "2025-04-24T06:46:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1438/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1438",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1438",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:35.374634",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "I just check the PG storage with the latest version, and everything appears to be functioning normally. \n\n<img width=\"1041\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a8e53211-096f-4f86-8a0f-ab2887132956\" />\n\nIf any issues arise, please provide the LightRAG server logs along with a ",
          "created_at": "2025-04-23T16:46:02Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "@TonicZhang If you are using Postgres AGE without the fix for the Age Issue#1709 the edge properties will not be updated with the existing upsert_edge method. \n\n[https://github.com/apache/age/issues/1709](https://github.com/apache/age/issues/1709)\n\nIf you cannot get the correct fix applied(which is ",
          "created_at": "2025-04-23T21:45:40Z"
        },
        {
          "author": "TonicZhang",
          "body": "@danielaskdd  here is the details:\n\nconfigurations:\n    LightRAG Version:1.3.3\n    Operating System: Ubuntu 18.04.6 LTS\n    Python Version: 3.12.9\n    Postgres:16.8\n    Age: 1.5.0\n\n\n\nnormal Logs :\n\n> INFO: Stored 1 new unique documents\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processi",
          "created_at": "2025-04-24T01:53:21Z"
        },
        {
          "author": "TonicZhang",
          "body": "@acsangamnerkar thanks for your infomation. \nI referenced the temporary solution in that age issue , and modified the upsert method by double \"SET\"  statements, and that works.\nbut still don't know how to fix age directly.\n\nhere is the results of temporary solution:\n\n![Image](https://github.com/user",
          "created_at": "2025-04-24T02:15:11Z"
        },
        {
          "author": "danielaskdd",
          "body": "I have added your temporary solution to the main branch, as I do not foresee any negative impact on the program.",
          "created_at": "2025-04-24T03:06:44Z"
        }
      ]
    },
    {
      "issue_number": 1441,
      "title": "[Question]:how it is different from minirag",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nhow it is different from minirag \nhttps://github.com/HKUDS/MiniRAG\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Sandy4321",
      "author_type": "User",
      "created_at": "2025-04-23T23:11:48Z",
      "updated_at": "2025-04-23T23:11:48Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1441/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1441",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1441",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:35.622469",
      "comments": []
    },
    {
      "issue_number": 1436,
      "title": "[Feature Request]: pggraphdb automatically creates index if not exist",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\npggraphdb automatically creates index if not exist\n```sql\nload 'age';\nSET search_path = ag_catalog, \"$user\", public;\nCREATE INDEX CONCURRENTLY entity_p_idx ON dickens.\"Entity\" (id);\nCREATE INDEX CONCURRENTLY vertex_p_idx ON dickens.\"_ag_label_vertex\" (id);\nCREATE INDEX CONCURRENTLY directed_p_idx ON dickens.\"DIRECTED\" (id);\nCREATE INDEX CONCURRENTLY directed_eid_idx ON dickens.\"DIRECTED\" (end_id);\nCREATE INDEX CONCURRENTLY directed_sid_idx ON dickens.\"DIRECTED\" (start_id);\nCREATE INDEX CONCURRENTLY directed_seid_idx ON dickens.\"DIRECTED\" (start_id,end_id);\nCREATE INDEX CONCURRENTLY edge_p_idx ON dickens.\"_ag_label_edge\" (id);\nCREATE INDEX CONCURRENTLY edge_sid_idx ON dickens.\"_ag_label_edge\" (start_id);\nCREATE INDEX CONCURRENTLY edge_eid_idx ON dickens.\"_ag_label_edge\" (end_id);\nCREATE INDEX CONCURRENTLY edge_seid_idx ON dickens.\"_ag_label_edge\" (start_id,end_id);\ncreate INDEX CONCURRENTLY vertex_idx_node_id ON dickens.\"_ag_label_vertex\" (ag_catalog.agtype_access_operator(properties, '\"node_id\"'::agtype));\ncreate INDEX CONCURRENTLY entity_idx_node_id ON dickens.\"Entity\" (ag_catalog.agtype_access_operator(properties, '\"node_id\"'::agtype));\nCREATE INDEX CONCURRENTLY entity_node_id_gin_idx ON dickens.\"Entity\" using gin(properties);\nALTER TABLE dickens.\"DIRECTED\" CLUSTER ON directed_sid_idx;\n```\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "zhouzhou12",
      "author_type": "User",
      "created_at": "2025-04-23T06:44:09Z",
      "updated_at": "2025-04-23T17:44:05Z",
      "closed_at": "2025-04-23T17:43:10Z",
      "labels": [
        "enhancement",
        "PostgreSQL",
        "Core"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1436/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1436",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1436",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:35.622491",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thanks for sharing",
          "created_at": "2025-04-23T17:12:29Z"
        }
      ]
    },
    {
      "issue_number": 1434,
      "title": "[Question]: Using Smoldocling VLM for OCR",
      "body": "\nI'm unsure of a way to configure a VLM for OCR in order to use the Smoldocling for PDFs with embedded images. Is it possible?",
      "state": "open",
      "author": "matbeedotcom",
      "author_type": "User",
      "created_at": "2025-04-22T18:52:06Z",
      "updated_at": "2025-04-23T16:17:28Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1434/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1434",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1434",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:37.722760",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG will support MinerU as file content extractor soon.",
          "created_at": "2025-04-23T16:17:27Z"
        }
      ]
    },
    {
      "issue_number": 1437,
      "title": "[Question]:ERROR: Failed to process document doc-243baa18c97ffbf8582f5884e0e69896:  (status code: 502)",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhen I use the LightRAG server to process documents, it shows Failed, and the error reason is as indicated in the title.\n\n### Additional Context\n\nthe .env file as follows:\nWEBUI_TITLE='Graph RAG Engine'\nWEBUI_DESCRIPTION=\"Simple and Fast Graph Based RAG System\"\n\nENABLE_LLM_CACHE_FOR_EXTRACT=true\nSUMMARY_LANGUAGE=English\n\nTIMEOUT=300\nTEMPERATURE=0.5\nMAX_ASYNC=4\nMAX_TOKENS=32768\n\nLLM_BINDING=openai\nLLM_MODEL=gpt-4\nLLM_BINDING_HOST=https://api.openai.com/v1\nLLM_BINDING_API_KEY=sk-p\n\nEMBEDDING_MODEL=text-embedding-ada-002\nEMBEDDING_DIM=1536\nEMBEDDING_BINDING_API_KEY=sk-p\nEMBEDDING_BINDING=openai\nLLM_BINDING_HOST=https://api.openai.com/v1\n\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\nAnd other parameters were ser by default",
      "state": "open",
      "author": "GanQ1028",
      "author_type": "User",
      "created_at": "2025-04-23T09:02:08Z",
      "updated_at": "2025-04-23T16:11:30Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1437/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1437",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1437",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:37.898079",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You did not provide specific error information. However, this type of error is usually caused by LLM rate limiting. When indexing with LightRAG, it requires frequent calls to the LLM, which can trigger the LLM's rate limiting protection. You may need to upgrade the service tier of your LLM API.",
          "created_at": "2025-04-23T16:11:29Z"
        }
      ]
    },
    {
      "issue_number": 1425,
      "title": "[Question]:Issue with CSV Delimiters in Retrieved Content",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhen using the LightRAG 1.3.3 API query, the returned CSV format behaves inconsistently depending on whether the search contains English-only entities or Chinese entities:\n\n## English-only entities:\n\nThe CSV uses double quotes and commas as delimiters (e.g., \"id\",\"content\").\nThis format works correctly when parsed in Python.\nChinese entities:\n\nThe header row uses commas + tabs (,\\t) as delimiters (e.g., id,\\tcontent).\nThe remaining rows use only commas, causing parsing issues.\nIf a retrieved string contains internal commas, they are incorrectly treated as delimiters, breaking the CSV structure.\n## Problems Caused by Inconsistent Delimiters\nPython parsing fails when processing mixed formats (e.g., ,\\t in headers vs. , in content).\nManually replacing ,\\t with , doesn’t fully resolve the issue due to embedded commas in the text.\n## Suggested Solution\nStandardize the CSV format to always use double-quoted fields with commas (e.g., \"id\",\"content\").\nThis ensures compatibility with standard CSV parsers (e.g., Python's csv module).\nPrevents misinterpretation of internal commas as delimiters.\nQuestion: Why are two different delimiter schemes used? Can the API enforce a consistent format (e.g., \"id\",\"content\") for all cases?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "simplecellzg",
      "author_type": "User",
      "created_at": "2025-04-21T17:11:39Z",
      "updated_at": "2025-04-23T10:08:48Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1425/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1425",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1425",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:38.053902",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "PR #1412 may have addressed your issues; please review it at your convenience.",
          "created_at": "2025-04-22T04:49:40Z"
        },
        {
          "author": "simplecellzg",
          "body": "> PR #1412 may have addressed your issues; please review it at your convenience.\n\nThank you very much. Outputting in JSON format greatly facilitates post-processing",
          "created_at": "2025-04-23T10:08:47Z"
        }
      ]
    },
    {
      "issue_number": 1384,
      "title": "[Feature Request]: <title>speed up merge node with parallel",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nPreliminary testing shows that the speed bottleneck is at the merge node stage. Is it possible to optimize this by changing to a concurrent merge? \nCan I remove the “await” and use “create_task” and “gather” in the following code? Was the use of await here because some graph databases, like NetworkX, do not support concurrent node insertion, so everything was changed to a serial mode?\n\n![Image](https://github.com/user-attachments/assets/41b660b0-00e8-4ede-9da1-a69a421adc21)",
      "state": "closed",
      "author": "TonicZhang",
      "author_type": "User",
      "created_at": "2025-04-16T09:18:07Z",
      "updated_at": "2025-04-23T09:05:48Z",
      "closed_at": "2025-04-23T09:05:48Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1384/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1384",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1384",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:38.257635",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Too many concurrent requests can generate excessive database connections, consuming a large amount of resources and leading to a performance decline. The correct approach is to use batch mode for data queries. The main branch has already been optimized following this principle. Currently, the perfor",
          "created_at": "2025-04-16T17:12:42Z"
        }
      ]
    },
    {
      "issue_number": 1088,
      "title": "[Bug]: <title>value too long for type character varying(255)",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI'm using the main branch, demo code as below:\n```python\nimport asyncio\nimport logging\nimport os\nimport time\nfrom dotenv import load_dotenv\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.zhipu import zhipu_complete\nfrom lightrag.llm.ollama import ollama_embedding\nfrom lightrag.llm.openai import openai_embed,openai_complete_if_cache\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nimport numpy as np\n\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nWORKING_DIR = f\"{CURRENT_DIR}/lightrag_data\"\n\nFILE_PATH = f\"{CURRENT_DIR}/../../data_dir/marker_output/2305_15323v1.md\"\nFILE_NAME = os.path.basename(FILE_PATH)\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.DEBUG)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# PG\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\nos.environ[\"POSTGRES_HOST\"] = \"localhost\"\nos.environ[\"POSTGRES_PORT\"] = \"5432\"\nos.environ[\"POSTGRES_USER\"] = \"postgres\"\nos.environ[\"POSTGRES_PASSWORD\"] = \"\"\nos.environ[\"POSTGRES_DATABASE\"] = \"lightrag\"\n\n# neo4j\nos.environ[\"NEO4J_URI\"] = \"neo4j://localhost:7687\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"admin123\"\n\nasync def _llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        model=\"qwen-plus-latest\",\n        prompt=prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        **kwargs\n    )\n\nasync def _embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=\"text-embedding-v3\",\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\nasync def initialize_rag():\n    rag = LightRAG(\n        namespace_prefix=FILE_NAME,\n        working_dir=WORKING_DIR,\n        llm_model_func=_llm_model_func,\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=_embedding_func,\n        ),\n        embedding_batch_num=10,\n        embedding_func_max_async=10,\n        embedding_cache_config={\n            \"enabled\": \"true\",\n            \"similarity_threshold\": 0.95,\n            \"use_llm_check\": False,\n        },\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"Neo4JStorage\",\n        vector_storage=\"PGVectorStorage\",\n        auto_manage_storages_states=False,\n        # llm_model_kwargs={\n        #     \"response_format\": {\"type\": \"json_object\"},\n        #     \"extra_body\": {\"enable_search\": True}\n        #     },\n        addon_params={\n            \"language\": \"Chinese\"\n        },\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def main():\n    # Initialize RAG instance\n    rag = await initialize_rag()\n\n\n    # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n    with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n        await rag.ainsert(f.read(), ids=[FILE_NAME])\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\nAnd get errors:\n```\nerror:value too long for type character varying(255)\nFailed to extract entities and relationships\nFailed to process document doc-6b187f963bb8be55d3cc73c6faf9f7db: value too long for type character varying(255)\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-03-14T06:46:23Z",
      "updated_at": "2025-04-23T01:37:49Z",
      "closed_at": "2025-04-23T01:37:48Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1088/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1088",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1088",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:38.443411",
      "comments": [
        {
          "author": "JoramMillenaar",
          "body": "I ran into a similar issue. I had a lot of data revolving around a single entity and LightRAG appends a new description to the entity every time the LLM recognizes that entity in the data. So, if that entity is detected in many parts of your data, it will keep appending new descriptors of that entit",
          "created_at": "2025-03-14T17:29:21Z"
        },
        {
          "author": "bzImage",
          "body": "after doing a git pull, creating again the postgres database and processing a bunch of input files:\n\n--\n\nerror:value too long for type character varying(255)                                                                    Failed to extract entities and relationships                               ",
          "created_at": "2025-03-20T20:00:55Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "I looked through it and tested it again and it's working for me (It is storing 255+ characters).\nTry to rebuild your graph with a fresh db. \n\nThere was this PR recently merged https://github.com/HKUDS/LightRAG/pull/1120 that changed the field again (to a VAR(255) Array), which might be why you're ex",
          "created_at": "2025-03-20T23:44:16Z"
        }
      ]
    },
    {
      "issue_number": 1113,
      "title": "[Bug]: <title>column \"chunk_ids\" of relation \"lightrag_vdb_entity\" does not exist",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI think there's a sql bug for lightrag_vdb_entity, the debug sql as below:\n```\nsql:INSERT INTO LIGHTRAG_VDB_ENTITY (workspace, id, entity_name, content,\n                      content_vector, chunk_ids, file_path)\n                      VALUES ($1, $2, $3, $4, $5, $6::varchar[], $7::varchar[])\n                      ON CONFLICT (workspace,id) DO UPDATE\n                      SET entity_name=EXCLUDED.entity_name,\n                      content=EXCLUDED.content,\n                      content_vector=EXCLUDED.content_vector,\n                      chunk_ids=EXCLUDED.chunk_ids,\n                      file_path=EXCLUDED.file_path,\n                      update_time=CURRENT_TIMESTAMP\n```\n\nBut I found the LIGHTRAG_VDB_ENTITY table have the create sql as below, in postgres_impl.py:\n```\n\"LIGHTRAG_VDB_ENTITY\": {\n        \"ddl\": \"\"\"CREATE TABLE LIGHTRAG_VDB_ENTITY (\n                    id VARCHAR(255),\n                    workspace VARCHAR(255),\n                    entity_name VARCHAR(255),\n                    content TEXT,\n                    content_vector VECTOR,\n                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    update_time TIMESTAMP,\n                    chunk_id TEXT NULL,\n                    file_path TEXT NULL,\n\t                CONSTRAINT LIGHTRAG_VDB_ENTITY_PK PRIMARY KEY (workspace, id)\n                    )\"\"\"\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-03-18T12:21:58Z",
      "updated_at": "2025-04-23T01:37:33Z",
      "closed_at": "2025-04-23T01:37:33Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1113/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1113",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1113",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:38.640343",
      "comments": [
        {
          "author": "Howe829",
          "body": "You're right. Based on #1085, the chunk_ids of LIGHTRAG_VDB_ENTITY and LIGHTRAG_VDB_RELATION should be an array of VARCHAR. I believe it's also related to #1091.",
          "created_at": "2025-03-19T03:20:34Z"
        },
        {
          "author": "bzImage",
          "body": "error:column \"chunk_ids\" of relation \"lightrag_vdb_entity\" does not exist\nFailed to extract entities and relationships\n\nreporting the same issue.. ",
          "created_at": "2025-03-19T16:44:31Z"
        },
        {
          "author": "Howe829",
          "body": "> error:column \"chunk_ids\" of relation \"lightrag_vdb_entity\" does not exist Failed to extract entities and relationships\n> \n> reporting the same issue..\n\nHey,  \n\nPlease pull the latest code and rebuild your database, as the schemas of the `lightrag_vdb_entity` and `lightrag_vdb_relation` tables have",
          "created_at": "2025-03-21T00:55:12Z"
        },
        {
          "author": "jasperchen01",
          "body": "Thanks, it works",
          "created_at": "2025-04-23T01:37:25Z"
        }
      ]
    },
    {
      "issue_number": 1428,
      "title": "[Bug]: lightrag.py->apipeline_process_enqueue_documents->history_messages",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI've identified an issue in the LightRAG implementation that causes a KeyError: 'history_messages' when inserting documents. \n\n\n### Steps to reproduce\n\n#1 - Initialize a LightRAG instance with embedding and LLM functions:\nrag = LightRAG(\n    working_dir=\"./rag-workflow\",\n    embedding_func=embedding_func,\n    llm_model_func=async_llm_call,\n    namespace_prefix=\"jarvis\",\n)\n\n2 - Attempt to insert documents directly:\nrag.insert(documents)\n\n3 - The error occurs in the apipeline_process_enqueue_documents method at line 858 in lightrag.py:\ndel pipeline_status[\"history_messages\"][:]\n\nThe root cause is that when initializing the pipeline_status dictionary in the shared storage, the \"history_messages\" key is not being created before the code attempts to clear it.\n\nThis happens because in the apipeline_process_enqueue_documents method, the code assumes that \"history_messages\" already exists in the pipeline_status dictionary, but in a fresh initialization, this key hasn't been created yet.\n\nThe error specifically occurs during the first document insertion when the pipeline status is being initialized, but before any history messages have been added to the dictionary.\n\n### Expected Behavior\n\nProposed corrective action - revise lightrag.py line 585 to include 1st time initialization (before a users code tries to clear it).  This action adds an initialization step to ensure a key exists before any document processing begins. Example:\n\nif \"history_messages\" in pipeline_status:\n    del pipeline_status[\"history_messages\"][:]\nelse:\n    pipeline_status[\"history_messages\"] = []\n\n\n### LightRAG Config Used\n\n# Paste your config here:\nUbuntu 24.04\nPython 3.12\nLightrag-hku 1.3.0\n\n### Logs and screenshots\n\nINFO: Process 9215 Shared-Data created for Single Process\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './rag-workflow/vdb_entities.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './rag-workflow/vdb_relationships.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './rag-workflow/vdb_chunks.json'} 0 data\nINFO: Process 9215 initialized updated flags for namespace: [full_docs]\nINFO: Process 9215 ready to initialize storage namespace: [full_docs]\nINFO: Process 9215 initialized updated flags for namespace: [text_chunks]\nINFO: Process 9215 ready to initialize storage namespace: [text_chunks]\nINFO: Process 9215 initialized updated flags for namespace: [entities]\nINFO: Process 9215 initialized updated flags for namespace: [relationships]\nINFO: Process 9215 initialized updated flags for namespace: [chunks]\nINFO: Process 9215 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 9215 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 9215 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 9215 initialized updated flags for namespace: [doc_status]\nINFO: Process 9215 ready to initialize storage namespace: [doc_status]\nTraceback (most recent call last):\n  File \"/home/user@domain.com/Dev/Jarvis/main.py\", line 61, in <module>\n    rag.insert(documents)\n  File \"/home/user@domain.com/Dev/Jarvis/.venv/lib/python3.12/site-packages/lightrag/lightrag.py\", line 567, in insert\n    loop.run_until_complete(\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/user@domain.com/Dev/Jarvis/.venv/lib/python3.12/site-packages/lightrag/lightrag.py\", line 593, in ainsert\n    await self.apipeline_process_enqueue_documents(\n  File \"/home/user@domain.com/Dev/Jarvis/.venv/lib/python3.12/site-packages/lightrag/lightrag.py\", line 858, in apipeline_process_enqueue_documents\n    del pipeline_status[\"history_messages\"][:]\n        ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\nKeyError: 'history_messages'\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "randall-netbbs",
      "author_type": "User",
      "created_at": "2025-04-21T19:05:38Z",
      "updated_at": "2025-04-22T21:52:28Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1428/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1428",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1428",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:38.828367",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You probably forgot this: (refer to README)\n```\ninitialize_pipeline_status()\n```",
          "created_at": "2025-04-22T01:42:46Z"
        },
        {
          "author": "randall-netbbs",
          "body": "Not really. Any code should handle an absent initialization from within the library if it ever wishes to meet global and routine production standards. Within regulatory and accreditation environments, after code review, security often denies production use. Denied because the developer oversight cre",
          "created_at": "2025-04-22T21:52:27Z"
        }
      ]
    },
    {
      "issue_number": 1429,
      "title": "[Bug]: Labels not getting returned when using postgres graph",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen getting all labels from the postgres graph the formatting was not as expected and therefore falling.\n\n### Steps to reproduce\n\nUse postgres as the graph db \nUpload test document. \nView Knowledge Graph\nSearch labels\nNo results\n\n\n### Expected Behavior\n\nReturn a list of labels and be able to search them.\n\n### LightRAG Config Used\n\n# Paste your config here\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=PGVectorStorage\nLIGHTRAG_GRAPH_STORAGE=PGGraphStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_USER=<user>\nPOSTGRES_PASSWORD=<password>\nPOSTGRES_DATABASE=lightrag\n\n### Logs and screenshots\n\n2025-04-21 22:40:30,106 - lightrag - ERROR - Error getting graph labels: 'label'\n2025-04-21 22:40:30,107 - lightrag - ERROR - Traceback (most recent call last):\n  File \"<removed>\\lightrag\\api\\routers\\graph_routes.py\", line 40, in get_graph_labels\n    return await rag.get_graph_labels()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<removed>\\lightrag\\lightrag.py\", line 511, in get_graph_labels\n    text = await self.chunk_entity_relation_graph.get_all_labels()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<removed>\\lightrag\\kg\\postgres_impl.py\", line 1816, in get_all_labels\n    labels = [result[\"label\"] for result in results]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<removed>\\lightrag\\kg\\postgres_impl.py\", line 1816, in <listcomp>\n    labels = [result[\"label\"] for result in results]\n              ~~~~~~^^^^^^^^^\nKeyError: 'label'\n\n### Additional Information\n\n- LightRAG Version:1.3.4\n- Operating System: Windows 11\n- Python Version: 3.11\n- Related Issues: N/A\n",
      "state": "closed",
      "author": "widgit",
      "author_type": "User",
      "created_at": "2025-04-22T09:40:22Z",
      "updated_at": "2025-04-22T10:41:29Z",
      "closed_at": "2025-04-22T10:41:28Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1429/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1429",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1429",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:39.013680",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thanks for reporting and fixing the bug.",
          "created_at": "2025-04-22T10:41:28Z"
        }
      ]
    },
    {
      "issue_number": 1390,
      "title": "[Question]: API query returns no information based on knowledge",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello dear developers! Thank you for your product! \n\nPlease advase how to limit the LLM to search for an answer ONLY from the knowledge base? I have put some IT process documents in it but when I do query for example \"What is an incident?\" the reponse seems like to be a LLM own knowledge and not the vectors and graph one?\n\nRegards,\nAndy\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ndrewpj",
      "author_type": "User",
      "created_at": "2025-04-17T06:49:59Z",
      "updated_at": "2025-04-22T07:20:43Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1390/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1390",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1390",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:39.191418",
      "comments": [
        {
          "author": "emgiezet",
          "body": "`Query` mode should be set to `local` \n\n```\nrag.query(\n        \"What are the top themes in this story?\",\n        param=QueryParam(mode=\"local\")\n    )\n```\n\nBy default it's `global` so it using llm own knowlage.",
          "created_at": "2025-04-18T12:34:52Z"
        },
        {
          "author": "ndrewpj",
          "body": "> `Query` mode should be set to `local`\n> \n> ```\n> rag.query(\n>         \"What are the top themes in this story?\",\n>         param=QueryParam(mode=\"local\")\n>     )\n> ```\n> \n> By default it's `global` so it using llm own knowlage.\n\nIf using \"mix\" mode it is the same, right? I mean I have a couple of I",
          "created_at": "2025-04-20T10:31:53Z"
        },
        {
          "author": "emgiezet",
          "body": "mix is global + local so the knowledge will be from llm and from rag embeds",
          "created_at": "2025-04-22T07:20:41Z"
        }
      ]
    },
    {
      "issue_number": 1404,
      "title": "[Feature Request]:Add full markdown and mermaid diagram support for query view of WebUI",
      "body": "### Feature Request Description\n\n- Add full markdown support for query view of WebUI\n- Add mermaid diagram support for query view of WebUI\n\nThis is the desired display effect:\n\n<img width=\"845\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c1b45d01-76f9-48a2-8f2f-b862cb2388a6\" />",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-18T07:34:55Z",
      "updated_at": "2025-04-22T05:04:30Z",
      "closed_at": "2025-04-22T05:04:30Z",
      "labels": [
        "enhancement",
        "ui"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1404/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1404",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1404",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:39.357742",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "@choizhang Are you able to handle this issue？",
          "created_at": "2025-04-20T17:34:26Z"
        }
      ]
    },
    {
      "issue_number": 1405,
      "title": "[Feature Request]:Revise the context format of chunks from CSV to JSON to enhance compatibility with LLM",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n[Feature Request]:Revise the context format of chunks from CSV to JSON to enhance compatibility with LLMs\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-18T08:23:26Z",
      "updated_at": "2025-04-22T04:50:00Z",
      "closed_at": "2025-04-22T04:50:00Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1405/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.3.8",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1405",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1405",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:39.518572",
      "comments": []
    },
    {
      "issue_number": 1285,
      "title": "[Feature Request]: Docling must be installed before starting server",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nThe Docling package is quite large, takes a long time to install, and is prone to errors. Therefore, it is not recommended to dynamically install Docling when the LightRAG server starts.  \n\nInstead, if the server detects that the document extraction option has enabled Docling during startup, it should check whether Docling has been properly installed. If not, it should prompt the user to manually install Docling first and refuse to start the server.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-06T18:44:41Z",
      "updated_at": "2025-04-22T04:33:32Z",
      "closed_at": "2025-04-22T04:33:30Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1285/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.3.8",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1285",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1285",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:39.518597",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Plang to use MinerU server instead of build-in file extractor. Docling should change to independent server also.",
          "created_at": "2025-04-22T04:33:30Z"
        }
      ]
    },
    {
      "issue_number": 1189,
      "title": "[Bug]: PostgreSQL database, error:syntax error at or near \",\"",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI tested with list of IDs with QueryParam, it spit this error. Tested using global and mix mode. Below is full traceback.\n\nI debug the generated SQL in `File \"/home/ubuntu/LightRAG/lightrag/kg/postgres_impl.py\", line 538, in query`, it seems fine, only not sure checking for empty array is the cause with IS NULL method. \n\n\n### Steps to reproduce\nSimple rag initiate using API examples. Pass ids as `ids=[list of ids]` in Query Param.\n\n### Expected Behavior\n\nShould retrieve docs based on filtered doc id.\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n```\nTraceback (most recent call last):\n  File \"/home/ubuntu/server/server_v2.py\", line 366, in query_endpoint\n    result = await rag.aquery(\n  File \"/home/ubuntu/LightRAG/lightrag/lightrag.py\", line 1343, in aquery\n    response = await kg_query(\n  File \"/home/ubuntu/LightRAG/lightrag/operate.py\", line 749, in kg_query\n    context = await _build_query_context(\n  File \"/home/ubuntu/LightRAG/lightrag/operate.py\", line 1144, in _build_query_context\n    entities_context, relations_context, text_units_context = await _get_edge_data(\n  File \"/home/ubuntu/LightRAG/lightrag/operate.py\", line 1488, in _get_edge_data\n    results = await relationships_vdb.query(\n  File \"/home/ubuntu/LightRAG/lightrag/kg/postgres_impl.py\", line 538, in query\n    results = await self.db.query(sql, params=params, multirows=True)\n  File \"/home/ubuntu/LightRAG/lightrag/kg/postgres_impl.py\", line 138, in query\n    rows = await connection.fetch(sql, *params.values())\n  File \"/home/ubuntu/anaconda3/envs/server/lib/python3.10/site-packages/asyncpg/connection.py\", line 690, in fetch\n    return await self._execute(\n  File \"/home/ubuntu/anaconda3/envs/server/lib/python3.10/site-packages/asyncpg/connection.py\", line 1864, in _execute\n    result, _ = await self.__execute(\n  File \"/home/ubuntu/anaconda3/envs/server/lib/python3.10/site-packages/asyncpg/connection.py\", line 1961, in __execute\n    result, stmt = await self._do_execute(\n  File \"/home/ubuntu/anaconda3/envs/server/lib/python3.10/site-packages/asyncpg/connection.py\", line 2004, in _do_execute\n    stmt = await self._get_statement(\n  File \"/home/ubuntu/anaconda3/envs/server/lib/python3.10/site-packages/asyncpg/connection.py\", line 432, in _get_statement\n    statement = await self._protocol.prepare(\n  File \"asyncpg/protocol/protocol.pyx\", line 165, in prepare\nasyncpg.exceptions.PostgresSyntaxError: syntax error at or near \",\"\n```\nHere is the output of SQL:\n```\nGenerated SQL: \n    WITH relevant_chunks AS (\n        SELECT id as chunk_id\n        FROM LIGHTRAG_DOC_CHUNKS\n        WHERE 'dbbe59b183eaa','3abff2bff7126' IS NULL OR full_doc_id = ANY(ARRAY['dbbe59b183eaa','3abff2bff7126'])\n    )\n    SELECT source_id as src_id, target_id as tgt_id\n    FROM (\n        SELECT r.id, r.source_id, r.target_id, 1 - (r.content_vector <=> '[-0.0281219482421875]... ::vector) as distance\n        FROM LIGHTRAG_VDB_RELATION r\n        JOIN relevant_chunks c ON c.chunk_id = ANY(r.chunk_ids)\n        WHERE r.workspace=$1\n    ) filtered\n    WHERE distance>$2\n    ORDER BY distance DESC\n    LIMIT $3\n```\n\n### Additional Information\n\n- LightRAG Version: 1.3.0\n- Operating System: Ubuntu\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "open",
      "author": "husaynirfan1",
      "author_type": "User",
      "created_at": "2025-03-25T18:54:18Z",
      "updated_at": "2025-04-21T18:34:53Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1189/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1189",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1189",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:39.698120",
      "comments": [
        {
          "author": "OxidBurn",
          "body": "Published a potential fix for the reported issue: https://github.com/HKUDS/LightRAG/pull/1427",
          "created_at": "2025-04-21T18:34:51Z"
        }
      ]
    },
    {
      "issue_number": 1277,
      "title": "[Question]: Graph storage performance - Postgres AGE is running slow",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI have been playing around with the Graph storage and have used Azure Postgres - Flex server with AGE and VECTOR extension to address all the storage needs of Lightrag. I have also created the indices as documented in Lightrag documentation but the performance is extremely slow in the retrieval process. Even for top k of 10, with a graph of 3500 nodes and 4500 edges, it is taking 3-5 minutes for the response. The knowledge graph process is not an issue. \n\nWith support for Mongo dropping as a solution for all storages, there is a need to improve the postgres  graph performance to increase the adoption of the lightrag as a solution. \n\n### Additional Context\n\nHere is the postgres query as the longest running query by Azure monitoring:\n\nSELECT query_sql_text FROM query_store.query_texts_view WHERE query_text_id=4153752584585438422;\n                                query_sql_text                                 \n-------------------------------------------------------------------------------\n SELECT * FROM cypher('chunk_entity_relation', $$                             +\n                     MATCH (n:base)                                           +\n                     OPTIONAL MATCH (n)-[r]->(target:base)                    +\n                     RETURN collect(distinct n) AS n, collect(distinct r) AS r+\n                     LIMIT 1000                                               +\n                     $$) AS (n agtype, r agtype)",
      "state": "open",
      "author": "acsangamnerkar",
      "author_type": "User",
      "created_at": "2025-04-05T21:12:37Z",
      "updated_at": "2025-04-21T12:36:22Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1277/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1277",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1277",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:39.919999",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "PostgreSQL graph performance issue is on the priority waiting list.",
          "created_at": "2025-04-08T16:56:28Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "> PostgreSQL graph performance issue is on the priority waiting list.\n\nThanks. I see the new branch created form the other issue. ",
          "created_at": "2025-04-08T23:04:54Z"
        },
        {
          "author": "hamzafj5",
          "body": "Hello,\nIs there any progress on this? As this is being eagerly waited on.\nOr if someone can help me in this, will be much appreciated.",
          "created_at": "2025-04-21T08:09:23Z"
        },
        {
          "author": "khizarhussain19",
          "body": "Hi,\nAny updates on this ?\nI was able to run queries easily within 3-4 seconds with PGVector + Age when I loaded 2 documents only. However when the size of my documents increased to about 30, my CPU usage was becoming 100% and I was getting no response.\nThe KG size is about 10k Nodes and 20k edges.",
          "created_at": "2025-04-21T08:24:06Z"
        },
        {
          "author": "danielaskdd",
          "body": "PostgreSQL AGE performance issues have been resolved since v1.3.3. You can pull the latest version from the main branch and give it a try.",
          "created_at": "2025-04-21T12:34:27Z"
        }
      ]
    },
    {
      "issue_number": 1421,
      "title": "[Question]: Openrouter API and Ollama embed",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am trying to combine OpenRouter API and Ollama Embedding. \ncan't figure it out why ERROR: Failed to extract entities and relationships\n\nimport os\nimport asyncio\nimport logging\nimport logging.config\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete\nfrom lightrag.llm.ollama import ollama_embed\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import logger, set_verbose_debug, EmbeddingFunc\nfrom openai import OpenAI\n\nWORKING_DIR = \"./dickens\"\n\n\ndef configure_logging():\n    \"\"\"Configure logging for the application\"\"\"\n\n    # Reset any existing handlers to ensure clean configuration\n    for logger_name in [\"uvicorn\", \"uvicorn.access\", \"uvicorn.error\", \"lightrag\"]:\n        logger_instance = logging.getLogger(logger_name)\n        logger_instance.handlers = []\n        logger_instance.filters = []\n\n    # Get log directory path from environment variable or use current directory\n    log_dir = os.getenv(\"LOG_DIR\", os.getcwd())\n    log_file_path = os.path.abspath(os.path.join(log_dir, \"lightrag_demo.log\"))\n\n    print(f\"\\nLightRAG demo log file: {log_file_path}\\n\")\n    os.makedirs(os.path.dirname(log_dir), exist_ok=True)\n\n    # Get log file max size and backup count from environment variables\n    log_max_bytes = int(os.getenv(\"LOG_MAX_BYTES\", 10485760))  # Default 10MB\n    log_backup_count = int(os.getenv(\"LOG_BACKUP_COUNT\", 5))  # Default 5 backups\n\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"default\": {\n                    \"format\": \"%(levelname)s: %(message)s\",\n                },\n                \"detailed\": {\n                    \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n                },\n            },\n            \"handlers\": {\n                \"console\": {\n                    \"formatter\": \"default\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stderr\",\n                },\n                \"file\": {\n                    \"formatter\": \"detailed\",\n                    \"class\": \"logging.handlers.RotatingFileHandler\",\n                    \"filename\": log_file_path,\n                    \"maxBytes\": log_max_bytes,\n                    \"backupCount\": log_backup_count,\n                    \"encoding\": \"utf-8\",\n                },\n            },\n            \"loggers\": {\n                \"lightrag\": {\n                    \"handlers\": [\"console\", \"file\"],\n                    \"level\": \"INFO\",\n                    \"propagate\": False,\n                },\n            },\n        }\n    )\n\n    # Set the logger level to INFO\n    logger.setLevel(logging.INFO)\n    # Enable verbose debug if needed\n    set_verbose_debug(os.getenv(\"VERBOSE_DEBUG\", \"false\").lower() == \"true\")\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\n# Initialize the OpenAI client with OpenRouter API configuration\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=\"sk-...\"\n)\n\nmodel = \"google/gemini-2.0-flash-lite-001\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"What city is in Taiwan?\"}\n]\n\ntry:\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    print(\"LLM Output:\", response.choices[0].message.content)\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n\nasync def initialize_rag():\n    def filtered_llm_model_func(input_text, **kwargs):\n        # Filter out unsupported arguments\n        supported_kwargs = {key: kwargs[key] for key in [\"temperature\", \"top_p\"] if key in kwargs}\n        response = client.chat.completions.create(\n            model=\"google/gemini-2.0-flash-lite-001\",\n            messages=[{\"role\": \"user\", \"content\": input_text}],\n            **supported_kwargs\n        )\n        return response.choices[0].message.content\n\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=filtered_llm_model_func,  # Updated to use OpenAI client\n        llm_model_name=\"google/gemini-2.0-flash-lite-001\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"temperature\": 0.7,\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=lambda texts: ollama_embed(\n                texts, embed_model=\"snowflake-arctic-embed2:568m\", host=\"http://192...\"\n            ),\n        ),\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def main():\n    try:\n        # Initialize RAG instance\n        rag = await initialize_rag()\n\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform naive search\n        print(\"\\n=====================\")\n        print(\"Query mode: naive\")\n        print(\"=====================\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\"\\n=====================\")\n        print(\"Query mode: local\")\n        print(\"=====================\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\"\\n=====================\")\n        print(\"Query mode: global\")\n        print(\"=====================\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\"\\n=====================\")\n        print(\"Query mode: hybrid\")\n        print(\"=====================\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        if rag:\n            await rag.finalize_storages()\n\n\nif __name__ == \"__main__\":\n    # Configure logging before running the main function\n    configure_logging()\n    asyncio.run(main())\n    print(\"\\nDone!\")\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "molion097",
      "author_type": "User",
      "created_at": "2025-04-21T07:41:34Z",
      "updated_at": "2025-04-21T10:09:56Z",
      "closed_at": "2025-04-21T10:09:56Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1421/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1421",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1421",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.096048",
      "comments": []
    },
    {
      "issue_number": 1072,
      "title": "What is the best storage solution currently available?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n1. There is a need for multiple knowledge bases. The Neo4j Community Edition currently cannot create new databases. Does it support multiple knowledge databases?\n2. PostgreSQL does not support namespace differentiation.  https://github.com/HKUDS/LightRAG/issues/814\n3. The performance of the JSON storage method is too poor. So, what is the best storage solution currently available that supports multiple knowledge bases?\n Thank you for your reply.\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ZhuLinsen",
      "author_type": "User",
      "created_at": "2025-03-12T12:13:43Z",
      "updated_at": "2025-04-21T08:17:43Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1072/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1072",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1072",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.096071",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You have the capability to launch multiple LightRAG instances by utilizing different .env files within your startup directories. LightRAG is scheduled to  developing a Workspace feature, which is slated for release in the near future. We encourage you to join the discussion on this topic in our dedi",
          "created_at": "2025-03-12T14:50:55Z"
        },
        {
          "author": "reqyou",
          "body": "This doesnt answer the question i think. The author wants to know if there is a graph_storage, kv_storage combination which works atm.  ",
          "created_at": "2025-03-13T15:24:18Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "I have played around a bit with various options of storage. I have not done any formal measurements but here are my observations. \n\nGraphstorage selection significantly impacts the performance of the solution.\n\n1) Postgres docker(the docker image that Shangor has prepared) but it was too slow, I saw",
          "created_at": "2025-03-17T02:18:57Z"
        },
        {
          "author": "khizarhussain19",
          "body": "Hi,\nAny updates on this ?\nI was able to run queries easily within 3-4 seconds with PGVector + Age when I loaded 2 documents only. However when the size of my documents increased to about 30, my CPU usage was becoming 100% and I was getting no response.\nThe KG size is about 10k Nodes and 20k edges.",
          "created_at": "2025-04-21T08:17:42Z"
        }
      ]
    },
    {
      "issue_number": 1420,
      "title": "[Question]:",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello, when I was building a knowledge graph (mixed dataset) during the paper reproduction process, why did lightrag take much longer to build the index than graphrag, and the token was also much longer than graphrag\n\napi call: gpt-4o, text-embedding-3-large\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Mitu-labda",
      "author_type": "User",
      "created_at": "2025-04-21T07:26:53Z",
      "updated_at": "2025-04-21T07:26:53Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1420/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1420",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1420",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.348663",
      "comments": []
    },
    {
      "issue_number": 1306,
      "title": "[Bug]: ERROR: Failed to process document doc...: POST predict: Post \"http://127.0.0.1:32034/completion",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processing batch 1 of 1.\nINFO: Inserting 1 to doc_status\nINFO: Inserting 10 to chunks\nINFO: Inserting 1 to full_docs\nINFO: Inserting 10 to text_chunks\nERROR: Failed to extract entities and relationships\nERROR: Failed to process document doc-fa31782fab5ad1625e0c507638b9013b: POST predict: Post \"http://127.0.0.1:32034/completion\": read tcp 127.0.0.1:32036->127.0.0.1:32034: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\nINFO: Inserting 1 to doc_status\nINFO: In memory DB persist to disk\nINFO: Completed batch 1 of 1.\nINFO: Document processing pipeline completed\n\n### Steps to reproduce\n\nLatest install with Ollama gemma3:12, Ollama embedding and postgres:\n\n\n### Expected Behavior\n\nPDF Document should be indexed. Tried multiple PDFs, obviously the access attempt to this port seems the issue,\n\n### LightRAG Config Used\n\n\nLLM_BINDING=ollama\nLLM_MODEL=gemma3:12b\nLLM_BINDING_HOST=http://192.168.128.14:4350\nMAX_TOKENS=8192\n\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://192.168.128.14:4350\nEMBEDDING_MODEL=nomic-embed-text:latest\nEMBEDDING_DIM=768\n\n### PostgreSQL Configuration\nPOSTGRES_HOST=192.168.119..11\nPOSTGRES_PORT=5432\nPOSTGRES_USER=lightrag\nPOSTGRES_PASSWORD='...'\nPOSTGRES_DATABASE=lightrag\n\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=PGVectorStorage\nLIGHTRAG_GRAPH_STORAGE=PGGraphStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\n### Logs and screenshots\n\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processing batch 1 of 1.\nINFO: Inserting 1 to doc_status\nINFO: Inserting 10 to chunks\nINFO: Inserting 1 to full_docs\nINFO: Inserting 10 to text_chunks\nERROR: Failed to extract entities and relationships\nERROR: Failed to process document doc-fa31782fab5ad1625e0c507638b9013b: POST predict: Post \"http://127.0.0.1:32034/completion\": read tcp 127.0.0.1:32036->127.0.0.1:32034: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\nINFO: Inserting 1 to doc_status\nINFO: In memory DB persist to disk\nINFO: Completed batch 1 of 1.\nINFO: Document processing pipeline completed\n\n### Additional Information\n\n- LightRAG Version: latest github or pip (tried both)\n- Operating System: Ubuntu 22.04 or WSL (tried both, same error)\n- Python Version: 3.12\n- Related Issues: Also tried without Postgres, default local files storage, same error\n\n\nSeems it's actually an error in ollama _client.py, see: https://github.com/ollama/ollama/issues/7640  \n\nI played with later versions (of ollama python package)  from GitHub instead of pip, finally I also tried the latest, unreleased version directly from the repo. Additionaly I tried to run lightrag-server on the same server where Ollama is running, so 127.0.0.1 would be a valid IP for Ollama. \n\nHowever, it still failed with the same issue. \n\nWhile testing I noticed another issue: AttributeError: 'list' object has no attribute 'upsert'. Did you mean: 'insert'?\n\n```\nERROR: Traceback (most recent call last):\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py\", line 964, in process_document\n    self.text_chunks.upsert(chunks)\nAttributeError: 'list' object has no attribute 'upsert'. Did you mean: 'insert'?\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/api/routers/document_routes.py\", line 401, in pipeline_index_file\n    await rag.apipeline_process_enqueue_documents()\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py\", line 1047, in apipeline_process_enqueue_documents\n    await asyncio.gather(*doc_tasks)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py\", line 1001, in process_document\n    text_chunks_task,\nUnboundLocalError: local variable 'text_chunks_task' referenced before assignment\n\nERROR: Failed to extract entities and relationships\nTask exception was never retrieved\nfuture: <Task finished name='Task-108' coro=<LightRAG._process_entity_relation_graph() done, defined at /opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py:1092> exception=ResponseError('POST predict: Post \"http://127.0.0.1:34896/completion\": read tcp 127.0.0.1:34898->127.0.0.1:34896: wsarecv: An existing connection was forcibly closed by the remote host.')>\nTraceback (most recent call last):\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py\", line 1108, in _process_entity_relation_graph\n    raise e\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py\", line 1096, in _process_entity_relation_graph\n    await extract_entities(\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/operate.py\", line 601, in extract_entities\n    results = await asyncio.gather(*tasks)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/operate.py\", line 552, in _process_single_content\n    final_result = await _user_llm_func_with_cache(hint_prompt)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/operate.py\", line 472, in _user_llm_func_with_cache\n    res: str = await use_llm_func(input_text)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/utils.py\", line 279, in wait_func\n    result = await func(*args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/llm/ollama.py\", line 99, in ollama_model_complete\n    return await _ollama_model_if_cache(\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/llm/ollama.py\", line 71, in _ollama_model_if_cache\n    response = await ollama_client.chat(model=model, messages=messages, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/ollama/_client.py\", line 837, in chat\n    return await self._request(\n  File \"/opt/lightrag/lib/python3.10/site-packages/ollama/_client.py\", line 682, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n  File \"/opt/lightrag/lib/python3.10/site-packages/ollama/_client.py\", line 626, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: POST predict: Post \"http://127.0.0.1:34896/completion\": read tcp 127.0.0.1:34898->127.0.0.1:34896: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\nERROR: Failed to extract entities and relationships\nTask exception was never retrieved\nfuture: <Task finished name='Task-112' coro=<LightRAG._process_entity_relation_graph() done, defined at /opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py:1092> exception=ResponseError('POST predict: Post \"http://127.0.0.1:34928/completion\": read tcp 127.0.0.1:34932->127.0.0.1:34928: wsarecv: An existing connection was forcibly closed by the remote host.')>\nTraceback (most recent call last):\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py\", line 1108, in _process_entity_relation_graph\n    raise e\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/lightrag.py\", line 1096, in _process_entity_relation_graph\n    await extract_entities(\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/operate.py\", line 601, in extract_entities\n    results = await asyncio.gather(*tasks)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/operate.py\", line 552, in _process_single_content\n    final_result = await _user_llm_func_with_cache(hint_prompt)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/operate.py\", line 472, in _user_llm_func_with_cache\n    res: str = await use_llm_func(input_text)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/utils.py\", line 279, in wait_func\n    result = await func(*args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/llm/ollama.py\", line 99, in ollama_model_complete\n    return await _ollama_model_if_cache(\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/lightrag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/lightrag/llm/ollama.py\", line 71, in _ollama_model_if_cache\n    response = await ollama_client.chat(model=model, messages=messages, **kwargs)\n  File \"/opt/lightrag/lib/python3.10/site-packages/ollama/_client.py\", line 837, in chat\n    return await self._request(\n  File \"/opt/lightrag/lib/python3.10/site-packages/ollama/_client.py\", line 682, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n  File \"/opt/lightrag/lib/python3.10/site-packages/ollama/_client.py\", line 626, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: POST predict: Post \"http://127.0.0.1:34928/completion\": read tcp 127.0.0.1:34932->127.0.0.1:34928: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n```\n",
      "state": "open",
      "author": "Network-Sec",
      "author_type": "User",
      "created_at": "2025-04-07T21:35:28Z",
      "updated_at": "2025-04-21T04:24:19Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1306/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1306",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1306",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.348677",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The issue appears to be caused by Ollama. Please ensure your Ollama service is running properly and accessible from LightRAG.",
          "created_at": "2025-04-08T11:14:38Z"
        },
        {
          "author": "Glitchfix",
          "body": "I encountered this error yesterday and found a solution to it\nAnd yes @danielaskdd  this is happening due to Ollama but only when the model is not loaded on your GPU memory for some reason\nI have shared my analysis and the fix here\n\nhttps://github.com/ollama/ollama/issues/7640#issuecomment-281528065",
          "created_at": "2025-04-18T11:28:11Z"
        },
        {
          "author": "danielaskdd",
          "body": "Thanks for sharing.",
          "created_at": "2025-04-21T04:24:17Z"
        }
      ]
    },
    {
      "issue_number": 1379,
      "title": "[Feature Request]: \"entity_continue_extraction\" should be formulated a bit differently & new chunking function",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nThis is a very minor change. I am currently experimenting with the new gpt-4.1-mini and it is wonderfull and works perfectly with my given instructions. \n\nThe only thing I noticed when doing the \"entity_continue_extraction\" is that gpt-4.1-mini starts renaming the same entities and relationships from the previous message which isn't necessary and will cause duplicate desciptions for a lot of entities. \n\n```python\nPROMPTS[\"entity_continue_extraction\"] = \"\"\"\nMANY entities and relationships might have been missed in the last extraction. This is critical for our dense database, which is essential to the company.\n\n---Remember Steps---\n\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, use same language as input text. If English, capitalized the name.\n- entity_type: One of the following types: [{entity_types}]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details\nFormat each relationship as (\"relationship\"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)\n\n3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.\nFormat the content-level key words as (\"content_keywords\"{tuple_delimiter}<high_level_keywords>)\n\n4. If there is exisiting data in the existing data result use that to add relationships where needed or to improve other relations.\n\n5. Return output in {language} as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.\n\n6. When finished, output {completion_delimiter}\n\n7. Do not write down the same entities or relationships already mentioned in your previous answer. This step should only output previously missed entities or relationships.\n\n---Output---\n\nAdd them below using the same format:\\n\n\"\"\".strip()\n```\n\nHere I added step 7 and I changed the starting sentence to mention that there \"might\" have been relationships and entities missed in the previous extraction.\n\nThis makes the AI perform better and gives less duplicate descriptions. It is a very minor but powerfull change.\n\n### Additional Context\n\nI request a differnt chunking method. Now I have noticed that sometimes you can have a chunk which is for example 5 tokens at the end. Lets say i have chunk size set to 1200 and a document which is 2500 tokens then I get 3 chunks. The first 2 chunks are fine but the final chunk has no context whathowevefr. Thats why I want the chunking function to make sure chunks are at least 800 tokens.\n\n```python\ndef chunking_by_token_size(\n    content: str,\n    split_by_character: str | None = None,\n    split_by_character_only: bool = False,\n    overlap_token_size: int = 128,\n    max_token_size: int = 1024,\n    tiktoken_model: str = \"gpt-4o\",\n    min_chunk_size: int = 800,  # new parameter to control merging\n) -> list[dict[str, Any]]:\n    tokens = encode_string_by_tiktoken(content, model_name=tiktoken_model)\n    results: list[dict[str, Any]] = []\n    if split_by_character:\n        raw_chunks = content.split(split_by_character)\n        new_chunks = []\n        if split_by_character_only:\n            for chunk in raw_chunks:\n                _tokens = encode_string_by_tiktoken(chunk, model_name=tiktoken_model)\n                new_chunks.append((len(_tokens), chunk))\n        else:\n            for chunk in raw_chunks:\n                _tokens = encode_string_by_tiktoken(chunk, model_name=tiktoken_model)\n                if len(_tokens) > max_token_size:\n                    for start in range(\n                        0, len(_tokens), max_token_size - overlap_token_size\n                    ):\n                        chunk_content = decode_tokens_by_tiktoken(\n                            _tokens[start : start + max_token_size],\n                            model_name=tiktoken_model,\n                        )\n                        new_chunks.append(\n                            (min(max_token_size, len(_tokens) - start), chunk_content)\n                        )\n                else:\n                    new_chunks.append((len(_tokens), chunk))\n        for index, (_len, chunk) in enumerate(new_chunks):\n            results.append(\n                {\n                    \"tokens\": _len,\n                    \"content\": chunk.strip(),\n                    \"chunk_order_index\": index,\n                }\n            )\n    else:\n        for index, start in enumerate(\n            range(0, len(tokens), max_token_size - overlap_token_size)\n        ):\n            chunk_content = decode_tokens_by_tiktoken(\n                tokens[start : start + max_token_size], model_name=tiktoken_model\n            )\n            results.append(\n                {\n                    \"tokens\": min(max_token_size, len(tokens) - start),\n                    \"content\": chunk_content.strip(),\n                    \"chunk_order_index\": index,\n                }\n            )\n    \n    # Merging step: iterate through the chunks and merge any chunk\n    # that has fewer tokens than the min_chunk_size with the previous chunk.\n    if results:\n        merged_results = []\n        # Start with the first chunk\n        current_chunk = results[0]\n        for chunk in results[1:]:\n            # If a chunk has fewer tokens than the minimum size, merge it.\n            if chunk[\"tokens\"] < min_chunk_size:\n                # Concatenate text with a space separator (you may adjust as needed)\n                current_chunk[\"content\"] = current_chunk[\"content\"].rstrip() + \" \" + chunk[\"content\"].lstrip()\n                # Update the token count (you could also re-encode if you need exact counts)\n                current_chunk[\"tokens\"] += chunk[\"tokens\"]\n            else:\n                merged_results.append(current_chunk)\n                current_chunk = chunk\n        # Append the last (merged) chunk.\n        merged_results.append(current_chunk)\n        results = merged_results\n\n    return results\n```\n\nEspecially in the future when AI's are even better at extracting there is no reason to be affraid to send a chunk which is a bit bigger. Now that I am looking at the function it might be better to just check the results final index the \"tokens\" and if it is less than 800 combine it with the one before that one.  ",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-04-15T13:32:46Z",
      "updated_at": "2025-04-21T04:12:44Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1379/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1379",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1379",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.554420",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Could you submit a PR to improve the chunking logic to prevent small trailing chunks? The preferred implementation should ensure the last chunk's size is at least half of the standard chunk size.",
          "created_at": "2025-04-16T17:56:20Z"
        },
        {
          "author": "danielaskdd",
          "body": "@LarFii could you please review the prompt optimization proposal and assess whether it should be incorporated into our standard prompts?",
          "created_at": "2025-04-16T17:57:46Z"
        },
        {
          "author": "danielaskdd",
          "body": "I have an idea of providing a more powerful chunker:\n\n- Identify the chapter structure of the document to avoid cross-chapter splits.\n- Provide metadata for each chunk, such as chapter, domain, and necessary contextual information.\n- Recognize table column headers and output RAG-friendly table chunk",
          "created_at": "2025-04-17T01:25:23Z"
        },
        {
          "author": "drahnreb",
          "body": "> I have an idea of providing a more powerful chunker:\n> \n> * Identify the chapter structure of the document to avoid cross-chapter splits.\n> * Provide metadata for each chunk, such as chapter, domain, and necessary contextual information.\n> * Recognize table column headers and output RAG-friendly t",
          "created_at": "2025-04-20T11:25:53Z"
        },
        {
          "author": "danielaskdd",
          "body": "@drahnreb can you implement a more powerful chunker and make chunking_func o be flexible Callable?",
          "created_at": "2025-04-21T04:12:43Z"
        }
      ]
    },
    {
      "issue_number": 1418,
      "title": "[Question]: Readme states `mix` mode would support image content through html tags",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIt [says](https://github.com/HKUDS/LightRAG/blob/733e307a8dd25c2b1f7fb03a70a6c4ed27f7e330/README-zh.md?plain=1#L157):\n`通过HTML img标签支持图像内容\n`\nI can see that `clean_str(input) is cleaning descriptions and html tags are only unescaped during **querying** time. That allows for _displaying_ of images in chunk descriptions of retrieved entities and relationships.\n\nSo in principle we could have multimodal chunks and **query** them e.g.:\n```markdown\nHere is some text before the image.\n\n<img src=\"data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAAAAUA\n    AAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO\n        9TXL0Y4OHwAAAABJRU5ErkJggg==\" alt=\"Red dot\" />\n\nAnd here is some text after the image.\n```\n\nQ1: Why is it restricted to `mix` mode in README?\nQ2: Why are we not **embedding** or **extracting entities** with **multimodality** to **retrieve** them?\n\nIf our `llm_model_func` is multimodal (which most are nowadays), you could extract base64 str (search for `<img src=\"data:[mime_type];base64,[data]`) per chunk and pass it to the `llm_model_func` as an additional vector embedding and entities and relationships by running separate runs for the images within `_process_single_content()`.\n\n**This would essentially build a powerful multimodal knowledge graph that allows for retrieval based on image content.**\n\nFor example:\n\n```python\ndef extract_images_from_soup(soup) -> list[tuple[str, bytes]]:\n    content_imgs = []\n    for image in soup:\n        mime_type = f\"image/{match.group(1).lower()}\"  # e.g. image/png\n        base64_data = match.group(\n            2\n        ).strip()  # Get data part and strip whitespace/newlines\n        content_imgs.append(mime_type, base64_data)\n    return content_imgs\n\n\ndef use_multimodal_llm_func(hint_prompt, images):\n    # for a multimodal llm\n    hint_prompt = [hint_prompt] + [\n        types.Part.from_bytes(data=image_bytes, mime_type=mime_type)\n        for image_bytes, mime_type in images\n    ]\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-001\",\n        content=content,\n    )\n    return response.text\n\n\nasync def _process_single_content(chunk_key_dp: tuple[str, TextChunkSchema]):\n    \"\"\"Process a single chunk\n    Args:\n        chunk_key_dp (tuple[str, TextChunkSchema]):\n            (\"chunk-xxxxxx\", {\"tokens\": int, \"content\": str, \"full_doc_id\": str, \"chunk_order_index\": int})\n    Returns:\n        tuple: (maybe_nodes, maybe_edges) containing extracted entities and relationships\n    \"\"\"\n    nonlocal processed_chunks\n    chunk_key = chunk_key_dp[0]\n    chunk_dp = chunk_key_dp[1]\n    content = chunk_dp[\"content\"]\n    # Get file path from chunk data or use default\n    file_path = chunk_dp.get(\"file_path\", \"unknown_source\")\n\n    hint_prompt = entity_extract_prompt.format(\n        **context_base, input_text=\"{input_text}\"\n    ).format(**context_base, input_text=content)\n\n    # Get initial extraction from text\n    final_result = await use_llm_func_with_cache(\n        hint_prompt,\n        use_llm_func,\n        llm_response_cache=llm_response_cache,\n        cache_type=\"extract\",\n    )\n    # Optionally get initial extraction from images\n    use_multimodal_llm_func: callable = global_config[\"multimodal_llm_model_func\"]\n    if use_multimodal_llm_func:\n        soup = BeautifulSoup(content, \"html.parser\")\n        img_tag = soup.find(\"img\")\n        img_src = img_tag.get(\"src\", \"\")\n        if img_tag and img_src:\n            match = re.match(\n                r\"data:image/(png|jpeg|jpg|webp);base64,(.*)\",\n                img_src,\n                re.IGNORECASE | re.DOTALL,\n            )\n            if match:\n                content_imgs = extract_images_from_soup(soup)\n\n                # could be commonalized with use_llm_func_with_cache\n                final_result_images = await use_multimodal_llm_func_with_cache(\n                    hint_prompt,\n                    images,\n                    use_multimodal_llm_func,\n                    llm_response_cache=llm_response_cache,\n                    cache_type=\"extract\",\n                )\n\n                # Process initial extraction with file path\n                maybe_nodes, maybe_edges = await _process_extraction_result(\n                    final_result_images, chunk_key, file_path\n                )\n\n                # history handling, gleaning and rest of code...\n\n```\n\n\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "drahnreb",
      "author_type": "User",
      "created_at": "2025-04-20T13:24:17Z",
      "updated_at": "2025-04-21T02:03:49Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1418/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1418",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1418",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.786938",
      "comments": []
    },
    {
      "issue_number": 1414,
      "title": "[Feature Request]: Add an optional reranking for querying",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n#### Until today:\nStandard retrieval methods (like vector similarity search) act as a first pass, retrieving an initial set of candidate documents.\n\n#### With new feature:\nReranking introduces a second stage where a model re-evaluates these candidates to produce a refined ordering based on deeper semantic relevance to the query.\n\n#### Suggested implementation:\nOffering reranking as an optional parameter within `QueryParam`, allowing users to consciously trade speed for higher relevance, for `naive` and `mix` mode.\n\n```\nclass QueryParam:\n    mode: Literal[\"local\", \"global\", \"hybrid\", \"naive\", \"mix\", \"bypass\"] = \"global\"\n    top_k: Optional[int] = None  # Keep existing top_k for initial retrieval\n    reranker_func: Optional[Callable] = None  # e.g. a custom reranker with model like 'bge-reranker-large' or 'cohere' and defined top_n, could be also mmr based etc.\n    #... other existing or potential parameters...\n```\n\nIf implemented, lightweight cross-encoders or efficient API-based solutions are preferable, integrated post-retrieval and pre-truncation. \n\n```\nasync def naive_query(\n    query: str,\n    chunks_vdb: BaseVectorStorage,\n    text_chunks_db: BaseKVStorage,\n    query_param: QueryParam,\n    global_config: dict[str, str],\n    hashing_kv: BaseKVStorage | None = None,\n    system_prompt: str | None = None,\n) -> str | AsyncIterator[str]:\n    \n    # existing implementation...\n    # Filter out invalid chunks\n    valid_chunks = [\n        chunk for chunk in chunks if chunk is not None and \"content\" in chunk\n    ]\n    \n    if not valid_chunks:\n        logger.warning(\"No valid chunks found after filtering\")\n        return PROMPTS[\"fail_response\"]\n\n    reranked_chunks = valid_chunks  # Default to valid chunks if no reranker\n\n    # reranking feature\n    if query_param.reranker_func is not None:\n         logger.debug(f\"Applying reranker function: {query_param.reranker_func}\")\n        try:\n            reranked_chunks = await query_param.reranker_func(chunks=valid_chunks, query=query)\n        except Exception as e:\n            logger.error(f\"Error during reranking: {e}\", exc_info=True)\n            reranked_chunks = valid_chunks  # Fallback to non-reranked chunks\n    \n    logger.debug(\n        f\"Reranked chunks from {valid_chunks} to {reranked_chunks}\"\n    )\n\n    # existing implementation...\n        \n```\n\nPotentially this could be done also for the graph-based (`local`, `global`, `hybrid`) modes by providing a final layer of query-specific relevance assessment.\n\n\nExample:\n```\nfrom typing import List, Any\nimport functools\nfrom lightrag.utils import cosine_similarity, EmbeddingFunc\n\ndef mmr_reranking(\n    query: str,\n    chunks: List[Any],\n    embedding_func: EmbeddingFunc,\n    lambda_param: float = 0.5,\n    top_n: int = 10\n) -> List[Any]:\n    if not chunks:\n        # no chunks to rerank\n        return []\n    if len(chunks) <= top_n:\n        # fewer or equal top_n elements (set higher top_k)\n        return chunks[:]\n    embedded_query = embedding_func.func(query)\n    embedded_chunks = embedding_func.func(chunks)\n    selected_indices = []\n    remaining_indices = list(range(len(chunks)))\n    while len(selected_indices) < top_n and remaining_indices:\n        mmr_scores = {}\n        query_similarities = {\n            idx: cosine_similarity(embedded_query, embedded_chunks[idx])\n            for idx in remaining_indices\n        }\n\n        # Find the highest query similarity for the first pick\n        if not selected_indices:\n            best_idx = max(query_similarities, key=query_similarities.get)\n            selected_indices.append(best_idx)\n            remaining_indices.remove(best_idx)\n            continue # Move to the next iteration\n\n        # Calculate MMR for remaining items\n        for idx in remaining_indices:\n            relevance_score = query_similarities[idx]\n\n            # Calculate max similarity to already selected items\n            diversity_penalty = 0.0\n            if selected_indices: # Should always be true here except first iteration\n                similarities_to_selected = [\n                    cosine_similarity(embedded_chunks[idx], embedded_chunks[sel_idx])\n                    for sel_idx in selected_indices\n                ]\n                diversity_penalty = max(similarities_to_selected) if similarities_to_selected else 0.0\n\n            mmr_score = lambda_param * relevance_score - (1 - lambda_param) * diversity_penalty\n            mmr_scores[idx] = mmr_score\n\n        if not mmr_scores: # Should not happen if remaining_indices is not empty\n            break\n\n        # Select the item with the highest MMR score\n        next_idx = max(mmr_scores, key=mmr_scores.get)\n        print(f\"Next pick: Index {next_idx} (MMR: {mmr_scores[next_idx]:.4f})\")\n        selected_indices.append(next_idx)\n        remaining_indices.remove(next_idx)\n    # Return the selected chunks in the order they were selected\n    reranked_chunks = [chunks[i] for i in selected_indices]\n    return reranked_chunks\n\nasync def async_mmr_wrapper(\n    query: str,\n    chunks: List[Any],\n    embedding_func: EmbeddingFunc,\n    lambda_param: float = 0.5,\n    top_n: int = 3\n) -> List[Any]:\n    return mmr_reranking(\n        query=query,\n        chunks=chunks,\n        embedding_func=embedding_func,\n        lambda_param=lambda_param,\n        top_n=top_n\n    )\n\nasync_reranker_func = functools.partial(\n    async_mmr_wrapper,\n    embedding_func=EmbeddingFunc(model_name=\"all-MiniLM-L6-v2-sim\"),\n    top_n=3\n)\n\nrag.query(\n    \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\", reranker_func=async_reranker_func)\n)\n```\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "drahnreb",
      "author_type": "User",
      "created_at": "2025-04-19T13:42:19Z",
      "updated_at": "2025-04-19T13:42:19Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1414/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1414",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1414",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.786964",
      "comments": []
    },
    {
      "issue_number": 1413,
      "title": "[Question]: Link an entity/relationship to multiple chunks using insert_custom_kg()",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIs/Would it be possible to link an entity or a relationship to multiple chunks when inserting a custom expert knowledge graph ?\n\n\n\n### Additional Context\n\n## Use Case\n\nI have several expert KG which I need to insert into LightRAG, there might be common entities or common relationships between these KG even if there were built using different documents. The case is, a same entity/relationship can be linked to different chunks of text from different sources.\n\n## What I tried\n\nI tried inserting two custom expert KG and going through the source code, I noticed the following :\n- When there is already an entity inside LightRAG, inserting another custom KG which contains the same entity would override previously related chunk (even if inserted source_id is UNKOWN, it will replace previously related chunk source_id)\n- There is no way to specify file_path parameter when inserting custom KG. Because I have two custom KG, it would be nice to know from which the information is retrieved instead of \"custom_kg\" for both.\n\nWould it be possible to link an entity/relationship to multiple chunks by concatenate chunk source_id or something like that ?\n\nThank you for your amazing work 👍 \n",
      "state": "open",
      "author": "Suna24",
      "author_type": "User",
      "created_at": "2025-04-19T10:34:14Z",
      "updated_at": "2025-04-19T10:35:38Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1413/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1413",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1413",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.786972",
      "comments": []
    },
    {
      "issue_number": 957,
      "title": "[Bug]: Properties of relationships not saved in graph when adding a document - Postgres",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n- Uploading a document in the postgres graph storage results with the column \"properties\" of DIRECTED table empty for all rows. \n- This causes a failure in the retrieval step for the global query mode, as no relationship's property is found in the database\n\n### Steps to reproduce\n\n- Upload a document in postgres storage (first document of the db/graph)\n- Launch query with mode: global\n- No relations found\n- Answer says that there was no relevant context\n\n### Expected Behavior\n\n- Populate the \"property\" column of the DIRECTED table with the properties of each relationship\n- Retrieve the relationships relevant to the query in the global query mode, creating the relevant context for the generation of the answer\n\n### LightRAG Config Used\n\n- LightRag version 1.2.1\n\n### Logs and screenshots\n\nINFO:Logger initialized for working directory: ./data\n**** Start Global Query ****\nINFO:Non-embedding cached missed(mode:global type:query)\nINFO:Non-embedding cached missed(mode:global type:keywords)\nINFO:Inserting 1 to llm_response_cache\nINFO:Query edges: Entità finanziarie, DORA, Regolamentazione finanziaria, top_k: 60, cosine: 0.2\nWARNING:No valid text chunks found\nINFO:Global query uses 0 entites, 0 relations, 0 chunks\nINFO:Inserting 1 to llm_response_cache\nNon ho informazioni specifiche riguardo al DORA e le entità finanziarie a cui si applica. Ti consiglio di consultare fonti ufficiali o documentazione specifica per ottenere dettagli accurati su questo argomento.\nGlobal Query Time: 8.701498031616211\n\n![Image](https://github.com/user-attachments/assets/972d917a-6b50-44ed-b142-b5cecd537a64)\n\n### Additional Information\n\n- LightRAG Version: 1.2.1\n- Operating System: Windows\n- Python Version: 3.11.9\n- Related Issues: NA\n",
      "state": "open",
      "author": "fedehann",
      "author_type": "User",
      "created_at": "2025-02-27T09:24:24Z",
      "updated_at": "2025-04-18T18:50:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/957/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/957",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/957",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.786980",
      "comments": [
        {
          "author": "imraninfuseai",
          "body": "have you configure age extension to save graph info ?",
          "created_at": "2025-03-01T19:43:48Z"
        },
        {
          "author": "fedehann",
          "body": "> have you configure age extension to save graph info ?\n\nYou mean installing the extension, creating the indexes and making sure that the objects are there? Yes.\n\nUPD: The properties are missing only wrt relationships, in nodes they are saved just fine to Postgres.",
          "created_at": "2025-03-04T17:03:58Z"
        },
        {
          "author": "Daggle24",
          "body": "Im having this same issue with a postgres image I built with the extensions pre-installed.\n\nbut when I use the shangor/postgres-for-rag:v1.0 image recommended in the docs, It works fine. I'm not sure what can be happening.",
          "created_at": "2025-03-20T20:53:46Z"
        },
        {
          "author": "j-kafer",
          "body": "Hello @fedehann, I have exactly the same problem you described.\n\nHave you managed to resolve it in any other way?",
          "created_at": "2025-03-31T19:31:21Z"
        },
        {
          "author": "acsangamnerkar",
          "body": "@fedehann I found the same issue in 1.3.3 with Azure postgres. Did you figure out the solution?",
          "created_at": "2025-04-18T18:49:40Z"
        }
      ]
    },
    {
      "issue_number": 1406,
      "title": "[Question]: how interact between neo4j and lightrag engine?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nFor my customized chunks of document and dataset, i want to load neo4j personally. during its works, i have a question. if i just utilize retrieval concept of lightrag engine, how interface existed neo4j graph store to lightrag request? \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "tteon",
      "author_type": "User",
      "created_at": "2025-04-18T08:41:21Z",
      "updated_at": "2025-04-18T11:44:48Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1406/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1406",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1406",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:40.994840",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG already support using Neo4j as graph storage.",
          "created_at": "2025-04-18T11:44:47Z"
        }
      ]
    },
    {
      "issue_number": 1397,
      "title": "[Bug]: ImportError: cannot import name 'EmbeddingFunc' from 'lightrag.utils'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHi all,\n\nI just tried to run the example file to use LightRAG in combination with Huggingface (lightrag_hf_demo.py).\nHowever, I stumble across the following error:\n\nImportError: cannot import name 'EmbeddingFunc' from 'lightrag.utils'\n\nThe following is my pip-list:\n```\nPackage            Version\n------------------ -----------\nattrs              25.3.0\nbackoff            2.2.1\ncertifi            2025.1.31\ncharset-normalizer 3.4.1\nidna               3.10\njinja2             3.1.6\njsonlines          4.0.0\nlightrag           0.1.0b6\nlightrag-hku       1.3.1\nmarkupsafe         3.0.2\nnest-asyncio       1.6.0\nnumpy              1.26.4\npandas             2.2.3\npython-dateutil    2.9.0.post0\npython-dotenv      1.1.0\npytz               2025.2\npyyaml             6.0.2\nregex              2024.11.6\nrequests           2.32.3\nsix                1.17.0\ntiktoken           0.7.0\ntqdm               4.67.1\ntzdata             2025.2\nurllib3            2.4.0\n```\n\nI also saw some other issues, but none of them mentioned how to fix this problem, therefore opening a new one.\nThanks for the help!\n\n### Steps to reproduce\n\nThis is the code used - copied from the github:\n\n```\nimport os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.hf import hf_model_complete, hf_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom transformers import AutoModel, AutoTokenizer\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\nimport asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=hf_model_complete,\n        llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n        embedding_func=EmbeddingFunc(\n            embedding_dim=384,\n            max_token_size=5000,\n            func=lambda texts: hf_embed(\n                texts,\n                tokenizer=AutoTokenizer.from_pretrained(\n                    \"sentence-transformers/all-MiniLM-L6-v2\"\n                ),\n                embed_model=AutoModel.from_pretrained(\n                    \"sentence-transformers/all-MiniLM-L6-v2\"\n                ),\n            ),\n        ),\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\ndef main():\n    rag = asyncio.run(initialize_rag())\n\n    with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n        rag.insert(f.read())\n\n    # Perform naive search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n\n    # Perform local search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\n\n    # Perform global search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n\n    # Perform hybrid search\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:",
      "state": "open",
      "author": "dirkcremers",
      "author_type": "User",
      "created_at": "2025-04-17T12:52:51Z",
      "updated_at": "2025-04-18T10:47:36Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1397/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1397",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1397",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:41.143770",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Your current directory is likely not set to the LightRAG examples folder.",
          "created_at": "2025-04-17T15:36:35Z"
        },
        {
          "author": "nunoandrecorreia",
          "body": "I have the exact same issue.\nI get the error for my own py file, but also when I try to execute one of the examples: \n\n(.venv) qwerty@LAPTOP-52029127H:~/PythonProjects/lightRAG/LightRAG/examples$ python lightrag_ollama_demo.py\nTraceback (most recent call last):\n  File \"/home/qwerty/PythonProjects/li",
          "created_at": "2025-04-17T16:08:23Z"
        },
        {
          "author": "danielaskdd",
          "body": "You should run the demo in the example folder. ie. cd to LightRAG/examples firstly",
          "created_at": "2025-04-17T16:11:32Z"
        },
        {
          "author": "emgiezet",
          "body": "I've encountered similar stuff. What helped me: Create proper `venv` setup and use `python 3.12`\n\nalign your requirements.txt with this:\n\n```\naiohttp\nconfigparser\nfuture\n\n# Basic modules\ngensim\n\n# Additional Packages for export Functionality\npandas>=2.0.0\n\n# Extra libraries are installed when needed",
          "created_at": "2025-04-18T10:23:53Z"
        },
        {
          "author": "nunoandrecorreia",
          "body": "That sorted the issue I was facing -  thanks! ",
          "created_at": "2025-04-18T10:47:34Z"
        }
      ]
    },
    {
      "issue_number": 1388,
      "title": "[Bug]: Graph Is Empty in the Knowledge Graph viewer while there are entities and relationships in rag_storage data.",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nThanks a lot for creating the great tool!\nI launched lightrag-server with the default data storage settings (ie NetworkX/NetworkXStorage for storing the graph data), and fed two files to lightrag. It digested the files and successfully extracted entities and relationships. I tested in Retrieval with a query and the response seemed OK. But When I tried to visualize the Knowledge Graph, there was no data at all to display (\"Graph Is Empty\"). Is there any way I can specify the path to the graph data for the viewer?   I ran LightRAG v1.3.2/0149 on Ubuntu 22.04. \n\n![Image](https://github.com/user-attachments/assets/4ab2506f-5251-4b17-82ac-10a4c51dbe7b)\n![Image](https://github.com/user-attachments/assets/055c8a9e-ed95-4377-be9f-4c0f23844504)\n![Image](https://github.com/user-attachments/assets/ae06c828-e824-45f6-b524-2be25e57f205)\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "PhilipNZ",
      "author_type": "User",
      "created_at": "2025-04-17T00:36:40Z",
      "updated_at": "2025-04-18T09:23:56Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1388/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1388",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1388",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:41.356880",
      "comments": [
        {
          "author": "choizhang",
          "body": "<img width=\"820\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b49581e5-b172-4309-9221-d9946c9bae17\" />\nYou can click this button to try it out",
          "created_at": "2025-04-18T07:35:43Z"
        },
        {
          "author": "Phil2025Code",
          "body": "Oh, it works! Thanks a lot!",
          "created_at": "2025-04-18T09:23:55Z"
        }
      ]
    },
    {
      "issue_number": 1190,
      "title": "[Question]: <title>Neo4j Excessive Graph Data Retreival calls. Is this needed?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI currently have a small knowledge graph. 297 Nodes and 321 Edges. \n\nI am currently testing my application under load but on my local machine im not even able to run 4 concurrent users and have low latency. So I decided to log functions with Logfire from Pydantic.\n\n![Image](https://github.com/user-attachments/assets/77d1fe4c-f039-480b-aa4c-d1f6f4855218)\n\n### Additional Context\nI get that this is using Hybrid Search but still the latency and the amount of calls is excessive or not? \n\nIs this the reason we should only call hybrid when the question of the user means it's necessary? \n\nOr is some other GraphDB solution way faster than Neo4j? \n\n_No response_",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-03-26T07:33:15Z",
      "updated_at": "2025-04-18T09:15:56Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1190/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1190",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1190",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:41.600629",
      "comments": [
        {
          "author": "tteon",
          "body": "awesome how you implement its monitoring tool? could you share your monitoring tool guidance? ",
          "created_at": "2025-04-18T08:45:15Z"
        },
        {
          "author": "frederikhendrix",
          "body": "Hello @tteon ,\n\n#1246 Here I explain it a bit more. I was using cProfile to see the number of functions being called. And Apache JMeter showed me how the calls improved when I set the MAX_CONNECTION_POOL_SIZE of Neo4j higher. \n\n\nThe monitoring tool you are seeing above is Logfire. Is a free tool (up",
          "created_at": "2025-04-18T09:14:55Z"
        }
      ]
    },
    {
      "issue_number": 1403,
      "title": "[Question]: <title> 404 cannot upload error",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi, I try to selfhost LightRag via Coolify application via dockerfile pull. Input all open ai verification information already with light RAG\n\nI get an error 404 trying to upload file to lightRag. what might be the issue? Any information would be appreciated \n\n![Image](https://github.com/user-attachments/assets/c2ffc10c-9997-4466-9008-9b94235a2741)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "thientai476",
      "author_type": "User",
      "created_at": "2025-04-18T06:47:45Z",
      "updated_at": "2025-04-18T08:38:14Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1403/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1403",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1403",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:41.908434",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You can not upload files to the server because there are files with the same file name in the server inputs folder already. You can just click `Scan` to restart the file indexing pipeline or click `Clear` to flush the server data and re-upload new files.",
          "created_at": "2025-04-18T07:39:32Z"
        },
        {
          "author": "thientai476",
          "body": "> You can not upload files to the server because there are files with the same file name in the server inputs folder already. You can just click `Scan` to restart the file indexing pipeline or click `Clear` to flush the server data and re-upload new files.\n\nThanks so much for your input. I try both ",
          "created_at": "2025-04-18T07:55:04Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please check the server console. If any errors occur, you can find the server logs there.",
          "created_at": "2025-04-18T08:38:12Z"
        }
      ]
    },
    {
      "issue_number": 1382,
      "title": "[Question]: Is there a way to set different LLMs for use at indexing vs at retrieval? Right now the only workaround I can think of is to change the LLM model after the indexing process is completed.",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n_No response_\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "hanafihaffidz",
      "author_type": "User",
      "created_at": "2025-04-16T03:00:20Z",
      "updated_at": "2025-04-18T08:28:58Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1382/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.3.8",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1382",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1382",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:42.230860",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Your suggestion is excellent. We're considering separating the LLMs for query and indexing operations. This approach would also prevent document indexing from occupying LLM resources, which currently causes unacceptably slow query response times.",
          "created_at": "2025-04-16T17:42:05Z"
        }
      ]
    },
    {
      "issue_number": 1394,
      "title": "[Feature Request]: Utilize prompts to control the summary text length during the node and edge merging stage.",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nUtilize prompts to control the summary text length during the node and edge merging stage. @LarFii \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-17T08:33:07Z",
      "updated_at": "2025-04-18T08:28:34Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1394/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": "v1.3.8",
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1394",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1394",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:42.424013",
      "comments": []
    },
    {
      "issue_number": 1380,
      "title": "[Feature Request]: Button/API Endpoint to Restart Document Processing",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nSometimes LightRAG hangs when processing documents.\nIf I restart the server there is no way to restart processing existing documents.\n\nAs a work around I've been send a period \".\" via the text API.\nThis tricks LightRAG into scanning the pending documents.\n \nIt would be nice to have a button in the UI and/or an API end point that can restart documents processing.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Caleb68864",
      "author_type": "User",
      "created_at": "2025-04-15T20:55:44Z",
      "updated_at": "2025-04-17T15:31:07Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1380/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1380",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1380",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:42.424033",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The web interface already includes a Scan feature that addresses this requirement.\n\n<img width=\"1278\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/59c9e914-1362-4870-a6dc-d5dbf2e30d13\" />",
          "created_at": "2025-04-16T17:47:48Z"
        },
        {
          "author": "Caleb68864",
          "body": "If I restart the light rag server and there are documents pending the scan button does not start processing existing documents.\nIf I add a new document and click the scan button it finds the new document and starts processing the pending documents as well.\n\nI've also run into other times where proce",
          "created_at": "2025-04-16T17:51:59Z"
        },
        {
          "author": "danielaskdd",
          "body": "I am unable to reproduce the error. However, I can restart the file processing job for all docs with failed or processing status.",
          "created_at": "2025-04-17T15:31:05Z"
        }
      ]
    },
    {
      "issue_number": 1386,
      "title": "[Bug]: <title>Failed to insert entities into PostgreSQL when the text include \"\\\"",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHere is the text need to rag:\n`\n1、修改classes\\resource\\spring\\applicationContext_redis.xml文件 把注释的配置放开\n2、修改classes\\resource\\properties\\application.properties 配置redis\n`\n\nhere is the error message:\n`\n2025-04-16 16:21:50,480 - lightrag - ERROR - PostgreSQL database, error:invalid escape sequence at or near \"\\s\"\nDETAIL:  Valid escape sequences are \\\", \\', \\/, \\\\, \\b, \\f, \\n, \\r, \\t, \\uXXXX, and \\UXXXXXXXX.\n2025-04-16 16:21:50,480 - lightrag - ERROR - Failed to extract entities and relationships\n2025-04-16 16:21:50,480 - lightrag - ERROR - Failed to process document doc-51f545f57a168b0e42e5b3bf4f209598: {'message': 'Error executing graph query: SELECT * FROM cypher(\\'chunk_entity_relation\\', $$\\n                     MATCH (n:base {entity_id: \"classes\\\\resource\\\\spring\\\\applicationContext_redis.xml\"})\\n                     RETURN n\\n                   $$) AS (n agtype)', 'wrapped': 'SELECT * FROM cypher(\\'chunk_entity_relation\\', $$\\n                     MATCH (n:base {entity_id: \"classes\\\\resource\\\\spring\\\\applicationContext_redis.xml\"})\\n                     RETURN n\\n                   $$) AS (n agtype)', 'detail': 'invalid escape sequence at or near \"\\\\s\"\\nDETAIL:  Valid escape sequences are \\\\\", \\\\\\', \\\\/, \\\\\\\\, \\\\b, \\\\f, \\\\n, \\\\r, \\\\t, \\\\uXXXX, and \\\\UXXXXXXXX.'}\n`\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "paranoiagu",
      "author_type": "User",
      "created_at": "2025-04-16T09:42:00Z",
      "updated_at": "2025-04-17T15:18:46Z",
      "closed_at": "2025-04-16T18:35:14Z",
      "labels": [
        "bug",
        "PostgreSQL"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1386/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1386",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1386",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:42.626847",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Thank you for reporting this bug. The issue has been fixed - please pull the latest code to verify the resolution.",
          "created_at": "2025-04-16T18:34:44Z"
        },
        {
          "author": "paranoiagu",
          "body": "Thanks. Buy there has another bug. that's the error message:\n\n`\nERROR: PostgreSQL database, error:syntax error at or near \"1.8\"\nERROR: Failed to extract entities and relationships\nERROR: Failed to process document doc-3afcd9e711f0559f6007d7d8816f07b9: {'message': 'Error executing graph query: SELECT",
          "created_at": "2025-04-17T02:36:57Z"
        },
        {
          "author": "danielaskdd",
          "body": "The issue has been resolved. Please pull the latest version and verify at your convenience. Thank you for your efforts.",
          "created_at": "2025-04-17T15:18:45Z"
        }
      ]
    },
    {
      "issue_number": 1391,
      "title": "[Feature Request]: Add merging progress logs",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nAdd merging progress logs. In fact, we don't need to check in real time from the logs which entities/relationships have been merged. Instead, we are more concerned about the merge progress. The merged entities/relationships are more conveniently viewed through the database rather than in the log files.\nFor example, we need:\n```\nMerge N: {success_num} of {total_num}\nMerge E: {success_num} of {total_num}\n```\nrather than:\n```\nMerge N: {entity_name} | {num_new_fragment}+{num_fragment-num_new_fragment}\nMerge E: {src_id} - {tgt_id} | {num_new_fragment}+{num_fragment-num_new_fragment}\n```\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhouzhou12",
      "author_type": "User",
      "created_at": "2025-04-17T06:54:30Z",
      "updated_at": "2025-04-17T06:54:30Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1391/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1391",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1391",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:42.819686",
      "comments": []
    },
    {
      "issue_number": 1378,
      "title": "[Bug]: (rag.query_with_separate_keyword_extraction)When I use word embeddings provided by SiliconFlow, there's a retrieval failure phenomenon, but this doesn't occur with the OpenAI interface.",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I use the rag.query() function, it returns normally. However, when I use rag.query_with_separate_keyword_extraction(), a retrieval failure occurs.\n\n### Steps to reproduce\n\n```\nfrom lightrag.llm.openai import openai_complete_if_cache,openai_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.llm.siliconcloud import siliconcloud_embedding\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.hf import hf_embed\nfrom lightrag.utils import EmbeddingFunc\n\n#DeepSeek的API\nLLM_MODEL = os.environ.get(\"LLM_MODEL\", \"deepseek-reasoner\")\nBASE_URL = os.environ.get(\"BASE_URL\", \"https://api.deepseek.com/v1\")\nAPI_KEY = os.environ.get(\"API_KEY\", \"sk-123456789\")\n\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    print(texts)\n    return await siliconcloud_embedding(\n        texts=texts,\n        model='BAAI/bge-m3',\n        base_url='https://api.siliconflow.cn/v1/embeddings',\n        api_key='sk-123456789',\n    )\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    print(embedding)\n    embedding_dim = embedding.shape[1]\n    print(f\"{embedding_dim=}\")\n    return embedding_dim    \n\n# LLM model function\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        model=LLM_MODEL,\n        prompt=prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        base_url=BASE_URL,\n        api_key=API_KEY,\n        **kwargs,\n    )\n \nworking_dir='/home/amax/qcjySONG/newsql/Spider2_0406/spider2-snow/resource/databases/GITHUB_REPOS/GITHUB_REPOS_emb'\n\npro=“my Pro”\n\nrag = LightRAG(\n    working_dir=working_dir,\n    llm_model_func=llm_model_func,\n    llm_model_max_token_size=65536,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=embedding_func\n    ),\n    enable_llm_cache=False\n)\n\n# temp=rag.query(\n#         pro,\n#         param=QueryParam(mode='global',only_need_context=True)\n#     )\n\n# print(temp)\n\n#\"local\",\"global\",\"global\",\"hybrid\"\n\ntemp=rag.query_with_separate_keyword_extraction(\n        query= \"How ……,\n        prompt=pro,\n        param=QueryParam(mode=\"local\",only_need_context=True)\n    )\nprint(temp)\n```\n\nreturn:\n`INFO: Process 7831 Shared-Data created for Single Process\nINFO:nano-vectordb:Load (37, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/amax/qcjySONG/newsql/Spider2_0406/spider2-snow/resource/databases/GITHUB_REPOS/GITHUB_REPOS_emb/vdb_entities.json'} 37 data\nINFO:nano-vectordb:Load (42, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/amax/qcjySONG/newsql/Spider2_0406/spider2-snow/resource/databases/GITHUB_REPOS/GITHUB_REPOS_emb/vdb_relationships.json'} 42 data\nINFO:nano-vectordb:Load (43, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/amax/qcjySONG/newsql/Spider2_0406/spider2-snow/resource/databases/GITHUB_REPOS/GITHUB_REPOS_emb/vdb_chunks.json'} 43 data\nINFO: Process 7831 initialized updated flags for namespace: [full_docs]\nINFO: Process 7831 ready to initialize storage namespace: [full_docs]\nINFO: Process 7831 initialized updated flags for namespace: [text_chunks]\nINFO: Process 7831 ready to initialize storage namespace: [text_chunks]\nINFO: Process 7831 initialized updated flags for namespace: [entities]\nINFO: Process 7831 initialized updated flags for namespace: [relationships]\nINFO: Process 7831 initialized updated flags for namespace: [chunks]\nINFO: Process 7831 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 7831 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 7831 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 7831 initialized updated flags for namespace: [doc_status]\nINFO: Process 7831 ready to initialize storage namespace: [doc_status]\nNo keywords found in query_param. Could default to global mode or fail.\nSorry, I'm not able to provide an answer to that question.[no-context]`\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\nLightRAG 1.3.1\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:1.3.1\n- Operating System:\n- Python Version:3.11\n- Related Issues:\n",
      "state": "closed",
      "author": "qcjySONG",
      "author_type": "User",
      "created_at": "2025-04-15T13:22:37Z",
      "updated_at": "2025-04-17T02:12:32Z",
      "closed_at": "2025-04-17T02:12:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1378/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1378",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1378",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:42.819703",
      "comments": [
        {
          "author": "qcjySONG",
          "body": "I'm truly sorry. The report still contains errors. The version I used previously worked perfectly fine. After the update to the current 1.3.1 version, even the em which is similar to the OpenAI interface can't use this function anymore (but it's not the ChatGPT model), specifically the rag.query_wit",
          "created_at": "2025-04-15T13:48:12Z"
        }
      ]
    },
    {
      "issue_number": 1370,
      "title": "[Feature Request]: reprocess failed files",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nMy ollama server drops sometimes leasding to all files to fail. I want to be able to have a button in front of each file so that I can retry. Maybe have a select all option too to do this.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-04-14T12:31:38Z",
      "updated_at": "2025-04-16T19:04:32Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1370/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1370",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1370",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:42.992202",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Clicking the Scan button in the Web UI will initiate reindexing of all failed jobs.",
          "created_at": "2025-04-16T19:04:32Z"
        }
      ]
    },
    {
      "issue_number": 1371,
      "title": "[Question]: <title>inserting with \"file_paths\" works, but at retrieval I get \"Error in get_vector_context: 'file_path'\"",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHave seen there is now the possibility to add a \"file_paths\" keyword to calls to \"insert\". Great. That allows to add information about the source and return that with an answer, to know from where the information came from, good.\n\nI now do my inserts with the document name (its file path information) in a \"file_paths\" keyword): \n       `rag.insert(text, file_paths=doc)`\nThis works without error. \n\nBut currently I get at retrieval time the error message \n`Error in get_vector_context: 'file_path'`\n\nI have checked and the chunks now do contain a \"file_path\" information in the json and also the keyword in insert is really \"file_paths\" (not \"file_path\") in lightrag.py. But perhaps somewhere else there is a mismatch (file_path <-> file_paths)...? \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zwurgl",
      "author_type": "User",
      "created_at": "2025-04-14T16:01:50Z",
      "updated_at": "2025-04-16T19:02:49Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1371/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1371",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1371",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:43.189642",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Pls pdate to version 1.3.1 or later, clear all your old data, and try again.",
          "created_at": "2025-04-16T19:02:48Z"
        }
      ]
    },
    {
      "issue_number": 1368,
      "title": "[Bug]: API Error During Entity Extraction Phase",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n## Problem Description\nWhen running LightRAG, document chunking completes successfully, but the entity extraction phase fails with a 401 authentication error. Interestingly, later query operations succeed in calling the API correctly.\n\n## Detailed Observation\n1. **Document Chunking Success**: The document is successfully chunked into 831 segments as shown in the log:\n   ```\n   INFO: Process 3982309 KV load text_chunks with 831 records\n   ```\n\n2. **Partial Entity Extraction**: The system begins entity extraction and successfully processes the first few chunks:\n   ```\n   INFO: Chk 1/831: extracted 1 Ent + 0 Rel\n   INFO: Chk 2/831: extracted 1 Ent + 0 Rel\n   INFO: Chk 3/831: extracted 2 Ent + 1 Rel\n   ```\n\n3. **Authentication Error**: After processing only 3 chunks, entity extraction fails with an API authentication error:\n   ```\n   ERROR: Failed to process document doc-addb4618e1697da0445ec72a648e1f92: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-************************************************************************************************************************lLwA...\n   ```\nHowever, this key is not what I had set in my `.env`.\n\n4. **Query Operations Succeed**: Despite the entity extraction failure, the subsequent query operations are complete. I am sure the API calling with my `.env` configuration is correct:\n   ```\n    🔍 Query: 'What are the main characters in the story?' (Mode: local)\n    INFO: Process 3982309 buidling query context...\n    INFO: Query nodes: Protagonist, Antagonist, Supporting characters, Character development, Plot, top_k: 60, cosine: 0.2\n    Execution time: 1.44 seconds\n    📊 Response length: 70 characters\n    📄 Response preview: Sorry, I'm not able to provide an answer to that question.[no-context]...\n    \n    🔍 Query: 'What happened during the Christmas celebration?' (Mode: global)\n    INFO: Process 3982309 buidling query context...\n    INFO: Query edges: Christmas celebration, Cultural practices, Festive events, top_k: 60, cosine: 0.2\n    Execution time: 3.82 seconds\n    📊 Response length: 70 characters\n    📄 Response preview: Sorry, I'm not able to provide an answer to that question.[no-context]...\n    \n    🔍 Query: 'How did Scrooge's character change throughout the story?' (Mode: mix)\n    INFO: Process 3982309 buidling query context...\n    INFO: Query nodes: A Christmas Carol, Redemption, Greed, Kindness, Interactions with Marley, Tiny Tim, top_k: 60, cosine: 0.2\n    INFO: Query edges: Character development, Literary analysis, Scrooge's transformation, top_k: 60, cosine: 0.2\n    Execution time: 4.03 seconds\n    📊 Response length: 70 characters\n    📄 Response preview: Sorry, I'm not able to provide an answer to that question.[no-context]...\n   ```\n\n\n\n### Steps to reproduce\n\n## Download data\n```sh\ncurl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt > lightrag_examples/sample_book.txt\n```\n\n## Script\n`basic_setup`\n```py\nimport os\nimport asyncio\nimport time\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import openai_embed, gpt_4o_mini_complete\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger\nfrom dotenv import load_dotenv\n\n# Environment setup\nload_dotenv()\n\n# Configure logging\nsetup_logger(\"lightrag\", level=\"INFO\")\n\nasync def initialize_rag(config=None):\n    \"\"\"Initialize a LightRAG instance with configurable parameters\"\"\"\n    \n    # Default configuration\n    default_config = {\n        \"working_dir\": \"./lightrag_cache\",\n        \"embedding_func\": openai_embed,\n        \"llm_model_func\": gpt_4o_mini_complete,\n        \"kv_storage\": \"JsonKVStorage\",\n        \"vector_storage\": \"NanoVectorDBStorage\",\n        \"graph_storage\": \"NetworkXStorage\",\n        \"chunk_token_size\": 1200,\n        \"chunk_overlap_token_size\": 100,\n        \"embedding_batch_num\": 32,\n        \"embedding_func_max_async\": 16,\n        \"llm_model_max_async\": 4,\n        \"max_parallel_insert\": 2,\n        \"entity_extract_max_gleaning\": 1,\n    }\n    \n    # Override defaults with provided config\n    if config:\n        default_config.update(config)\n    \n    # Create LightRAG instance\n    rag = LightRAG(\n        working_dir=default_config[\"working_dir\"],\n        embedding_func=default_config[\"embedding_func\"],\n        llm_model_func=default_config[\"llm_model_func\"],\n        kv_storage=default_config[\"kv_storage\"],\n        vector_storage=default_config[\"vector_storage\"],\n        graph_storage=default_config[\"graph_storage\"],\n        chunk_token_size=default_config[\"chunk_token_size\"],\n        chunk_overlap_token_size=default_config[\"chunk_overlap_token_size\"],\n        embedding_batch_num=default_config[\"embedding_batch_num\"],\n        embedding_func_max_async=default_config[\"embedding_func_max_async\"],\n        llm_model_max_async=default_config[\"llm_model_max_async\"],\n        max_parallel_insert=default_config[\"max_parallel_insert\"],\n        entity_extract_max_gleaning=default_config[\"entity_extract_max_gleaning\"],\n    )\n    \n    # Initialize storages and pipeline status\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n    \n    return rag\n\nasync def measure_performance(func, *args, **kwargs):\n    \"\"\"Measure execution time of a given function\"\"\"\n    start_time = time.time()\n    result = await func(*args, **kwargs)\n    end_time = time.time()\n    print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n    return result\n\nasync def load_document(rag, file_path):\n    \"\"\"Load a document from a file\"\"\"\n    with open(file_path, 'r') as f:\n        content = f.read()\n    return content\n\nasync def insert_document(rag, document, split_by_character=None, split_by_character_only=False):\n    \"\"\"Insert a document into LightRAG\"\"\"\n    await rag.ainsert(document, split_by_character, split_by_character_only)\n    print(f\"Document inserted. Length: {len(document)} characters\")\n\nasync def query_document(rag, query, mode=\"global\", top_k=60):\n    \"\"\"Query the document with the specified mode\"\"\"\n    param = QueryParam(mode=mode, top_k=top_k)\n    response = await rag.aquery(query, param=param)\n    return response\n\ndef run_async(coroutine):\n    \"\"\"Run an async function\"\"\"\n    return asyncio.run(coroutine)\n```\n\n\n`main.py`\n\n```py\nimport os\nimport asyncio\nfrom lightrag_examples.basic_setup import initialize_rag, measure_performance, load_document, insert_document, query_document\n\nasync def run_embedding_optimized():\n    \"\"\"Run LightRAG optimized for embedding bottlenecks\"\"\"\n    print(\"⚙️ Running LightRAG with embedding optimization\")\n    \n    # Configure for embedding bottlenecks\n    config = {\n        \"working_dir\": \"./lightrag_cache_embedding_opt\",\n        \"embedding_batch_num\": 64,                  # Increased batch size for embeddings\n        \"embedding_func_max_async\": 32,             # More concurrent embedding operations\n        \"chunk_token_size\": 2000,                   # Larger chunks to reduce total embeddings\n        \"chunk_overlap_token_size\": 200,            # Increased overlap for better context preservation\n        # Enable embedding cache to avoid redundant computations\n        \"embedding_cache_config\": {\n            \"enabled\": True,\n            \"similarity_threshold\": 0.92,\n            \"use_llm_check\": False,\n        }\n    }\n    \n    # Initialize RAG with optimized config\n    rag = await initialize_rag(config)\n    \n    # Load document\n    document = await load_document(rag, \"lightrag_examples/sample_book.txt\")\n    \n    # Insert document with performance measurement\n    print(\"📥 Inserting document with embedding optimization...\")\n    await measure_performance(insert_document, rag, document, \"\\n\\n\", False)\n    \n    # Query with various modes\n    print(\"\\n📝 Testing queries with different modes:\")\n    \n    queries = [\n        (\"What are the main characters in the story?\", \"local\"),\n        (\"What happened during the Christmas celebration?\", \"global\"),\n        (\"How did Scrooge's character change throughout the story?\", \"mix\"),\n    ]\n    \n    for query, mode in queries:\n        print(f\"\\n🔍 Query: '{query}' (Mode: {mode})\")\n        response = await measure_performance(query_document, rag, query, mode)\n        print(f\"📊 Response length: {len(str(response))} characters\")\n        print(f\"📄 Response preview: {str(response)[:200]}...\")\n    \n    print(\"\\n✅ Embedding-optimized example completed\")\n```\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\nINFO: Process 3982309 Shared-Data created for Single Process\nINFO: Loaded graph from ./lightrag_cache_embedding_opt/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\nINFO:nano-vectordb:Load (0, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './lightrag_cache_embedding_opt/vdb_entities.json'} 0 data\nINFO:nano-vectordb:Load (0, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './lightrag_cache_embedding_opt/vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Load (0, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': './lightrag_cache_embedding_opt/vdb_chunks.json'} 0 data\nINFO: Process 3982309 initialized updated flags for namespace: [full_docs]\nINFO: Process 3982309 ready to initialize storage namespace: [full_docs]\nINFO: Process 3982309 KV load full_docs with 1 records\nINFO: Process 3982309 initialized updated flags for namespace: [text_chunks]\nINFO: Process 3982309 ready to initialize storage namespace: [text_chunks]\nINFO: Process 3982309 KV load text_chunks with 831 records\nINFO: Process 3982309 initialized updated flags for namespace: [entities]\nINFO: Process 3982309 initialized updated flags for namespace: [relationships]\nINFO: Process 3982309 initialized updated flags for namespace: [chunks]\nINFO: Process 3982309 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 3982309 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 3982309 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 3982309 KV load llm_response_cache with 12 records\nINFO: Process 3982309 initialized updated flags for namespace: [doc_status]\nINFO: Process 3982309 ready to initialize storage namespace: [doc_status]\nINFO: Process 3982309 doc status load doc_status with 1 records\nINFO: Process 3982309 storage namespace already initialized: [full_docs]\nINFO: Process 3982309 storage namespace already initialized: [text_chunks]\nINFO: Process 3982309 storage namespace already initialized: [llm_response_cache]\nINFO: Process 3982309 storage namespace already initialized: [doc_status]\nINFO: Process 3982309 Pipeline namespace initialized\n📥 Inserting document with embedding optimization...\nINFO: No new unique documents were found.\nINFO: Storage Initialization completed!\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processing batch 1 of 1.\nINFO: Processing file: unknown_source\nINFO: Processing d-id: doc-addb4618e1697da0445ec72a648e1f92\nINFO: Process 3982309 doc status writting 1 records to doc_status\nINFO: Chk 1/831: extracted 1 Ent + 0 Rel\nINFO: Chk 2/831: extracted 1 Ent + 0 Rel\nINFO: Chk 3/831: extracted 2 Ent + 1 Rel\nERROR: Failed to process document doc-addb4618e1697da0445ec72a648e1f92: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-************************************************************************************************************************lLwA. You can find your API key at https://platform.openai.com/account/api-keys. (request id: 2025041417484467030518430jtGrk6) (request id: 202504141748446011421233WddDLyx)', 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_api_key'}}\nINFO: Process 3982309 doc status writting 1 records to doc_status\nINFO: Process 3982309 KV writting 1 records to full_docs\nINFO: Process 3982309 KV writting 831 records to text_chunks\nINFO: Writing graph with 0 nodes, 0 edges\nINFO: In memory DB persist to disk\nINFO: Completed batch 1 of 1.\nINFO: Document processing pipeline completed\nDocument inserted. Length: 185067 characters\nExecution time: 4.03 seconds\n\n📝 Testing queries with different modes:\n\n🔍 Query: 'What are the main characters in the story?' (Mode: local)\nINFO: Process 3982309 buidling query context...\nINFO: Query nodes: Protagonist, Antagonist, Supporting characters, Character development, Plot, top_k: 60, cosine: 0.2\nExecution time: 1.44 seconds\n📊 Response length: 70 characters\n📄 Response preview: Sorry, I'm not able to provide an answer to that question.[no-context]...\n\n🔍 Query: 'What happened during the Christmas celebration?' (Mode: global)\nINFO: Process 3982309 buidling query context...\nINFO: Query edges: Christmas celebration, Cultural practices, Festive events, top_k: 60, cosine: 0.2\nExecution time: 3.82 seconds\n📊 Response length: 70 characters\n📄 Response preview: Sorry, I'm not able to provide an answer to that question.[no-context]...\n\n🔍 Query: 'How did Scrooge's character change throughout the story?' (Mode: mix)\nINFO: Process 3982309 buidling query context...\nINFO: Query nodes: A Christmas Carol, Redemption, Greed, Kindness, Interactions with Marley, Tiny Tim, top_k: 60, cosine: 0.2\nINFO: Query edges: Character development, Literary analysis, Scrooge's transformation, top_k: 60, cosine: 0.2\nExecution time: 4.03 seconds\n📊 Response length: 70 characters\n📄 Response preview: Sorry, I'm not able to provide an answer to that question.[no-context]...\n\n✅ Embedding-optimized example completed\nINFO: Creating a new event loop in main thread.\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "SHAO-Jiaqi757",
      "author_type": "User",
      "created_at": "2025-04-14T10:03:02Z",
      "updated_at": "2025-04-16T18:59:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1368/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1368",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1368",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:43.360803",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The OS environment variables take precedence over the .env file. Please launch a new terminal session for the updated .env file changes to take effect.",
          "created_at": "2025-04-16T18:59:50Z"
        }
      ]
    },
    {
      "issue_number": 1376,
      "title": "[Question]: I need to increase maximum number of graph nodes, how to do? (setting MAX_GRAPH_NODES doesn't work)",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi, I've created a graph that has 2623 nodes, when trying to visualize it in the web UI, I got the following image\n\n![Image](https://github.com/user-attachments/assets/c367aa7f-b5e5-47eb-8ffb-810fe64dbb41)\n\nApparently, the maximum number of nodes is limited to 1000. I searched, and it seems like I should set `MAX_GRAPH_NODES` to lift this constraint. \n\nI changed it to 5000 (made sure `echo $MAX_GRAPH_NODES` is 5000, and restarted the lightrag server, and it still wouldn't visualize the knowledge graph\n```\nINFO: Graph truncated: 2623 nodes found, limited to 1000\nINFO: Subgraph query successful | Node count: 1000 | Edge count: 2359\nINFO: 127.0.0.1:42076 - \"GET /graphs?label=*&max_depth=3&max_nodes=1000 HTTP/1.1\" 200\n```\n\nIt seems that the endpoint enforces this 1000 max nodes and I don't have a way to change it. Can you give me some pointers?\n\nUpdate: I gradually reduce t he max nodes setting to 910 and then the plotting works.\n\n### Additional Context\n\nI've even changed the endpoint:\n\n![Image](https://github.com/user-attachments/assets/de1ccf5e-2715-4fe3-aec2-d9afedec812e)\n\nLooking at the response of the endpoint, it seems that the response is correcet, however, the UI cannot draw it.\n\n![Image](https://github.com/user-attachments/assets/6ea76fa3-0575-44c4-a566-40b1a1a8abdb)",
      "state": "open",
      "author": "duguyue100",
      "author_type": "User",
      "created_at": "2025-04-15T08:16:31Z",
      "updated_at": "2025-04-16T17:31:19Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1376/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1376",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1376",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:43.536478",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The frontend has its own maximum node limit. Currently, MAX_GRAPH_NODES only controls the default node cap for graph queries when the frontend does not explicitly specify a limit.",
          "created_at": "2025-04-16T17:19:26Z"
        },
        {
          "author": "danielaskdd",
          "body": "There appears to be a bug where MAX_GRAPH_NODES only applies when configured as an OS environment variable, but not when set in the .env file. This issue has been resolved in the main branch.",
          "created_at": "2025-04-16T17:31:18Z"
        }
      ]
    },
    {
      "issue_number": 1297,
      "title": "[Bug]: \"List index out of range\" when a stream answer is displayed",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nAfter getting the knowledge graph by uploading the document, I tried to do a Q&A test and found that everything was fine with just the only need context or prompt, while the \"stream response\" mode would only show \"Error in stream response: list index out of range\" no matter how much it was asked\n\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n The LLM model used is GPT-4O-2024-08-06\n\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/cc528cbd-4d15-470c-816d-dc9a8e0d3f02)\n\n### Additional Information\n\n- LightRAG Version:1.3.1\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "ujnxs123",
      "author_type": "User",
      "created_at": "2025-04-07T08:55:10Z",
      "updated_at": "2025-04-16T16:26:41Z",
      "closed_at": "2025-04-16T16:26:39Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1297/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1297",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1297",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:43.722695",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The error occurs when you  do query with both stream and only_need_context enable, is that the case?",
          "created_at": "2025-04-07T15:29:52Z"
        },
        {
          "author": "ujnxs123",
          "body": "> The error occurs when you do query with both stream and only_need_context enable, is that the case?\nNOI，exactly ，As soon as the stream response option is enabled, an error will be reported, regardless of whether other options are turned on.\n",
          "created_at": "2025-04-08T03:28:47Z"
        },
        {
          "author": "danielaskdd",
          "body": "Thanks. This issue will be fixed if we can reproduce it.",
          "created_at": "2025-04-08T11:02:03Z"
        },
        {
          "author": "danielaskdd",
          "body": "Fixex in main branch",
          "created_at": "2025-04-16T16:26:39Z"
        }
      ]
    },
    {
      "issue_number": 1363,
      "title": "[Question]: Vector database upserts redundant / inefficient?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am working to apply LightRAG to a large scale dataset and it occurs to me that the entity and relationship vector database incremental upserts may be redundant and inefficient. It does not appear that LightRAG uses these storages during ingestion, and that there will be many unnecessary upserts. Would it make sense to add a new task post-graph construction that does a bulk insertion of all entities and relationships into vector tables to solve this?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "alineberry",
      "author_type": "User",
      "created_at": "2025-04-12T18:10:51Z",
      "updated_at": "2025-04-16T10:15:30Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 21,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1363/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1363",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1363",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:43.902318",
      "comments": []
    },
    {
      "issue_number": 1344,
      "title": "[Bug]: <ModuleNotFoundError: No module named 'exceptions'>",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nAn error occurred while uploading the docx document. The parsing engine for docx files has poor compatibility.\n\n### Steps to reproduce\n\nuploading the docx document\n\n### Expected Behavior\n\nThe docx document was parsed successfully.\n\n### LightRAG Config Used\n\n# Paste your config here\ndefault config\n\n### Logs and screenshots\n\n```\nlightrag-1  | INFO: 118.114.232.67:58182 - \"POST /documents/upload HTTP/1.1\" 200\nlightrag-1  | Collecting docx\nlightrag-1  |   Downloading docx-0.2.4.tar.gz (54 kB)\nlightrag-1  |      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.9/54.9 kB 490.8 kB/s eta 0:00:00\nlightrag-1  |   Preparing metadata (setup.py): started\nlightrag-1  |   Preparing metadata (setup.py): finished with status 'done'\nlightrag-1  | Collecting lxml (from docx)\nlightrag-1  |   Downloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\nlightrag-1  | Requirement already satisfied: Pillow>=2.0 in /root/.local/lib/python3.11/site-packages (from docx) (11.1.0)\nlightrag-1  | Downloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\nlightrag-1  |    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 8.5 MB/s eta 0:00:00\nlightrag-1  | Building wheels for collected packages: docx\nlightrag-1  |   Building wheel for docx (setup.py): started\nlightrag-1  |   Building wheel for docx (setup.py): finished with status 'done'\nlightrag-1  |   Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53892 sha256=c15ae6a7cded02d9bb813c9778fff9df106656b5e4700d60f40913845b500550\nlightrag-1  |   Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\nlightrag-1  | Successfully built docx\nlightrag-1  | Installing collected packages: lxml, docx\nlightrag-1  | Successfully installed docx-0.2.4 lxml-5.3.2\nlightrag-1  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nlightrag-1  | \nlightrag-1  | [notice] A new release of pip is available: 24.0 -> 25.0.1\nlightrag-1  | [notice] To update, run: pip install --upgrade pip\nlightrag-1  | ERROR: Error processing or enqueueing file demo.docx: No module named 'exceptions'\nlightrag-1  | ERROR: Traceback (most recent call last):\nlightrag-1  |   File \"/app/lightrag/api/routers/document_routes.py\", line 503, in pipeline_enqueue_file\nlightrag-1  |     from docx import Document  # type: ignore\nlightrag-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^\nlightrag-1  |   File \"/usr/local/lib/python3.11/site-packages/docx.py\", line 30, in <module>\nlightrag-1  |     from exceptions import PendingDeprecationWarning\nlightrag-1  | ModuleNotFoundError: No module named 'exceptions'\n```\n\n### Additional Information\n\n- LightRAG Version: v1.3.1\ndocker\n\n",
      "state": "open",
      "author": "zhudongwork",
      "author_type": "User",
      "created_at": "2025-04-11T02:57:58Z",
      "updated_at": "2025-04-16T08:57:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1344/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1344",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1344",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:43.902343",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "There appears to be an issue with the Python environment. Kindly proceed with a manual installation of the docx module and address any errors that may occur during the installation process.",
          "created_at": "2025-04-11T04:45:37Z"
        }
      ]
    },
    {
      "issue_number": 973,
      "title": "[Bug]: <title>This Neo4j instance does not support creating databases",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nThis Neo4j instance does not support creating databases\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "YDS854394028",
      "author_type": "User",
      "created_at": "2025-03-02T07:28:09Z",
      "updated_at": "2025-04-16T07:45:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/973/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/973",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/973",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:44.122903",
      "comments": [
        {
          "author": "yaleimeng",
          "body": "Neo4j社区版不能创建新的数据库，只能有一个默认的数据库。",
          "created_at": "2025-03-03T05:29:40Z"
        },
        {
          "author": "ArindamRoy23",
          "body": "Facing the same ",
          "created_at": "2025-03-04T12:28:00Z"
        },
        {
          "author": "Jeremy4455",
          "body": "Facing the same, it leads to \"Sorry, I'm not able to provide an answer to that question.[no-context]\", because lightrag cannot retrieve the data in the neo4j and cannot ask the query",
          "created_at": "2025-03-12T13:35:09Z"
        },
        {
          "author": "frederikhendrix",
          "body": "Same but I was running into:\n\nThis Neo4j instance does not support creating databases. Try to use Neo4j Desktop/Enterprise version or DozerDB instead. Fallback to use the default database.\n\nWhen i have a neo4j container running and have tested it with other graphs and that works ",
          "created_at": "2025-03-12T15:46:38Z"
        },
        {
          "author": "shmily1012",
          "body": "As a workaround, you can put this part of code into yours. Then LightRAG will use the default database.\n\nos.environ[\"NEO4J_DATABASE\"] = \"neo4j\"\n",
          "created_at": "2025-03-29T07:14:08Z"
        }
      ]
    },
    {
      "issue_number": 1313,
      "title": "[Bug]: <title>文档处理失败，不能提取出实体和关系（Document processing failed, unable to extract entities and relationships）",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n文档处理失败，不能提取出实体和关系\n\n### Steps to reproduce\n\n多次尝试上传文档，但是文档处理失败\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n#千问\nLLM_BINDING=openai\nLLM_MODEL=qwen-max\nLLM_BINDING_HOST=https://dashscope.aliyuncs.com/compatible-mode/v1\n\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/99773457-713e-4199-b4ef-55f4cafc333d)\n![Image](https://github.com/user-attachments/assets/b3d18d35-c804-4286-838a-2949f6c038e0)\n\n### Additional Information\n\n- Operating System:win10\n- Python Version:3.11\n\n",
      "state": "closed",
      "author": "wenquxing1",
      "author_type": "User",
      "created_at": "2025-04-08T10:26:04Z",
      "updated_at": "2025-04-15T08:49:31Z",
      "closed_at": "2025-04-10T06:46:49Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1313/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1313",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1313",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:44.338866",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Please download the latest version of LightRAG.",
          "created_at": "2025-04-08T11:08:46Z"
        },
        {
          "author": "wenquxing1",
          "body": "I reinstalled the latest version of LightRAG, but it still doesn't fix the problem\n\n![Image](https://github.com/user-attachments/assets/d5def13f-d9bd-4a78-a29f-d103c91cee0c)",
          "created_at": "2025-04-09T03:22:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please install from Github source code. ",
          "created_at": "2025-04-09T03:24:31Z"
        },
        {
          "author": "danielaskdd",
          "body": "Install 1.3.1",
          "created_at": "2025-04-09T03:25:33Z"
        },
        {
          "author": "wenquxing1",
          "body": "I installed version 1.3.1, but it still doesn't fix the problem, my problem seems to be that I can only successfully process one document, and subsequent documents will fail\n\n![Image](https://github.com/user-attachments/assets/06717ce0-93df-4159-ab47-5cd7ae022605)\n![Image](https://github.com/user-at",
          "created_at": "2025-04-09T11:45:22Z"
        }
      ]
    },
    {
      "issue_number": 1121,
      "title": "[Question]: <title>请问现在更新了中文提示词吗，是否还有计划？",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n请问现在更新了中文提示词吗，是否还有计划？\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Bboyjie",
      "author_type": "User",
      "created_at": "2025-03-19T06:52:07Z",
      "updated_at": "2025-04-15T08:04:55Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1121/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1121",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1121",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:44.534133",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "系统的默认提示词还是会保留英文，但是抽取出来的内容给可以通过环境变量设置为是中文的。请参考项目根目录下的环境变量示例文件：env.example\n\n```\nSUMMARY_LANGUAGE=Chinese\n```\n> 使用的时候把它复制为 .env",
          "created_at": "2025-03-20T14:39:45Z"
        },
        {
          "author": "liuyuchen-cz",
          "body": "> 系统的默认提示词还是会保留英文，但是抽取出来的内容给可以通过环境变量设置为是中文的。请参考项目根目录下的环境变量示例文件：env.example\n> \n> ```\n> SUMMARY_LANGUAGE=Chinese\n> ```\n> \n> > 使用的时候把它复制为 .env\n\n@danielaskdd 这个修改应该并没有把提示词改为中文吧，实际体验之后感觉用英文提示词抽取的中文实体还是不太好，自己修改了一下变成中文提示词但是会有些bug，出现提取0实体0关系的情况，可能是多提示词之间联动的时候输出格式有误，不知官方是否会有计划上线一版中文的提示词？",
          "created_at": "2025-04-15T07:45:19Z"
        },
        {
          "author": "danielaskdd",
          "body": "很快就会支持定制化提示词了，到时候会提供官方的纯中文提示词。",
          "created_at": "2025-04-15T08:04:54Z"
        }
      ]
    },
    {
      "issue_number": 1246,
      "title": "[Feature Request]: Neo4j Batching With UNWIND on get_node, get_edge, node_degree, edge_degree, get_node_edges",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n### Overview\n\nThis feature request proposes a performance enhancement for LightRAG when using Neo4j as the GraphDB storage. Currently, individual queries are executed for functions like `get_edge` and `edge_degree`, resulting in thousands of Neo4j queries for a single user query. By batching these calls using the `UNWIND` clause, we can drastically reduce the number of queries sent to Neo4j, thereby improving performance and reducing connection overhead.\n\n---\n\n### Motivation\n\n- **Performance Issues:**  \n  Profiling with Logfire and cProfile revealed that for one question, Neo4j was being queried thousands of times due to non-batched function calls.\n\nOld cProfile results:\n\n![Image](https://github.com/user-attachments/assets/4f9fba92-bd2d-4271-bc35-3b49d3c30488)\n\nNew cProfile results (with all Neo4j batched):\n\n![Image](https://github.com/user-attachments/assets/be76b3cc-839b-4d3b-b7f2-a77738591317)\n\n  \n- **Connection Pool Limitations:**  \n  The current implementation relies on a connection pool (with a default size set via the environment variable `NEO4J_MAX_CONNECTION_POOL_SIZE`, fallback to 50). Increasing the pool size was previously used as a workaround for 30s timeouts of Neo4j. With batching, the number of queries—and hence the load on the connection pool—should be significantly reduced, mitigating this issue.\n\n- **Consistency Across Storage Systems:**  \n  Similar batching improvements have already been applied for Redis (using `MGET`), which greatly reduced the number of calls. Applying a similar strategy to Neo4j (and eventually to all graph databases) will ensure consistent performance improvements across the system.\n\n---\n\n### Proposed Changes\n\n1. **Batching Functions:**  \n   - Create batched versions for the following functions:\n     - `get_node`\n     - `get_edge`\n     - `node_degree`\n     - `edge_degree`\n     - `get_node_edges`\n   - For example, instead of calling:\n     ```python\n     # Old way:\n     edge_datas, edge_degrees = await asyncio.gather(\n         asyncio.gather(\n             *[knowledge_graph_inst.get_edge(r[\"src_id\"], r[\"tgt_id\"]) for r in results]\n         ),\n         asyncio.gather(\n             *[knowledge_graph_inst.edge_degree(r[\"src_id\"], r[\"tgt_id\"]) for r in results]\n         ),\n     )\n     ```\n     we propose:\n     ```python\n     # New batched way:\n     edge_pairs_dicts = [{\"src\": r[\"src_id\"], \"tgt\": r[\"tgt_id\"]} for r in results]\n     edge_pairs_tuples = [(r[\"src_id\"], r[\"tgt_id\"]) for r in results]\n\n     edge_data_dict, edge_degrees_dict = await asyncio.gather(\n         knowledge_graph_inst.get_edges_batch(edge_pairs_dicts),\n         knowledge_graph_inst.get_edges_degree_batch(edge_pairs_tuples)\n     )\n     ```\n   - The batched functions should use Neo4j’s `UNWIND` clause to process the entire list in a single query.\n\n2. **APOC Dependency:**  \n   - Ensure that the APOC file is downloaded and copied over to the Neo4j container’s data folder so that the necessary procedures are available.\n\n3. **Connection Pool Settings:**  \n   - Highlight that while `MAX_CONNECTION_POOL_SIZE` is configurable (default is 50), with this new batching mechanism the connection pool is less likely to be overwhelmed, reducing the need to set this value higher.\n\n4. **Extend to Other Graph DBs:**  \n   - Although this feature is primarily targeted at Neo4j, the same batching approach could be applied to any graph database to improve performance. (I would assume that most GraphDB applications have some type of batching)\n\n5. **Basic Neo4j Improvements:**\n   - If you check my Neo4j code thoroughly you can see that even the basic functions have been changed to hopefully improve the code when running into things like \"super-nodes\" in a knowledge graph. An example is using the node_degree like this:\n  ```python\n  # query = \"\"\"\n  #     MATCH (n:base {entity_id: $entity_id})\n  #     OPTIONAL MATCH (n)-[r]-()\n  #     RETURN COUNT(r) AS degree\n  # \"\"\"\n  # This should be an improved query for when traversing every single edge of a node.\n  query = \"\"\"\n  \tMATCH (n:base {entity_id: $entity_id})\n  \tRETURN count { (n)--() } AS degree;\n  \"\"\"\n  ```\n\n\n---\n\n### Implementation Details\n\nAs shown in the title 5 functions have been replaced by batch calls: get_node > get_nodes_batch, get_edge > get_edges_batch, node_degree > get_node_degrees_batch, edge_degree > get_edges_degree_batch, get_node_edges > get_nodes_edges_batch. \n\n**In `neo4j_impl.py`:**  \n  ```python\n  async def get_node(self, node_id: str) -> dict[str, str] | None:\n        \"\"\"Get node by its label identifier.\n\n        Args:\n            node_id: The node label to look up\n\n        Returns:\n            dict: Node properties if found\n            None: If node not found\n\n        Raises:\n            ValueError: If node_id is invalid\n            Exception: If there is an error executing the query\n        \"\"\"\n        async with self._driver.session(\n            database=self._DATABASE, default_access_mode=\"READ\"\n        ) as session:\n            try:\n                query = \"MATCH (n:base {entity_id: $entity_id}) RETURN n\"\n                result = await session.run(query, entity_id=node_id)\n                try:\n                    \n                    records = await result.fetch(2)  # Get 2 records for duplication check\n\n                    if len(records) > 1:\n                        logger.warning(\n                            f\"Multiple nodes found with label '{node_id}'. Using first node.\"\n                        )\n                    if records:\n                        node = records[0][\"n\"]\n                        node_dict = dict(node)\n                        # Remove base label from labels list if it exists\n                        if \"labels\" in node_dict:\n                            node_dict[\"labels\"] = [\n                                label\n                                for label in node_dict[\"labels\"]\n                                if label != \"base\"\n                            ]\n                        logger.debug(f\"Neo4j query node {query} return: {node_dict}\")\n                        return node_dict\n                    return None\n                finally:\n                    await result.consume()  # Ensure result is fully consumed\n            except Exception as e:\n                logger.error(f\"Error getting node for {node_id}: {str(e)}\")\n                raise\n\n    async def get_nodes_batch(self, node_ids: list[str]) -> dict[str, dict]:\n        \"\"\"\n        Retrieve multiple nodes in one query using UNWIND.\n        \n        Args:\n            node_ids: List of node entity IDs to fetch.\n            \n        Returns:\n            A dictionary mapping each node_id to its node data (or None if not found).\n        \"\"\"\n        async with self._driver.session(\n            database=self._DATABASE, default_access_mode=\"READ\"\n        ) as session:\n            query = \"\"\"\n            UNWIND $node_ids AS id\n            MATCH (n:base {entity_id: id})\n            RETURN n.entity_id AS entity_id, n\n            \"\"\"\n            result = await session.run(query, node_ids=node_ids)\n            nodes = {}\n            async for record in result:\n                entity_id = record[\"entity_id\"]\n                node = record[\"n\"]\n                node_dict = dict(node)\n                # Remove the 'base' label if present in a 'labels' property\n                if \"labels\" in node_dict:\n                    node_dict[\"labels\"] = [label for label in node_dict[\"labels\"] if label != \"base\"]\n                nodes[entity_id] = node_dict\n            await result.consume()  # Make sure to consume the result fully\n            return nodes\n\n    #@logfire.instrument\n    async def node_degree(self, node_id: str) -> int:\n        \"\"\"Get the degree (number of relationships) of a node with the given label.\n        If multiple nodes have the same label, returns the degree of the first node.\n        If no node is found, returns 0.\n\n        Args:\n            node_id: The label of the node\n\n        Returns:\n            int: The number of relationships the node has, or 0 if no node found\n\n        Raises:\n            ValueError: If node_id is invalid\n            Exception: If there is an error executing the query\n        \"\"\"\n        async with self._driver.session(\n            database=self._DATABASE, default_access_mode=\"READ\"\n        ) as session:\n            try:\n                # query = \"\"\"\n                #     MATCH (n:base {entity_id: $entity_id})\n                #     OPTIONAL MATCH (n)-[r]-()\n                #     RETURN COUNT(r) AS degree\n                # \"\"\"\n                # This should be an improved query for when traversing every single edge of a node.\n                query = \"\"\"\n                    MATCH (n:base {entity_id: $entity_id})\n                    RETURN count { (n)--() } AS degree;\n                \"\"\"\n                result = await session.run(query, entity_id=node_id)\n                try:\n                    record = await result.single()\n\n                    if not record:\n                        logger.warning(f\"No node found with label '{node_id}'\")\n                        return 0\n\n                    degree = record[\"degree\"]\n                    logger.debug(\n                        \"Neo4j query node degree for {node_id} return: {degree}\"\n                    )\n                    return degree\n                finally:\n                    await result.consume()  # Ensure result is fully consumed\n            except Exception as e:\n                logger.error(f\"Error getting node degree for {node_id}: {str(e)}\")\n                raise\n\n    async def get_node_degrees_batch(self, node_ids: list[str]) -> dict[str, int]:\n        \"\"\"\n        Retrieve the degree for multiple nodes in a single query using UNWIND.\n        \n        Args:\n            node_ids: List of node labels (entity_id values) to look up.\n        \n        Returns:\n            A dictionary mapping each node_id to its degree (number of relationships). \n            If a node is not found, its degree will be set to 0.\n        \"\"\"\n        async with self._driver.session(\n            database=self._DATABASE, default_access_mode=\"READ\"\n        ) as session:\n            query = \"\"\"\n                UNWIND $node_ids AS id\n                MATCH (n:base {entity_id: id})\n                RETURN n.entity_id AS entity_id, count { (n)--() } AS degree;\n            \"\"\"\n            result = await session.run(query, node_ids=node_ids)\n            degrees = {}\n            async for record in result:\n                entity_id = record[\"entity_id\"]\n                degrees[entity_id] = record[\"degree\"]\n            await result.consume()  # Ensure result is fully consumed\n            \n            # For any node_id that did not return a record, set degree to 0.\n            for nid in node_ids:\n                if nid not in degrees:\n                    logger.warning(f\"No node found with label '{nid}'\")\n                    degrees[nid] = 0\n            \n            logger.debug(f\"Neo4j batch node degree query returned: {degrees}\")\n            return degrees\n\n    #@logfire.instrument\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        \"\"\"Get the total degree (sum of relationships) of two nodes.\n\n        Args:\n            src_id: Label of the source node\n            tgt_id: Label of the target node\n\n        Returns:\n            int: Sum of the degrees of both nodes\n        \"\"\"\n        src_degree = await self.node_degree(src_id)\n        trg_degree = await self.node_degree(tgt_id)\n\n        # Convert None to 0 for addition\n        src_degree = 0 if src_degree is None else src_degree\n        trg_degree = 0 if trg_degree is None else trg_degree\n\n        degrees = int(src_degree) + int(trg_degree)\n        return degrees\n    \n    async def get_edges_degree_batch(self, edge_pairs: list[tuple[str, str]]) -> dict[tuple[str, str], int]:\n        \"\"\"\n        Calculate the combined degree for each edge (sum of the source and target node degrees)\n        in batch using the already implemented get_node_degrees_batch.\n        \n        Args:\n            edge_pairs: List of (src, tgt) tuples.\n        \n        Returns:\n            A dictionary mapping each (src, tgt) tuple to the sum of their degrees.\n        \"\"\"\n        # Collect unique node IDs from all edge pairs.\n        unique_node_ids = {src for src, _ in edge_pairs}\n        unique_node_ids.update({tgt for _, tgt in edge_pairs})\n        \n        # Get degrees for all nodes in one go.\n        degrees = await self.get_node_degrees_batch(list(unique_node_ids))\n        \n        # Sum up degrees for each edge pair.\n        edge_degrees = {}\n        for src, tgt in edge_pairs:\n            edge_degrees[(src, tgt)] = degrees.get(src, 0) + degrees.get(tgt, 0)\n        return edge_degrees\n\n\n    #@logfire.instrument\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> dict[str, str] | None:\n        \"\"\"Get edge properties between two nodes.\n\n        Args:\n            source_node_id: Label of the source node\n            target_node_id: Label of the target node\n\n        Returns:\n            dict: Edge properties if found, default properties if not found or on error\n\n        Raises:\n            ValueError: If either node_id is invalid\n            Exception: If there is an error executing the query\n        \"\"\"\n        try:\n            async with self._driver.session(\n                database=self._DATABASE, default_access_mode=\"READ\"\n            ) as session:\n                # r is changed to r:DIRECTED for specifying the edge type, but for now all edge types are DIRECTED.\n                query = \"\"\"\n                MATCH (start:base {entity_id: $source_entity_id})-[r:DIRECTED]-(end:base {entity_id: $target_entity_id})\n                RETURN properties(r) as edge_properties\n                \"\"\"\n                result = await session.run(\n                    query,\n                    source_entity_id=source_node_id,\n                    target_entity_id=target_node_id,\n                )\n                try:\n                    records = await result.fetch(2)\n\n                    if len(records) > 1:\n                        logger.warning(\n                            f\"Multiple edges found between '{source_node_id}' and '{target_node_id}'. Using first edge.\"\n                        )\n                    if records:\n                        try:\n                            edge_result = dict(records[0][\"edge_properties\"])\n                            logger.debug(f\"Result: {edge_result}\")\n                            # Ensure required keys exist with defaults\n                            required_keys = {\n                                \"weight\": 0.0,\n                                \"source_id\": None,\n                                \"description\": None,\n                                \"keywords\": None,\n                            }\n                            for key, default_value in required_keys.items():\n                                if key not in edge_result:\n                                    edge_result[key] = default_value\n                                    logger.warning(\n                                        f\"Edge between {source_node_id} and {target_node_id} \"\n                                        f\"missing {key}, using default: {default_value}\"\n                                    )\n\n                            logger.debug(\n                                f\"{inspect.currentframe().f_code.co_name}:query:{query}:result:{edge_result}\"\n                            )\n                            return edge_result\n                        except (KeyError, TypeError, ValueError) as e:\n                            logger.error(\n                                f\"Error processing edge properties between {source_node_id} \"\n                                f\"and {target_node_id}: {str(e)}\"\n                            )\n                            # Return default edge properties on error\n                            return {\n                                \"weight\": 0.0,\n                                \"source_id\": None,\n                                \"description\": None,\n                                \"keywords\": None,\n                            }\n\n                    logger.debug(\n                        f\"{inspect.currentframe().f_code.co_name}: No edge found between {source_node_id} and {target_node_id}\"\n                    )\n                    # Return default edge properties when no edge found\n                    return {\n                        \"weight\": 0.0,\n                        \"source_id\": None,\n                        \"description\": None,\n                        \"keywords\": None,\n                    }\n                finally:\n                    await result.consume()  # Ensure result is fully consumed\n\n        except Exception as e:\n            logger.error(\n                f\"Error in get_edge between {source_node_id} and {target_node_id}: {str(e)}\"\n            )\n            raise\n\n    async def get_edges_batch(self, pairs: list[dict[str, str]]) -> dict[tuple[str, str], dict]:\n        \"\"\"\n        Retrieve edge properties for multiple (src, tgt) pairs in one query.\n        \n        Args:\n            pairs: List of dictionaries, e.g. [{\"src\": \"node1\", \"tgt\": \"node2\"}, ...]\n        \n        Returns:\n            A dictionary mapping (src, tgt) tuples to their edge properties.\n        \"\"\"\n        async with self._driver.session(\n            database=self._DATABASE, default_access_mode=\"READ\"\n        ) as session:\n            query = \"\"\"\n            UNWIND $pairs AS pair\n            MATCH (start:base {entity_id: pair.src})-[r:DIRECTED]-(end:base {entity_id: pair.tgt})\n            RETURN pair.src AS src_id, pair.tgt AS tgt_id, collect(properties(r)) AS edges\n            \"\"\"\n            result = await session.run(query, pairs=pairs)\n            edges_dict = {}\n            async for record in result:\n                src = record[\"src_id\"]\n                tgt = record[\"tgt_id\"]\n                edges = record[\"edges\"]\n                if edges and len(edges) > 0:\n                    edge_props = edges[0]  # choose the first if multiple exist\n                    # Ensure required keys exist with defaults\n                    for key, default in {\"weight\": 0.0, \"source_id\": None, \"description\": None, \"keywords\": None}.items():\n                        if key not in edge_props:\n                            edge_props[key] = default\n                    edges_dict[(src, tgt)] = edge_props\n                else:\n                    # No edge found – set default edge properties\n                    edges_dict[(src, tgt)] = {\"weight\": 0.0, \"source_id\": None, \"description\": None, \"keywords\": None}\n            await result.consume()\n            return edges_dict\n    \n    \n    # Hopefully improved query function.\n    #@logfire.instrument\n    async def get_node_edges(self, source_node_id: str) -> list[tuple[str, str]]:\n        \"\"\"Retrieves all edges (relationships) for a particular node identified by its label.\n\n        Args:\n            source_node_id: Label of the node to get edges for\n\n        Returns:\n            list[tuple[str, str]]: List of (source_label, target_label) tuples representing edges.\n                                    Returns an empty list if no edges are found.\n\n        Raises:\n            ValueError: If source_node_id is invalid.\n            Exception: If there is an error executing the query.\n        \"\"\"\n        try:\n            async with self._driver.session(\n                database=self._DATABASE, default_access_mode=\"READ\"\n            ) as session:\n                try:\n                    query = \"\"\"\n                        MATCH (n:base {entity_id: $entity_id})\n                        OPTIONAL MATCH (n)-[r]-(connected:base)\n                        RETURN n.entity_id AS source_entity_id, connected.entity_id AS target_entity_id\n                    \"\"\"\n                    results = await session.run(query, entity_id=source_node_id)\n\n                    edges = []\n                    async for record in results:\n                        source_label = record[\"source_entity_id\"]\n                        target_label = record[\"target_entity_id\"]\n\n                        # Only add the edge if both labels are present\n                        if source_label and target_label:\n                            edges.append((source_label, target_label))\n\n                    await results.consume()  # Ensure all results are consumed\n                    return edges\n                except Exception as e:\n                    logger.error(f\"Error getting edges for node {source_node_id}: {str(e)}\")\n                    await results.consume()  # Ensure results are consumed even on error\n                    raise\n        except Exception as e:\n            logger.error(f\"Error in get_node_edges for {source_node_id}: {str(e)}\")\n            raise\n\n    # New batch function using UNWIND\n    async def get_nodes_edges_batch(self, node_ids: list[str]) -> dict[str, list[tuple[str, str]]]:\n        \"\"\"\n        Batch retrieve edges for multiple nodes in one query using UNWIND.\n        \n        Args:\n            node_ids: List of node IDs (entity_id) for which to retrieve edges.\n            \n        Returns:\n            A dictionary mapping each node ID to its list of edge tuples (source, target).\n        \"\"\"\n        async with self._driver.session(database=self._DATABASE, default_access_mode=\"READ\") as session:\n            query = \"\"\"\n                UNWIND $node_ids AS id\n                MATCH (n:base {entity_id: id})\n                OPTIONAL MATCH (n)-[r]-(connected:base)\n                RETURN id AS queried_id, n.entity_id AS source_entity_id, connected.entity_id AS target_entity_id\n            \"\"\"\n            result = await session.run(query, node_ids=node_ids)\n            # Initialize the dictionary with empty lists for each node ID\n            edges_dict = {node_id: [] for node_id in node_ids}\n            async for record in result:\n                queried_id = record[\"queried_id\"]\n                source_label = record[\"source_entity_id\"]\n                target_label = record[\"target_entity_id\"]\n                if source_label and target_label:\n                    edges_dict[queried_id].append((source_label, target_label))\n            await result.consume()  # Ensure results are fully consumed\n            return edges_dict\n  ```\n\n**In `operate.py`**:\n  ```python\n    # In _get_node_data()\n    # get entity information (CURRENT LIGHTRAG IMPLEMENTATION)\n    # node_datas, node_degrees = await asyncio.gather(\n    #     asyncio.gather(\n    #         *[knowledge_graph_inst.get_node(r[\"entity_name\"]) for r in results]\n    #     ),\n    #     asyncio.gather(\n    #         *[knowledge_graph_inst.node_degree(r[\"entity_name\"]) for r in results]\n    #     ),\n    # )\n\n    # My implementation using UNWIND to batch ids into a list and send one request to Neo4j\n    # Extract all entity IDs from your results list\n    node_ids = [r[\"entity_name\"] for r in results]\n\n    # Call the batch node retrieval and degree functions concurrently.\n    nodes_dict, degrees_dict = await asyncio.gather(\n        knowledge_graph_inst.get_nodes_batch(node_ids),   # Your previously defined batch node retrieval\n        knowledge_graph_inst.get_node_degrees_batch(node_ids)\n    )\n  ```\n\n```python\n    # In _find_most_related_entities_from_relationships()\n    # Original approach using individual calls:\n    # node_datas, node_degrees = await asyncio.gather(\n    #     asyncio.gather(\n    #         *[knowledge_graph_inst.get_node(entity_name) for entity_name in entity_names]\n    #     ),\n    #     asyncio.gather(\n    #         *[knowledge_graph_inst.node_degree(entity_name) for entity_name in entity_names]\n    #     ),\n    # )\n    # node_datas = [\n    #     {**n, \"entity_name\": k, \"rank\": d}\n    #     for k, n, d in zip(entity_names, node_datas, node_degrees)\n    # ]\n\n    # Batch approach: Retrieve nodes and their degrees concurrently with one query each.\n    nodes_dict, degrees_dict = await asyncio.gather(\n        knowledge_graph_inst.get_nodes_batch(entity_names),\n        knowledge_graph_inst.get_node_degrees_batch(entity_names)\n    )\n  ```\n\nThis is one example of converting the code from get_node to get_nodes_batch.\n\n---\n\n### Additional Context\n\n- **Redis Improvement Example:**  \n  A similar optimization was applied to Redis by converting the loop into a single bulk call using mget (multiple get):\n  ```python\n  async def get_by_ids_batch(self, ids: list[str]) -> list[dict[str, Any]]:\n      keys = [f\"{self.namespace}:{id}\" for id in ids]\n      data_list = await self._redis.mget(*keys)\n      return [json.loads(data) if data else None for data in data_list]\n  ```\n  \n- **Benefits:**\n  - **Performance:** Drastically reduce the number of queries to Neo4j.\n  - **Connection Efficiency:** Reduce the stress on the Neo4j connection pool, minimizing timeouts.\n  - **Generalization:** Sets a precedent for batching across all graph DB integrations in LightRAG.\n\n---\n\n### Conclusion\n\nBy implementing batched queries using UNWIND in Neo4j and updating the connection pool handling, we expect a significant performance improvement in LightRAG when processing queries. This feature will also simplify the overall architecture and extend the benefits to other graph database integrations.\n\nI look forward to the team's feedback and collaboration on integrating this enhancement.\n\nPS: My full stack currently includes Milvus, Redis, MongoDB, Neo4j, and FastAPI with Gunicorn. I’m a student, and I’ll be hosting this on a Linux server and running it in production in a minimal environment. If I spot even more improvements, I’ll post them.\n\n### Additional Context\n\n[neo4j_impl.txt](https://github.com/user-attachments/files/19546877/neo4j_impl.txt)\n[operate.txt](https://github.com/user-attachments/files/19546876/operate.txt)\n\nI couldn't post the full code so I'm hoping this works. Notice that I am mocking something like the calls to OpenAI so you can not just copy paste everything. Also the Base.py should also be updated ofcourse to include the new batch functions.\n\nPossible related issues:\n[https://github.com/HKUDS/LightRAG/issues/1190](url)\n[https://github.com/HKUDS/LightRAG/issues/1180](url)\n[https://github.com/HKUDS/LightRAG/issues/1179](url)",
      "state": "closed",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-04-01T07:47:40Z",
      "updated_at": "2025-04-15T06:41:18Z",
      "closed_at": "2025-04-15T06:41:18Z",
      "labels": [
        "bug",
        "enhancement",
        "neo4j",
        "Core"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1246/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "frederikhendrix",
        "danielaskdd"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1246",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1246",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:44.728224",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "PR #1258 introduce a batch operation  to prevent making to many parallel query to database. Is that address this problem?",
          "created_at": "2025-04-05T14:01:09Z"
        },
        {
          "author": "frederikhendrix",
          "body": "> PR [#1258](https://github.com/HKUDS/LightRAG/pull/1258) introduce a batch operation to prevent making to many parallel query to database. Is that address this problem?\n\nWell yes kind of. The issue you linked talks about RedisKV and adjusting pooling. In my design I only used the MGET key. I think ",
          "created_at": "2025-04-05T15:39:05Z"
        },
        {
          "author": "danielaskdd",
          "body": "Your suggestion is excellent. Could you submit a PR to help us complete the program modifications?",
          "created_at": "2025-04-06T18:35:08Z"
        },
        {
          "author": "frederikhendrix",
          "body": "Hi @danielaskdd ,\n\nJust to clarify: when you mentioned “your suggestion,” are you specifically referring to combining the calls into one larger database call? I ask because when I implement this for Neo4j, all GraphDBStorage object classes will have the added \"batch\" functions (as mentioned in the f",
          "created_at": "2025-04-06T22:00:16Z"
        },
        {
          "author": "danielaskdd",
          "body": "To ensure consistency and improve performance, batch processing functionality should be implemented across all graph storage systems. We will prioritize Neo4j first, followed by NetworkX and PostgreSQL AGE. A dedicated development branch will be created for this initiative. The branch will only be m",
          "created_at": "2025-04-06T22:36:25Z"
        }
      ]
    },
    {
      "issue_number": 1342,
      "title": "[Feature Request]: Edit node/edge name and description by webui",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n### Edit node/edge name and description by webui\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-10T19:47:42Z",
      "updated_at": "2025-04-15T06:38:57Z",
      "closed_at": "2025-04-15T06:38:57Z",
      "labels": [
        "enhancement",
        "ui"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1342/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1342",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1342",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:44.926135",
      "comments": [
        {
          "author": "choizhang",
          "body": "Let me try",
          "created_at": "2025-04-11T07:14:06Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please note that editing an entity name may result in a naming conflict. So a name conflict check must be performed before saving any changes.\n\nBy the way, merging two or more nodes is discuss in #1343",
          "created_at": "2025-04-11T07:28:11Z"
        },
        {
          "author": "danielaskdd",
          "body": "The upsert_node and upsert_edge APIs are currently not implemented. If you encounter any challenges while implementing these APIs, please don't hesitate to reach out.",
          "created_at": "2025-04-12T00:53:17Z"
        },
        {
          "author": "choizhang",
          "body": "The API issue has been resolved, and real-time updates of graph data require some time for debugging",
          "created_at": "2025-04-12T14:14:21Z"
        }
      ]
    },
    {
      "issue_number": 1354,
      "title": "[Question]: Why does knowledge graph contain nodes that are not available in the documents?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThanks for open-sourcing the project. I've tried with a naive setup that uses the `nomic-embed-text` as the text embedder and Llama 3.2 3B as the LLM. I fed all my doc pages into LightRAG, and to my surprise, there are so many nodes in the knowledge graph that has nothing to do with the docs I put in (140 markdown technical documents). Does anyone have an idea what's going on? I used the `lightrag-serve` function. Besides configuring the text embedder and LLM (with context window 32k), I used all default settings.\n\n\n\n### Additional Context\n\n![Image](https://github.com/user-attachments/assets/dd0041c0-08d8-4968-b251-eb3f1da08094)\n\n![Image](https://github.com/user-attachments/assets/8491a7d3-e578-4031-bd45-53a407118b9a)",
      "state": "closed",
      "author": "duguyue100",
      "author_type": "User",
      "created_at": "2025-04-11T08:40:53Z",
      "updated_at": "2025-04-14T10:36:30Z",
      "closed_at": "2025-04-14T08:08:45Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1354/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1354",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1354",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:45.133594",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "This is just speculation, but you could add logging like \"print(answer_llm)\" to the console and check that the model output is what you expect, or just take the \"entity_extraction\" prompt with and example chunk and paste it in the powershell after running \"ollama run llama3.2:3b\" and see what the ou",
          "created_at": "2025-04-11T09:28:27Z"
        },
        {
          "author": "duguyue100",
          "body": "I see, thanks! I will give it a try, I didn't expect a small LLM that can hallucinate this much. I don't have the freedom to use a public model right now, so maybe I would just switch to a bigger model and give it another push.",
          "created_at": "2025-04-11T10:00:15Z"
        },
        {
          "author": "frederikhendrix",
          "body": "> I see, thanks! I will give it a try, I didn't expect a small LLM that can hallucinate this much. I don't have the freedom to use a public model right now, so maybe I would just switch to a bigger model and give it another push.\n\nYes I would always try to use the biggest model you can run for extra",
          "created_at": "2025-04-11T10:04:11Z"
        },
        {
          "author": "danielaskdd",
          "body": "At least use model of 32B, the bigger the better, embedding is also importance for query result, nomic-embed-text  is not  a good choice.\n\nWe recommend using a model with at least 32B parameters, the bigger the better. Additionally, the choice of embedding model significantly impacts query performan",
          "created_at": "2025-04-12T00:40:33Z"
        },
        {
          "author": "duguyue100",
          "body": "@danielaskdd Really? This kind of know-how is very important, as I heard everywhere that `nomic-embed-text` is a high-performance one. Are there top ones you would recommend?",
          "created_at": "2025-04-14T08:08:45Z"
        }
      ]
    },
    {
      "issue_number": 1369,
      "title": "[Question]: <title> 建立文档之间的知识图谱",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n我想要通过rag_storage中的graph_chunk_entity_relation.graphml提取每个文档之间的有多少条联系，然后构建为知识图谱的形势在web_ui中展示出来，我该怎么做？大概像这样的形式\n\n![Image](https://github.com/user-attachments/assets/a1cbabac-c098-44ef-8efa-89e607cc33d6)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "luo-xingyu",
      "author_type": "User",
      "created_at": "2025-04-14T10:30:40Z",
      "updated_at": "2025-04-14T10:30:40Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1369/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1369",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1369",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:45.326290",
      "comments": []
    },
    {
      "issue_number": 1366,
      "title": "[Question]: It seems no related chunks are retrieved while there does exist related chunks and nodes in the KG",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi ! I run lightrag-server in Chinese mode and use Ollama Deepseek 32B and bge-m3 as models. After uploading the text file in the WebUI, the KG can be displayed although nodes in it are not exactly what i am expecting. I try to ask question based the text, but the answer is totally irrelevant to the text. How should I solve this problem, and could you help me with improving the KG quality. More nodes and precise relations are needed.\n\n### Additional Context\nINFO: Started server process [2424958]\nINFO: Waiting for application startup.\nINFO: Process 2424958 initialized updated flags for namespace: [full_docs]\nINFO: Process 2424958 ready to initialize storage namespace: [full_docs]\nINFO: Process 2424958 KV load full_docs with 1 records\nINFO: Process 2424958 initialized updated flags for namespace: [text_chunks]\nINFO: Process 2424958 ready to initialize storage namespace: [text_chunks]\nINFO: Process 2424958 KV load text_chunks with 115 records\nINFO: Process 2424958 initialized updated flags for namespace: [entities]\nINFO: Process 2424958 initialized updated flags for namespace: [relationships]\nINFO: Process 2424958 initialized updated flags for namespace: [chunks]\nINFO: Process 2424958 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 2424958 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 2424958 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 2424958 KV load llm_response_cache with 2 records\nINFO: Process 2424958 initialized updated flags for namespace: [doc_status]\nINFO: Process 2424958 ready to initialize storage namespace: [doc_status]\nINFO: Process 2424958 doc status load doc_status with 1 records\nINFO: Process 2424958 Pipeline namespace initialized\n\nServer is ready to accept connections! 🚀\n\nINFO: Application startup complete.\nINFO: 100.103.105.101:43594 - \"GET /auth-status HTTP/1.1\" 200\nINFO: 100.103.105.101:43593 - \"GET /webui/logo.png HTTP/1.1\" 304\nINFO: 100.103.105.101:43593 - \"GET /docs HTTP/1.1\" 200\nINFO: 100.103.105.101:43594 - \"GET /openapi.json HTTP/1.1\" 200\nINFO: Process 2424958 KV writting 0 records to text_chunks\nINFO: Process 2424958 drop text_chunks\nINFO: Process 2424958 KV writting 0 records to full_docs\nINFO: Process 2424958 drop full_docs\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/liuyuchen/lightrag/rag_storage/vdb_entities.json'} 0 data\nINFO: Process 2424958 drop entities(file:/home/liuyuchen/lightrag/rag_storage/vdb_entities.json)\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/liuyuchen/lightrag/rag_storage/vdb_relationships.json'} 0 data\nINFO: Process 2424958 drop relationships(file:/home/liuyuchen/lightrag/rag_storage/vdb_relationships.json)\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/liuyuchen/lightrag/rag_storage/vdb_chunks.json'} 0 data\nINFO: Process 2424958 drop chunks(file:/home/liuyuchen/lightrag/rag_storage/vdb_chunks.json)\nINFO: Process 2424958 drop graph chunk_entity_relation (file:/home/liuyuchen/lightrag/rag_storage/graph_chunk_entity_relation.graphml)\nINFO: Process 2424958 doc status writting 0 records to doc_status\nINFO: Process 2424958 drop doc_status\nINFO: Successfully dropped JsonKVStorage\nINFO: Successfully dropped JsonKVStorage\nINFO: Successfully dropped NanoVectorDBStorage\nINFO: Successfully dropped NanoVectorDBStorage\nINFO: Successfully dropped NanoVectorDBStorage\nINFO: Successfully dropped NetworkXStorage\nINFO: Successfully dropped JsonDocStatusStorage\nINFO: 100.103.105.101:43594 - \"DELETE /documents HTTP/1.1\" 200\nINFO: 100.103.105.101:43594 - \"POST /documents/upload HTTP/1.1\" 200\nINFO: Process 2424958 doc status writting 1 records to doc_status\nINFO: Stored 1 new unique documents\nINFO: Successfully fetched and enqueued file: risk.txt\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processing batch 1 of 1.\nINFO: Process 2424958 doc status writting 1 records to doc_status\nINFO:  == LLM cache == saving 19f004a0f7a06df956b4c978a3b11eee\nINFO:  == LLM cache == saving d4cbac0ee64e2190f3cf942494edd6e7\nINFO:  == LLM cache == saving 53daf64e335e4b878c721d70d2c5141f\nINFO:  == LLM cache == saving 06748f1e93c0949a4e4a8f07646d97ca\nINFO:  == LLM cache == saving 79ead2eff7cea021589c150dc6e3a6df\nERROR: Failed to extract entities and relationships\nERROR: Failed to process document doc-d80dd417ab1d37a0fc7dde0559ca9bc1: \nINFO: Process 2424958 doc status writting 1 records to doc_status\nINFO: Process 2424958 KV writting 1 records to full_docs\nINFO: Process 2424958 KV writting 5 records to text_chunks\nINFO: Process 2424958 KV writting 7 records to llm_response_cache\nINFO: Writing graph with 0 nodes, 0 edges\nINFO: In memory DB persist to disk\nINFO: Completed batch 1 of 1.\nINFO: Document processing pipeline completed\nINFO:  == LLM cache == saving f51ce619b3ab13eaa2d870b99399d13e\nINFO: Chk 1/5: extracted 1 Ent + 0 Rel (deduplicated)\nINFO:  == LLM cache == saving 23ad0193f9bd070dd2dae66f5733294b\nINFO: Chk 2/5: extracted 16 Ent + 5 Rel (deduplicated)\nINFO:  == LLM cache == saving a05a14838ce03e2cbf5ee853585c0214\nINFO: Chk 3/5: extracted 2 Ent + 0 Rel (deduplicated)\nINFO:  == LLM cache == saving 82ba1ed7f6b05aa0f7040bcb349d3dc0\nINFO: Chk 4/5: extracted 4 Ent + 4 Rel (deduplicated)\nINFO: Process 2424958 KV writting 0 records to text_chunks\nINFO: Process 2424958 drop text_chunks\nINFO: Process 2424958 KV writting 0 records to full_docs\nINFO: Process 2424958 drop full_docs\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/liuyuchen/lightrag/rag_storage/vdb_entities.json'} 0 data\nINFO: Process 2424958 drop entities(file:/home/liuyuchen/lightrag/rag_storage/vdb_entities.json)\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/liuyuchen/lightrag/rag_storage/vdb_relationships.json'} 0 data\nINFO: Process 2424958 drop relationships(file:/home/liuyuchen/lightrag/rag_storage/vdb_relationships.json)\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/home/liuyuchen/lightrag/rag_storage/vdb_chunks.json'} 0 data\nINFO: Process 2424958 drop chunks(file:/home/liuyuchen/lightrag/rag_storage/vdb_chunks.json)\nINFO: Process 2424958 drop graph chunk_entity_relation (file:/home/liuyuchen/lightrag/rag_storage/graph_chunk_entity_relation.graphml)\nINFO: Process 2424958 doc status writting 0 records to doc_status\nINFO: Process 2424958 drop doc_status\nINFO: Successfully dropped JsonKVStorage\nINFO: Successfully dropped JsonKVStorage\nINFO: Successfully dropped NanoVectorDBStorage\nINFO: Successfully dropped NanoVectorDBStorage\nINFO: Successfully dropped NanoVectorDBStorage\nINFO: Successfully dropped NetworkXStorage\nINFO: Successfully dropped JsonDocStatusStorage\nINFO: 100.103.105.101:44473 - \"DELETE /documents HTTP/1.1\" 200\nINFO: 100.103.105.101:44473 - \"POST /documents/upload HTTP/1.1\" 200\nINFO: Process 2424958 doc status writting 1 records to doc_status\nINFO: Stored 1 new unique documents\nINFO: Successfully fetched and enqueued file: risk.txt\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processing batch 1 of 1.\nINFO: Process 2424958 doc status writting 1 records to doc_status\nINFO: Chk 1/5: extracted 1 Ent + 0 Rel (deduplicated)\nINFO: Chk 2/5: extracted 16 Ent + 5 Rel (deduplicated)\nINFO: Chk 3/5: extracted 2 Ent + 0 Rel (deduplicated)\nINFO:  == LLM cache == saving 936108fa63451c2613572b8a02f86ca6\nINFO: Chk 4/5: extracted 0 Ent + 0 Rel (deduplicated)\nINFO:  == LLM cache == saving 82ba1ed7f6b05aa0f7040bcb349d3dc0\nINFO: Chk 5/5: extracted 4 Ent + 4 Rel (deduplicated)\nINFO: Merge N: RO | 2+0\nINFO: Extracted 22 entities + 9 relationships (total)\nINFO: Process 2424958 doc status writting 1 records to doc_status\nINFO: Process 2424958 KV writting 1 records to full_docs\nINFO: Process 2424958 KV writting 5 records to text_chunks\nINFO: Process 2424958 KV writting 12 records to llm_response_cache\nINFO: Writing graph with 22 nodes, 9 edges\nINFO: In memory DB persist to disk\nINFO: Completed batch 1 of 1.\nINFO: Document processing pipeline completed\nINFO: 100.103.105.101:46638 - \"GET /graph/label/list HTTP/1.1\" 200\nINFO: Subgraph query successful | Node count: 22 | Edge count: 9\nINFO: 100.103.105.101:46638 - \"GET /graphs?label=*&max_depth=3&max_nodes=1000 HTTP/1.1\" 200\nERROR: JSON parsing error: Extra data: line 5 column 1 (char 123)\nWARNING: low_level_keywords and high_level_keywords is empty\nINFO: 100.103.105.101:2033 - \"POST /query/stream HTTP/1.1\" 200\nINFO: Process 2424958 buidling query context...\nINFO: Query nodes: 风险回避, top_k: 10, cosine: 0.2\nINFO: Local query uses 10 entites, 3 relations, 3 chunks\nINFO: 100.103.105.101:4292 - \"POST /query/stream HTTP/1.1\" 200\nINFO: Process 2424958 KV writting 13 records to llm_response_cache\nINFO: 100.103.105.101:4333 - \"POST /query/stream HTTP/1.1\" 200\nINFO: Process 2424958 buidling query context...\nINFO: Query nodes: 风险回避, top_k: 10, cosine: 0.2\nINFO: Query edges: 关系, top_k: 10, cosine: 0.2\nINFO: Global query uses 14 entites, 9 relations, 2 chunks\nINFO: Local query uses 10 entites, 3 relations, 3 chunks\nINFO: 100.103.105.101:4396 - \"POST /query/stream HTTP/1.1\" 200\n^CINFO: Shutting down\nINFO: Waiting for application shutdown.\nINFO: Application shutdown complete.\nINFO: Finished server process [2424958]\n\n\n\nThe query is \"风险回避有什么策略\". At first there are errors noting that \"high level keywords and low level key words are empty\" and  I fill them with “关系“ and “风险回避” respectively. \n\nThe answer is :\"好吧，我现在得想想风险回避的策略。这个题目听起来有点专业，不过我尽量理清楚。首先，风险回避应该是指在面对潜在风险时采取的一些措施来避免这些风险发生或者减少它们的影响吧？那具体的策略都有哪些呢？\n\n\n我记得以前学过风险管理，里面提到过几种方法，比如风险转移、风险减轻、风险接受等。那风险回避应该是在这些里面的一种。不过问题问的是风险回避的具体策略，所以我要想有哪些策略属于风险回避的范畴。\n\n\n首先，可能就是避免行动本身。比如说，如果一个项目有可能带来很大的风险，那么直接不做这个项目是不是一种风险回避？比如投资某个高风险的市场，为了避免潜在损失，选择不进入。这应该是最直接的风险回避方式了。\n\n\n其次，改变目标或者范围，这样可以降低风险发生的可能性。比如，原本计划在三个月内完成一个复杂的产品开发，但考虑到时间压力大，容易出问题，那就把时间延长到五个月，这样风险可能会小一些。这种调整也是一种风险回避策略吧？\n\n\n另外，使用替代方案也是一个方法。当某个计划有较高的风险时，可以寻找风险较低的替代方案来代替。比如在选择供应商的时候，如果原来的供应商可能有供应不稳定的风险，就换一个更有保障的供应商。\n\n\n还有严格控制条件，确保在可控范围内进行操作。比如说，在进行化学实验的时候，严格遵守安全规程和使用防护设备，这样可以避免事故的发生。这也是一种风险回避策略。\n\n\n另外，建立后备方案也是一种方式。比如在项目管理中，制定应急计划，当某些关键环节出现问题时，能够及时切换到备用方案，减少损失。不过这可能更偏向于风险应对而非完全回避，但有时候也能起到避免风险的作用。\n\n\n还有分散资源和任务，通过分摊来降低每个部分的风险。比如投资的时候，不把所有资金投入一个高风险项目，而是分散到多个低风险的项目中去。这样即使某个项目出现问题，整体损失也不会太大。\n\n\n再有就是依赖可靠的技术或方法，使用已经被验证过的技术来减少不确定性带来的风险。比如说，在软件开发中，采用成熟稳定的框架而不是尝试新的不稳定技术，这样可以避免因为新技术不成熟导致的风险。\n\n\n另外，寻求专业意见也是一个好策略。在面对复杂问题时，咨询专家的意见可以帮助识别潜在风险，并找到更好的解决方案，从而规避风险的发生。\n\n\n还有就是进行彻底的尽职调查，在做重大决策前，全面了解相关信息和可能的风险因素，确保做出的决定是基于充分的信息基础上的，这样可以避免因为信息不全而带来的风险。\n\n\n最后一点，减少不确定性。通过更多的研究、分析和数据收集，降低对未知因素的依赖，从而更好地预测和控制潜在风险。\n\n\n总结一下，风险回避的策略主要有：不做高风险的事情、调整目标或范围以降低风险、使用替代方案、严格控制条件、建立后备方案、分散资源、依赖可靠技术、寻求专业意见、进行尽职调查以及减少不确定性。这些都是用来主动避免风险发生的方法。\n\n\n不过在实际应用中，有时候完全回避风险可能不太现实，因为某些项目虽然有风险，但也有很大的收益机会。这时候需要权衡利弊，选择最适合的策略。比如对于一些高回报但高风险的机会，可能更倾向于接受风险而不是完全避开。所以，具体情况具体分析才是关键。\n\n\n另外，我还需要考虑这些策略在不同情境下的适用性。比如说，在商业决策中，避免进入一个竞争激烈的市场可能是合理的；而在个人健康方面，改变生活习惯来避免疾病风险也是一种策略。不同的领域可能会有不同的侧重点和方法。\n\n\n总的来说，风险回避需要综合考虑各种因素，并结合具体情况选择合适的策略。这不仅需要对潜在的风险有足够的认识，还需要有灵活的思维和决策能力，才能在复杂的环境中做出明智的选择。\n</think>\n\n\n风险回避是指通过采取一系列措施来避免或减少潜在风险的发生及其影响。以下是一些常见的风险回避策略：\n\n\n风险回避策略\n\n\n\n避免行动本身\n\n如果某个项目或行为存在高风险，直接放弃或不参与，以避免潜在的损失。\n\n\n\n\n调整目标或范围\n\n通过缩小目标规模或延长完成时间，降低风险发生的可能性。\n\n\n\n\n使用替代方案\n\n寻找风险较低的替代方法或资源，以减少风险暴露。\n\n\n\n\n严格控制条件\n\n在可控范围内操作，确保遵守安全规程和使用防护措施，避免事故发生。\n\n\n\n\n建立后备方案\n\n制定应急计划，在关键环节出现问题时及时切换到备用方案，减少损失。\n\n\n\n\n分散资源和任务\n\n将资源或任务分配到多个部分，降低每个部分的风险影响。\n\n\n\n\n依赖可靠技术或方法\n\n采用成熟稳定的解决方案，避免因新技术不成熟带来的风险。\n\n\n\n\n寻求专业意见\n\n咨询专家的意见，帮助识别潜在风险并找到更好的解决方案。\n\n\n\n\n进行尽职调查\n\n在做出重大决策前，全面了解相关信息和可能的风险因素。\n\n\n\n\n减少不确定性\n\n通过研究、分析和数据收集，降低对未知因素的依赖，更好地预测和控制风险。\n\n\n\n\n总结\n\n风险回避需要综合考虑潜在风险和具体情境，并选择最适合的策略。在某些情况下，完全避免风险可能不现实，因此需权衡利弊，做出明智决策。不同领域可能有不同的侧重点和方法，灵活性和决策能力是关键。\"\nwhich are totally wrong because it has nothing to do with the given text file.",
      "state": "open",
      "author": "liuyuchen-cz",
      "author_type": "User",
      "created_at": "2025-04-14T03:13:12Z",
      "updated_at": "2025-04-14T08:20:32Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1366/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1366",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1366",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:45.326318",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "INFO: Process 2424958 doc status writting 1 records to doc_status\nINFO: Chk 1/5: extracted 1 Ent + 0 Rel (deduplicated)\nINFO: Chk 2/5: extracted 16 Ent + 5 Rel (deduplicated)\nINFO: Chk 3/5: extracted 2 Ent + 0 Rel (deduplicated)\nINFO: == LLM cache == saving 936108fa63451c2613572b8a02f86ca6\nINFO: Chk",
          "created_at": "2025-04-14T06:54:20Z"
        },
        {
          "author": "liuyuchen-cz",
          "body": "@frederikhendrix Thanks for your valuable advise I will try to change the prompt language in prompt.py manually. I thought it would change the prompt language by setting SUMMARY_LANGUAGE as  \"Chinese\", which I did but it seems nothing happens XD. ",
          "created_at": "2025-04-14T08:20:06Z"
        }
      ]
    },
    {
      "issue_number": 1204,
      "title": "[Bug]: <title>The index is out of range after uploading the file",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n我调上传文件的接口后，文本块处理完显示索引超出范围。\n\n### Steps to reproduce\n\n调用http://localhost:9621/documents/upload  接口上传文件后台处理出现错误\n\n### Expected Behavior\n\n上传成功啊\n\n### LightRAG Config Used\n\nUbuntu 24.04\nA16  显卡\nLLM :  ollama 的 llama3.1:8B\nEMBEEDING : ollama 的mxbai-embed-large:latest\n\n### Logs and screenshots\n\n(deduplicated)\n2025-03-27 04:11:07,391 - lightrag - INFO -   Chunk 253/258: extracted 30 entities and 5 relationships (deduplicated)\n2025-03-27 04:11:07,393 - lightrag - INFO -   Chunk 254/258: extracted 90 entities and 4 relationships (deduplicated)\n2025-03-27 04:11:07,394 - lightrag - INFO -   Chunk 255/258: extracted 4 entities and 6 relationships (deduplicated)\n2025-03-27 04:11:07,394 - lightrag - INFO -   Chunk 256/258: extracted 45 entities and 0 relationships (deduplicated)\n2025-03-27 04:11:07,395 - lightrag - INFO -   Chunk 257/258: extracted 0 entities and 18 relationships (deduplicated)\n2025-03-27 04:11:07,396 - lightrag - INFO -   Chunk 258/258: extracted 13 entities and 0 relationships (deduplicated)\n2025-03-27 04:11:07,414 - lightrag - ERROR - Failed to process document doc-2a279080c1fcc5232bcca56d37e55689: index 277 is out of bounds for axis 0 with size 277\n\n### Additional Information\n\n- LightRAG Version: v1.2.6\n- Operating System: linux   Ubuntu 24.04\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "closed",
      "author": "xtao783",
      "author_type": "User",
      "created_at": "2025-03-27T07:52:07Z",
      "updated_at": "2025-04-14T07:46:23Z",
      "closed_at": "2025-04-01T07:44:39Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1204/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1204",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1204",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:45.513017",
      "comments": [
        {
          "author": "sir3mat",
          "body": "how did you solve this?\n",
          "created_at": "2025-04-14T07:46:22Z"
        }
      ]
    },
    {
      "issue_number": 1320,
      "title": "[Question]: Does LightRAG offer a built-in document id filtering",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nDoes LightRAG offer a built-in document id filtering that is done with ids parameter in QueryParam? If not, will it be the case in near future? Thanks :)\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "CaglayanSiarDilsiz",
      "author_type": "User",
      "created_at": "2025-04-09T08:55:19Z",
      "updated_at": "2025-04-14T07:36:19Z",
      "closed_at": "2025-04-14T07:36:19Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1320/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1320",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1320",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:45.683459",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "There problem doing document id filtering with Graph base RAG, because the entity and relation summarize can not be filter by id. The best way of filter doc-ids is create an new workspace and reindex doc-ids. LightRAG is planning to support workspace in near future.",
          "created_at": "2025-04-09T11:04:58Z"
        },
        {
          "author": "CaglayanSiarDilsiz",
          "body": "Then current state of LightRAG only allows filtering the \"text chunks\" if one uses PGVector. Is this statement correct? ",
          "created_at": "2025-04-09T12:47:16Z"
        },
        {
          "author": "danielaskdd",
          "body": "This modification originates from community code and is not officially recommended, so the related functionality is not implemented in other storage systems. Simply filtering certain document chunks is incorrect. Because even after filtering out chunks, the answers can still be influenced by the des",
          "created_at": "2025-04-09T12:59:10Z"
        },
        {
          "author": "CaglayanSiarDilsiz",
          "body": "Thanks for the clarification :) Could you please explain what features the planned workspace support will include? Will there be an optimization to merge workspaces with minimal LLM call? Also, is there an estimated timeline for this workspace feature? Thanks.",
          "created_at": "2025-04-09T13:41:33Z"
        },
        {
          "author": "danielaskdd",
          "body": "LLM cache can significantly speed up the process of setting up workspace using existing docs. However, we cannot yet provide a definitive timeline for the release of the Workspace feature, as this depends on the resolution speed of some existing urgent issues.",
          "created_at": "2025-04-09T14:13:08Z"
        }
      ]
    },
    {
      "issue_number": 1231,
      "title": "[Bug]: <title>Failed to extract entities and relationships version::1.3.0",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nUse examples/openai-compatible-demo.py remind me Failed to extract entities and relationships\nthe last version::1.2.3 I use is OK to extract\n\n`INFO: Process 16352 Shared-Data already initialized (multiprocess=False)\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data\nINFO: Process 16352 storage namespace already initialized: [full_docs]\nINFO: Process 16352 storage namespace already initialized: [text_chunks]\nINFO: Process 16352 storage namespace already initialized: [llm_response_cache]\nINFO: Process 16352 storage namespace already initialized: [doc_status]\nINFO: Process 16352 storage namespace already initialized: [full_docs]\nINFO: Process 16352 storage namespace already initialized: [text_chunks]\nINFO: Process 16352 storage namespace already initialized: [llm_response_cache]\nINFO: Process 16352 storage namespace already initialized: [doc_status]\nFailed to extract entities and relationships\nFailed to process document doc-4df872eb6c198dbe276d7021c18ef71b: 'entity_continue_extraction'\nDetected embedding dimension: 1024\n`\n\n### Additional Information\n\n- LightRAG Version:1.3.0\n- Operating System:win11\n- Python Version:3.12\n- Related Issues:\n",
      "state": "closed",
      "author": "Buzeg",
      "author_type": "User",
      "created_at": "2025-03-30T08:51:21Z",
      "updated_at": "2025-04-14T06:58:10Z",
      "closed_at": "2025-03-31T03:50:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1231/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1231",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1231",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:45.875394",
      "comments": [
        {
          "author": "Buzeg",
          "body": "一更新怎么全都用不了了",
          "created_at": "2025-03-30T09:01:19Z"
        },
        {
          "author": "fscgod",
          "body": "> 一更新怎么全都用不了了\n你好，请问你解决了吗\n\n",
          "created_at": "2025-04-01T08:37:30Z"
        },
        {
          "author": "Buzeg",
          "body": "> > 一更新怎么全都用不了了\n> > 你好，请问你解决了吗\n\n你好，已经解决了",
          "created_at": "2025-04-01T08:48:18Z"
        },
        {
          "author": "fscgod",
          "body": "> > > 一更新怎么全都用不了了\n> > > 你好，请问你解决了吗\n> \n> 你好，已经解决了\n\n请问具体是怎么解决的呢",
          "created_at": "2025-04-01T08:54:34Z"
        },
        {
          "author": "nguyenthekhoig7",
          "body": "Same issue here, I still can not solve\n\nI tried both:\n- Clone the code and run `pip install -e .` --> `pip list | grep lightrag` returned the version was `1.3.2`\n- Run `pip install \"lightrag-hku[api]\"==1.3.1`\n\nBut still have the issue:\n> ERROR - Failed to extract entities and relationships\n---\n@Buze",
          "created_at": "2025-04-14T06:28:09Z"
        }
      ]
    },
    {
      "issue_number": 1365,
      "title": "[Bug]: Unlisted dependencies on clean installation",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nDependencies on pipmaster and graspologic (and others) are not declared in setup.py causing setup via `pip install lightrag-hku` per the Readme and example snippet to fail.  \n\n```\nFile \"/Users/jspv/src/app/.venv/lib/python3.11/site-packages/lightrag/llm/openai.py\", line 10, in <module>\n    import pipmaster as pm  # Pipmaster for dynamic library install\nModuleNotFoundError: No module named 'pipmaster'\n```\n```\n  File \"/Users/jspv/src/app/.venv/lib/python3.11/site-packages/lightrag/kg/networkx_impl.py\", line 19, in <module>\n    from graspologic import embed\nModuleNotFoundError: No module named 'graspologic'\n```\n\nEdit: To get the example to run, the following lightrag-hku dependencies were needed:\n\n    \"future>=1.0.0\",\n    \"graspologic>=3.4.1\",\n    \"nano-vectordb>=0.0.4.3\",\n    \"pipmaster>=0.5.4\",\n\nNote: I then hit [bug 1251](https://github.com/HKUDS/LightRAG/issues/1251) which seems to not have a solution at this point.  \n\n### Steps to reproduce\n\nClean setup.  Per Readme:\n\n- pip install lightrag-hku\n- curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt > ./book.txt\n- save and run the example snippet (modifying \"your text\" to use the downloaded book)\n\n\n### Expected Behavior\n- installing via pip should also install all necessary dependencies for lightrag-hku and/or if the dependencies are specific to the calling application (the example), they should be there.  \n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.3.1\n- Operating System: MacOS 15.4\n- Python Version: 3.11\n- Related Issues:\n",
      "state": "open",
      "author": "jspv",
      "author_type": "User",
      "created_at": "2025-04-13T14:09:14Z",
      "updated_at": "2025-04-13T21:21:45Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1365/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1365",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1365",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:46.048902",
      "comments": []
    },
    {
      "issue_number": 1288,
      "title": "[Feature Request]: The Graph Viewer supports node editing and node merging functionality.",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nDetailed requirements are pending supplementation\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-06T19:28:29Z",
      "updated_at": "2025-04-13T19:15:52Z",
      "closed_at": "2025-04-13T19:15:52Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1288/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1288",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1288",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:46.048923",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Close by duplicated #1343 ",
          "created_at": "2025-04-13T19:15:52Z"
        }
      ]
    },
    {
      "issue_number": 1286,
      "title": "[Feature Request]:  Enhance WebUI Streaming Response Support",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nThe WebUI's query functionality does not adequately support streaming responses. The following improvements are needed:  \n\n1. When selecting streaming responses for queries, the WebUI should process LightRAG's responses according to streaming response specifications.  \n2. Switching tabs during a streaming response should not interrupt the response (currently, tab switching only hides the page and should not require special handling).  \n3. Only session history should be maintained. The session history should be stored in the backend.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-06T19:03:46Z",
      "updated_at": "2025-04-13T19:13:10Z",
      "closed_at": "2025-04-13T19:12:00Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1286/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1286",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1286",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:46.220995",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Session history is not implemented.",
          "created_at": "2025-04-13T19:13:09Z"
        }
      ]
    },
    {
      "issue_number": 1336,
      "title": "[Question]: AsyncHttpxClientWrapper' object has no attribute '_transport'",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nInstall from source, python version 3.12, and startup lightrag-server, \nwhen upload pdf or txt, always fail,\ni tried v1.3.0 and v1.3.1, the same error.\nlogs below:\n\nINFO: 127.0.0.1:50322 - \"POST /documents/upload HTTP/1.1\" 200\nINFO: Inserting 1 records to doc_status\nINFO: Process 8648 doc status writting 1 records to doc_status\nINFO: Stored 1 new unique documents\nINFO: Successfully fetched and enqueued file: 5bb73f9e32a84052af8140b8ca2ac986.pdf\nINFO: Processing 1 document(s) in 1 batches\nINFO: Start processing batch 1 of 1.\nINFO: Inserting 1 records to doc_status\nINFO: Process 8648 doc status writting 1 records to doc_status\nINFO: Inserting 12 to chunks\nINFO: Inserting 1 records to full_docs\nINFO: Inserting 12 records to text_chunks\nERROR: Failed to extract entities and relationships\nERROR: Failed to process document doc-95c04ecb2655605652ea88e8bc1dab85: Unknown scheme for proxy URL URL('socks://127.0.0.1:7890/')\nINFO: Inserting 1 records to doc_status\nINFO: Process 8648 doc status writting 1 records to doc_status\nINFO: Process 8648 KV writting 1 records to full_docs\nINFO: Process 8648 KV writting 12 records to text_chunks\nINFO: Writing graph with 0 nodes, 0 edges\nERROR:asyncio:Task exception was never retrieved\nfuture: <Task finished name='Task-249' coro=<AsyncClient.aclose() done, defined at /home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py:1978> exception=AttributeError(\"'AsyncHttpxClientWrapper' object has no attribute '_transport'\")>\nTraceback (most recent call last):\n  File \"/home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1985, in aclose\n    await self._transport.aclose()\n          ^^^^^^^^^^^^^^^\nAttributeError: 'AsyncHttpxClientWrapper' object has no attribute '_transport'\nERROR:asyncio:Task exception was never retrieved\nfuture: <Task finished name='Task-250' coro=<AsyncClient.aclose() done, defined at /home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py:1978> exception=AttributeError(\"'AsyncHttpxClientWrapper' object has no attribute '_transport'\")>\nTraceback (most recent call last):\n  File \"/home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1985, in aclose\n    await self._transport.aclose()\n          ^^^^^^^^^^^^^^^\nAttributeError: 'AsyncHttpxClientWrapper' object has no attribute '_transport'\nERROR:asyncio:Task exception was never retrieved\nfuture: <Task finished name='Task-251' coro=<AsyncClient.aclose() done, defined at /home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py:1978> exception=AttributeError(\"'AsyncHttpxClientWrapper' object has no attribute '_transport'\")>\nTraceback (most recent call last):\n  File \"/home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1985, in aclose\n    await self._transport.aclose()\n          ^^^^^^^^^^^^^^^\nAttributeError: 'AsyncHttpxClientWrapper' object has no attribute '_transport'\nERROR:asyncio:Task exception was never retrieved\nfuture: <Task finished name='Task-252' coro=<AsyncClient.aclose() done, defined at /home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py:1978> exception=AttributeError(\"'AsyncHttpxClientWrapper' object has no attribute '_transport'\")>\nTraceback (most recent call last):\n  File \"/home/ted/LightRAG/venv/lib/python3.12/site-packages/httpx/_client.py\", line 1985, in aclose\n    await self._transport.aclose()\n          ^^^^^^^^^^^^^^^\nAttributeError: 'AsyncHttpxClientWrapper' object has no attribute '_transport'\nINFO: In memory DB persist to disk\nINFO: Completed batch 1 of 1.\nINFO: Document processing pipeline completed\n\n\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "zghnwsq",
      "author_type": "User",
      "created_at": "2025-04-10T13:16:45Z",
      "updated_at": "2025-04-12T12:41:54Z",
      "closed_at": "2025-04-12T12:41:54Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1336/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1336",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1336",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:46.410106",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Make sure you LLM and Embedding configuration is correct. https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README-zh.md",
          "created_at": "2025-04-10T17:30:16Z"
        }
      ]
    },
    {
      "issue_number": 1335,
      "title": "[Feature Request]: Add zh_TW (Traditional Chinese, Taiwan) to i18n",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nHello team,\n\nI would like to propose adding support for Traditional Chinese (Taiwan), represented by the locale code zh_TW, to the internationalization (i18n) of the project.\n\nWould it be possible for me to take on the task of implementing this feature? I am happy to contribute to this effort.\n\nThank you for your consideration.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "iLuJack",
      "author_type": "User",
      "created_at": "2025-04-10T13:14:23Z",
      "updated_at": "2025-04-12T09:02:01Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "ui"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1335/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1335",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1335",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:46.599075",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Certainly. Welcome to the LightRAG community.",
          "created_at": "2025-04-10T17:21:07Z"
        }
      ]
    },
    {
      "issue_number": 1311,
      "title": "[Feature Request]: <title>提取entity和relationship的prompt什么时候支持中文呢？",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n_No response_\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "whaleprince",
      "author_type": "User",
      "created_at": "2025-04-08T08:00:25Z",
      "updated_at": "2025-04-12T08:50:13Z",
      "closed_at": "2025-04-12T08:50:13Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1311/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1311",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1311",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:46.770560",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You can config the summary language in .env file:\n\n```\nSUMMARY_LANGUAGE=Chinese\n```\n\n<img width=\"702\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/30a9e957-c5fc-4e31-821c-e2f9bc2d5805\" />",
          "created_at": "2025-04-08T11:07:01Z"
        }
      ]
    },
    {
      "issue_number": 1358,
      "title": "[Bug]: 'file_path' Error in Mix Mode Retrieval",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n## Issue Description\nWhen using LightRAG's mix mode for queries, the following error consistently appears in logs:\n```\nERROR: Error in get_vector_context: 'file_path'\n```\n\nDespite this error, the mix mode continues to function and returns results successfully. The error appears to be non-critical but may indicate an underlying issue in the vector context retrieval process.\n\n## Environment\n- Python version: 3.10\n- OpenAI API version: 1.72.0\n- LightRAG version: 1.3.1 (lightrag-hku)\n- OS: Linux (Pop_OS)\n\n## Steps to Reproduce\n1. Initialize LightRAG with OpenAI embeddings and GPT-4o-mini for completions\n2. Add a document using `ainsert()`\n3. Query using mix mode with `aquery()` and `QueryParam(mode=\"mix\")`\n\n## Code Sample\n```python\nimport os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete, openai_embed\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\nasync def main():\n    # Initialize\n    rag = LightRAG(\n        working_dir=\"./resources/lightrag_test\",\n        embedding_func=openai_embed,\n        llm_model_func=gpt_4o_mini_complete\n    )\n    \n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n    \n    # Add document\n    doc_id = await rag.ainsert(\"Your test document content here\")\n    \n    # Query with mix mode\n    result = await rag.aquery(\n        \"What is in this document?\",\n        param=QueryParam(mode=\"mix\")\n    )\n    \n    # The error occurs during this query but doesn't prevent results\n    \nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Debug Information\nThe error occurs during the retrieval step in mix mode:\n```\nINFO: Process XXX buidling query context...\nINFO: Query nodes: [...], top_k: 60, cosine: 0.2\nINFO: Query edges: [...], top_k: 60, cosine: 0.2\nERROR: Error in get_vector_context: 'file_path'\n```\n\nThe error doesn't appear with other modes (naive, local, global, hybrid) - only with mix mode.\n\n## Attempted Solutions\nI've tried various parameter adjustments with QueryParam, including:\n- Reducing token limits\n- Adjusting top_k values\n- Providing explicit keywords\n- Modifying context parameters\n\nNone of these adjustments resolved the issue.\n\n## Impact\nWhile the error doesn't seem to prevent getting results from the mix mode, it may:\n- Indicate missing functionality\n- Affect result quality in ways that aren't immediately obvious\n- Create unnecessary log noise\n\n## Additional Notes\n- All storage directories and files appear to be properly initialized\n- The mix mode successfully retrieves and combines information despite the error\n- Debug-level logging shows the error occurs during vector context retrieval\n\nAny assistance in resolving this issue would be appreciated. If additional debug information is needed, please let me know.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n\n",
      "state": "open",
      "author": "LinuxIsCool",
      "author_type": "User",
      "created_at": "2025-04-12T02:19:17Z",
      "updated_at": "2025-04-12T08:35:14Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1358/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1358",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1358",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:46.967509",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Kindly update to the latest version (1.3.1) and perform a complete reindex of all your files.",
          "created_at": "2025-04-12T08:35:13Z"
        }
      ]
    },
    {
      "issue_number": 1355,
      "title": "[Feature Request]: insert faster, general documents are only saved to vectorDB, only important documents are saved to vectorDB+graphDB",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nFor large scale data, in order to insert faster, general documents are only saved to vectorDB, only important documents are saved to both vectorDB and graphDB.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhouzhou12",
      "author_type": "User",
      "created_at": "2025-04-11T09:24:42Z",
      "updated_at": "2025-04-12T03:11:36Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1355/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1355",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1355",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:47.146886",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The default hybrid query mode currently only searches the vector database, which may result in incomplete document retrieval if  only important documents are saved to both vector and graph DB . Could you share more about your specific use case requirements?",
          "created_at": "2025-04-12T00:48:28Z"
        },
        {
          "author": "zhouzhou12",
          "body": "We have a large knowledge data, maybe more than 1GB. We use a 14B-parameter LLM on two GPUs.\nIf saving all the data to both vector database and graph database, it will take a very long time.\nIf the data is divided into important and unimportant data, with the unimportant data stored only in the vect",
          "created_at": "2025-04-12T03:11:28Z"
        }
      ]
    },
    {
      "issue_number": 1143,
      "title": "[Question]: <title>no response running hf_demo",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nterminal output:\nINFO: Process 606303 Shared-Data created for Single Process\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_chunks.json'} 0 data\nINFO: Process 606303 initialized updated flags for namespace: [full_docs]\nINFO: Process 606303 ready to initialize storage namespace: [full_docs]\nINFO: Process 606303 initialized updated flags for namespace: [text_chunks]\nINFO: Process 606303 ready to initialize storage namespace: [text_chunks]\nINFO: Process 606303 initialized updated flags for namespace: [entities]\nINFO: Process 606303 initialized updated flags for namespace: [relationships]\nINFO: Process 606303 initialized updated flags for namespace: [chunks]\nINFO: Process 606303 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 606303 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 606303 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 606303 initialized updated flags for namespace: [doc_status]\nINFO: Process 606303 ready to initialize storage namespace: [doc_status]\nINFO: Process 606303 storage namespace already initialized: [full_docs]\nINFO: Process 606303 storage namespace already initialized: [text_chunks]\nINFO: Process 606303 storage namespace already initialized: [llm_response_cache]\nINFO: Process 606303 storage namespace already initialized: [doc_status]\nINFO: Process 606303 Pipeline namespace initialized\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.36s/it]\n/media/mldadmin/home/s124mdg31_02/miniconda3/envs/light/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n\n\nAlso found that only kv_store_doc_status.json was created in ./dickens\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhuamaaa",
      "author_type": "User",
      "created_at": "2025-03-20T15:09:55Z",
      "updated_at": "2025-04-11T16:50:09Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1143/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1143",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1143",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:47.415499",
      "comments": [
        {
          "author": "Arris233",
          "body": "I've also encountered this question,may i ask if you've solved it?",
          "created_at": "2025-04-11T15:31:23Z"
        },
        {
          "author": "zhuamaaa",
          "body": "i originally used qwen and encountered this problem, then i tried gemma and got response ",
          "created_at": "2025-04-11T16:50:07Z"
        }
      ]
    },
    {
      "issue_number": 1346,
      "title": "[Question]: <title> 'ascii' codec can't encode characters",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n我使用lightrag-server时，llm模型使用gpt-4o时运行正常，但是使用火山引擎的豆包或DeepSeek以及阿里的通义千问时，会发生如下错误。\nParams: {'temperature': 0.0}, Got: 'ascii' codec can't encode characters in position 46-47: ordinal not in range(128)\n我尝试在.env文件中设置PYTHONIOENCODING=utf-8，但是还是未得到解决。\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "luo-xingyu",
      "author_type": "User",
      "created_at": "2025-04-11T03:18:15Z",
      "updated_at": "2025-04-11T04:40:50Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1346/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1346",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1346",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:47.657133",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "建议使用Deepseek官方的API测试一下。使用火山方舟的Deepseek，大部份情况都整差功能，但有时候会触发合规审核，导致返回的结果为空。但没有遇到过您说描述的问题。",
          "created_at": "2025-04-11T04:40:48Z"
        }
      ]
    },
    {
      "issue_number": 1347,
      "title": "[Question]: <title>怎么修改图标",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n我想应用项目，修改图标文字这些，但是我实在找不到在哪些文件中修改这些内容\n\n### Additional Context\n\n![Image](https://github.com/user-attachments/assets/a7180f3b-5d95-46db-809e-0d32f29c93d2)",
      "state": "closed",
      "author": "wenquxing1",
      "author_type": "User",
      "created_at": "2025-04-11T03:36:57Z",
      "updated_at": "2025-04-11T04:02:36Z",
      "closed_at": "2025-04-11T04:02:36Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1347/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1347",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1347",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:47.894159",
      "comments": []
    },
    {
      "issue_number": 853,
      "title": "How to Contribute to LightRAG",
      "body": "# 🚀 Handy Tips for Developers Contributing to the Project\n\n### Before you start\n\n* If you come across an issue labeled with [Feature Request] and are interested in contributing to it, please leave a comment on the issue to confirm its objectives and proposed solution before starting to write code. \n* If you have your own ideas or features to implement, first create a [Feature Request] issue, clearly outlining the background, goals, and proposed solution before beginning development. This process helps avoid duplicating work or proposing solutions that may not align with the community's expectations. \n* If you simply find a bug that needs fixing, you can directly make the changes and submit a PR.\n\n### ✅ Setup\n\nBefore committing your changes, please ensure to follow this steps.\n\n### 📖 Setup Guide\n\n## 1️⃣ Install the Project\n• Clone the LightRAG repository: `git clone <repository_url>`\n• Navigate to the root directory of the project: `cd LightRAG`\n• Install dependencies: `pip install . -e`\n\n## 2️⃣ Install and Run Pre-commit Hooks\n• Install pre-commit using pip: `pip install pre-commit`\n• Initialize pre-commit in your repository: `pre-commit install`\n• Run pre-commit hooks manually: `pre-commit run --all-files`\n\n## 3️⃣ Before Creating a Pull Request\n\nTo ensure all CI validations pass before opening a pull request, run: `pre-commit run --all-files`\n\nFix any reported issues, and you’re good to go! 🚀\n\nThanks 🙏🏻",
      "state": "open",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-19T08:06:55Z",
      "updated_at": "2025-04-11T01:29:54Z",
      "closed_at": null,
      "labels": [
        "documentation",
        "information"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/853/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/853",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/853",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:47.894329",
      "comments": []
    },
    {
      "issue_number": 1140,
      "title": "[Question]: Cannot find the Username and Password when logging into WebUI",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI'm following this guide for setup:\nhttps://github.com/HKUDS/LightRAG/blob/main/lightrag_webui/README.md\n\nSystem Info:\n- OS: Linux (Ubuntu)\n- Bun version: Bun 1.2.5\n\n(I'm not sure if it's relevant, but…) \nI have already set up a Neo4j server on my Linux OS.\n\n### Additional Context\n\n![Image](https://github.com/user-attachments/assets/38317d85-dede-4a85-badd-1908940c8456)",
      "state": "closed",
      "author": "TzuC-tw",
      "author_type": "User",
      "created_at": "2025-03-20T10:11:08Z",
      "updated_at": "2025-04-11T01:14:29Z",
      "closed_at": "2025-03-20T15:07:49Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1140/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1140",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1140",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:49.646603",
      "comments": [
        {
          "author": "Dragonliu2018",
          "body": "make sure the API server is already installed. refer to https://github.com/HKUDS/LightRAG/tree/main/lightrag/api#readme\n\nand open url like http://localhost:9621/webui/#/login,  not http://localhost:5173/webui/#/login .",
          "created_at": "2025-03-20T13:35:23Z"
        },
        {
          "author": "danielaskdd",
          "body": "User name and password is config by .env file. You can find a sample in project root: env.example",
          "created_at": "2025-03-20T14:30:53Z"
        },
        {
          "author": "TzuC-tw",
          "body": "Thank you very much! After installing the API server and running `lightrag-server`, I was able to successfully log in to the web page at http://localhost:9621/webui/#/login using the parameters from `env.example`.",
          "created_at": "2025-03-20T15:06:43Z"
        },
        {
          "author": "wenquxing1",
          "body": "What is your LLM model and Embedding model? I do not use ollama, but it is reported that my ollama is not configured\n（ERROR: Failed to process document doc-f282f53ef0d04650be2785583fbad574: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.\nc",
          "created_at": "2025-03-28T07:49:34Z"
        },
        {
          "author": "noboomu",
          "body": "I don't see the a default username or password in the env.example for the webui... What are the environment variable names?",
          "created_at": "2025-04-10T21:53:13Z"
        }
      ]
    },
    {
      "issue_number": 1300,
      "title": "[Bug]: LightRAG server hangs after running for ~3 hours",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nMy INPUT_DIR has about 500 PDFs of scientific papers. The lightrag server initially runs totally fine, but after about 3 hours, there is a time-out error and then the server totally got frozen. If I kill the current job and rerun the lightrag-server, it is able to continue from the previous stage for another few hours and then the same error occurs. This has been consistent so far - I have killed and restarted it for more than five times now and the behaviour has been consistent. I have checked the memory usage and it's totally fine. The LLM is hosted locally and seems to be running fine from vLLM's log file (and LLM seems to receive no http requests after the server hangs). I wonder what might be causing this? Maybe one process in LightRAG got frozen and the whole thing is somehow waiting for that? \n\n### Steps to reproduce\n\nlightrag-gunicorn --workers 4 --auto-scan-at-startup --log-level DEBUG --verbose\n\n\n### Expected Behavior\n\nThe server should be able to process all the files until no new files are found.\n\n### LightRAG Config Used\n\n# Paste your config here\nHOST=0.0.0.0\nPORT=9621\nWORKERS=3\n### separating data from difference Lightrag instances\n# NAMESPACE_PREFIX=lightrag\n### Max nodes return from grap retrieval\nMAX_GRAPH_NODES=1000\n# CORS_ORIGINS=http://localhost:3000,http://localhost:8080\n\n### Optional SSL Configuration\n# SSL=true\n# SSL_CERTFILE=/path/to/cert.pem\n# SSL_KEYFILE=/path/to/key.pem\n\n### Directory Configuration (defaults to current working directory)\nWORKING_DIR=./lightrag_server/WORKING_DIR\nINPUT_DIR=../geneReviews_subsample500\n\n### Ollama Emulating Model Tag\nOLLAMA_EMULATING_MODEL_TAG=latest\n\n### Logging level\nLOG_LEVEL=INFO\nVERBOSE=False\nLOG_MAX_BYTES=10485760\nLOG_BACKUP_COUNT=5\n### Logfile location (defaults to current working directory)\n# LOG_DIR=/path/to/log/directory\n\n### Settings for RAG query\nHISTORY_TURNS=3\nCOSINE_THRESHOLD=0.2\nTOP_K=60\nMAX_TOKEN_TEXT_CHUNK=4000\nMAX_TOKEN_RELATION_DESC=4000\nMAX_TOKEN_ENTITY_DESC=4000\n\n### Settings for document indexing\nENABLE_LLM_CACHE_FOR_EXTRACT=true\nSUMMARY_LANGUAGE=English\nCHUNK_SIZE=1200\nCHUNK_OVERLAP_SIZE=100\n### Max tokens for entity or relations summary\nMAX_TOKEN_SUMMARY=500\n### Number of parallel processing documents in one patch\nMAX_PARALLEL_INSERT=8\n\n### Num of chunks send to Embedding in single request\nEMBEDDING_BATCH_NUM=32\n### Max concurrency requests for Embedding\nEMBEDDING_FUNC_MAX_ASYNC=16\nMAX_EMBED_TOKENS=32000\n\n### LLM Configuration\n### Time out in seconds for LLM, None for infinite timeout\nTIMEOUT=150\n### Some models like o1-mini require temperature to be set to 1\nTEMPERATURE=0.7\n### Max concurrency requests of LLM\nMAX_ASYNC=32\n### Max tokens send to LLM (less than context size of the model)\nMAX_TOKENS=51200\n\n### Ollama example (For local services installed with docker, you can use host.docker.internal as host)\n# LLM_BINDING=ollama\n# LLM_MODEL=mistral-nemo:latest\n# LLM_BINDING_API_KEY=your_api_key\n# LLM_BINDING_HOST=http://localhost:11434\n\n### OpenAI alike example\nLLM_BINDING=openai\nLLM_MODEL=Llama-3.3-70B-Instruct\nLLM_BINDING_HOST=http://localhost:8000/v1\nLLM_BINDING_API_KEY=your_api_key\n### lollms example\n# LLM_BINDING=lollms\n# LLM_MODEL=mistral-nemo:latest\n# LLM_BINDING_HOST=http://localhost:9600\n# LLM_BINDING_API_KEY=your_api_key\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nEMBEDDING_MODEL=gte-Qwen2-7B-instruct\nEMBEDDING_DIM=3584\nEMBEDDING_BINDING_API_KEY=your_api_key\n### ollama example\n# EMBEDDING_BINDING=ollama\n# EMBEDDING_BINDING_HOST=http://localhost:11434\n### OpenAI alike example\nEMBEDDING_BINDING=openai\nEMBEDDING_BINDING_HOST=http://localhost:8000/v1\n### Lollms example\n# EMBEDDING_BINDING=lollms\n# EMBEDDING_BINDING_HOST=http://localhost:9600\n\n### Optional for Azure (LLM_BINDING_HOST, LLM_BINDING_API_KEY take priority)\n# AZURE_OPENAI_API_VERSION=2024-08-01-preview\n# AZURE_OPENAI_DEPLOYMENT=gpt-4o\n# AZURE_OPENAI_API_KEY=your_api_key\n# AZURE_OPENAI_ENDPOINT=https://myendpoint.openai.azure.com\n\n# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large\n# AZURE_EMBEDDING_API_VERSION=2023-05-15\n\n### Data storage selection\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\n### Oracle Database Configuration\nORACLE_DSN=localhost:1521/XEPDB1\nORACLE_USER=your_username\nORACLE_PASSWORD='your_password'\nORACLE_CONFIG_DIR=/path/to/oracle/config\n#ORACLE_WALLET_LOCATION=/path/to/wallet\n#ORACLE_WALLET_PASSWORD='your_password'\n### separating all data from difference Lightrag instances(deprecating, use NAMESPACE_PREFIX in future)\n#ORACLE_WORKSPACE=default\n\n### TiDB Configuration\nTIDB_HOST=localhost\nTIDB_PORT=4000\nTIDB_USER=your_username\nTIDB_PASSWORD='your_password'\nTIDB_DATABASE=your_database\n### separating all data from difference Lightrag instances(deprecating, use NAMESPACE_PREFIX in future)\n#TIDB_WORKSPACE=default\n\n### PostgreSQL Configuration\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_USER=your_username\nPOSTGRES_PASSWORD='your_password'\nPOSTGRES_DATABASE=your_database\n### separating all data from difference Lightrag instances(deprecating, use NAMESPACE_PREFIX in future)\n#POSTGRES_WORKSPACE=default\n\n### Independent AGM Configuration(not for AMG embedded in PostreSQL)\nAGE_POSTGRES_DB=\nAGE_POSTGRES_USER=\nAGE_POSTGRES_PASSWORD=\nAGE_POSTGRES_HOST=\n# AGE_POSTGRES_PORT=8529\n\n### separating all data from difference Lightrag instances(deprecating, use NAMESPACE_PREFIX in future)\n# AGE Graph Name(apply to PostgreSQL and independent AGM)\n# AGE_GRAPH_NAME=lightrag\n\n### Neo4j Configuration\nNEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD='your_password'\n\n### MongoDB Configuration\nMONGO_URI=mongodb://root:root@localhost:27017/\nMONGO_DATABASE=LightRAG\n### separating all data from difference Lightrag instances(deprecating, use NAMESPACE_PREFIX in future)\n# MONGODB_GRAPH=false\n\n### Milvus Configuration\nMILVUS_URI=http://localhost:19530\nMILVUS_DB_NAME=lightrag\n# MILVUS_USER=root\n# MILVUS_PASSWORD=your_password\n# MILVUS_TOKEN=your_token\n\n### Qdrant\nQDRANT_URL=http://localhost:16333\n# QDRANT_API_KEY=your-api-key\n\n### Redis\nREDIS_URI=redis://localhost:6379\n\n### For JWT Auth\n# AUTH_ACCOUNTS='admin:admin123,user1:pass456'\n# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server\n# TOKEN_EXPIRE_HOURS=4\n\n### API-Key to access LightRAG Server API\nWHITELIST_PATHS=/health,/api/*\n\n### Logs and screenshots\n\n######################\nOutput:\n2025-04-07 03:40:18,482 - lightrag - DEBUG - ===== Sending Query to LLM =====\n2025-04-07 03:40:18,974 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:19,428 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:19,881 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:20,334 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:20,788 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:21,241 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:21,695 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:22,156 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:22,613 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:23,066 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:23,520 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:23,973 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:24,426 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:24,881 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:25,334 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:25,830 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:26,284 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:26,737 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:27,191 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:27,645 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:28,099 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:28,555 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:29,010 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:29,463 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:29,957 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:30,411 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:30,865 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:31,318 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:31,776 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:32,241 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:32,703 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:33,163 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:33,622 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:34,083 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:34,551 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:35,018 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:35,476 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:35,975 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:36,434 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:36,892 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n2025-04-07 03:40:37,351 - lightrag - DEBUG - Non-embedding cached missed(mode:default type:extract)\n**2025-04-07 03:40:37,569 [CRITICAL] gunicorn.error: WORKER TIMEOUT (pid:68900)\n2025-04-07 03:40:41,868 [ERROR] gunicorn.error: Worker (pid:68900) was sent SIGKILL! Perhaps out of memory?\n2025-04-07 03:40:41,911 [INFO] gunicorn.error: Booting worker with pid: 21329\n2025-04-07 03:40:42,042 - lightrag - DEBUG - Initialized Storages**\n\n### Additional Information\n\n- LightRAG Version: v1.3.1\n- Operating System: CentOS Linux 7\n- Python Version: 3.9\n- Related Issues: N/A\n",
      "state": "closed",
      "author": "hyl317",
      "author_type": "User",
      "created_at": "2025-04-07T13:07:02Z",
      "updated_at": "2025-04-10T19:57:24Z",
      "closed_at": "2025-04-10T19:57:24Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1300/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1300",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1300",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:49.883187",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "To verify the behavior, please test the single-process mode `lightrag-server` instead  to check if the hanging issue persists. Note that multi-worker configuration is not designed to accelerate file indexing operations. Its primary purpose is to prevent query processing from being blocked during fil",
          "created_at": "2025-04-07T15:19:30Z"
        },
        {
          "author": "hyl317",
          "body": "thanks for the prompt reply! After setting the worker number to one, the hanging issue disappears. I understand that having multiple workers doesn't parallelize file indexing, but then why would this lead to hanging issues then? In addition, I noticed from my vllm log that after setting # of workers",
          "created_at": "2025-04-08T15:11:21Z"
        },
        {
          "author": "danielaskdd",
          "body": "In previous versions, file indexing in multi-process mode could lead to deadlocks, causing the process to be killed by  timeout . However, this issue would occur immediately after processing the first batch of files, rather than taking hours of indexing jobs. \n\nVersion 1.3.0 has fixed this problem. ",
          "created_at": "2025-04-08T16:08:49Z"
        },
        {
          "author": "danielaskdd",
          "body": "To verify whether MAX_ASYNC functions as intended, I monitored the connection between LightRAG and the Deepseek API server using ss (socket statistics). With MAX_ASYNC set to 4, the observations indicate that it performs as expected.\n\n```\n09:04 $ ss -tunap | grep 116.205.40.120\ntcp   ESTAB      0   ",
          "created_at": "2025-04-09T01:32:22Z"
        },
        {
          "author": "danielaskdd",
          "body": "Please note that during the file indexing process, only a single LLM request is applied in the node and edge merge stages. This is because the current algorithm does not support parallel merging.",
          "created_at": "2025-04-09T06:14:59Z"
        }
      ]
    },
    {
      "issue_number": 1327,
      "title": "[Bug]: Unable to read or process the text provide for Postgres example code",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHard to say if this is bug but it is an issue.  I setup the demo code for using postges (as found on the github respository and used a 10 page article as the test text. \n\nI got the following error while ingesting the document -- but no stack trace ---\n\nnote: I added the following to print the 'rag' object \nrag = await initialize_rag()\n    print(f\"rag apparently initialized {rag}\")\n\n\nNFO: Process 41598 Shared-Data created for Single Process\nINFO: Process 41598 Pipeline namespace initialized\n\n**rag apparently initialized** LightRAG(working_dir='/Users/jm/jmautogpt/lightRag_one//data', kv_storage='PGKVStorage', vector_storage='PGVectorStorage', graph_storage='PGGraphStorage', doc_status_storage='PGDocStatusStorage', log_level=None, log_file_path=None, entity_extract_max_gleaning=1, entity_summary_to_max_tokens=500, chunk_token_size=1200, chunk_overlap_token_size=100, tiktoken_model_name='gpt-4o-mini', chunking_func=<function chunking_by_token_size at 0x13ceb6980>, node_embedding_algorithm='node2vec', node2vec_params={'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, embedding_func=<function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x13e0b5d00>, embedding_batch_num=32, embedding_func_max_async=16, embedding_cache_config={'enabled': False, 'similarity_threshold': 0.95, 'use_llm_check': False}, llm_model_func=<function limit_async_func_call.<locals>.final_decro.<locals>.wait_func at 0x13e0ef6a0>, llm_model_name='glm-4-flashx', llm_model_max_token_size=32768, llm_model_max_async=4, llm_model_kwargs={}, vector_db_storage_cls_kwargs={'cosine_better_than_threshold': 0.2}, namespace_prefix='', enable_llm_cache=True, enable_llm_cache_for_entity_extract=True, max_parallel_insert=2, addon_params={'language': 'English'}, auto_manage_storages_states=False, convert_response_to_json_func=<function convert_response_to_json at 0x11f8b8ea0>, cosine_better_than_threshold=0.2, _storages_status=<StoragesStatus.INITIALIZED: 'initialized'>)\n\n**[note i added a print statement for the message below to keep track of location in code ]**\nawaiting the read of document /Users/jm/jmautogpt/lightRag_one/article_of_medium_length.txt\n\nkeys: {'doc-2697df43809479fc260b3b130dad8ab2'}\nnew_keys: set()\n\nFailed to extract entities and relationships\nFailed to process document doc-2697df43809479fc260b3b130dad8ab2: 未提供api_key，请通过参数或环境变量提供\n\n\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\nimport asyncio\nimport logging\nimport os\nimport time\nfrom dotenv import load_dotenv\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.zhipu import zhipu_complete\nfrom lightrag.llm.ollama import ollama_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\nload_dotenv()\nROOT_DIR = os.environ.get(\"ROOT_DIR\")\nWORKING_DIR = f\"{ROOT_DIR}/data\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# AGE\nos.environ[\"AGE_GRAPH_NAME\"] = \"darpa\"\n\nos.environ[\"POSTGRES_HOST\"] = \"localhost\"\nos.environ[\"POSTGRES_PORT\"] = \"5432\"\nos.environ[\"POSTGRES_USER\"] = \"rag\"\nos.environ[\"POSTGRES_PASSWORD\"] = \"rag\"\nos.environ[\"POSTGRES_DATABASE\"] = \"rag\"\n\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=zhipu_complete,\n        llm_model_name=\"glm-4-flashx\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=lambda texts: ollama_embedding(\n                texts, embed_model=\"bge-m3\", host=\"http://localhost:11434\"\n            ),\n        ),\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        vector_storage=\"PGVectorStorage\",\n        auto_manage_storages_states=False,\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def main():\n    # Initialize RAG instance\n    rag = await initialize_rag()\n    print(f\"rag apparently initialized {rag}\")\n\n    # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n    with open(f\"{ROOT_DIR}/darpa-mindcontrol.txt\", \"r\", encoding=\"utf-8\") as f:\n        print(f\"awaiting the read of document {ROOT_DIR}/article_of_medium_length.txt\")\n        await rag.ainsert(f.read())\n\n    # *****\n\n    # *****\n\n    print(\"==== Trying to test the rag queries ====\")\n    print(\"**** Start Naive Query ****\")\n    start_time = time.time()\n    # Perform naive search\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n    print(f\"Naive Query Time: {time.time() - start_time} seconds\")\n    # Perform local search\n    print(\"**** Start Local Query ****\")\n    start_time = time.time()\n    print(\n        await rag.aquery(\n            \"Is there a document with the name Jack Kruse that you can summarize?\", param=QueryParam(mode=\"local\")\n        )\n    )\n    print(f\"Local Query Time: {time.time() - start_time} seconds\")\n    # Perform global search\n    print(\"**** Start Global Query ****\")\n    start_time = time.time()\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n    print(f\"Global Query Time: {time.time() - start_time}\")\n    # Perform hybrid search\n    print(\"**** Start Hybrid Query ****\")\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n    print(f\"Hybrid Query Time: {time.time() - start_time} seconds\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: lightrag-hku==1.3.0\n- Operating System: Mac OS\n- Python Version: 3.12\n- Related Issues:\n",
      "state": "open",
      "author": "steelliberty",
      "author_type": "User",
      "created_at": "2025-04-09T16:41:04Z",
      "updated_at": "2025-04-10T17:27:46Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1327/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1327",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1327",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:50.072525",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "According the the error message, API Key for LLM is missing. It should be set by  environment variable. LightRAG  Server is much easy for newcomer to try LightRAG easyly. https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README-zh.md",
          "created_at": "2025-04-10T17:27:45Z"
        }
      ]
    },
    {
      "issue_number": 1321,
      "title": "[Bug]: 当使用local/global/naive时，有时会出现乱码",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n当使用local/global/naive时，有时会出现乱码。\n我上传的知识库是使用pdfplumber识别的pdf文件，上传过程正常，但问问题会出现乱码，如下：但是naive模式没有问题。local/global/naive会出现乱码\n`郑州,任职\n\"\n`` unknown\n\n---\",\"没有 ---...\n\n    \"\n---\n\n<\"\n\n   2.。\",\"UNKNOWN\n    10\",\"\",\",\"\",\"\",\"\",\"7.\",\"\",\"\",\"\",\"---……\"\n\n7..\",\"unknown\nSource\n\"\",\"\",\"\",\"\",\"未\n   END\n_source\",\" unknown\n     unknown take.未知\n--- unknown\n``未知\n---未---...\n\n---source ,\"\n\n   \"\n    \"\n<\"\n   \",\"未知\n>\",\"未知\n\n   6\",\"unknown\n``---\n\n\",\"\",\"unknown7源郑州\n\n    \"\n...\n\"\n   \"\n---\n\n源\n---\n\n   ”\n   未知\n   \"\n\n   \"\n\n   \"\n\n``\",\"未知 ---未---=_(\"郑州\",\"\",\"\",\"unknown\n<unknown\n源\n`` unknown ------\n\n\"\n\n---\n \"\"\n\n   _---\n\n\",\"\",\",\"\",\"\",\"\",\"\",\" unknown中..<**\",\"\",\"\",\"unknown\n\"unknown\n\"\",\" unknown---\",\"未知\n\"未知 about\n<\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"unknown\n\"UNKNOWNtones\n       unknown_source源\",\"unknown\n<unknown源\n\",\"\"\n   ……\"\n_source\n    \"\n``***\n郑州1_source源\n_source源桓\n\" unknown\n``\",\"\",\"unknown源\n\n“unknown源\n``未知{\"unknown\n\n   未中\n未知\n   ``郑州 ---郑州 当<…… ...\n\n（\",\"\",\"\"\n\n``郑州\n<……\",\"UNKNOWN Source源\",\"unknown\n\"\",\"<未\n\n   ……\"\n-\",\"\"\n-UNKNOWN---unknown\n\"unknown to be the_source\n-\",\"\",\"\",\"unknown\",\"\",\"unknown\n\",\"unknown微妙.Schema\n< unknown\n\"unknown---未知\n   \"\n\"\",\"\",\"\",\"unknown the_source源_source源源\n\n---\n\",\"\",\"<unknown\n<unknown源_source源部\n\"\",\"unknown ---郑州\",\"\"\n\n（\",\",**...\n未知\n\",\"...\"\n---\n\n\",\"\",\"...\n    \"\n...\n\",\"\",\"\",\" \",\"\"\n< unknown ---------\n\n……\",\"……---\n\n\",\"……\",\"unknown---un_source源_source源\n    unknown\n   ---\n\n---\n\n---\n\n---未\",\"---\n\n  \"\n---\n unknown\n\n---\n\n   未知\n_source源 some the_source_source_source源\n\n---...\n\n---\n\n\",\"unknown至源源_source源_source_source Source\n \n \n   ---\",\"unknown源\n\",\" unknown\n    unknown\n    unknown ---根据请郑州源\",\"\n\n\n   1在郑州\n-unknown\n\"unknown\n\n   \"\n\n---\n\n\",\"unknown源\n   ---\n\n\"...\n   ---\",\"（\",\"unknown源\n\"\n`` � unknownues源选源_source了_source\",\"\",\"\"\n\n\",\"\",\"\",\"unknown源.\",\"unknown_source_source_source_source源源\n   \"\n   \"\n``---\n…… \"\n ...未知\n\n\"...\n   \"\n<未知\n育\n\n---\"\n\n\"\n   \"\n\n   \"\n\n\"\n\n\"\n   \"\n\n< unknown\n<\"\n   _unknown--- unknown\n\",\",\"\"\n\n\",\"\",\"未知\n\n   \"\n   ...\n   1未知\n\n<\"\n``unknown (_\n\n   ……\"\n``\n   \"\n<\",\"\"\n”\n\"\n\n\"\n\"\n\n``未 the_source源源源_source_source\n...\n\"\n``（\",\"\"\n   \"\n源源\n\",\",\"\"\n\"\n\n\"\n---unknown\",\"unknown源_source源_source9unknown_source resume_sourceUNKNOWN\",\"\",\"\"\n   1源\n    \"\n    \"\n<\",\"...\"\n\",\"\",\" unknown\n<\",\"未知---\n<unknown源\",\" \",\"unknown \n---\n\n\",\" unknown\",\"\",\"\"\n<（<unknown_source---unknown ,\",\"unknown源_source\",\"5源 --- � unknown\n   \",\",\"unknown\n\",\"\",\"unknown\n`` unknown源_source\n`` unknown_sourceunknownsourceunknown\n\n”unknown郑州---未知 ------.unknown\n<---\nunknown\"\n---\n\n<unknown继续source源 ---source源的中\n\n   ***\n\n\"\n\n\"\n   \"\n---\n\n unknown\",\"\"\n   1.\",\"unknown Source unknown源源_source unknown\n   ……unknown ---SEP\n......unknown �\",\"unknown\n<< unknown中未知\n\n   _source继续\n\n\"未知郑州源unknown当前\n\n   \"\n\n------\n\n unknown\n\n   \"\n\n    unknown2源\n\n   _郑州\n\n   ---\n\n\",\"郑州\n   ……...\n<\",\"UNKNOWN请郑州\n\n\"\n\n------\n在源\n---\n…unknown ---\n\n<（<”\n---郑州中<unknown继续源.0000\",\"1\"\n郑州郑州中的未9\n<……（\",\"\",\"\",\"\",\"\"\n   _ \"\n<\"\n\n…… 郑州\n<\",\"unknown<\"\n\n---\n``\n\n<...\n\",\"\",\"unknown\n   \"\n\",\"unknown\n”\n\",\"unknown源7Aunknown\n\n\",\"unly_ _\",\"未知\n<unknown<\",\"\",\"\"\n\"\n\",\" unknown\n\",\"\",\"\",\"\",\"\",\"\",\"unknown unknown\",\"\",\"unknown\n\n<...\n\n   unknown\n<......\n\n。。\",\"\",\"UNKNOWN\n\",\"unknown源\n源_source\",\"`` ……\"\n`` \n\n   延续\n\n   _郑州SEP\n   ……\"\n\n    \"unknown\n   \"\n\n   \"\n`` \n\n``SEP\"\n``\n   \"\n\n    (（\",\"郑州源源 to be\"\n---unknown源\n   \"\n``\n\n    unknown源源\n   \"\n``\"\n``\n<（”\n的未......<\"\n``target...\"\n   ……的当前\",\"\"\n\n\"\n``\",\"...\"\nunknown\n\"\n……\",\"未知\n\n<\"\n<unknown的\",\"未知\n\n    �\",\"unknown源000. unknownly源\n...\n\n``…… \"\n`` ”\n    \"\n\",\"\",\"\"\n<\",\"郑州\n``（……SEP\n    …\"\n\"\n �未知由\",\"未知\n\n\"如果se\n   \"\n源\",unknown\",\"\",\"\"\n未知郑州郑州源\n``\n   未知的_source\",\"\",\"\"\n   \"\n`` 2.\",\"\",\"\"\n``\n``  …库\",\"\"\n``未知未知\n\n``SEP with_source_source\n\n``SEP\n``未\"\n   \"\n``---\n\n\"\n``郑州unknown源郑州郑州源\",\"unknown\n``_source未\n\n`` 郑州\n\"\n,\",\"\",\"````\n`` 未知\n``  unknown怎么办\n`` \n\n`` unknown_source源....``unknown\n\n\"\nunknown\n\n`` \n\n   （”\n``unknown\n<UNKNOWN\n``SEP\n``unknown提供\n``\n``````郑州---\",\"``!!!!unknown源\"\n````...\n   ``````未`` \n\n``...\"\n\n\",\"`` \n\n`` \n\n``（\"\n``````\"],\"未郑州\n_\"\n\n`` �`` �UNKNOWN\n``们\n``\n``郑州\n\n``unknown\n``unknown\n\"\n`` ``郑州部郑州悟源源 a-_unknown\n”\n\n”\n``郑州\n郑州\n郑州源\n`` Source_source郑州\n_source<\"\n   \"\n``未知unknown\n`` unknown\n”\n``---\n\n未知\"\n\n\"\n   （\"\n``\n\n``\",\"\"\n``<SourceSEP\"\n\"未知\n<SEP<SEP<unknown源\n<``\"\n1_source ---（郑州 ---unknown请un_source\n\n郑州源_sourceunknown\n_source源\n\",”\n   ....\",\"unknown\n\"\n``郑州\n<unknown\n``\"\n<\"\nunknown\n<郑州\n郑州\n\"\n和郑州SEP\n\n``unknown未知_source中 an_source<郑州---unknown\n\",\",\"unknown_source\n\",…… � 郑州源_source源\n郑州 source_source\",\"\"\n\",\"\"\n   ,郑州unknown源教育\n”\nunknown（大学````郑州\n郑州 ---\",\"郑州---郑州源未未源\n``未\"\n``unknown...unknown源源<未知源大学\n郑州\n郑州\",\"未知_source（郑州---郑州\n\"\n”\n\"\n\"\n\"\n\"\n\"\n\"”\n,\",\"unknown源unknown\",\"\"\n   \"\n\"\n源person\n\"\n\"\"\"\n\"\n\"大学unknown\n\"\n\"\n\"\n``\",\"\"\n\"\n\",\"\",\"UNKNOWN\n郑州\n\"\n\"\n\n\"\n未知\n\"\n``（……\"\n_source郑州---unknown\n\"\n\"\n``sourceunknownpersonsource_source\",\" \"\n``...\n``...\n\"\n\"\n\"\n\"\n和unknown源\n”\n\"\n\"\n<S未SE\"\n源_source\n\",\"……unknown源\",\"未知\n\"\nsourceunknown源源未知大学\"\n_source源\"\n\nunknown\n郑州---unknown\",\"\"\n\"\n\"\n\"\n\"\n<未知..... unknown\n<\"\n`\n``华南\"\n\",\"0.。\"\n\"\n<\"\n\n\"\n”\n``郑州\n<\"\n\"\",\"”\n``unknown\n\"\n``未知\"\n\n\",\"未知\",\"未知\"\n<University\",\"未知源郑州\n”\n``\"\n”郑州\n\",\"\"\n\"\n\"”\n\"\n<未知---\n\n”\n``unknown\n\"\n``SEP unknown\n_source_sourceunknown\n郑州\"\n”\n\"\n\",\"郑州````未知\n\n郑州　unknown\"\n   \"\n``\n<......\n\"\n``未知大学源\"\n,\"源,\",\"````````和郑州\n”\n15_\"\n\n\",\"\"\n<郑州\n”\n\",\"\",\"……unknown_sourceunknown\n``个关于\n<\"\n源unknown郑州\",\"6\n\"\n   \",\"郑州1_source<大学\",\"\"\n``\n\", unknown\n``unknown``\"\n````unknown\",\"``源\n<````<”\nunknownperson\n\"\n``郑州\n<UNKNOWN元\n”\n\",源源源中\"\n   …未知\n<……郑州<``unknown源Source00\",\"\"\n``\"\n``未知源unknown***\"\n-\",\"\",unknown大学<\"\n,\",\"\",\"\",\"\",\"”\n\"\n<SEP\",\"\",\"郑州源源源源\n”\n``郑州源_source。未知\n”\n未知源 unknown源_source未知源郑州\n````…”\",\"unknown\n   ``\"\n\n（”\n``unknown源源\n<未知..<（\",\"\n\",\"\",\"”\n\n”``<……\",\"unknown---unknown\",\"unknown\n\",\"\"\n<\",\"unknown\n\"\n\"unknownsourceunknown\"\n\n<”\n源 unknown源 unknown源_source<郑州\n”\n<unknown\n``unknownperson源 source_source<...\n``”\n\n\n\"\n\"\n\n”``````未知\",\"的_sourceunknown源 unknown源unknown\",\"source\n\"\n``未知````”\n<S……\"\n``....\",\"\"\n\",\"unknown<\"\n”\n< \"\n\n”\n\"\n````\n\"\n”\n<\"\n”\n\"\n``unknown\",\"\"\n````未知源源unknownunknown\n”\n``\",\"unknown\n”\n   ”\n<”\n大学\n\"\n``<\",\"源\n\"\"\"\n``unknown\",\"””\n<\"\n<”\n<”\n<unknown<\"\n<\"\n``郑州\",”\n\"\n\"\"\"\n\"\n\"\n\"\n\n\"\n<\"\n”\n\"\n<未知<\"\n\n\"\n\"\n``\"\n\n\"\n<\"\n\"\n\"中 �\"\n   \"\n   \"\n`` unknown\",\"\"\n<``...\n\"\n<未知\n”\n````郑州---\n\n\",\" unknown\n\"\n    unknown\",\"\"\n\",\"\"\n<\"\n源源\",\"\",\"     \"\n7Source<UNKNOWN<\"\n_source\n“”\n\"\n\"\n<_source\n\",\"unknown。……\"\n``unknown未知源郑州<”\n\",\"未知......\",\"未知未知unknown\n<\"\n郑州郑州郑州\n\",\"unknown源\",\"unknown\n\"\n_source未知.unknown\n_source源\")\n\"\n\"\n\n<”\n\n_1郑州unknown关于\n\"\n``”\n\",\"< unknown\n”\n<..\nunknown\n_source...\n```...\n\"\n   \"\n”\n\"\n\"unknown\",\"\"\n\"\n``郑州<AI\n\"\n````未知\n”\"\n\"”\n\"\n\"\n<<”\n``未知\"\n````unknown\n\"\nunknown<……\",\"``unknown\",\"”\n”\n“”\n``\",\"\"\n\"\n_source。\n_source........---\"\n`` unknown\",\"\"\n\"\n郑州 �unknown…”\n\",\"unknown***unknown\n\",\",\"````unknown<person\n\"\n\"\n``<\"\n”\n\",\"unknown<\"\n_source郑州源\",\"``...\n\",\"\"\n-unknown\n_source9\"\n   \"\n_source源_source------unknown---关于郑州7unknownunknown\n\"\n。\n\",\"\"\n   \"\n\"\n大学继续郑州郑州<unknown...\n   郑州\n\"\n   \"\n\"\n\"\n\"\n\"\n\",\"\"\n\",\"\"\n   \",\"\"\n\",\"\",\"\"\n\"<\"\n   \"\n\",\"\",\"\",\"\",\"\",\"\"\n\"\n\",\"\"\n\",\"\"\n”\n``”\n。\",\"\"\n\"\n\",\"UNKNOWN......personunknown\"\n``\",\"\",\"_郑州\",\"\"\n``郑州\n\"\n_sourceunknown中_source郑州和⼀\n郑州---<\",\"unknown人person郑州\n\",\"未知\",\"UNKNOWN未知,\"\",\",\"\",\"\",\"0\",\"\",\"\",\"\"\n\"\n\"\n\",\"unknown\",\"\",\"\",\"\"\n\",\"\",\"\",\"\"\n,\",\"\",\"\"\n<\"\n\"\n\",\"\"\n_source继续\"\n郑州\",\"\"\n\",\"\"\n\",\"\"\n\",\"\",\"\",\"\",\"\"\n\"\n<\"\n_未知\",\"\"\n\",\"\"\n``\n\n_source\n\",\"unknown\",\"\",\"unknown\n``unknown\"\n2\"\n``unknown源_source源。\",\"郑州\",\"\",\"\",\"unknown\"\n\",\"\",\"\"\n\"\n``\n\",<郑州郑州歌人郑州\n\"\n``unknown（\",\"\"\n<\",\"\",\"\",\"”\n\"\n\"\n\",\"\",\"\",\"\",\"\"\n\",\"\"\n<,\"\"\n\",\"\"\n\"\n\",\"\"\n<\"\n``\"\n,\",\"\"\n从郑州源科学院\",\"\"\n\"\"\n,\",\"“\"\n\"\n<\",\"unknown\n\",\"\"\n   _\"\n\",\"\",\"\n大学\",\"未知\n\",\"郑州\n\",\"\",\"\",\"\"\n\"\n\",\"\n\"\n\"\n\"\n\",\"\",\"\"\n\"\n\",\"\"\n_source源\",\"\"\n\"\n-\",\"\"\n\"\n\",\"\"\n\"\n,\",\"\"\n\"\n\"\n`` \"\n-与郑州郑州显示\",\"\"\n\"”\n---\"\n\",\"\n`\n\",\"”\n``\",\"unknown郑州继续郑州\",\"5_郑州txt郑州\n”\n``\"\n(\"\n``\"\n<\"\n\"\n   \"\n郑州---\",\"\n-\",\"unknown\",\"”\n``\"\n``郑州\n\n``和�人郑州\",\"和\",\"”\n``...\n源<……\"\n”\n”\n``”\n`\n````……”\n``\",\"`\n````UNKNOWN\",\"\"\n``---``\"\n\"\n\"\n_source郑州WISE\",\"未知，”\n``中4中。“”\n``\",\"``\",\"”\n\"\n”\n   ”\n---\n,\"\n<”\n``郑州\n,”\"\n\"\n`` \"\n\"\n\n”\n”\n````和”\n``sourceunknown\n-郑州 郑州源源\n_source\",\"\"\n   _\"\n\n_sourceUNKNOWN\n_source关于 \",\"\n   未知源郑州<\"\n\",\"\"\n\"\n\"\n\"\n”\n<<\",\"\"\n源\"\n<\"\n\"\n\",\"unknown_sourcepersonunknown\n0.\",\"unknown\n郑州\n”\n<郑州中郑州\n\"\n的_source源\n郑州\n郑州\n“”\n``---\n郑州---\n郑州 Fold源\",\"unknown\",\"   _（\"\n”\n\"\n``<\"\n”\n\"\n\"\n\"\n\"\n\"\n\"\n   \"\n   \"\n   \"\n   ”\n\n\"\n   ”\n   未知\n工程\n\"\n\",\"\"\n```\"\n   大学\n\"\n``\"\n``（郑州\n\"\n\"”\n``unknown郑州源郑州源\n\"\"\"\n``郑州\n<\"\n   ``\"\n``unknownEmily郑州大学\",\"<\"\n``\",\"\"\n”\n（\",\"\"\n``\n\n\n   郑州 ---\n\n”\n\n``郑州\n提供\",\"\"\n``\"\n”\n,\"\",\"郑州\n\"\n”\n``_未郑州\n郑州\",\"\",\"\",\"卢\n``\",\"\"\n``郑州\n郑州\n郑州\n郑州\n   \"\n   \"\n   郑州<\",\"\"\n   郑州\n   19郑州\",\"1郑州\n”\n   _\",\"\",\"<……\"\n_source\n[\"\n   “\"\n\",\"工程关于.\",\"郑州中了\",\"\"\n---unknown郑州\n郑州<郑州 �在\",\"\",\"\",\"unknown教育\n\"\n   \"\n   \"\n(\",\"工程教育\",\"\",\"\",\"\",\"\",\"\",\"未知.未知.\",\"郑州若\n“\",\"unknown\",\"\",\"\",\"\"\n   \",\"\",\"<郑州UNKNOWN\n……personunknownperson\n\",\"\",\"\",\"\",\"\"\n教育personunknown.\",\"10.”\n<\",\"\"\n[\"\n<\"\n\",\"\",\"unknownunknown郑州\",\"\"\n\"\n\"\n\"\n\"\n\"\n_source\n\",\"\",\"未知\n\"\n\n教育\",\"\"\n\"\n,\"\"\n   ……\"\n[\",\"0.\",\"未知\",\"unknown\n提供.\",\",<”\"\n<2\",\"unknowncategoryunknownperson\n[unknownpersonunknown source\",\"\"\n``……\"\n\"\n\"\n源\",\"\"\n”\n\n郑州\nunknown\",\"unknown\",\" unknown源\n”\n\"\n\n\"\"\"\nunknownunknown源\",\" unknown源\n<郑州\n\",\"``\",\"``郑州\n郑州\n\"\n   _未知\",\"\"\n”\n\",\"\",\"郑州 According。\",\"01_郑州emons_郑州\n   工程源郑州\n<\",\"unknownperson郑州{\" unknown\",\" \",\",\" unknown\",\"0郑州\n郑州郑州郑州SEP\n_source……``\",\"\"\n\"\n``...\n   郑州\n\"\n郑州\",\"<郑州\",\"\"\n   \"\n[郑州郑州郑州\n\",\"\"\n”\n<\"\n<郑州<\",\"\",\"unknown郑州\n<郑州有person\n\"\n,\"郑州\n   ....011郑州郑州1�\n\",\"<1郑州\n\",\"郑州\n\"\n_source_source\",\"unknown\",\"<<\",\"unknown郑州\n\",\"6�<\",\"<郑州郑州_sourceunknownunknown\n郑州\n郑州\n\",\"<<郑州大学郑州\n中国的\n\",\"\",\"……郑州\n<郑州<<……郑州（郑州源Lu全部unknown__UNKNOWN\",\"\",\"郑州关于_unknown\n<……_ unknown源郑州\n\",\"2....“……\",\"\",\"\",\" unknown伟.信息person\n_\"\n   \",\"。\",\"<\"\n\"\n``21_sourceperson在……unknown...unknown源source\",\"郑州\n\",\"unknown郑州 todo郑州 unknown{\",\"语\n``未知\n\n\n<_|�\"\n<<�\n”\n,\"……郑州\n”\n``<\"\n``\",\"\",\"<unknown(\"<unknown_source unknown_SOURCEunknown郑州\n\"\n“”\n``---\n\n<<郑州\n郑州继续郑州_（郑州郑州\n<unknown伟若\",\"\"\n-UNKNOWN\"\"\n< �person�unknown源源\",\"<”\n\",\"<郑州\n\",\"\"\n-\"\n``_个 the_source… �\n”\n<郑州端unknown\n\n\",\"unknown未知源unknown…unknown（\",\"unknownunknownunknown郑州\",\"\",\"<|_.unknown\",\",\"…………未知 unknown\n,”...\n`` unknown\",\"学unknown\n… �``unknown作为中文…\n未知\n\",\"<\",\"未知\n``未知_sourcepersonperson\n\n<……<来源......\"\n\",\"``郑州\n``\",\"\",\"\",\"\",\"\",\"<……郑州\n郑州0\",\",\"...\"\n<4 unknown\n”\n``..<|�郑州\n<（\",\"``未知\n\",\"\",\"\",\"\"\n..\nunknown.。（\"\n``eña|…（……unknown源\n源未知\"`\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "sherlockma11",
      "author_type": "User",
      "created_at": "2025-04-09T09:22:28Z",
      "updated_at": "2025-04-09T14:11:55Z",
      "closed_at": "2025-04-09T14:11:55Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1321/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1321",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1321",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:50.247273",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "感觉是文档提取的时候出现问题了，图谱上保存的内容有问题。请使用 light-server 来测试，用内置的webui上传和处理文件，看一下情况如何。",
          "created_at": "2025-04-09T11:00:08Z"
        },
        {
          "author": "sherlockma11",
          "body": "问题应该解决了，我是用3B模型先做测试，3B模型理解能力太差了不行。用巨大模型可以理解",
          "created_at": "2025-04-09T13:46:59Z"
        }
      ]
    },
    {
      "issue_number": 1319,
      "title": "[Feature Request]: how to return the source files used to answer a query? Like Asking LighRag to find a legal document relating to a contract for purchase of shares for X company.?",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n_No response_\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Arslan-Mehmood1",
      "author_type": "User",
      "created_at": "2025-04-09T06:04:58Z",
      "updated_at": "2025-04-09T06:24:28Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1319/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1319",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1319",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:50.431497",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "There is a References passage at the end of LightRag's response of the query. Is this what you want?\n\n<img width=\"1213\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0c520241-1da3-4c9a-ac4e-5564c211e3d5\" />",
          "created_at": "2025-04-09T06:24:27Z"
        }
      ]
    },
    {
      "issue_number": 1309,
      "title": "[Question]: Is it possible to merge different workdirs or have different workdirs work together?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nDue to the large amount of data, I divided the data into multiple copies. In order to prevent conflicts, I chose different workdirs for processing. Does LightRAG support merging different workdirs or unified query processing so that I don’t have to make the same query request in different workdirs? This is my ultimate appeal.\n\nNow what I do is merge the kv_store_llm_response_cache.json in different workdirs, then delete other files and rerun (as mentioned in the link https://github.com/HKUDS/LightRAG/issues/1280). Even with the cache, I found that the speed is much slower than I expected.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Socratesa",
      "author_type": "User",
      "created_at": "2025-04-08T05:56:44Z",
      "updated_at": "2025-04-09T05:56:36Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1309/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1309",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1309",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:50.609991",
      "comments": [
        {
          "author": "Socratesa",
          "body": "<img width=\"356\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9b345b09-fc51-47e8-9bb9-f49cc9467a04\" />\n\n",
          "created_at": "2025-04-08T05:58:00Z"
        },
        {
          "author": "Socratesa",
          "body": "I re-run for about an hour and can process more than 700 chunks, but I have about 40,000 chunks, so this time cost is unbearable.",
          "created_at": "2025-04-08T06:00:06Z"
        },
        {
          "author": "Socratesa",
          "body": "Or does it support multiple processes inserting into the same workdir?",
          "created_at": "2025-04-08T10:25:25Z"
        },
        {
          "author": "Socratesa",
          "body": "> Or does it support multiple processes inserting into the same workdir?\n\nI've verified it, it's very slow, it doesn't seem feasible",
          "created_at": "2025-04-09T05:56:35Z"
        }
      ]
    },
    {
      "issue_number": 1066,
      "title": "[Bug]: <title> Shared dictionaries not initialized",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I config LightGAG with PGKVStorage,PGVectorStorage,PGGraphStorage,PGDocStatusStorage, the initialize_pipeline_status() method raised error:\n`  File \"/Users/apple/workplace/AI/src/rag/lightrag_wrapper.py\", line 69, in _initialize_rag\n    await initialize_pipeline_status()\n  File \"/Users/apple/workplace/AI/libs/lightrag/lightrag/kg/shared_storage.py\", line 278, in initialize_pipeline_status\n    pipeline_namespace = await get_namespace_data(\"pipeline_status\")\n  File \"/Users/apple/workplace/AI/libs/lightrag/lightrag/kg/shared_storage.py\", line 428, in get_namespace_data\n    raise ValueError(\"Shared dictionaries not initialized\")\nValueError: Shared dictionaries not initialized\n`\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-03-12T07:03:18Z",
      "updated_at": "2025-04-08T18:56:19Z",
      "closed_at": "2025-03-12T08:18:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1066/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1066",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1066",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:50.821532",
      "comments": [
        {
          "author": "takibombom",
          "body": "For those who have encountered this problem and have not found an answer.\nLocally this problem was solved by initializing share data inside rag initialization func\n```\n await rag.initialize_storages()\n **initialize_share_data(workers=1)**\n await initialize_pipeline_status()\n```",
          "created_at": "2025-04-08T18:56:17Z"
        }
      ]
    },
    {
      "issue_number": 1247,
      "title": "[Feature Request]: Do you have plans to support Nebula graph databases",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nFrom multiple perspectives such as performance, learning cost, and compatibility with the business, Nebula Graph is easy to use and can significantly improve business efficiency.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "SairoUltraman",
      "author_type": "User",
      "created_at": "2025-04-01T09:30:25Z",
      "updated_at": "2025-04-08T16:52:05Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1247/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1247",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1247",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:51.008696",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Not yet in schedule. There is some performance optimize jobs for Neo4j and PostgreSQL AGE graph on the waiting list.",
          "created_at": "2025-04-08T16:52:04Z"
        }
      ]
    },
    {
      "issue_number": 1303,
      "title": "[Question]: Anybody deployed LightRAG in production with millions of vectors?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am looking into re-working my 'recommendation engine' where I am using a native similarity search. \n\nI've got about 4 million vectors and from what I have been able to find online I doesn't seem that anybody has implemented this many vectors. I've even heard about a 20 minute loading time for just 50 pages of text. In comparison, I have over 3.8 million pages of text.\n\nWould love to hear from people that have really put LightRAG to the test on millions of vectors and any guidance in doing so.\n\nKind regards, Dennis\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "dejoma",
      "author_type": "User",
      "created_at": "2025-04-07T14:46:36Z",
      "updated_at": "2025-04-08T16:49:42Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1303/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1303",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1303",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:51.202131",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG still has issues when it comes to handling large-scale data. Although the project initially built all storage on an embedded in-memory database and later added support for serval type of DB, there remains a significant amount of optimization work needed for large-scale data processing.",
          "created_at": "2025-04-08T16:49:40Z"
        }
      ]
    },
    {
      "issue_number": 1287,
      "title": "[Feature Request]: Add `/context` query prefix to Ollama model simulation for LightRAG Server",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nAdd `/context` query prefix to Ollama model simulation for LightRAG Server。 If user send a query weith `/context` prefix, Ollama interface should return context information from LightRAG by utilizing the only_need_context option of query API.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-06T19:16:40Z",
      "updated_at": "2025-04-08T09:45:14Z",
      "closed_at": "2025-04-08T09:45:14Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1287/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1287",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1287",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:51.406544",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "There is a query param address this issue: only_need_context",
          "created_at": "2025-04-07T02:26:17Z"
        },
        {
          "author": "Exploding-Soda",
          "body": "It's quite embarrassing that I didn’t notice the `/query` endpoint already provided this functionality (`only_need_context`) at first.  \nSo for my own use, after learning about the `only_need_context` parameter, I created a new `/context` endpoint under `routes`. You can use it just like the `/query",
          "created_at": "2025-04-07T03:30:24Z"
        },
        {
          "author": "danielaskdd",
          "body": "It's about the Ollama emulation api, which make LightRAG act as an Ollama module. For example, Dify can access LightRAG through Ollama API, and get only context information from LightRAG.",
          "created_at": "2025-04-07T04:36:10Z"
        },
        {
          "author": "Exploding-Soda",
          "body": "> It's about the Ollama emulation api, which make LightRAG act as an Ollama module. For example, Dify can access LightRAG through Ollama API, and get only context information from LightRAG.\n\nThis is very interesting! I've been thinking about integrating LightRAG as a separate knowledge base module r",
          "created_at": "2025-04-07T05:09:02Z"
        },
        {
          "author": "danielaskdd",
          "body": "It would be great if you have time to implement this. In fact, the Ollama API already supports many query prefixes, and you just need to add a /context prefix, which is indeed a very simple task. For the query prefixes currently supported by Ollama, please refer to:  https://github.com/HKUDS/LightRA",
          "created_at": "2025-04-07T06:17:36Z"
        }
      ]
    },
    {
      "issue_number": 1294,
      "title": "[Bug]: 知识库似乎不互通",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nexamples/lightrag_api_openai_compatible_demo.py生成的知识库似乎不能被examples/lightrag_openai_compatible_demo.py读取，我确定它们是同一个工作目录，并且知识库中有相关内容。\n如下：\n数据库通过examples/lightrag_openai_compatible_demo.py更新了苏州大学的内容，但是examples/lightrag_api_openai_compatible_demo.py无法读取，examples/lightrag_api_openai_compatible_demo.py只能读取自己生成的内容\n`query:苏州大学的校长是谁`\n\n`answer:根据提供的文档内容，文档中没有提及苏州大学的校长信息。文档主要描述的是郑州大学的历史、学科设置、科研成果及发展情况等内容，并未涉及苏州大学的相关信息。\n\n如果您需要了解苏州大学校长的信息，建议直接访问苏州大学的官方网站或者通过其他可靠渠道获取最新的信息。`\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "sherlockma11",
      "author_type": "User",
      "created_at": "2025-04-07T06:45:49Z",
      "updated_at": "2025-04-08T06:28:46Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1294/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1294",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1294",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:51.584245",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "您是说同时运行两个实例的 lightrag_api_openai_compatible_demo.py吗。系统默认才用时嵌入式文件数据库，两个实例是不能够共享同一套数据文件的。",
          "created_at": "2025-04-07T15:23:12Z"
        },
        {
          "author": "sherlockma11",
          "body": "不是这个意思，我首先使用examples/lightrag_api_openai_compatible_demo.py的insert生成了知识库，然后使用examples/lightrag_openai_compatible_demo.py调用这些知识库，模型的回答是不能找到相关信息。\n\n经过我的debug，我发现问题出在lightrag/kg/nano_vector_db_impl.py下query()调用client.query()时，它计算之前的知识库向量和query向量的相似度scores为负数，导致被cosine_better_than_threshold过滤掉。\n注意：\n\n1. 如",
          "created_at": "2025-04-08T05:52:59Z"
        },
        {
          "author": "danielaskdd",
          "body": "考虑一下是否索引和查询使用了不同的embeding模型",
          "created_at": "2025-04-08T06:28:45Z"
        }
      ]
    },
    {
      "issue_number": 1215,
      "title": "[Bug]: Failed to extract entities and relationships",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nFailed to extract entities and relationships\nFailed to process document doc-b15c2eafcc86764e2b11ad628d818ff4: 'metadata'\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-03-28T09:17:17Z",
      "updated_at": "2025-04-07T15:41:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1215/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1215",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1215",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:51.792781",
      "comments": [
        {
          "author": "JoramMillenaar",
          "body": "What LightRAG version do you have? I had this error myself, but it seems to be fixed in the latest version.",
          "created_at": "2025-03-28T17:31:21Z"
        },
        {
          "author": "harshffy",
          "body": "Failed to extract entities and relationships\nFailed to process document doc-xxxxxx.....: 'p'\nI was using the cloned repo yesterday",
          "created_at": "2025-03-29T06:18:59Z"
        },
        {
          "author": "leonardocerliani",
          "body": "I experienced the same problem with version 1.2.6. Specifically, the very first document always processed with no problem, while the following occurred with the subsequent documents. (I tried both with batch insertion and by inserting manually one after another)\n\n```\nFailed to extract entities and r",
          "created_at": "2025-03-29T06:27:42Z"
        },
        {
          "author": "harshffy",
          "body": "Hi, thanks for the clarification. can you please check and let me know the correct version of git tag/commit? I did a git describe just now after cloning and I got v1.3.0-143-g7cf6381. ",
          "created_at": "2025-03-29T07:15:22Z"
        },
        {
          "author": "leonardocerliani",
          "body": "Hi! I cloned it this morning\n\n```bash\ngit describe --tags\nv1.3.0-143-g7cf6381\n\npip list | grep lightrag\nlightrag-hku            1.3.1\n```\n\nIf it can be of any help, here's the code I used for the batch insertion (NB: jupyter nb):\n\n```python\nimport os\nimport asyncio\nfrom lightrag import LightRAG, Que",
          "created_at": "2025-03-29T10:44:57Z"
        }
      ]
    },
    {
      "issue_number": 1280,
      "title": "[Question]: <title>How to fix the lost matrix",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nMaybe it's because I killed my process by mistake, which caused the last set of data in my vdb_relationships.json to be incomplete. Of course, I can delete this relationship to roll back to a certain extent, but what I didn't expect was that my matrix was lost. How should I remedy it?\n The amount of data is relatively large, and it is not suitable to generate it from scratch. Is there any way to backfill the matrix, or continue to run without relying on the matrix in the future? \nWhat is the role of the matrix?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "Socratesa",
      "author_type": "User",
      "created_at": "2025-04-06T11:39:10Z",
      "updated_at": "2025-04-07T06:19:35Z",
      "closed_at": "2025-04-07T06:19:35Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1280/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1280",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1280",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:51.988921",
      "comments": [
        {
          "author": "Socratesa",
          "body": "<img width=\"1230\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e7202777-fa22-4f58-86c8-71404c2c2670\" />\n\nnow，I can remove rel-222f99df87dbc5b34981531b1fcab6d3，but how can i fix missing matrix",
          "created_at": "2025-04-06T11:40:41Z"
        },
        {
          "author": "danielaskdd",
          "body": "If you are using LightRAG Server, LLM cache is enable by default. So re-run the document index jobs is much faster than the first time.",
          "created_at": "2025-04-06T20:25:17Z"
        },
        {
          "author": "Socratesa",
          "body": "Need I delete vdb_relationships.json to re-run ？\nI just run insert not query，and now it cannot start because of the above error\nI am new to LightRAG, please tell me more about it.\n\nIn order not to delay the progress of the atlas, after the first error, I switched work_dir and continued to run. This ",
          "created_at": "2025-04-07T04:07:52Z"
        },
        {
          "author": "danielaskdd",
          "body": "Delete all the data files in rag_storage except `kv_store_llm_response_cache.json`",
          "created_at": "2025-04-07T04:30:22Z"
        },
        {
          "author": "Socratesa",
          "body": "Thanks very much, does this mean that I can combine the llm_response of multiple workdirs and run it once, and the index will be built together?",
          "created_at": "2025-04-07T04:33:30Z"
        }
      ]
    },
    {
      "issue_number": 1293,
      "title": "[Feature Request]: Using LightRAG as a Knowledge Retriever Only",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nHi, so when I access LightRAG via an API request to solve problems based on the knowledge graph, the /query endpoint appears to be the only available option (according to the API tab in the UI). As a result, I receive a fully generated response from the LLM after making a request.\n\nHowever, I would prefer to use LightRAG solely as a knowledge retriever. Ideally, after sending a request, the server would return only the relevant knowledge retrieved for my question — without generating a full response.\n\nThis way, I can compose the final prompt elsewhere (e.g., in Dify), enabling the use of different LLMs and allowing for better separation of responsibilities in the overall workflow.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "Exploding-Soda",
      "author_type": "User",
      "created_at": "2025-04-07T01:51:51Z",
      "updated_at": "2025-04-07T02:57:18Z",
      "closed_at": "2025-04-07T02:57:18Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1293/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1293",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1293",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.181465",
      "comments": []
    },
    {
      "issue_number": 955,
      "title": "[Bug]: <title>ValueError: shapes (0,512) and (1024,) not aligned: 512 (dim 1) != 1024 (dim 0)",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n使用Ollama的BGE作为embedding，写的很清楚embedding_dim是512，但是会对query  嵌入成1024的形式\n\n![Image](https://github.com/user-attachments/assets/36ec8d2c-8fd2-42db-974c-6106c2fcc256)\n\n![Image](https://github.com/user-attachments/assets/daa366fd-105b-48e5-8444-c2c8debe78ea)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Harris-Xie",
      "author_type": "User",
      "created_at": "2025-02-26T20:12:26Z",
      "updated_at": "2025-04-06T07:09:19Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/955/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/955",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/955",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.181490",
      "comments": [
        {
          "author": "Buzeg",
          "body": "same here",
          "created_at": "2025-02-27T08:21:45Z"
        },
        {
          "author": "Salary-only-17k",
          "body": "一样 我也遇到了 \n`async def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"qwen2.5:14b\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http:",
          "created_at": "2025-04-06T07:09:19Z"
        }
      ]
    },
    {
      "issue_number": 1266,
      "title": "[Question]: <title>lightrag_openai_compatible_demo.py会报错An error occurred: 404 page not found",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\napi都没有问题，但是会报错，不知道为什么\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "1137043480",
      "author_type": "User",
      "created_at": "2025-04-03T18:11:16Z",
      "updated_at": "2025-04-04T16:40:53Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1266/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1266",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1266",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.370995",
      "comments": [
        {
          "author": "yimengl940",
          "body": "similar issue:\n\nAn error occurred: Error code: 401 - {'error': {'message': \"Authentication Error, LiteLLM Virtual Key expected. Received=xxx, expected to start with 'sk-'.\", 'type': 'auth_error', 'param': 'None', 'code': '401'}}\n\nthe api-key and url are verified working properly.",
          "created_at": "2025-04-04T16:40:51Z"
        }
      ]
    },
    {
      "issue_number": 1230,
      "title": "[Question]: How to persist state, and load from persisted?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHow can I save the state of my LightRag instance, so that it can be loaded later, without having to re-index?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "lajd",
      "author_type": "User",
      "created_at": "2025-03-29T17:48:44Z",
      "updated_at": "2025-04-04T10:58:09Z",
      "closed_at": "2025-04-04T10:57:57Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1230/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1230",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1230",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.590037",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You can run LightRAG Server which you can found in lightrag/api",
          "created_at": "2025-03-31T05:54:04Z"
        },
        {
          "author": "lajd",
          "body": "Thanks, was hoping for an easy way to facilitate this for local experimentation. Closing!",
          "created_at": "2025-04-04T10:57:57Z"
        }
      ]
    },
    {
      "issue_number": 1236,
      "title": "[Feature Request]: Drop all documents in webui",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nClear document button is not working in WebUI. This must be fix to build and drop KG without digging into the backend server.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-03-31T05:48:25Z",
      "updated_at": "2025-04-04T05:30:16Z",
      "closed_at": "2025-03-31T06:20:39Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1236/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "danielaskdd"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1236",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1236",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.800412",
      "comments": []
    },
    {
      "issue_number": 1267,
      "title": "Feat: Refactoring PostgreSQL AGE graph DB",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nRefactoring PostgreSQL AGE graph DB:\n- Fix compatible problem with Graph UI \n- Keep origin node and edge in Graph data\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-04-04T05:14:12Z",
      "updated_at": "2025-04-04T05:18:45Z",
      "closed_at": "2025-04-04T05:16:55Z",
      "labels": [
        "enhancement",
        "PostgreSQL",
        "Core"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1267/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1267",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1267",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.800433",
      "comments": []
    },
    {
      "issue_number": 1263,
      "title": "[Bug]: <title>readme文件中milvus设置异常",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n错误信息：\nMilvusVectorDBStorge        Milvus\n正确信息：\nMilvusVectorDBStorage        Milvus\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "shaoqing404",
      "author_type": "User",
      "created_at": "2025-04-03T07:53:10Z",
      "updated_at": "2025-04-03T07:53:10Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1263/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1263",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1263",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.800440",
      "comments": []
    },
    {
      "issue_number": 1259,
      "title": "[Question]: Why can't the KG built through Python execution be seen in the LightRAG Server?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello. I have successfully installed and started LightRAG and the Server, and I was also able to upload documents via the UI and parse out the KG through the pipeline. However, when I switched to another method (uploading documents via Python code), after the parsing was completed, I did not see the KG. My steps are as follows:\n\n1. I created a new project directory, created a `.env` file, and ran `lightrag-server` in the project directory.\n2. I ran the RAG initialization code in Python, and uploaded and parsed the documents (it took an hour to parse 10 documents). The code is as follows:\n```\nimport asyncio\nimport nest_asyncio\n\n# 在 Jupyter Notebook 中启用嵌套的事件循环\nnest_asyncio.apply()\n\nimport os\nimport inspect\nimport logging\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.ollama import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\n\n# 设置工作目录\nWORKING_DIR = \"/data/github/LightRAG/znkf\"\n\n# 配置日志\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\n# 初始化 RAG 实例\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"qwen2.5:14b\",\n        llm_model_max_async=2,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",\n            \"options\": {\"num_ctx\": 32768},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=512,\n            func=lambda texts: ollama_embed(\n                texts, embed_model=\"quentinz/bge-large-zh-v1.5:latest\", host=\"http://localhost:11434\"\n            ),\n        ),\n        addon_params={\n            \"language\": \"Simplified Chinese\", \n            \"entity_types\": [\"产品系列\", \"手机品牌\", \"服务类型\", \"公司\", \"支付方式\", \"产品名称\", \"城市名\", \n                             \"电话号码\", \"日期\", \"卡片类型\", \"优惠政策\", \"金额\"], \n        }\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n# 初始化 RAG 实例\nloop = asyncio.get_event_loop()  # 获取当前事件循环\nrag = loop.run_until_complete(initialize_rag())\n\n# 初始构建KG\nfile_path = '/data/notebooks/knowledge/kg/kg_test'\nfile_list = []\nfor root, _, files in os.walk(file_path):\n    for file in files:\n        if any(file.endswith(ext) for ext in [\".md\", \".txt\"]):\n            filex = os.path.abspath(os.path.join(root, file))\n            print(f\">>> 文件 {filex} 处理中...\")\n            file_list.append(filex)\n            with open(filex, \"r\", encoding=\"utf-8\") as f:\n                rag.insert(f.read())\n```\n4. I can see the generated documents in my project directory, but I don't see any results in the UI.\n\n![Image](https://github.com/user-attachments/assets/b29f26ca-9e4b-4776-ab8b-812217ed30d5)\n![Image](https://github.com/user-attachments/assets/c0654cad-aa75-4ca0-baa1-8f50850be337)\n![Image](https://github.com/user-attachments/assets/65d1e548-cfce-4e7f-85a6-b13ef4d79b51)\n\n### Additional Context\n\n中文表述：\n\n您好。我已经成功的安装并启动LightRAG以及Server，并且也成功的通过UI上传文档并通过pipeline解析出KG。但是当我换了一个方式（通过python代码进行文档上传），最终解析完成后却没有看到KG。\n我的步骤是这样的：\n1. 我新建了项目目录，新建了.env，并且在项目目录下运行了`lightrag-server`\n2. 我在python中运行了rag初始化代码，并且上传和解析文档（一共10个文档解析了一个小时）。代码如下：\n```代码如上```\n3. 我能从我的项目目录看到已经生成的文档，但是我在UI中没有看到任何结果。（截图如上）",
      "state": "open",
      "author": "tigflanker",
      "author_type": "User",
      "created_at": "2025-04-03T05:41:55Z",
      "updated_at": "2025-04-03T07:11:30Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1259/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1259",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1259",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.800446",
      "comments": [
        {
          "author": "tigflanker",
          "body": "It feels like the Python execution and UI operations are disconnected. The UI operation makes it very convenient to upload documents directly, but in reality, I need to customize many execution parameters.\n有一种感觉，就像python执行和UI操作两种方式是割裂的。UI操作直接上传文档很方便，但是实际上我需要自定义很多执行参数。\n\nBased on the execution time an",
          "created_at": "2025-04-03T05:50:34Z"
        },
        {
          "author": "Exploding-Soda",
          "body": "I tried both insert() method and uploading file via document/file endpoint.\nOn my lightrag-server, each document I uploaded is displayed correctly.\nSo does the knowledge graph tab.\n\nIm running lightrag-server v1.3.0/1.2.6",
          "created_at": "2025-04-03T06:00:02Z"
        },
        {
          "author": "tigflanker",
          "body": "@Exploding-Soda I know how to proceed now. \nI just need to move all the files generated after executing Python into the subdirectory `rag_storage`. \nI will continue testing functions like `rag.create_entity`.",
          "created_at": "2025-04-03T06:06:12Z"
        },
        {
          "author": "tigflanker",
          "body": "@Exploding-Soda When I upload documents separately using the endpoint (starting `lightrag-server` in the project directory `/data/github/LightRAG/znkf`), everything works fine, and the correct KG is constructed.  👍\n\nHowever, when I use Python code to perform `rag.insert` (starting `lightrag-server` ",
          "created_at": "2025-04-03T06:21:07Z"
        },
        {
          "author": "Exploding-Soda",
          "body": "well just here's all I know and wanna share, I think `inputs` and `rag_storage` dir will affect what you see on UI. If you empty those two dirs, you empty the UI, so it can be insert() function doesnt really make changes to files under those two, but I'm not sure.\nFor my senario I use docuemnt/files",
          "created_at": "2025-04-03T07:11:30Z"
        }
      ]
    },
    {
      "issue_number": 1261,
      "title": "[Feature Request]: Is there a way to represent entities of the same type in the same color?",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nFor example, companies could be represented in blue, employees in green, and products in yellow. \n\nThe specific colors don’t need to be predefined, but using the same color for the same type of entity would provide a very intuitive understanding. 💗\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "tigflanker",
      "author_type": "User",
      "created_at": "2025-04-03T06:37:34Z",
      "updated_at": "2025-04-03T06:37:34Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1261/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1261",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1261",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.980344",
      "comments": []
    },
    {
      "issue_number": 1260,
      "title": "Save LLM generated entities and triples locally, and add counting function for entities and triples！！！",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n将llm生成的实体、三元组，保存到本地.txt文档，并添加计数功能。\n如果需要重复使用这些数据（例如后续分析、模型训练、知识图谱构建），保存到本地是必要的。\n下面我将给出详细的代码生成步骤：\n1、将LLM生成的实体，保存到本地.txt文档。提取实体并计数。将下列代码放到./dickens文件夹下即可。\n```python\nimport json\n\n# 读取 JSON 文件\ninput_file = 'vdb_entities.json'  # 替换为你的 JSON 文件路径\noutput_file = 'entity_names1.txt'  # 输出的文本文件路径\n\n# 打开 JSON 文件并加载数据\nwith open(input_file, 'r', encoding='utf-8') as f:\n    data = json.load(f)\n\n# 提取所有 entity_name 的值\nentity_names = [item['entity_name'].strip('\"') for item in data['data']]\n\n# 将 entity_name 写入文本文件\nwith open(output_file, 'w', encoding='utf-8') as f:\n    for name in entity_names:\n        f.write(name + '\\n')\n\nprint(f\"共提取了 {len(entity_names)} 个 entity_name，已保存到 {output_file}\")\n```\n\n2、将LLM生成的三元组，保存到本地.txt文档。提取三元组并计数。将下列代码放到./dickens文件夹下即可。\n\n```python\nimport xml.etree.ElementTree as ET\n\n# 解析GraphML文件\ntree = ET.parse('graph_chunk_entity_relation.graphml')\nroot = tree.getroot()\n\n# 定义命名空间（GraphML 使用默认命名空间）\nns = {\"g\": \"http://graphml.graphdrawing.org/xmlns\"}\n\n# 存储提取的关系\nrelations = []\n\n# 遍历所有 <edge> 元素\nfor edge in root.findall(\".//g:edge\", ns):\n    source = edge.get(\"source\").strip('\"')  # 去除多余的引号\n    target = edge.get(\"target\").strip('\"')\n    \n    # 提取 d5（关系类型）\n    d5 = edge.find('.//g:data[@key=\"d5\"]', ns)\n    if d5 is not None:\n        relation_type = d5.text.strip('\"')  # 去除引号\n        relations.append(f\"({source}, {relation_type}, {target})\")\n\n# 写入 TXT 文件\nwith open(\"relations.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(relations))\n\nprint(f\"已提取 {len(relations)} 条关系，并保存到 relations.txt\")\n```\n\n### Additional Context\n\n1）运行代码一、生成实体并计数\n![Image](https://github.com/user-attachments/assets/941eeeca-1315-420c-a423-072b4b410f79)\n\n![Image](https://github.com/user-attachments/assets/6d103aab-65fb-4b41-9e10-9122aaa3750a)\n2）运行代码二、生成三元组并计数\n\n![Image](https://github.com/user-attachments/assets/963dc543-8333-443d-a738-af9c82c36d9b)\n\n![Image](https://github.com/user-attachments/assets/ed103a39-7862-4066-b2c8-ce60e6869d17)",
      "state": "open",
      "author": "Idol-Dou2021",
      "author_type": "User",
      "created_at": "2025-04-03T06:18:12Z",
      "updated_at": "2025-04-03T06:18:12Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1260/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1260",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1260",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.980364",
      "comments": []
    },
    {
      "issue_number": 1112,
      "title": "Has anyone reproduced the experimental results of the LightRAG paper?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHas anyone reproduced the experimental results of the LightRAG paper?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "youzi668786",
      "author_type": "User",
      "created_at": "2025-03-18T11:37:28Z",
      "updated_at": "2025-04-03T03:39:12Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1112/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1112",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1112",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:52.980370",
      "comments": [
        {
          "author": "youzi668786",
          "body": "我复现的一小部分结果如下：\n\n![Image](https://github.com/user-attachments/assets/81f06a95-1df9-422d-9d92-fffd80419867)",
          "created_at": "2025-03-18T11:45:51Z"
        },
        {
          "author": "Victory66",
          "body": "> 我复现的一小部分结果如下：\n> \n> ![Image](https://github.com/user-attachments/assets/81f06a95-1df9-422d-9d92-fffd80419867)\n\n你好，请问可以分享一下你的源码吗？我也在尝试复现论文。",
          "created_at": "2025-03-21T04:19:32Z"
        },
        {
          "author": "youzi668786",
          "body": "不好意思，现在才看到，可以加我QQ：2662929541，我们交流一下实验部分",
          "created_at": "2025-04-03T03:39:11Z"
        }
      ]
    },
    {
      "issue_number": 1254,
      "title": "[Question]: Is there any way to retrieve specific range of nodes in the knowledge database for query?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nNeo4j Community edition does not support creating multiple databases, so normal queries go through the whole knowledge database created previously. If you store everything inside one database, I think it will **inevitably extract some irrelevant content**.\n\nI tried to ran this test to put two completely conflicting descriptions of the same character \"Hamlet\" into the same neo4j database. And then made a query to ask about Hamlet's character\n\n`\n1. The Indecision Overthinker:\n\"I'm Hamlet—a man who thinks too much and acts too little. I question everything, second-guess every move, and get lost in my own doubts. Even when I know what must be done, I hesitate, trapped in endless 'what ifs.' My mind is my greatest enemy, paralyzing me when action is needed most. I'm not weak—just too aware of consequences, too afraid of making the wrong choice.\"\n\n2. The Ruthless Avenger:\n\"I’m Hamlet—a man who doesn’t hesitate when justice demands blood. I play the fool to deceive my enemies, but when the moment comes, I strike without mercy. I’ve sent traitors to their deaths, manipulated friends, and embraced violence when necessary. My father’s ghost called for vengeance, and I delivered. If others think me cruel, so be it—betrayal deserves no pity.\"\n`\n\n![Image](https://github.com/user-attachments/assets/ab525433-bcc1-48e3-80c6-afc416e89117)\n\n![Image](https://github.com/user-attachments/assets/f5407ee1-5d43-4b1f-a06a-6dd7458b7fa0)\n\nCompelete Output:\n`\nHamlet's Character Overview\n\nHamlet is depicted as a complex figure torn between indecision driven by overthinking and a ruthless pursuit of vengeance. His indecisiveness stems from existential doubt, fear of consequences, and a mind that acts as his \"greatest enemy\" (KG: The Indecisive Overthinker.txt). Overthinking paralyzes him, creating cycles of hesitation where he second-guesses every decision and fixates on potential negative outcomes. This mental paralysis manifests as inaction, even when he recognizes the necessity of action. Conversely, his vengeful side embraces violence as a \"necessary means to achieve vengeance\" (KG: The Ruthless Avenger.txt), driven by his father’s ghost and a moral framework justifying cruelty in the name of justice.\n\n\nKey Traits and Contradictions\n\n\n\nIndecision and Overthinking:\n\nHamlet’s analytical nature leads to excessive doubt and procrastination. His mind amplifies fears of consequences, trapping him in \"endless 'what ifs'\" (DC: Source 1). Existential doubt further fuels this cycle, making decisions feel insurmountable. Relationships like those between Overthinking and Hesitation (KG: id21, 22) highlight how his cognitive struggles directly hinder action.\n\n\n\n\nVengeance and Violence:\n\nWhile indecisive in personal matters, Hamlet acts decisively when pursuing vengeance. He views violence as \"necessary\" to punish betrayal (KG: id30) and frames his actions as fulfilling a just cause (KG: id26). However, his merciless methods are perceived as cruel by others, creating a moral contradiction (KG: id29, 10).\n\n\n\n\nConclusion\n\nHamlet’s duality—philosophical inaction versus violent resolve—defines his character. His internal conflicts (mind vs. action) and external actions (vengeance) reflect a struggle between introspection and retribution, making him a tragic figure caught between thought and consequence.\n\n\n\nReferences:\n\n\n\n[DC] \"I'm Hamlet—a man who thinks too much and acts too little...\" (File: The Indecisive Overthinker.txt)\n\n[DC] \"I'm Hamlet—a man who doesn’t hesitate when justice demands blood...\" (File: The Ruthless Avenger.txt)\n\n[KG] \"Hamlet's overthinking is the root cause of his indecision...\" (Relationship id1, File: The Indecisive Overthinker.txt)\n\n[KG] \"Hamlet embraces violence as a necessary tool to fulfill his vengeance.\" (Relationship id5, File: The Ruthless Avenger.txt)\n\n[KG] \"Vengeance is directly tied to the betrayal committed against Hamlet's father...\" (Relationship id30, File: The Ruthless Avenger.txt)\n`\n\nIn sum, If we store everything in one database, every query will extract all the knowledge about this entity, whether they are valid or invalid, real or fake.\n\nEach node saved under Neo4j database has file_path and upload date properties, **is there a way to only fetch nodes with specific value of the property, like upload file name or upload date?**\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Exploding-Soda",
      "author_type": "User",
      "created_at": "2025-04-02T09:49:11Z",
      "updated_at": "2025-04-03T01:16:46Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1254/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1254",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1254",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:53.160800",
      "comments": [
        {
          "author": "Exploding-Soda",
          "body": "I realize that this problem may have a more professional solution, or in order to achieve the function I mentioned, it may require a very large code modification, so I think it may not be in the developer's plan. And the method I mentioned may not be rigorous enough to meet the conditions for making",
          "created_at": "2025-04-03T01:16:45Z"
        }
      ]
    },
    {
      "issue_number": 915,
      "title": "[Bug]: Neo.TransientError.Request.NoThreadsAvailable} {message: There are no available threads to serve this request at the moment.",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nfull error\nNeo.TransientError.Request.NoThreadsAvailable} {message: There are no available threads to serve this request at the moment. You can retry at a later time or consider increasing max thread pool size for bolt connector(s)\n\nWhen exec\nfinal_response = await self.__model.aquery(\n            query=query,\n            param=query_param,\n            system_prompt=base_prompt,\n        )\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "ultrageopro",
      "author_type": "User",
      "created_at": "2025-02-21T14:23:04Z",
      "updated_at": "2025-04-02T09:27:04Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/915/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/915",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/915",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:53.377626",
      "comments": [
        {
          "author": "ultrageopro",
          "body": "@ParisNeo ",
          "created_at": "2025-02-21T15:31:34Z"
        },
        {
          "author": "bzImage",
          "body": "neo4j ? \n\nmost likely u need to raise the default config of neo4j..\n\n\n",
          "created_at": "2025-02-21T23:02:07Z"
        },
        {
          "author": "seanzhang-zhichen",
          "body": "The main cause of this problem is that the concurrency is too high, and Neo4j is unable to handle such a large number of concurrent requests. Modifying the Neo4j configuration cannot fundamentally solve the problem. The best solution is to limit the concurrency of queries. You can follow these steps",
          "created_at": "2025-04-02T09:27:04Z"
        }
      ]
    },
    {
      "issue_number": 1252,
      "title": "[Question]: <title>How does LightRAG chunk documents during Indexing phrase?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nAccording to the paper: \"To ensure consistency, the chunk size is set to 1200 across all datasets.\" So I assume each chunk might should be the same length. However I observered log of vLLM and it shows not only the length of each chunk can be very different, but also lot chunks were being truncated during the Indexing:\n(Im indexing some of the contents from \"Journey to the West\", and if I set everything right which I hope so, these are some truncations.) \nIm using the commit on Mar 26, 2025 but I took a glance late commits they didn't mention about chunking so I assume this might be the same in later updates.\n\nSo the question is how does LightRAG chunk the raw documents\n\n![Image](https://github.com/user-attachments/assets/25205b14-1e45-4cc8-891a-ba9b2aed08bd)\n\n![Image](https://github.com/user-attachments/assets/e4dd159c-dad7-4d0d-a484-d4c1e64a3ed9)\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "Exploding-Soda",
      "author_type": "User",
      "created_at": "2025-04-02T06:40:29Z",
      "updated_at": "2025-04-02T07:46:23Z",
      "closed_at": "2025-04-02T07:46:22Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1252/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1252",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1252",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:53.621306",
      "comments": [
        {
          "author": "Exploding-Soda",
          "body": "I checked https://learnopencv.com/lightrag/\n\nSo each chunk is indeed less than or equal to 1200 tokens under kv_store_text_chunks.json.\nIt's not a must that the content of each chunk will only appear once. For example, the same 100 tokens of content may appear in two chunks at the same time. And thi",
          "created_at": "2025-04-02T07:46:22Z"
        }
      ]
    },
    {
      "issue_number": 1191,
      "title": "[Bug]: server error on chat: cosine_better_than_threshold",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen chatting on lightrag-server I keep seeing this error. (Using openai for llm, ollama for embedding)\n\n500 Internal Server Error\n\"{\"detail\":\"'NoneType' object has no attribute 'cosine_better_than_threshold'\"}\"\n/query/stream\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "promet99",
      "author_type": "User",
      "created_at": "2025-03-26T07:51:17Z",
      "updated_at": "2025-04-02T07:04:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1191/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1191",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1191",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:53.900749",
      "comments": [
        {
          "author": "berataydin",
          "body": "Same llm configuration same error",
          "created_at": "2025-04-02T07:04:19Z"
        }
      ]
    },
    {
      "issue_number": 1144,
      "title": "LightRAG Initialization Fails if VectorDB is used as QDRANT",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nINFO: Connected to lightragchunk-entity-relation at xxxxxxxxxxxxxxxxxx\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/app/lightrag/api/lightrag_server.py\", line 604, in <module>\n    main()\n  File \"/app/lightrag/api/lightrag_server.py\", line 581, in main\n    app = create_app(args)\n          ^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/api/lightrag_server.py\", line 269, in create_app\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 37, in __init__\n  File \"/app/lightrag/lightrag.py\", line 387, in __post_init__\n    self.entities_vdb: BaseVectorStorage = self.vector_db_storage_cls(  # type: ignore\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/utils.py\", line 891, in import_class\n    return cls(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\nTypeError: Can't instantiate abstract class QdrantVectorDBStorage with abstract methods get_by_id, get_by_ids\n\n\n\n### Steps to reproduce\n\nConfigure in .env file LIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage \n\n### Expected Behavior\n\nService is not coming up as methods  get_by_id/get_by_ids is missing in qdrant_imply.py\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.2.6\n- Operating System: Debian GNU/Linux 12 (bookworm)\n- Python Version: 3.12.3\n- Related Issues:\n",
      "state": "open",
      "author": "sudhirvadlamani",
      "author_type": "User",
      "created_at": "2025-03-20T15:41:49Z",
      "updated_at": "2025-04-02T06:45:29Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1144/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1144",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1144",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:54.167180",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The current QDRANT storage implementation is out of maintain, seeking developer(s) with QDRANT database expertise for urgent maintenance.",
          "created_at": "2025-03-22T02:14:03Z"
        },
        {
          "author": "towpi13",
          "body": "just added the fix hope it'll approved soon",
          "created_at": "2025-04-02T06:45:29Z"
        }
      ]
    },
    {
      "issue_number": 1180,
      "title": "[Question]: What is stable to put on production ?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nTried Postgres backend for everything.. it has this error #1176 .. \n\nNeo4j has this error #1179 \n\nWhat can we use that is stable .. as storage ?\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "bzImage",
      "author_type": "User",
      "created_at": "2025-03-24T22:55:31Z",
      "updated_at": "2025-04-01T08:00:17Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1180/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1180",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1180",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:54.363338",
      "comments": [
        {
          "author": "frederikhendrix",
          "body": "I got it working using MongoDB, Redis, Neo4j and Milvus. Are you running everything in docker? Have you tested setting up each solution seperately so it's not comming down to bad connection configurations? \n\nWhen load testing my stack I also got into a problem with neo4j hitting the Timeout connecti",
          "created_at": "2025-03-26T08:03:36Z"
        },
        {
          "author": "bzImage",
          "body": "I first tried with mongodb but.. 2 weeks ago it was broken ..\n\nso tried with milvus- neo4j - redis .. neo4j is broken..\n\n so i tried with postgres, but, it seems that when you really push documents/relations/entities.. (>1000 entities) it goes down in flames..\n\n",
          "created_at": "2025-03-27T15:37:42Z"
        },
        {
          "author": "bzImage",
          "body": "BTW if you store everything in text files.. it works.. it even works faster than using a proper database behind..\n\n",
          "created_at": "2025-03-27T15:38:22Z"
        },
        {
          "author": "frederikhendrix",
          "body": "> I first tried with mongodb but.. 2 weeks ago it was broken ..\n> \n> so tried with milvus- neo4j - redis .. neo4j is broken..\n> \n> so i tried with postgres, but, it seems that when you really push documents/relations/entities.. (>1000 entities) it goes down in flames..\n\nNeo4j isn't broken and MongoD",
          "created_at": "2025-03-28T07:25:45Z"
        },
        {
          "author": "bzImage",
          "body": ">I was thinking of posting how I improved Neo4j to become 5x faster and a lot less CPU heavy.\nplease do..\n\n",
          "created_at": "2025-03-28T20:47:32Z"
        }
      ]
    },
    {
      "issue_number": 1245,
      "title": "How to use markdown data?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n![Image](https://github.com/user-attachments/assets/d0d8831c-1d34-4856-a8d0-be2eb511c643). I convert pdf data to markdown. How to use this format data just like in the image? \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "FanZhang91",
      "author_type": "User",
      "created_at": "2025-04-01T03:49:29Z",
      "updated_at": "2025-04-01T05:52:13Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1245/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1245",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1245",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:54.531702",
      "comments": [
        {
          "author": "0bserver07",
          "body": "which pipeline are you using the lightrag-server or one of the scripts?\nBy default it picks up any text and you can read with python then pass into the rag.insert()\nbut there are also examples on how to utilize other types of docs.\ncheck this for inspiration https://github.com/HKUDS/LightRAG/blob/7a",
          "created_at": "2025-04-01T05:52:12Z"
        }
      ]
    },
    {
      "issue_number": 1206,
      "title": "[Feature Request]: support token consume statistic",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nwhen i use lightrag via calling api, i want to know the token consumed.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "Xinteny",
      "author_type": "User",
      "created_at": "2025-03-27T09:33:15Z",
      "updated_at": "2025-04-01T05:42:10Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1206/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1206",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1206",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:54.695543",
      "comments": [
        {
          "author": "Xinteny",
          "body": "ok, the new version has supported token usage statistics, but i can't know the token consumed while inserting a new document to kg. I want to implement this function, maybe a context manager can?",
          "created_at": "2025-04-01T05:42:09Z"
        }
      ]
    },
    {
      "issue_number": 1244,
      "title": "[Feature Suggestion]: Change / Pass Workspace Name in PgSQL Storages",
      "body": "\n\nWould you folks like me to do a PR to fix this feature?\n\n\nCurrently every PgStorage \"workspace\" is set to default, regardless of changing the config in the LightRAG Client\n\n\nI'll explain how I will modify the code to implement approach. First, let me show you the changes we need to make:\n\n```python:lightrag/kg/postgres_impl.py\nclass ClientManager:\n    _instances: dict[str, Any] = {\"db\": None, \"ref_count\": 0}\n    _lock = asyncio.Lock()\n\n    @staticmethod\n    def get_config(global_config: dict[str, Any] | None = None) -> dict[str, Any]:\n        # First try to get workspace from global config\n        workspace = None\n        if global_config and \"vector_db_storage_cls_kwargs\" in global_config:\n            workspace = global_config[\"vector_db_storage_cls_kwargs\"].get(\"workspace\")\n\n        # Read standard config\n        config = configparser.ConfigParser()\n        config.read(\"config.ini\", \"utf-8\")\n\n        return {\n            \"host\": os.environ.get(\n                \"POSTGRES_HOST\",\n                config.get(\"postgres\", \"host\", fallback=\"localhost\"),\n            ),\n            \"port\": os.environ.get(\n                \"POSTGRES_PORT\", config.get(\"postgres\", \"port\", fallback=5432)\n            ),\n            \"user\": os.environ.get(\n                \"POSTGRES_USER\",\n                config.get(\"postgres\", \"user\", fallback=None),\n            ),\n            \"password\": os.environ.get(\n                \"POSTGRES_PASSWORD\",\n                config.get(\"postgres\", \"password\", fallback=None),\n            ),\n            \"database\": os.environ.get(\n                \"POSTGRES_DATABASE\",\n                config.get(\"postgres\", \"database\", fallback=None),\n            ),\n            # Use workspace from global config if available, otherwise fall back to env/config.ini\n            \"workspace\": workspace or os.environ.get(\n                \"POSTGRES_WORKSPACE\",\n                config.get(\"postgres\", \"workspace\", fallback=\"default\"),\n            ),\n        }\n\n    @classmethod\n    async def get_client(cls, global_config: dict[str, Any] | None = None) -> PostgreSQLDB:\n        async with cls._lock:\n            if cls._instances[\"db\"] is None:\n                config = cls.get_config(global_config)\n                db = PostgreSQLDB(config)\n                await db.initdb()\n                await db.check_tables()\n                cls._instances[\"db\"] = db\n                cls._instances[\"ref_count\"] = 0\n            cls._instances[\"ref_count\"] += 1\n            return cls._instances[\"db\"]\n```\nNow we need to modify the storage classes to pass their global config to the ClientManager. Let me update those:\n\n```python:lightrag/kg/postgres_impl.py\n@final\n@dataclass\nclass PGKVStorage(BaseKVStorage):\n    db: PostgreSQLDB = field(default=None)\n\n    def __post_init__(self):\n        namespace_prefix = self.global_config.get(\"namespace_prefix\")\n        self.base_namespace = self.namespace.replace(namespace_prefix, \"\")\n        self._max_batch_size = self.global_config[\"embedding_batch_num\"]\n\n    async def initialize(self):\n        if self.db is None:\n            self.db = await ClientManager.get_client(self.global_config)\n\n// ... existing code ...\n\n@final\n@dataclass\nclass PGVectorStorage(BaseVectorStorage):\n    db: PostgreSQLDB | None = field(default=None)\n\n    def __post_init__(self):\n        self._max_batch_size = self.global_config[\"embedding_batch_num\"]\n        namespace_prefix = self.global_config.get(\"namespace_prefix\")\n        self.base_namespace = self.namespace.replace(namespace_prefix, \"\")\n        config = self.global_config.get(\"vector_db_storage_cls_kwargs\", {})\n        cosine_threshold = config.get(\"cosine_better_than_threshold\")\n        if cosine_threshold is None:\n            raise ValueError(\n                \"cosine_better_than_threshold must be specified in vector_db_storage_cls_kwargs\"\n            )\n        self.cosine_better_than_threshold = cosine_threshold\n\n    async def initialize(self):\n        if self.db is None:\n            self.db = await ClientManager.get_client(self.global_config)\n\n// ... existing code ...\n\n@final\n@dataclass\nclass PGDocStatusStorage(DocStatusStorage):\n    db: PostgreSQLDB = field(default=None)\n\n    async def initialize(self):\n        if self.db is None:\n            self.db = await ClientManager.get_client(self.global_config)\n```\nHere's what these changes do:\n\n1. In `ClientManager`:\n   - Added `global_config` parameter to `get_config` and `get_client` methods\n   - The `get_config` method now first checks for workspace in the global config's `vector_db_storage_cls_kwargs`\n   - If found, uses that workspace; otherwise falls back to environment variables and config.ini\n\n2. In the storage classes (`PGKVStorage`, `PGVectorStorage`, `PGDocStatusStorage`):\n   - Modified `initialize` methods to pass their `global_config` to `ClientManager.get_client()`\n   - This ensures the workspace setting from your configuration gets used\n\nNow when you run LightRAG client:\n1. The workspace from can be set in the config\n2. This config flows through LightRAG to the storage classes\n3. The storage classes pass it to ClientManager\n4. ClientManager uses this workspace value instead of falling back to default\n\nThis should make PostgreSQL respect the workspace setting you provide in your configuration.\n\n### Additional Context\n\n```markdown\n\n+++ b/lightrag/kg/postgres_impl.py\n@@ -192,7 +192,13 @@ class ClientManager:\n     _lock = asyncio.Lock()\n\n     @staticmethod\n-    def get_config() -> dict[str, Any]:\n+    def get_config(global_config: dict[str, Any] | None = None) -> dict[str, Any]:\n+        # First try to get workspace from global config\n+        workspace = None\n+        if global_config and \"vector_db_storage_cls_kwargs\" in global_config:\n+            workspace = global_config[\"vector_db_storage_cls_kwargs\"].get(\"workspace\")\n+\n+        # Read standard config\n         config = configparser.ConfigParser()\n         config.read(\"config.ini\", \"utf-8\")\n\n@@ -205,7 +211,8 @@ class ClientManager:\n                 \"POSTGRES_PORT\", config.get(\"postgres\", \"port\", fallback=5432)\n             ),\n             \"user\": os.environ.get(\n-                \"POSTGRES_USER\", config.get(\"postgres\", \"user\", fallback=None)\n+                \"POSTGRES_USER\",\n+                config.get(\"postgres\", \"user\", fallback=None),\n             ),\n             \"password\": os.environ.get(\n                 \"POSTGRES_PASSWORD\",\n@@ -215,17 +222,18 @@ class ClientManager:\n                 \"POSTGRES_DATABASE\",\n                 config.get(\"postgres\", \"database\", fallback=None),\n             ),\n-            \"workspace\": os.environ.get(\n+            # Use workspace from global config if available, otherwise fall back to env/config.ini\n+            \"workspace\": workspace or os.environ.get(\n                 \"POSTGRES_WORKSPACE\",\n                 config.get(\"postgres\", \"workspace\", fallback=\"default\"),\n             ),\n         }\n\n     @classmethod\n-    async def get_client(cls) -> PostgreSQLDB:\n+    async def get_client(cls, global_config: dict[str, Any] | None = None) -> PostgreSQLDB:\n         async with cls._lock:\n             if cls._instances[\"db\"] is None:\n-                config = ClientManager.get_config()\n+                config = cls.get_config(global_config)\n                 db = PostgreSQLDB(config)\n                 await db.initdb()\n                 await db.check_tables()\n@@ -260,7 +268,7 @@ class PGKVStorage(BaseKVStorage):\n\n     async def initialize(self):\n         if self.db is None:\n-            self.db = await ClientManager.get_client()\n+            self.db = await ClientManager.get_client(self.global_config)\n\n     async def finalize(self):\n         if self.db is not None:\n@@ -405,7 +413,7 @@ class PGVectorStorage(BaseVectorStorage):\n\n     async def initialize(self):\n         if self.db is None:\n-            self.db = await ClientManager.get_client()\n+            self.db = await ClientManager.get_client(self.global_config)\n\n     async def finalize(self):\n         if self.db is not None:\n@@ -698,7 +706,7 @@ class PGDocStatusStorage(DocStatusStorage):\n\n     async def initialize(self):\n         if self.db is None:\n-            self.db = await ClientManager.get_client()\n         config.read(\"config.ini\", \"utf-8\")\n\n@@ -205,7 +211,8 @@ class ClientManager:\n                 \"POSTGRES_PORT\", config.get(\"postgres\", \"port\", fallback=5432)\n             ),\n             \"user\": os.environ.get(\n-                \"POSTGRES_USER\", config.get(\"postgres\", \"user\", fallback=None)\n+                \"POSTGRES_USER\",\n+                config.get(\"postgres\", \"user\", fallback=None),\n             ),\n             \"password\": os.environ.get(\n                 \"POSTGRES_PASSWORD\",\n@@ -215,17 +222,18 @@ class ClientManager:\n                 \"POSTGRES_DATABASE\",\n                 config.get(\"postgres\", \"database\", fallback=None),\n             ),\n-            \"workspace\": os.environ.get(\n+            # Use workspace from global config if available, otherwise fall back to env/config.ini\n+            \"workspace\": workspace or os.environ.get(\n                 \"POSTGRES_WORKSPACE\",\n                 config.get(\"postgres\", \"workspace\", fallback=\"default\"),\n             ),\n         }\n\n     @classmethod\n-    async def get_client(cls) -> PostgreSQLDB:\n+    async def get_client(cls, global_config: dict[str, Any] | None = None) -> PostgreSQLDB:\n         async with cls._lock:\n             if cls._instances[\"db\"] is None:\n-                config = ClientManager.get_config()\n+                config = cls.get_config(global_config)\n                 db = PostgreSQLDB(config)\n                 await db.initdb()\n                 await db.check_tables()\n@@ -260,7 +268,7 @@ class PGKVStorage(BaseKVStorage):\n\n     async def initialize(self):\n         if self.db is None:\n-            self.db = await ClientManager.get_client()\n+            self.db = await ClientManager.get_client(self.global_config)\n\n     async def finalize(self):\n         if self.db is not None:\n@@ -405,7 +413,7 @@ class PGVectorStorage(BaseVectorStorage):\n\n     async def initialize(self):\n         if self.db is None:\n-            self.db = await ClientManager.get_client()\n+            self.db = await ClientManager.get_client(self.global_config)\n\n     async def finalize(self):\n         if self.db is not None:\n@@ -698,7 +706,7 @@ class PGDocStatusStorage(DocStatusStorage):\n\n     async def initialize(self):\n         if self.db is None:\n-            self.db = await ClientManager.get_client()\n+            self.db = await ClientManager.get_client(self.global_config)\n\n     async def finalize(self):\n         if self.db is not None:\n\n```",
      "state": "open",
      "author": "0bserver07",
      "author_type": "User",
      "created_at": "2025-04-01T03:14:46Z",
      "updated_at": "2025-04-01T04:07:39Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1244/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1244",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1244",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:54.863946",
      "comments": []
    },
    {
      "issue_number": 1227,
      "title": "[Bug]: 关于1.3.0 版本问题",
      "body": "\n版本：\n`\"lightrag-hku[api]>=1.3.0\"`\n\n文件构建成功后，在查询时报以下错误：\n\n<img width=\"1096\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e7aceb67-9853-4c8c-ab01-7a0da583af92\" />\n\n检查了一下提交记录，是 `operate.py` 这个文件有点问题。 [commit](https://github.com/HKUDS/LightRAG/commit/87fbffde14a93f2ca39f2a3f4affbac120a7722d#diff-18e047fa54c9c5637f6c89f2b32a3ea0115b41e3730661f2c9650cdf8f31627a)\n\n<img width=\"1395\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/06e151f0-f8e4-4f84-a141-dcb66c1be519\" />\n\n我比对了 main 分支和 1.3.0 tag的代码，发现 1.3.0 tag 没有加  file_path, 但我通过 pypi 下载的版本里却有。是不是发布的 1.3.0 版本有点问题。\n\n",
      "state": "closed",
      "author": "jmaxhu",
      "author_type": "User",
      "created_at": "2025-03-29T11:07:58Z",
      "updated_at": "2025-04-01T01:31:34Z",
      "closed_at": "2025-04-01T01:31:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1227/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1227",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1227",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:54.863972",
      "comments": [
        {
          "author": "LarFii",
          "body": "因为不是同时发布的，所以有点差别，之后会按tag上传，以保证完全同步",
          "created_at": "2025-03-29T12:18:23Z"
        },
        {
          "author": "jmaxhu",
          "body": "> 因为不是同时发布的，所以有点差别，之后会按tag上传，以保证完全同步\n\n这个问题怎么解决呢？  我用的是 pgsql 的存储。",
          "created_at": "2025-03-30T08:18:24Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "Here's a quick fix I used to solve this issue in the meantime. Hope it helps!\nhttps://github.com/HKUDS/LightRAG/pull/1243",
          "created_at": "2025-03-31T23:36:28Z"
        },
        {
          "author": "jmaxhu",
          "body": "@JoramMillenaar Thanks!",
          "created_at": "2025-04-01T00:34:36Z"
        }
      ]
    },
    {
      "issue_number": 968,
      "title": "[Question]: Custom Ontology",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIs there a way to specify a custom ontology as an input for matching entities and relationships more precisely. for example match an arbitary financial PDF against the [FIBO Ontology](https://schema.org/docs/financial.html) \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "kiritbasu",
      "author_type": "User",
      "created_at": "2025-03-01T04:08:26Z",
      "updated_at": "2025-03-31T13:49:59Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/968/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/968",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/968",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:56.762716",
      "comments": [
        {
          "author": "Michael-Bar",
          "body": "I am also interested in this. There are currently ways to edit the entities (nodes) but not the relations (edges) in the subsequent knowledge graph in LightRAG",
          "created_at": "2025-03-04T02:00:51Z"
        },
        {
          "author": "soichisumi",
          "body": "+1\nI also need this feature.\nPerhaps currently lightrag can modify kg by using custom kg feature but cannot force ontology(schema for kg), is it correct? @LarFii \n",
          "created_at": "2025-03-31T00:27:19Z"
        },
        {
          "author": "soichisumi",
          "body": "Related issue\nhttps://github.com/HKUDS/LightRAG/issues/405",
          "created_at": "2025-03-31T00:38:06Z"
        },
        {
          "author": "choizhang",
          "body": "I think the knowledge graph constructed through LLM extraction lacks clear ontology concepts. I don't know if this is the case",
          "created_at": "2025-03-31T13:49:59Z"
        }
      ]
    },
    {
      "issue_number": 1234,
      "title": "[Question]: Is LightRAG agentic?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIs LightRAG agentic?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "look4pritam",
      "author_type": "User",
      "created_at": "2025-03-31T03:45:40Z",
      "updated_at": "2025-03-31T13:06:37Z",
      "closed_at": "2025-03-31T13:06:37Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1234/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1234",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1234",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:56.950156",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG acts as RAG engine for AI Agents. You can wrap LightRAG as a Tool for any AI Agent. What's more LightRAG Server can act as an Ollama model to any AI Agents or chat bots.",
          "created_at": "2025-03-31T05:57:15Z"
        }
      ]
    },
    {
      "issue_number": 1238,
      "title": "[Question]: <title>我在本地，部署api，可以访问到，但是我使用其他电脑无法访问到",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n`import requests\nimport json\nimport time\n\n\nstart_time = time.time()\n\n# 1.url\n# url = 'http://xxxxxxxxxxxxx:8020/query'  # 你的IP\nurl = 'http://xxxxxxxxxxxxx:8020/query'  # 你的IP\n\n# 2.data\ndata = {\"query\": \"苏州大学图书馆都有哪些书籍\", \"mode\":\"naive\"}\n\n# 3.将字典转换为 JSON 字符串\njson_payload = json.dumps(data)\n\n# 4.发送 POST 请求\nheaders = {'Content-Type': 'application/json'}\nresponse = requests.post(url, data=json_payload, headers=headers)\n\n# 5.打印响应内容\nprint(response.json().get(\"data\", []))  # 命令行启动，用这个打印\n# print(response.json())\nprint(f\"耗时{time.time()-start_time}\")\n`\n\n我隐藏了自己的IP地址，使用curl也无法调用。设备已联网，本地同命令可以运行。\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "sherlockma11",
      "author_type": "User",
      "created_at": "2025-03-31T08:33:00Z",
      "updated_at": "2025-03-31T12:28:15Z",
      "closed_at": "2025-03-31T12:28:15Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1238/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1238",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1238",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.118467",
      "comments": []
    },
    {
      "issue_number": 1241,
      "title": "[Bug]: Error while clearing cache: 'PGKVStorage' object has no attribute 'delete'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI am using instructions provided in the doc in order to clear the cache:\n        \n`await rag.aclear_cache(modes=[\"naive\", \"local\", \"global\", \"hybrid\", \"mix\"])`\n\nBut it displays the following error message in the console:\n\n`Error while clearing cache: 'PGKVStorage' object has no attribute 'delete'`\n\n\n\n### Steps to reproduce\n\nJust launch the following code:\n\n```python\nimport os\nimport asyncio\nimport time\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom dotenv import load_dotenv\nfrom sentence_transformers import SentenceTransformer\nfrom lightrag.utils import setup_logger\nfrom google import genai\nfrom google.genai import types\nimport nest_asyncio\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nsetup_logger(\"lightrag-gemini-demo\", level=\"DEBUG\")\n\n# Read env variables from the .env file\nload_dotenv()\n\nWORKING_DIR = \"./cache/gemini-demo\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    # 1. Initialize the GenAI Client with your Gemini API Key\n    client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n\n    # 2. Combine prompts: system prompt, history, and user prompt\n    if history_messages is None:\n        history_messages = []\n\n    combined_prompt = \"\"\n    if system_prompt:\n        combined_prompt += f\"{system_prompt}\\n\"\n\n    for msg in history_messages:\n        # Each msg is expected to be a dict: {\"role\": \"...\", \"content\": \"...\"}\n        combined_prompt += f\"{msg['role']}: {msg['content']}\\n\"\n\n    # Finally, add the new user prompt\n    combined_prompt += f\"user: {prompt}\"\n\n    # 3. Call the Gemini model\n    response = client.models.generate_content(\n        model=os.environ[\"GOOGLE_MODEL\"],\n        contents=[combined_prompt],\n        config=types.GenerateContentConfig(max_output_tokens=500, temperature=0.1),\n    )\n\n    # 4. Return the response text\n    return response.text\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    embeddings = model.encode(texts, convert_to_numpy=True)\n    return embeddings\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\nasync def initialize_rag():\n    embedding_dimension = await get_embedding_dim()\n    print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dimension,\n            max_token_size=8192,\n            func=embedding_func,\n        ),\n        llm_model_max_async=4,\n        enable_llm_cache_for_entity_extract=True,\n        graph_storage=\"Neo4JStorage\", \n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        vector_storage=\"PGVectorStorage\",\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def main():\n    try:\n        # Initialize RAG instance\n        rag = await initialize_rag()\n\n        # Clear cache for multiple modes\n        await rag.aclear_cache(modes=[\"naive\", \"local\", \"global\", \"hybrid\", \"mix\"])\n\n        book_file_path = \"book.txt\"\n        with open(book_file_path, \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read(), file_paths = [book_file_path])\n\n        # Query using different retrieval modes\n        modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n        query = \"What are the top themes in this story?\"\n\n        for mode in modes:\n            start_time = time.time()\n            print(f\"-------------------------------------------------------------\")\n            print(f\"Results using {mode} mode:\\n\")\n            print(await rag.aquery(query, param=QueryParam(mode=mode)))\n            print(f\"Query Time (mode={mode}): {time.time() - start_time} seconds\")\n        \n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n\n### Expected Behavior\n\nThere should not be any error message related to cache cleanup.\n\n\n### LightRAG Config Used\n\nUsing:\n\n- PosgreSQL 17.4\n- Neo4J 5.27.0 (community)\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 3.0\n- Operating System: MacOS (Darwin Kernel Version 24.3.0) \n- Python Version: 3.10.16\n- Related Issues:\n",
      "state": "open",
      "author": "vmahe35",
      "author_type": "User",
      "created_at": "2025-03-31T10:57:21Z",
      "updated_at": "2025-03-31T11:14:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1241/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1241",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1241",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.118491",
      "comments": []
    },
    {
      "issue_number": 1175,
      "title": "Compatibility of LightRAG with Azure Cosmos DB (MongoDB API)",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI'm considering using LightRAG in an Azure environment where MongoDB is provided through Cosmos DB's MongoDB API. However, Azure Cosmos DB offers partial compatibility with the MongoDB API, supporting around 32% of MongoDB's features (especially limitations in aggregation pipelines, specific indexes, and certain MongoDB commands).\n\nCould you clarify whether LightRAG relies on specific MongoDB functionalities, aggregation pipelines, or indexing methods that may not be supported by Cosmos DB? If possible, could you outline briefly which MongoDB features or commands LightRAG uses that might face compatibility issues? \n\nThanks in advance for your insights!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "stevenveenma",
      "author_type": "User",
      "created_at": "2025-03-24T14:47:54Z",
      "updated_at": "2025-03-31T09:41:17Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1175/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1175",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1175",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.118499",
      "comments": [
        {
          "author": "stevenveenma",
          "body": "An additional question on this: The logger in lightrag.py creates a new folder and writes logging to it. The webapp deployment environment is read-only. So to be able to deploy and use Lighrag in Azure Webapps the logging should be stored on another location (blob storage?), suppressed or integrated",
          "created_at": "2025-03-31T09:40:57Z"
        }
      ]
    },
    {
      "issue_number": 1240,
      "title": "[Question]: <title> How to get the reply content position of the origin document",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHow to achieve that the content retrieved through lightrag also contains the coordinate position in the original text corresponding to the content?\n\nFor example, if a user queries the specific meaning of a keyword in a document through the lightrag query interface, lightrag gives some replies, but the user also wants to know that replies content position of the original document\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-03-31T09:28:42Z",
      "updated_at": "2025-03-31T09:28:42Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1240/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1240",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1240",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.332895",
      "comments": []
    },
    {
      "issue_number": 1174,
      "title": "[Bug]: rag.insert()出错，1.2.3版本之前好的，升级到最新版本错误，无法更新",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n  @@今天升级到了1.2.6版本，发觉insert方法出错\n错误信息：\n  File \"..\\fastApiServer.py\", line 375, in insert_endpoint\n    result = rag.insert(request.text,ids=unique_id)\n  File \"..\\lib\\site-packages\\lightrag\\lightrag.py\", line 565, in insert\n    loop.run_until_complete(\n  File \"\\lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n    return f.result()\n  File \"..\\lib\\asyncio\\futures.py\", line 201, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"..\\lib\\asyncio\\tasks.py\", line 232, in __step\n    result = coro.send(None)\n  File \"..\\lib\\site-packages\\lightrag\\lightrag.py\", line 590, in ainsert\n    await self.apipeline_enqueue_documents(input, ids, file_paths)\n  File \"..\\lib\\site-packages\\lightrag\\lightrag.py\", line 772, in apipeline_enqueue_documents\n    unique_new_doc_ids = await self.doc_status.filter_keys(all_new_doc_ids)\n  File \"..\\lib\\site-packages\\lightrag\\kg\\json_doc_status_impl.py\", line 56, in filter_keys\n    async with self._storage_lock:\nAttributeError: __aenter__\n\n\ninsert核心方法是这么写的：\n       # 生成唯一ID\n        import uuid\n        unique_id = str(uuid.uuid4())\n        print(unique_id)\n\n        #await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        result = await loop.run_in_executor(\n            None, lambda: rag.ainsert(request.text,ids=unique_id)\n        )\n        print(unique_id)\n        print(result)\n我回退到1.2.3版本是正常的，但最新版出错，还请各位大拿帮忙下！\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "aiwenForGit",
      "author_type": "User",
      "created_at": "2025-03-24T13:48:26Z",
      "updated_at": "2025-03-30T10:10:55Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1174/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1174",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1174",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.332916",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "请升级到v1.3.0或最新版",
          "created_at": "2025-03-28T20:03:07Z"
        },
        {
          "author": "Buzeg",
          "body": "> 请升级到v1.3.0或最新版\n\n你好，我在1.2.3用openai_compatible_demo.py的时候是正常的，但是一来到1.3.0，对于同一个文件，却报错了，openai_compatible_demo.py也是用了最新的版本来跑，还是报错fail to extract entities and relationships\n",
          "created_at": "2025-03-30T10:10:54Z"
        }
      ]
    },
    {
      "issue_number": 1226,
      "title": "[Question]: 新手关于图谱可视化的问题",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n1. 图谱上没有显示关系， 是因为关系没有解析存储吗，还是有什么隐藏的功能需要开启？\n2. 现在是否支持编辑节点，在界面上手动创建新的lightrag没有解析出来的节点？\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "thunder95",
      "author_type": "User",
      "created_at": "2025-03-29T08:20:29Z",
      "updated_at": "2025-03-30T01:57:41Z",
      "closed_at": "2025-03-30T01:57:41Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1226/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1226",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1226",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.514266",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "1. 左下角有个设置按钮，勾选“边事件”，之后鼠标就可以选择边了\n2. 暂时还不支持节点编辑和通过UI添加节点",
          "created_at": "2025-03-29T14:05:41Z"
        },
        {
          "author": "thunder95",
          "body": "@danielaskdd 明白了，感谢大佬回复",
          "created_at": "2025-03-30T01:57:37Z"
        }
      ]
    },
    {
      "issue_number": 1169,
      "title": "[Question]: KeyError: 'history_messages'",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nKeyError: 'history_messages' while using NanoVectorDBStorage as vector storage and NetworkXStorage as graph storage\n\n### Additional Context\n\nlib/python3.12/site-packages/or_lib/lightrag.py\", line 1079, in apipeline_process_enqueue_documents\n    pipeline_status[\"history_messages\"].append(log_message)\n    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\nKeyError: 'history_messages'",
      "state": "closed",
      "author": "ShamshadAhmedShorthillsAI",
      "author_type": "User",
      "created_at": "2025-03-24T07:15:34Z",
      "updated_at": "2025-03-28T20:04:17Z",
      "closed_at": "2025-03-28T20:04:17Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1169/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1169",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1169",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.727457",
      "comments": [
        {
          "author": "stevenveenma",
          "body": "https://github.com/HKUDS/LightRAG/issues/1104?",
          "created_at": "2025-03-24T13:05:25Z"
        }
      ]
    },
    {
      "issue_number": 1216,
      "title": "[Question]: <title>excel文档去检索感觉准确率不是很高",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nexcel文档去检索感觉准确率不是很高\n体验了insertexcel文档 但是问题回答不了 感觉切片的数据也很乱  是否有新的微信群可以交流呢\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "wang-ship-it",
      "author_type": "User",
      "created_at": "2025-03-28T09:20:09Z",
      "updated_at": "2025-03-28T20:00:17Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1216/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1216",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1216",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:57.913133",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "<img width=\"329\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4aa412eb-abdd-485b-8caa-c42aa4d5b6ed\" />",
          "created_at": "2025-03-28T20:00:17Z"
        }
      ]
    },
    {
      "issue_number": 1218,
      "title": "[Bug]:  Wrong .env File Detection Path in LightRAG API",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nIncorrect .env file detection path in  [#1195 ](https://github.com/HKUDS/LightRAG/pull/1195)：\n\nThe check_env_file() function in utils_api.py incorrectly checks for the .env file in Working Directory instead of the API directory\n\nThis creates following issues:\nWorking directory constraint that requires users to start the server from a specific location\n\n\n## Current Behavior\n\n1. .env detection:\n```python\ndef check_env_file():\n    \"\"\"\n    Check if .env file exists and handle user confirmation if needed.\n    Returns True if should continue, False if should exit.\n    \"\"\"\n    if not os.path.exists(\".env\"):\n        warning_msg = (\n            \"Warning: .env file not found. Some features may not work properly.\"\n        )\n        ASCIIColors.yellow(warning_msg)\n\n        # Check if running in interactive terminal\n        if sys.stdin.isatty():\n            response = input(\"Do you want to continue? (yes/no): \")\n            if response.lower() != \"yes\":\n                ASCIIColors.red(\"Server startup cancelled\")\n                return False\n    return True\n```\n\n\n\n\n## Suggested Solutions\n\n1. Update environment file detection (just a example):\n````python\ndef check_env_file():\n    \"\"\"\n    Check if .env file exists and handle user confirmation if needed.\n    Returns True if should continue, False if should exit.\n    \"\"\"\n    # Get the absolute path of the current script's directory\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    env_path = os.path.join(current_dir, '.env')\n    \n    if not os.path.exists(env_path):\n        # Add debug information\n        warning_msg = (\n            f\"Warning: .env file not found at {env_path}. \"\n            \"Some features may not work properly.\"\n        )\n        ASCIIColors.yellow(warning_msg)\n        \n        # Check if running in an interactive terminal\n        if sys.stdin.isatty():\n            response = input(\"Do you want to continue? (yes/no): \")\n            if response.lower() != \"yes\":\n                ASCIIColors.red(\"Server startup cancelled\")\n                return False\n    else:\n        ASCIIColors.green(f\"Found .env file at: {env_path}\")\n    \n    return True\n\n````\n\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n1. The server should be able to locate the .env file regardless of the working directory\n2. Users should be able to start the server from any location\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Starfallan",
      "author_type": "User",
      "created_at": "2025-03-28T14:20:13Z",
      "updated_at": "2025-03-28T19:53:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1218/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1218",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1218",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.101905",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The requirement for the .env file to be in the startup directory is intentionally designed this way. The purpose is to support users in launching multiple LightRAG instances simultaneously. Allow different .env files for different instances.",
          "created_at": "2025-03-28T19:53:23Z"
        }
      ]
    },
    {
      "issue_number": 1142,
      "title": "[Question]: <title>Create a LightRAG with documents in French",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThank you for your nice work. It is very useful to achieve concrete results and it works very well on my example in English. Thanks!!!\n \nI want to create a LightRAG using **documents in French**. \nI am looking for recommendations for my aim.\nI am not an expert in LLM but to reach my aim, I will choose an embedding model good in French (e.g. \"sentence-transformers/all-MiniLM-L6-v2\" ) and a LLM strong in multilingual (e.g. Gemma3).\nThen I will try to use a custom prompt in French. Specifying to answer in French etc...\nMy question: shall I translate to French all the prompts: in prompt.py?\nAny other settings or code modifications recommended?\n\nThank you very much for your help.\n\n### Additional Context\n\nMy code is based on the example: [ollama](https://github.com/HKUDS/LightRAG/blob/main/examples/lightrag_ollama_demo.py )\n\nI am using a MacMini M4",
      "state": "open",
      "author": "roumat",
      "author_type": "User",
      "created_at": "2025-03-20T14:41:17Z",
      "updated_at": "2025-03-28T13:10:28Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1142/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1142",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1142",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.298435",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "There is summary language setting in .env file, which may address your issue.",
          "created_at": "2025-03-22T02:27:24Z"
        },
        {
          "author": "roumat",
          "body": "Thank you very much. I will try and give an update.",
          "created_at": "2025-03-28T13:10:27Z"
        }
      ]
    },
    {
      "issue_number": 1211,
      "title": "[Bug]: Invalid namespace prefixes for PSQL",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen you use lightrag with PSQL and AGE and pass a namespace prefix that starts with a number, the PSQL queries will fail and raise the following error. It seems that PSQL and AGE put restrictions on identifiers.\n\n```\n    raise PGGraphQueryException(\nlightrag.kg.postgres_impl.PGGraphQueryException: {'message': \"Error executing graph query: SELECT * FROM cypher('67c880chunk_entity_relation', $$\\n                        MATCH (n:Entity)\\n                        OPTIONAL MATCH (n)-[r]->(m:Entity)\\n                        RETURN n, r, m\\n                        LIMIT 1000\\n                      $$) AS (n agtype, r agtype, m agtype)\", 'wrapped': \"SELECT * FROM cypher('67c880chunk_entity_relation', $$\\n                        MATCH (n:Entity)\\n                        OPTIONAL MATCH (n)-[r]->(m:Entity)\\n                        RETURN n, r, m\\n                        LIMIT 1000\\n                      $$) AS (n agtype, r agtype, m agtype)\", 'detail': 'graph name is invalid'}\n^[[B^[[A^[[B^CINFO: Shutting down\n```\n\n### Steps to reproduce\n\n- Start a lightrag server using PSQL and age for all storage\n`lightrag-server --namespace-prefix 67c880`\n- Call any endpoint or simply go to the webui and it will throw an error.\n\n### Expected Behavior\n\nPerhaps an error should be thrown at initialization time, or some prefix should be added to the namespace-prefix (which is done for other identifiers, like the document ID being prefixed with 'doc-')\n\n### LightRAG Config Used\n\nLLM_BINDING=openai\nLLM_BINDING_HOST=https://api.openai.com/v1\nLLM_MODEL=gpt-4o-mini\nEMBEDDING_BINDING=openai\nEMBEDDING_MODEL=text-embedding-3-small\nEMBEDDING_DIM=1536\n\nPOSTGRES_USER=user\nPOSTGRES_DB=postgres\nPOSTGRES_PASSWORD=password\nPOSTGRES_DATABASE=postgres\n\nLIGHTRAG_GRAPH_STORAGE=PGGraphStorage\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=PGVectorStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "JoramMillenaar",
      "author_type": "User",
      "created_at": "2025-03-28T00:23:29Z",
      "updated_at": "2025-03-28T00:23:29Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1211/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1211",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1211",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.474801",
      "comments": []
    },
    {
      "issue_number": 1210,
      "title": "[Question]: Performance Table",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello author, how can I get the performance of other methods in Table 1 in the paper? Would you consider providing process files of other methods?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "small-code-cat",
      "author_type": "User",
      "created_at": "2025-03-28T00:23:19Z",
      "updated_at": "2025-03-28T00:23:19Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1210/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1210",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1210",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.474819",
      "comments": []
    },
    {
      "issue_number": 1203,
      "title": "[Question]: The get_summary function in step_2 is a bit strange",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\ndef get_summary(context, tot_tokens=2000):\n    tokens = tokenizer.tokenize(context)\n    half_tokens = tot_tokens // 2\n\n    start_tokens = tokens[1000 : 1000 + half_tokens]\n    end_tokens = tokens[-(1000 + half_tokens) : 1000]\n\n    summary_tokens = start_tokens + end_tokens\n    summary = tokenizer.convert_tokens_to_string(summary_tokens)\n\n    return summary\n\nThis end_tokens is always empty\nShould it be changed to tokens[-(1000 + half_tokens) : -1000]\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "small-code-cat",
      "author_type": "User",
      "created_at": "2025-03-27T07:00:54Z",
      "updated_at": "2025-03-27T07:00:54Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1203/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1203",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1203",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.474826",
      "comments": []
    },
    {
      "issue_number": 352,
      "title": "Can we Insert KG from Neo4j database?",
      "body": "Nice work!\r\nI noticed that your team added the ability to insert custom KG a few days ago, as well as the support for Neo4JStorage that already existed before.\r\nI wonder if there exists any way to get the KG directly from the Neo4j database for insertion?\r\nSuppose I use another tool (such as Docs2KG) to extract the file as KG and store it in Neo4j database, can I directly use Lightrag based on it?",
      "state": "closed",
      "author": "Yi-Eaaa",
      "author_type": "User",
      "created_at": "2024-11-29T16:05:18Z",
      "updated_at": "2025-03-27T00:59:36Z",
      "closed_at": "2025-02-17T10:59:02Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/352/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/352",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/352",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.474832",
      "comments": [
        {
          "author": "LarFii",
          "body": "Thank you for your suggestion! That's indeed a great idea, and we will consider adding support for it in the future.",
          "created_at": "2024-12-05T13:24:15Z"
        },
        {
          "author": "adamwuyu",
          "body": "> Thank you for your suggestion! That's indeed a great idea, and we will consider adding support for it in the future.\r\n\r\nDoes this means we can instert KG into database, but can not extract KG from database and do QA?",
          "created_at": "2025-01-08T02:06:21Z"
        },
        {
          "author": "javi-horizon",
          "body": "I have the same question, I have generated a KG and I have it in Neo4j format, I would like to use this framework to answer questions, is there a way to do this now?",
          "created_at": "2025-01-21T18:34:40Z"
        },
        {
          "author": "jhluaa",
          "body": "same question",
          "created_at": "2025-03-27T00:59:35Z"
        }
      ]
    },
    {
      "issue_number": 1199,
      "title": "[Bug]: ainsert_custom_chunks doc status",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\ninsert a doc by ainsert_custom_chunks, doc_status not updated.\ncan't delete by doc id\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.3.0\n- Operating System: windows\n- Python Version: 3.11.0\n- Related Issues:\n",
      "state": "open",
      "author": "newbie-Li",
      "author_type": "User",
      "created_at": "2025-03-27T00:34:06Z",
      "updated_at": "2025-03-27T00:34:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1199/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1199",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1199",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.681534",
      "comments": []
    },
    {
      "issue_number": 1109,
      "title": "[Bug]: <title> error in delete_by_doc_id",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n_No response_\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "tianyi323",
      "author_type": "User",
      "created_at": "2025-03-18T07:04:59Z",
      "updated_at": "2025-03-26T17:56:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1109/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1109",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1109",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.681555",
      "comments": [
        {
          "author": "tianyi323",
          "body": "When using PGKVStorage or RedisKVStorage as kv_storage and delete_by_doc_id, an error occurs while deleting document 1235567777: PGKVStorage 'object has no attribute' get-all ', this issue also occurs with RedisKVStorage",
          "created_at": "2025-03-18T07:05:51Z"
        },
        {
          "author": "husaynirfan1",
          "body": "Yes. Verified. I tested delete with `await rag.adelete_by_doc_id(docs[0])` where `docs[0]='dbbe59b183eaa'` would return `Error while deleting document dbbe59b183eaa: 'PGKVStorage' object has no attribute 'get_all'`. The issue with handling with docs id seems to be a lot. I also raised issue #1189 wh",
          "created_at": "2025-03-26T17:56:38Z"
        }
      ]
    },
    {
      "issue_number": 1196,
      "title": "[Question]: <title>About the returned context content",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIs it because the model's parameter size is insufficient（Qwen2.5-3b-Instruct） that setting `query_param.only_need_context=True` fails to return the context content?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "SavannaBlad",
      "author_type": "User",
      "created_at": "2025-03-26T14:10:32Z",
      "updated_at": "2025-03-26T15:19:11Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1196/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1196",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1196",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.931756",
      "comments": []
    },
    {
      "issue_number": 1194,
      "title": "[Bug]: <title>",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/6585d574-dd21-4eaa-9fcd-b09be15def43) 想问一下大家，这里是因为调用的大模型没有返回所以报错吗?\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "fishfish123-win",
      "author_type": "User",
      "created_at": "2025-03-26T13:05:06Z",
      "updated_at": "2025-03-26T13:05:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1194/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1194",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1194",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.931777",
      "comments": []
    },
    {
      "issue_number": 1075,
      "title": "[Question]: Try to create namespace before Shared-Data is initialized<title>",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nINPUT_FILE: book.txt\nWORKING_DIR: index_default\nINFO:     Started server process [19648]\nINFO:     Waiting for application startup.\nINFO: Process 19648 Shared-Data created for Single Process\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'index_default\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'index_default\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'index_default\\\\vdb_chunks.json'} 0 data\nERROR:    Traceback (most recent call last):\n  File \"D:\\lightrag\\LightRAG\\Lib\\site-packages\\starlette\\routing.py\", line 692, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\lightrag\\LightRAG\\LightRAG-main\\lightrag\\examples\\lightrag_api_ollama_demo.py\", line 62, in lifespan\n    rag = await init()\n          ^^^^^^^^^^^^\n  File \"D:\\lightrag\\LightRAG\\LightRAG-main\\lightrag\\examples\\lightrag_api_ollama_demo.py\", line 53, in init\n    await rag.initialize_storages()\n  File \"D:\\lightrag\\LightRAG\\LightRAG-main\\lightrag\\lightrag\\lightrag.py\", line 469, in initialize_storages\n    await asyncio.gather(*tasks)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\tasks.py\", line 385, in __wakeup\n    future.result()\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"D:\\lightrag\\LightRAG\\LightRAG-main\\lightrag\\lightrag\\kg\\json_kv_impl.py\", line 37, in initialize\n    self.storage_updated = await get_update_flag(self.namespace)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\lightrag\\LightRAG\\LightRAG-main\\lightrag\\lightrag\\kg\\shared_storage.py\", line 311, in get_update_flag\n    raise ValueError(\"Try to create namespace before Shared-Data is initialized\")\nValueError: Try to create namespace before Shared-Data is initialized\n\nERROR:    Application startup failed. Exiting.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "yulink1981",
      "author_type": "User",
      "created_at": "2025-03-13T00:41:12Z",
      "updated_at": "2025-03-26T06:17:49Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1075/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1075",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1075",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:58.931784",
      "comments": [
        {
          "author": "yulink1981",
          "body": "init namespace failure \n",
          "created_at": "2025-03-13T00:42:39Z"
        },
        {
          "author": "yulink1981",
          "body": "Thank you for your reply.",
          "created_at": "2025-03-13T00:43:52Z"
        },
        {
          "author": "danielaskdd",
          "body": "I think you are not starting the API server in a correct way. Please refer to README of API Server, install and run it with lightrag-server command.  https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md ",
          "created_at": "2025-03-14T03:57:13Z"
        },
        {
          "author": "yulink1981",
          "body": "> I think you are not starting the API server in a correct way. Please refer to README of API Server, install and run it with lightrag-server command. https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md\n\nThe problem still exists.\n\n\nINFO: Process 26864 Shared-Data created for Single Pr",
          "created_at": "2025-03-14T07:19:58Z"
        },
        {
          "author": "danielaskdd",
          "body": "The issue appears to be related to Python package dependency versions. It is recommended to create a new virtual environment and reinstall the system using pip to check for error messages. ",
          "created_at": "2025-03-22T02:45:42Z"
        }
      ]
    },
    {
      "issue_number": 1186,
      "title": "[Question]: <title> rule-based graph structure",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI have some rule-based (rather than LLM-generated) graph structures that I want to insert into a knowledge graph. Are there any ready-made interfaces available for this?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "vcvycy",
      "author_type": "User",
      "created_at": "2025-03-25T13:27:11Z",
      "updated_at": "2025-03-26T00:58:42Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1186/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1186",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1186",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:59.135115",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Currently, the system lacks an interface for users to add nodes and relations to the Graph. Our development roadmap prioritizes the implementation of node merge and delete functionalities first.",
          "created_at": "2025-03-26T00:58:41Z"
        }
      ]
    },
    {
      "issue_number": 943,
      "title": "[Question]: User isolation",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nCurrently, it appears that all users are sharing the same database? If we want to allow users to store and access data in their own databases, how should we implement this? Ths~\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "wangzhen38",
      "author_type": "User",
      "created_at": "2025-02-25T08:18:09Z",
      "updated_at": "2025-03-25T18:58:16Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/943/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/943",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/943",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:59.330593",
      "comments": [
        {
          "author": "CoachbotAI",
          "body": "Have you tried to manage this topic by workspaces?",
          "created_at": "2025-02-25T14:43:32Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "Using a different workspace per user will just separate the metadata (docs, response cache, chunks, etc.). But changing the workspace does not actually do anything on the database level I believe.\n\nI'm looking for solutions myself as well. Since I'm storing sensitive user data in the graph database,",
          "created_at": "2025-03-01T20:08:59Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "Update: I found an existing solution; the `namespace_prefix` argument. As the docstring states; \"Prefix for namespacing stored data across different environments.\". \nSo, this would allow you to put data in separate environments 🙌. Hope it helps\n\n```py\n        rag_instances[user_id] = LightRAG(\n     ",
          "created_at": "2025-03-03T22:58:55Z"
        },
        {
          "author": "husaynirfan1",
          "body": "I currently using the docs filtering, so when user query, it only retrieve filtered docs correlated to the user.",
          "created_at": "2025-03-25T18:58:15Z"
        }
      ]
    },
    {
      "issue_number": 1149,
      "title": "Production version not working anymiore after last updates",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n\nWhen I run lightrag-gunicorn, I get this:\n```\n================================================================================\nMAIN PROCESS INITIALIZATION\nProcess ID: 803610\nWorkers setting: 1\n================================================================================\n\n\nError: unsupported operand type(s) for *: 'NoneType' and 'int'\n```\n\nThe lightrag-server works but not the gunicorn version.\nI use .env to store my configurations in my launch folder as always.\n@danielaskdd do you know this issue.\n\nSorry for being abscent lately.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-03-21T13:39:40Z",
      "updated_at": "2025-03-25T15:02:16Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1149/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1149",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1149",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:59.532074",
      "comments": [
        {
          "author": "bzImage",
          "body": "no answer when a question is made to the rag system.\n\ni can see the post to embeddings to \"map\" the query vectors\n\nDEBUG:HTTP Request: POST https://api.openai.com/v1/embeddings \"200 OK\"\nDEBUG:close.started\nDEBUG:close.started\nDEBUG:close.started\nDEBUG:close.started\nDEBUG:close.complete\nDEBUG:close.c",
          "created_at": "2025-03-21T16:50:27Z"
        },
        {
          "author": "danielaskdd",
          "body": "LightRAG is using preload mode of Gunicorn which Windows is not supported.",
          "created_at": "2025-03-22T02:08:40Z"
        },
        {
          "author": "ParisNeo",
          "body": "I'm not using windows. I'm on linux ubuntu:\nAnd this error doesn't seem to be linked to the system:\n\nError: unsupported operand type(s) for *: 'NoneType' and 'int'\n",
          "created_at": "2025-03-25T10:21:06Z"
        },
        {
          "author": "ParisNeo",
          "body": "maybe we need a debug mode to log more infos",
          "created_at": "2025-03-25T10:21:27Z"
        },
        {
          "author": "bzImage",
          "body": "Just find out that.. if you are using postgres backend.. something its wrong.. and .. it takes up to 20-30 seconds to timeout from the \"query\" and fail.. meanwhile your cpu goes to 100% ..\n\n",
          "created_at": "2025-03-25T15:02:15Z"
        }
      ]
    },
    {
      "issue_number": 680,
      "title": "instantiate azure openai",
      "body": "Hi everyone,\n\nquick question - in case I would like to use AzureOpenAI, do I need to do anything else beside:\n\n\n```python\nfrom lightrag.llm.azure_openai import azure_openai_complete, azure_openai_embed\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=azure_openai_complete,\n    embedding_func=azure_openai_embed,\n)\n```\n\nbecause at the moment, I always get an error that the entity or relationship extraction does not work and I have to check my llm. The LLM is working. If I just call this method and apply a prompt, it will return something.\n\nIf I check the assigned functions for LightRAG after running the above code, it keeps the embedding as well as the llm func emtpy.\n\nHow to do it properly?",
      "state": "open",
      "author": "ChristianWunderlich",
      "author_type": "User",
      "created_at": "2025-01-30T14:38:14Z",
      "updated_at": "2025-03-25T14:53:19Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/680/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/680",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/680",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:34:59.828201",
      "comments": [
        {
          "author": "stevenveenma",
          "body": "Same question from me. Without Azure openAI support LightRAG is useless for me from a professional perspective",
          "created_at": "2025-01-30T16:00:46Z"
        },
        {
          "author": "ChristianWunderlich",
          "body": "@LarFii would love if we can get here a quick statement! Also note that the azure example is the only one which got not updated but the code did. So please make sure this is aligned!",
          "created_at": "2025-01-31T09:29:21Z"
        },
        {
          "author": "LarFii",
          "body": "> [@LarFii](https://github.com/LarFii) would love if we can get here a quick statement! Also note that the azure example is the only one which got not updated but the code did. So please make sure this is aligned!\n\nThe AzureOpenAI-related code is community-contributed, which is why it may not have b",
          "created_at": "2025-01-31T15:39:15Z"
        },
        {
          "author": "TaulantAsllani",
          "body": "Hi @LarFii, Could we get an update on this? We are also trying to use LightRAG from a professional perspective but without this it would be impossible",
          "created_at": "2025-02-17T14:54:09Z"
        },
        {
          "author": "poojatambe",
          "body": "Any solution? getting error for azure openai.\n\n![Image](https://github.com/user-attachments/assets/99adf034-2ee5-4ed4-b5dd-18ddb07b91d8)",
          "created_at": "2025-02-18T08:53:34Z"
        }
      ]
    },
    {
      "issue_number": 1179,
      "title": "[Bug]: neo4j backend: Error getting edge/node degree for xxxx: failed to obtain a connection from the pool within 30.0s (timeout)",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n\nusing the milvus- neo4j - redis (valkey) example... after processing 100 files and doing a query:\n\n\n\nError in get_edge between Red Mongoose Daemon and correos electrónicos maliciosos: failed to obtain a connect[2164/2164]\ne pool within 30.0s (timeout)\nError in get_edge between Red Mongoose Daemon and troyano bancario: failed to obtain a connection from the pool within 3\n0.0s (timeout)\nError in get_edge between Red Mongoose Daemon and hxxps[:]//i.pinimg.com/236x/5b/f8/1a/5bf81a501ab9d26db806e7fec4edfa75.jpg: failed to obtain a connection from the pool within 30.0s (timeout)                                                 Error in get_edge between Red Mongoose Daemon and cuarta y quinta semana de mayo: failed to obtain a connection from the pool within 30.0s (timeout)                                                                                            Error in get_edge between Red Mongoose Daemon and hxxps[:]//rdcontra.com/clientes/index.php: failed to obtain a connection from the pool within 30.0s (timeout)\nError in get_edge between Red Mongoose Daemon and k7: failed to obtain a connection from the pool within 30.0s (timeout)Error in get_edge between Red Mongoose Daemon and T1566.001: failed to obtain a connection from the pool within 30.0s (timeout)                                                                                                                 Error in get_edge between FC704F988372D9CDED8C1B12BB167579 and notif.detallada.deud_online.nu.msi_06280.zip: failed to obtain a connection from the pool within 30.0s (timeout)                                                                 Error in get_edge between TimbreStealer and cumplimiento19[.]altavista100[.]com: failed to obtain a connection from the pool within 30.0s (timeout)\nError in get_edge between TimbreStealer and manderlyx[.]com: failed to obtain a connection from the pool within 30.0s (timeout)                                             \n\n...\n\n\n\nError getting node degree for 2FC53109F3CCC0CDB34703069E47FBE95028D6E54D8EC0353D4D5FDFA9CE5E44: failed to obtain a connection from the pool within 30.0s (timeout)                                                                              Error getting node degree for B174D3DB1C240CA66A1F0855DDA391C218F94150F4F4D9BC1F9A47E7B0DD9C4E: failed to obtain a connection from the pool within 30.0s (timeout)                                                                              Error getting node degree for 5RF.rar: failed to obtain a connection from the pool within 30.0s (timeout)               Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)          Error getting node degree for Botnet Fenix: failed to obtain a connection from the pool within 30.0s (timeout)\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "bzImage",
      "author_type": "User",
      "created_at": "2025-03-24T22:51:28Z",
      "updated_at": "2025-03-24T22:53:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1179/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1179",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1179",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:00.416103",
      "comments": []
    },
    {
      "issue_number": 1178,
      "title": "[Feature Request]: graph refactoring",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nIt would be great to be able to refactor the graph.  for example, on the sample data, I got 4 nodes that refer to \"Scrooge\" in some way:\n\n<img width=\"534\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/48e923ca-0f52-4e11-ad4c-d994397fe08a\" />\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "jvsteiner",
      "author_type": "User",
      "created_at": "2025-03-24T22:35:52Z",
      "updated_at": "2025-03-24T22:35:52Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1178/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1178",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1178",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:00.416125",
      "comments": []
    },
    {
      "issue_number": 1176,
      "title": "[Bug]: Postgres Backend - Error executing graph query - Entity failed to be updated.",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n\nsql:SELECT * FROM cypher('chunk_entity_relation', $$                                                                                                    MERGE (n:Entity {node_id: \"x43686165243438\"})                                                                                      SET n += {`entity_id`: \"Chae$48\", `source_id`: \"chunk-0ef8d761ca2d520d4c80f27a02abfe4f\", `description`: \"El malware utiliza Pycryptodome para implementar algoritmos de cifrado necesarios en su funcionamiento.\", `entity_type`: \"UNKNOWN\", `file_path`: \"EW2405-197.txt\"}                                                                                                                              RETURN n                                                                                                                         $$) AS (n agtype),                                                                                              data:None,                                                                                                                         error:Entity failed to be updated: 3                                                                                               POSTGRES, Error during upsert: {{'message': 'Error executing graph query: SELECT * FROM cypher(\\'chunk_entity_relation\\', $$\\n                     MERGE (n:Entity {node_id: \"x43686165243438\"})\\n                     SET n += {`entity_id`: \"Chae$48\", `source_id`: \"chunk-0ef8d761ca2d520d4c80f27a02abfe4f\", `description`: \"El malware utiliza Pycryptodome para implementar algoritmos de cifrado necesarios en su funcionamiento.\", `entity_type`: \"UNKNOWN\", `file_path`: \"EW2405-197.txt\"}\\n                     RETURN n\\n                   $$) AS (n agtype)', 'wrapped': 'SELECT * FROM cypher(\\'chunk_entity_relation\\', $$\\n                     MERGE (n:Entity {node_id: \"x43686165243438\"})\\n                     SET n += {`entity_id`: \"Chae$48\", `source_id`: \"chunk-0ef8d761ca2d520d4c80f27a02abfe4f\", `description`: \"El malware utiliza Pycryptodome para implementar algoritmos de cifrado necesarios en su funcionamiento.\", `entity_type`: \"UNKNOWN\", `file_path`: \"EW2405-197.txt\"}\\n                     RETURN n\\n                   $$) AS (n agtype)', 'detail': 'Entity failed to be updated: 3'}}    \n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "bzImage",
      "author_type": "User",
      "created_at": "2025-03-24T19:02:18Z",
      "updated_at": "2025-03-24T19:29:43Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1176/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1176",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1176",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:00.416132",
      "comments": [
        {
          "author": "bzImage",
          "body": "Also reports high response times on postgres queries after this error.. the db works but takes a lot to answer (170 seconds - 200 seconds)\n\n",
          "created_at": "2025-03-24T19:10:50Z"
        },
        {
          "author": "bzImage",
          "body": "also reporting high cpu usage\n\ntop snip..\n\n 857162 systemd-r  20   0  343M  106M 95184 R 99.7  0.3  0:53.74 postgres: 16/main: rag rag 172.17.0.1(49784) authentication\n 857163 systemd-r  20   0  343M 87924 74360 R 99.7  0.3  0:53.71 postgres: 16/main: rag rag 172.17.0.1(49812) idle\n 857165 systemd-r",
          "created_at": "2025-03-24T19:29:42Z"
        }
      ]
    },
    {
      "issue_number": 1172,
      "title": "[Bug]: AttributeError: 'Node' object has no attribute 'properties'. Did you mean: '_properties'?",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nwhen i try to run ` rag.get_knowledge_graph` i get the below error on neo4j\n\n```\nFile \"/Users/yared/Documents/Programing/personal/agent/.venv/lib/python3.12/site-packages/lightrag/kg/neo4j_impl.py\", line 899, in _robust_fallback\n    await traverse(start_node, None, 0)\n  File \"/Users/yared/Documents/Programing/personal/agent/.venv/lib/python3.12/site-packages/lightrag/kg/neo4j_impl.py\", line 853, in traverse\n    properties=dict(b_node.properties),\n                    ^^^^^^^^^^^^^^^^^\nAttributeError: 'Node' object has no attribute 'properties'. Did you mean: '_properties'?\n\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "yaredtsy",
      "author_type": "User",
      "created_at": "2025-03-24T10:50:30Z",
      "updated_at": "2025-03-24T10:50:30Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1172/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1172",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1172",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:00.604339",
      "comments": []
    },
    {
      "issue_number": 1157,
      "title": "[Question]: Can't connect to webui from a remote machine",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI have started the webui and specify the host as 0.0.0.0 to bind to all interfaces,  I can see that port 5173 is bound to 0.0.0.0 via netstat. I can connect from the same machine running the webui using both localhost and its ip with no problem but not from a remote server on the same network.  Any ideas of what I am doing wrong?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "tonyye",
      "author_type": "User",
      "created_at": "2025-03-22T04:40:24Z",
      "updated_at": "2025-03-24T02:11:01Z",
      "closed_at": "2025-03-24T02:11:00Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1157/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1157",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1157",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:00.604357",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Check firewall setting of your machine, it's likely remote access is blocked by you firewall..",
          "created_at": "2025-03-22T13:11:38Z"
        },
        {
          "author": "tonyye",
          "body": "I don't have any firewalls installed.  I can connect to other services such as open webui, ssh, etc.",
          "created_at": "2025-03-22T14:00:35Z"
        },
        {
          "author": "danielaskdd",
          "body": "5173 is default port of vite dev mode. If you a debug webui using vite proxy, vite proxy is only listening to localhost by default.",
          "created_at": "2025-03-22T14:18:23Z"
        },
        {
          "author": "tonyye",
          "body": "I am start the webui with this command \"bun dev --host 0.0.0.0\". Is there any way to remove the proxy? I tried removing the proxy configuration from vite.config.ts but it is still not allowing remove connections.",
          "created_at": "2025-03-22T17:44:49Z"
        },
        {
          "author": "danielaskdd",
          "body": "After LightRAG API Server started, the webui becomes immediately accessible without the necessity of executing the bun dev command.",
          "created_at": "2025-03-22T18:55:52Z"
        }
      ]
    },
    {
      "issue_number": 1155,
      "title": "[Bug]: <title> can not set the works",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nwhen i set the \"WORKERS=10\" in the .env file, but it do not work\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "lookhua",
      "author_type": "User",
      "created_at": "2025-03-22T02:29:31Z",
      "updated_at": "2025-03-23T16:42:50Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1155/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1155",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1155",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:00.775108",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The WORKERS parameter takes effect only when the LightRAG Server is started using lightrag-gunicorn.",
          "created_at": "2025-03-23T16:42:49Z"
        }
      ]
    },
    {
      "issue_number": 1165,
      "title": "视频->将LightRAG做成MCP Server在Claude和AutoGen中调用",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nhttps://youtu.be/KGZ_zM6Xi-U\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "win4r",
      "author_type": "User",
      "created_at": "2025-03-23T13:26:11Z",
      "updated_at": "2025-03-23T16:23:40Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1165/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1165",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1165",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:00.935481",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG Server can act as Ollama Model, so  any agent supporting Ollama can connect to LightRAG. Pls refer to: https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md\n",
          "created_at": "2025-03-23T16:23:39Z"
        }
      ]
    },
    {
      "issue_number": 1164,
      "title": "Running lightrag reported an error: input is too large to process, please increase the physical batch size， How to modify?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nimport sys; print('Python %s on %s' % (sys.version, sys.platform))\nD:\\work\\code\\python\\LightRAG\\venv\\Scripts\\python.exe -X pycache_prefix=C:\\Users\\twinh\\AppData\\Local\\JetBrains\\PyCharm2024.1\\cpython-cache D:/tools/python/pycharm/pycharm-professional-2024.1.win/plugins/python/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55625 --file D:\\work\\code\\python\\LightRAG\\examples\\lightrag_openai_compatible_demo.py \nConnected to pydev debugger (build 241.14494.241)\nDetected embedding dimension: 1024\nINFO: Process 38140 Shared-Data created for Single Process\nINFO:nano-vectordb:Load (0, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Load (0, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Load (0, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data\nINFO: Process 38140 initialized updated flags for namespace: [full_docs]\nINFO: Process 38140 ready to initialize storage namespace: [full_docs]\nINFO: Process 38140 initialized updated flags for namespace: [text_chunks]\nINFO: Process 38140 ready to initialize storage namespace: [text_chunks]\nINFO: Process 38140 initialized updated flags for namespace: [entities]\nINFO: Process 38140 initialized updated flags for namespace: [relationships]\nINFO: Process 38140 initialized updated flags for namespace: [chunks]\nINFO: Process 38140 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 38140 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 38140 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 38140 initialized updated flags for namespace: [doc_status]\nINFO: Process 38140 ready to initialize storage namespace: [doc_status]\nINFO: Process 38140 storage namespace already initialized: [full_docs]\nINFO: Process 38140 storage namespace already initialized: [text_chunks]\nINFO: Process 38140 storage namespace already initialized: [llm_response_cache]\nINFO: Process 38140 storage namespace already initialized: [doc_status]\nINFO: Process 38140 Pipeline namespace initialized\nINFO:openai._base_client:Retrying request to /embeddings in 0.495400 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.396173 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.470077 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.403318 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.791662 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.923904 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.971611 seconds\nINFO:openai._base_client:Retrying request to /embeddings in 0.846225 seconds\nFailed to process document doc-b24fd78846163c7b6260ae3efd9553b3: Error code: 500 - {'error': {'code': 500, 'message': 'input is too large to process, please increase the physical batch size', 'type': 'server_error'}, 'detail': 'input is too large to process, please increase the physical batch size'}\nSorry, I'm not able to provide an answer to that question.[no-context]\nSorry, I'm not able to provide an answer to that question.[no-context]\nSorry, I'm not able to provide an answer to that question.[no-context]\nSorry, I'm not able to provide an answer to that question.[no-context]\n\n\n### Additional Context\n\nFailed to process document doc-b24fd78846163c7b6260ae3efd9553b3: Error code: 500 - {'error': {'code': 500, 'message': 'input is too large to process, please increase the physical batch size', 'type': 'server_error'}, 'detail': ",
      "state": "open",
      "author": "Twinhead880",
      "author_type": "User",
      "created_at": "2025-03-23T09:26:28Z",
      "updated_at": "2025-03-23T16:20:18Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1164/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1164",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1164",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:01.128313",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Try to reduce EMBEDDING_BATCH_NUM, which default is 32, try 16",
          "created_at": "2025-03-23T16:20:17Z"
        }
      ]
    },
    {
      "issue_number": 895,
      "title": "[Bug]: <title>lightrag-webui Backend Health Check Error!",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/dc47f232-cec9-4632-a6d5-0de6685cf15b)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "watch-Ultra",
      "author_type": "User",
      "created_at": "2025-02-20T03:19:29Z",
      "updated_at": "2025-03-22T19:33:48Z",
      "closed_at": "2025-03-22T19:33:48Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/895/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/895",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/895",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:01.297370",
      "comments": []
    },
    {
      "issue_number": 948,
      "title": "[Question]: <title>中文的Entity Types Neo4j里的Entity Type都带着双引号",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n定义中文Entity Types Neo4j里的Entity Type都带着双引号。\n\naddon_params = {\n        \"entity_types\": [\"原料或成分或组成成分或化学成分或菌株或品种\",\"产地与地理标志\",\"器械或设备或装置\",\"技术或方法或种植技术或加工方法\",\"症状\",\"疾病\",\"作用或功效或价值或优势或药理作用或改进\",\"指标或参数或评估指数或性\n能指标\",\"应用范围或应用领域或临床应用\",\"提取物或营养品或化妆品或药品或药物或制剂或药方或药或药剂或处方\",\"产品或物品或品牌或制品\",\"发明或专利\",\"质量标准\",\"市场与消费者反馈\",\"政策支持与补贴\",\"新闻与事件\",\"健康风险与副作用\n\",\"建议与小帮助或经验\",\"研究机构或学者\",\"公司或企业或政府部门或机构\"],\n        \"language\":\"Simplified Chinese\",\n}\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=embedding_dim,\n        max_token_size=8192,\n        func=embedding_func\n    ),\n    addon_params = addon_params,\n    graph_storage=\"Neo4JStorage\", #<-----------override KG default\n    log_level=\"DEBUG\"  #<-----------override log_level default\n)\n\n之后查看neo4j，里边的entity，entity_type字段为  \"研究机构或学者\"  带双引号。\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "shawn-maxiao",
      "author_type": "User",
      "created_at": "2025-02-25T14:56:27Z",
      "updated_at": "2025-03-22T19:32:23Z",
      "closed_at": "2025-03-22T19:32:23Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/948/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/948",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/948",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:01.297390",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "这个问题在最新的main分支已经解决。",
          "created_at": "2025-03-02T18:01:36Z"
        }
      ]
    },
    {
      "issue_number": 947,
      "title": "[Question]: <title> LightRAG生成到Neo4j的Entity和Relastionship，Label都是直接是Entity/Relationship名字，而不是Type吗",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nLightRAG生成到Neo4j的Entity和Relationship的Label好像是直接用的Entity/Relationship的名字，而不是Entity/Relationship的Type？\n是不是应该使用Entity /Relationship Type作为Label，名字只是作为属性，否则一方面显示的特别乱，另一方面Neo4j对Label是有数量限制的，不能超过65535个。\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "shawn-maxiao",
      "author_type": "User",
      "created_at": "2025-02-25T14:29:47Z",
      "updated_at": "2025-03-22T19:31:49Z",
      "closed_at": "2025-03-22T19:31:48Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/947/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/947",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/947",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:01.462944",
      "comments": [
        {
          "author": "shawn-maxiao",
          "body": "另外微信群不能加了",
          "created_at": "2025-02-25T14:40:05Z"
        },
        {
          "author": "YDS854394028",
          "body": "它这个就是有问题，可视化出来很混乱",
          "created_at": "2025-03-02T07:34:48Z"
        },
        {
          "author": "danielaskdd",
          "body": "Neo4j的问题在主分支已经修复",
          "created_at": "2025-03-22T19:31:48Z"
        }
      ]
    },
    {
      "issue_number": 961,
      "title": "[Question]:  Why did we move from docling to pypdf2?!",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHi there!\n@danielaskdd @ArnoChenFx @LarFii @YanSte \nToday I was playing around with pdf importing and one of my PDFs was simply read as empty!\ndocling is way more capable than pypdf2, built by IBM, it is the state of the art text extraction library. it can even do OCR and so read scanned pdfs.\n\nWhy did we retrograde from docling which is built for AI RAG to pypdf2 which is way more primitive\n\nBest regards\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-02-27T17:04:40Z",
      "updated_at": "2025-03-22T19:30:14Z",
      "closed_at": "2025-03-22T19:30:14Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/961/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/961",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/961",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:01.633896",
      "comments": [
        {
          "author": "ArnoChenFx",
          "body": "docling is too heavy. \nIt will automatically download AI models and consume a significant amount of computer resources.",
          "created_at": "2025-02-27T17:27:40Z"
        },
        {
          "author": "ParisNeo",
          "body": "OK then, can we at least make it an option?\nIt is done just once then the user can use it. I think it is still woth using. But if you don't like the fact that it is heavy, we cna give an option to use it or not in environment variables.",
          "created_at": "2025-02-27T17:41:25Z"
        },
        {
          "author": "ParisNeo",
          "body": "And by the way. The connections between nodes is never displayed. Can you please add something like this:\n\nNow, when I select a node, I see its parameters on the right panel. When I hover the description i can see the description which I would like to convert to a list of blocks (use the <SEP> to se",
          "created_at": "2025-02-27T18:00:25Z"
        },
        {
          "author": "ArnoChenFx",
          "body": "> And by the way. The connections between nodes is never displayed. Can you please add something like this:\n> \n> Now, when I select a node, I see its parameters on the right panel. When I hover the description i can see the description which I would like to convert to a list of blocks (use the to se",
          "created_at": "2025-02-27T18:30:43Z"
        },
        {
          "author": "ParisNeo",
          "body": "I found that if I activate edge events, I can select the edge and view its properties but it is really hard to select edges. How about making them a little thicker when hovered?\n\nBy the way. Do you want to do this or do you want me to do it? I have recovered the UI and started taking a look at the c",
          "created_at": "2025-02-27T19:33:46Z"
        }
      ]
    },
    {
      "issue_number": 1092,
      "title": "[Bug]: <title>切换不了中文菜单",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n将import i18n from \"i18next\";\nimport { initReactI18next } from \"react-i18next\";\n\nimport en from \"./locales/en.json\";\nimport zh from \"./locales/zh.json\";\n\ni18n\n  .use(initReactI18next)\n  .init({\n    resources: {\n      en: { translation: en },\n      zh: { translation: zh }\n    },\n    lng: \"zh\", // 默认使用中文\n    fallbackLng: \"zh\",\n    interpolation: {\n      escapeValue: false\n    }\n  });\n\nexport default i18n;\n\n\n改成zh了，甚至删除en\n一直都是英文界面，求指点\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "hhtao",
      "author_type": "User",
      "created_at": "2025-03-15T00:53:14Z",
      "updated_at": "2025-03-22T19:29:04Z",
      "closed_at": "2025-03-22T19:29:03Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1092/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1092",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1092",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:01.823150",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "New webui is coming soon.  It'll address your issues. you can try it by pulling this branch: webui-node-expansion",
          "created_at": "2025-03-18T08:41:34Z"
        },
        {
          "author": "danielaskdd",
          "body": "This issue is fixed on main branch.",
          "created_at": "2025-03-22T19:29:03Z"
        }
      ]
    },
    {
      "issue_number": 980,
      "title": "AttributeError: 'function' object has no attribute 'embedding_dim'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhile running the rag.insert cell, I am facing the error as - AttributeError: 'function' object has no attribute 'embedding_dim'\n\nPlease help. \n\n\nimport openai\nimport nest_asyncio\nimport tiktoken  # Import tokenizer\n\n# 🔹 Set your Azure OpenAI details\nAZURE_OPENAI_ENDPOINT = \"end point\"  \nAZURE_OPENAI_KEY = \"key\nAZURE_OPENAI_API_VERSION = \"2024-02-01\"  \nAZURE_OPENAI_EMBEDDING_DEPLOYMENT = \"text-embedding-ada-002\"  \nAZURE_OPENAI_LLM_DEPLOYMENT = \"gpt-4o\"  \n\n# ✅ Explicitly define the tokenizer for manual use\nTOKENIZER_NAME = \"cl100k_base\"  # GPT-4o & GPT-4 use \"cl100k_base\"\ntokenizer = tiktoken.get_encoding(TOKENIZER_NAME)\n\n# Configure OpenAI client for Azure\nclient = openai.AzureOpenAI(\n    api_key=AZURE_OPENAI_KEY,\n    api_version=AZURE_OPENAI_API_VERSION,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT\n)\n\n# ✅ Function to tokenize and then embed using Azure OpenAI\ndef azure_openai_embed(texts):\n    tokenized_texts = [\" \".join(tokenizer.encode(text, disallowed_special=())) for text in texts]  # Manual tokenization\n    response = client.embeddings.create(\n        model=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,  \n        input=tokenized_texts  # Use tokenized text\n    )\n    return [data.embedding for data in response.data]\n\n# ✅ Function to generate text completion using Azure OpenAI GPT-4o\ndef azure_gpt_4o_complete(prompt):\n    response = client.chat.completions.create(\n        model=AZURE_OPENAI_LLM_DEPLOYMENT,  \n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                  {\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n# ✅ Initialize LightRAG with Azure OpenAI (without tokenizer argument)\nrag = LightRAG(\n    working_dir=\"/Workspace/Users/puneeth.raviprakash@accenture.com/Han's POCs/RAG_Notebooks\",\n    embedding_func=azure_openai_embed,\n    llm_model_func=azure_gpt_4o_complete\n)\n\n# Fix event loop issues in Jupyter\nnest_asyncio.apply()\n\n# Insert document into RAG\nawait rag.insert(document_text)\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\nWhile running the rag.insert cell, I am facing the error as - AttributeError: 'function' object has no attribute 'embedding_dim'\n\nPlease help. \n\n%pip install lightrag-hku python-dotenv pipmaster \nimport os\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed\nimport openai\nimport nest_asyncio\nimport tiktoken  # Import tokenizer\n\n# 🔹 Set your Azure OpenAI details\nAZURE_OPENAI_ENDPOINT = \"end point\"  \nAZURE_OPENAI_KEY = \"key\nAZURE_OPENAI_API_VERSION = \"2024-02-01\"  \nAZURE_OPENAI_EMBEDDING_DEPLOYMENT = \"text-embedding-ada-002\"  \nAZURE_OPENAI_LLM_DEPLOYMENT = \"gpt-4o\"  \n\n# ✅ Explicitly define the tokenizer for manual use\nTOKENIZER_NAME = \"cl100k_base\"  # GPT-4o & GPT-4 use \"cl100k_base\"\ntokenizer = tiktoken.get_encoding(TOKENIZER_NAME)\n\n# Configure OpenAI client for Azure\nclient = openai.AzureOpenAI(\n    api_key=AZURE_OPENAI_KEY,\n    api_version=AZURE_OPENAI_API_VERSION,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT\n)\n\n# ✅ Function to tokenize and then embed using Azure OpenAI\ndef azure_openai_embed(texts):\n    tokenized_texts = [\" \".join(tokenizer.encode(text, disallowed_special=())) for text in texts]  # Manual tokenization\n    response = client.embeddings.create(\n        model=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,  \n        input=tokenized_texts  # Use tokenized text\n    )\n    return [data.embedding for data in response.data]\n\n# ✅ Function to generate text completion using Azure OpenAI GPT-4o\ndef azure_gpt_4o_complete(prompt):\n    response = client.chat.completions.create(\n        model=AZURE_OPENAI_LLM_DEPLOYMENT,  \n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                  {\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n# ✅ Initialize LightRAG with Azure OpenAI (without tokenizer argument)\nrag = LightRAG(\n    working_dir=\"/Workspace/Users/puneeth.raviprakash@accenture.com/Han's POCs/RAG_Notebooks\",\n    embedding_func=azure_openai_embed,\n    llm_model_func=azure_gpt_4o_complete\n)\n\n# Fix event loop issues in Jupyter\nnest_asyncio.apply()\n\n# Insert document into RAG\nawait rag.insert(document_text)\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.2.3\n- Operating System:  Windows\n- Python Version: 3.11\n- Related Issues:\n",
      "state": "open",
      "author": "PuneethPo",
      "author_type": "User",
      "created_at": "2025-03-03T15:01:05Z",
      "updated_at": "2025-03-22T13:22:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/980/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/980",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/980",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:02.077303",
      "comments": []
    },
    {
      "issue_number": 1147,
      "title": "[Bug]: RuntimeWarning: coroutine 'LightRAG.finalize_storages' was never awaited",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI loaded a DB I generated with \"LightRAG\" (on disk), ran queries with it according to the README doc, but then at the end of the script I get a warning saying \"RuntimeWarning: coroutine 'LightRAG.finalize_storages' was never awaited\".\n\nApart from that it worked and I got the right results. I am just wondering how to remove that warning and ensure everything runs properly.\n\n### Steps to reproduce\n\n```\nimport asyncio\nfrom lightrag import QueryParam\n\nasync def initialize_rag():\n\n    rag = LightRAG(\n        working_dir=\"data/lightrag\",\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=embedding_func\n        ), \n        llm_model_func=llm_model_func,\n        llm_model_max_token_size=8000\n    )\n\n    await rag.initialize_storages()\n\n    return rag  \n\n\nif __name__ == \"__main__\":\n\n    rag = asyncio.run(initialize_rag())\n\n    output = rag.query(\n        \"What is SQuAD?\",\n        param=QueryParam(mode=\"mix\")\n    )\n    print(output)\n\n    output = rag.query(\n        \"What is the default CPU configuration for a created endpoint?\",\n        param=QueryParam(mode=\"mix\")\n    )\n    print(output)\n```\n\n### Expected Behavior\n\nNo warning\n\n### LightRAG Config Used\n\n# Paste your config here\nnothing special, no env variables overwritten\n\n### Logs and screenshots\n\n```\nException ignored in: <function LightRAG.__del__ at 0x7fa6db7da4d0>\nRuntimeWarning: coroutine 'LightRAG.finalize_storages' was never awaited\n```\n\n### Additional Information\n\n- LightRAG Version:1.2.3\n- Operating System:linux\n- Python Version:Python 3.10.14\n- Related Issues: https://github.com/HKUDS/LightRAG/issues/903 \n",
      "state": "closed",
      "author": "GTimothee",
      "author_type": "User",
      "created_at": "2025-03-21T09:36:58Z",
      "updated_at": "2025-03-21T16:13:21Z",
      "closed_at": "2025-03-21T16:13:20Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1147/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1147",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1147",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:02.077323",
      "comments": [
        {
          "author": "GTimothee",
          "body": "Found the issue, it was related to my environment.",
          "created_at": "2025-03-21T16:13:20Z"
        }
      ]
    },
    {
      "issue_number": 1146,
      "title": "[Bug]: <title>TypeError: unhashable type: 'list'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I use the insert with doc_id:\n```python\nself.rag.insert(input=[contents], ids=[doc_id])\n```\n\nThen get the error:\n```\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/lightrag.py\", line 565, in insert\n    loop.run_until_complete(\n  File \"/opt/miniconda3/envs/matetext/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/lightrag.py\", line 590, in ainsert\n    await self.apipeline_enqueue_documents(input, ids, file_paths)\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/lightrag.py\", line 743, in apipeline_enqueue_documents\n    if content not in unique_contents:\nTypeError: unhashable type: 'list'\n```\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-03-21T08:36:18Z",
      "updated_at": "2025-03-21T08:36:18Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1146/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1146",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1146",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:02.242419",
      "comments": []
    },
    {
      "issue_number": 1135,
      "title": "[Bug]: <title>error in _merge_nodes_then_upsert",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n```python\nalready_node = await knowledge_graph_inst.get_node(entity_name)\n    if already_node is not None:\n        already_entity_types.append(already_node[\"entity_type\"])\n        already_source_ids.extend(\n            split_string_by_multi_markers(already_node[\"source_id\"], [GRAPH_FIELD_SEP])\n        )\n        already_file_paths.extend(\n            split_string_by_multi_markers(\n                already_node[\"metadata\"][\"file_path\"], [GRAPH_FIELD_SEP]\n            )\n        )\n        already_description.append(already_node[\"description\"])\n```\nthis code will generate an error below\n```\n  File \"/workspace/LightRAG/lightrag/operate.py\", line 235, in _merge_nodes_then_upsert\n    already_node[\"metadata\"][\"file_path\"], [GRAPH_FIELD_SEP]\nKeyError: 'metadata'\n```\nIn fact, there is no \"metadata\" in the already_node\n\n```python\nalready_file_paths.extend(\n            split_string_by_multi_markers(\n                already_node[\"file_path\"], [GRAPH_FIELD_SEP]\n            )\n        )\n```\nif remove `[\"metadata\"]`, this code can run scucessfully\n\nfunction`_merge_edges_then_upsert` has the same bug\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "etoileYue",
      "author_type": "User",
      "created_at": "2025-03-20T06:43:37Z",
      "updated_at": "2025-03-21T03:37:14Z",
      "closed_at": "2025-03-21T03:37:14Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1135/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1135",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1135",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:02.242439",
      "comments": [
        {
          "author": "etoileYue",
          "body": "fixed in 486a9e8a529c9e92ab5daf50de1bf0ad0faa16db",
          "created_at": "2025-03-21T03:37:05Z"
        }
      ]
    },
    {
      "issue_number": 1082,
      "title": "[Bug]: Having trouble just to make it work",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI am trying to use lightrag with my llm deployment (openai api compatible) and it fails at multiple points:\n\nWhile ingesting data I get this:\n\n`Processing documents:  25%|████████▎                        | 2/8 [00:01<00:04,  1.35it/s]INFO:lightrag:Inserting 1 to doc_status\nINFO:lightrag:Stored 1 new unique documents\nINFO:lightrag:Number of batches to process: 1.\nINFO:lightrag:Start processing batch 1 of 1.\nINFO:lightrag:Inserting 1 to doc_status\nINFO:lightrag:Inserting 1 to chunks\nINFO:lightrag:Inserting 1 to full_docs\nINFO:lightrag:Inserting 1 to text_chunks\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nERROR:lightrag:Failed to process document doc-40723ec49f1dad04b4823be95d04b22c: index 0 is out of bounds for axis 0 with size 0`\n\nor this:\n\n`INFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nERROR:lightrag:Failed to process document doc-984c029a349b75d6a5d1293e65c59695: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4096 and the array at index 1 has size 1024`\n\nAnd when I try to use the rag of course it does not work and gives me error like : \n\n`INFO:lightrag:Inserting 1 to llm_response_cache\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Inserting 1 to llm_response_cache\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Inserting 1 to llm_response_cache\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Inserting 1 to llm_response_cache\nINFO:lightrag:Query nodes: SQuAD, Stanford Question Answering Dataset, Natural Language Processing, Machine learning, top_k: 60, cosine: 0.2\nINFO:lightrag:Query edges: Question answering, Language models, Artificial intelligence, top_k: 60, cosine: 0.2\nERROR:lightrag:Error in get_kg_context: shapes (0,4096) and (1024,) not aligned: 4096 (dim 1) != 1024 (dim 0)\nSorry, I'm not able to provide an answer to that question.[no-context]`\n\nThe errors are really not explicit, we don't even know what component fails. I cannot activate logging because your README example is not up to date with the PYPI package and therefore these imports fail (module not found errors):\n\n`from lightrag.kg.shared_storage import initialize_pipeline_status\n from lightrag.utils import setup_logger`\n\n### Steps to reproduce\n\nHere is the file I am using: https://gist.github.com/GTimothee/32027026e8aef7dc5cb290b9b913953b \n\n### Expected Behavior\n\nJust work without error\n\n### LightRAG Config Used\n\nDefault\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: lightrag-hku==1.2.3\n- Operating System: linux\n- Python Version: 3.10.14\n- Related Issues:\n",
      "state": "closed",
      "author": "GTimothee",
      "author_type": "User",
      "created_at": "2025-03-13T10:35:29Z",
      "updated_at": "2025-03-20T22:31:21Z",
      "closed_at": "2025-03-20T22:30:29Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1082/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1082",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1082",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.196853",
      "comments": [
        {
          "author": "ekinsenler",
          "body": "It looks like you didn't set your `EMBEDDING_DIM`  to match your embedding model's dim 4096 in your case.",
          "created_at": "2025-03-13T12:33:59Z"
        },
        {
          "author": "GTimothee",
          "body": "thanks for your help. Where should I put the EMBEDDING_DIM then ? I am passing the embedding model to the same rag object that embeds the data and that processes the query so I would expect that it does both with the model I pass ? What I am missing ? ",
          "created_at": "2025-03-13T13:42:22Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "The error looks unfamiliar, but after you passed the EMBEDDING_DIM, you might need to drop your current vector db's. Since it might be mismatching the dimensions with what you already have stored.",
          "created_at": "2025-03-13T16:53:45Z"
        },
        {
          "author": "ekinsenler",
          "body": "You have to set that inside the .env file",
          "created_at": "2025-03-13T18:35:12Z"
        },
        {
          "author": "bastianwegge",
          "body": "@GTimothee for me this response helped: https://github.com/HKUDS/LightRAG/issues/727#issuecomment-2646628492\nEssentially, you want to select an embedding-model and the according dimension it comes with, if I understood this correctly.",
          "created_at": "2025-03-16T13:06:51Z"
        }
      ]
    },
    {
      "issue_number": 1001,
      "title": "[Bug]: Large Document Processing Failure and Missing Graph Relationships",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nProcessing failure observed when handling a large text document (2.3MB TXT file). The system is unable to complete the expected processing tasks.\n\nCurrent Behavior  \n1. Document processing task does not complete successfully \n2. Expected data is missing from the database  \n3. Neo4j database shows connection blocking issues  \n4. Graph nodes exist in isolation without relationships between them\n\n\n\n### Steps to reproduce\n\n1. Upload a 2.3MB text document  \n2. Wait for processing to complete  \n3. Check database for results\n\n### Expected Behavior\n\n1. Database Connection  \n• Neo4j connection blocking observed  \n• Insufficient debug information in backend logs  \n2. Data Integrity  \n• Graph relationships are not established  \n• Nodes exist but are disconnected  \n• Expected processing results are missing from database\n\n### LightRAG Config Used\n\nDocument Size: 2.3MB\nFile Type: TXT\nDatabase: Neo4j (Graph Database)\n\n### Storage Configuration\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=Neo4JStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\n### Logs and screenshots\n\nneo4j logs\n\n![Image](https://github.com/user-attachments/assets/b7c3ef2b-0041-4486-8983-97da7ce8aeb8)\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "axunrun",
      "author_type": "User",
      "created_at": "2025-03-05T03:12:53Z",
      "updated_at": "2025-03-20T14:37:09Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1001/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1001",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1001",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.367849",
      "comments": [
        {
          "author": "15388576368",
          "body": "你好，你上传大型文件花费的时间长嘛\n",
          "created_at": "2025-03-20T14:37:07Z"
        }
      ]
    },
    {
      "issue_number": 1137,
      "title": "[Bug]: HF Cuda is not available on MacOS with Apple Silicon",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nOn MacOs with Apple Silicon, if you try to use HF models, you'll get the error:\n\"AssertionError: Torch not compiled with CUDA enabled\" \nbecause CUDA is not available.\n\n```\nasync def hf_embed(texts: list[str], tokenizer, embed_model) -> np.ndarray:\n    device = next(embed_model.parameters()).device\n    encoded_texts = tokenizer(\n        texts, return_tensors=\"pt\", padding=True, truncation=True\n    ).to(device)\n    with torch.no_grad():\n        outputs = embed_model(\n            input_ids=encoded_texts[\"input_ids\"],\n            attention_mask=encoded_texts[\"attention_mask\"],\n        )\n        embeddings = outputs.last_hidden_state.mean(dim=1)\n    if embeddings.dtype == torch.bfloat16:\n        return embeddings.detach().to(torch.float32).cpu().numpy()\n    else:\n        return embeddings.detach().cpu().numpy()\n```\n\n\n### Possibile Solution\n\nChanging the way the device is retrieved solved the issue\n\n    if torch.cuda.is_available():\n        device = next(embed_model.parameters()).device # Use CUDA if available\n    elif torch.backends.mps.is_available():\n        device = torch.device(\"mps\")  # Use MPS for Apple Silicon\n    else:\n        device = torch.device(\"cpu\")  # Fallback to CPU\n",
      "state": "open",
      "author": "mvignieri",
      "author_type": "User",
      "created_at": "2025-03-20T08:23:07Z",
      "updated_at": "2025-03-20T09:27:25Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1137/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1137",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1137",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.533426",
      "comments": []
    },
    {
      "issue_number": 1116,
      "title": "[Bug]: Context Reference section does not correctly display file_id or file_path",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nThe references provided in the response do not properly display the file_id or source.\n\n### Steps to reproduce\n\nSend a query using any of the mode settings and verify the response reference section.\n\n### Expected Behavior\n\nThe file references should correctly indicate the filename or source.\n\n### LightRAG Config Used\n\n### Server Configuration\nHOST=0.0.0.0\nPORT=9621\n# NAMESPACE_PREFIX=lightrag  # separating data from difference Lightrag instances\n# CORS_ORIGINS=http://localhost:3000,http://localhost:8080\n\n### Optional SSL Configuration\n# SSL=true\n# SSL_CERTFILE=/path/to/cert.pem\n# SSL_KEYFILE=/path/to/key.pem\n\n### Security (empty for no api-key is needed)\nLIGHTRAG_API_KEY=\n\nLLM_BINDING=\"azure_openai\"\nLLM_BINDING_HOST=\nLLM_MODEL=\"gpt-4o\"\nLLM_BINDING_API_KEY=\n\nEMBEDDING_BINDING=\"azure_openai\"\nEMBEDDING_MODEL=text-embedding-3-large\n\nAZURE_OPENAI_API_VERSION=\"2024-10-21\"\nAZURE_OPENAI_DEPLOYMENT=\"gpt-4o\"\nAZURE_OPENAI_API_KEY=\nAZURE_OPENAI_ENDPOINT=\n\n### Settings for document indexing\nCHUNK_SIZE=1200\nCHUNK_OVERLAP_SIZE=100\nMAX_TOKENS=32768             # Max tokens send to LLM for summarization\nMAX_TOKEN_SUMMARY=500        # Max tokens for entity or relations summary\nSUMMARY_LANGUAGE=English\nEMBEDDING_DIM=3072\n\n### Logging level\nLOG_LEVEL=INFO\nVERBOSE=True\n\n### Max async calls for LLM\nMAX_ASYNC=8\n### Optional Timeout for LLM\nTIMEOUT=90  # Time out in seconds, None for infinite timeout\n\n### Settings for RAG query\nHISTORY_TURNS=3\nCOSINE_THRESHOLD=0.2\nTOP_K=20\nMAX_TOKEN_TEXT_CHUNK=20000\nMAX_TOKEN_RELATION_DESC=4000\nMAX_TOKEN_ENTITY_DESC=4000\n\nWORKING_DIR=\\\\sgnt-dev-sgvt01\\d$\\VALI_DB\n\n### Data storage selection\nLIGHTRAG_KV_STORAGE=JsonKVStorage\nLIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nLIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\n\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/f27b17c3-8b35-42bb-920b-4165c63fbe0f)\n![Image](https://github.com/user-attachments/assets/ced7164d-aa22-48af-94c6-33a29fe81d65)\n![Image](https://github.com/user-attachments/assets/d04c0a8f-0fee-47da-864e-8f98760011a8)\n![Image](https://github.com/user-attachments/assets/49edf8bf-c90a-4722-bc4c-55bdbd4468fa)\n![Image](https://github.com/user-attachments/assets/f0e979c3-ecdc-40a3-9f7e-34c999133d41)\n\n### Additional Information\n\n- LightRAG Version: 1.2.6\n- Operating System: Windows 11\n- Python Version: 3.12.5\n- Related Issues:\n",
      "state": "open",
      "author": "BireleyX",
      "author_type": "User",
      "created_at": "2025-03-18T12:57:31Z",
      "updated_at": "2025-03-20T03:13:32Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1116/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1116",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1116",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.533445",
      "comments": [
        {
          "author": "BireleyX",
          "body": "The screenshots were taken one for each of the modes. \nFrom top to bottom: global, hybrid, local, mix, naive",
          "created_at": "2025-03-18T13:03:50Z"
        },
        {
          "author": "BireleyX",
          "body": "## This is taken from the WebUI document management tab. I think this may be related.\nThe error message started appearing after updating lightrag to 1.2.6:\n\n![Image](https://github.com/user-attachments/assets/f382e876-40f4-4fb4-bcc7-ebefa6133051)\n\n## And here is the corresponding error log from the ",
          "created_at": "2025-03-18T13:33:28Z"
        }
      ]
    },
    {
      "issue_number": 1131,
      "title": "[Question]: <Toggle the configuration of the llm or embedding model during use>",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIs it possible for us to switch the configuration of the llm or embedding model during use？\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "tianyi323",
      "author_type": "User",
      "created_at": "2025-03-20T02:15:07Z",
      "updated_at": "2025-03-20T02:15:07Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1131/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1131",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1131",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.737712",
      "comments": []
    },
    {
      "issue_number": 1130,
      "title": "[Feature Request]: <title> Gemini /  Gemini Embeddings Support",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\ncan we integrate Gemini and many other LLM models into Lightrag system? ensuring that it also works on backend server side too?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "psygenlab",
      "author_type": "User",
      "created_at": "2025-03-19T22:40:34Z",
      "updated_at": "2025-03-19T22:40:34Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1130/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1130",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1130",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.737733",
      "comments": []
    },
    {
      "issue_number": 1118,
      "title": "[Bug]: The example on the documentation.... don't work.",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nCloned the repo fresh and tried the example on the main page with debug enabled.. (downloaded the book, selected a working dir.. run the example)\n\n```\naintains continuity with the conversation history.\n\n---Data Sources---\n\n1. From Knowledge Graph(KG):\n-----Entities-----\n    ```csv\n    id, entity, type,   description,    rankcreated_at\n1,      Your text,category,Your text refers to the content being assessed, indicating a subject for analysis but lacks specific attributes and activities.,0,UNKNOWN\n    ```\n    -----Relationships-----\n    ```csv\n    id, source, target, description,    keywords,       weight, rank,   created_at\n    ```\n    -----Sources-----\n    ```csv\n    id, content\n1,      Your text\n    ```\n\n2. From Document Chunks(DC):\nNo relevant text information found\n\n---Response Rules---\n\n- Target format and length: Multiple Paragraphs\n- Use markdown formatting with appropriate section headings\n- Please respond in the same language as the user's question.\n- Ensure the response maintains continuity with the conversation history.\n- Organize answer in sesctions focusing on one main point or aspect of the answer\n- Use clear and descriptive section titles that reflect the content\n- List up to 5 most important reference sources at the end under \"References\" sesction. Clearly indicating whether each source is from Knowledge Graph (KG) or Vector Data (DC), in the following format: [KG/DC] Source content\n- If you don't know the answer, just say so. Do not make anything up.\n- Do not include information not provided by the Data Sources.\nDEBUG: Finalized Storages\n$ \n                                                                                   \n\n```\n\n\nNo reply to the question .. just loaded the document and .. finishes..\n\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "bzImage",
      "author_type": "User",
      "created_at": "2025-03-18T20:24:42Z",
      "updated_at": "2025-03-19T16:43:31Z",
      "closed_at": "2025-03-19T16:43:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1118/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1118",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1118",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.737741",
      "comments": []
    },
    {
      "issue_number": 1117,
      "title": "[Bug]: Installation Issue: Unable to Install LightRAG on Windows without Admin Rights",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n## Environment\n- Operating System: Windows\n- Python Version: 3.11\n- Installation Method: both pip install -e . and pip install lightrag-hku\n\n## Problem Description\nI have installed LightRAG frequently in the past without problems. But recently I am running into problems with it. I'm trying to install LightRAG on a Windows environment where I don't have administrator privileges. The installation initially fails because it attempts to build NumPy from source, which requires C++ compilers that I cannot install due to lack of admin rights.\n\nWhen trying to install dependencies with `--only-binary=:all:` flag, the initial pip installation seems to work, but when I attempt to run the program, it tries to automatically install additional dependencies that again require compilation.\n\n## Error Message\nWhen trying to install, I get this error:\n```\nPreparing metadata (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [21 lines of output]\n      + C:\\Python313\\python.exe C:\\Users\\local_veenmas\\Temp\\pip-install-_bemjigf\\numpy_2e857bb560ff436e9849db116dd8589c\\vendored-meson\\meson\\meson.py setup C:\\Users\\local_veenmas\\Temp\\pip-install-_bemjigf\\numpy_2e857bb560ff436e9849db116dd8589c C:\\Users\\local_veenmas\\Temp\\pip-install-_bemjigf\\numpy_2e857bb560ff436e9849db116dd8589c\\.mesonpy-xgjy16gy -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\local_veenmas\\Temp\\pip-install-_bemjigf\\numpy_2e857bb560ff436e9849db116dd8589c\\.mesonpy-xgjy16gy\\meson-python-native-file.ini\n      The Meson build system\n      Version: 1.2.99\n      Source dir: C:\\Users\\local_veenmas\\Temp\\pip-install-_bemjigf\\numpy_2e857bb560ff436e9849db116dd8589c\n      Build dir: C:\\Users\\local_veenmas\\Temp\\pip-install-_bemjigf\\numpy_2e857bb560ff436e9849db116dd8589c\\.mesonpy-xgjy16gy\n      Build type: native build\n      Project name: NumPy\n      Project version: 1.26.4\n      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n```\n\nThis error occurs because I don't have C++ compilers installed, and I cannot install them due to lack of administrative privileges.\n\n## What I've Tried\n1. Installing with `--only-binary=:all:` flag: `pip install --only-binary=:all: -r requirements.txt`\n2. Installing specific versions of packages that might have pre-compiled wheels\n3. Installing LightRAG with `--no-deps` after installing the dependencies: `pip install --no-deps git+https://github.com/HKUDS/LightRAG.git`\n\n## Feature Request\nWould it be possible to:\n1. Update the installation process to fully support binary-only installations?\n2. Add specific instructions for Windows users without admin rights?\n3. Consider packaging pre-compiled wheels for dependencies that typically require compilation?\n4. Prevent automatic installation of dependencies during runtime that require compilation?\n\nAny guidance or workarounds would be greatly appreciated. Thank you!\n\n### Steps to reproduce\n\nRefer to description\n\n### Expected Behavior\n\nNo errors\n\n### LightRAG Config Used\n\nThe error occured before LightRAG can be started.\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 18-03-2025\n- Operating System: Windows 11 enterprise  23H2\n- Python Version: 3.11\n- Related Issues: none\n",
      "state": "closed",
      "author": "stevenveenma",
      "author_type": "User",
      "created_at": "2025-03-18T16:37:31Z",
      "updated_at": "2025-03-19T09:09:11Z",
      "closed_at": "2025-03-19T09:06:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1117/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1117",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1117",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.737750",
      "comments": [
        {
          "author": "stevenveenma",
          "body": "I did some additional investigation and could pinpoint the problem to graspologic. Installing this package in 3.11 gives the discribed error, even in a fresch venv just using pip install graspologic. Unfortunately graspologic offers no pre-built binaries for 3.11. I don't understand why with earlier",
          "created_at": "2025-03-19T08:31:09Z"
        },
        {
          "author": "stevenveenma",
          "body": "Python 3.13 appeared to be installed on the AVD, without notice. So the solution is easy: install a venv with python3.11. ",
          "created_at": "2025-03-19T09:09:10Z"
        }
      ]
    },
    {
      "issue_number": 1123,
      "title": "[Question]: <title> Params: {}, Got: 'input'   what's the reason of this error",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nOpenAI API Call Failed,\nModel: qwen-2p5-32b-instruct-hw,\nParams: {}, Got: 'input'\nOpenAI API Call Failed,\nModel: qwen-2p5-32b-instruct-hw,\nParams: {}, Got: 'input'\nFailed to extract entities and relationships\nFailed to process document doc-3fb75ac7ee028455093f638ed0eb4974: 'input'\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "jxxj-66",
      "author_type": "User",
      "created_at": "2025-03-19T08:54:22Z",
      "updated_at": "2025-03-19T08:54:22Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1123/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1123",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1123",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.963571",
      "comments": []
    },
    {
      "issue_number": 1122,
      "title": "[Question]: <title>Export Knowledge Graph features not included in Realease logs 1.2.6?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThis is just a small question, but when I was checking the new release 1.2.6 I mainly saw just the issues fixed, but there wasn't any mention of the export lightrag to csv, txt etc which did get added in the lightrag.py file. \n\nWhy wasn't there any mention of it in the release notes? Is it because the features haven't been tested yet?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-03-19T07:11:32Z",
      "updated_at": "2025-03-19T07:11:32Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1122/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1122",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1122",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.963584",
      "comments": []
    },
    {
      "issue_number": 1119,
      "title": "[Bug]: numpy.rec runtime error",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nRuntimeError: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)\n\nI didnt understand well , Im using colab\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "emredeveloper",
      "author_type": "User",
      "created_at": "2025-03-18T23:12:14Z",
      "updated_at": "2025-03-18T23:12:14Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1119/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1119",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1119",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.963588",
      "comments": []
    },
    {
      "issue_number": 1089,
      "title": "[Question]: RedisKVStorage is not compatible with DOC_STATUS_STORAGE",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nIm trying to run the \"lightrag_openai_neo4j_milvus_redis_demo.py\" file and it contains the RedisKVStorage as doc_status_storage variable.\n\n rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        llm_model_max_token_size=32768,\n        embedding_func=embedding_func,\n        chunk_token_size=512,\n        chunk_overlap_token_size=256,\n        kv_storage=\"RedisKVStorage\",\n        graph_storage=\"Neo4JStorage\",\n        vector_storage=\"MilvusVectorDBStorage\",\n        doc_status_storage=\"RedisKVStorage\",\n    )\n\nI know from the init file that RedisKVStorage isn't one from the doc_storage list, but why is it included here then? Is this something you are planning on adding in the future?\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "frederikhendrix",
      "author_type": "User",
      "created_at": "2025-03-14T10:42:26Z",
      "updated_at": "2025-03-18T19:23:07Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1089/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1089",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1089",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:04.963592",
      "comments": [
        {
          "author": "husaynirfan1",
          "body": "Verified. I tested the same example file, `lightrag_openai_neo4j_milvus_redis_demo.py` and found that `doc_status_storage=RedisKVStorage`. Installed LightRAG from source.\n\nLightRAG Version: v1.2.5\nPython Version: 3.10.16\nOperating System: Ubuntu 24.04.1 LTS\n",
          "created_at": "2025-03-15T16:32:55Z"
        },
        {
          "author": "bzImage",
          "body": "so... \n\njust tyring the example with milvus i found:\n\nValueError: Storage implementation 'RedisKVStorage' is not compatible with DOC_STATUS_STORAGE. Compatible implementations are: JsonDocStatusStorage, PGDocStatusStorage, PGDocStatusStorage, MongoDocStatusStorage\n\n\nso.. RedisKVStorage does not exis",
          "created_at": "2025-03-18T19:23:06Z"
        }
      ]
    },
    {
      "issue_number": 958,
      "title": "[Bug]: ModuleNotFoundError: No module named 'graspologic' when running lightrag_ollama_demo.py",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen executing lightrag_ollama_demo.py, I get the following error:\n\nTraceback (most recent call last):\n  File \"C:\\Dev\\LightRAG\\examples\\lightrag_ollama_demo.py\", line 16, in <module>\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n    ...<11 lines>...\n        ),\n    )\n  File \"<string>\", line 37, in __init__\n  File \"C:\\Dev\\LightRAG\\lightrag\\lightrag.py\", line 350, in __post_init__\n    self.chunk_entity_relation_graph: BaseGraphStorage = self.graph_storage_cls(  # type: ignore\n                                                         ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n        namespace=make_namespace(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        embedding_func=self.embedding_func,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Dev\\LightRAG\\lightrag\\utils.py\", line 780, in import_class\n    module = importlib.import_module(module_name, package=package)\n  File \"C:\\Users\\birel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py\", line 88, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"C:\\Dev\\LightRAG\\lightrag\\kg\\networkx_impl.py\", line 25, in <module>\n    from graspologic import embed\nModuleNotFoundError: No module named 'graspologic'\n\n==========================================\n\nWhen attempting to install graspologic, I get:\n\n  Preparing metadata (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [78 lines of output]\n      C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:270: UserWarning: Unknown distribution option: 'test_suite'\n        warnings.warn(msg)\n      C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:270: UserWarning: Unknown distribution option: 'tests_require'\n        warnings.warn(msg)\n      running dist_info\n      creating C:\\Users\\birel\\AppData\\Local\\Temp\\pip-modern-metadata-a4ctva3t\\gensim.egg-info\n      writing C:\\Users\\birel\\AppData\\Local\\Temp\\pip-modern-metadata-a4ctva3t\\gensim.egg-info\\PKG-INFO\n      writing dependency_links to C:\\Users\\birel\\AppData\\Local\\Temp\\pip-modern-metadata-a4ctva3t\\gensim.egg-info\\dependency_links.txt  \n      writing requirements to C:\\Users\\birel\\AppData\\Local\\Temp\\pip-modern-metadata-a4ctva3t\\gensim.egg-info\\requires.txt\n      writing top-level names to C:\\Users\\birel\\AppData\\Local\\Temp\\pip-modern-metadata-a4ctva3t\\gensim.egg-info\\top_level.txt\n      writing manifest file 'C:\\Users\\birel\\AppData\\Local\\Temp\\pip-modern-metadata-a4ctva3t\\gensim.egg-info\\SOURCES.txt'\n      Traceback (most recent call last):\n        File \"c:\\Dev\\LightRAG\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>   \n          main()\n          ~~~~^^\n        File \"c:\\Dev\\LightRAG\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main       \n          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"c:\\Dev\\LightRAG\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 175, in prepare_metadata_for_build_wheel\n          return hook(metadata_directory, config_settings)\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 377, in prepare_metadata_for_build_wheel\n          self.run_setup()\n          ~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n          super().run_setup(setup_script=setup_script)\n          ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n          exec(code, locals())\n          ~~~~^^^^^^^^^^^^^^^^\n        File \"<string>\", line 367, in <module>\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 117, in setup\n          return distutils.core.setup(**attrs)\n                 ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 186, in setup\n          return run_commands(dist)\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 202, in run_commands\n          dist.run_commands()\n          ~~~~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 983, in run_commands\n          self.run_command(cmd)\n          ~~~~~~~~~~~~~~~~^^^^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 999, in run_command\n          super().run_command(command)\n          ~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_command\n          cmd_obj.run()\n          ~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\command\\dist_info.py\", line 94, in run\n          self.egg_info.run()\n          ~~~~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 312, in run\n          self.find_sources()\n          ~~~~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 320, in find_sources\n          mm.run()\n          ~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 543, in run\n          self.add_defaults()\n          ~~~~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\command\\egg_info.py\", line 581, in add_defaults\n          sdist.add_defaults(self)\n          ~~~~~~~~~~~~~~~~~~^^^^^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\command\\sdist.py\", line 109, in add_defaults\n          super().add_defaults()\n          ~~~~~~~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\sdist.py\", line 239, in add_defaults\n          self._add_defaults_ext()\n          ~~~~~~~~~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\sdist.py\", line 323, in _add_defaults_ext\n          build_ext = self.get_finalized_command('build_ext')\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 316, in get_finalized_command\n          cmd_obj.ensure_finalized()\n          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\birel\\AppData\\Local\\Temp\\pip-build-env-2mqs1z9c\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 124, in ensure_finalized\n          self.finalize_options()\n          ~~~~~~~~~~~~~~~~~~~~~^^\n        File \"<string>\", line 111, in finalize_options\n      AttributeError: 'dict' object has no attribute '__NUMPY_SETUP__' and no __dict__ for setting new attributes\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n\n### Steps to reproduce\n\nFollowed all setup instructions for lightrag and ollama.\nAfter correctly modifying examples/lightrag_ollama_demo.py with choice of ollama models, execute the script.\n\n### Expected Behavior\n\nNo errors.\n\n### LightRAG Config Used\n\nv1.2.2\nOllama = llama3.1:latest, nomic-embed-text\n[lightrag_ollama_demo.py](https://github.com/HKUDS/LightRAG/blob/main/examples/lightrag_ollama_demo.py)\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "BireleyX",
      "author_type": "User",
      "created_at": "2025-02-27T11:22:40Z",
      "updated_at": "2025-03-18T13:07:28Z",
      "closed_at": "2025-03-18T13:07:27Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/958/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/958",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/958",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:05.172598",
      "comments": [
        {
          "author": "tonyatpeking",
          "body": "Same issue here. Downgrading python 3.13 -> 3.12 fixed it for me. From the graspologic repo they list python 3.12 as their latest supported version.",
          "created_at": "2025-02-27T21:34:07Z"
        },
        {
          "author": "Mybigwang",
          "body": "i have similar issue about gensim,  it shows cant find some h file in word2vec.c file of gensim when i use python 3.13. Then, i use python 3.10 retrying and it fixed.",
          "created_at": "2025-03-16T11:52:10Z"
        },
        {
          "author": "BireleyX",
          "body": "Issue is fixed. latest codebase 1.2.6 works with python 3.12.5",
          "created_at": "2025-03-18T13:07:27Z"
        }
      ]
    },
    {
      "issue_number": 1101,
      "title": "[Feature Request]: Dynamically set the default vector database",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nWhen I follow official example to initializing `light_rag`, if I only fill in the parameters of `graph_storage`, even though my `graph_storage` supports vector db's functionality (such as `age`), `light_rag` still uses the default `nano_vector` as entities_db for low_level_keyword query.This is unfriendly to users because it will directly report an error\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "pengjunfeng11",
      "author_type": "User",
      "created_at": "2025-03-17T07:12:13Z",
      "updated_at": "2025-03-18T08:38:55Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1101/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1101",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1101",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:05.361142",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Try to use the API Server and follow the README to select storage implementation of： KV、vector、graph and doc_status. You can explore the lightrag-server.py for how to setup storage if you like to integrate LightRAG core in your project.\n\nhttps://github.com/HKUDS/LightRAG/blob/main/lightrag/api/READM",
          "created_at": "2025-03-18T08:38:54Z"
        }
      ]
    },
    {
      "issue_number": 1106,
      "title": "[Bug]: Type error in Utils.py",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nLightRAG is not finishing cleanly. Producing a Type error.  See error below:\nException ignored in: <function LightRAG.__del__ at 0x000001BE034E6FC0>\nTraceback (most recent call last):\n  File \"c:\\Users\\lkden\\OneDrive\\Documents\\GitHub\\pythonLearning\\lightrag2-env\\Lib\\site-packages\\lightrag\\lightrag.py\", line 433, in __del__\n  File \"c:\\Users\\lkden\\OneDrive\\Documents\\GitHub\\pythonLearning\\lightrag2-env\\Lib\\site-packages\\lightrag\\lightrag.py\", line 438, in _run_async_safely\n  File \"c:\\Users\\lkden\\OneDrive\\Documents\\GitHub\\pythonLearning\\lightrag2-env\\Lib\\site-packages\\lightrag\\utils.py\", line 863, in always_get_an_event_loop\nTypeError: 'NoneType' object is not callable\n\n### Steps to reproduce\n\nUsing Python 3.12.6\nLightRAG 1.2.5\nRun the demo code in the Readme.txt file.  When everything is done and the Python script is finishing, then the error is shown.  This is also happening when using setting up LightRAG to be \"served\" in a Python web server. \n\n### Expected Behavior\n\nClose and finish cleanly and quietly. \n\n### LightRAG Config Used\n\n# Paste your config here\nasync def llm_model_func(prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs) -> str:\n    return await openai_complete_if_cache(\n        \"gpt-4o\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        base_url=BASE_URL,\n        api_key=API_KEY,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=\"text-embedding-3-small\",\n        base_url=BASE_URL,\n        api_key=API_KEY,\n    )\n\n\nasync def ragInit():\n    #embedding_dimension = await get_embedding_dim()\n\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1536,\n            max_token_size=8192,\n            func=embedding_func,\n        ),\n    )\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.2.5\n- Operating System: Windows 11\n- Python Version: 3.12.6\n- Related Issues:  None\n",
      "state": "open",
      "author": "LorenDH",
      "author_type": "User",
      "created_at": "2025-03-18T00:31:07Z",
      "updated_at": "2025-03-18T08:31:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1106/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1106",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1106",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:05.691298",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Which demo code you are runing? Try to install LihgtRAG API Server, and use the webui to add files and to some test query.\n\nhttps://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md",
          "created_at": "2025-03-18T08:31:05Z"
        }
      ]
    },
    {
      "issue_number": 1110,
      "title": "[Bug]: Missing modules reported when running LightRAG API server using Gunicorn",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI'm having issue running using gunicorn (in Windows 11). There seems to be some missing packages:\n- FCNTL\n- PWD\nI tried installing WinFCNTL and modified the references to FCNTL. \nBut after that, another missing module PWD is raised.\nI couldn't find a suitable package in PYPI for PWD.\nI can run \"lightrag-server.exe\" just fine. So don't think I missed a setup step.\nHas anyone had this problem?\n\nHere's the error I receive after 'fixing' FCNTL:\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"C:\\dev\\LightRAG\\.venv\\Scripts\\lightrag-gunicorn.exe\\__main__.py\", line 7, in <module>\n  File \"C:\\DEV\\LightRAG\\lightrag\\api\\run_with_gunicorn.py\", line 74, in main\n    from gunicorn.app.base import BaseApplication\n  File \"c:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\gunicorn\\app\\base.py\", line 10, in <module>\n    from gunicorn import util\n  File \"c:\\DEV\\LightRAG\\.venv\\Lib\\site-packages\\gunicorn\\util.py\", line 14, in <module>\n    import pwd\nModuleNotFoundError: No module named 'pwd'\n\n_Originally posted by @BireleyX in https://github.com/HKUDS/LightRAG/issues/969#issuecomment-2731985473_\n            \n\n### Steps to reproduce\n\nEnvironment:\nWindows 11, Python 3.12.5\n\nSetup the project following the LightRAG API readme.md:\nhttps://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md\n\nUsed base LightRag code from repository, and used PYPI for API module/package.\n\n### Expected Behavior\n\nNo missing modules.\n\n### LightRAG Config Used\n\n### Server Configuration\n# HOST=0.0.0.0\n# PORT=9621\n# NAMESPACE_PREFIX=lightrag  # separating data from difference Lightrag instances\n# CORS_ORIGINS=http://localhost:3000,http://localhost:8080\n\n### Optional SSL Configuration\n# SSL=true\n# SSL_CERTFILE=/path/to/cert.pem\n# SSL_KEYFILE=/path/to/key.pem\n\n### Security (empty for no api-key is needed)\nLIGHTRAG_API_KEY=abc123\n\nLLM_BINDING=azure_openai\nLLM_BINDING_HOST=\nLLM_MODEL=gpt-4o-mini\nLLM_BINDING_API_KEY=\nAZURE_OPENAI_API_VERSION=\"2024-10-21\"\n\nAZURE_OPENAI_API_KEY=\nAZURE_OPENAI_ENDPOINT=\nAZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large\nAZURE_EMBEDDING_API_VERSION=\"2024-10-21\"\n\nEMBEDDING_BINDING=azure_openai\nEMBEDDING_MODEL=text-embedding-3-large\nEMBEDDING_DIM=3072\n\nCHUNK_SIZE=400\nCHUNK_OVERLAP_SIZE=100\n\nWORKING_DIR=\\\\sgnt-dev-sgvt01\\d$\\VALI_DB\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: 1.2.5\n- Operating System: Windows 11\n- Python Version: 3.12.5\n\n",
      "state": "closed",
      "author": "BireleyX",
      "author_type": "User",
      "created_at": "2025-03-18T08:04:40Z",
      "updated_at": "2025-03-18T08:20:39Z",
      "closed_at": "2025-03-18T08:20:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1110/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1110",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1110",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:05.951468",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Multi worker mode in not supported by Gunicorn.",
          "created_at": "2025-03-18T08:20:38Z"
        }
      ]
    },
    {
      "issue_number": 1076,
      "title": "[Bug]: <title> rag query error with PG storage: AttributeError: 'int' object has no attribute 'count'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI used the git main branch, and the demo code as below:\n```python\nload_dotenv()\nROOT_DIR = \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag\"\nWORKING_DIR = f\"{ROOT_DIR}/dickens-pg\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\nos.environ[\"POSTGRES_HOST\"] = \"localhost\"\nos.environ[\"POSTGRES_PORT\"] = \"5432\"\nos.environ[\"POSTGRES_USER\"] = \"postgres\"\nos.environ[\"POSTGRES_PASSWORD\"] = \"\"\nos.environ[\"POSTGRES_DATABASE\"] = \"lightrag\"\n\nasync def _llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        model=\"qwen-plus-latest\",\n        prompt=prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        **kwargs\n    )\n\nasync def _embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=\"text-embedding-v3\",\n        api_key=\"***\",\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=_llm_model_func,\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=1024,\n            max_token_size=8192,\n            func=_embedding_func,\n        ),\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        vector_storage=\"PGVectorStorage\",\n        auto_manage_storages_states=False,\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n\nasync def main():\n    rag = await initialize_rag()\n\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n    with open(f\"{ROOT_DIR}/docs/DockerDeployment.md\", \"r\", encoding=\"utf-8\") as f:\n        await rag.ainsert(f.read())\n\n    print(\"==== Trying to test the rag queries ====\")\n\n    print(\"**** Start Hybrid Query ****\")\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nRun it, then get errors:\n```\nTraceback (most recent call last):\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/examples/lightrag_zhipu_postgres_demo.py\", line 128, in <module>\n    asyncio.run(main())\n  File \"/opt/miniconda3/envs/matetext/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/miniconda3/envs/matetext/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/examples/lightrag_zhipu_postgres_demo.py\", line 103, in main\n    await rag.aquery(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/lightrag.py\", line 1253, in aquery\n    response = await kg_query(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/operate.py\", line 714, in kg_query\n    context = await _build_query_context(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/operate.py\", line 1095, in _build_query_context\n    entities_context, relations_context, text_units_context = await _get_node_data(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/operate.py\", line 1185, in _get_node_data\n    node_datas, node_degrees = await asyncio.gather(\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 1136, in node_degree\n    record = (await self._query(query))[0]\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 1082, in _query\n    result = [self._record_to_dict(d) for d in data]\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 1082, in <listcomp>\n    result = [self._record_to_dict(d) for d in data]\n  File \"/Users/apple/workplace/AI/matetext-ai/libs/lightrag/lightrag/kg/postgres_impl.py\", line 953, in _record_to_dict\n    if v is None or (v.count(\"{\") < 1 and v.count(\"[\") < 1):\nAttributeError: 'int' object has no attribute 'count'\n```\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-03-13T02:56:13Z",
      "updated_at": "2025-03-17T16:45:46Z",
      "closed_at": "2025-03-15T02:17:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1076/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1076",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1076",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:06.212549",
      "comments": [
        {
          "author": "lameroot",
          "body": " I have the same error\n```\n(lightrag) ➜  examples git:(main) ✗ python lightrag_zhipu_postgres_demo.py\nINFO: Process 48145 Shared-Data created for Single Process\nINFO: Process 48145 Pipeline namespace initialized\nkeys: {'doc-b3d3cc8224922a1de0e390c25ed321fb'}\nnew_keys: set()\nFailed to extract entitie",
          "created_at": "2025-03-13T12:28:26Z"
        },
        {
          "author": "OxidBurn",
          "body": "Released resolution in the [PR #1085](https://github.com/HKUDS/LightRAG/pull/1085)",
          "created_at": "2025-03-13T15:40:44Z"
        },
        {
          "author": "reqyou",
          "body": "Good job. Will there be a pypi release soon? ",
          "created_at": "2025-03-17T16:45:45Z"
        }
      ]
    },
    {
      "issue_number": 1099,
      "title": "[Bug]: When using the postgres@15 version, there will be an error",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHere is bug detail\n```\nPostgreSQL database, error:subquery in FROM must have an alias\nHINT:  For example, FROM (SELECT ...) [AS] foo.\n```\nIt's should be the SQL Template problem\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "pengjunfeng11",
      "author_type": "User",
      "created_at": "2025-03-17T02:49:30Z",
      "updated_at": "2025-03-17T02:54:50Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1099/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1099",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1099",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:06.469062",
      "comments": [
        {
          "author": "pengjunfeng11",
          "body": "And i have commit pr in #1100 to fix this BUG",
          "created_at": "2025-03-17T02:54:17Z"
        }
      ]
    },
    {
      "issue_number": 1095,
      "title": "[Question]: How is OpenAI API Call Concurrency Managed?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n\n\n\n\n**Hi there,**  \n\nFirst of all, thank you for sharing this amazing project! I’ve been going through the code, and I really appreciate the effort and thought put into it.  \n\nI had a question regarding how the project manages concurrent OpenAI API calls. From what I can see, multiple functions (e.g., `extract_entities`, `kg_query`, `mix_kg_vector_query`, etc.) use `llm_model_func`, which in turn calls `openai_complete_if_cache`. However, I didn’t notice any explicit mechanism limiting the number of concurrent requests to OpenAI.  \n\nGiven that OpenAI enforces rate limits, I was wondering:  \n\n1. **Is there an existing mechanism to control the number of concurrent API calls that I might have missed?**  \n2. **If not, was this an intentional design choice, or could this potentially lead to exceeding rate limits when multiple requests are sent simultaneously?**  \n3. **Would adding an async semaphore (e.g., `asyncio.Semaphore`) be a recommended way to limit concurrent calls if needed?**  \n\nI just wanted to clarify this before running it at scale. I really appreciate any insights you can provide. Thanks again for the great work!  \n\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "zhangxingeng",
      "author_type": "User",
      "created_at": "2025-03-16T00:56:43Z",
      "updated_at": "2025-03-16T17:23:37Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1095/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1095",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1095",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:06.658341",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Optimize the search for the parameter `llm_model_max_async`, which governs the maximum number of parallel requests for the Large Language Model (LLM). In the event of reaching the rate limit, the system will implement a randomized delay followed by a retry mechanism.",
          "created_at": "2025-03-16T17:23:36Z"
        }
      ]
    },
    {
      "issue_number": 975,
      "title": "[Feature Request]: Node Expansion and Hiding Features to Knowledge Graph Frontend",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nWhen inspecting knowledge graphs with numerous nodes, it's often necessary to expand nodes hierarchically one by one, or prune (hide) nodes from large and cluttered graphs.\n\n#### Feature1: Node Expansion\nSelect a node of interest in the existing subgraph for expansion, adding all neighboring nodes connected to it.\n\n#### Feature2: Node Hiding:\nSelect an irrelevant node in the existing subgraph, removing that node and all its neighboring nodes. Before hiding nodes, check if the operation would result in an empty graph. If so, the hiding operation should be rejected.\n\n#### Feature3: More flexible graph search\nThe system supports both inclusive and exact search functionalities, while also enabling users to specify the minimum degree threshold for returned nodes.\n\n#### UI Implementation Suggestion\nIt is recommended to add two buttons in the node's detail tooltip window, providing expand and hide functionalities respectively.\n\n### Additional Context\n\n<img width=\"1176\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9afebd62-c486-40c3-92d6-b58fbbaa038a\" />",
      "state": "closed",
      "author": "danielaskdd",
      "author_type": "User",
      "created_at": "2025-03-02T17:17:27Z",
      "updated_at": "2025-03-16T07:17:36Z",
      "closed_at": "2025-03-16T07:17:36Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/975/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/975",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/975",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:06.831768",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "@ArnoChenFx @ParisNeo @LarFii @ArnoChenFx Any idea about this new feature?",
          "created_at": "2025-03-04T05:15:16Z"
        },
        {
          "author": "danielaskdd",
          "body": "jobs done:\n\n<img width=\"888\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/abbb0e2e-069c-4968-a2bd-37b57995c13f\" />",
          "created_at": "2025-03-16T07:16:36Z"
        }
      ]
    },
    {
      "issue_number": 1040,
      "title": "[Question]: <title>在windows环境中通过lightrag-server 命令直接启动并没有加载自定义的.env文件中的配置",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n在windows环境中直接使用pip安装api服务后看到启动窗口中Working Directory是 C:\\Users\\xxx\\rag_storage,所以在该目录下创建了.env文件并且更改了服务启动的端口为9622，再次运行linght-server命令启动时启动端口并未改变，请问这种未加载.env文件的情况如何处理？\n\n![Image](https://github.com/user-attachments/assets/c7b78dc2-7bbd-4215-8842-125f1d0d0cf2)\n\n![Image](https://github.com/user-attachments/assets/4cc3c789-3779-4bba-8c0d-f1b91c138788)\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "zfpromise",
      "author_type": "User",
      "created_at": "2025-03-10T09:35:39Z",
      "updated_at": "2025-03-15T15:54:01Z",
      "closed_at": "2025-03-11T09:01:36Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1040/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1040",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1040",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:07.008559",
      "comments": [
        {
          "author": "danielaskdd",
          "body": ".env  需要放在当前目录",
          "created_at": "2025-03-11T08:52:53Z"
        },
        {
          "author": "zfpromise",
          "body": "> .env 需要放在当前目录\n\n我使用命令 pip install \"lightrag-hku[api]\" 安装的\n运行 lightrag-server 命令启动的api服务，请问",
          "created_at": "2025-03-11T09:01:36Z"
        },
        {
          "author": "danielaskdd",
          "body": "pip库中的版本可能太旧了，请改用源码安装吧",
          "created_at": "2025-03-15T15:54:00Z"
        }
      ]
    },
    {
      "issue_number": 1039,
      "title": "[Bug]:  Unauthorized Access Issues in Latest Version (Post 1.2.1) of LightRAG",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nAfter upgrading to a version beyond 1.2.1, encounter 401 Unauthorized errors when accessing the web UI. \n\n### Steps to reproduce\n\n- Ensure you are using a version of LightRAG after 1.2.1 (affected versions unknown).\n- A functional environment with LightRAG installed.\n- .env file with default authentication settings (without extended WHITELIST_PATHS).\n\nTry accessing any API end point \"GET /graph/label/list HTTP/1.1\" 401 \n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\nlightrag>1.2.1\n\n\n\n### Logs and screenshots\n\nlightrag.log shows:\n2025-03-09 12:15:55,235 - lightrag - INFO - Created new empty graph\n... [everything ok]\n2025-03-09 12:15:55,280 - uvicorn.error - INFO - Application startup complete.\n2025-03-09 12:15:55,280 - uvicorn.error - INFO - Uvicorn running on http://0.0.0.0:9621/ (Press CTRL+C to quit)\n2025-03-09 12:16:00,641 - uvicorn.access - INFO - 127.0.0.1:55827 - \"GET /documents HTTP/1.1\" 401\nand some others of that type... e.g.:\nINFO: 127.0.0.1:65027 - \"GET /graph/label/list HTTP/1.1\" 401\n\n### Additional Information\n\n- LightRAG Version:1.2.3\n- Operating System:ubuntu\n- Python Version:py 3.10\n- Related Issues:\n",
      "state": "closed",
      "author": "ArindamRoy23",
      "author_type": "User",
      "created_at": "2025-03-10T06:25:47Z",
      "updated_at": "2025-03-15T06:57:44Z",
      "closed_at": "2025-03-15T06:57:44Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1039/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1039",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1039",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:07.181097",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Pls remove authentication settings. The login page is not implemented yet.",
          "created_at": "2025-03-11T08:57:14Z"
        },
        {
          "author": "ArindamRoy23",
          "body": "Hey! Where do I remove these from, could you please help me out",
          "created_at": "2025-03-12T07:17:50Z"
        },
        {
          "author": "danielaskdd",
          "body": "remove the authentication settings  from .env",
          "created_at": "2025-03-14T02:33:55Z"
        }
      ]
    },
    {
      "issue_number": 1056,
      "title": "[Question]: <title> How to do the data isolation?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIf there are lots of users updoad text to lightrag, and query content, need to do data isolation, what should I do?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "jasperchen01",
      "author_type": "User",
      "created_at": "2025-03-11T10:52:25Z",
      "updated_at": "2025-03-15T02:16:29Z",
      "closed_at": "2025-03-15T02:16:29Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1056/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1056",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1056",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:07.362295",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "LightRAG is currently planning the Workspace feature. Let's discuss #1016 to align on the next steps.",
          "created_at": "2025-03-12T04:03:02Z"
        },
        {
          "author": "JoramMillenaar",
          "body": "I'm still trying to understand the system, but doesn't the namespace_prefix (argument) offer data isolation? You can't necessarily pass that at runtime, but I think it's a way of spinning up a separate environment for the server.",
          "created_at": "2025-03-13T16:57:56Z"
        },
        {
          "author": "jasperchen01",
          "body": "> I'm still trying to understand the system, but doesn't the namespace_prefix (argument) offer data isolation? You can't necessarily pass that at runtime, but I think it's a way of spinning up a separate environment for the server.\n\nThat's a option, thx",
          "created_at": "2025-03-15T02:16:09Z"
        }
      ]
    },
    {
      "issue_number": 584,
      "title": "Feature Request: Add JSON Support",
      "body": "Currently, the project supports plaintext well for embedding and retrieval. However, adding JSON support would significantly enhance its usability by enabling users to integrate the RAG framework more seamlessly into modern workflows where JSON is the standard format for data interchange. Will LightRAG add support for structured data like JSON?",
      "state": "open",
      "author": "LouisLiuNova",
      "author_type": "User",
      "created_at": "2025-01-14T13:22:52Z",
      "updated_at": "2025-03-14T14:06:17Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/584/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/584",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/584",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:07.612751",
      "comments": [
        {
          "author": "nickjfrench",
          "body": "Interested to learn more about this as I was planning to insert JSON. How does it behave currently when you pass a JSON file in?",
          "created_at": "2025-01-21T16:08:27Z"
        },
        {
          "author": "LouisLiuNova",
          "body": "> Interested to learn more about this as I was planning to insert JSON. How does it behave currently when you pass a JSON file in?\n\nI plan to use this for storing JSON-formatted data, but I haven’t extracted the semantic data from my dataset yet. Once I complete that, I’ll provide more detailed info",
          "created_at": "2025-01-29T12:02:39Z"
        },
        {
          "author": "nickjfrench",
          "body": "> I plan to use this for storing JSON-formatted data, but I haven’t extracted the semantic data from my dataset yet. Once I complete that, I’ll provide more detailed information.\n\nI tried uploading a somewhat large Json Structure, it was somewhat okay, but wasn't great. I think the lack of sentences",
          "created_at": "2025-01-30T13:59:03Z"
        },
        {
          "author": "LouisLiuNova",
          "body": "> I tried uploading a somewhat large Json Structure, it was somewhat okay, but wasn't great. I think the lack of sentences and fluff didn't allow the LightRAG processing calls to understand the context of the information. E.g. the nesting of objects should almost be the edges in the graph (at least ",
          "created_at": "2025-02-01T07:09:08Z"
        },
        {
          "author": "nickjfrench",
          "body": "> I anticipated this scenario, as LightRAG is primarily designed for natural language processing and storage. My plan is to refactor the JSON data into a complete sentence for storage, rather than directly inputting structured data.\n\nIf you don't mind sharing what you end up doing, I'd love to see w",
          "created_at": "2025-02-03T18:50:37Z"
        }
      ]
    },
    {
      "issue_number": 998,
      "title": "[Question]: Too long lasting queries on LightRAG",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHow long is supposed to last a query to LighRAG (any mode)? I've noticed a huge difference between the model's response time (qwen2.5:14b) when it's used on its own (~8s) and when it's used with LightRAG (~5min).\nIs such a difference normal? Where could the bottleneck be? \n\n### Additional Context\n\nGPU : A10, 24g VRAM\nQueries model: local qwen2.5:14b (with extended context as required)\nDB: Postgres for all the storage running on a docker container\n\n",
      "state": "open",
      "author": "Brocowlee",
      "author_type": "User",
      "created_at": "2025-03-04T16:18:02Z",
      "updated_at": "2025-03-14T09:55:02Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/998/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/998",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/998",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:07.811101",
      "comments": [
        {
          "author": "Michael-Bird123",
          "body": "it‘s to hard.......",
          "created_at": "2025-03-05T09:13:27Z"
        },
        {
          "author": "danielaskdd",
          "body": "The issue is likely attributed to the LLM's context size limitations. LightRAG necessitates a substantial context size for the LLM to effectively perform summarization tasks. While your LLM efficiently handles quick and intelligent Q&A within smaller context sizes, it experiences a significant slowd",
          "created_at": "2025-03-12T04:15:01Z"
        },
        {
          "author": "Brocowlee",
          "body": "Thank you for your answer.\nAs I mentioned in my post the retrieval model I'm using has an extended context window to fit the requirements mentioned in the README. \nDo you advise me to extend it even more, or should I completely change the model I'm using ?",
          "created_at": "2025-03-14T09:55:02Z"
        }
      ]
    },
    {
      "issue_number": 1057,
      "title": "[Bug]: graspologic module not available in docker container",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nAfter building and running LightRAG server in a docker container, I found the following errors in the container log: ModuleNotFoundError: No module named 'graspologic'. The LightRAG server WebUI is not accessible.\n\n\n### Steps to reproduce\n\nRun `docker-compose up -d --build` and check the logs with `docker logs --tail 1000 -f ...` \n\n### Expected Behavior\n\nLightRAG Server should be running without errors and restarting and should be available on `http://localhost:9621/webui`\n\n### LightRAG Config Used\n\nThe issues is not related to configuration settings\n\n### Logs and screenshots\n\n\n```\nCollecting graspologic\n  Downloading graspologic-3.4.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting POT<0.10,>=0.9 (from graspologic)\n  Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (34 kB)\nCollecting anytree<3.0.0,>=2.12.1 (from graspologic)\n  Downloading anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\nCollecting beartype<0.19.0,>=0.18.5 (from graspologic)\n  Downloading beartype-0.18.5-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: gensim<5.0.0,>=4.3.2 in /root/.local/lib/python3.11/site-packages (from graspologic) (4.3.3)\nCollecting graspologic-native<2.0.0,>=1.2.1 (from graspologic)\n  Downloading graspologic_native-1.2.3.tar.gz (2.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 2.0 MB/s eta 0:00:00\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'error'\n  error: subprocess-exited-with-error\n  \n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [6 lines of output]\n      \n      Cargo, the Rust package manager, is not installed or is not on PATH.\n      This package requires Rust and Cargo to compile extensions. Install it through\n      the system's package manager or via https://rustup.rs/\n      \n      Checking for Rust toolchain....\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n[notice] A new release of pip is available: 24.0 -> 25.0.1\n[notice] To update, run: pip install --upgrade pip\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/app/lightrag/api/lightrag_server.py\", line 586, in <module>\n    main()\n  File \"/app/lightrag/api/lightrag_server.py\", line 563, in main\n    app = create_app(args)\n          ^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/api/lightrag_server.py\", line 300, in create_app\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 37, in __init__\n  File \"/app/lightrag/lightrag.py\", line 377, in __post_init__\n    self.chunk_entity_relation_graph: BaseGraphStorage = self.graph_storage_cls(  # type: ignore\n                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/utils.py\", line 888, in import_class\n    module = importlib.import_module(module_name, package=package)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/app/lightrag/kg/networkx_impl.py\", line 19, in <module>\n    from graspologic import embed\nModuleNotFoundError: No module named 'graspologic'\n\n### Additional Information\n\n- LightRAG Version: v1.2.5\n- Operating System: Docker\n- Python Version: 3.11\n- Related Issues:\n",
      "state": "closed",
      "author": "coredevorg",
      "author_type": "User",
      "created_at": "2025-03-11T11:14:18Z",
      "updated_at": "2025-03-14T09:03:32Z",
      "closed_at": "2025-03-14T09:03:32Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1057/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1057",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1057",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:07.988883",
      "comments": []
    },
    {
      "issue_number": 1086,
      "title": "[Bug]: Is there even a single configuration working atm?",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHi, \nis it possible that there isnt a single configuration working atm?\nNeo4j ends up with throwing:\n`cannot access local variable 'result' where it is not associated with a value`\nPG ends up with throwing:\n`'bool' object has no attribute 'count'`\nMONGO:\ni dont know, never worked for me. \n\nSetting no graph or kv storage also doesnt work anymore. \n\nCan anyone help out here? Is there atleast one setup atm working out? \n\nBest,\nreQ\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "reqyou",
      "author_type": "User",
      "created_at": "2025-03-13T15:19:54Z",
      "updated_at": "2025-03-14T03:50:12Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1086/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1086",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1086",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:07.988904",
      "comments": [
        {
          "author": "JoramMillenaar",
          "body": "This repo is definitely under active development, so it's a little unstable unfortunately.\nRegarding your PG issues (https://github.com/HKUDS/LightRAG/issues/1076), there's a fix for that in the pipeline; https://github.com/HKUDS/LightRAG/pull/1085. \n\nPip installing the latest version, rather than p",
          "created_at": "2025-03-13T16:51:17Z"
        },
        {
          "author": "danielaskdd",
          "body": "You can use API server for getting start. It come up with a working default setting. https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/README.md ",
          "created_at": "2025-03-14T03:49:59Z"
        }
      ]
    },
    {
      "issue_number": 939,
      "title": "[Question]: Slow query on connections",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhen using mixed query and Postgres for graph storage we use this kind of a query:\n\n```\nSELECT * FROM cypher('type_id_935ef95e_945b_4fbf_b2a9_2b52789033ec', $$\n                      MATCH (n:Entity {node_id: \"x524f42555354204f47204b4c494d4154494c504153534554204b4f4e535452554b534a4f4e\"})\n                      OPTIONAL MATCH (n)-[]-(connected)\n                      RETURN n, connected\n                    $$) AS (n agtype, connected agtype)\n```\n\nAs we have created index for Entity it does use it but my suspicion is that it goes sequentually on OPTIONAL MATCH part and it takes a lot of time to query.\n\nSo :\n\n```\nEXPLAIN\nSELECT *\nFROM cypher('type_id_935ef95e_945b_4fbf_b2a9_2b52789033ec', $$\n    MATCH (n:Entity {node_id: \"x46415341444550524f534a454b54\"})\n    OPTIONAL MATCH (n)-[]-(connected)\n    RETURN n, connected\n$$) AS (n agtype, connected agtype);\n```\n\nGives:\n```\nNested Loop Left Join  (cost=11.52..2546265.46 rows=2424 width=64)\n\"  Join Filter: (((_age_default_alias_0.start_id = (age_id(_agtype_build_vertex(n.id, _label_name('33410'::oid, n.id), n.properties)))::graphid) AND (_age_default_alias_0.end_id = connected.id)) OR ((_age_default_alias_0.end_id = (age_id(_agtype_build_vertex(n.id, _label_name('33410'::oid, n.id), n.properties)))::graphid) AND (_age_default_alias_0.start_id = connected.id)))\"\n\"  ->  Bitmap Heap Scan on \"\"Entity\"\" n  (cost=11.52..17.10 rows=3 width=260)\"\n\"        Recheck Cond: (properties @> '{\"\"node_id\"\": \"\"x46415341444550524f534a454b54\"\"}'::agtype)\"\n        ->  Bitmap Index Scan on entity_node_id_gin_idx  (cost=0.00..11.52 rows=3 width=0)\n\"              Index Cond: (properties @> '{\"\"node_id\"\": \"\"x46415341444550524f534a454b54\"\"}'::agtype)\"\n  ->  Nested Loop  (cost=0.00..202357.05 rows=16159709 width=56)\n        ->  Append  (cost=0.00..175.34 rows=6023 width=16)\n              ->  Seq Scan on _ag_label_edge _age_default_alias_0_1  (cost=0.00..0.00 rows=1 width=16)\n\"              ->  Seq Scan on \"\"DIRECTED\"\" _age_default_alias_0_2  (cost=0.00..145.22 rows=6022 width=16)\"\n        ->  Materialize  (cost=0.00..192.06 rows=2683 width=40)\n              ->  Append  (cost=0.00..178.65 rows=2683 width=40)\n                    ->  Seq Scan on _ag_label_vertex connected_1  (cost=0.00..0.01 rows=1 width=40)\n\"                    ->  Seq Scan on \"\"Entity\"\" connected_2  (cost=0.00..165.23 rows=2682 width=40)\"\n```\n\nAnd I actually created indexes from the README file of this repo under Postgresql chapter. \n\nIs that a common issue?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "abylikhsanov",
      "author_type": "User",
      "created_at": "2025-02-24T21:29:56Z",
      "updated_at": "2025-03-13T21:46:11Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/939/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/939",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/939",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:08.242723",
      "comments": [
        {
          "author": "Daggle24",
          "body": "I've notice this on the last updates, on previous releases, this won't happen.\nSuddenly the queries are a lot slower. It has a lot of relationship with this query in particular.\n\nI've ran a performance test and this query is the one that is taking TOO LONG\n\nIn my case:\n\n```sql\n* FROM cypher('amaine_",
          "created_at": "2025-02-26T01:51:49Z"
        },
        {
          "author": "Daggle24",
          "body": "I've implemented a new function in the postgresql_impl in order to do this same thing but all entities in a batch call.\n\n```python\nasync def get_node_edges_batch(self, source_node_ids: list[str]) -> list[tuple[str, str]] | None:\n        \"\"\"\n        Retrieves all edges (relationships) for multiple no",
          "created_at": "2025-02-26T03:39:03Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "> I've implemented a new function in the postgresql_impl in order to do this same thing but all entities in a batch call.\n> \n> async def get_node_edges_batch(self, source_node_ids: list[str]) -> list[tuple[str, str]] | None:\n>         \"\"\"\n>         Retrieves all edges (relationships) for multiple no",
          "created_at": "2025-02-26T07:03:38Z"
        },
        {
          "author": "Daggle24",
          "body": "\n> Thank you. I wonder if this code can be deployed for other types?\n\nSure, It would follow the same login for all other storages, I will try to contribute with this this weekend !",
          "created_at": "2025-02-26T21:08:44Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "> > \n> \n> Sure, It would follow the same login for all other storages, I will try to contribute with this this weekend !\n\nI am very grateful to you for this",
          "created_at": "2025-02-27T02:22:38Z"
        }
      ]
    },
    {
      "issue_number": 979,
      "title": "[Bug]: <insert_custom_kg ->query>",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nTypeError: openai_embed() got an unexpected keyword argument 'hashing_kv'\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "YuanQiNeng",
      "author_type": "User",
      "created_at": "2025-03-03T12:01:58Z",
      "updated_at": "2025-03-13T11:01:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/979/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/979",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/979",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:08.447951",
      "comments": [
        {
          "author": "reqyou",
          "body": "same here\n\n`TypeError(\"llm_model_func() got an unexpected keyword argument 'hashing_kv'\")`",
          "created_at": "2025-03-13T10:04:00Z"
        }
      ]
    },
    {
      "issue_number": 1081,
      "title": "[Bug]: <title>failed to delete doc by id: some functions are not implimented",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI used the git main branch with MongoKVStorage, MongoDocStatusStorage, QdrantVectorDBStorage and Neo4JStorage.\nand the demo code as below:\n```\n        try:\n            # 检查文档是否存在\n            doc_status = await rag.doc_status.get_by_id(doc_id)\n            if not doc_status:\n                raise HTTPException(\n                    status_code=404,\n                    detail=f\"Document with ID '{doc_id}' not found\"\n                )\n\n            # 删除文档及其相关数据\n            await rag.adelete_by_doc_id(doc_id)\n            \n            return InsertResponse(\n                status=\"success\",\n                message=f\"Document '{doc_id}' and its related data deleted successfully\"\n            )\n        except Exception as e:\n            logger.error(f\"Error DELETE /documents/{doc_id}: {str(e)}\")\n            logger.error(traceback.format_exc())\n```\n\nAnd I got these error:\n\n```\nheai_kb    | Traceback (most recent call last):                                                                                                                                                                                                                             \nheai_kb    |   File \"<frozen runpy>\", line 198, in _run_module_as_main\nheai_kb    |   File \"<frozen runpy>\", line 88, in _run_code                                                                                                                                                                                                                 \nheai_kb    |   File \"/app/lightrag/api/lightrag_server.py\", line 600, in <module>                                                                                                                                                                                           \nheai_kb    |     main()                                                                                                                                                                                                                                                     \nheai_kb    |   File \"/app/lightrag/api/lightrag_server.py\", line 577, in main                                                                                                                                                                                               \nheai_kb    |     app = create_app(args)\nheai_kb    |           ^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                     \nheai_kb    |   File \"/app/lightrag/api/lightrag_server.py\", line 314, in create_app                                                                                                                                                                                         \nheai_kb    |     rag = LightRAG(                                                                                                                                                                                                                                            \nheai_kb    |           ^^^^^^^^^\nheai_kb    |   File \"<string>\", line 37, in __init__                                                                                                                                                                                                                        \nheai_kb    |   File \"/app/lightrag/lightrag.py\", line 387, in __post_init__                                                                                                                                                                                                 \nheai_kb    |     self.entities_vdb: BaseVectorStorage = self.vector_db_storage_cls(  # type: ignore                                                                                                                                                                         \nheai_kb    |                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                         \nheai_kb    |   File \"/app/lightrag/utils.py\", line 890, in import_class\nheai_kb    |     return cls(*args, **kwargs)                                                                                                                                                                                                                                \nheai_kb    |            ^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                \nheai_kb    | TypeError: Can't instantiate abstract class QdrantVectorDBStorage with abstract methods get_by_id, get_by_ids   \n```\nWhen I review the code, I find that  \"QdrantVectorDBStorage .get_by_id\", \"QdrantVectorDBStorage .get_by_ids\" ,  \"QdrantVectorDBStorage.client_storage\", \"MongoKVStorage.get_all\", \"MongoKVStorage.delete\" and \"MongoDocStatusStorage.delete\"  are not implemented. So the function \"adelete_by_doc_id\" is completely unusable.\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "KangCaijun",
      "author_type": "User",
      "created_at": "2025-03-13T10:06:18Z",
      "updated_at": "2025-03-13T10:06:18Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1081/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1081",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1081",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:08.638942",
      "comments": []
    },
    {
      "issue_number": 1071,
      "title": "What is the best storage solution currently available?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n1. There is a need for multiple knowledge bases. The Neo4j Community Edition currently cannot create new databases. Does it support multiple knowledge databases?\n2. PostgreSQL does not support namespace differentiation.  https://github.com/HKUDS/LightRAG/issues/814\n3. The performance of the JSON storage method is too poor. So, what is the best storage solution currently available that supports multiple knowledge bases?\n Thank you for your reply.\n\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "ZhuLinsen",
      "author_type": "User",
      "created_at": "2025-03-12T12:13:32Z",
      "updated_at": "2025-03-13T01:34:23Z",
      "closed_at": "2025-03-13T01:34:23Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1071/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1071",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1071",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:08.638966",
      "comments": []
    },
    {
      "issue_number": 1065,
      "title": "[Bug]: <title>LightRAG-API Server did not answer with user's query",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nAfter starting `lightrag-server` with .env config file (filled with appropiate environment variables)\nServer running without error and restarting and available on:\n\nhttp://localhost:9621/webui\n\n- Start upload document in tab `Document` and \n- and view entire documents' graph at tab `Knowledge Graph` sucessfull\n- Error occur with `Retrieval` tab, server response to every user's query with the following errror:\n\n```\n500 Internal Server Error\n\"{\"detail\":\"NanoVectorDBStorage.query() got an unexpected keyword argument 'ids'\"}\"\n/query/stream\n```\n\n```\n[ERROR][2025-03-12 13:40:52] Traceback (most recent call last):\n  File \"/Users/richardma/code/run-lightrag/LightRAG/lightrag/api/routers/query_routes.py\", line 193, in query_text_stream\n    response = await rag.aquery(request.query, param=param)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/richardma/code/run-lightrag/LightRAG/lightrag/lightrag.py\", line 1253, in aquery\n    response = await kg_query(\n               ^^^^^^^^^^^^^^^\n  File \"/Users/richardma/code/run-lightrag/LightRAG/lightrag/operate.py\", line 714, in kg_query\n    context = await _build_query_context(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/richardma/code/run-lightrag/LightRAG/lightrag/operate.py\", line 1111, in _build_query_context\n    ll_data, hl_data = await asyncio.gather(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/richardma/code/run-lightrag/LightRAG/lightrag/operate.py\", line 1178, in _get_node_data\n    results = await entities_vdb.query(\n                    ^^^^^^^^^^^^^^^^^^^\nTypeError: NanoVectorDBStorage.query() got an unexpected keyword argument 'ids'\n```\n\nHow to workaround this issues?\n\nThank you in advanced.\nRichard.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "homeidjsc",
      "author_type": "User",
      "created_at": "2025-03-12T06:56:38Z",
      "updated_at": "2025-03-12T14:55:15Z",
      "closed_at": "2025-03-12T14:55:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1065/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1065",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1065",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:08.638974",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "This bug is fixed.  Pls pull the newest repo from main branch.",
          "created_at": "2025-03-12T14:55:05Z"
        }
      ]
    },
    {
      "issue_number": 912,
      "title": "[Feature Request]: Health check for Neo4j PostgreSQL and Qdrant and other databases",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nI noticed that the neo4j starts slower than the api_server, however the api server tried to connect at the very first, and of course it can't. So it's necessary to implement some sort of \"Health Check\" feature for the backend\n\n### Additional Context\n\nmy logs:\n\nDEBUG:[#0000]  _: <POOL> trying to hand out new connection\nDEBUG:[#0000]  _: <RESOLVE> in: neo4j:7687\nDEBUG:[#0000]  _: <RESOLVE> dns resolver out: 172.19.0.3:7687\nDEBUG:[#0000]  C: <OPEN> 172.19.0.3:7687\nDEBUG:[#0000]  S: <ERROR> ConnectionRefusedError 111 'Connection refused'\nDEBUG:[#0000]  C: <CLOSE> 172.19.0.3:7687\nDEBUG:[#0000]  S: <CONNECTION FAILED> 172.19.0.3:7687 ServiceUnavailable: Failed to establish connection to ResolvedIPv4Address(('172.19.0.3', 7687)) (reason [Errno 111] Connection refused)\nERROR:Lightragchunk-entity-relation at bolt://neo4j:7687 is not available\nDEBUG:[#0000]  _: <POOL> close\nTraceback (most recent call last):\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py\", line 409, in _connect_secure\n    s.connect(resolved_address)\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_bolt_socket.py\", line 328, in connect\n    s = cls._connect_secure(\n        ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py\", line 426, in _connect_secure\n    raise ServiceUnavailable(\nneo4j.exceptions.ServiceUnavailable: Failed to establish connection to ResolvedIPv4Address(('172.19.0.3', 7687)) (reason [Errno 111] Connection refused)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/app/lightrag/api/lightrag_server.py\", line 1802, in <module>\n    main()\n  File \"/app/lightrag/api/lightrag_server.py\", line 1784, in main\n    app = create_app(args)\n          ^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/api/lightrag_server.py\", line 1101, in create_app\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 34, in __init__\n  File \"/app/lightrag/lightrag.py\", line 504, in __post_init__\n    self.chunk_entity_relation_graph: BaseGraphStorage = self.graph_storage_cls(  # type: ignore\n                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 200, in import_class\n    return cls(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/kg/neo4j_impl.py\", line 109, in __init__\n    raise e\n  File \"/app/lightrag/kg/neo4j_impl.py\", line 102, in __init__\n    session.run(\"MATCH (n) RETURN n LIMIT 0\")\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/work/session.py\", line 313, in run\n    self._connect(self._config.default_access_mode)\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/work/session.py\", line 136, in _connect\n    super()._connect(\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py\", line 199, in _connect\n    self._connection = self._pool.acquire(**acquire_kwargs_)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 662, in acquire\n    return self._acquire(\n           ^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 408, in _acquire\n    return connection_creator()\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 230, in connection_creator\n    connection = self.opener(\n                 ^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 624, in opener\n    return Bolt.open(\n           ^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py\", line 369, in open\n    s, protocol_version, handshake, data = BoltSocket.connect(\n                                           ^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_bolt_socket.py\", line 376, in connect\n    raise ServiceUnavailable(\nneo4j.exceptions.ServiceUnavailable: Couldn't connect to neo4j:7687 (resolved to ('172.19.0.3:7687',)):\nFailed to establish connection to ResolvedIPv4Address(('172.19.0.3', 7687)) (reason [Errno 111] Connection refused)\nINFO:Logger initialized for working directory: /app/rag_storage\nDEBUG:LightRAG init with param:\n  working_dir = /app/rag_storage,\n  embedding_cache_config = {'enabled': True, 'similarity_threshold': 0.95, 'use_llm_check': False},\n  kv_storage = PGKVStorage,\n  vector_storage = QdrantVectorDBStorage,\n  graph_storage = Neo4JStorage,\n  doc_status_storage = PGDocStatusStorage,\n  log_level = DEBUG,\n  log_dir = /app,\n  chunk_token_size = 1200,\n  chunk_overlap_token_size = 100,\n  tiktoken_model_name = gpt-4o-mini,\n  entity_extract_max_gleaning = 1,\n  entity_summary_to_max_tokens = 8192,\n  node_embedding_algorithm = node2vec,\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\n  embedding_func = {'embedding_dim': 1024, 'max_token_size': 8192, 'func': <function create_app.<locals>.<lambda> at 0x7f86138d1d00>},\n  embedding_batch_num = 32,\n  embedding_func_max_async = 16,\n  llm_model_func = <function create_app.<locals>.openai_alike_model_complete at 0x7f861416e0c0>,\n  llm_model_name = deepseek-r1-distill-qwen:32b,\n  llm_model_max_token_size = 65536,\n  llm_model_max_async = 8,\n  llm_model_kwargs = {'timeout': 500},\n  vector_db_storage_cls_kwargs = {'cosine_better_than_threshold': 0.2},\n  namespace_prefix = lightrag,\n  enable_llm_cache = True,\n  enable_llm_cache_for_entity_extract = False,\n  addon_params = {},\n  auto_manage_storages_states = False,\n  convert_response_to_json_func = <function convert_response_to_json at 0x7f8615ad3600>,\n  chunking_func = <function chunking_by_token_size at 0x7f8614a32ca0>\n\nDEBUG:[#0000]  _: <POOL> created, direct address IPv4Address(('neo4j', 7687))\nDEBUG:[#0000]  _: <POOL> created, direct address IPv4Address(('neo4j', 7687))\nDEBUG:[#0000]  _: <WORKSPACE> routing towards fixed database: lightragchunk-entity-relation\nDEBUG:[#0000]  _: <WORKSPACE> pinning database: 'lightragchunk-entity-relation'\nDEBUG:[#0000]  _: <POOL> acquire direct connection, access_mode='WRITE', database=AcquisitionDatabase(name='lightragchunk-entity-relation', guessed=False)\nDEBUG:[#0000]  _: <POOL> trying to hand out new connection\nDEBUG:[#0000]  _: <RESOLVE> in: neo4j:7687\nDEBUG:[#0000]  _: <RESOLVE> dns resolver out: 172.19.0.3:7687\nDEBUG:[#0000]  C: <OPEN> 172.19.0.3:7687\nDEBUG:[#0000]  S: <ERROR> ConnectionRefusedError 111 'Connection refused'\nDEBUG:[#0000]  C: <CLOSE> 172.19.0.3:7687\nDEBUG:[#0000]  S: <CONNECTION FAILED> 172.19.0.3:7687 ServiceUnavailable: Failed to establish connection to ResolvedIPv4Address(('172.19.0.3', 7687)) (reason [Errno 111] Connection refused)\nERROR:Lightragchunk-entity-relation at bolt://neo4j:7687 is not available\nDEBUG:[#0000]  _: <POOL> close\nTraceback (most recent call last):\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py\", line 409, in _connect_secure\n    s.connect(resolved_address)\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_bolt_socket.py\", line 328, in connect\n    s = cls._connect_secure(\n        ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py\", line 426, in _connect_secure\n    raise ServiceUnavailable(\nneo4j.exceptions.ServiceUnavailable: Failed to establish connection to ResolvedIPv4Address(('172.19.0.3', 7687)) (reason [Errno 111] Connection refused)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/app/lightrag/api/lightrag_server.py\", line 1802, in <module>\n    main()\n  File \"/app/lightrag/api/lightrag_server.py\", line 1784, in main\n    app = create_app(args)\n          ^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/api/lightrag_server.py\", line 1101, in create_app\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 34, in __init__\n  File \"/app/lightrag/lightrag.py\", line 504, in __post_init__\n    self.chunk_entity_relation_graph: BaseGraphStorage = self.graph_storage_cls(  # type: ignore\n                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 200, in import_class\n    return cls(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/kg/neo4j_impl.py\", line 109, in __init__\n    raise e\n  File \"/app/lightrag/kg/neo4j_impl.py\", line 102, in __init__\n    session.run(\"MATCH (n) RETURN n LIMIT 0\")\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/work/session.py\", line 313, in run\n    self._connect(self._config.default_access_mode)\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/work/session.py\", line 136, in _connect\n    super()._connect(\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/work/workspace.py\", line 199, in _connect\n    self._connection = self._pool.acquire(**acquire_kwargs_)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 662, in acquire\n    return self._acquire(\n           ^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 408, in _acquire\n    return connection_creator()\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 230, in connection_creator\n    connection = self.opener(\n                 ^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_pool.py\", line 624, in opener\n    return Bolt.open(\n           ^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py\", line 369, in open\n    s, protocol_version, handshake, data = BoltSocket.connect(\n                                           ^^^^^^^^^^^^^^^^^^^\n  File \"/root/.local/lib/python3.11/site-packages/neo4j/_sync/io/_bolt_socket.py\", line 376, in connect\n    raise ServiceUnavailable(\nneo4j.exceptions.ServiceUnavailable: Couldn't connect to neo4j:7687 (resolved to ('172.19.0.3:7687',)):\nFailed to establish connection to ResolvedIPv4Address(('172.19.0.3', 7687)) (reason [Errno 111] Connection refused)\nINFO:Logger initialized for working directory: /app/rag_storage\n",
      "state": "open",
      "author": "someone132s",
      "author_type": "User",
      "created_at": "2025-02-21T12:15:18Z",
      "updated_at": "2025-03-12T08:53:55Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/912/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/912",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/912",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:08.817662",
      "comments": [
        {
          "author": "ridiculers",
          "body": "same error here, even i ran it in local.",
          "created_at": "2025-03-12T08:53:54Z"
        }
      ]
    },
    {
      "issue_number": 1059,
      "title": "[Bug]: NanoVectorDBStorage.query() got an unexpected keyword argument 'ids'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n(lightrag) ➜  examples git:(main) python lightrag_ollama_demo.py\nINFO: Process 17180 Shared-Data created for Single Process\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens/vdb_entities.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens/vdb_relationships.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens/vdb_chunks.json'} 0 data\nINFO: Process 17180 initialized updated flags for namespace: [full_docs]\nINFO: Process 17180 ready to initialize storage namespace: [full_docs]\nINFO: Process 17180 initialized updated flags for namespace: [text_chunks]\nINFO: Process 17180 ready to initialize storage namespace: [text_chunks]\nINFO: Process 17180 initialized updated flags for namespace: [entities]\nINFO: Process 17180 initialized updated flags for namespace: [relationships]\nINFO: Process 17180 initialized updated flags for namespace: [chunks]\nINFO: Process 17180 initialized updated flags for namespace: [chunk_entity_relation]\nINFO: Process 17180 initialized updated flags for namespace: [llm_response_cache]\nINFO: Process 17180 ready to initialize storage namespace: [llm_response_cache]\nINFO: Process 17180 initialized updated flags for namespace: [doc_status]\nINFO: Process 17180 ready to initialize storage namespace: [doc_status]\nINFO: Process 17180 storage namespace already initialized: [full_docs]\nINFO: Process 17180 storage namespace already initialized: [text_chunks]\nINFO: Process 17180 storage namespace already initialized: [llm_response_cache]\nINFO: Process 17180 storage namespace already initialized: [doc_status]\nINFO: Process 17180 Pipeline namespace initialized\nFailed to process document doc-addb4618e1697da0445ec72a648e1f92: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n\nNaive Search:\nTraceback (most recent call last):\n  File \"/workspace/Deepseek-R1-Chat-14B/LightRAG/examples/lightrag_ollama_demo.py\", line 102, in <module>\n    main()\n  File \"/workspace/Deepseek-R1-Chat-14B/LightRAG/examples/lightrag_ollama_demo.py\", line 63, in main\n    rag.query(\n  File \"/workspace/Deepseek-R1-Chat-14B/LightRAG/lightrag/lightrag.py\", line 1233, in query\n    return loop.run_until_complete(self.aquery(query, param, system_prompt))  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n    return f.result()\n           ^^^^^^^^^^\n  File \"/root/.pyenv/versions/3.11.1/lib/python3.11/asyncio/futures.py\", line 203, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/root/.pyenv/versions/3.11.1/lib/python3.11/asyncio/tasks.py\", line 267, in __step\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"/workspace/Deepseek-R1-Chat-14B/LightRAG/lightrag/lightrag.py\", line 1265, in aquery\n    response = await naive_query(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/workspace/Deepseek-R1-Chat-14B/LightRAG/lightrag/operate.py\", line 1686, in naive_query\n    results = await chunks_vdb.query(\n                    ^^^^^^^^^^^^^^^^^\nTypeError: NanoVectorDBStorage.query() got an unexpected keyword argument 'ids'\n\n### Steps to reproduce\n\npython lightrag_ollama_demo.py\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your casync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"deepseek-r1:14b\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",\n            \"options\": {\"num_ctx\": 32768},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embed(\n                texts, embed_model=\"bge-m3:latest\", host=\"http://localhost:11434\"\n            ),\n        ),\n    )onfig here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:Python 3.11.1\n- Related Issues:\n",
      "state": "closed",
      "author": "RickNote",
      "author_type": "User",
      "created_at": "2025-03-11T13:35:18Z",
      "updated_at": "2025-03-12T04:00:09Z",
      "closed_at": "2025-03-12T04:00:09Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1059/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1059",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1059",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:08.984497",
      "comments": [
        {
          "author": "RickNote",
          "body": "when I use python lightrag_api_openai_compatible_demo.py, and then \n\n(lightrag) ➜  examples git:(main) ✗ curl -X POST \"http://0.0.0.0:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"What are the top themes in this story?\", \"mode\": \"hybrid\"}'\n{\"detail\":\"NanoVectorDBStorage.query() got ",
          "created_at": "2025-03-11T14:15:15Z"
        },
        {
          "author": "ArindamRoy23",
          "body": "Please pull and test from this pull req(https://github.com/HKUDS/LightRAG/pull/1060), should fix. ",
          "created_at": "2025-03-11T15:28:12Z"
        },
        {
          "author": "RickNote",
          "body": "> Please pull and test from this pull req([#1060](https://github.com/HKUDS/LightRAG/pull/1060)), should fix.请从该拉取请求（ [#1060](https://github.com/HKUDS/LightRAG/pull/1060) ）拉取并测试，应可修复。\n\nVery sorry, I don't know how to do this. I'm a newer in Computer Science.",
          "created_at": "2025-03-12T00:28:37Z"
        },
        {
          "author": "danielaskdd",
          "body": "#1060 is merged. Thx @ArindamRoy23's quick response.",
          "created_at": "2025-03-12T03:58:23Z"
        }
      ]
    },
    {
      "issue_number": 941,
      "title": "[Question]: <title>模型提取实体和关系的质量还行，但最终构建的图谱质量很差",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n使用v1.1.1版本构建本地知识库，出现模型提取实体和关系的质量还行，但最终构建的图谱质量很差的情况，大部分实体和关系都遗漏了。\n数据：我采用已经处理好的中药精简数据，例如中药：南沙参，具有的功效：养阴、清肺、益胃、生津、化痰、益气，主治疾病：肺热、燥咳、阴虚、劳嗽、干咳、痰黏、胃阴不足、食少、呕吐、气阴不足、烦热、口干。\n模型：ollama的qwen2.5：7B\n描述：以明党参和南沙参为例，再cache中都正常提取了，但明党参没有其它的边，儿南沙参却有。而且大部分实体和关系都遗漏了\n图谱的截图：![Image](https://github.com/user-attachments/assets/b0565f06-4533-4fdd-8581-50a2be1bf0dd)\n选取的部分response_cache:\n{\"29ef6dd199c0102bd90be88c476fe5bf\": {\n      \"return\": \"(\\\"entity\\\"<|>\\\"明党参\\\"<|>\\\"中药\\\"<|>\\\"明党参是一种具有多种功效的传统中药。\\\")\\n(\\\"entity\\\"<|>\\\"润肺\\\"<|>\\\"功效\\\"<|>\\\"润肺是指缓解肺部干燥，减轻咳嗽等症状。\\\")\\n(\\\"entity\\\"<|>\\\"化痰\\\"<|>\\\"功效\\\"<|>\\\"化痰是通过药物帮助清除呼吸道中的痰液。\\\")\\n(\\\"entity\\\"<|>\\\"养阴\\\"<|>\\\"功效\\\"<|>\\\"养阴即滋养人体的阴液，常用于治疗阴虚引起的症状。\\\")\\n(\\\"entity\\\"<|>\\\"和胃\\\"<|>\\\"功效\\\"<|>\\\"和胃是指调和脾胃功能，改善消化系统问题。\\\")\\n(\\\"entity\\\"<|>\\\"平肝\\\"<|>\\\"功效\\\"<|>\\\"平肝是通过药物作用达到舒缓肝脏的功效。\\\")\\n(\\\"entity\\\"<|>\\\"解毒\\\"<|>\\\"功效\\\"<|>\\\"解毒是指清除体内的毒素或有害物质。\\\")\\n(\\\"entity\\\"<|>\\\"肺热\\\"<|>\\\"主治\\\"<|>\\\"肺热是一种因内热引起呼吸道症状的疾病状态。\\\")\\n(\\\"entity\\\"<|>\\\"咳嗽\\\"<|>\\\"主治\\\"<|>\\\"咳嗽是呼吸系统常见的症状，表现为呼吸时伴有声音和排痰。\\\")\\n(\\\"entity\\\"<|>\\\"呕吐\\\"<|>\\\"主治\\\"<|>\\\"呕吐是指无法控制地将胃内容物排出体外的症状。\\\")\\n(\\\"entity\\\"<|>\\\"反胃\\\"<|>\\\"主治\\\"<|>\\\"反胃指的是患者感到不适并试图吐出胃内容物但未能成功的情况。\\\")\\n(\\\"entity\\\"<|>\\\"食少\\\"<|>\\\"主治\\\"<|>\\\"食少是食欲不振、进食量减少的状态。\\\")\\n(\\\"entity\\\"<|>\\\"口干\\\"<|>\\\"主治\\\"<|>\\\"口干是指口腔干燥，缺乏唾液分泌的症状。\\\")\\n(\\\"entity\\\"<|>\\\"目赤\\\"<|>\\\"主治\\\"<|>\\\"目赤即眼睛红肿发炎的表现。\\\")\\n(\\\"entity\\\"<|>\\\"眩晕\\\"<|>\\\"主治\\\"<|>\\\"眩晕是头部感觉不稳定或旋转的不适症状。\\\")\\n(\\\"entity\\\"<|>\\\"疔毒\\\"<|>\\\"主治\\\"<|>\\\"疔毒是指皮肤上出现的一种急性感染，通常伴有疼痛和局部红肿。\\\")\\n(\\\"entity\\\"<|>\\\"疮疡\\\"<|>\\\"主治\\\"<|>\\\"疮疡指的是任何类型的皮肤创面或炎症性病变。\\\")\\n\\n(\\\"relationship\\\"<|>\\\"明党参\\\"<|>\\\"润肺\\\"<|>\\\"明党参具有润肺的功效，可用于治疗肺热引起的咳嗽等症状<|>\\\"润肺\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"明党参\\\"<|>\\\"化痰\\\"<|>\\\"明党参能帮助化痰，适合用于缓解由痰液引起的呼吸道症状<|>\\\"化痰\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"明党参\\\"<|>\\\"养阴\\\"<|>\\\"明党参能够滋养人体的阴液，对于阴虚引起的症状有疗效<|>\\\"养阴\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"明党参\\\"<|>\\\"和胃\\\"<|>\\\"明党参可以调和脾胃功能，改善食少等症状<|>\\\"和胃\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"明党参\\\"<|>\\\"平肝\\\"<|>\\\"明党参具有平肝的功效，有助于舒缓肝脏不适<|>\\\"平肝\\\"<|>6)\\n(\\\"relationship\\\"<|>\\\"明党参\\\"<|>\\\"解毒\\\"<|>\\\"明党参能够解毒，对于疔毒、疮疡等皮肤感染有治疗作用<|>\\\"解毒\\\"<|>8)\\n\\n(\\\"content_keywords\\\"<|>\\\"润肺化痰、养阴和胃、平肝解毒\\\"<|COMPLETE|>\",\n      \"embedding\": null,\n      \"embedding_shape\": null,\n      \"embedding_min\": null,\n      \"embedding_max\": null,\n      \"original_prompt\": \"-Goal-\\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\\nUse English or Chinese as output language.\\n\\n-Steps-\\n1. Identify all entities. For each identified entity, extract the following information:\\n- entity_name: Name of the entity, use same language as input text. If English, capitalized the name.\\n- entity_type: One of the following types: [中药,功效,主治]\\n- entity_description: Comprehensive description of the entity's attributes and activities\\nFormat each entity as (\\\"entity\\\"<|><entity_name><|><entity_type><|><entity_description>)\\n\\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\\nFor each pair of related entities, extract the following information:\\n- source_entity: name of the source entity, as identified in step 1\\n- target_entity: name of the target entity, as identified in step 1\\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\\n- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details\\nFormat each relationship as (\\\"relationship\\\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_keywords><|><relationship_strength>)\\n\\n3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.\\nFormat the content-level key words as (\\\"content_keywords\\\"<|><high_level_keywords>)\\n\\n4. Return output in English or Chinese as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\\n\\n5. When finished, output <|COMPLETE|>\\n\\n######################\\n-Examples-\\n######################\\nExample 1:\\n    \\nEntity_types: [中药、功效、主治]\\nText:\\n中药：一枝黄花，具有的功效：清热、解毒、疏散风热，主治疾病：喉痹、乳蛾、咽喉肿痛、疮疖肿毒、风热感冒\\n中药：丁公藤，具有的功效：祛风、除湿、消肿、止痛，主治疾病：风湿痹痛、半身不遂、跌扑肿痛\\n#############\\nOutput：\\n(\\\"entity\\\"<|>\\\"一枝黄花\\\"<|>\\\"中药\\\"<|>一枝黄花是一种中药，具有清热、解毒、疏散风热的功效。)\\n(\\\"entity\\\"<|>\\\"清热\\\"<|>\\\"功效\\\"<|>清热指的是清除体内的热邪，常用于治疗因热引起的病症。)\\n(\\\"entity\\\"<|>\\\"解毒\\\"<|>\\\"功效\\\"<|>解毒是指通过药物作用减轻或消除体内毒素的影响。)\\n(\\\"entity\\\"<|>\\\"疏散风热\\\"<|>\\\"功效\\\"<|>疏散风热是针对外感风热病邪的一种治疗方法，帮助缓解由风热引起的症状。)\\n(\\\"entity\\\"<|>\\\"喉痹\\\"<|>\\\"主治\\\"<|>喉痹是中医术语，指喉咙部位的红肿疼痛等不适症状。)\\n(\\\"entity\\\"<|>\\\"乳蛾\\\"<|>\\\"主治\\\"<|>乳蛾是指发生在咽喉部位的急性炎症，表现为咽喉肿痛等症状。)\\n(\\\"entity\\\"<|>\\\"咽喉肿痛\\\"<|>\\\"主治\\\"<|>咽喉肿痛是指喉咙和咽部出现的肿胀及疼痛现象。)\\n(\\\"entity\\\"<|>\\\"疮疖肿毒\\\"<|>\\\"主治\\\"<|>疮疖肿毒指的是皮肤上出现的感染性病变，包括局部红肿、热痛等症状。)\\n(\\\"entity\\\"<|>\\\"风热感冒\\\"<|>\\\"主治\\\"<|>风热感冒是由外感风热引起的一种常见疾病，表现为发热、头痛、咳嗽等症状。)\\n\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"清热\\\"<|>一枝黄花具有清热的功效，可用于治疗因热引起的疾病<|>\\\"清热\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"解毒\\\"<|>一枝黄花具有解毒的功效，能够对抗体内毒素<|>\\\"解毒\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"疏散风热\\\"<|>一枝黄花能够疏散风热，适合用于治疗风热相关的症状<|>\\\"疏散风热\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"喉痹\\\"<|>一枝黄花对于治疗喉痹有效<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"乳蛾\\\"<|>一枝黄花可以用于治疗乳蛾<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"咽喉肿痛\\\"<|>一枝黄花对咽喉肿痛有疗效<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"疮疖肿毒\\\"<|>一枝黄花适用于疮疖肿毒的治疗<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"风热感冒\\\"<|>一枝黄花可用于治疗风热感冒<|>\\\"治疗\\\"<|>7)\\n\\n(\\\"entity\\\"<|>\\\"丁公藤\\\"<|>\\\"中药\\\"<|>丁公藤是一种传统中药，具有祛风、除湿、消肿、止痛的作用。)\\n(\\\"entity\\\"<|>\\\"祛风\\\"<|>\\\"功效\\\"<|>祛风是指排除体内的风邪，常用于治疗因风邪所致的关节疼痛等症状。)\\n(\\\"entity\\\"<|>\\\"除湿\\\"<|>\\\"功效\\\"<|>除湿指的是去除体内湿气，常用于治疗因湿气重导致的身体不适。)\\n(\\\"entity\\\"<|>\\\"消肿\\\"<|>\\\"功效\\\"<|>消肿是指减少或消除身体某部位的肿胀现象。)\\n(\\\"entity\\\"<|>\\\"止痛\\\"<|>\\\"功效\\\"<|>止痛是指减轻或消除身体上的疼痛感受。)\\n(\\\"entity\\\"<|>\\\"风湿痹痛\\\"<|>\\\"主治\\\"<|>风湿痹痛通常指由于风寒湿三气杂至合而为痹所引起的关节疼痛。)\\n(\\\"entity\\\"<|>\\\"半身不遂\\\"<|>\\\"主治\\\"<|>半身不遂指的是身体一侧的肌肉无力或无法运动的症状。)\\n(\\\"entity\\\"<|>\\\"跌扑肿痛\\\"<|>\\\"主治\\\"<|>跌扑肿痛是指由于跌倒或碰撞导致的身体某部位出现的肿胀和疼痛。)\\n\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"祛风\\\"<|>丁公藤具有祛风的作用，能有效改善因风邪引发的症状<|>\\\"祛风\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"除湿\\\"<|>丁公藤能够去除体内湿气，有助于缓解湿气重引起的不适<|>\\\"除湿\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"消肿\\\"<|>丁公藤有助于减少身体不同部位的肿胀现象<|>\\\"消肿\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"止痛\\\"<|>丁公藤有止痛的效果，能够缓解身体各处的疼痛<|>\\\"止痛\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"风湿痹痛\\\"<|>丁公藤对风湿痹痛有良好的治疗效果<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"半身不遂\\\"<|>丁公藤可用于辅助治疗半身不遂<|>\\\"治疗\\\"<|>6)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"跌扑肿痛\\\"<|>丁公藤对于跌扑后出现的肿痛有一定的疗效<|>\\\"治疗\\\"<|>7)\\n\\n(\\\"content_keywords\\\"<|>\\\"清热解毒、疏散风热、祛风除湿、消肿止痛\\\"<|COMPLETE|>\\n#############################\\n\\n#############################\\n-Real Data-\\n######################\\nEntity_types: 中药,功效,主治\\nText: 中药：明党参，具有的功效：润肺、化痰、养阴、和胃、平肝、解毒，主治疾病：肺热、咳嗽、呕吐、反胃、食少、口干、目赤、眩晕、疔毒、疮疡。\\n######################\\nOutput:\\n\"\n    },\n\"4c9fc158c2a2bee9fd2a441d1600bb42\": {\n      \"return\": \"(\\\"entity\\\"<|>\\\"南沙参\\\"<|>\\\"中药\\\"<|>\\\"南沙参是一种具有多种功效的中药材，常用于治疗肺热、燥咳等症状。\\\")\\n(\\\"entity\\\"<|>\\\"养阴\\\"<|>\\\"功效\\\"<|>\\\"养阴指的是滋养阴液，适用于阴虚导致的各种症状。\\\")\\n(\\\"entity\\\"<|>\\\"清肺\\\"<|>\\\"功效\\\"<|>\\\"清肺是指清除肺部的热邪和毒素，用于缓解因肺热引起的咳嗽等症状。\\\")\\n(\\\"entity\\\"<|>\\\"益胃\\\"<|>\\\"功效\\\"<|>\\\"益胃指的是增强脾胃功能，改善消化不良、食欲不振等情况。\\\")\\n(\\\"entity\\\"<|>\\\"生津\\\"<|>\\\"功效\\\"<|>\\\"生津是指促进体内津液生成和循环，有助于缓解口干、烦热等症状。\\\")\\n(\\\"entity\\\"<|>\\\"化痰\\\"<|>\\\"功效\\\"<|>\\\"化痰指的是帮助稀释和排出体内的痰液，适用于痰多黏稠的情况。\\\")\\n(\\\"entity\\\"<|>\\\"益气\\\"<|>\\\"功效\\\"<|>\\\"益气是指增强体质和免疫力，用于治疗因气虚引起的各种症状。\\\")\\n(\\\"entity\\\"<|>\\\"肺热\\\"<|>\\\"主治\\\"<|>\\\"肺热是中医术语，指肺部出现的发热、咳嗽等症状。\\\")\\n(\\\"entity\\\"<|>\\\"燥咳\\\"<|>\\\"主治\\\"<|>\\\"燥咳指的是由于干燥引起的咳嗽，伴有咽喉干痒等表现。\\\")\\n(\\\"entity\\\"<|>\\\"阴虚\\\"<|>\\\"主治\\\"<|>\\\"阴虚是指体内阴液不足导致的各种症状，如口干、烦热等。\\\")\\n(\\\"entity\\\"<|>\\\"劳嗽\\\"<|>\\\"主治\\\"<|>\\\"劳嗽是因长期劳累引起的咳嗽，伴有气短、乏力等症状。\\\")\\n(\\\"entity\\\"<|>\\\"干咳\\\"<|>\\\"主治\\\"<|>\\\"干咳是指没有痰或少痰的持续性咳嗽。\\\")\\n(\\\"entity\\\"<|>\\\"痰黏\\\"<|>\\\"主治\\\"<|>\\\"痰黏指的是痰液稠厚不易排出的情况。\\\")\\n(\\\"entity\\\"<|>\\\"胃阴不足\\\"<|>\\\"主治\\\"<|>\\\"胃阴不足指脾胃功能减弱，津液不足导致的症状，如食欲不振、呕吐等。\\\")\\n(\\\"entity\\\"<|>\\\"食少\\\"<|>\\\"主治\\\"<|>\\\"食少是消化不良、食欲减退的表现。\\\")\\n(\\\"entity\\\"<|>\\\"呕吐\\\"<|>\\\"主治\\\"<|>\\\"呕吐是指食物或液体无法正常下咽并从胃中排出的症状。\\\")\\n(\\\"entity\\\"<|>\\\"气阴不足\\\"<|>\\\"主治\\\"<|>\\\"气阴不足指的是气血和津液同时不足的状态，常表现为疲乏、烦热等症状。\\\")\\n\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"养阴\\\"<|>\\\"南沙参具有养阴的功效，可以改善因阴虚引起的各种症状<|>\\\"养阴\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"清肺\\\"<|>\\\"南沙参能够清除肺部热邪，用于治疗肺热引起的咳嗽等症状<|>\\\"清肺\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"益胃\\\"<|>\\\"南沙参有助于增强脾胃功能，改善消化不良、食欲不振等情况<|>\\\"益胃\\\"<|>6)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"生津\\\"<|>\\\"南沙参能够促进体内津液生成和循环，缓解口干、烦热等症状<|>\\\"生津\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"化痰\\\"<|>\\\"南沙参会帮助稀释和排出体内的痰液，适用于痰多黏稠的情况<|>\\\"化痰\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"益气\\\"<|>\\\"南沙参会增强体质和免疫力，用于治疗因气虚引起的各种症状<|>\\\"益气\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"肺热\\\"<|>\\\"南沙参可以用于治疗由肺热引起的咳嗽等症状<|>\\\"治疗\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"燥咳\\\"<|>\\\"南沙参会缓解因干燥引起的咳嗽，伴有咽喉干痒等表现<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"阴虚\\\"<|>\\\"南沙参会改善体内阴液不足导致的症状，如口干、烦热等<|>\\\"治疗\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"劳嗽\\\"<|>\\\"南沙参会用于辅助治疗因长期劳累引起的咳嗽等症状<|>\\\"治疗\\\"<|>6)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"干咳\\\"<|>\\\"南沙参会缓解没有痰或少痰的持续性咳嗽<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"痰黏\\\"<|>\\\"南沙参会有助于稀释和排出体内的痰液，改善痰多黏稠的情况<|>\\\"治疗\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"胃阴不足\\\"<|>\\\"南沙参会用于缓解因胃部阴液不足引起的症状如食欲不振、呕吐等<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"食少\\\"<|>\\\"南沙参会改善消化不良、食欲减退的情况<|>\\\"治疗\\\"<|>6)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"呕吐\\\"<|>\\\"南沙参会用于缓解食物或液体无法正常下咽并从胃中排出的症状<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"南沙参\\\"<|>\\\"气阴不足\\\"<|>\\\"南沙参会改善气血和津液同时不足的状态，常表现为疲乏、烦热等症状<|>\\\"治疗\\\"<|>8)\\n\\n(\\\"content_keywords\\\"<|>\\\"养阴清肺、益胃生津、化痰益气\\\"<|COMPLETE|>\",\n      \"embedding\": null,\n      \"embedding_shape\": null,\n      \"embedding_min\": null,\n      \"embedding_max\": null,\n      \"original_prompt\": \"-Goal-\\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\\nUse English or Chinese as output language.\\n\\n-Steps-\\n1. Identify all entities. For each identified entity, extract the following information:\\n- entity_name: Name of the entity, use same language as input text. If English, capitalized the name.\\n- entity_type: One of the following types: [中药,功效,主治]\\n- entity_description: Comprehensive description of the entity's attributes and activities\\nFormat each entity as (\\\"entity\\\"<|><entity_name><|><entity_type><|><entity_description>)\\n\\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\\nFor each pair of related entities, extract the following information:\\n- source_entity: name of the source entity, as identified in step 1\\n- target_entity: name of the target entity, as identified in step 1\\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\\n- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details\\nFormat each relationship as (\\\"relationship\\\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_keywords><|><relationship_strength>)\\n\\n3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.\\nFormat the content-level key words as (\\\"content_keywords\\\"<|><high_level_keywords>)\\n\\n4. Return output in English or Chinese as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\\n\\n5. When finished, output <|COMPLETE|>\\n\\n######################\\n-Examples-\\n######################\\nExample 1:\\n    \\nEntity_types: [中药、功效、主治]\\nText:\\n中药：一枝黄花，具有的功效：清热、解毒、疏散风热，主治疾病：喉痹、乳蛾、咽喉肿痛、疮疖肿毒、风热感冒\\n中药：丁公藤，具有的功效：祛风、除湿、消肿、止痛，主治疾病：风湿痹痛、半身不遂、跌扑肿痛\\n#############\\nOutput：\\n(\\\"entity\\\"<|>\\\"一枝黄花\\\"<|>\\\"中药\\\"<|>一枝黄花是一种中药，具有清热、解毒、疏散风热的功效。)\\n(\\\"entity\\\"<|>\\\"清热\\\"<|>\\\"功效\\\"<|>清热指的是清除体内的热邪，常用于治疗因热引起的病症。)\\n(\\\"entity\\\"<|>\\\"解毒\\\"<|>\\\"功效\\\"<|>解毒是指通过药物作用减轻或消除体内毒素的影响。)\\n(\\\"entity\\\"<|>\\\"疏散风热\\\"<|>\\\"功效\\\"<|>疏散风热是针对外感风热病邪的一种治疗方法，帮助缓解由风热引起的症状。)\\n(\\\"entity\\\"<|>\\\"喉痹\\\"<|>\\\"主治\\\"<|>喉痹是中医术语，指喉咙部位的红肿疼痛等不适症状。)\\n(\\\"entity\\\"<|>\\\"乳蛾\\\"<|>\\\"主治\\\"<|>乳蛾是指发生在咽喉部位的急性炎症，表现为咽喉肿痛等症状。)\\n(\\\"entity\\\"<|>\\\"咽喉肿痛\\\"<|>\\\"主治\\\"<|>咽喉肿痛是指喉咙和咽部出现的肿胀及疼痛现象。)\\n(\\\"entity\\\"<|>\\\"疮疖肿毒\\\"<|>\\\"主治\\\"<|>疮疖肿毒指的是皮肤上出现的感染性病变，包括局部红肿、热痛等症状。)\\n(\\\"entity\\\"<|>\\\"风热感冒\\\"<|>\\\"主治\\\"<|>风热感冒是由外感风热引起的一种常见疾病，表现为发热、头痛、咳嗽等症状。)\\n\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"清热\\\"<|>一枝黄花具有清热的功效，可用于治疗因热引起的疾病<|>\\\"清热\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"解毒\\\"<|>一枝黄花具有解毒的功效，能够对抗体内毒素<|>\\\"解毒\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"疏散风热\\\"<|>一枝黄花能够疏散风热，适合用于治疗风热相关的症状<|>\\\"疏散风热\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"喉痹\\\"<|>一枝黄花对于治疗喉痹有效<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"乳蛾\\\"<|>一枝黄花可以用于治疗乳蛾<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"咽喉肿痛\\\"<|>一枝黄花对咽喉肿痛有疗效<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"疮疖肿毒\\\"<|>一枝黄花适用于疮疖肿毒的治疗<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"一枝黄花\\\"<|>\\\"风热感冒\\\"<|>一枝黄花可用于治疗风热感冒<|>\\\"治疗\\\"<|>7)\\n\\n(\\\"entity\\\"<|>\\\"丁公藤\\\"<|>\\\"中药\\\"<|>丁公藤是一种传统中药，具有祛风、除湿、消肿、止痛的作用。)\\n(\\\"entity\\\"<|>\\\"祛风\\\"<|>\\\"功效\\\"<|>祛风是指排除体内的风邪，常用于治疗因风邪所致的关节疼痛等症状。)\\n(\\\"entity\\\"<|>\\\"除湿\\\"<|>\\\"功效\\\"<|>除湿指的是去除体内湿气，常用于治疗因湿气重导致的身体不适。)\\n(\\\"entity\\\"<|>\\\"消肿\\\"<|>\\\"功效\\\"<|>消肿是指减少或消除身体某部位的肿胀现象。)\\n(\\\"entity\\\"<|>\\\"止痛\\\"<|>\\\"功效\\\"<|>止痛是指减轻或消除身体上的疼痛感受。)\\n(\\\"entity\\\"<|>\\\"风湿痹痛\\\"<|>\\\"主治\\\"<|>风湿痹痛通常指由于风寒湿三气杂至合而为痹所引起的关节疼痛。)\\n(\\\"entity\\\"<|>\\\"半身不遂\\\"<|>\\\"主治\\\"<|>半身不遂指的是身体一侧的肌肉无力或无法运动的症状。)\\n(\\\"entity\\\"<|>\\\"跌扑肿痛\\\"<|>\\\"主治\\\"<|>跌扑肿痛是指由于跌倒或碰撞导致的身体某部位出现的肿胀和疼痛。)\\n\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"祛风\\\"<|>丁公藤具有祛风的作用，能有效改善因风邪引发的症状<|>\\\"祛风\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"除湿\\\"<|>丁公藤能够去除体内湿气，有助于缓解湿气重引起的不适<|>\\\"除湿\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"消肿\\\"<|>丁公藤有助于减少身体不同部位的肿胀现象<|>\\\"消肿\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"止痛\\\"<|>丁公藤有止痛的效果，能够缓解身体各处的疼痛<|>\\\"止痛\\\"<|>8)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"风湿痹痛\\\"<|>丁公藤对风湿痹痛有良好的治疗效果<|>\\\"治疗\\\"<|>7)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"半身不遂\\\"<|>丁公藤可用于辅助治疗半身不遂<|>\\\"治疗\\\"<|>6)\\n(\\\"relationship\\\"<|>\\\"丁公藤\\\"<|>\\\"跌扑肿痛\\\"<|>丁公藤对于跌扑后出现的肿痛有一定的疗效<|>\\\"治疗\\\"<|>7)\\n\\n(\\\"content_keywords\\\"<|>\\\"清热解毒、疏散风热、祛风除湿、消肿止痛\\\"<|COMPLETE|>\\n#############################\\n\\n#############################\\n-Real Data-\\n######################\\nEntity_types: 中药,功效,主治\\nText: 中药：南沙参，具有的功效：养阴、清肺、益胃、生津、化痰、益气，主治疾病：肺热、燥咳、阴虚、劳嗽、干咳、痰黏、胃阴不足、食少、呕吐、气阴不足、烦热、口干。\\n######################\\nOutput:\\n\"\n    }\n}\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "huangjw2001",
      "author_type": "User",
      "created_at": "2025-02-25T03:36:48Z",
      "updated_at": "2025-03-11T10:45:40Z",
      "closed_at": "2025-02-25T06:07:26Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/941/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/941",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/941",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:09.145016",
      "comments": [
        {
          "author": "LarFii",
          "body": "可以添加微信群发我一下具体的内容，因为图谱构建是以LLM输出为准的，如果LLM抽取没问题，图谱构建应该是正常的。",
          "created_at": "2025-02-25T03:58:15Z"
        },
        {
          "author": "LarFii",
          "body": "我看了一下log，是返回格式不对的问题，“Use ## as the list delimiter”，实际上LLM却是用的\\n\n\n![Image](https://github.com/user-attachments/assets/2357f449-900c-48bd-b94e-8d0c80a18cc2)",
          "created_at": "2025-02-25T04:02:35Z"
        },
        {
          "author": "huangjw2001",
          "body": "感谢，我测试了一下，确实是的。",
          "created_at": "2025-02-25T06:07:26Z"
        },
        {
          "author": "yaleimeng",
          "body": "像比较正式的业务问答场合，最好还是自己准备好构建KG的数据（可以借助LLM处理再加人工审核修正），通过自定义KG插入到数据库中。。指望导入文本完全自动化生成KG必然是不能满足需要的，因为根本不受控，噪声无法接受。",
          "created_at": "2025-03-03T05:28:00Z"
        },
        {
          "author": "huangjw2001",
          "body": "嗯好的，感谢您的意见",
          "created_at": "2025-03-03T06:27:30Z"
        }
      ]
    },
    {
      "issue_number": 721,
      "title": "Postgres Query Error - Ollama and Postgres",
      "body": "Traceback (most recent call last):\n  File \"/Users/apple/Documents/Projects/DeepStreet/llm-call/agents/lightRAG.py\", line 114, in <module>\n    asyncio.run(main())\n  File \"/Users/apple/.pyenv/versions/3.10.12/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/Users/apple/.pyenv/versions/3.10.12/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/Users/apple/Documents/Projects/DeepStreet/llm-call/agents/lightRAG.py\", line 80, in main\n    await rag.aquery(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/lightrag.py\", line 919, in aquery\n    response = await kg_query(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/operate.py\", line 618, in kg_query\n    context = await _build_query_context(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/operate.py\", line 997, in _build_query_context\n    ll_data, hl_data = await asyncio.gather(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/operate.py\", line 1059, in _get_node_data\n    results = await entities_vdb.query(query, top_k=query_param.top_k)\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/kg/postgres_impl.py\", line 419, in query\n    results = await self.db.query(sql, params=params, multirows=True)\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/kg/postgres_impl.py\", line 111, in query\n    rows = await connection.fetch(sql, *params.values())\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 690, in fetch\n    return await self._execute(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 1864, in _execute\n    result, _ = await self.__execute(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 1961, in __execute\n    result, stmt = await self._do_execute(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 2004, in _do_execute\n    stmt = await self._get_statement(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 432, in _get_statement\n    statement = await self._protocol.prepare(\n  File \"asyncpg/protocol/protocol.pyx\", line 165, in prepare\nasyncpg.exceptions.PostgresSyntaxError: subquery in FROM must have an alias\nHINT:  For example, FROM (SELECT ...) [AS] foo.",
      "state": "closed",
      "author": "Mcode2020",
      "author_type": "User",
      "created_at": "2025-02-06T09:38:48Z",
      "updated_at": "2025-03-11T09:44:30Z",
      "closed_at": "2025-02-19T20:29:45Z",
      "labels": [
        "bug",
        "PostgreSQL"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/721/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/721",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/721",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:09.342949",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi, Please update with the last version.\n\n",
          "created_at": "2025-02-19T20:29:43Z"
        },
        {
          "author": "brandonbevans",
          "body": "I'm using 1.2.3 and still getting this error.",
          "created_at": "2025-03-01T20:03:22Z"
        },
        {
          "author": "jasperchen01",
          "body": "I'm using the 1.2.5, also have this issue when query in postgresql",
          "created_at": "2025-03-11T09:44:29Z"
        }
      ]
    },
    {
      "issue_number": 1044,
      "title": "[Bug]: Keyword extraction output is not a valid JSON",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI use self-hosted `meta-llama/Llama-3.1-8B-Instruct`, modify`lightrag_api_openai_compatible_demo_simplified` to use the LLM, and run the script on 《The Project Gutenberg eBook of A Christmas Carol》, it will fail on the keyword extraction of the query. \n\nDebug into the code, the LLM output as below for keyword extraction. As you can see, the LLM wrote a code and tried to run it, so it will fail the JSON parser. I believe we need more strict restrictions on output formats.\n\nI fixed it by improving the prompt; I will create a PR. Thanks\n\n```\n\n### Keyword Extraction\\n\\nTo identify high-level and low-level keywords in the query and conversation history, we\\'ll use a combination of natural language processing (NLP) and machine learning techniques.\\n\\n```python\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\nfrom nltk.stem import PorterStemmer\\nfrom collections import Counter\\nimport re\\n\\n# Initialize the NLTK Download\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\n\\ndef extract_keywords(query, conversation_history):\\n    # Tokenize the query and conversation history\\n    query_tokens = word_tokenize(query)\\n    conversation_history_tokens = [word for sent in conversation_history for word in word_tokenize(sent)]\\n\\n    # Combine all tokens and remove stop words\\n    all_tokens = query_tokens + conversation_history_tokens\\n    stop_words = set(stopwords.words(\\'english\\'))\\n    all_tokens = [word for word in all_tokens if word.lower() not in stop_words]\\n\\n    # Stem the tokens\\n    stemmer = PorterStemmer()\\n    all_tokens = [stemmer.stem(word) for word in all_tokens]\\n\\n    # Count the frequency of each token\\n    token_counts = Counter(all_tokens)\\n\\n    # Separate high-level and low-level keywords\\n    high_level_keywords = []\\n    low_level_keywords = []\\n\\n    # Get the top 5 tokens by frequency\\n    top_tokens = token_counts.most_common(5)\\n\\n    # Separate high-level and low-level keywords\\n    for token, count in top_tokens:\\n        if len(token.split()) > 1:\\n            high_level_keywords.append(token)\\n        else:\\n            low_level_keywords.append(token)\\n\\n    return {\\n        \"high_level_keywords\": high_level_keywords,\\n        \"low_level_keywords\": low_level_keywords\\n    }\\n\\nconversation_history = [\\n    \"The story revolves around the theme of love and loss.\",\\n    \"However, the main character\\'s life is also influenced by technology and social media.\",\\n    \"The story tackles the complex issue of mental health and its impact on relationships.\"\\n]\\n\\ncurrent_query = \"What are the top themes in this story?\"\\n\\nresult = extract_keywords(current_query, conversation_history)\\nprint(\"High-level keywords:\", result[\"high_level_keywords\"])\\nprint(\"Low-level keywords:\", result[\"low_level_keywords\"])\\n```\\n\\n**Output:**\\n```\\nHigh-level keywords: [\\'love\\', \\'technology social media\\', \\'mental health relationships\\']\\nLow-level keywords: [\\'life\\', \\'character\\', \\'issue\\']\\n```\\n\\nThis code tokenizes the query and conversation history, removes stop words, stems the tokens, and counts the frequency of each token. It then separates the high-level and low-level keywords based on the number of words in each token. The top 5 tokens by frequency are selected, and the remaining tokens are assigned to either high-level or low-level keywords.\n```\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\nquery keyword extraction success\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "zhenya-zhu",
      "author_type": "User",
      "created_at": "2025-03-11T04:06:28Z",
      "updated_at": "2025-03-11T07:22:20Z",
      "closed_at": "2025-03-11T07:22:20Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1044/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1044",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1044",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:09.499212",
      "comments": [
        {
          "author": "zhenya-zhu",
          "body": "close as PR been merged",
          "created_at": "2025-03-11T07:22:20Z"
        }
      ]
    },
    {
      "issue_number": 962,
      "title": "[Question]: What is better for input, plain text or a data structure such as JSON ?",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n\nSince im parsing highly technical documents with lots of info in tables and graphic figures, im planning to pre-parse my pdfs with doclling and later feed them to lightRAG..\n\nChatGPT suggested that i output everything in JSON and later feed this json structure to ligthRAG.. \n\n\nThe nature of documents im interested in parsing are malware/ransomware/phishing techinical reports, they include a lot of IP's, Hashes, urls, domains, asking chatgpt.. it advises to create a structure like this:\n\n```\n{\n  \"threat_name\": \"XYZ Malware\",\n  \"MITRE_TTPs\": [\"T1566.001\", \"T1071\", \"T1204\", \"T1105\"],\n  \"IoCs\": [\n    {\n      \"type\": \"IP\",\n      \"value\": \"192[.]168[.]1[.]10\",\n      \"defanged\": true,\n      \"MITRE_TTPs\": [\"T1071\", \"T1568.002\"],\n      \"related_threats\": [\"XYZ Malware\"]\n    },\n    {\n      \"type\": \"IP\",\n      \"value\": \"10[.]10[.]10[.]1\",\n      \"defanged\": true,\n      \"MITRE_TTPs\": [\"T1071\"],\n      \"related_threats\": [\"XYZ Malware\"]\n    },\n    {\n      \"type\": \"Domain\",\n      \"value\": \"malicious-site[.]com\",\n      \"defanged\": true,\n      \"MITRE_TTPs\": [\"T1566.001\"],\n      \"related_threats\": [\"XYZ Malware\"]\n    },\n    {\n      \"type\": \"Domain\",\n      \"value\": \"c2-command[.]net\",\n      \"defanged\": true,\n      \"MITRE_TTPs\": [\"T1071\"],\n      \"related_threats\": [\"XYZ Malware\", \"APT29\"]\n    },\n    {\n      \"type\": \"URL\",\n      \"value\": \"hxxps://malicious-site[.]com/download.exe\",\n      \"defanged\": true,\n      \"MITRE_TTPs\": [\"T1105\"],\n      \"related_threats\": [\"XYZ Malware\"]\n    },\n    {\n      \"type\": \"URL\",\n      \"value\": \"hxxp://c2-command[.]net/api\",\n      \"defanged\": true,\n      \"MITRE_TTPs\": [\"T1071\"],\n      \"related_threats\": [\"XYZ Malware\", \"APT29\"]\n    },\n    {\n      \"type\": \"MD5\",\n      \"value\": \"d41d8cd98f00b204e9800998ecf8427e\",\n      \"MITRE_TTPs\": [\"T1204\"],\n      \"related_threats\": [\"XYZ Malware\"]\n    },\n    {\n      \"type\": \"SHA256\",\n      \"value\": \"3a7bd3e2360a43c07ef2b5c0b8b1e77fbbdc4df73262b8c7c833e59a80c2ab6b\",\n      \"MITRE_TTPs\": [\"T1204\", \"T1071\"],\n      \"related_threats\": [\"XYZ Malware\"]\n    }\n  ],\n  \"fragments\": [\n    {\n      \"fragment_id\": \"xyz-1\",\n      \"text\": \"XYZ Malware se propaga principalmente a través de campañas de spearphishing con documentos maliciosos adjuntos. Los correos electrónicos contienen enlaces a hxxps://malicious-site[.]com/download.exe, donde la carga útil se descarga y ejecuta en la máquina de la víctima.\",\n      \"related_IoCs\": [\"malicious-site[.]com\", \"hxxps://malicious-site[.]com/download.exe\"],\n      \"chunk_number\": 1\n    },\n    {\n      \"fragment_id\": \"xyz-2\",\n      \"text\": \"Una vez ejecutado, el malware establece comunicación con su servidor de comando y control (C2) en la IP 192[.]168[.]1[.]10, utilizando el protocolo HTTPS modificado para evitar detección por IDS.\",\n      \"related_IoCs\": [\"192[.]168[.]1[.]10\"],\n      \"chunk_number\": 2\n    },\n    {\n      \"fragment_id\": \"xyz-3\",\n      \"text\": \"El análisis de tráfico de red reveló que XYZ Malware usa el dominio c2-command[.]net para enviar comandos encriptados a los dispositivos comprometidos.\",\n      \"related_IoCs\": [\"c2-command[.]net\"],\n      \"chunk_number\": 3\n    },\n    {\n      \"fragment_id\": \"xyz-4\",\n      \"text\": \"Se identificó una variante del malware que se instala a través de un ejecutable descargado desde hxxp://c2-command[.]net/api. Esta versión incluye funcionalidades adicionales para el robo de credenciales.\",\n      \"related_IoCs\": [\"hxxp://c2-command[.]net/api\"],\n      \"chunk_number\": 4\n    },\n    {\n      \"fragment_id\": \"xyz-5\",\n      \"text\": \"XYZ Malware utiliza una técnica de evasión basada en hashes MD5 cambiantes. Un hash identificado en la última campaña es d41d8cd98f00b204e9800998ecf8427e.\",\n      \"related_IoCs\": [\"d41d8cd98f00b204e9800998ecf8427e\"],\n      \"chunk_number\": 5\n    },\n    {\n      \"fragment_id\": \"xyz-6\",\n      \"text\": \"Durante su ejecución, XYZ Malware descarga módulos adicionales de un servidor remoto. Se ha identificado un SHA256 asociado: 3a7bd3e2360a43c07ef2b5c0b8b1e77fbbdc4df73262b8c7c833e59a80c2ab6b.\",\n      \"related_IoCs\": [\"3a7bd3e2360a43c07ef2b5c0b8b1e77fbbdc4df73262b8c7c833e59a80c2ab6b\"],\n      \"chunk_number\": 6\n    }\n  ]\n}\n```\n\nWhere the information its on a json data structure, the indicatores of compromise are separeated(tagged?), content its fragmented and the related iocs to each fragment is stated... \n\nIs this way of feeding data to lighRAG beneficial in any way, compared to just plain text ? does LightRAG recognizes this json structure ?\n\nThanks for your insights..\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "bzImage",
      "author_type": "User",
      "created_at": "2025-02-27T19:08:49Z",
      "updated_at": "2025-03-10T19:34:04Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/962/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/962",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/962",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:11.390527",
      "comments": [
        {
          "author": "JoramMillenaar",
          "body": "It depends on your LLM that you'll use for LightRag, but it's safe to say that you don't need to structure your data (e.g. converting it to JSON). I would say that's one of the main pros of LightRAG; it turns unstructured data into a structured knowledge graph.\n\nWhat happens is that the (unstructure",
          "created_at": "2025-03-03T16:43:37Z"
        },
        {
          "author": "bzImage",
          "body": "according to #584 .. lightRAG don't support json..\n\n",
          "created_at": "2025-03-06T17:52:19Z"
        },
        {
          "author": "bzImage",
          "body": "> It depends on your LLM that you'll use for LightRag, but it's safe to say that you don't need to structure your data (e.g. converting it to JSON). I would say that's one of the main pros of LightRAG; it turns unstructured data into a structured knowledge graph.\n\nit makes mistakes.. treats all urls",
          "created_at": "2025-03-06T23:08:55Z"
        },
        {
          "author": "ccq1",
          "body": "> > It depends on your LLM that you'll use for LightRag, but it's safe to say that you don't need to structure your data (e.g. converting it to JSON). I would say that's one of the main pros of LightRAG; it turns unstructured data into a structured knowledge graph.这取决于你将用于 LightRag 的LLM，但可以肯定的是，你不需要",
          "created_at": "2025-03-10T19:33:46Z"
        }
      ]
    },
    {
      "issue_number": 1041,
      "title": "[Question]: RAG Query response cache and document management used in search",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI have a couple of questions about implementing cache and document management used in search.\n\nI am using source code without API and WEB UI.\n\nI have uploaded some files, for example 1.txt, 2.txt.\nIn 1.txt I have several objects listed, for example:\n\n```\n1.txt\n\nReal estate aсttributes:\nHouse \nApartment \nRoom\n```\n\n\nI ask the question \n```\nrag.query(\n            “List existing real estate properties”.\n            param=QueryParam(mode=“hybrid”, stream=False),\n        )\n```\n\nrag answers me with “There are 3 entities, House, Apartment, Room”.\n\nI see that there is a caching mechanism for llm responses. \nBut if I modify the file 1.txt by deleting 1 entity, e.g. Room, the next time I query rag.query(...) I will get data from the cache which will be irrelevant.\n\nHence the following questions:\n1. Is there \\planned a mechanism to update the llm response cache based on changes to existing files?\n2. Is there\\planned to retrieve information about which file the llm response is ultimately generated from for future use, such as user navigation to that file?\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "mszhigachev",
      "author_type": "User",
      "created_at": "2025-03-10T14:29:04Z",
      "updated_at": "2025-03-10T15:52:02Z",
      "closed_at": "2025-03-10T15:52:02Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1041/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1041",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1041",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:11.598811",
      "comments": [
        {
          "author": "LarFii",
          "body": "Currently, `clear_cache()` function is provided to clear the cache.\n![Image](https://github.com/user-attachments/assets/d410ca13-8d46-4af1-8ceb-ab5a610390ce)",
          "created_at": "2025-03-10T15:02:14Z"
        }
      ]
    },
    {
      "issue_number": 988,
      "title": "[Question]: <title>ERROR:lightrag:Failed to process document doc-edc02c05d65545a55a3247efed2f1cbd: Error code: 400",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nERROR:lightrag:Failed to process document doc-edc02c05d65545a55a3247efed2f1cbd: Error code: 400 - {'error': {'code': 'InvalidParameter', 'param': None, 'message': '<400> InternalError.Algo.InvalidParameter: Value error, batch size is invalid, it should not be larger than 10.: input.contents', 'type': 'InvalidParameter'}, 'id': 'a2197ed6-d34f-9517-a0e6-c6610f2fe9f3', 'request_id': 'a2197ed6-d34f-9517-a0e6-c6610f2fe9f3'}\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "fengshaoA",
      "author_type": "User",
      "created_at": "2025-03-04T05:56:37Z",
      "updated_at": "2025-03-10T02:39:42Z",
      "closed_at": "2025-03-10T02:39:41Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/988/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/988",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/988",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:11.881597",
      "comments": [
        {
          "author": "yizhangliu",
          "body": "Me too. Why?",
          "created_at": "2025-03-09T15:54:56Z"
        },
        {
          "author": "fengshaoA",
          "body": "> Me too. Why?\n\nThe parameter embedding_batch_num in lightRAG has a default value of 32. You need to adjust the value of embedding_batch_num based on the \"Batch size for embedding computations\" required by the embedding model you are using. For instance, if you are using the text-embedding-v3 model,",
          "created_at": "2025-03-10T02:39:41Z"
        }
      ]
    },
    {
      "issue_number": 1038,
      "title": "[Question]: Regarding LightRAG and PathRAG",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am new to RAG.  I am just starting to explore using LightRAG.  I've also heard about PathRAG which is supposed to be faster, and address some limitation with LightRAG.  Information on PathRAG:\n\n1. Research paper: https://arxiv.org/html/2502.14902v1 \n2. Github repo: https://github.com/BUPT-GAMMA/PathRAG\n\nI think LightRAG is more mature, has more traction / contributors.  Is there any plan to review PathRAG and make those improvements to LightRAG?\n\nThank you!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "khaidoan",
      "author_type": "User",
      "created_at": "2025-03-09T20:49:56Z",
      "updated_at": "2025-03-09T20:49:56Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1038/reactions",
        "total_count": 17,
        "+1": 12,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 2,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1038",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1038",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:17.109118",
      "comments": []
    },
    {
      "issue_number": 1037,
      "title": "[Question]: Regarding the lightrag_ollama_age_demo.py example and the lightrag_zhipu_postgres_demo.py example",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am new to LightRAG.  I am looking inside the example folder.  It has the lightrag_ollama_age_demo.py file, and the lightrag_zhipu_postgres_demo.py file.  I am exploring both of these.  They both creates the chunk_entity_relation graph.  However, the lightrag_zhipu_postgres_demo.py file also creates the following tables: lightrag_doc_chunks, lightrag_doc_full, lightrag_doc_status, lightrag_llm_cache, lightrag_vdb_entity, lightrag_vdb_relation.  When I tried the lightrag_ollama_age_demo.py example, I also saw a lot of errors, something like \"too many connections\".  My questions:\n\n1. Does the lightrag_ollama_age_demo.py also create similar tables?\n2. Beside the above error, which example should I follow?  Which one provide more benefits?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "khaidoan",
      "author_type": "User",
      "created_at": "2025-03-09T20:36:39Z",
      "updated_at": "2025-03-09T20:36:39Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1037/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1037",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1037",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:17.109145",
      "comments": []
    },
    {
      "issue_number": 1035,
      "title": "[Question]: <title>system_prompt自定义提示词",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n我的自定义提示词得不到正确的结果\n\n![Image](https://github.com/user-attachments/assets/1617bb8e-40fa-46e9-9584-096fd77549af)\n\n![Image](https://github.com/user-attachments/assets/12ba57d7-ff65-4698-9cc3-9a888b806df2)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "JiaoYuJiaoYu",
      "author_type": "User",
      "created_at": "2025-03-09T14:30:47Z",
      "updated_at": "2025-03-09T14:30:47Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1035/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1035",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1035",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:17.109153",
      "comments": []
    },
    {
      "issue_number": 921,
      "title": "[Feature Request]: Add progress display function to ollama_demo",
      "body": "### Feature Request Description\n\n**I found that the recent new version does not display the percentage progress when running [example/lightrag_ollama_demo.py](https://github.com/HKUDS/LightRAG/blob/main/examples/lightrag_ollama_demo.py), which has caused me a lot of trouble in monitoring the overall progress. I hope it can display the percentage progress like the previous version.**\n————————————————————————————————————\n**_Here is the running output of lightrag_ollama_demo.py in the latest version 1.2.0:_**\n\nINFO:Logger initialized for working directory: ./AAA_Test/output/ITA/DS_70_Deng_P4\nINFO:Load KV llm_response_cache with 0 data\nINFO:Load KV full_docs with 0 data\nINFO:Load KV text_chunks with 0 data\nINFO:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './AAA_Test/output/ITA/DS_70_Deng_P4/vdb_entities.json'} 0 data\nINFO:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './AAA_Test/output/ITA/DS_70_Deng_P4/vdb_relationships.json'} 0 data\nINFO:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './AAA_Test/output/ITA/DS_70_Deng_P4/vdb_chunks.json'} 0 data\nINFO:Loaded document status storage with 0 records\nINFO:Inserting 1 to doc_status\nINFO:Stored 1 new unique documents\nINFO:Number of batches to process: 1.\nINFO:Start processing batch 1 of 1.\nINFO:Inserting 1 to doc_status\nINFO:Inserting 18 to chunks\nINFO:Inserting 1 to full_docs\nINFO:Inserting 18 to text_chunks\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Inserting 1 to llm_response_cache\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Inserting 1 to llm_response_cache\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Inserting 1 to llm_response_cache\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Inserting 1 to llm_response_cache\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Inserting 1 to llm_response_cache\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Inserting 1 to llm_response_cache\nINFO:Non-embedding cached missed(mode:default type:extract)\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "FloretKu",
      "author_type": "User",
      "created_at": "2025-02-22T05:15:50Z",
      "updated_at": "2025-03-09T08:53:12Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/921/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/921",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/921",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:17.110653",
      "comments": [
        {
          "author": "shoaibshaikh07",
          "body": "Did you found the solution?",
          "created_at": "2025-03-09T08:53:12Z"
        }
      ]
    },
    {
      "issue_number": 1019,
      "title": "[Question]: Logs for rag.insert()",
      "body": "### Do you need to ask a question?\n\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n- [x] I have searched the existing question and discussions and this question is not already answered.\n\n### Your Question\n\nHow to see the realtime logs of rag.insert() process?\n\n### Additional Context\n\nSo, I'm trying to insert a **1,044** characters document.\n\nOnly after a few minutes i can see in my embeddings API dashboard that **126,833** tokens were used, it's been 10 minutes since and idk why it's still not finished (not any response) when the embeddings are done and still my GPU is utilizing 90-100%.\n\nI want to know what exactly is happening realtime by logs.\n\nModel: Ollama Llama 3.2 3B (modified to 32768 context parameters)\nGPU: Nvidia L4 (24G)",
      "state": "closed",
      "author": "shoaibshaikh07",
      "author_type": "User",
      "created_at": "2025-03-07T07:25:49Z",
      "updated_at": "2025-03-09T05:40:47Z",
      "closed_at": "2025-03-09T05:40:46Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1019/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1019",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1019",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:17.282997",
      "comments": [
        {
          "author": "LarFii",
          "body": "You can set logger as follows:\n```python\nfrom lightrag.utils import setup_logger\n\nsetup_logger(\"lightrag\", level=\"INFO\")\n```",
          "created_at": "2025-03-08T18:14:50Z"
        },
        {
          "author": "shoaibshaikh07",
          "body": "Thank you.",
          "created_at": "2025-03-09T05:40:46Z"
        }
      ]
    },
    {
      "issue_number": 1020,
      "title": "[Bug]: <title> Remove your api keys from the file </title>",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nRemove your deepseek api keys from the files:\nexamples/lightrag_openai_neo4j_milvus_redis_demo.py\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "bhaswata08",
      "author_type": "User",
      "created_at": "2025-03-07T07:28:21Z",
      "updated_at": "2025-03-08T18:12:58Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1020/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1020",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1020",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:17.516038",
      "comments": [
        {
          "author": "shoaibshaikh07",
          "body": "Nice, you reporting after using all the credits: {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}} 😳",
          "created_at": "2025-03-07T07:33:07Z"
        },
        {
          "author": "LarFii",
          "body": "Thanks for the reminder! It seems that one of the contributors forgot to delete it.",
          "created_at": "2025-03-08T18:12:57Z"
        }
      ]
    },
    {
      "issue_number": 930,
      "title": "run with ollama throw errors?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n```\nINFO:lightrag:Query edges: Summit, Conference, Speaker, top_k: 60, cosine: 0.2\nTraceback (most recent call last):\n  File \"/data/LightRAG/test.py\", line 23, in <module>\n    rag.query(\n  File \"/data/LightRAG/lightrag/lightrag.py\", line 982, in query\n    return loop.run_until_complete(self.aquery(query, param, system_prompt))  # type: ignore\n  File \"/root/miniconda3/envs/lightrag/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/data/LightRAG/lightrag/lightrag.py\", line 1002, in aquery\n    response = await kg_query(\n  File \"/data/LightRAG/lightrag/operate.py\", line 623, in kg_query\n    context = await _build_query_context(\n  File \"/data/LightRAG/lightrag/operate.py\", line 1008, in _build_query_context\n    entities_context, relations_context, text_units_context = await _get_edge_data(\n  File \"/data/LightRAG/lightrag/operate.py\", line 1320, in _get_edge_data\n    results = await relationships_vdb.query(keywords, top_k=query_param.top_k)\n  File \"/data/LightRAG/lightrag/kg/nano_vector_db_impl.py\", line 85, in query\n    results = self._client.query(\n  File \"/root/miniconda3/envs/lightrag/lib/python3.10/site-packages/nano_vectordb/dbs.py\", line 155, in query\n    return self.usable_metrics[self.metric](\n  File \"/root/miniconda3/envs/lightrag/lib/python3.10/site-packages/nano_vectordb/dbs.py\", line 179, in _cosine_query\n    scores = np.dot(use_matrix, query)\nValueError: shapes (0,1024) and (768,) not aligned: 1024 (dim 1) != 768 (dim 0)\n```\n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "lddlww",
      "author_type": "User",
      "created_at": "2025-02-24T04:11:32Z",
      "updated_at": "2025-03-08T15:37:29Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/930/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/930",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/930",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:17.768923",
      "comments": [
        {
          "author": "LarFii",
          "body": "This issue is caused by a mismatch between the embedding model dimensions used during querying and insertion.",
          "created_at": "2025-02-25T04:11:06Z"
        },
        {
          "author": "RushiChaganti",
          "body": "@lddlww  Try this \nrag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"mistral_ctx:latest\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434",
          "created_at": "2025-03-08T15:37:28Z"
        }
      ]
    },
    {
      "issue_number": 1023,
      "title": "[Bug]: <title> graph \"chunk_entity_relation\" already exists when using 'graph_storage=\"AGEStorage\"'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI am testing lightrag with PG16+PGVector+ ApacheAGE\n\n# PG+PGVector+AGE version\n\nhere below are info from PG16:\nwayahead@ubox:/var/log/postgresql$ psql --dbname=lightagedb --username=postgres --password\npsql (16.8 (Ubuntu 16.8-0ubuntu0.24.04.1))\n\nlightagedb=# \\dx\n                             List of installed extensions\n  Name   | Version |   Schema   |                     Description\n---------+---------+------------+------------------------------------------------------\n age     | 1.5.0   | ag_catalog | AGE database extension\n plpgsql | 1.0     | pg_catalog | PL/pgSQL procedural language\n vector  | 0.6.0   | ag_catalog | vector data type and ivfflat and hnsw access methods\n(3 rows)\n\n# LightRAG Version\n\nI use lightrag with below version:\ncommit 27ab894d00f03c2186fe69a128760d7c082e5f12 (HEAD -> main, origin/main, origin/HEAD)\nMerge: 3286a0d e822f35\nAuthor: Yannick Stephan <stephan.yannick@me.com>\nDate:   Fri Mar 7 21:26:43 2025 +0100\n\nThis bug exists from 1.2.3.\n\n# Code to use LightRAG\n\n## 1. Vector+AGE\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"deepseek-r1:8b\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",\n            \"options\": {\"num_ctx\": 32768},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embed(\n                texts,\n                embed_model=\"nomic-embed-text\",\n                host=\"http://localhost:11434\"\n            ),\n        ),\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        vector_storage=\"PGVectorStorage\",\n        graph_storage=\"AGEStorage\",\n        auto_manage_storages_states=False,\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n## 2. AGE\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"deepseek-r1:8b\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\n            \"host\": \"http://localhost:11434\",\n            \"options\": {\"num_ctx\": 32768},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embed(\n                texts,\n                embed_model=\"nomic-embed-text\",\n                host=\"http://localhost:11434\"\n            ),\n        ),\n        graph_storage=\"AGEStorage\",\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\n# Logs from example:\n\nTraceback (most recent call last):\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/kg/age_impl.py\", line 343, in _query\n    await curs.execute(wrapped_query)\n  File \"/Users/wayahead/workspace/rag/venv/lightrag/lib/python3.12/site-packages/psycopg/cursor_async.py\", line 97, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.InternalError_: unhandled cypher(cstring) function call\nDETAIL:  chunk_entity_relation\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/wayahead/workspace/rag/lightrag/tests/ollama_pg/ollama_pg_age_vec.py\", line 142, in <module>\n    asyncio.run(main())\n  File \"/Users/wayahead/workspace/rag/venv/lightrag/lib/python3.12/site-packages/nest_asyncio.py\", line 30, in run\n    return loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/venv/lightrag/lib/python3.12/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n    return f.result()\n           ^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 202, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n    result = coro.throw(exc)\n             ^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/lightrag/tests/ollama_pg/ollama_pg_age_vec.py\", line 111, in main\n    await rag.aquery(\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/lightrag.py\", line 1255, in aquery\n    response = await kg_query(\n               ^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/operate.py\", line 644, in kg_query\n    context = await _build_query_context(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/operate.py\", line 1021, in _build_query_context\n    entities_context, relations_context, text_units_context = await _get_node_data(\n                                                              ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/operate.py\", line 1125, in _get_node_data\n    use_text_units, use_relations = await asyncio.gather(\n                                    ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n    future.result()\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n    result = coro.throw(exc)\n             ^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/operate.py\", line 1305, in _find_most_related_edges_from_entities\n    all_related_edges = await asyncio.gather(\n                        ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n    future.result()\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/kg/age_impl.py\", line 496, in get_node_edges\n    results = await self._query(query, **params)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wayahead/workspace/rag/lightrag/LightRAG/lightrag/kg/age_impl.py\", line 347, in _query\n    raise AGEQueryException(\nlightrag.kg.age_impl.AGEQueryException: {'message': 'Error executing graph query: \\n                MATCH (n:`x5472616e73666f726d6174696f6e`)\\n                OPTIONAL MATCH (n)-[r]-(connected)\\n                RETURN n, r, connected\\n                ', 'detail': 'unhandled cypher(cstring) function call\\nDETAIL:  chunk_entity_relation'}\n\n\n# Logs from PG16\n\nHere below are logs from PG16:\n\n2025-03-08 02:39:52.221 UTC [2018] postgres@lightagedb ERROR:  graph \"chunk_entity_relation\" already exists\n2025-03-08 02:39:52.221 UTC [2018] postgres@lightagedb STATEMENT:  SELECT create_graph('chunk_entity_relation')\n2025-03-08 02:39:52.222 UTC [2037] postgres@lightagedb ERROR:  graph \"chunk_entity_relation\" already exists\n2025-03-08 02:39:52.222 UTC [2037] postgres@lightagedb STATEMENT:  SELECT create_graph('chunk_entity_relation')\n2025-03-08 02:39:52.222 UTC [2062] postgres@lightagedb ERROR:  unhandled cypher(cstring) function call\n2025-03-08 02:39:52.222 UTC [2062] postgres@lightagedb DETAIL:  chunk_entity_relation\n2025-03-08 02:39:52.222 UTC [2062] postgres@lightagedb STATEMENT:  SELECT * FROM ag_catalog.cypher('chunk_entity_relation', $$\n\n                        MATCH (n:`x5472616e73666f726d6174696f6e`)\n                        OPTIONAL MATCH (n)-[r]-(connected)\n                        RETURN n, r, connected\n\n                $$) AS (n agtype, r agtype, connected agtype);\n2025-03-08 02:39:52.223 UTC [2063] postgres@lightagedb ERROR:  unhandled cypher(cstring) function call\n2025-03-08 02:39:52.223 UTC [2063] postgres@lightagedb DETAIL:  chunk_entity_relation\n2025-03-08 02:39:52.223 UTC [2063] postgres@lightagedb STATEMENT:  SELECT * FROM ag_catalog.cypher('chunk_entity_relation', $$\n\n                        MATCH (n:`x416c692042616261`)\n                        OPTIONAL MATCH (n)-[r]-(connected)\n                        RETURN n, r, connected\n\n                $$) AS (n agtype, r agtype, connected agtype);\n2025-03-08 02:39:52.224 UTC [2067] postgres@lightagedb ERROR:  unhandled cypher(cstring) function call\n2025-03-08 02:39:52.224 UTC [2067] postgres@lightagedb DETAIL:  chunk_entity_relation\n2025-03-08 02:39:52.224 UTC [2067] postgres@lightagedb STATEMENT:  SELECT * FROM ag_catalog.cypher('chunk_entity_relation', $$\n\n                        MATCH (n:`x5468652067726f7570206f6620776f6d656e`)\n                        OPTIONAL MATCH (n)-[r]-(connected)\n                        RETURN n, r, connected\n\n                $$) AS (n agtype, r agtype, connected agtype);\n2025-03-08 02:39:52.225 UTC [2055] postgres@lightagedb ERROR:  graph \"chunk_entity_relation\" already exists\n2025-03-08 02:39:52.225 UTC [2055] postgres@lightagedb STATEMENT:  SELECT create_graph('chunk_entity_relation')\n2025-03-08 02:39:52.227 UTC [2035] postgres@lightagedb ERROR:  graph \"chunk_entity_relation\" already exists\n2025-03-08 02:39:52.227 UTC [2035] postgres@lightagedb STATEMENT:  SELECT create_graph('chunk_entity_relation')\n2025-03-08 02:39:52.229 UTC [2056] postgres@lightagedb ERROR:  unhandled cypher(cstring) function call\n2025-03-08 02:39:52.229 UTC [2056] postgres@lightagedb DETAIL:  chunk_entity_relation\n2025-03-08 02:39:52.229 UTC [2056] postgres@lightagedb STATEMENT:  SELECT * FROM ag_catalog.cypher('chunk_entity_relation', $$\n\n                        MATCH (n:`x4562656e657a6572`)\n                        OPTIONAL MATCH (n)-[r]-(connected)\n                        RETURN n, r, connected\n\n                $$) AS (n agtype, r agtype, connected agtype);\n2025-03-08 02:39:52.230 UTC [2028] postgres@lightagedb ERROR:  graph \"chunk_entity_relation\" already exists\n2025-03-08 02:39:52.230 UTC [2028] postgres@lightagedb STATEMENT:  SELECT create_graph('chunk_entity_relation')\n2025-03-08 02:39:52.319 UTC [1376] postgres@lightragdb LOG:  could not receive data from client: Connection reset by peer\n2025-03-08 02:39:59.239 UTC [2041] postgres@lightagedb LOG:  could not send data to client: Broken pipe\n2025-03-08 02:39:59.239 UTC [2041] postgres@lightagedb STATEMENT:  SELECT * FROM ag_catalog.cypher('chunk_entity_relation', $$\n\n                        MATCH (n:`x426f62204372617463686974`)\n                        OPTIONAL MATCH (n)-[r]-(connected)\n                        RETURN n, r, connected\n\n                $$) AS (n agtype, r agtype, connected agtype);\n2025-03-08 02:39:59.239 UTC [2041] postgres@lightagedb FATAL:  connection to client lost\n2025-03-08 02:39:59.239 UTC [2041] postgres@lightagedb STATEMENT:  SELECT * FROM ag_catalog.cypher('chunk_entity_relation', $$\n\n                        MATCH (n:`x426f62204372617463686974`)\n                        OPTIONAL MATCH (n)-[r]-(connected)\n                        RETURN n, r, connected\n\n                $$) AS (n agtype, r agtype, connected agtype);\n2025-03-08 02:41:44.112 UTC [829] LOG:  checkpoint complete: wrote 1304 buffers (8.0%); 0 WAL file(s) added, 0 removed, 1 recycled; write=134.987 s, sync=0.087 s, total=135.084 s; sync files=1029, longest=0.003 s, average=0.001 s; distance=7808 kB, estimate=7808 kB; lsn=0/957CF80, redo lsn=0/957CF48\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "wayahead",
      "author_type": "User",
      "created_at": "2025-03-08T03:08:59Z",
      "updated_at": "2025-03-08T03:08:59Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1023/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1023",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1023",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.045914",
      "comments": []
    },
    {
      "issue_number": 949,
      "title": "[Question]: Getting import error  \"from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\"",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nGetting following error on the line \nfrom lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\n\nError is :  from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\nImportError: cannot import name 'gpt_4o_mini_complete' from 'lightrag.llm'\n\nInstalled lightrag both from pypy as well as git clone option. This is something very basic , not happening for any other library of langchain, llamaindex. And this error coming for any model I try to import.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "shubhamoyleo",
      "author_type": "User",
      "created_at": "2025-02-25T19:15:30Z",
      "updated_at": "2025-03-07T18:38:40Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/949/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/949",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/949",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.045937",
      "comments": [
        {
          "author": "JotaDeRodriguez",
          "body": "`from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed\n`\n\nThis worked for me. \nHope this helps!",
          "created_at": "2025-03-07T18:38:39Z"
        }
      ]
    },
    {
      "issue_number": 1017,
      "title": "API Server Not Starting - Investigate pkg_resources.DistributionNotFound: async-timeout",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI am encountering significant issues installing the LightRAG API Server from source, following the instructions in the README.md. I consistently receive a pkg_resources.DistributionNotFound: The 'async-timeout<6.0,>=4.0; python_version < \"3.11\"' distribution was not found and is required by aiohttp error, preventing the server from starting.\n\nProblem Description:\n\nI am attempting to install LightRAG API server from source using the command provided in the README:\n\n`pip install -e \".[api]\"`\n\nInitially, the installation failed with a \"Multiple top-level packages discovered in a flat-layout\" error. This seemed to be related to the project structure not being properly recognized as a package. I noted that the cloned repository in the lightrag directory did not contain a pyproject.toml file initially.\n\nAfter creating a pyproject.toml file (with [tool.setuptools.packages.find] section to explicitly define the lightrag package location, as suggested in online resources for similar issues), the \"flat-layout\" error was resolved, and pip install -e \".[api]\" appeared to proceed further without immediate errors during the initial installation phase.\n\nHowever, when attempting to start the server using lightrag-server, I consistently encounter the following traceback:\n\n`Traceback (most recent call last):\n  File \"/home/anushkas/.local/bin/lightrag-server\", line 6, in <module>\n    from pkg_resources import load_entry_point\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 3253, in <module>\n    @_call_aside\n     ^^^^^^^^^^^\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 3237, in _call_aside\n    f(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 3266, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 584, in _build_master\n    ws.require(__requires__)\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 901, in require\n    needed = self.resolve(parse_requirements(requirements))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 787, in resolve\n    raise DistributionNotFound(req, requirers)\npkg_resources.DistributionNotFound: The 'async-timeout<6.0,>=4.0; python_version < \"3.11\"' distribution was not found and is required by aiohttp`\n\n\nThis error persists despite numerous troubleshooting steps.\n\nAttempted Solutions (Unsuccessful):\n\nI have tried the following steps in a dedicated Python virtual environment (venv) to resolve this issue, but none have been successful in starting the lightrag-server:\n\n- **Created multiple fresh virtual environments:** Including creating a new virtual environment with python -m venv new_venv and activating it.\n\n- Ensured __init__.py exists in the lightrag directory.\n\n- Created pyproject.toml in the repository root with a [tool.setuptools.packages.find] section to guide package discovery.\n\n- Upgraded pip to the latest version within the virtual environment: pip install --upgrade pip.\n\n- Reinstalled aiohttp (multiple times) with --no-cache-dir --force-reinstall: pip install --no-cache-dir --force-reinstall aiohttp.\n\n- Reinstalled async-timeout (multiple times) with --no-cache-dir --force-reinstall:\n       pip install --no-cache-dir --force-reinstall async-timeout \n       pip install --no-cache-dir --force-reinstall \"async-timeout<6.0,>=4.0; python_version < '3.11'\"\n      pip install --no-cache-dir --force-reinstall async-timeout==5.0.0 (downgrading to a specific version).\n\n- Reinstalled setuptools and wheel with --no-cache-dir --force-reinstall: pip install --no-cache-dir --force-reinstall setuptools wheel.\n\n- Created a virtual environment with --system-site-packages: python -m venv new_venv --system-site-packages.\n\n- Attempted to reinstall pkg-resources directly: pip install --no-cache-dir --force-reinstall pkg-resources, which failed with \"ERROR: Could not find a version that satisfies the requirement pkg-resources (from versions: none) ERROR: No matching distribution found for pkg-resources\".\n\n- Tried installing LightRAG from PyPI (non-editable): pip install lightrag-hku[api].\n\nDespite all these attempts, the pkg_resources.DistributionNotFound: async-timeout error persists when running lightrag-server.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "anusonawane",
      "author_type": "User",
      "created_at": "2025-03-07T06:25:21Z",
      "updated_at": "2025-03-07T06:25:27Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1017/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1017",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1017",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.303521",
      "comments": []
    },
    {
      "issue_number": 865,
      "title": "INFO:lightrag:Non-embedding cached missed(mode:default type:extract)",
      "body": "while using hugging face models for embedding",
      "state": "open",
      "author": "syash-acog",
      "author_type": "User",
      "created_at": "2025-02-19T11:28:20Z",
      "updated_at": "2025-03-06T13:41:55Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 19,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/865/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/865",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/865",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.303541",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi,\nCould you please have a look to the documentation.\n\nThanks.",
          "created_at": "2025-02-19T11:29:38Z"
        },
        {
          "author": "syash-acog",
          "body": "yes, following instruction from documentation\n\nmain.py \n\nfrom lightrag import LightRAG\nfrom lightrag.llm.hf import hf_model_complete, hf_embed\nfrom transformers import AutoModel, AutoTokenizer\nfrom lightrag.utils import EmbeddingFunc\n#\nEMBEDDING_MODEL = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-m",
          "created_at": "2025-02-19T11:37:04Z"
        },
        {
          "author": "cjdfree",
          "body": "**Similar problem, I used the Siliconcloud demo. And my code is as follows. **\n\nload_dotenv()\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) ",
          "created_at": "2025-02-19T12:45:01Z"
        },
        {
          "author": "yedunnn",
          "body": "What does this sentence mean for \"Non-embedding cached missed(mode:default type:extract)\"\n",
          "created_at": "2025-02-20T03:15:38Z"
        },
        {
          "author": "syash-acog",
          "body": "this issue is also getting for opanai model",
          "created_at": "2025-02-20T09:25:12Z"
        }
      ]
    },
    {
      "issue_number": 851,
      "title": "Ollama taking too long for 10 lines of content",
      "body": "i'm trying out examples/lightrag_ollama_demo.py\n\nTaking more than 5 hours to process 10 lines of content.\n\nINFO:Logger initialized for working directory: ./dickens\nINFO:Load KV llm_response_cache with 0 data\nINFO:Load KV full_docs with 0 data\nINFO:Load KV text_chunks with 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data\nINFO:Loaded document status storage with 3 records\nINFO:Stored 1 new unique documents\nINFO:Number of batches to process: 1.\nINFO:Inserting 6 vectors to chunks\nGenerating embeddings: 100%|███████████████████████████████████████████| 1/1 [00:08<00:00,  8.33s/batch]INFO:Non-embedding cached missed(mode:default type:extract)                     | 0/6 [00:00<?, ?chunk/s] \nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nINFO:Non-embedding cached missed(mode:default type:extract)\nGenerating embeddings: 100%|███████████████████████████████████████████| 1/1 [00:08<00:00,  8.57s/batch] \n",
      "state": "open",
      "author": "ghost",
      "author_type": "User",
      "created_at": "2025-02-19T06:42:27Z",
      "updated_at": "2025-03-06T07:38:57Z",
      "closed_at": null,
      "labels": [
        "question",
        "ollama"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/851/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/851",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/851",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.490263",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi Could please provide more details on the versions ? Thanks.\n\nDo you use Ollama on your machine ?",
          "created_at": "2025-02-19T07:37:17Z"
        },
        {
          "author": "ghost",
          "body": "ollama mode = llama3.2 and qwen2\nembed_model=\"nomic-embed-text\"\nbook.txt = 10 lines of content\n\n\nCode:\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"llama3.2\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kw",
          "created_at": "2025-02-19T14:44:57Z"
        },
        {
          "author": "YanSte",
          "body": "Thanks for sharing.\n\nIf Ollama is running on your machine, it is because you do not have the computer, it is normal that it takes time.\n\nCould you please try with OpenAI for example.\n\nThanks.",
          "created_at": "2025-02-19T18:49:21Z"
        },
        {
          "author": "LarFii",
          "body": "I'm not entirely sure about the number of parameters in the model being used and the machine configuration, but based on our test results, using a small model with Ollama does indeed tend to be very slow.",
          "created_at": "2025-02-20T03:03:45Z"
        },
        {
          "author": "ghost",
          "body": "My goal is to develop a chatbot based on the local data. During testing with a 260KB document, the LLaMA 3.2 model taking over a day to generate the graph, still in process. I can't use an openAI - gpt model. \n\nCould you clarify,  \"Is LightRAG lagging in performance when used with Ollama models ?  o",
          "created_at": "2025-02-20T15:39:44Z"
        }
      ]
    },
    {
      "issue_number": 1013,
      "title": "End-to-End Guide for Attaching Backend to LightRAG Web UI with Example",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello,\nThank you for developing LightRAG! I am currently exploring its capabilities and trying to integrate the backend with the web UI. While the documentation provides great insights into running the API server and configuring backends, it would be extremely helpful to have a detailed, step-by-step guide for connecting the backend to the web UI.\n\nSpecifically, I am looking for:\n\n**Backend Setup:**\n\n- Instructions for installing and running the backend server (e.g., using FastAPI).\n- \n- Configuration of environment variables and API keys (e.g., LLM_BINDING_HOST, EMBEDDING_BINDING_HOST, etc.).\n\n**Web UI Integration:**\n\n- Steps to configure the web UI to communicate with the backend API endpoints.\n- \n- Example code snippets for making API calls from the web UI (e.g., using fetch or Axios).\n\n**End-to-End Example:**\n\n- A practical example showing how a query is processed by the backend and displayed on the web UI.\n- \n- Troubleshooting tips for common issues like CORS errors or misconfigured endpoints.\n\nHaving this guide would make it easier for new users to understand and implement LightRAG in their projects.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "anusonawane",
      "author_type": "User",
      "created_at": "2025-03-06T06:56:59Z",
      "updated_at": "2025-03-06T07:00:53Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1013/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1013",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1013",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.697552",
      "comments": []
    },
    {
      "issue_number": 963,
      "title": "[Bug]: <title>函数`_find_most_related_text_unit_from_entities`存在问题",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n`operate.py`中的函数`_find_most_related_text_unit_from_entities`存在问题\n```python\nall_text_units_lookup = {}\n    tasks = []\n    for index, (this_text_units, this_edges) in enumerate(zip(text_units, edges)):\n        for c_id in this_text_units:\n            if c_id not in all_text_units_lookup:\n                tasks.append((c_id, index, this_edges))\n```\n该函数的这段代码中，`all_text_units_lookup`没有起到任何作用。\n`if c_id not in all_text_units_lookup:`这行判断始终为`True`。\n我想它原本的作用应该是避免索引大的`text_units`覆盖掉索引更小的也就是相似度更高的`text_units`\n这导致在后续的这段代码中，相似度更低的实体对应的`chunk`反而会留在`all_text_units_lookup`中\n```python\nfor (c_id, index, this_edges), data in zip(tasks, results):\n\tall_text_units_lookup[c_id] = {\n\t\t\"data\": data,\n\t\t\"order\": index,\n\t\t\"relation_counts\": 0,\n\t}\n```\n而在函数`_find_related_text_unit_from_relationships`中，类似的功能是正常的。以下在该函数下的实现类似功能的代码则不会存在相似度高的`chunk`被覆盖的问题，因为在`fetch_chunk_data`的`if c_id not in all_text_units_lookup`判断是有效的\n```python\nall_text_units_lookup = {}\n\nasync def fetch_chunk_data(c_id, index):\n\tif c_id not in all_text_units_lookup:\n\t\tchunk_data = await text_chunks_db.get_by_id(c_id)\n\t\t# Only store valid data\n\t\tif chunk_data is not None and \"content\" in chunk_data:\n\t\t\tall_text_units_lookup[c_id] = {\n\t\t\t\t\"data\": chunk_data,\n\t\t\t\t\"order\": index,\n\t\t\t}\n\ntasks = []\nfor index, unit_list in enumerate(text_units):\n\tfor c_id in unit_list:\n\t\ttasks.append(fetch_chunk_data(c_id, index))\n\nawait asyncio.gather(*tasks)\n```\n\n另外在`lightrag\\kg\\neo4j_impl.py`中的`get_edge`函数中存在以下代码\n```python\n async with self._driver.session(database=self._DATABASE) as session:\n\tquery = f\"\"\"\n\tMATCH (start:`{entity_name_label_source}`)-[r]->(end:`{entity_name_label_target}`)\n\tRETURN properties(r) as edge_properties\n\tLIMIT 1\n\t\"\"\".format(\n\t\tentity_name_label_source=entity_name_label_source,\n\t\tentity_name_label_target=entity_name_label_target,\n\t)\n```\n这里同时使用了`f-string`和`.format()`。\n而在我先前利用lightrag的框架提取的实体中，大模型提取了一个实体名为`主人公}`的实体，这导致如果关键词匹配到该实体，则代码运行到此处时会报错\n```\nValueError: Single '}' encountered in format string\n```\n建议去除`.format`。不知道是否有其他代码也有类似的错误，请检查以下。\n希望我对代码的理解没有问题。\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "etoileYue",
      "author_type": "User",
      "created_at": "2025-02-28T03:19:55Z",
      "updated_at": "2025-03-06T01:09:24Z",
      "closed_at": "2025-03-06T01:09:24Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/963/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "LarFii"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/963",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/963",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.697572",
      "comments": [
        {
          "author": "LarFii",
          "body": "你觉得修改成这样合理吗\n```pyhton\nall_text_units_lookup = {}\ntasks = []\n\nfor index, (this_text_units, this_edges) in enumerate(zip(text_units, edges)):\n    for c_id in this_text_units:\n        if c_id not in all_text_units_lookup:\n            all_text_units_lookup[c_id] = index\n            tasks.append((c_id, i",
          "created_at": "2025-03-01T08:14:58Z"
        },
        {
          "author": "etoileYue",
          "body": "我觉得没有问题",
          "created_at": "2025-03-01T14:40:20Z"
        }
      ]
    },
    {
      "issue_number": 1010,
      "title": "[Bug]: User input Document ID not being updated in PG database",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen I provide custom document IDs to the RAG insert, it still goes ahead and creates its own document ID for the lightrag_doc_full store and the related tables.  \n\n### Steps to reproduce\n\n```\nrag.insert(payload.content, payload.id)\n\n##PG table lightrag_doc_full will create its own document id on completion\n```\n\n### Expected Behavior\n\n```\nrag.insert(payload.content, payload.id)\n\n##PG table lightrag_doc_full will match document id on completion\n```\n\n### LightRAG Config Used\n\nLightRAG(\n        working_dir=working_dir,\n        llm_model_func=azure_openai_complete,\n        embedding_func=embedding_func,\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"Neo4JStorage\",\n        vector_storage=\"PGVectorStorage\",\n        vector_db_storage_cls_kwargs={\n            \"cosine_better_than_threshold\": 0.7\n        }\n    )\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/4a6481fd-7859-41ef-8427-1d84799508c0)\n\n### Additional Information\n\n- LightRAG Version: 1.2.3\n- Operating System: Ubuntu\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "closed",
      "author": "ArindamRoy23",
      "author_type": "User",
      "created_at": "2025-03-05T18:04:36Z",
      "updated_at": "2025-03-05T20:09:56Z",
      "closed_at": "2025-03-05T20:09:55Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1010/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1010",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1010",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:18.889315",
      "comments": [
        {
          "author": "ArindamRoy23",
          "body": "<img width=\"960\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/effbb2c2-1c26-4a82-94d7-c1bdb44f53fe\" />",
          "created_at": "2025-03-05T18:05:36Z"
        },
        {
          "author": "PiochU19",
          "body": "Hi @ArindamRoy23, I've seen your comment under my PR.\n\nAs per `README.md`:\n```py\n# Insert single text, and provide ID for it\nrag.insert(\"TEXT1\", ids=[\"ID_FOR_TEXT1\"])\n```\nCan you try it this way?\n```py\nrag.insert(payload.content, ids=[payload.id])\n```\nYou are passing `payload.id` as second param to ",
          "created_at": "2025-03-05T18:18:06Z"
        },
        {
          "author": "ArindamRoy23",
          "body": "Hello! It worked! Thanks a ton for helping for these stupid errors! ",
          "created_at": "2025-03-05T20:09:55Z"
        }
      ]
    },
    {
      "issue_number": 985,
      "title": "[Bug]: Document Deletion Issues",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n## Describe the bug\nDocument deletion through API and WebUI is not working properly:\n   - DELETE `/documents` endpoint only clears memory references but not database data\n   - WebUI document cleanup operation fails to delete documents\n   - No support for deleting individual documents by ID\n\n\n### Steps to reproduce\n\n### Document Deletion Issues:\n1. Through API:\n   ```powershell\n   Invoke-WebRequest -Method DELETE -Uri \"http://host:port/documents\"\n   ```\n   - Check documents via GET `/documents`\n   - Documents still exist in database\n\n2. Through WebUI:\n   - Navigate to documents management page\n   - Click \"Clear All Documents\" button\n   - Refresh page\n   - Documents still present in the list\n\n3. Single Document:\n   - No option available to delete individual document\n   - No API endpoint for single document deletion\n\n\n\n### Expected Behavior\n\n### Document Management:\n1. DELETE `/documents` API should:\n   - Clear all data from memory\n   - Delete all records from databases\n   - Remove all related vectors and relationships\n\n2. WebUI document cleanup should:\n   - Successfully delete all documents\n   - Clear UI display\n   - Show success confirmation\n\n3. Single document deletion should:\n   - Allow deleting specific documents by ID\n   - Remove all related data\n   - Update UI immediately\n\n\n\n### LightRAG Config Used\n\n- OS: Windows 10\n- Database Configuration:\n  ```ini\n  [postgres]\n  host = 192.168.1.99\n  port = 5432\n  database = lightrag\n\n  [neo4j]\n  uri = bolt://192.168.1.99:7687\n  \n  [qdrant]\n  uri = http://192.168.1.99:6333\n  ```\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "axunrun",
      "author_type": "User",
      "created_at": "2025-03-03T18:49:32Z",
      "updated_at": "2025-03-05T15:29:27Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/985/reactions",
        "total_count": 5,
        "+1": 5,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/985",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/985",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.089281",
      "comments": [
        {
          "author": "jradikk",
          "body": "I'm interested in this as well. There's a functionality that allows setting your own IDs per document(s) on insert https://github.com/HKUDS/LightRAG/pull/892. So, even though, that's quite tedious to manage and keep track of, it allows deleting the documents from the DB. However, there's no metadata",
          "created_at": "2025-03-04T09:33:20Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "I have applied a temporary fix; you can refer to it here:\nhttps://github.com/HKUDS/LightRAG/pull/960.",
          "created_at": "2025-03-05T15:29:26Z"
        }
      ]
    },
    {
      "issue_number": 1008,
      "title": "[Question]: Why status for full_docs and chunks storage in OracleDB",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhat is the purpose of storing the document status in the `LIGHTRAG_DOC_FULL` and `LIGHTRAG_DOC_CHUNKS` for the oracle implementation? I haven't seen it in other storage implementations, only for the document status storage, which is currently not implemented for Oracle. When I try to run the graph indexing, I get a `KeyError` in the upsert method for the OracleKVStorage, as the document status is not provided here:\n\n```python\nasync def upsert(self, data: dict[str, dict[str, Any]]) -> None:\n        logger.info(f\"Inserting {len(data)} to {self.namespace}\")\n        if not data:\n            return\n\n        if is_namespace(self.namespace, NameSpace.KV_STORE_TEXT_CHUNKS):\n            list_data = [\n                {\n                    \"id\": k,\n                    **{k1: v1 for k1, v1 in v.items()},\n                }\n                for k, v in data.items()\n            ]\n            contents = [v[\"content\"] for v in data.values()]\n            batches = [\n                contents[i : i + self._max_batch_size]\n                for i in range(0, len(contents), self._max_batch_size)\n            ]\n            embeddings_list = await asyncio.gather(\n                *[self.embedding_func(batch) for batch in batches]\n            )\n            embeddings = np.concatenate(embeddings_list)\n            for i, d in enumerate(list_data):\n                d[\"__vector__\"] = str(embeddings[i].tolist())\n\n            merge_sql = SQL_TEMPLATES[\"merge_chunk\"]\n            for item in list_data:\n                _data = {\n                    \"id\": item[\"id\"],\n                    \"content\": item[\"content\"],\n                    \"workspace\": self.db.workspace,\n                    \"tokens\": item[\"tokens\"],\n                    \"chunk_order_index\": item[\"chunk_order_index\"],\n                    \"full_doc_id\": item[\"full_doc_id\"],\n                    \"content_vector\": item[\"__vector__\"],\n                    \"status\": item[\"status\"],\n                }\n                await self.db.execute(merge_sql, _data)\n```\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ghost",
      "author_type": "User",
      "created_at": "2025-03-05T13:02:45Z",
      "updated_at": "2025-03-05T13:02:45Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1008/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1008",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1008",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.256955",
      "comments": []
    },
    {
      "issue_number": 1007,
      "title": "[Question]: Result referencies",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHey there!\n\nI would like to know how to get the referencies of the top_k used to produce the result of the query?\n\nBest regards.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "eaedk",
      "author_type": "User",
      "created_at": "2025-03-05T11:54:06Z",
      "updated_at": "2025-03-05T11:54:06Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1007/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1007",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1007",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.256976",
      "comments": []
    },
    {
      "issue_number": 1004,
      "title": "[Bug]: Empty Vectors with Qdrant Configuration",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen using Qdrant as vector storage, documents can be inserted but their vector representations are empty (all zeros). This leads to search responses returning \"[no-context]\". However, when using PostgreSQL vector storage, vectors are properly stored and search functions normally.\n\n\n### Steps to reproduce\n\n1. Start service with above Qdrant configuration\n2. Insert test document via API:\n3. Check vector values in Qdrant (all zeros)\n4. Execute search request (returns no-context)\n5. Switch to PostgreSQL configuration\n6. Repeat steps 2-4 (works normally)\n\n## Error Logs\n[Please provide relevant error logs or exception information]\n\n## Expected Behavior\n- Documents should be properly processed and stored in Qdrant\n- Vector values should be correctly calculated and stored\n- Search requests should return relevant results\n\n## Actual Behavior\n- Documents can be stored in Qdrant\n- Vector values are all zeros\n- Search requests return \"[no-context]\" error\n\n## Possible Causes\n1. Issue with embedding service vector generation\n2. Problem in vector data transfer between embedding service and Qdrant\n3. Mismatch in vector dimensions or format\n4. Configuration issue with Qdrant client\n5. Connection problems between services\n\n## Additional Notes\n- All other storage components (KV, Graph, Doc Status) are functioning normally\n- The same embedding model and configuration work correctly with PostgreSQL\n- Qdrant server is accessible and running\n\n## Questions for Investigation\n1. Is the embedding service generating vectors correctly?\n2. Are the vectors being properly transferred to Qdrant?\n3. Is there any data transformation issue between services?\n4. Are there any relevant error logs in the embedding service?\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n### Server Configuration\nHOST=0.0.0.0\nPORT=9621\nWORKERS=1\nNAMESPACE_PREFIX=lightrag\nMAX_GRAPH_NODES=1000\nCORS_ORIGINS=http://localhost:3000,http://localhost:8080\n\n### SSL Configuration\nSSL=false\nSSL_CERTFILE=\nSSL_KEYFILE=\n\n### Security\nLIGHTRAG_API_KEY=\n\n### Directory Configuration\nWORKING_DIR=/app/data/rag_storage\nINPUT_DIR=/app/data/inputs\n\n### Ollama Configuration\nOLLAMA_EMULATING_MODEL_TAG=latest\n\n### Logging Configuration\nLOG_LEVEL=INFO\nVERBOSE=false\nLOG_DIR=/app/data/rag_storage/logs\nLOG_MAX_BYTES=10485760\nLOG_BACKUP_COUNT=5\n\n### LLM Configuration\nMAX_ASYNC=4\nTIMEOUT=150\n\n### RAG Query Settings\nHISTORY_TURNS=3\nCOSINE_THRESHOLD=0.2\nTOP_K=60\nMAX_TOKEN_TEXT_CHUNK=4000\nMAX_TOKEN_RELATION_DESC=4000\nMAX_TOKEN_ENTITY_DESC=4000\n\n### Document Indexing Settings\nCHUNK_SIZE=1200\nCHUNK_OVERLAP_SIZE=100\nMAX_TOKENS=32768\nMAX_TOKEN_SUMMARY=500\nSUMMARY_LANGUAGE=Chinese\nMAX_EMBED_TOKENS=8192\n\n### LLM Binding Configuration\nLLM_BINDING=openai\nLLM_MODEL=\nLLM_BINDING_HOST=\nLLM_BINDING_API_KEY=\n\n### Embedding Configuration\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://192.168.1.99:11434\nEMBEDDING_BINDING_API_KEY=\n\n### Storage Configuration\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=Neo4JStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\n### PostgreSQL Configuration\nPOSTGRES_HOST=192.168.1.99\nPOSTGRES_PORT=5432\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=\nPOSTGRES_DATABASE=lightrag\n\n### Neo4j Configuration\nNEO4J_URI=bolt://192.168.1.99:7687\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=\n\n### Qdrant Configuration\n### QDRANT_URL=http://192.168.1.99:6333 \n\n\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/47ce64e0-ba71-4c96-83d1-4feb270dc4c6)\n\n![Image](https://github.com/user-attachments/assets/0a852b46-e22d-4829-81bd-fe6e9355424e)\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "axunrun",
      "author_type": "User",
      "created_at": "2025-03-05T06:40:40Z",
      "updated_at": "2025-03-05T06:40:40Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1004/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/1004",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1004",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.256984",
      "comments": []
    },
    {
      "issue_number": 942,
      "title": "[Question]: Does using a GPU improve query speed?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHello everyone, it seems that when using mix, the query speed is very slow. I'm looking for every possible way to speed it up. Will using a GPU improve the speed?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-02-25T04:10:33Z",
      "updated_at": "2025-03-05T01:08:41Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/942/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/942",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/942",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.256990",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "The low speed is due to LLM respond in most of the case. LightRAG interact with LLM serval times with large contexts. So do what ever possible to improve LLM respond speed.",
          "created_at": "2025-03-02T15:34:30Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "> The low speed is due to LLM respond in most of the case. LightRAG interact with LLM serval times with large contexts. So do what ever possible to improve LLM respond speed.\n\nI don't think it's due to the speed of the LLM because I'm using OpenAI's API, and they have done a great job with this.",
          "created_at": "2025-03-05T01:08:40Z"
        }
      ]
    },
    {
      "issue_number": 992,
      "title": "[Bug]: Batch File Upload Causing Content Duplication",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen uploading multiple files simultaneously (e.g., multiple novel chapters), the system incorrectly duplicates the content (usually the first file's content) across different doc_ids in the database, resulting in incorrect document mapping relationships.\n\n### Steps to reproduce\n\n1.Prepare multiple txt files (e.g., xx_chapter_001.txt to xx_chapter_012.txt)  \n2. Use the web interface's batch upload feature to upload these files simultaneously\n3. Observe the lightrag_doc_full table in PostgreSQL database\n\nCurrent Behavior\n1.Each file generates a unique doc_id  \n2. In the lightrag_doc_full table, different doc_ids contain identical content  \n3.Typically, all records contain the content of the first file\n4. This leads to incorrect subsequent processing (text chunking, entity extraction, etc.)\n\nImportant Finding\nSequential Upload Works Correctly: When uploading files one by one and waiting for each file to complete processing before uploading the next, the system works as expected. This strongly indicates that the issue lies in the batch processing queue mechanism rather than in the basic file processing logic.\n\n### Expected Behavior\n\n1. Each file should be processed independently\n2. Each doc_id should correspond to its correct unique content\n3. The content in the database should match the original file content\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n<img width=\"527\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/db52e349-3391-4173-a7c1-e6d97569a168\" />\n\n### Additional Information\n\nOS: Windows 10  • PostgreSQL Version: Latest  • Neo4j Version: Latest  • Python Version: 3.13\n",
      "state": "open",
      "author": "axunrun",
      "author_type": "User",
      "created_at": "2025-03-04T10:05:17Z",
      "updated_at": "2025-03-04T17:26:08Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/992/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/992",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/992",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.439698",
      "comments": [
        {
          "author": "axunrun",
          "body": "I've tried several approaches to fix this issue. Currently, as a temporary workaround, I'm using a queue lock that prevents files from being uploaded to the input directory all at once. Instead, files are now uploaded one by one into the queue for processing. While this prevents the duplicate proces",
          "created_at": "2025-03-04T17:26:07Z"
        }
      ]
    },
    {
      "issue_number": 999,
      "title": "[Feature Request]: Feature Request: Document Tagging System for Better Content Organization",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n### Current Situation\nCurrently, when uploading multiple documents, especially split chapters from different sources (like multiple novels), all documents are stored in the same workspace without any categorization. This makes it difficult to:\n- Distinguish between different document sources\n- Group related content together (e.g., chapters from the same novel)\n- Filter or query documents based on their origin or type\n\n### Feature Request\nAdd a document tagging system that would allow users to:\n1. Add custom tags to documents during upload\n2. Automatically generate tags based on filename patterns\n3. Filter and query documents based on tags\n\n### Use Cases\n1. **Novel Chapter Management**\n   - When uploading split chapters from multiple novels\n   - Each chapter could be tagged with its novel name\n   - Makes it easier to query or process chapters from a specific novel\n\n2. **Document Source Tracking**\n   - Different document sources (books, articles, reports) can be tagged accordingly\n   - Helps maintain clear separation between different content types\n\n3. **Automated Organization**\n   - Automatically extract tags from filenames (e.g., \"Novel1_Chapter1.txt\" → tags: [\"Novel1\", \"Chapter\"])\n   - Allow custom tag rules based on filename patterns\n\n\n### Suggested Implementation\n1. **API Enhancement**\n   ```python\n   # Example API additions\n   POST /documents/upload\n   {\n     \"file\": binary_data,\n     \"tags\": [\"novel_name\", \"chapter_1\", \"source_type\"]\n   }\n   \n   POST /documents/file_batch\n   {\n     \"files\": binary_data[],\n     \"tag_rules\": {\n       \"pattern\": \"^(.*?)_Chapter\",  # Extract novel name before \"_Chapter\"\n       \"tags\": [\"source:{match_1}\"]  # Create tag using matched pattern\n     }\n   }\n   ```\n\n2. **Storage**\n   - Add tags field to document metadata\n   - Include tag information in document status responses\n\n3. **Query Enhancement**\n   ```python\n   GET /documents?tags=novel_name\n   GET /documents?tags=source:novel1,chapter:1\n   ```\n\n### Benefits\n1. Better organization of large document collections\n2. Improved document retrieval and filtering\n3. Clear separation between different content sources\n4. Enhanced content management for split documents\n\n### Impact\nThis feature would significantly improve the usability of LightRAG for users working with:\n- Multiple document sources\n- Split documents (like book chapters)\n- Large document collections requiring organization\n- Content from different domains or categories\n\nWould love to hear your thoughts on this feature request and discuss any implementation details or alternatives you might suggest.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "axunrun",
      "author_type": "User",
      "created_at": "2025-03-04T17:21:04Z",
      "updated_at": "2025-03-04T17:21:04Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/999/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/999",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/999",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.644150",
      "comments": []
    },
    {
      "issue_number": 997,
      "title": "The invite in the discord link is invalid.",
      "body": "\nThe invite in the discord link is invalid. Please update\n\n",
      "state": "open",
      "author": "melihcansahin0305",
      "author_type": "User",
      "created_at": "2025-03-04T14:14:31Z",
      "updated_at": "2025-03-04T14:17:49Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/997/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/997",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/997",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.644192",
      "comments": []
    },
    {
      "issue_number": 996,
      "title": "experimental results",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nThe experimental results obtained from comparing naiveRAG and lightRAG on the mix dataset differ significantly from the results in the original paper. Could the authors provide the experimental procedure from the original paper?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "youzi668786",
      "author_type": "User",
      "created_at": "2025-03-04T14:01:35Z",
      "updated_at": "2025-03-04T14:01:35Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/996/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/996",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/996",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.644198",
      "comments": []
    },
    {
      "issue_number": 984,
      "title": "[Feature Request]: Customizing the graph creation prompt for a better context aware extraction",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n@ArnoChenFx @danielaskdd @LarFii  @YanSte  I think we need to add an option to set the entities extraction prompt to reduce noise.\n\nFor example, in my papers analysis database I get alot of completely useless information about IEEE conferences, and the authors etc. I want the content, not noise. I can enhancde this by customizing the prompt.\n\nAnd it should be cool if we can do that in the ui. Hense the need for a settings page where we can group all settings including these customizations.\n\nWhat do you think?\n\nWe can also add these as arguments or as environment variables too.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-03-03T17:14:42Z",
      "updated_at": "2025-03-04T12:03:48Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/984/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/984",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/984",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.644203",
      "comments": [
        {
          "author": "yaleimeng",
          "body": "you can  edit  lightrag/prompt.py ",
          "created_at": "2025-03-04T07:55:39Z"
        },
        {
          "author": "ParisNeo",
          "body": "Thanks",
          "created_at": "2025-03-04T12:03:47Z"
        }
      ]
    },
    {
      "issue_number": 993,
      "title": "[Bug]: <title>",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nLightRAG Server stalls after processing large 99% of the chunks. When we restart, the server re-scans and starts extraction of entities from first chunk, increasing the cost of binding model use. \n\nIt is always the last of the chunk where it stalls, for example 98/99 or 219/220 chunks. I have added screenshot below for reference. \n\n\n<img width=\"835\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b36eac92-6cc7-4884-852b-f502d11c4cfd\" />\n\n### Steps to reproduce\n\nI am using a text file of about 400KB. \n\n### Expected Behavior\n\nLarge documents stalls & re-processing it may work.\n\n### LightRAG Config Used\n\n# Paste your config here\n\nBinding Model: gpt-4o-mini\nLLM Model: get-4o-mini\n\nStorage: Postgres with vector and age extension.\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: V1.2.3\n- Operating System: Docker\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "nirmaaan",
      "author_type": "User",
      "created_at": "2025-03-04T10:08:51Z",
      "updated_at": "2025-03-04T10:08:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/993/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/993",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/993",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.838442",
      "comments": []
    },
    {
      "issue_number": 945,
      "title": "[Bug]: Query results are abnormal",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen using different models, the same question and data set, after switching models, the query directly returns the same content, and the speed is also very fast. Generally speaking, switching different models should have different answer results. How to solve this problem?\n\n### Steps to reproduce\n\nChange the backbone model.\n\n### Expected Behavior\n\nGenerated responses are different, and each query needs more time.\n\n### LightRAG Config Used\n\nBefore:(gpt-4o-mini)\n    rag = LightRAG(working_dir=WORKING_DIR)\nAfter:\nrag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=\"llama3.1\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}}, \n    )\n    query_param = QueryParam(mode=mode)\n\n### Logs and screenshots\n\n![Image](https://github.com/user-attachments/assets/c53aee9d-c68a-4723-8614-5c2083002593)\n\n### Additional Information\n\n- LightRAG Version: 1.1.0\n- Operating System: Ubuntu24\n- Python Version: 3.10\n- Related Issues: No\n",
      "state": "open",
      "author": "WinstonCHEN1",
      "author_type": "User",
      "created_at": "2025-02-25T09:48:28Z",
      "updated_at": "2025-03-04T09:46:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/945/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/945",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/945",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:19.838461",
      "comments": [
        {
          "author": "WinstonCHEN1",
          "body": "Another problem is that when using the ollama model with naive mode, this fails to generate the answer but gets stuck",
          "created_at": "2025-02-25T10:01:54Z"
        },
        {
          "author": "ParisNeo",
          "body": "This is due to the caching mechanism. I have already notified @LarFii  about it. This should be fixed. The caching must be renewed if:\n1 - new data has been added to the dataset\n2 - another model is being used.\n\nSo we need to make the cache store information about the AI being used. I also think tha",
          "created_at": "2025-02-26T08:35:55Z"
        },
        {
          "author": "WinstonCHEN1",
          "body": "> This is due to the caching mechanism. I have already notified [@LarFii](https://github.com/LarFii) about it. This should be fixed. The caching must be renewed if: 1 - new data has been added to the dataset 2 - another model is being used.\n> \n> So we need to make the cache store information about t",
          "created_at": "2025-02-28T00:23:58Z"
        },
        {
          "author": "danielaskdd",
          "body": "You can disable llm cache by setting the enable_llm_cache attribute of LightRAG object to False.",
          "created_at": "2025-03-02T18:40:33Z"
        },
        {
          "author": "ParisNeo",
          "body": "> You can disable llm cache by setting the enable_llm_cache attribute of LightRAG object to False.\n\nYes, but even when we activate caching, I still think it should be smarter and invalidate cache whenever a new file is added or if the llm has changed.",
          "created_at": "2025-03-03T11:46:40Z"
        }
      ]
    },
    {
      "issue_number": 991,
      "title": "[Bug]: <title>'LightRAG' object has no attribute 'delete_by_doc_id'",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nI inserted two articles using the `rag.insert()` function, with IDs 1 and 2 respectively. \n\nI also checked the `.\\workspace\\kv_store_full_docs.json` file, which contains:\n`{\n  \"1\": {\n    \"content\": \"xxxxxx\"\n  },\n  \"2\": {\n    \"content\": \"xxxxxx\"\n  }\n}`\n\nHowever, when I execute `rag.delete_by_doc_id([\"2\"])`,\n\n it shows an\n``AttributeError: 'LightRAG' object has no attribute 'delete_by_doc_id'`` \n\nversion：lightrag-hku-1.2.3\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Buzeg",
      "author_type": "User",
      "created_at": "2025-03-04T08:19:45Z",
      "updated_at": "2025-03-04T08:26:50Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/991/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/991",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/991",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:20.036009",
      "comments": []
    },
    {
      "issue_number": 830,
      "title": "删除文档 adelete_by_doc_id",
      "body": "一般图数据库没有默认支持级联删除，应该先删除边，再删除实体",
      "state": "open",
      "author": "newbie-Li",
      "author_type": "User",
      "created_at": "2025-02-18T07:38:02Z",
      "updated_at": "2025-03-04T07:45:58Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/830/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/830",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/830",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:20.036030",
      "comments": [
        {
          "author": "YanSte",
          "body": "Yes, this is a bug reporter. This will be implemented in the roadmap.\n\n",
          "created_at": "2025-02-18T09:06:50Z"
        },
        {
          "author": "Buzeg",
          "body": "> Yes, this is a bug reporter. This will be implemented in the roadmap.\n\nMy usage scenario is to perform GraphRAG on an article, ask questions to get corresponding answers, then delete that article, and proceed to perform GraphRAG on the next article, ask questions to get answers, delete it, and rep",
          "created_at": "2025-03-04T07:45:58Z"
        }
      ]
    },
    {
      "issue_number": 966,
      "title": "[Question]: <title>WARNING:low_level_keywords and high_level_keywords is empty",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nIssue Description​\nI deployed deepseek-r1:8b using Ollama and used bge-m3 as the embedding model. Below is my LightRAG configuration:\n```\nLightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"deepseek-r1:8b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    # chunk_token_size=2048,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts, embed_model=\"bge-m3\", host=\"http://localhost:11434\"\n        ),\n    ),\n)\n```\nWhen I run a query, I get a warning:\nWARNING:low_level_keywords and high_level_keywords is empty\nThis is my query statement：\n```\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\",\n                                                                         ll_keywords=[\"story\", \"plot\"],\n                                                                         hl_keywords=[\"themes\", \"analysis\"]))\n)\n```\nQuestion​\nHow can I resolve this issue?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "IAMmengxin",
      "author_type": "User",
      "created_at": "2025-02-28T09:58:56Z",
      "updated_at": "2025-03-04T03:55:08Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/966/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/966",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/966",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:20.228568",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "Check system logs when you inserting document to LightRAG, to see if there is any problem there.",
          "created_at": "2025-03-02T18:45:12Z"
        },
        {
          "author": "IAMmengxin",
          "body": "> 将文档插入 LightRAG 时检查系统日志，查看是否存在任何问题。\n\nApologies for the delayed response. \nWhen I inserted the document into LightRAG, no errors occurred. Moreover, when using Gemma2 as the large model, everything worked fine, and my rag.query received a normal response.\nWhen I was using DeepSeek as the LLM, I enco",
          "created_at": "2025-03-03T06:30:04Z"
        },
        {
          "author": "danielaskdd",
          "body": "可能是因为本地8B参数量的Deepseek没有遵照提示词生成复合要求求的关键字。更换云端的Deepseek API试一下，如果恢复正常，说明需要提高本地模型的参数规模。",
          "created_at": "2025-03-03T08:47:23Z"
        },
        {
          "author": "IAMmengxin",
          "body": "> 可能是因为本地8B参数量的Deepseek没有遵照提示词生成复合要求求的关键字。更换云端的Deepseek API试一下，如果恢复正常，说明需要提高本地模型的参数规模。\n\nOkay, I'll give it a try. thank you",
          "created_at": "2025-03-03T09:45:13Z"
        },
        {
          "author": "IAMmengxin",
          "body": "> 可能是因为本地8B参数量的Deepseek没有遵照提示词生成复合要求求的关键字。更换云端的Deepseek API试一下，如果恢复正常，说明需要提高本地模型的参数规模。\n\nAfter I updated to version 1.2.4 and replaced the deepseek-r1 model with the 7b version, the issue was resolved.",
          "created_at": "2025-03-04T03:55:07Z"
        }
      ]
    },
    {
      "issue_number": 977,
      "title": "Will LightRAG support IBM Watsonx.ai?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n_No response_\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "jayakumarselvan",
      "author_type": "User",
      "created_at": "2025-03-03T06:16:44Z",
      "updated_at": "2025-03-03T18:48:39Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/977/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/977",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/977",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:20.451427",
      "comments": [
        {
          "author": "leonmeijer",
          "body": "With https://github.com/aseelert/watsonx-openai-api, you can host an OpenAI compatible API which should be compatible LightRAG . Have not tested this, but will try this out coming weeks.",
          "created_at": "2025-03-03T18:48:38Z"
        }
      ]
    },
    {
      "issue_number": 983,
      "title": "[Question]: Why is the param 'graph_storage' a string and not a dependency injection?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n_More of a software design questions for the devs, but if anyone else has an idea on why this could be preferred, let me know!_\n\nWhy do we specify which database to use by passing a string identifier?\n```python\nLightRAG(\n    working_dir=work_dir,\n    embedding_func=openai_embed,\n    graph_storage=\"Neo4JStorage\",  # <- Note a string being passed here\n    llm_model_func=gpt_4o_mini_complete,\n)\n```\nRather than;\n```python\nfrom lightrag.kg.neo4j_impl import Neo4JStorage  # Probably would improve this import\nLightRAG(\n    working_dir=work_dir,\n    embedding_func=openai_embed,\n    graph_storage=Neo4JStorage,  # <- Note the passing of the kg dependency directly\n    llm_model_func=gpt_4o_mini_complete,\n)\n```\n\nI believe injecting the dependency directly would allow people to extend behavior and have more fine-grained control of LightRAG. For example, [Issue 943](https://github.com/HKUDS/LightRAG/issues/943) would need a different schema or table per user, which currently is challenging to do.\n\n## My Proposal\nFor backwards compatibility, add the passing of a kg implementation class an option an top of the current string implementation. A protocol/interface would need to be defined and typed that enforces the methods that must be included on a kg implementation ('GraphStorage' in the example below).\n\n```python\n@final\n@dataclass\nclass LightRAG:\n    graph_storage: str | GraphStorage = field(default=\"NetworkXStorage\")\n    \"\"\"Storage backend for knowledge graphs.\"\"\"\n```\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "JoramMillenaar",
      "author_type": "User",
      "created_at": "2025-03-03T17:05:10Z",
      "updated_at": "2025-03-03T17:05:10Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/983/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/983",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/983",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:20.652397",
      "comments": []
    },
    {
      "issue_number": 967,
      "title": "[Bug]: KeyError when uploading a document",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nWhen it tries to index any document (txt or pdf), I then have keyerror for each file.\n\n### Steps to reproduce\n\nI use LightRAG with Postgres for all the storage in a docker compose with the [provided docker image for psql ](https://hub.docker.com/r/shangor/postgres-for-rag).  I use a local Ollama for all the process. \n\nThen, whenever I try to upload a document, thought the api or the WebUI, I go the error.\n\n### Expected Behavior\n\nNormal indexing and the enqueuing of the file for the embedding. \n\n### LightRAG Config Used\n\n# Paste your config here\nHere is my yaml file:\n\n```\nversion: '3.8'\n\nservices:\n  lightrag:\n    build: .\n    container_name: lightrag-lightrag\n    ports:\n      - \"${PORT:-9621}:9621\"\n    volumes:\n      - ./data/rag_storage:/app/data/rag_storage\n      - ./data/inputs:/app/data/inputs\n      - .env:/app/.env\n    env_file:\n      - .env\n    environment:\n      - TZ=UTC\n    restart: unless-stopped\n    networks:\n      - lightrag_net\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n\n  postgres:\n    image: shangor/postgres-for-rag:v1.0\n    container_name: lightrag-postgres\n    restart: unless-stopped\n    volumes:\n      - ./postgres_data/data:/var/lib/postgresql/data\n      - .env:/app/.env\n    env_file:\n      - .env\n    ports:\n      - \"${POSTGRES_PORT:-5432}:5432\"\n    networks:\n      - lightrag_net\n\nnetworks:\n  lightrag_net:\n    driver: bridge\n\n```\n\nAnd my .env file:\n\n```\n### Logging level\nLOG_LEVEL=INFO\nVERBOSE=False\n\n### Optional Timeout\nTIMEOUT=300\n\n# Ollama Emulating Model Tag\n# OLLAMA_EMULATING_MODEL_TAG=latest\n\n### RAG Configuration\nMAX_ASYNC=4\nEMBEDDING_DIM=1024\nMAX_EMBED_TOKENS=8192\n### Settings relative to query\nHISTORY_TURNS=3\nCOSINE_THRESHOLD=0.2\nTOP_K=60\nMAX_TOKEN_TEXT_CHUNK=4000\nMAX_TOKEN_RELATION_DESC=4000\nMAX_TOKEN_ENTITY_DESC=4000\n### Settings relative to indexing\nCHUNK_SIZE=1200\nCHUNK_OVERLAP_SIZE=100\nMAX_TOKENS=32768\nMAX_TOKEN_SUMMARY=500\nLANGUAGE=French\n\n### LLM Configuration (Use valid host. For local services, you can use host.docker.internal)\nLLM_BINDING=ollama\nLLM_BINDING_HOST=http://host.docker.internal:12345\nLLM_MODEL=qwen2.5m:14b\n\n### Embedding Configuration (Use valid host. For local services, you can use host.docker.internal)\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://host.docker.internal:12345\nEMBEDDING_MODEL=nomic-embed-text\n\n### Data storage selection\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=PGVectorStorage\nLIGHTRAG_GRAPH_STORAGE=PGGraphStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\n### PostgreSQL Configuration\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_USER=rag\nPOSTGRES_PASSWORD='rag'\nPOSTGRES_DATABASE=rag\nPOSTGRES_DB=rag\n```\n\n### Logs and screenshots\n\n```\nINFO:     172.19.0.1:48304 - \"POST /documents/scan HTTP/1.1\" 200 OK\nINFO:Scanning for .txt files in /app/inputs\nINFO:Scanning for .md files in /app/inputs\nINFO:Scanning for .pdf files in /app/inputs\nINFO:Scanning for .docx files in /app/inputs\nINFO:Scanning for .pptx files in /app/inputs\nINFO:Scanning for .xlsx files in /app/inputs\nINFO:Found 6 new files to index.\nERROR:Error processing or enqueueing file fccd2652fd6eaec2fef62e0010c9d3fe.txt: 'b'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1382, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n  File \"/app/lightrag/lightrag.py\", line 691, in apipeline_enqueue_documents\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 691, in <dictcomp>\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n                        ~~~~~~~~^^^^^^^^\nKeyError: 'b'\n\nERROR:Error processing or enqueueing file fab8f00e9c5df5d4c4016ff5e5e5daad.txt: 'b'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1382, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n  File \"/app/lightrag/lightrag.py\", line 691, in apipeline_enqueue_documents\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 691, in <dictcomp>\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n                        ~~~~~~~~^^^^^^^^\nKeyError: 'b'\n\nERROR:Error processing or enqueueing file fc6a449fabf2aa282fc3a613d61674a1.txt: 'b'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1382, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n  File \"/app/lightrag/lightrag.py\", line 691, in apipeline_enqueue_documents\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 691, in <dictcomp>\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n                        ~~~~~~~~^^^^^^^^\nKeyError: 'b'\n\nERROR:Error processing or enqueueing file f54027ec487d84343b53d2ffea7266e7.txt: 'e'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1382, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n  File \"/app/lightrag/lightrag.py\", line 691, in apipeline_enqueue_documents\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 691, in <dictcomp>\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n                        ~~~~~~~~^^^^^^^^\nKeyError: 'e'\n\nERROR:Error processing or enqueueing file f2339daaba58936cbf4ab54dbcb3a335.txt: 'b'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1382, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n  File \"/app/lightrag/lightrag.py\", line 691, in apipeline_enqueue_documents\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 691, in <dictcomp>\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n                        ~~~~~~~~^^^^^^^^\nKeyError: 'b'\n\nINFO:     172.19.0.1:48304 - \"GET /documents HTTP/1.1\" 200 OK\nERROR:Error processing or enqueueing file 0bf0d6c007404cdc96fc608859ea0cb4.pdf: 'b'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1382, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n  File \"/app/lightrag/lightrag.py\", line 691, in apipeline_enqueue_documents\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 691, in <dictcomp>\n    new_docs = {doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids}\n                        ~~~~~~~~^^^^^^^^\nKeyError: 'b'\n```\n\n### Additional Information\n\n- LightRAG Version: latest\n- Operating System: ubuntu\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "Brocowlee",
      "author_type": "User",
      "created_at": "2025-02-28T13:57:13Z",
      "updated_at": "2025-03-03T14:46:13Z",
      "closed_at": "2025-03-03T14:46:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/967/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/967",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/967",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:20.652420",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "May be there is so illegal chars in your uploaded documents. Try to upload some normal utf-8 text or md file to see what happens.",
          "created_at": "2025-03-02T14:28:36Z"
        },
        {
          "author": "Brocowlee",
          "body": "I thought that wasn't the problem as I could try to add a single word such as \"string\" into the RAG, and the same problem would appear; however, I just pulled the latest commits, and it seems that it has been fixed! Thank you so much 🙏",
          "created_at": "2025-03-03T14:46:12Z"
        }
      ]
    },
    {
      "issue_number": 862,
      "title": "LlamaIndex LLM Interface",
      "body": "a module that leverages LlamaIndex to provide asynchronous chat completions and embedding generation in LightRAG.\n\nKey Features:\n\n- Async Chat Completion:\n- Embedding Generation:\n\nGenerates embeddings for a list of texts using a configurable LlamaIndex embedding model.\nEnforces embedding dimensions and token limits through a decorator.\n\nConfiguration & Dependencies:\n- Allows global configuration of LlamaIndex settings.\n- Automatically checks and installs the required llama-index dependency using pipmaster.\n\nUsage Example:\n```\npython\nCopy\n# Chat completion\nresponse = await llama_index_complete(\n    prompt=\"What are the benefits of integrating LlamaIndex?\",\n    system_prompt=\"You are a knowledgeable assistant.\",\n    history_messages=[{\"role\": \"user\", \"content\": \"Tell me about LightRAG features\"}],\n    llm_instance=my_llm_instance,\n)\n\n# Embedding generation\nembeddings = await llama_index_embed(\n    texts=[\"First sentence.\", \"Second sentence.\"],\n    embed_model=my_embedding_model,\n)\n```\n\n\n\n\n\n\n",
      "state": "closed",
      "author": "spo0nman",
      "author_type": "User",
      "created_at": "2025-02-19T11:04:48Z",
      "updated_at": "2025-03-03T09:40:15Z",
      "closed_at": "2025-03-03T09:40:15Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/862/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/862",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/862",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:20.839100",
      "comments": [
        {
          "author": "spo0nman",
          "body": "https://github.com/HKUDS/LightRAG/pull/864 i made an attempt at an implementation. please review.\n",
          "created_at": "2025-02-19T11:18:07Z"
        }
      ]
    },
    {
      "issue_number": 828,
      "title": "Retrieval of information is failing using Mistral API endpoint",
      "body": "Hi Team,\nI am trying to implement LightRAG with Mistral API endpoint for Model: mistralai/Mixtral-8x7B-Instruct-v0.1. When I am inserting new document which is not available online and asking document specific query, the model is not able to retrieve the content.  I have also tried by deleting earlier documents and inserting the new document alone but still not able to get the success.  While LightRAG implementation with local ollama model :qwen2 is able to retrieve the information properly. \n\nlightrag) PS C:\\Lightrag\\LG_mistral\\lightrag\\examples> python lightrag_mistral_chat_pdf_JBS.py\nINFO:Logger initialized for working directory: C:/Lightrag/LG_mistral/LightRAG/examples\nINFO:Load KV json_doc_status_storage with 0 data\nINFO:Load KV llm_response_cache with 6 data\nINFO:Load KV full_docs with 4 data\nINFO:Load KV text_chunks with 444 data\nINFO:Loaded graph from C:/Lightrag/LG_mistral/LightRAG/examples\\graph_chunk_entity_relation.graphml with 2089 nodes, 1194 edges\nINFO:Load (1978, 4096) data\nINFO:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': 'C:/Lightrag/LG_mistral/LightRAG/examples\\\\vdb_entities.json'} 1978 data\nINFO:Load (1194, 4096) data\nINFO:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': 'C:/Lightrag/LG_mistral/LightRAG/examples\\\\vdb_relationships.json'} 1194 data\nINFO:Load (482, 4096) data\nINFO:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': 'C:/Lightrag/LG_mistral/LightRAG/examples\\\\vdb_chunks.json'} 482 data\nINFO:Loaded document status storage with 4 records\nINFO:Processing 1 new unique documents\nProcessing batch 1:   0%|                                                                                                                      | 0/1 [00:00<?, ?it/s]INFO:Inserting 1 vectors to chunks\nGenerating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/batch]\nGenerating embeddings: 100%|█████████████████████████████████████████████WARNING:Didn't extract any relationships███████████████████| 1/1 [00:01<00:00,  1.54s/batch] \nINFO:Inserting 8 vectors to entities\nGenerating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.18s/batch]\nINFO:Inserting 0 vectors to relationships███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.16s/batch] \nWARNING:You insert an empty data to vector DB\nINFO:Writing graph with 2097 nodes, 1194 edges\nProcessing batch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.58s/it]",
      "state": "closed",
      "author": "j-shah7",
      "author_type": "User",
      "created_at": "2025-02-18T06:31:40Z",
      "updated_at": "2025-03-03T05:19:23Z",
      "closed_at": "2025-02-19T20:39:30Z",
      "labels": [
        "mistral"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/828/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/828",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/828",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:21.086736",
      "comments": [
        {
          "author": "YanSte",
          "body": "We are not supporting Mistral for the moment.",
          "created_at": "2025-02-18T09:07:40Z"
        },
        {
          "author": "YanSte",
          "body": "Mistral doesn't support OpenAI calls.",
          "created_at": "2025-02-19T20:38:49Z"
        },
        {
          "author": "YanSte",
          "body": "If you would like, you can help us with this LLM.",
          "created_at": "2025-02-19T20:39:28Z"
        },
        {
          "author": "j-shah7",
          "body": "Sure. Please let me know which part of code needs modification to retrieve the data effectively as it is not throwing as such any error but at the same time not able to retrieve the data. Thanks",
          "created_at": "2025-03-03T05:19:22Z"
        }
      ]
    },
    {
      "issue_number": 936,
      "title": "[Bug]: API - loading existing nano-vectordb fails with:  \"Embedding dim mismatch\"",
      "body": "### Do you need to file an issue?\n\n- [ ] I have searched the existing issues and this bug is not already filed.\n- [ ] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n```\nexport LIGHTRAG_KV_STORAGE=JsonKVStorage \nexport LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage   \nexport LIGHTRAG_GRAPH_STORAGE=NetworkXStorage\nexport LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage\nexport LLM_BINDING=openai\nexport LLM_MODEL=GPT-4o-mini\nexport EMBEDDING_BINDING=openai\nexport EMBEDDING_MODEL=text-embedding-3-small\n# export LLM_BINDING_API_KEY .. already set on environment\n# export OPENAI_API_KEY ... already set on env\n\npython3 -m lightrag.api.lightrag_server  --host 0.0.0.0 --port 9621 --working-dir mytest --log-level DEBUG --verbose --input-dir input \n```\n\n/mytest contains a set of already processed vector data and stuff..\n\n```\nls -l mytest/\ntotal 139036\n-rw-rw-r-- 1 demon demon  4072823 Feb 21 00:45 graph_chunk_entity_relation.graphml\n-rw-rw-r-- 1 demon demon  1298618 Feb 21 00:45 kv_store_doc_status.json\n-rw-rw-r-- 1 demon demon  1298088 Feb 21 00:45 kv_store_full_docs.json\n-rw-rw-r-- 1 demon demon 21183627 Feb 21 16:01 kv_store_llm_response_cache.json\n-rw-rw-r-- 1 demon demon  1477271 Feb 21 00:45 kv_store_text_chunks.json\ndrwxrwxr-x 2 demon demon     4096 Feb 21 00:20 old\n-rw-rw-r-- 1 demon demon  2990571 Feb 21 00:45 vdb_chunks.json\n-rw-rw-r-- 1 demon demon 63281103 Feb 21 00:45 vdb_entities.json\n-rw-rw-r-- 1 demon demon 46742884 Feb 21 00:45 vdb_relationships.json\n```\n\n\nthe API try to load the  nano vector db and fails with:\n\n```\n                               0)]})\nDEBUG:SSA violators <numba.core.utils.OrderedSet object at 0x7f98982b8580>\nINFO:Loaded graph from /home/demon/try1/LightRAG/mytest/graph_chunk_entity_relation.graphml with 7741 nodes, 5596 edges\nINFO:Load (7596, 1536) data\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/demon/try1/LightRAG/lightrag/api/lightrag_server.py\", line 493, in <module>\n    main()\n  File \"/home/demon/try1/LightRAG/lightrag/api/lightrag_server.py\", line 474, in main\n    app = create_app(args)\n  File \"/home/demon/try1/LightRAG/lightrag/api/lightrag_server.py\", line 320, in create_app\n    rag = LightRAG(\n  File \"<string>\", line 37, in __init__\n  File \"/home/demon/try1/LightRAG/lightrag/lightrag.py\", line 357, in __post_init__\n    self.entities_vdb: BaseVectorStorage = self.vector_db_storage_cls(  # type: ignore\n  File \"/home/demon/try1/LightRAG/lightrag/utils.py\", line 782, in import_class\n    return cls(*args, **kwargs)\n  File \"<string>\", line 8, in __init__\n  File \"/home/demon/try1/LightRAG/lightrag/kg/nano_vector_db_impl.py\", line 43, in __post_init__\n    self._client = NanoVectorDB(\n  File \"<string>\", line 6, in __init__\n  File \"/home/demon/.local/lib/python3.10/site-packages/nano_vectordb/dbs.py\", line 73, in __post_init__\n    storage[\"embedding_dim\"] == self.embedding_dim\nAssertionError: Embedding dim mismatch, expected: 1024, but loaded: 1536\n```\n\ni have a sample code to query the embedings and relationships on /mytest and that code.. works..\n\n```\n$ python3 query_test.py\n.:: LIGHTRAG SAMPLE MOCKUP ::.\n\nLoading Vector Store and Graph Data\n\nLigthRAG debug/info enabled..\nINFO:nano-vectordb:Load (7596, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'mytest/vdb_entities.json'} 7596 dat\na\nINFO:nano-vectordb:Load (5596, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'mytest/vdb_relationships.json'} 559\n6 data\nINFO:nano-vectordb:Load (361, 1536) data\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'mytest/vdb_chunks.json'} 361 data\nINFO:lightrag:Loaded document status storage with 2 records\n --- DATA LOADED ----\nSPENT TIME:  12  seconds to load data\n\nWill answer questions with this modes: ['hybrid', 'mix']\n\n Introduce tu consulta (o escribe 'salir' para terminar): \n\n```\n\nThis time the embedings dimmensions are 1536 and not 1024 as the api expects\n\n\n\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "bzImage",
      "author_type": "User",
      "created_at": "2025-02-24T15:52:42Z",
      "updated_at": "2025-03-02T18:50:53Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/936/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/936",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/936",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:21.279030",
      "comments": [
        {
          "author": "bzImage",
          "body": "--\n\nchecking the code..\n\n  ```\nutils_api.py:    args.embedding_dim = get_env_value(\"EMBEDDING_DIM\", 1024, int)\n```\n\nfound that there is an ENV variable.. EMBEDDING_DIM that i have not set..\n",
          "created_at": "2025-02-24T16:07:44Z"
        },
        {
          "author": "bzImage",
          "body": "even setting the env varialble and hardcoding the value on utils.py.. fails..\n\n```\nG$ cat lightrag/api/utils_api.py |grep EMB\n        default=get_env_value(\"EMBEDDING_BINDING\", \"ollama\"),\n        \"EMBEDDING_BINDING_HOST\", get_default_host(args.embedding_binding)\n    args.embedding_binding_api_key = ",
          "created_at": "2025-02-24T16:13:18Z"
        },
        {
          "author": "bzImage",
          "body": "harcoding the emb dimension on lightrag_server.py\n\n```\n embedding_func = EmbeddingFunc(\n        embedding_dim=1536,\n        max_token_size=args.max_embed_tokens,\n        func=lambda texts: lollms_embed(\n            texts,\n            embed_model=args.embedding_model,\n            host=args.embedding_",
          "created_at": "2025-02-24T16:15:52Z"
        },
        {
          "author": "danielaskdd",
          "body": "> AssertionError: Embedding dim mismatch, expected: 1024, but loaded: 1536\n\nThe embedding model expecting dim to be 1024, you should respect it and set EMBEDDING_DIM=1024",
          "created_at": "2025-03-02T18:50:52Z"
        }
      ]
    },
    {
      "issue_number": 954,
      "title": "[Question]: <title>where is the knowledge graph file graph_chunk_entity_relation.graphml?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am looking for the file graph_chunk_entity_relation.graphml, but did not find it in source codes. It seems that entities and relations are stored in vector storage. not in such as file. Is it right?\n\nhow to generate such a graph file for viewing?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "samuelxhu",
      "author_type": "User",
      "created_at": "2025-02-26T15:16:03Z",
      "updated_at": "2025-03-02T18:26:08Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/954/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/954",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/954",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:21.496275",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "graph_chunk_entity_relation.graphml is in working directory, the default location is rag_storage",
          "created_at": "2025-03-02T18:24:26Z"
        },
        {
          "author": "danielaskdd",
          "body": "You can view the graph by the web-ui provide by LightRAG now. Just bring up the API Server of LightRAG.",
          "created_at": "2025-03-02T18:26:08Z"
        }
      ]
    },
    {
      "issue_number": 951,
      "title": "[Bug]: Issue with Document Processing Parallelization",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nIn the apipeline_process_enqueue_documents method, there's a subtle but significant issue with how asynchronous batch processing is implemented. The line batches.append(batch(batch_idx, docs_batch, len(docs_batches))) doesn't append the coroutine object to be awaited later but rather immediately schedules the function for execution and appends its future result (which will be None).\nThis causes irregular concurrency behavior:\n\nWhen max_parallel_insert is large (e.g., 32) and docs are few (e.g., 10): Documents process sequentially\nWhen max_parallel_insert is small (e.g., 1) and docs are many (e.g., 10): Documents process in parallel\n\nThis is the opposite of the intended behavior, where larger max_parallel_insert should allow more docs to be processed concurrently.\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\nHere is the corrected code :\n\n    async def apipeline_process_enqueue_documents(\n            self,\n            split_by_character: str | None = None,\n            split_by_character_only: bool = False,\n    ) -> None:\n        \"\"\"\n        Process pending documents by splitting them into chunks, processing\n        each chunk for entity and relation extraction, and updating the\n        document status.\n\n        1. Get all pending, failed, and abnormally terminated processing documents.\n        2. Split document content into chunks\n        3. Process each chunk for entity and relation extraction\n        4. Update the document status\n        \"\"\"\n        # 1. Get all pending, failed, and abnormally terminated processing documents.\n        # Run the asynchronous status retrievals in parallel using asyncio.gather\n        processing_docs, failed_docs, pending_docs = await asyncio.gather(\n            self.doc_status.get_docs_by_status(DocStatus.PROCESSING),\n            self.doc_status.get_docs_by_status(DocStatus.FAILED),\n            self.doc_status.get_docs_by_status(DocStatus.PENDING),\n        )\n        to_process_docs: dict[str, DocProcessingStatus] = {}\n        to_process_docs.update(processing_docs)\n        to_process_docs.update(failed_docs)\n        to_process_docs.update(pending_docs)\n        if not to_process_docs:\n            logger.info(\"All documents have been processed or are duplicates\")\n            return\n\n        # Convert the dictionary of documents to a list of (id, status) pairs\n        docs_list = list(to_process_docs.items())\n        total_docs = len(docs_list)\n        logger.info(f\"Processing {total_docs} documents with max parallel {self.max_parallel_insert}\")\n\n        # Process all documents in parallel with concurrency limited by semaphore\n        async def process_document(doc_id: str, status_doc: DocProcessingStatus) -> None:\n            try:\n                # Generate chunks from document\n                chunks: dict[str, Any] = {\n                    compute_mdhash_id(dp[\"content\"], prefix=\"chunk-\"): {\n                        **dp,\n                        \"full_doc_id\": doc_id,\n                    }\n                    for dp in self.chunking_func(\n                        status_doc.content,\n                        split_by_character,\n                        split_by_character_only,\n                        self.chunk_overlap_token_size,\n                        self.chunk_token_size,\n                        self.tiktoken_model_name,\n                    )\n                }\n                # Process document (text chunks and full docs) in parallel\n                tasks = [\n                    self.doc_status.upsert(\n                        {\n                            doc_id: {\n                                \"status\": DocStatus.PROCESSING,\n                                \"updated_at\": datetime.now().isoformat(),\n                                \"content\": status_doc.content,\n                                \"content_summary\": status_doc.content_summary,\n                                \"content_length\": status_doc.content_length,\n                                \"created_at\": status_doc.created_at,\n                            }\n                        }\n                    ),\n                    self.chunks_vdb.upsert(chunks),\n                    self._process_entity_relation_graph(chunks),\n                    self.full_docs.upsert(\n                        {doc_id: {\"content\": status_doc.content}}\n                    ),\n                    self.text_chunks.upsert(chunks),\n                ]\n                await asyncio.gather(*tasks)\n                await self.doc_status.upsert(\n                    {\n                        doc_id: {\n                            \"status\": DocStatus.PROCESSED,\n                            \"chunks_count\": len(chunks),\n                            \"content\": status_doc.content,\n                            \"content_summary\": status_doc.content_summary,\n                            \"content_length\": status_doc.content_length,\n                            \"created_at\": status_doc.created_at,\n                            \"updated_at\": datetime.now().isoformat(),\n                        }\n                    }\n                )\n            except Exception as e:\n                logger.error(f\"Failed to process document {doc_id}: {str(e)}\")\n                await self.doc_status.upsert(\n                    {\n                        doc_id: {\n                            \"status\": DocStatus.FAILED,\n                            \"error\": str(e),\n                            \"content\": status_doc.content,\n                            \"content_summary\": status_doc.content_summary,\n                            \"content_length\": status_doc.content_length,\n                            \"created_at\": status_doc.created_at,\n                            \"updated_at\": datetime.now().isoformat(),\n                        }\n                    }\n                )\n\n        # Create a semaphore to limit concurrency\n        sem = asyncio.Semaphore(self.max_parallel_insert)\n\n        async def process_with_semaphore(doc_id: str, status_doc: DocProcessingStatus) -> None:\n            async with sem:\n                await process_document(doc_id, status_doc)\n\n        # Create tasks for all documents with semaphore control\n        tasks = [process_with_semaphore(doc_id, status_doc) for doc_id, status_doc in docs_list]\n\n        # Process all documents with controlled concurrency\n        await asyncio.gather(*tasks)\n        await self._insert_done()",
      "state": "open",
      "author": "liornabat",
      "author_type": "User",
      "created_at": "2025-02-26T09:37:09Z",
      "updated_at": "2025-03-02T17:48:29Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/951/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/951",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/951",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:21.693352",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "This PR addresses your issue: #969.",
          "created_at": "2025-03-02T17:48:27Z"
        }
      ]
    },
    {
      "issue_number": 972,
      "title": "[Question]: LIGHTRAG_WEBUI | POSTGRES EXISTING STORAGE",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nHow can i view existing storage of postgres on lightrag_webui , how to configure web ui to use existing storage , any one having any idea ?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "imraninfuseai",
      "author_type": "User",
      "created_at": "2025-03-01T19:46:49Z",
      "updated_at": "2025-03-02T11:10:29Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/972/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/972",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/972",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:21.851887",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "You can put you PostgreSQL connection params in to .env file. There is a sample .env file in to root directory of the project .env.example",
          "created_at": "2025-03-02T11:10:28Z"
        }
      ]
    },
    {
      "issue_number": 922,
      "title": "Inquiry About Multi-hop Query Capability",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n Hi LightRAG Team,\n\nThanks for the great work on LightRAG! I’m exploring its retrieval mechanism and was curious about how it handles **multi-hop queries** in the knowledge graph.\n\n### **Question:**\n\nFrom my understanding, functions like `_find_most_related_edges_from_entities()` in `operate.py` focus on retrieving **one-hop neighbors** when querying related entities. I was wondering if LightRAG currently supports or plans to support **multi-hop expansion**, where queries could traverse multiple relationships to retrieve deeper contextual information.",
      "state": "closed",
      "author": "sjx07",
      "author_type": "User",
      "created_at": "2025-02-22T09:09:40Z",
      "updated_at": "2025-03-02T07:52:00Z",
      "closed_at": "2025-03-02T07:51:59Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/922/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/922",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/922",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:22.035926",
      "comments": [
        {
          "author": "LarFii",
          "body": "We have plans to explore multi-hop reasoning, but we have not yet found a suitable implementation approach.",
          "created_at": "2025-02-25T04:11:53Z"
        },
        {
          "author": "sjx07",
          "body": "thanks!",
          "created_at": "2025-03-02T07:51:59Z"
        }
      ]
    },
    {
      "issue_number": 881,
      "title": "[Bug]: LightRAG WebUI doen't work with LightRAG-Server",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nHi,\n\nI’m launching the LightRAG server and trying to access the WebUI, but it isn’t working. I have no access to the interface.\n\n### Steps to reproduce\n\n- Launch LightRAG-Server\n- http://localhost:9626/webui\n\n### Expected Behavior\n\nSee webui\n\n### LightRAG Config Used\n\n- Last version on Master\n\n### Additional Information\n\n- LightRAG Version: Last version\n- Operating System: Docker\n- Python Version: 3.10\n- Related Issues: NA\n",
      "state": "closed",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-19T20:44:00Z",
      "updated_at": "2025-03-01T20:16:17Z",
      "closed_at": "2025-03-01T20:16:17Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/881/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/881",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/881",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:22.205513",
      "comments": [
        {
          "author": "ArnoChenFx",
          "body": "I built the image using the latest main branch, but unable to reproduce the issue.\nHere is my `compose.yaml`:\n```compose.yaml\nservices:\n  lightrag:\n    container_name: lightrag\n    image: arnochen/lightrag:latest\n    restart: always\n    ports:\n      - 9621:9621\n    volumes:\n      - ./data/rag_storag",
          "created_at": "2025-02-20T13:01:20Z"
        },
        {
          "author": "gsaluncf",
          "body": "I had the same problem but for various reasons can't use docker.\nI found that the default media type for the /webui was coming back as \"text/html\" by looking at the console in my browser.\nI changed the definition of the root in `lightrag_server.py` to:\n```\n@app.get(\"/webui/\")\n    async def webui_roo",
          "created_at": "2025-03-01T19:26:55Z"
        }
      ]
    },
    {
      "issue_number": 959,
      "title": "[Question]: <title>For production use case, which is the preferable stable lightRAG version that can be used? Any solution for data insertion issues in azure postgreSQL and slow query time ?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI am using lightrag with azure postgresql. While indexing it is showing error while edges merging. Any solution or suggestion for not getting this? Even with less documents it is giving this error.\nError:\npy\", line 523, in extract_entities\n    all_relationships_data.append(await result)\n  File \"/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n    return f.result()  # May raise f.exception().\n  File \"/lib/python3.10/asyncio/futures.py\", line 201, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n    result = coro.send(None)\n  File \"/lib/python3.10/site-packages/lightrag/operate.py\", line 241, in _merge_edges_then_upsert\n    already_weights.append(already_edge[\"weight\"])\nTypeError: 'NoneType' object is not subscriptable\n \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "poojatambe",
      "author_type": "User",
      "created_at": "2025-02-27T13:06:02Z",
      "updated_at": "2025-03-01T19:41:33Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/959/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/959",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/959",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:22.410156",
      "comments": [
        {
          "author": "poojatambe",
          "body": "Hi,\n\n**LightRAG version:  v1.1.5.**\n**Storage: Azure PostgreSQL**\n\n- Facing slow query execution time issue(greater than  10 mins). Any suggestion for getting faster response?\n- Suggest stable LightRAG version that can be used for production use case?\n- Facing data insertion issue of None type objec",
          "created_at": "2025-02-28T12:37:13Z"
        },
        {
          "author": "imraninfuseai",
          "body": "> ### Do you need to ask a question?\n> * [x]  I have searched the existing question and discussions and this question is not already answered.[ ]  I believe this is a legitimate question, not just a bug or feature request.\n> \n> ### Your Question\n> I am using lightrag with azure postgresql. While ind",
          "created_at": "2025-03-01T19:41:32Z"
        }
      ]
    },
    {
      "issue_number": 965,
      "title": "[Question]: <title>",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n为什么没有找到实验部分的代码\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "1hfq",
      "author_type": "User",
      "created_at": "2025-02-28T06:19:25Z",
      "updated_at": "2025-03-01T08:35:45Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/965/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/965",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/965",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:22.600597",
      "comments": [
        {
          "author": "LarFii",
          "body": "在reproduce文件夹下，可以根据readme中的步骤复现",
          "created_at": "2025-03-01T08:35:43Z"
        }
      ]
    },
    {
      "issue_number": 796,
      "title": "Bug: adelete_by_doc_id function fails to execute",
      "body": "I tested the delete function, but it still doesn't seem to work. It always falls into the case where the text chunk has len == 0.\n\nThen, I tried modifying doc_id.replace(\"doc\", \"chunk\"), and the result looked like this, but I couldn't find any enties_vdb linked to the chunk.\n\n![Image](https://github.com/user-attachments/assets/5c5337a2-6978-4860-96dc-3326419546d2)\n\n![Image](https://github.com/user-attachments/assets/f3e59b79-dae7-4e4d-9c34-1f41ec295857)",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-02-16T12:42:52Z",
      "updated_at": "2025-02-27T10:08:19Z",
      "closed_at": null,
      "labels": [
        "bug",
        "neo4j",
        "chroma"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/796/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/796",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/796",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:22.776609",
      "comments": [
        {
          "author": "YanSte",
          "body": "Yes, they is a bug fixed in the next version.",
          "created_at": "2025-02-16T13:23:02Z"
        },
        {
          "author": "YanSte",
          "body": "I checked but didn’t find anything. Could you investigate further and provide more context?",
          "created_at": "2025-02-17T22:03:37Z"
        },
        {
          "author": "newbie-Li",
          "body": "I guess maybe you use `insert_custom_kg` function to add data\nI've compared the result between `insert` and `insert_custom_kg`, modify the code about doc id, chunk id, source id, and then delete_by_doc_id works\n\nanother change: delete relation before entity",
          "created_at": "2025-02-18T08:51:39Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "\n\n\n\n\n\n\n\n\n\n> I guess maybe you use `insert_custom_kg` function to add data I've compared the result between `insert` and `insert_custom_kg`, modify the code about doc id, chunk id, source id, and then delete_by_doc_id works\n> \n> another change: delete relation before entity\n\nI use ainsert to insert, ",
          "created_at": "2025-02-18T11:27:44Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "@YanSte |\nAs described, I couldn't find the chunk based on the doc-id.\nAfter changing doc to chunk, it seems like the chunk was found, but I couldn't find the edge and relationships.\n\n![Image](https://github.com/user-attachments/assets/a42b0d54-38ff-47a1-bd18-385f1a768413)\n\n![Image](https://github.c",
          "created_at": "2025-02-18T14:14:44Z"
        }
      ]
    },
    {
      "issue_number": 929,
      "title": "[Feature Request]: Being able to add youtube transcripts and also simple text",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\n@ArnoChenFx I think this can be a cool addition so tha we get somethjing like NotebookLM's style where we can add a youtube video transcript (i know how to do that, I already did it in lollms), and also just being able to send text.\n\nIf you would do the front end coding  of this, I'll do the backend.\nIs that something you would want to do?\n\nAlso, we need to be able to remove documents with a trush button.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-02-23T23:09:03Z",
      "updated_at": "2025-02-25T11:08:19Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/929/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/929",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/929",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:23.017603",
      "comments": [
        {
          "author": "ArnoChenFx",
          "body": "Why not design it to be more universal? Just input a URL, and we can automatically parse out youtube transcript, web page or article content from it.",
          "created_at": "2025-02-24T11:42:28Z"
        },
        {
          "author": "ParisNeo",
          "body": "Nice, let's do that!",
          "created_at": "2025-02-24T12:23:46Z"
        },
        {
          "author": "ParisNeo",
          "body": "Also here are some remarks I found when testing the graph visualization.\nWhen enabling the edge names, I get this:\n![Image](https://github.com/user-attachments/assets/942be5e2-a14c-4e04-b664-c67c4f934343)\n\nI am using the default Networkx version built by @danielaskdd . I followed the code to this:\n`",
          "created_at": "2025-02-24T12:41:14Z"
        },
        {
          "author": "danielaskdd",
          "body": "@ParisNeo I can not find a suitable information in edge_data representing the type of relationship. ",
          "created_at": "2025-02-25T01:05:50Z"
        },
        {
          "author": "danielaskdd",
          "body": "The frontend performance is significantly impacted when retrieving thousands of labels from the backend, resulting in the label dropdown box freezing during user input. To enhance search efficiency and ensure a more meaningful user experience, it is imperative to implement a mechanism allowing users",
          "created_at": "2025-02-25T01:14:21Z"
        }
      ]
    },
    {
      "issue_number": 944,
      "title": "[Bug]: lightrag_lmdeploy_demo.py",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\ncan't run demo lightrag_lmdeploy_demo.py\n\n### Steps to reproduce\n\n    llm_model_name=\"stelterlab/DeepSeek-R1-Distill-Qwen-14B-AWQ\",  # please use definite path for local model\n    return await lmdeploy_model_if_cache(\n        model_name,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        ## please specify chat_template if your local path does not follow original HF file name,\n        ## or model_name is a pytorch model on huggingface.co,\n        ## you can refer to https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/model.py\n        ## for a list of chat_template available in lmdeploy.\n#        chat_template=\"llama3\",\n           chat_template=\"deepseek-r1\",\n        model_format ='awq', # if you are using awq quantization model.\n        # quant_policy=8, # if you want to use online kv cache, 4=kv int4, 8=kv int8.\n        **kwargs,\n    )\n\n\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\npython lightrag_lmdeploy_demo.py \nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_chunks.json'} 0 data\nINFO:lightrag:Loaded document status storage with 0 records\nINFO:lightrag:Inserting 1 to doc_status\nINFO:lightrag:Stored 1 new unique documents\nINFO:lightrag:Number of batches to process: 1.\nINFO:lightrag:Start processing batch 1 of 1.\nINFO:lightrag:Inserting 1 to doc_status\nINFO:lightrag:Inserting 22 to chunks\nINFO:lightrag:Inserting 1 to full_docs\nINFO:lightrag:Inserting 22 to text_chunks\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nFetching 10 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 105649.97it/s]\n[TM][WARNING] [LlamaTritonModel] `max_context_token_num` is not set, default to 131072.\n2025-02-25 08:55:27,687 - lmdeploy - WARNING - turbomind.py:217 - get 915 model params\n[TM][WARNING] Devicle 0 peer access Device 1 is not available.                                                                                                                                          \n[WARNING] gemm_config.in is not found; using default GEMM algo\n[TM][WARNING] No enough blocks for `session_len` (131072), `session_len` truncated to 49792.\n2025-02-25 08:55:41,781 - lmdeploy - WARNING - tokenizer.py:425 - Detected duplicate bos token 151646 in prompt, this will likely reduce response quality, one of them will beremoved\nERROR:lightrag:Failed to extract entities and relationships\nERROR:lightrag:Failed to process document doc-1891b9dbaaaa6aeca22c23e54a8bb33c: \nTraceback (most recent call last):\n  File \"/media/ssd/lyj/LightRAG/examples/lightrag_lmdeploy_demo.py\", line 65, in <module>\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n  File \"/media/ssd/lyj/LightRAG/lightrag/lightrag.py\", line 982, in query\n    return loop.run_until_complete(self.aquery(query, param, system_prompt))  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/media/ssd/lyj/LightRAG/lightrag/lightrag.py\", line 1023, in aquery\n    response = await naive_query(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/lyj/LightRAG/lightrag/operate.py\", line 1619, in naive_query\n    response = await use_model_func(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/lyj/LightRAG/lightrag/utils.py\", line 185, in wait_func\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/lyj/LightRAG/examples/lightrag_lmdeploy_demo.py\", line 23, in lmdeploy_model_complete\n    return await lmdeploy_model_if_cache(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/lyj/LightRAG/lightrag/llm/lmdeploy.py\", line 141, in lmdeploy_model_if_cache\n    async for res in lmdeploy_pipe.generate(\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/lmdeploy/serve/async_engine.py\", line 694, in generate\n    async with self.model_inst(session_id) as inst:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/ssd/miniconda3/envs/ds/lib/python3.12/site-packages/lmdeploy/serve/async_engine.py\", line 561, in model_inst\n    assert session_id not in self.id2inst\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\nAborted (core dumped)\n\n### Additional Information\n\n- LightRAG Version: latest\n- Operating System: ubuntu22\n- Python Version: 3.12\n- Related Issues:\n",
      "state": "open",
      "author": "lyj0309",
      "author_type": "User",
      "created_at": "2025-02-25T08:57:12Z",
      "updated_at": "2025-02-25T08:57:12Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/944/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/944",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/944",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:23.194611",
      "comments": []
    },
    {
      "issue_number": 937,
      "title": "[Question]: Faiss with local embedding model (on Ollama)",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI would like to use a language specific embedding model locally running on Ollama with faiss vector storage. \nThe documentation shows examples with `sentence-transformers` but also mentions `OpenAIEmbedding`. All my attempts to use a local model have failed so far. So is it generally possible or is the use of faiss currently restricted to these two?\nThanks in advance!\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "TimmLehmberg",
      "author_type": "User",
      "created_at": "2025-02-24T17:27:37Z",
      "updated_at": "2025-02-25T04:08:07Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/937/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/937",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/937",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:23.194631",
      "comments": [
        {
          "author": "LarFii",
          "body": "Yes, using a local model is possible. You just need to replace both the LLM and the embedding model with local versions.",
          "created_at": "2025-02-25T04:08:06Z"
        }
      ]
    },
    {
      "issue_number": 938,
      "title": "[Question]: db empty and insert_custom_chunks shows \"This document is already in the storage.\"",
      "body": "### Do you need to ask a question?\n\n- [ ] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nI found that adding insert_custom_chunks issued with a wiped db issued the warning:\nlogger.warning(\"This document is already in the storage.\")\n\nSo that change of \nlightrag.py, line 684, \nmade it work...\n            # _add_doc_keys = await self.full_docs.filter_keys(set(doc_key))\n            _add_doc_keys = await self.full_docs.filter_keys(set([doc_key]))\notherwise, the set contains the single chars of the string and not the string in a set.\nIm not sure if this was a general bugfix or if I destroyed other cases with that change... \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "bennoloeffler",
      "author_type": "User",
      "created_at": "2025-02-24T20:47:23Z",
      "updated_at": "2025-02-25T04:06:28Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/938/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/938",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/938",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:23.398209",
      "comments": [
        {
          "author": "LarFii",
          "body": "Thanks! This issue was related to a previous version, and it has already been fixed in the latest version.",
          "created_at": "2025-02-25T04:06:27Z"
        }
      ]
    },
    {
      "issue_number": 749,
      "title": "Unexpected Entity Extraction Results",
      "body": "I encountered unexpected results during the entity extraction process while creating a RAG (Retrieval-Augmented Generation) setup. Below are the details of the issue.\n\n### Input String:\n```text\nCVE-2019-13298 is a vulnerability with title ImageMagick: heap-based buffer overflow at MagickCore/pixel-accessor.h in SetPixelViaPixelInfo because of a MagickCore/enhance.c error. There are several functions related to this vulnerability:\\n    GetPixelChannel: Retrieves the value of a specified pixel channel for a given pixel in an image.\\nSetPixelViaPixelInfo: Sets the pixel values of an image based on a provided PixelInfo structure.\\n    CVE-2019-13298 is related to the following CWEs:['CWE-119', 'CWE-787']\\n    CVE-2019-13298 is caused by the following flaws:An out-of-bounds read vulnerability in ImageMagick could be triggered by malformed image files, leading to a denial of service or potential arbitrary code execution.: Improper validation of pixel channel traits can lead to incorrect accessing and manipulating pixel data, potentially allowing an attacker to read memory out-of-bounds or manipulate the program's execution flow.\\nIn 'pixel-accessor.h', accessing a nested index from the `channel_map` array without proper bounds checking can access invalid memory regions if certain conditions are met.\\nIn 'enhance.c', writing pixels into an incorrect image context ('image' instead of 'enhance_image') may lead to writing outside the valid memory region allocated for the target image, causing undefined behavior.\\n    CVE-2019-13298 is fixed by deploying the following patch methods:Changed the incorrect nested index access to the correct single index access. This ensures that only valid channel indices are accessed, preventing out-of-bounds accesses.\\nCorrected the target image for pixel writing from 'image' to 'enhance_image'. This ensures that pixels are written into the correctly allocated and initialized memory region, avoiding out-of-bounds writes.\n```\n\n### Configuration:\nThe following configuration was used to create the RAG and process the input string:\n```python\nrag = LightRAG(\n    working_dir=f\"./{target_db.value}\",\n    llm_model_func=ollama_model_complete,\n    llm_model_name=llm_model,\n    llm_model_kwargs={\"host\": llm_host, \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts, embed_model=embed_model, host=llm_host\n        ),\n    ),\n)\n```\n\n### Observed Results:\nEven though the entire input string was saved in `kv_store_full_docs.json` and `kv_store_text_chunks.json`, the extracted entities and relationships were fewer than expected. Below are the observed results:\n\n#### Extracted Entities (`vdb_entities.json`):\n```json\n{\n    \"embedding_dim\": 1024,\n    \"data\": [\n        { \"__id__\": \"ent-e6944e9709302ce3172c8974727fb6ee\", \"entity_name\": \"\\\"CVE-2019-13298\\\"\", \"__created_at__\": 1739284454.4860775 },\n        { \"__id__\": \"ent-c5a00d7311222c2f497d9899bdc9a86e\", \"entity_name\": \"\\\"CWE-119\\\"\", \"__created_at__\": 1739284454.4860775 },\n        { \"__id__\": \"ent-81dd040dcc92b4c0307feb1bf9d2c1ef\", \"entity_name\": \"\\\"GETPIXELCHANNEL\\\"\", \"__created_at__\": 1739284454.4860775 },\n        { \"__id__\": \"ent-13510710ad3725e39bd823028caadc3e\", \"entity_name\": \"\\\"MAGICKCORE/ENHANCE.C\\\"\", \"__created_at__\": 1739284454.4860775 },\n        { \"__id__\": \"ent-25d488e168874feadb164b1ac5ac809f\", \"entity_name\": \"\\\"CWE-787\\\"\", \"__created_at__\": 1739284454.4860775 },\n        { \"__id__\": \"ent-50666aa9e106f98027f46fe20d306ba9\", \"entity_name\": \"\\\"IMAGEMAGICK\\\"\", \"__created_at__\": 1739284454.4860775 },\n        { \"__id__\": \"ent-0c8a115f1daa067b7dae5e4a806d95fa\", \"entity_name\": \"\\\"SETPIXELVIAPIXELINFO\\\"\", \"__created_at__\": 1739284454.4860775 },\n        { \"__id__\": \"ent-ad8fc863b1b10d970bad66c8ac3bedf3\", \"entity_name\": \"\\\"MAGICKCORE/PIXEL-ACCESSOR.H\\\"\", \"__created_at__\": 1739284454.4860775 }\n    ],\n    \"matrix\": \"...\"\n}\n```\n\n#### Extracted Relationships (`vdb_relationships.json`):\n```json\n{\n    \"embedding_dim\": 1024,\n    \"data\": [\n        { \"__id__\": \"rel-241af0a5a43a3c40f9e7d00deed40b7a\", \"src_id\": \"\\\"CVE-2019-13298\\\"\", \"tgt_id\": \"\\\"GETPIXELCHANNEL\\\"\" },\n        { \"__id__\": \"rel-6c65e30971fbfe26f53f6509aa22f159\", \"src_id\": \"\\\"CVE-2019-13298\\\"\", \"tgt_id\": \"\\\"CWE-119\\\"\" },\n        { \"__id__\": \"rel-2ce8a955d926cfb585339b272d8428dc\", \"src_id\": \"\\\"CVE-2019-13298\\\"\", \"tgt_id\": \"\\\"MAGICKCORE/PIXEL-ACCESSOR.H\\\"\" },\n        { \"__id__\": \"rel-9178d458ff5ec2bc9eb26ccfabaaef90\", \"src_id\": \"\\\"CVE-2019-13298\\\"\", \"tgt_id\": \"\\\"MAGICKCORE/ENHANCE.C\\\"\" },\n        { \"__id__\": \"rel-4345165bf43641c94a6014f0ae302c15\", \"src_id\": \"\\\"CVE-2019-13298\\\"\", \"tgt_id\": \"\\\"CWE-787\\\"\" },\n        { \"__id__\": \"rel-0980a37a5ca9cb7f0f623e8d3d269df7\", \"src_id\": \"\\\"CVE-2019-13298\\\"\", \"tgt_id\": \"\\\"SETPIXELVIAPIXELINFO\\\"\" },\n        { \"__id__\": \"rel-d02fea3ae52c3546c364764f188b7831\", \"src_id\": \"\\\"CVE-2019-13298\\\"\", \"tgt_id\": \"\\\"IMAGEMAGICK\\\"\" }\n    ],\n    \"matrix\": \"...\"\n}\n```\n\n### Issue:\n- The extracted entities seem incomplete compared to the input string. For example, several key terms such as `enhance_image`, `channel_map`, `PixelInfo`, and others were not extracted as entities.\n- This incomplete extraction may lead to suboptimal query and generation performance, as critical details are potentially missing from the knowledge graph.\n\n### Questions:\n1. Is the observed result correct given the provided configuration and input?\n2. Could there be a misconfiguration or an issue with the entity recognition logic in the RAG pipeline?\n3. Are there any recommendations for improving entity extraction to ensure all relevant terms are captured?",
      "state": "open",
      "author": "LouisLiuNova",
      "author_type": "User",
      "created_at": "2025-02-11T15:08:14Z",
      "updated_at": "2025-02-25T02:35:12Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/749/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/749",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/749",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:23.650342",
      "comments": [
        {
          "author": "bzImage",
          "body": "Just starting with lightrag.. and i see you are trying to extract cybersec entities.. actor, hash, cve, etc..(trying to do the same) did you create a special prompt to extract those entities ?",
          "created_at": "2025-02-18T15:48:13Z"
        },
        {
          "author": "LouisLiuNova",
          "body": "> Just starting with lightrag.. and i see you are trying to extract cybersec entities.. actor, hash, cve, etc..(trying to do the same) did you create a special prompt to extract those entities ?\n\nI am currently using the default settings only, and I have not created a custom prompt for this.",
          "created_at": "2025-02-19T09:34:02Z"
        },
        {
          "author": "LarFii",
          "body": "I'm not sure about the specific model being used, but the process of entity extraction mainly depends on the capabilities of the LLM.",
          "created_at": "2025-02-20T03:08:12Z"
        },
        {
          "author": "LouisLiuNova",
          "body": "> I'm not sure about the specific model being used, but the process of entity extraction mainly depends on the capabilities of the LLM.\n\nI utilized Qwen2.5:32B for entity extraction. However, the problem persists when switching to other models.",
          "created_at": "2025-02-25T02:35:11Z"
        }
      ]
    },
    {
      "issue_number": 903,
      "title": "[Bug]: Crash: coroutine 'JsonKVStorage.upsert' was never awaited",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nCrash: coroutine 'JsonKVStorage.upsert' was never awaited\n- MacOS\n- Python 3.10\n- I cloned lightrag.\n- addes some files.\n- worked\n- added some more.\n- some of them where still pending\n- wanted to clear documents\nEverything went well - but when I clear documents, this happens:\n\n/Users/benno/projects/ai/LightRAG_PURE/lightrag/lightrag.py:846: RuntimeWarning: coroutine 'JsonKVStorage.upsert' was never awaited\n  self.text_chunks.upsert(chunks),\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n/Users/benno/projects/ai/LightRAG_PURE/lightrag/lightrag.py:846: RuntimeWarning: coroutine 'LightRAG._process_entity_relation_graph' was never awaited\n  self.text_chunks.upsert(chunks),\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n/Users/benno/projects/ai/LightRAG_PURE/lightrag/lightrag.py:846: RuntimeWarning: coroutine 'NanoVectorDBStorage.upsert' was never awaited\n  self.text_chunks.upsert(chunks),\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:Error indexing file AMF-Katalog-Vakuumspannsysteme.pdf.md.step05.ANALYSED_CHUNKS.json.txt: 'list' object has no attribute 'upsert'\nERROR:Traceback (most recent call last):\n  File \"/Users/benno/projects/ai/LightRAG_PURE/lightrag/api/lightrag_server.py\", line 1245, in pipeline_index_file\n    await rag.apipeline_process_enqueue_documents()\n  File \"/Users/benno/projects/ai/LightRAG_PURE/lightrag/lightrag.py\", line 846, in apipeline_process_enqueue_documents\n    self.text_chunks.upsert(chunks),\nAttributeError: 'list' object has no attribute 'upsert'. Did you mean: 'insert'?\n\n### Steps to reproduce\n\n- clone fresh lightrag.\n- added some files.\n- was asking qestions: worked\n- added some more files.\n- some of them where still pending\n- wanted to clear documents (wanted to remove the pending ones... right?) \n\nThe system was unstable, no more files could be added.\n\n### Expected Behavior\n\n- (at least) pending files should be removed from pending waiting list\n- or all files should be removed from data\n\n### LightRAG Config Used\n\n# Paste your config here\nINFO:Logger initialized for working directory: /Users/benno/projects/ai/LightRAG_PURE/rag_storage\nDEBUG:LightRAG init with param:\n  working_dir = /Users/benno/projects/ai/LightRAG_PURE/rag_storage,\n  embedding_cache_config = {'enabled': True, 'similarity_threshold': 0.95, 'use_llm_check': False},\n  kv_storage = JsonKVStorage,\n  vector_storage = NanoVectorDBStorage,\n  graph_storage = NetworkXStorage,\n  doc_status_storage = JsonDocStatusStorage,\n  log_level = DEBUG,\n  log_dir = /Users/benno/projects/ai/LightRAG_PURE,\n  chunk_token_size = 1200,\n  chunk_overlap_token_size = 100,\n  tiktoken_model_name = gpt-4o-mini,\n  entity_extract_max_gleaning = 1,\n  entity_summary_to_max_tokens = 500,\n  node_embedding_algorithm = node2vec,\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\n  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function create_app.<locals>.<lambda> at 0x16829b910>},\n  embedding_batch_num = 32,\n  embedding_func_max_async = 16,\n  llm_model_func = <function create_app.<locals>.openai_alike_model_complete at 0x125481870>,\n  llm_model_name = gpt-4o-mini,\n  llm_model_max_token_size = 32768,\n  llm_model_max_async = 4,\n  llm_model_kwargs = {'timeout': None},\n  vector_db_storage_cls_kwargs = {'cosine_better_than_threshold': 0.2},\n  namespace_prefix = ,\n  enable_llm_cache = True,\n  enable_llm_cache_for_entity_extract = False,\n  addon_params = {},\n  auto_manage_storages_states = False,\n  convert_response_to_json_func = <function convert_response_to_json at 0x102efc040>,\n  chunking_func = <function chunking_by_token_size at 0x105481bd0>\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version: v1.0.5\n- Operating System: MacOS\n- Python Version: 3.10\n- Related Issues:\n",
      "state": "open",
      "author": "bennoloeffler",
      "author_type": "User",
      "created_at": "2025-02-20T13:35:55Z",
      "updated_at": "2025-02-24T20:41:26Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/903/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/903",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/903",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:23.931595",
      "comments": [
        {
          "author": "bennoloeffler",
          "body": "having added this:\n\nnumpy==1.26.4\nfuture\n\nto requirements.txt\n\nand \ndowngrading pip to version pip 23.0.1\nbecause with version 24 dependency-problems came up\n\nGot the impression, that made the problems disappear.",
          "created_at": "2025-02-24T20:41:23Z"
        }
      ]
    },
    {
      "issue_number": 935,
      "title": "[Question]: Has anyone else experienced excessive API calls during indexing?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\n**Has anyone else experienced excessive API calls during indexing?** \n\n### Additional Context\n\nI have been experiencing cases where I am hitting API limits and costs are 100x more expensive per document inexed. I switched to different model, adjusted chunking parameters but nothing worked. ",
      "state": "open",
      "author": "FarisLab",
      "author_type": "User",
      "created_at": "2025-02-24T14:09:24Z",
      "updated_at": "2025-02-24T17:30:12Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/935/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/935",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/935",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:24.174439",
      "comments": []
    },
    {
      "issue_number": 907,
      "title": "[Bug]: RuntimeError: This event loop is already running",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n- LightRAG Version: current..\n- Operating System:\n- Python Version:\n- Related Issues:\n\ngit status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\n```\nCreating LightRAG Instance....\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir/vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir/vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './local_neo4jWorkDir/vdb_chunks.json'} 0 data\nINFO:lightrag:Loaded document status storage with 1 records\nTraceback (most recent call last):\n  File \"/home/demon/LightRAG/test_neo4j.py\", line 66, in <module>\n    rag = asyncio.run(initialize_rag())\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/home/demon/LightRAG/test_neo4j.py\", line 53, in initialize_rag\n    return  LightRAG(\n  File \"<string>\", line 37, in __init__\n  File \"/home/demon/LightRAG/lightrag/lightrag.py\", line 407, in __post_init__\n    loop.run_until_complete(self.initialize_storages())\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 625, in run_until_complete\n    self._check_running()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 584, in _check_running\n    raise RuntimeError('This event loop is already running')\nRuntimeError: This event loop is already running\nINFO:lightrag:Creating a new event loop in main thread.\n```\n----\n\nTraced back to this code snippet:\n\n```\n return  LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model\n        embedding_func=embedding_func_instance,   # define ebeddings function\n        graph_storage=\"Neo4JStorage\"\n        #log_level=\"INFO\"\n        # llm_model_func=gpt_4o_complete  # Optionally, use a stronger model\n        )\n\n```\n",
      "state": "closed",
      "author": "bzImage",
      "author_type": "User",
      "created_at": "2025-02-20T22:57:29Z",
      "updated_at": "2025-02-24T14:17:20Z",
      "closed_at": "2025-02-21T17:36:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/907/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/907",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/907",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:24.174458",
      "comments": [
        {
          "author": "ghost",
          "body": "Add below lines and try\n\nimport nest_asyncio\nnest_asyncio.apply()",
          "created_at": "2025-02-21T03:46:00Z"
        },
        {
          "author": "bzImage",
          "body": "yep that fixed it.. i have another issue but is not related to this.. so thanks.. i close this incident now..\n",
          "created_at": "2025-02-21T17:36:18Z"
        },
        {
          "author": "fatehss",
          "body": "This error is still happening for me despite adding those lines above. I'm running lightrag in a async background task in fastapi. It works when it's not in FastAPI and call lightrag outside it",
          "created_at": "2025-02-21T22:43:40Z"
        },
        {
          "author": "ArindamRoy23",
          "body": "Error is still in fast api \n\n`    def __del__(self):\n        # Finalize storages\n        if self.auto_manage_storages_states:\n            loop = always_get_an_event_loop()\n            loop.run_until_complete(self.finalize_storages())`\n\nThis function in lightrag.py seems to be the issue ",
          "created_at": "2025-02-24T14:00:24Z"
        },
        {
          "author": "ArindamRoy23",
          "body": "Ok, so not sure what the exact issue is, but looks like the GC tries to start another loop.\n\n```\n# Adding loop asyncio fixed \nuvicorn main:app --port 8000 --reload --loop asyncio \n\n# with \n\nimport nest_asyncio\nnest_asyncio.apply()\n\n# fixed it for me.  \n```",
          "created_at": "2025-02-24T14:17:18Z"
        }
      ]
    },
    {
      "issue_number": 934,
      "title": "[Bug]: <title> Changing the direction of edges given by llm",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/677a62eb-c958-4923-8713-2b2436c04c7e)\n\nthis image is from extract_entities function from oparate.py \n\nhere the tuple (source entity, target entity) is sorted according to alphabetical order. This may change the direction of edge decided by llm. This will result in, many entities (mostly starting with U, V or S etc, i.e, end alphabets of English alphabets) having no edges or very less edges, from themselves to some other entity. Because after sorting most of the time they will come at second place ie, the target entity\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### LightRAG Config Used\n\n# Paste your config here\n\n\n### Logs and screenshots\n\n_No response_\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "open",
      "author": "Aryabhattacharjee",
      "author_type": "User",
      "created_at": "2025-02-24T12:31:22Z",
      "updated_at": "2025-02-24T12:34:12Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/934/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/934",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/934",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:26.246347",
      "comments": []
    },
    {
      "issue_number": 926,
      "title": "[Question]: Support rewirte function adelete_by_doc_id",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n\n---\n\n**Hello everyone,**  \n\nI am currently researching and rewriting the `adelete_by_doc_id` function, but it is not working as expected.  \n\nWhen I insert data, the following files are generated:  \n- `kv_store_doc_status.json`  \n- `kv_store_full_docs.json`  \n- `kv_store_llm_response_cache.json`  \n- `kv_store_text_chunks.json`  \n- `vdb_chunks.json`  \n- `vdb_entities.json`  \n- `vdb_relationships.json`  \n\nHowever, after performing a delete operation, only `kv_store_doc_status.json` and `kv_store_full_docs.json` are cleared.  \nThe other files, such as `kv_store_text_chunks.json`, `vdb_chunks.json`, `vdb_entities.json`, and `vdb_relationships.json`, still retain data.  \n\nWhen I execute the delete function, the log output looks like this:  \n\n```\nEntity \"CHƯƠNG TRÌNH HỖ TRỢ SINH VIÊN KHUYẾT TẬT\" will be updated with new source_id: chunk-162bce74c760fe66c99533a859f0a99\n...\nRelationship \"ĐẠI HỌC FPT\"-\"HỌC PHÍ\" will be updated with new source_id: chunk-0394bc729a1a95b5ed78520c45a90f0f<SEP>chunk-07faeca75ea529d68bebc9e1091eec8d<SEP>chunk-f39536497bcaec92d0ac376e4b61683a<SEP>chunk-4362f624b29f410655a1368526e76f77<SEP>chunk-c39de74fa236f94e2d915488411f73e0<SEP>chunk-490fa4d198f8570338fe53b9743a508c<SEP>chunk-e73aeb5552144dcf82d5fbc81212cdf2<SEP>chunk-62e09ccb2c3dd69d68ceba6a3cc955b3<SEP>chunk-2f7b29bb73884c282149cce1dceda279<SEP>chunk-0ac6c7ccc71c9a895860e2c25ab1c2de<SEP>chunk-c0806ff190e6a16a376d3227cde6624d<SEP>chunk-e107bba564834ace74c34f2be9e7627e<SEP>chunk-137923f6465df5c0693bb192b9f00f08<SEP>chunk-2b335583f7fd18045987663ac98a9319<SEP>chunk-bf19d4b9a24dbc166ffff75f36238200<SEP>chunk-d914e3cf67ebc677cd073f8d98c8a370<SEP>chunk-963a8aded463f18dc584fe3cce413325<SEP>chunk-c3e78e09392f89cb9794a172a16fca5d\n...\nUpdated entity \"CÂU HỎI 5\" with new source_id: chunk-102a463cac1fa32c7a584bac79b093ca\n...\nUpdated relationship \"TOP 40 THPT NĂM 2025\"-\"ĐIỂM LỚP 11\" with new source_id: chunk-65890d78138df035f0f1b26eacbacc4a\n...\n```\n\nIt seems like the **chunks are not being deleted from the database**.  \n\nI have already created an issue for this, but it has not been fixed yet.  \nSince I frequently need to delete documents and update them for my project, I usually have to **wipe all data and reinsert everything**, which is very costly and time-consuming.  \n\nTo fix this, I am investigating and modifying the function. I suspect that the issue is with this line:  \n\n```python\nchunks = await self.text_chunks.get_by_id(doc_id)\n```\n\nI replaced `\"doc\"` with `\"chunk\"` to see if it works:  \n\n```python\nchunks = await self.text_chunks.get_by_id(doc_id.replace(\"doc\", \"chunk\"))\n```\n\nHowever, the **chunks are still not being deleted from the database**.  \n\nIf anyone has experience with this issue, please share your insights.  \nI would greatly appreciate any help!  \n\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-02-23T10:32:24Z",
      "updated_at": "2025-02-24T09:54:54Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/926/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/926",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/926",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:26.246382",
      "comments": [
        {
          "author": "chain-qq",
          "body": "me too",
          "created_at": "2025-02-24T09:54:52Z"
        }
      ]
    },
    {
      "issue_number": 927,
      "title": "[Question]: What is the purpose of returning the sources in query?",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWhen aquery is performed it eventually has a context that is supplied to the LLM response prompt that includes graph entities, relations and sources (in mix you also have vector db chunks). LLM has no understanding of how entities or relations relate to the sources as there are no \"foreign keys\" so I wonder if I am missing something here? It does not make sense to include the sources as they are now included\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "abylikhsanov",
      "author_type": "User",
      "created_at": "2025-02-23T10:50:28Z",
      "updated_at": "2025-02-24T09:39:44Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/927/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/927",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/927",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:26.428825",
      "comments": [
        {
          "author": "ParisNeo",
          "body": "@LarFii Woudln't it be a good idea either to add some links between the entities and the sources or even has an option to actually just provide the entities and links, no sources.",
          "created_at": "2025-02-23T23:06:23Z"
        },
        {
          "author": "LarFii",
          "body": "> [@LarFii](https://github.com/LarFii) Woudln't it be a good idea either to add some links between the entities and the sources or even has an option to actually just provide the entities and links, no sources.\n\nI've considered this before. My idea was to add it as metadata to the edges and nodes.",
          "created_at": "2025-02-24T09:39:41Z"
        }
      ]
    },
    {
      "issue_number": 825,
      "title": "如果希望向量库和图数据库都从外部导入，只利用项目的综合检索问答能力，应该怎样对接？",
      "body": "因为有很多内容不能被大模型正确地解析为三元组，我们需要人工整理好并存储起来。对应的文本也可以自己解决向量存储问题。完全不需要用到项目自身的文件和文本导入功能。\n但问答的时候希望利用到图数据库的信息辅助。那么具体在外部数据库的存储方面，跟本项目的问答功能怎样对接？\n我使用的是neo4j和qdrant，简单改了一下config.ini配置信息，无法正常工作。\nc:\\*****\\LightRAG\\lightrag\\kg\\qdrant_impl.py:71: UserWarning: Api key is used with an insecure consecure connection.\n  self._client = QdrantClient(\nSorry, I'm not able to provide an answer to that question.[no-context]\n\n",
      "state": "closed",
      "author": "yaleimeng",
      "author_type": "User",
      "created_at": "2025-02-18T01:00:55Z",
      "updated_at": "2025-02-24T05:44:48Z",
      "closed_at": "2025-02-24T05:44:29Z",
      "labels": [
        "qdrant"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/825/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/825",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/825",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:26.633589",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi,\nCould you please check your Api key? Thanks",
          "created_at": "2025-02-18T13:16:18Z"
        },
        {
          "author": "yaleimeng",
          "body": "我使用custom_KG 将数据存入，已经可以正常问答。 不过我用的是默认nanovec存储。",
          "created_at": "2025-02-24T05:44:29Z"
        }
      ]
    },
    {
      "issue_number": 919,
      "title": "[Feature Request]: Use batches when upset edges and nodes in neo4j",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nIt is necessary to add a function to the Neo4j implementation that allows inserting multiple nodes and edges in a single batch.\n\nThis is needed to reduce network load and avoid errors related to connection pool overflow.\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ultrageopro",
      "author_type": "User",
      "created_at": "2025-02-21T23:00:32Z",
      "updated_at": "2025-02-23T18:24:49Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/919/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/919",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/919",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:26.856016",
      "comments": [
        {
          "author": "acsangamnerkar",
          "body": "+1\n\nWithout this feature, files with 80+ chunks most of the time end up failing.",
          "created_at": "2025-02-23T18:24:48Z"
        }
      ]
    },
    {
      "issue_number": 924,
      "title": "[Feature Request]: <title>How to Integrate Real-Time Data from Web Search API in LightRAG for Document Q&A",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nHi, I’m currently using LightRAG to implement a document Q&A system. In addition to associating with a local knowledge base, I’d like to integrate real-time data from a web search API. Could you please guide me on where and how I should incorporate the real-time data retrieval in the code? Specifically, which part of the code should handle the API calls and merge the results with the local knowledge base?\nThank you for your help!\n\n### Additional Context\n\nI’ve tried using conversation_history and system_prompt to handle this, but the results haven’t been satisfactory. ",
      "state": "open",
      "author": "chain-qq",
      "author_type": "User",
      "created_at": "2025-02-23T03:09:10Z",
      "updated_at": "2025-02-23T03:09:10Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/924/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/924",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/924",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:27.066153",
      "comments": []
    },
    {
      "issue_number": 917,
      "title": "[Bug]: Cannot read file",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\nAn error occourred when uploading a file.\n\n### Steps to reproduce\n\nSelect tab \"Documents\" and try to upload any files.\n\n### Expected Behavior\n\nUpload failure and error was throughout in the logs\n\n### LightRAG Config Used\n\n# Paste your config here\n\nroot@Debian12-Template:/opt/lightrag# cat docker-compose.yml\nvolumes:\n  # PostgreSQL 数据卷\n  lightrag_postgres_data:\n  # Qdrant 数据卷\n  lightrag_qdrant_data:\n  # DozerDB 数据卷（用于图存储）\n  lightrag_dozerdb_data:\n  #DozerDB 杂项\n  lightrag_dozerdb_misc:\n  # Redis 数据卷 (保留, 先注释, 如需启用再取消注释)\n  # lightrag_redis_data:\n\nservices:\n\n  ###################################################################\n  #  专用 PostgreSQL: 用于 KV_STORAGE + DOC_STATUS_STORAGE\n  ###################################################################\n  postgres:\n    image: shangor/postgres-for-rag:v1.0\n    container_name: lightrag-postgres\n    env_file:\n      - .env\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_DB=${POSTGRES_DATABASE}\n      - PGDATA=/var/lib/postgresql/data/pgdata\n    command: sh -c \"service postgresql start && sleep infinity\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - lightrag_postgres_data:/var/lib/postgresql/data\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n\n  ###################################################################\n  #  Redis: 暂时注释掉，不启用\n  ###################################################################\n  # redis:\n  #   image: redis:latest\n  #   container_name: lightrag-redis\n  #   env_file:\n  #     - .env\n  #   volumes:\n  #     - lightrag_redis_data:/data\n  #   networks:\n  #     - lightrag_net\n  #   command: [\"redis-server\", \"--appendonly\", \"yes\"]\n  #   restart: unless-stopped\n\n  ###################################################################\n  #  Qdrant: 用于 VECTOR_STORAGE\n  ###################################################################\n  qdrant:\n    image: qdrant/qdrant:latest\n    container_name: lightrag-qdrant\n    env_file:\n      - .env\n    ports:\n      - \"6333:6333\"  # Qdrant Web UI\n    volumes:\n      - lightrag_qdrant_data:/qdrant/storage\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n\n  ###################################################################\n  #  DozerDB: 用于 GRAPH_STORAGE + Web UI (替换原来的 Neo4j)\n  ###################################################################\n  dozerdb:\n    image: graphstack/dozerdb:latest\n    container_name: lightrag-dozerdb\n    ports:\n      - \"7474:7474\"  # Web UI 端口（如果 DozerDB 提供类似 Neo4j 的浏览器界面）\n      - \"7687:7687\"  # Bolt 协议端口\n    environment:\n      - NEO4J_AUTH=neo4j/neo4jpasswd\n      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes\n      - NEO4J_PLUGINS=[\"apoc\"]\n      - NEO4J_apoc_export_file_enabled=true\n      - NEO4J_apoc_import_file_enabled=true\n      - NEO4J_dbms_security_procedures_unrestricted='*'\n      - TZ=UTC\n#      - NEO4J_SERVER_LOGS_DEBUG_ENABLED=true\n#      - NEO4J_SERVER_LOGS_GC_ENABLED=true\n    volumes:\n      - lightrag_dozerdb_data:/data\n      - lightrag_dozerdb_misc:/logs\n      - lightrag_dozerdb_misc:/var/lib/neo4j/import\n      - lightrag_dozerdb_misc:/plugins\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n\n  ###################################################################\n  #  LightRAG 主服务\n  ###################################################################\n  lightrag:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      args:\n        HTTP_PROXY: \"http://10.248.155.2:7890\"\n        HTTPS_PROXY: \"http://10.248.155.2:7890\"\n    container_name: lightrag-server\n    env_file:\n      - .env\n    ports:\n      - \"${PORT:-9621}:9621\"\n    volumes:\n      - .env:/app/.env\n      - ./data/rag_storage:/app/data/rag_storage\n      - ./data/inputs:/app/data/inputs\n      - ./config.ini:/app/config.ini\n    environment:\n      - TZ=UTC\n    depends_on:\n      - postgres\n      - qdrant\n      - dozerdb\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n#    extra_hosts:\n#      - \"host.docker.internal:host-gateway\"\n\nnetworks:\n  lightrag_net:\n    driver: bridge\nroot@Debian12-Template:/opt/lightrag# cat .env\n#############################################################################\n#            LightRAG Server Configuration\n#############################################################################\nHOST=0.0.0.0\nPORT=9621\n#NAMESPACE_PREFIX=lightrag       # 用于区分不同lightrag实例的数据命名空间\n\n# Logging\nLOG_LEVEL=INFO\nVERBOSE=True\n\n#############################################################################\n#            Timeouts / Others\n#############################################################################\nTIMEOUT=500     # 单次请求超时时长，可根据需要自行调整\n\n#############################################################################\n#            其他可选的LLM / Embedding 配置示例\n#############################################################################\n# 例如OpenAI风格API\nLLM_BINDING=openai\nLLM_MODEL=deepseek-r1-distill-qwen:32b\nLLM_BINDING_HOST=http://10.248.155.223:3005/v1\nLLM_BINDING_API_KEY=sk-KZ1cKwPbs4VLw1LuD4F3C53062674692A30784A5C13cFc2f\n### for OpenAI LLM (LLM_BINDING_API_KEY take priority)\nOPENAI_API_KEY=sk-KZ1cKwPbs4VLw1LuD4F3C53062674692A30784A5C13cFc2f\n\nEMBEDDING_BINDING=openai\nEMBEDDING_MODEL=bge-m3\nEMBEDDING_BINDING_HOST=http://10.248.155.223:3005/v1\n\n### RAG Configuration\nMAX_ASYNC=16\nEMBEDDING_DIM=1024\nMAX_EMBED_TOKENS=8192\n### Settings relative to query\nHISTORY_TURNS=100\nCOSINE_THRESHOLD=0.2\nTOP_K=60\nMAX_TOKEN_TEXT_CHUNK=16384\nMAX_TOKEN_RELATION_DESC=16384\nMAX_TOKEN_ENTITY_DESC=16384\n### Settings relative to indexing\nCHUNK_SIZE=1200\nCHUNK_OVERLAP_SIZE=100\nMAX_TOKENS=65536\nMAX_TOKEN_SUMMARY=8192\nSUMMARY_LANGUAGE=English\n\n#############################################################################\n#            Storage Selections\n#  Postgres -> KV + DocStatus\n#  Qdrant -> Vector\n#  Neo4j -> Graph\n#############################################################################\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=Neo4JStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\n#############################################################################\n#            Redis Configuration\n#############################################################################\n# 格式 redis://host:port/dbindex\n#REDIS_URI=redis://redis:6379/0\n\n#############################################################################\n# PostgreSQL 配置\n#############################################################################\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_USER=rag\nPOSTGRES_PASSWORD=rag\nPOSTGRES_DATABASE=rag\n\n#############################################################################\n#            Qdrant Configuration\n#############################################################################\n# 访问qdrant容器时的地址\nQDRANT_URL=http://qdrant:6333\n# QDRANT_API_KEY=xxxx  # 如果你启用了Qdrant的API Key认证，可以在这里填写\n\n#############################################################################\n#            DozerDB Configuration\n#############################################################################\nNEO4J_URI=bolt://dozerdb:7687\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=neo4jpasswd   # 注意与docker-compose.yml中对应\n\n\n\n### Logs and screenshots\n\nERROR:Error processing or enqueueing file test2s.txt: LightRAG.apipeline_enqueue_documents() missing 1 required positional argument: 'ids'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/routers/document_routes.py\", line 297, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: LightRAG.apipeline_enqueue_documents() missing 1 required positional argument: 'ids'\n\nERROR:Error processing or enqueueing file test2-short.txt: LightRAG.apipeline_enqueue_documents() missing 1 required positional argument: 'ids'\nERROR:Traceback (most recent call last):\n  File \"/app/lightrag/api/routers/document_routes.py\", line 297, in pipeline_enqueue_file\n    await rag.apipeline_enqueue_documents(content)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: LightRAG.apipeline_enqueue_documents() missing 1 required positional argument: 'ids'\n\n\n### Additional Information\n\n- LightRAG Version: 1.2.1\n- Operating System: Debian12.9\n- Python Version: 3.11-slim\n- Related Issues: None\n",
      "state": "open",
      "author": "someone132s",
      "author_type": "User",
      "created_at": "2025-02-21T16:07:54Z",
      "updated_at": "2025-02-22T06:48:48Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/917/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/917",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/917",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:27.066175",
      "comments": [
        {
          "author": "bzImage",
          "body": "Censor your api keys.. ",
          "created_at": "2025-02-21T18:06:31Z"
        },
        {
          "author": "someone132s",
          "body": "> Censor your api keys.. \n\nthank u man. however this key is generated from my selfhosted oneapi thus it dosen's matter",
          "created_at": "2025-02-22T06:48:47Z"
        }
      ]
    },
    {
      "issue_number": 918,
      "title": "[Question]: <title>reproduce ， AttributeError: 'function' object has no attribute 'embedding_dim'，",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [ ] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nreproduce folder, step 1.py: error. do I need to add all the argument including embedding_dim to rag?\n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "wycoal",
      "author_type": "User",
      "created_at": "2025-02-21T16:08:51Z",
      "updated_at": "2025-02-22T04:44:02Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/918/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/918",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/918",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:27.240343",
      "comments": [
        {
          "author": "wycoal",
          "body": "Traceback (most recent call last):\n  File \"c:\\Data\\AILearning\\chat\\llamaparser-example-main\\LightRAG-main\\reproduce\\Step_1.py\", line 32, in <module>\n    rag = LightRAG(working_dir=WORKING_DIR)\n  File \"<string>\", line 37, in __init__\n  File \"c:\\data\\ailearning\\chat\\llamaparser-example-main\\lightrag-m",
          "created_at": "2025-02-21T16:10:28Z"
        },
        {
          "author": "LarFii",
          "body": "You can refer to `examples/lightrag_openai_demo.py` and `examples/lightrag_openai_compatible_demo.py`.",
          "created_at": "2025-02-22T04:44:01Z"
        }
      ]
    },
    {
      "issue_number": 909,
      "title": "[Bug]: func \"get_by_id\" crashes and leads to a failure quest",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\na bug happens when I try to retrive something from the database\n\ni'm pretty sure it's a bug because i implemented a fresh start.\n\naccoring to the error and the source, it seems to be related to the commits these days, especially the code clean up.\n\n### Steps to reproduce\n\nselect tab \"retrive\" and type in anything and commit, then the bug happens immidiately\n\n### Expected Behavior\n\nretival failed and the server continue to run.\n\n### LightRAG Config Used\n\n# Paste your config here\nroot@Debian12-Template:/opt/lightrag# cat docker-compose.yml\nvolumes:\n  # PostgreSQL 数据卷\n  lightrag_postgres_data:\n  # Qdrant 数据卷\n  lightrag_qdrant_data:\n  # Neo4j 数据卷\n  lightrag_neo4j_data:\n  # Neo4j 日志卷\n  lightrag_neo4j_logs:\n  # Redis 数据卷 (保留, 先注释, 如需启用再取消注释)\n  # lightrag_redis_data:\n\nservices:\n\n  ###################################################################\n  #  专用 PostgreSQL: 用于 KV_STORAGE + DOC_STATUS_STORAGE\n  ###################################################################\n  postgres:\n    image: shangor/postgres-for-rag:v1.0\n    container_name: lightrag-postgres\n    build:\n     args:\n       HTTP_PROXY: \"http://10.248.155.2:7890\"\n       HTTPS_PROXY: \"http://10.248.155.2:7890\"\n    env_file:\n      - .env\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_DB=${POSTGRES_DATABASE}\n      # 下面可选, 让PG数据放到 /var/lib/postgresql/data/pgdata\n      - PGDATA=/var/lib/postgresql/data/pgdata\n    command: sh -c \"service postgresql start && sleep infinity\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - lightrag_postgres_data:/var/lib/postgresql/data\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n\n  ###################################################################\n  #  Redis: 暂时注释掉，不启用\n  ###################################################################\n  # redis:\n  #   image: redis:latest\n  #   container_name: lightrag-redis\n  #   env_file:\n  #     - .env\n  #   volumes:\n  #     - lightrag_redis_data:/data\n  #   networks:\n  #     - lightrag_net\n  #   command: [\"redis-server\", \"--appendonly\", \"yes\"]\n  #   restart: unless-stopped\n\n  ###################################################################\n  #  Qdrant: 用于 VECTOR_STORAGE\n  ###################################################################\n  qdrant:\n    image: qdrant/qdrant:latest\n    container_name: lightrag-qdrant\n    env_file:\n      - .env\n    ports:\n      - \"6333:6333\"  # Qdrant Web UI\n    volumes:\n      - lightrag_qdrant_data:/qdrant/storage\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n\n  ###################################################################\n  #  Neo4j: 用于 GRAPH_STORAGE + Web UI\n  ###################################################################\n  neo4j:\n    image: neo4j:latest\n    container_name: lightrag-neo4j\n#    env_file:\n#      - .env\n    ports:\n      - \"7474:7474\"\n      - \"7687:7687\"\n    environment:\n      - NEO4J_AUTH=neo4j/neo4jpasswd\n      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes\n      - NEO4J_PLUGINS=[\"apoc\"]\n      - NEO4J_apoc_export_file_enabled=true\n      - NEO4J_apoc_import_file_enabled=true\n#      - NEO4J_dbms_logs_debug_level=DEBUG\n      - NEO4J_server_logs_debug_enabled=true\n      - NEO4J_server_logs_gc_enabled=true\n#      - NEO4J_server_memory_heap_initial_size=4G\n#      - NEO4J_server_memory_heap_max_size=16G\n#      - NEO4J_server_memory_pagecache_size=8G\n      - TZ=UTC\n    volumes:\n      - lightrag_neo4j_data:/data\n      - lightrag_neo4j_logs:/logs\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n\n  ###################################################################\n  #  LightRAG 主服务\n  ###################################################################\n  lightrag:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      args:\n        HTTP_PROXY: \"http://10.248.155.2:7890\"\n        HTTPS_PROXY: \"http://10.248.155.2:7890\"\n    container_name: lightrag-server\n    env_file:\n      - .env\n    ports:\n      - \"${PORT:-9621}:9621\"\n    volumes:\n      - .env:/app/.env\n      - ./data/rag_storage:/app/data/rag_storage\n      - ./data/inputs:/app/data/inputs\n    environment:\n      - TZ=UTC\n    depends_on:\n      # - redis  # 注释掉redis\n      - postgres  # 需要 Postgres\n      - qdrant\n      - neo4j\n    networks:\n      - lightrag_net\n    restart: unless-stopped\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n\nnetworks:\n  lightrag_net:\n    driver: bridge\nroot@Debian12-Template:/opt/lightrag# cat .env\n#############################################################################\n#            LightRAG Server Configuration\n#############################################################################\nHOST=0.0.0.0\nPORT=9621\nNAMESPACE_PREFIX=lightrag       # 用于区分不同lightrag实例的数据命名空间\n\n# Logging\nLOG_LEVEL=INFO\nVERBOSE=True\n\n#############################################################################\n#            Timeouts / Others\n#############################################################################\nTIMEOUT=500     # 单次请求超时时长，可根据需要自行调整\n\n#############################################################################\n#            其他可选的LLM / Embedding 配置示例\n#############################################################################\n# 例如OpenAI风格API\nLLM_BINDING=openai\nLLM_MODEL=deepseek-r1-distill-qwen:32b\nLLM_BINDING_HOST=http://10.248.155.223:3005/v1\nLLM_BINDING_API_KEY=key\n### for OpenAI LLM (LLM_BINDING_API_KEY take priority)\nOPENAI_API_KEY=key\n\nEMBEDDING_BINDING=openai\nEMBEDDING_MODEL=bge-m3\nEMBEDDING_BINDING_HOST=http://10.248.155.223:3005/v1\n\n### RAG Configuration\nMAX_ASYNC=16\nEMBEDDING_DIM=1024\nMAX_EMBED_TOKENS=8192\n### Settings relative to query\nHISTORY_TURNS=100\nCOSINE_THRESHOLD=0.2\nTOP_K=60\nMAX_TOKEN_TEXT_CHUNK=4000\nMAX_TOKEN_RELATION_DESC=4000\nMAX_TOKEN_ENTITY_DESC=4000\n### Settings relative to indexing\nCHUNK_SIZE=1200\nCHUNK_OVERLAP_SIZE=100\nMAX_TOKENS=32768\nMAX_TOKEN_SUMMARY=500\nSUMMARY_LANGUAGE=English\n\n#############################################################################\n#            Storage Selections\n#  Postgres -> KV + DocStatus\n#  Qdrant -> Vector\n#  Neo4j -> Graph\n#############################################################################\nLIGHTRAG_KV_STORAGE=PGKVStorage\nLIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage\nLIGHTRAG_GRAPH_STORAGE=Neo4JStorage\nLIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage\n\n#############################################################################\n#            Redis Configuration\n#############################################################################\n# 格式 redis://host:port/dbindex\n#REDIS_URI=redis://redis:6379/0\n\n#############################################################################\n# PostgreSQL 配置\n#############################################################################\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_USER=rag\nPOSTGRES_PASSWORD=rag\nPOSTGRES_DATABASE=rag\n\n#############################################################################\n#            Qdrant Configuration\n#############################################################################\n# 访问qdrant容器时的地址\nQDRANT_URL=http://qdrant:6333\n# QDRANT_API_KEY=xxxx  # 如果你启用了Qdrant的API Key认证，可以在这里填写\n\n#############################################################################\n#            Neo4j Configuration\n#############################################################################\nNEO4J_URI=bolt://neo4j:7687\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=neo4jpasswd   # 注意与docker-compose.yml中对应\n\n\nroot@Debian12-Template:/opt/lightrag#\n\n\n### Logs and screenshots\n\nDEBUG:get_best_cached_response:  mode=local cache_type=query use_llm_check=False\n[ERROR][2025-02-21 07:49:06] Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1621, in query_text\n    response = await rag.aquery(\n               ^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/lightrag.py\", line 962, in aquery\n    response = await kg_query(\n               ^^^^^^^^^^^^^^^\n  File \"/app/lightrag/operate.py\", line 580, in kg_query\n    cached_response, quantized, min_val, max_val = await handle_cache(\n                                                   ^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/utils.py\", line 554, in handle_cache\n    best_cached_response = await get_best_cached_response(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/utils.py\", line 400, in get_best_cached_response\n    mode_cache = await hashing_kv.get_by_id(mode)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/lightrag/kg/postgres_impl.py\", line 272, in get_by_id\n    sql = SQL_TEMPLATES[\"get_by_id_\" + self.namespace]\n          ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyError: 'get_by_id_lightragllm_response_cache'\n\nINFO:     172.19.0.1:34658 - \"POST /query HTTP/1.1\" 500 Internal Server Error\nINFO:     172.19.0.1:49778 - \"GET /health HTTP/1.1\" 200 OK\n\n\n### Additional Information\n\n- LightRAG Version: 1.1.12\n- Operating System: debian12.9\n- Python Version: 3.11-slim\n- Related Issues:\n",
      "state": "open",
      "author": "someone132s",
      "author_type": "User",
      "created_at": "2025-02-21T08:49:56Z",
      "updated_at": "2025-02-22T03:16:44Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/909/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/909",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/909",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:27.475572",
      "comments": [
        {
          "author": "someone132s",
          "body": "I have to say this bug even affect version 1.1.7",
          "created_at": "2025-02-21T12:49:47Z"
        },
        {
          "author": "fatehss",
          "body": "I have also noticed that these code cleanup commits have introduced lots of breaking changes for my existing code...",
          "created_at": "2025-02-21T22:46:06Z"
        },
        {
          "author": "someone132s",
          "body": "> I have also noticed that these code cleanup commits have introduced lots of breaking changes for my existing code...\n\nThere‘s no sense to upgrade at this moment",
          "created_at": "2025-02-22T03:16:43Z"
        }
      ]
    },
    {
      "issue_number": 838,
      "title": "Neo4j | Still facing issues after latest code changes.",
      "body": "Hi so I cloned and ran the newest code merged in #780 but I noticed that the issue mentioned in #780 is still happening. Below is the output while running my code, the actual code and the sample dummy text I used. \n\n[dummytext.txt](https://github.com/user-attachments/files/18842744/dummytext.txt)\n\n```\n\nG:\\Project\\LightRAG\\.venv\\Scripts\\python.exe G:\\Work\\test\\lightrag_api_ollama_neo4j_demo.py \nINPUT_FILE: dummytext.txt\nWORKING_DIR: G:\\Work\\test\\myKG\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': 'G:\\\\Work\\\\test\\\\myKG\\\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': 'G:\\\\Work\\\\test\\\\myKG\\\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': 'G:\\\\Work\\\\test\\\\myKG\\\\vdb_chunks.json'} 0 data\nINFO:lightrag:Loaded document status storage with 0 records\nINFO:     Started server process [21704]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8020 (Press CTRL+C to quit)\nread input file dummytext.txt successfully\nINFO:lightrag:Stored 1 new unique documents\nINFO:lightrag:Number of batches to process: 1.\nINFO:lightrag:Inserting 2 vectors to chunks\nGenerating embeddings:   0%|          | 0/1 [00:00<?, ?batch/s]\nGenerating embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.70s/batch]\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\nINFO:lightrag:Non-embedding cached missed(mode:default type:extract)\n\nLevel 2 - Extracting entities and relationships:  50%|█████     | 1/2 [03:15<03:15, 195.23s/chunk]\nLevel 2 - Extracting entities and relationships: 100%|██████████| 2/2 [03:52<00:00, 102.15s/chunk]\n                                                                                                  \n\nLevel 3 - Inserting entities:   0%|          | 0/23 [00:00<?, ?entity/s]WARNING:lightrag:Label 'TREVI FOUNTAIN' does not exist in Neo4j\nWARNING:lightrag:Label 'LOS ANGELES' does not exist in Neo4j\nWARNING:lightrag:Label 'CULTURAL AND CULINARY FESTIVALS' does not exist in Neo4j\nWARNING:lightrag:Label 'SOFIA'S' does not exist in Neo4j\nWARNING:lightrag:Label 'FARM-TO-TABLE INITIATIVES' does not exist in Neo4j\nWARNING:lightrag:Label 'CULINARY WORKSHOPS AND MENTORSHIP PROGRAMS' does not exist in Neo4j\nWARNING:lightrag:Label 'TUSCANY' does not exist in Neo4j\nWARNING:lightrag:Label 'PIETRO CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'NONNA LUCIA (LUCIA)' does not exist in Neo4j\nWARNING:lightrag:Label 'SOFIA CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'ANTONIO CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'LA TERRA DI SIENA' does not exist in Neo4j\nWARNING:lightrag:Label 'GIOVANNI CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'NEW YORK CITY' does not exist in Neo4j\nWARNING:lightrag:Label 'AMICO CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'AMALFI COAST' does not exist in Neo4j\nWARNING:lightrag:Label 'SANTA CATERINA' does not exist in Neo4j\nWARNING:lightrag:Label 'FOOD FOR ALL: COMMUNITY KITCHENS AND FOOD DRIVES' does not exist in Neo4j\nWARNING:lightrag:Label 'IL MARE NOSTRUM' does not exist in Neo4j\nWARNING:lightrag:Label 'MARIA' does not exist in Neo4j\nWARNING:lightrag:Label 'LUCIA CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'LA DOLCE VITA' does not exist in Neo4j\nWARNING:lightrag:Label 'SUPPORT FOR LOCAL ARTISTS AND ARTISANS' does not exist in Neo4j\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: CULINARY WORKSHOPS AND MENTORSHIP PROGRAMS)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`CULINARY WORKSHOPS AND MENTORSHIP PROGRAMS`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: TUSCANY)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`TUSCANY`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: TREVI FOUNTAIN)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`TREVI FOUNTAIN`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: FARM-TO-TABLE INITIATIVES)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`FARM-TO-TABLE INITIATIVES`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: SOFIA CARUSO)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`SOFIA CARUSO`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: LOS ANGELES)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`LOS ANGELES`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: NONNA LUCIA (LUCIA))} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`NONNA LUCIA (LUCIA)`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: GIOVANNI CARUSO)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`GIOVANNI CARUSO`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: PIETRO CARUSO)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`PIETRO CARUSO`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: LA TERRA DI SIENA)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`LA TERRA DI SIENA`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: CULTURAL AND CULINARY FESTIVALS)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`CULTURAL AND CULINARY FESTIVALS`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: NEW YORK CITY)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`NEW YORK CITY`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: ANTONIO CARUSO)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`ANTONIO CARUSO`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: SOFIA'S)} {position: line: 1, column: 10, offset: 9} for query: \"MATCH (n:`SOFIA'S`) RETURN n\"\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: AMICO CARUSO)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`AMICO CARUSO`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: AMALFI COAST)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`AMALFI COAST`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: SANTA CATERINA)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`SANTA CATERINA`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: IL MARE NOSTRUM)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`IL MARE NOSTRUM`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: MARIA)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`MARIA`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: LA DOLCE VITA)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`LA DOLCE VITA`) RETURN n'\nWARNING:lightrag:Label 'CULINARY WORKSHOPS AND MENTORSHIP PROGRAMS' does not exist in Neo4j\nWARNING:lightrag:Label 'TUSCANY' does not exist in Neo4j\nWARNING:lightrag:Label 'TREVI FOUNTAIN' does not exist in Neo4j\nWARNING:lightrag:Label 'FARM-TO-TABLE INITIATIVES' does not exist in Neo4j\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: LUCIA CARUSO)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`LUCIA CARUSO`) RETURN n'\nWARNING:lightrag:Label 'SOFIA CARUSO' does not exist in Neo4j\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: FOOD FOR ALL: COMMUNITY KITCHENS AND FOOD DRIVES)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`FOOD FOR ALL: COMMUNITY KITCHENS AND FOOD DRIVES`) RETURN n'\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: SUPPORT FOR LOCAL ARTISTS AND ARTISANS)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`SUPPORT FOR LOCAL ARTISTS AND ARTISANS`) RETURN n'\nWARNING:lightrag:Label 'LOS ANGELES' does not exist in Neo4j\nWARNING:lightrag:Label 'NONNA LUCIA (LUCIA)' does not exist in Neo4j\nWARNING:lightrag:Label 'GIOVANNI CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'PIETRO CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'LA TERRA DI SIENA' does not exist in Neo4j\nWARNING:lightrag:Label 'CULTURAL AND CULINARY FESTIVALS' does not exist in Neo4j\nWARNING:lightrag:Label 'NEW YORK CITY' does not exist in Neo4j\nWARNING:lightrag:Label 'ANTONIO CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'SOFIA'S' does not exist in Neo4j\nWARNING:lightrag:Label 'AMICO CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'AMALFI COAST' does not exist in Neo4j\nWARNING:lightrag:Label 'SANTA CATERINA' does not exist in Neo4j\nWARNING:lightrag:Label 'IL MARE NOSTRUM' does not exist in Neo4j\nWARNING:lightrag:Label 'MARIA' does not exist in Neo4j\nWARNING:lightrag:Label 'LA DOLCE VITA' does not exist in Neo4j\nWARNING:lightrag:Label 'LUCIA CARUSO' does not exist in Neo4j\nWARNING:lightrag:Label 'FOOD FOR ALL: COMMUNITY KITCHENS AND FOOD DRIVES' does not exist in Neo4j\nWARNING:lightrag:Label 'SUPPORT FOR LOCAL ARTISTS AND ARTISANS' does not exist in Neo4j\n\n\nLevel 3 - Inserting entities:   4%|▍         | 1/23 [00:00<00:03,  6.43entity/s]\n\n                                                                                \n\n\nLevel 3 - Inserting relationships:   0%|          | 0/19 [00:00<?, ?relationship/s]WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: AMICO'S COMMUNITY INVOLVEMENT AND SOCIAL CAUSES)} {position: line: 1, column: 10, offset: 9} for query: \"MATCH (a:`AMICO'S COMMUNITY INVOLVEMENT AND SOCIAL CAUSES`)-[r]-(b:`CULINARY WORKSHOPS AND MENTORSHIP PROGRAMS`) RETURN COUNT(r) > 0 AS edgeExists\"\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: AMICO'S FAMILY)} {position: line: 1, column: 10, offset: 9} for query: \"MATCH (a:`AMICO'S FAMILY`)-[r]-(b:`NONNA LUCIA (LUCIA)`) RETURN COUNT(r) > 0 AS edgeExists\"\nWARNING:lightrag:Label 'AMICO'S COMMUNITY INVOLVEMENT AND SOCIAL CAUSES' does not exist in Neo4j\nWARNING:lightrag:Label 'AMICO'S FAMILY' does not exist in Neo4j\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: AMICO'S COMMUNITY INVOLVEMENT AND SOCIAL CAUSES)} {position: line: 1, column: 10, offset: 9} for query: \"MATCH (n:`AMICO'S COMMUNITY INVOLVEMENT AND SOCIAL CAUSES`) RETURN count(n) > 0 AS node_exists\"\nWARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: AMICO'S FAMILY)} {position: line: 1, column: 10, offset: 9} for query: \"MATCH (n:`AMICO'S FAMILY`) RETURN count(n) > 0 AS node_exists\"\nWARNING:lightrag:Label 'AMICO'S COMMUNITY INVOLVEMENT AND SOCIAL CAUSES' does not exist in Neo4j\nWARNING:lightrag:Label 'AMICO'S FAMILY' does not exist in Neo4j\n\n\n\nLevel 3 - Inserting relationships:   5%|▌         | 1/19 [00:00<00:06,  2.93relationship/s]\n\n\n                                                                                           INFO:lightrag:Inserting 23 vectors to entities\nGenerating embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.77s/batch]\nINFO:lightrag:Inserting 19 vectors to relationships\nGenerating embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.33batch/s]\nINFO:lightrag:New entities or relationships extracted.\nINFO:lightrag:Completed batch 1 of 1.\n```\n\n```\n\nimport asyncio\nimport os\nfrom typing import Optional, Literal\n\nimport aiofiles\nimport nest_asyncio\nfrom fastapi import FastAPI, HTTPException, File, UploadFile\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.ollama import ollama_embed, ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\nfrom pydantic import BaseModel\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\n# Base configuration\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nDEFAULT_RAG_DIR = os.path.join(ROOT_DIR, \"myKG\")\nDEFAULT_INPUT_FILE = \"dummytext.txt\"\n\n# Environment configuration\nINPUT_FILE = os.environ.get(\"INPUT_FILE\", DEFAULT_INPUT_FILE)\nWORKING_DIR = os.environ.get(\"RAG_DIR\", DEFAULT_RAG_DIR)\n\nprint(f\"INPUT_FILE: {INPUT_FILE}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\n\n# Database configurations\n# MongoDB\nos.environ[\"MONGO_URI\"] = os.environ.get(\"MONGO_URI\", \"mongodb://localhost:27017/\")\nos.environ[\"MONGO_DATABASE\"] = os.environ.get(\"MONGO_DATABASE\", \"LightRAG\")\nos.environ[\"MONGO_COLLECTION\"] = os.environ.get(\"MONGO_COLLECTION\", \"rag_data\")\n\n# Neo4j\nBATCH_SIZE_NODES = 500\nBATCH_SIZE_EDGES = 100\nos.environ[\"NEO4J_URI\"] = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687/\")\nos.environ[\"NEO4J_USERNAME\"] = os.environ.get(\"NEO4J_USERNAME\", \"neo4j\")\nos.environ[\"NEO4J_PASSWORD\"] = os.environ.get(\"NEO4J_PASSWORD\", \"password\")\n\n# Initialize RAG system\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"qwen2.5:14b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\n        \"host\": \"http://127.0.0.1:11434\",\n        \"options\": {\"num_ctx\": 32768}\n    },\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts=texts,\n            embed_model=\"bge-m3:latest\",\n            host=\"http://127.0.0.1:11434\"\n        ),\n    ),\n    kv_storage=\"MongoKVStorage\",\n    graph_storage=\"Neo4JStorage\",\n)\n\napp = FastAPI(title=\"LightRAG API\", description=\"API for RAG operations with multiple databases\")\n\n\n# Data models\nclass QueryRequest(BaseModel):\n    query: str\n    mode: Literal[\"mix\"]\n    only_need_context: bool = False\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[str] = None\n    message: Optional[str] = None\n\n\n# API routes\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None,\n            lambda: rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    only_need_context=request.only_need_context\n                ),\n            ),\n        )\n        return Response(status=\"success\", data=result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            content = file_content.decode(\"gbk\")\n\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\"\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_default_file\", response_model=Response)\n@app.get(\"/insert_default_file\", response_model=Response)\nasync def insert_default_file():\n    try:\n        async with aiofiles.open(INPUT_FILE, \"r\", encoding=\"utf-8\") as file:\n            content = await file.read()\n        print(f\"read input file {INPUT_FILE} successfully\")\n\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {INPUT_FILE} inserted successfully\"\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"databases\": {\n            \"mongodb\": os.environ.get(\"MONGO_URI\"),\n            \"neo4j\": os.environ.get(\"NEO4J_URI\")\n        }\n    }\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n\n```",
      "state": "open",
      "author": "saigauthamr",
      "author_type": "User",
      "created_at": "2025-02-18T10:02:42Z",
      "updated_at": "2025-02-21T18:04:30Z",
      "closed_at": null,
      "labels": [
        "bug",
        "neo4j"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/838/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/838",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/838",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:27.684654",
      "comments": [
        {
          "author": "saigauthamr",
          "body": "Value of CALL db.labels() showing that the labels exist in the database.\n\n╒══════════════════════════════════════════════════╕\n│label                                             │\n╞══════════════════════════════════════════════════╡\n│\"SOFIA CARUSO\"                                    │\n├─────────────",
          "created_at": "2025-02-18T10:07:48Z"
        },
        {
          "author": "spo0nman",
          "body": "Neo4j issues warnings indicating that many labels used in queries are not recognized in the database. Although the code attempts to “ensure” a label—by sanitizing the input and register the label during an upsert operation—the labels still appear as missing. the current approach to ensure a label's ",
          "created_at": "2025-02-19T10:34:46Z"
        },
        {
          "author": "bzImage",
          "body": "just tried the neo4j integration and im getting multiple errors like:\n\n`\nWARNING:lightrag:Label 'WWW1[.]EMPRESARIALL[.]SHOP' does not exist in Neo4j                                              ERROR:lightrag:Error checking label existence: failed to obtain a connection from the pool within 60.0s (t",
          "created_at": "2025-02-21T18:04:28Z"
        }
      ]
    },
    {
      "issue_number": 896,
      "title": "[Question]: Why is something importing mlatplotlib at lightrag-server startup",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\ntoday I was testing and got this error:\n\nUserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n\nLightrag should not use matplotlib (ever) as this the library itself is not supposed to show graphs on its own. We can put matplotlib graph representation on the application side or in the case of the webui, in the front end via the different endpoints.\n\nI checked any matplitlib import and I don't seem to find any one inside the code!\n\nDoes anyone know what library we are using may be ceausing this? This happens when I load the api server.\n\nBest regards\n\n \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-02-20T09:01:53Z",
      "updated_at": "2025-02-21T13:09:46Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/896/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/896",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/896",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:27.927585",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi,\n\nI also agree.",
          "created_at": "2025-02-20T10:01:58Z"
        },
        {
          "author": "ArnoChenFx",
          "body": "It's `networkx`",
          "created_at": "2025-02-20T13:10:58Z"
        },
        {
          "author": "ParisNeo",
          "body": "By the way, Do you plan on doing some upgrades to the retreival UI?\n\nI guess we need to make it capable of rendering code also as well as adding the possibility to use special prompts etc ...\nI also noticed it is adding alot of empty lines to the message resulting into a non compact representation. ",
          "created_at": "2025-02-21T13:09:43Z"
        }
      ]
    },
    {
      "issue_number": 911,
      "title": "[Feature Request]: Seperate Web server and API server",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nThe present LightRAG Server is very complicated and it's hard to maintain. Is it possible to separate the Web server and the API server? This could alleviate the development pressure and make the structure usage much more flexible.\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "someone132s",
      "author_type": "User",
      "created_at": "2025-02-21T11:22:30Z",
      "updated_at": "2025-02-21T11:33:31Z",
      "closed_at": "2025-02-21T11:33:31Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/911/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/911",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/911",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:28.102099",
      "comments": [
        {
          "author": "ParisNeo",
          "body": "Hi, we are upgrading the api server to make it more modular and easier to maintain.\n\n@danielaskdd Has made some cool progress on splitting the code into multiple files making it more readable and easier to maintain.\n\nJust wait for the PR to be accepted by @LarFii ",
          "created_at": "2025-02-21T11:31:22Z"
        },
        {
          "author": "someone132s",
          "body": "Thank you for the great effort you've made, I'm really excited to see the new version of the server",
          "created_at": "2025-02-21T11:33:27Z"
        }
      ]
    },
    {
      "issue_number": 910,
      "title": "[Feature Request]: Need multiple KnowledgeBase support",
      "body": "### Do you need to file a feature request?\n\n- [x] I have searched the existing feature request and this feature request is not already filed.\n- [x] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\nMultiple knowledgebase support in one server instance is very helpful. This would make the management much more easier. \n\n### Additional Context\n\n_No response_",
      "state": "open",
      "author": "someone132s",
      "author_type": "User",
      "created_at": "2025-02-21T11:06:37Z",
      "updated_at": "2025-02-21T11:06:37Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/910/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/910",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/910",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:28.309170",
      "comments": []
    },
    {
      "issue_number": 821,
      "title": "create_graph does not exist in pg implementation",
      "body": "Hi, \n\nIt seems like we are missing a create_graph sql function:\n\n```\nERROR:Task exception was never retrieved\nfuture: <Task finished name='Task-772' coro=<_merge_nodes_then_upsert() done, defined at /home/abyl/PycharmProjects/volve-ai/venv/lib/python3.10/site-packages/lightrag/operate.py:178> exception=PGGraphQueryException({'message': 'Error executing graph query: SELECT * FROM cypher(\\'chunk_entity_relation\\', $$\\n                     MATCH (n:Entity {node_id: \"x504f4f5220414e4420444553544954555445\"})\\n                     RETURN n\\n                   $$) AS (n agtype)', 'wrapped': 'SELECT * FROM cypher(\\'chunk_entity_relation\\', $$\\n                     MATCH (n:Entity {node_id: \"x504f4f5220414e4420444553544954555445\"})\\n                     RETURN n\\n                   $$) AS (n agtype)', 'detail': 'function create_graph(unknown) does not exist\\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.'})>\nTraceback (most recent call last):\n  File \"/home/abyl/PycharmProjects/volve-ai/venv/lib/python3.10/site-packages/lightrag/kg/postgres_impl.py\", line 760, in _query\n    data = await self.db.query(\n  File \"/home/abyl/PycharmProjects/volve-ai/venv/lib/python3.10/site-packages/lightrag/kg/postgres_impl.py\", line 110, in query\n    await PostgreSQLDB._prerequisite(connection, graph_name)\n  File \"/home/abyl/PycharmProjects/volve-ai/venv/lib/python3.10/site-packages/lightrag/kg/postgres_impl.py\", line 170, in _prerequisite\n    await conn.execute(f\"\"\"select create_graph('{graph_name}')\"\"\")\n  File \"/home/abyl/PycharmProjects/volve-ai/venv/lib/python3.10/site-packages/asyncpg/connection.py\", line 349, in execute\n    result = await self._protocol.query(query, timeout)\n  File \"asyncpg/protocol/protocol.pyx\", line 375, in query\nasyncpg.exceptions.UndefinedFunctionError: function create_graph(unknown) does not exist\n```\n",
      "state": "closed",
      "author": "abylikhsanov",
      "author_type": "User",
      "created_at": "2025-02-17T19:25:02Z",
      "updated_at": "2025-02-21T08:41:30Z",
      "closed_at": "2025-02-21T08:41:30Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/821/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/821",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/821",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:28.309194",
      "comments": [
        {
          "author": "YanSte",
          "body": "Did you follow the documentation ?",
          "created_at": "2025-02-17T21:28:06Z"
        },
        {
          "author": "abylikhsanov",
          "body": "@YanSte If you mean this then yes:\nhttps://github.com/HKUDS/LightRAG#using-postgresql-for-storage",
          "created_at": "2025-02-18T08:13:43Z"
        },
        {
          "author": "YanSte",
          "body": "Are you using the API ?\n\nIf yes here is the doc: https://github.com/HKUDS/LightRAG/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md",
          "created_at": "2025-02-18T08:15:38Z"
        },
        {
          "author": "abylikhsanov",
          "body": "I am not using API as I just wanted Postgres to serve as a storage.\n\nI've actually was looking to use Neon due to its PGBouncer but where is the implementation of function create_graph? Could not find it across the repo",
          "created_at": "2025-02-18T09:12:56Z"
        },
        {
          "author": "YanSte",
          "body": "I will have a look. Thanks for sharing.",
          "created_at": "2025-02-18T09:30:35Z"
        }
      ]
    },
    {
      "issue_number": 746,
      "title": "How to modify an existing knowledge graph?",
      "body": "Hi there.\n\nI found that the knowledge graph generated by LightRAG has errors with some nodes and edges. Is there any way for me to correct it?  It would be nice to have a GUI to fix it.\n\nThanks.",
      "state": "open",
      "author": "lryan599",
      "author_type": "User",
      "created_at": "2025-02-11T08:55:43Z",
      "updated_at": "2025-02-20T13:44:38Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/746/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/746",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/746",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:28.501691",
      "comments": [
        {
          "author": "YanSte",
          "body": "@ArnoChenFx Could you please help ?",
          "created_at": "2025-02-19T20:30:41Z"
        },
        {
          "author": "lryan599",
          "body": "I believe this feature is very useful in real production.\n\nKnowledge graphs take a lot of people to maintain.",
          "created_at": "2025-02-20T05:29:39Z"
        },
        {
          "author": "ArnoChenFx",
          "body": "> [@ArnoChenFx](https://github.com/ArnoChenFx) Could you please help ?\n\nSorry, I don't know how to implement this feature, as it requires additional development for each backend and familiarity with each type of backend database.",
          "created_at": "2025-02-20T13:44:35Z"
        }
      ]
    },
    {
      "issue_number": 677,
      "title": "Error due to lack of processing of escape characters",
      "body": "It seems this is because characters like `` can appear inside the object\nand neo4j starts processing them as a request\n\n```\n2025-01-30 07:37:52 vec-box asyncio[1] ERROR Task exception was never retrieved\nfuture: <Task finished name='Task-341385' coro=<_merge_nodes_then_upsert() done, defined at /root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/operate.py:179> exception=CypherSyntaxError('Invalid input \\'SCHEMA\\': expected a parameter, \\'&\\', \\')\\', \\':\\', \\'WHERE\\', \\'{\\' or \\'|\\' (line 1, column 19 (offset: 18))\\n\"MATCH (n:`ОБЪЕКТ `SCHEMA` И ЕГО ПАРАМЕТРЫ ГИБКОСТИ`) RETURN n\"\\n                   ^')>\nTraceback (most recent call last):\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/operate.py\", line 190, in _merge_nodes_then_upsert\n    already_node = await knowledge_graph_inst.get_node(entity_name)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/kg/neo4j_impl.py\", line 144, in get_node\n    result = await session.run(query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/neo4j/_async/work/session.py\", line 327, in run\n    await self._auto_result._run(\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/neo4j/_async/work/result.py\", line 231, in _run\n    await self._attach()\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/neo4j/_async/work/result.py\", line 425, in _attach\n    await self._connection.fetch_message()\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/neo4j/_async/io/_common.py\", line 195, in inner\n    await coroutine_func(*args, **kwargs)\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/neo4j/_async/io/_bolt.py\", line 994, in fetch_message\n    res = await self._process_message(tag, fields)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/neo4j/_async/io/_bolt5.py\", line 1204, in _process_message\n    await response.on_failure(summary_metadata or {})\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/neo4j/_async/io/_common.py\", line 254, in on_failure\n    raise self._hydrate_error(metadata)\nneo4j.exceptions.CypherSyntaxError: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'SCHEMA': expected a parameter, '&', ')', ':', 'WHERE', '{' or '|' (line 1, column 19 (offset: 18))\n\"MATCH (n:`ОБЪЕКТ `SCHEMA` И ЕГО ПАРАМЕТРЫ ГИБКОСТИ`) RETURN n\"\n```",
      "state": "open",
      "author": "KaymeKaydex",
      "author_type": "User",
      "created_at": "2025-01-30T07:46:07Z",
      "updated_at": "2025-02-20T12:34:57Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/677/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/677",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/677",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:28.693707",
      "comments": [
        {
          "author": "ultrageopro",
          "body": "same problem",
          "created_at": "2025-01-30T17:14:57Z"
        },
        {
          "author": "ParisNeo",
          "body": "This must be fixed. I have no time to do that so if any one is up to do it, please do and submit a PR.",
          "created_at": "2025-01-31T07:36:24Z"
        },
        {
          "author": "YanSte",
          "body": "@ParisNeo any progress with neo4j ?",
          "created_at": "2025-02-19T20:28:27Z"
        },
        {
          "author": "ParisNeo",
          "body": "We had this problem before on another implémentation. I guess we need to add escaping non utf8 characters.",
          "created_at": "2025-02-20T12:34:54Z"
        }
      ]
    },
    {
      "issue_number": 625,
      "title": "Open AI compatible servers do not work anymore",
      "body": "Hi there\n\n@danielaskdd, you have made a refactoring of the API which is OK, but that broke some functionalities:\n\nSome people use lightrag with lm studio via OpenAI API.\n\nYou have removed the possibility of specifying the host for openai api models resulting in the impossibility to use the lightrag api with an openai compatible service like lm studio.\n\nWould you please do the update so that we can use openai api locally?\n\n\nBest regards",
      "state": "closed",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-01-22T09:51:05Z",
      "updated_at": "2025-02-20T10:45:52Z",
      "closed_at": "2025-02-20T10:45:52Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/625/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/625",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/625",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:28.954464",
      "comments": [
        {
          "author": "danielaskdd",
          "body": "If someone want to use OpenAI alike LLM, he can setup the server according to `lightrag/api/README.md`\n\n#### For OpenAI Alike Server\n- Requires environment variables setup or command line argument provided\n- Environment variables: LLM_BINDING=ollama, LLM_BINDING_HOST, LLM_MODEL, LLM_BINDING_API_KEY\n",
          "created_at": "2025-01-22T10:08:33Z"
        },
        {
          "author": "danielaskdd",
          "body": "Here is the `.env` file example for access Open WebUI's OpenAI compatible API: \n```\n# OpenAI alike example\nLLM_BINDING=openai\nLLM_BINDING_HOST=https://localhost/api\nLLM_MODEL=deepseek-chat\nLLM_BINDING_API_KEY=your_api_key\n```",
          "created_at": "2025-01-22T10:12:14Z"
        },
        {
          "author": "ParisNeo",
          "body": "Thanks for this. But according to the code, you do not pass the arguments to the LightRAG kwargs in case openai is selected:\n\n```python\n    # Initialize RAG\n    if args.llm_binding in [\"lollms\", \"ollama\"]:\n        rag = LightRAG(\n            working_dir=args.working_dir,\n            llm_model_func=l",
          "created_at": "2025-01-22T14:27:28Z"
        },
        {
          "author": "danielaskdd",
          "body": "I submitted a PR to fix this issue. The OpenAI client only requires specific parameters - passing additional kwargs may cause errors.\n\nhttps://github.com/HKUDS/LightRAG/pull/628",
          "created_at": "2025-01-22T17:24:56Z"
        },
        {
          "author": "ParisNeo",
          "body": "Hi I took a look at the pull request but I think this still won't fix it as you don't send the host address nor api key.\nI would add:\n```\nllm_model_kwargs={\n                \"host\": args.llm_binding_host,\n                \"api_key\": args.llm_binding_api_key,\n            },\n```\n\nAlthough I don't really",
          "created_at": "2025-01-22T17:29:31Z"
        }
      ]
    },
    {
      "issue_number": 893,
      "title": "LightRAG-1.1.9/.env: no such file or directory",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\nWARN[0000] /home/peak/RAG/LightRAG-1.1.9/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion\nenv file /home/peak/RAG/LightRAG-1.1.9/.env not found: stat /home/peak/RAG/LightRAG-1.1.9/.env: no such file or directory\n\n### Additional Context\n\n_No response_",
      "state": "closed",
      "author": "watch-Ultra",
      "author_type": "User",
      "created_at": "2025-02-20T01:50:51Z",
      "updated_at": "2025-02-20T08:21:02Z",
      "closed_at": "2025-02-20T08:21:02Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/893/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/893",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/893",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:29.183839",
      "comments": [
        {
          "author": "YanSte",
          "body": "Thanks for sharing.\n\nI will have a look.",
          "created_at": "2025-02-20T07:43:50Z"
        },
        {
          "author": "YanSte",
          "body": "Please follow the documentation.\n\nYou did add you .env.\n\nPlease see doc examples.",
          "created_at": "2025-02-20T08:21:00Z"
        }
      ]
    },
    {
      "issue_number": 861,
      "title": "遇到了json.decoder.JSONDecodeError: Expecting ',' delimiter: line 23697 column 16195 (char 59959183)",
      "body": "前几次都可以正常运行，也基本上代码变动，就出现了以下问题，想问一下是什么问题，是./dickens_new出了问题吗，应该如何解决，谢谢\n\nINFO:lightrag:Logger initialized for working directory: ./dickens_new\nTraceback (most recent call last):\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag_ollama_answer.py\", line 19, in <module>\n    rag = LightRAG(\n  File \"<string>\", line 32, in __init__\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag/lightrag.py\", line 220, in __post_init__\n    self.llm_response_cache = self.key_string_value_json_storage_cls(\n  File \"<string>\", line 6, in __init__\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag/storage.py\", line 34, in __post_init__\n    self._data = load_json(self._file_name) or {}\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag/utils.py\", line 150, in load_json\n    return json.load(f)\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/__init__.py\", line 293, in load\n    return loads(fp.read(),\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n    obj, end = self.scan_once(s, idx)\njson.decoder.JSONDecodeError: Expecting ',' delimiter: line 23697 column 16195 (char 59959183)",
      "state": "closed",
      "author": "xiayi0409",
      "author_type": "User",
      "created_at": "2025-02-19T10:43:39Z",
      "updated_at": "2025-02-20T06:40:04Z",
      "closed_at": "2025-02-20T06:40:03Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/861/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/861",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/861",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:29.370507",
      "comments": [
        {
          "author": "YanSte",
          "body": "Thanks for sharing, could you please share context and code.\n\nThanks.",
          "created_at": "2025-02-19T18:46:40Z"
        },
        {
          "author": "xiayi0409",
          "body": "\n> Thanks for sharing, could you please share context and code.\n> \n> Thanks.\n\nThank you for your reply! I just found the problem and have solved it. There is a problem in kv_store_llm_response_cache.json in my Lightrag's dickens. I think it may be that I forced the program to be paused during the op",
          "created_at": "2025-02-20T06:40:03Z"
        }
      ]
    },
    {
      "issue_number": 683,
      "title": "Functionality of Time-Weighted Retriever",
      "body": "Does LightRAG provide a functionality similar to a [Time-Weighted Retriever](https://js.langchain.com/docs/integrations/retrievers/time-weighted-retriever/), allowing searches to consider the time factor of the information?\n\nFor example:\n\nA Time-Weighted Retriever is a retriever that takes into account recency in addition to similarity. The scoring algorithm is:\n```\nlet score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;\n```\n\nNotably, `hoursPassed` above refers to the time since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain `fresh` and score higher.\n\n`this.decayRate` is a configurable decimal number between 0 and 1. A lower number means that documents will be `remembered` for longer, while a higher number strongly weights more recently accessed documents.\n\nNote that setting a decay rate of exactly 0 or 1 makes `hoursPassed` irrelevant and makes this retriever equivalent to a standard vector lookup.",
      "state": "open",
      "author": "ksmooi",
      "author_type": "User",
      "created_at": "2025-01-31T07:21:16Z",
      "updated_at": "2025-02-20T03:17:06Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/683/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/683",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/683",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:29.588265",
      "comments": [
        {
          "author": "YanSte",
          "body": "@LarFii ?",
          "created_at": "2025-02-19T20:28:56Z"
        },
        {
          "author": "LarFii",
          "body": "Indeed, exploring time-sensitive content is an area we’re currently investigating. There is a lack of temporal awareness, including the intrinsic time relationships within text, and we plan to consider incorporating this functionality in the future. Thank you very much for your insight!",
          "created_at": "2025-02-20T03:17:05Z"
        }
      ]
    },
    {
      "issue_number": 676,
      "title": "Perform a naive search query fails using HuggingFace models",
      "body": "Code\n```python\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.hf import hf_model_complete, hf_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom transformers import AutoModel, AutoTokenizer\n\nWORKING_DIR = \"/kaggle/working/dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=hf_model_complete,\n    llm_model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=5000,\n        func=lambda texts: hf_embed(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\"),\n            embed_model=AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\"),\n        ),\n    ),\n    log_level=\"DEBUG\",\n)\n\n# Matches all .txt files in the current directory\nfile_list = glob.glob(\"/kaggle/input/lilian-weng-blog/lilianweng_*.txt\")\n\ntotal_files = len(file_list)\nprint(f\"Found {total_files} files to process.\")\n\nfor idx, file in enumerate(file_list, start=1):\n    print(f\"Processing {idx}/{total_files}: {file}\")\n    with open(file, \"r\", encoding=\"utf-8\") as f:\n        rag.insert(f.read())  # Insert the content into RAG\n\n# Perform a local search query (work fine)\nlocal_result = rag.query(\"What are the top themes in this blog?\", param=QueryParam(mode=\"local\"))\nprint(local_result)\n\n# Perform a global search query (work fine)\nglobal_result = rag.query(\"What are the top themes in this blog?\", param=QueryParam(mode=\"global\"))\nprint(global_result)\n\n# Perform a hybrid search query (work fine)\nhybrid_result = rag.query(\"What are the top themes in this blog?\", param=QueryParam(mode=\"hybrid\"))\nprint(hybrid_result)\n\n# Perform a naive search query (REPORTED ERRORS)\nnaive_result = rag.query(\"What are the top themes in this blog?\", param=QueryParam(mode=\"naive\"))\nprint(naive_result)\n```\n\nErrors\n```\nKeyError                                  Traceback (most recent call last)\n<ipython-input-6-a525dc3e28af> in <cell line: 2>()\n      1 # Perform a naive search query\n----> 2 naive_result = rag.query(\"What are the top themes in this blog?\", param=QueryParam(mode=\"naive\"))\n      3 print(naive_result)\n\n/usr/local/lib/python3.10/dist-packages/lightrag/lightrag.py in query(self, query, param)\n    886     def query(self, query: str, param: QueryParam = QueryParam()):\n    887         loop = always_get_an_event_loop()\n--> 888         return loop.run_until_complete(self.aquery(query, param))\n    889 \n    890     async def aquery(self, query: str, param: QueryParam = QueryParam()):\n\n/usr/local/lib/python3.10/dist-packages/nest_asyncio.py in run_until_complete(self, future)\n     96                 raise RuntimeError(\n     97                     'Event loop stopped before Future completed.')\n---> 98             return f.result()\n     99 \n    100     def _run_once(self):\n\n/usr/lib/python3.10/asyncio/futures.py in result(self)\n    199         self.__log_traceback = False\n    200         if self._exception is not None:\n--> 201             raise self._exception.with_traceback(self._exception_tb)\n    202         return self._result\n    203 \n\n/usr/lib/python3.10/asyncio/tasks.py in __step(***failed resolving arguments***)\n    230                 # We use the `send` method directly, because coroutines\n    231                 # don't have `__iter__` and `__next__` methods.\n--> 232                 result = coro.send(None)\n    233             else:\n    234                 result = coro.throw(exc)\n\n/usr/local/lib/python3.10/dist-packages/lightrag/lightrag.py in aquery(self, query, param)\n    908             )\n    909         elif param.mode == \"naive\":\n--> 910             response = await naive_query(\n    911                 query,\n    912                 self.chunks_vdb,\n\n/usr/local/lib/python3.10/dist-packages/lightrag/operate.py in naive_query(query, chunks_vdb, text_chunks_db, query_param, global_config, hashing_kv)\n   1536 \n   1537     sys_prompt_temp = PROMPTS[\"naive_rag_response\"]\n-> 1538     sys_prompt = sys_prompt_temp.format(\n   1539         content_data=section, response_type=query_param.response_type\n   1540     )\n\nKeyError: 'history'\n```",
      "state": "open",
      "author": "ksmooi",
      "author_type": "User",
      "created_at": "2025-01-30T07:37:53Z",
      "updated_at": "2025-02-20T03:12:09Z",
      "closed_at": null,
      "labels": [
        "bug",
        "faiss"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/676/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/676",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/676",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:29.780532",
      "comments": [
        {
          "author": "LarFii",
          "body": "In the latest code, this bug has been fixed.",
          "created_at": "2025-02-20T03:12:08Z"
        }
      ]
    },
    {
      "issue_number": 756,
      "title": "Create relations between 2 same object names",
      "body": "If we write down the name of the object, the key in the graph and the embedding of the name in the vector store, then when selecting new objects we can create a relation between 2 objects with the same name, which means we are talking about the same objects. This will help avoid duplication of objects in the m.konovalov and Maksim Konovalov formats.",
      "state": "open",
      "author": "KaymeKaydex",
      "author_type": "User",
      "created_at": "2025-02-12T12:09:50Z",
      "updated_at": "2025-02-20T03:06:06Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "question"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/756/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/756",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/756",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.009031",
      "comments": []
    },
    {
      "issue_number": 868,
      "title": "[Feature Request]: Need a way to stop processing  gracefully",
      "body": "@danielaskdd @ArnoChenFx @YanSte @LarFii \n\nHi, I had to reboot my PC while it was processing files. I just pressed ctrl+C and I lost 10 hours worth of file processing beceause the json file was corrupted when I pressed the Ctrl+C. (I'm not happy with that).\n\nI think we need a way to stop the processing (some endpoints + some buttons) so that we can stop processing before rebooting the server. Also we may need someway to stop processing one file etc.\n\nAlso we need to store file names and add them to both the files webui and the RAG query so that the AI can give detailed information about the source of its answer.",
      "state": "open",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-02-19T13:13:27Z",
      "updated_at": "2025-02-19T21:32:09Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/868/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 2,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/868",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/868",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.009063",
      "comments": [
        {
          "author": "ArnoChenFx",
          "body": "Youe can try implementing the [finalize](https://github.com/HKUDS/LightRAG/blob/1e3fa84c2e1a1a06fb234cbd7b4d005daa6cc193/lightrag/base.py#L94) interface for `JsonKVStorage`  to see if the save operation can be executed in the function when pressing Ctrl+C.\n\nOr maybe we should remove `JsonKVStorage` ",
          "created_at": "2025-02-19T13:42:00Z"
        },
        {
          "author": "danielaskdd",
          "body": "There should be a way to delete all the information of a file from the chunk、kv、vector and graph storage, and adjust the weight of relation ships。For entity and relation summarize, it is difficult to revert the impact of a file. May be we should keep all original description for rebuilding the summa",
          "created_at": "2025-02-19T13:56:15Z"
        },
        {
          "author": "danielaskdd",
          "body": "> Youe can try implementing the [finalize](https://github.com/HKUDS/LightRAG/blob/1e3fa84c2e1a1a06fb234cbd7b4d005daa6cc193/lightrag/base.py#L94) interface for `JsonKVStorage` to see if the save operation can be executed in the function when pressing Ctrl+C.\n> \n> Or maybe we should remove `JsonKVStor",
          "created_at": "2025-02-19T14:07:37Z"
        },
        {
          "author": "ParisNeo",
          "body": "SQLite is a great solution by the way. I think we should build a sqlite_impl.py. I can start one.",
          "created_at": "2025-02-19T18:20:09Z"
        }
      ]
    },
    {
      "issue_number": 742,
      "title": "Feature Request: Allow Prompt Input in Mode Mix",
      "body": "Hello,\n\nCurrently, in Mode Mix, it is not possible to directly pass a prompt as input. This limitation restricts flexibility in generating mixed outputs dynamically.\n\nCould you consider adding support for prompt input in Mode Mix? This would enable more customized and controlled output generation, improving usability for various applications.\n\nLooking forward to your feedback.",
      "state": "closed",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-02-10T12:54:39Z",
      "updated_at": "2025-02-19T20:30:21Z",
      "closed_at": "2025-02-19T20:30:20Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/742/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/742",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/742",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.274025",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi, you can use the last version you can.",
          "created_at": "2025-02-19T20:30:19Z"
        }
      ]
    },
    {
      "issue_number": 606,
      "title": "使用Postgre DB时出现错误",
      "body": "请问要如何解决这个错误？谢谢🙏\n\n```\nncoded label: xe694afe4bb98e69cbae69e84                                                                                                                                    | 0/37 [00:00<?, ?entity/s]\nEncoded label: xe58f8de6b497e992b1\nEncoded label: xe993b6e8a18c\nEncoded label: xe4b8ade59bbde4babae6b091e993b6e8a18c\n                                                                                                                                                                                                     ERROR:lightrag:PostgreSQL database error: duplicate key value violates unique constraint \"pg_class_relname_nsp_index\"                                               | 1/37 [00:00<00:25,  1.42entity/s]\nDETAIL:  Key (relname, relnamespace)=(xe794b5e4bfa1e7bd91e7bb9ce696b0e59e8be8bf9de6b395e78aafe_id_seq, 28908) already exists.\nSELECT * FROM ag_catalog.cypher('graph_qatest', $$\n           MERGE (n:`xe794b5e4bfa1e7bd91e7bb9ce696b0e59e8be8bf9de6b395e78aafe7bdaae4baa4e69893e9a38ee999a9e4ba8be4bbb6e7aea1e79086e5b9b3e58fb0`)\n                SET n += {`entity_type`: \"\\\"ORGANIZATION\\\"\", `description`: \"\\\"\\u7535\\u4fe1\\u7f51\\u7edc\\u65b0\\u578b\\u8fdd\\u6cd5\\u72af\\u7f6a\\u4ea4\\u6613\\u98ce\\u9669\\u4e8b\\u4ef6\\u7ba1\\u7406\\u5e73\\u53f0\\u662f\\u516c\\u5b89\\u673a\\u5173\\u7528\\u4e8e\\u7ba1\\u7406\\u548c\\u67e5\\u8be2\\u7535\\u4fe1\\u7f51\\u7edc\\u65b0\\u578b\\u8fdd\\u6cd5\\u72af\\u7f6a\\u4ea4\\u6613\\u98ce\\u9669\\u4e8b\\u4ef6\\u7684\\u5e73\\u53f0\\u3002\\\"\", `source_id`: \"chunk-1815708c765a0bac407d91788fa567ca\"}\n        $$) AS (a agtype)\nNone\nERROR:lightrag:Error during upsert: {{'message': 'Error executing graph query: MERGE (n:`xe794b5e4bfa1e7bd91e7bb9ce696b0e59e8be8bf9de6b395e78aafe7bdaae4baa4e69893e9a38ee999a9e4ba8be4bbb6e7aea1e79086e5b9b3e58fb0`)\\n                SET n += {`entity_type`: \"\\\\\"ORGANIZATION\\\\\"\", `description`: \"\\\\\"\\\\u7535\\\\u4fe1\\\\u7f51\\\\u7edc\\\\u65b0\\\\u578b\\\\u8fdd\\\\u6cd5\\\\u72af\\\\u7f6a\\\\u4ea4\\\\u6613\\\\u98ce\\\\u9669\\\\u4e8b\\\\u4ef6\\\\u7ba1\\\\u7406\\\\u5e73\\\\u53f0\\\\u662f\\\\u516c\\\\u5b89\\\\u673a\\\\u5173\\\\u7528\\\\u4e8e\\\\u7ba1\\\\u7406\\\\u548c\\\\u67e5\\\\u8be2\\\\u7535\\\\u4fe1\\\\u7f51\\\\u7edc\\\\u65b0\\\\u578b\\\\u8fdd\\\\u6cd5\\\\u72af\\\\u7f6a\\\\u4ea4\\\\u6613\\\\u98ce\\\\u9669\\\\u4e8b\\\\u4ef6\\\\u7684\\\\u5e73\\\\u53f0\\\\u3002\\\\\"\", `source_id`: \"chunk-1815708c765a0bac407d91788fa567ca\"}', 'wrapped': 'SELECT * FROM ag_catalog.cypher(\\'graph_qatest\\', $$\\n           MERGE (n:`xe794b5e4bfa1e7bd91e7bb9ce696b0e59e8be8bf9de6b395e78aafe7bdaae4baa4e69893e9a38ee999a9e4ba8be4bbb6e7aea1e79086e5b9b3e58fb0`)\\n                SET n += {`entity_type`: \"\\\\\"ORGANIZATION\\\\\"\", `description`: \"\\\\\"\\\\u7535\\\\u4fe1\\\\u7f51\\\\u7edc\\\\u65b0\\\\u578b\\\\u8fdd\\\\u6cd5\\\\u72af\\\\u7f6a\\\\u4ea4\\\\u6613\\\\u98ce\\\\u9669\\\\u4e8b\\\\u4ef6\\\\u7ba1\\\\u7406\\\\u5e73\\\\u53f0\\\\u662f\\\\u516c\\\\u5b89\\\\u673a\\\\u5173\\\\u7528\\\\u4e8e\\\\u7ba1\\\\u7406\\\\u548c\\\\u67e5\\\\u8be2\\\\u7535\\\\u4fe1\\\\u7f51\\\\u7edc\\\\u65b0\\\\u578b\\\\u8fdd\\\\u6cd5\\\\u72af\\\\u7f6a\\\\u4ea4\\\\u6613\\\\u98ce\\\\u9669\\\\u4e8b\\\\u4ef6\\\\u7684\\\\u5e73\\\\u53f0\\\\u3002\\\\\"\", `source_id`: \"chunk-1815708c765a0bac407d91788fa567ca\"}\\n        $$) AS (a agtype)', 'detail': 'duplicate key value violates unique constraint \"pg_class_relname_nsp_index\"\\nDETAIL:  Key (relname, relnamespace)=(xe794b5e4bfa1e7bd91e7bb9ce696b0e59e8be8bf9de6b395e78aafe_id_seq, 28908) already exists.'}}\n...\n\n...\nEncoded label: xe98791e89e8de4bfa1e794a8e4bfa1e681afe59fbae7a180e695b0e68daee5\nEncoded label: xe587bae7a79fe38081e587bae5809fe38081e587bae594aee38081e8b4ade4\nEncoded label: xe794b5e4bfa1e7bd91e7bb9ce696b0e59e8be8bf9de6b395e78aafe7bdaa\nEncoded label: xe5aea2e688b7e9a38ee999a9e8af84e7baa7\nEncoded label: xe794b5e4bfa1e7bd91e7bb9ce696b0e59e8be8bf9de6b395e78aafe7bdaae4\nEncoded label: xe58692e5908de5bc80e688b7\nEncoded label: xe5aea2e688b7\nEncoded label: xe696b0e5bc80e7ab8be8b4a6e688b7e4b89ae58aa1\nEncoded label: xe5aea1e6a0b8e58a9be5baa6\nEncoded label: xe8aebee58cbae79a84e5b882e7baa7e58f8ae4bba5e4b88ae585ace5ae89e6\nEncoded label: xe58d95e4bd8d\nEncoded label: xe4b8aae4baba\nTraceback (most recent call last):\n  File \"/xxx/LightRAG/postgres_test.py\", line 191, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/xxx/LightRAG/postgres_test.py\", line 165, in main\n    await rag.aquery(\n  File \"/xxx/LightRAG/lightrag/lightrag.py\", line 699, in aquery\n    response = await kg_query(\n  File \"/xxx/LightRAG/lightrag/operate.py\", line 633, in kg_query\n    context = await _build_query_context(\n  File \"/xxx/LightRAG/lightrag/operate.py\", line 698, in _build_query_context\n    entities_context, relations_context, text_units_context = await _get_node_data(\n  File \"/xxx/LightRAG/lightrag/operate.py\", line 769, in _get_node_data\n    node_datas = await asyncio.gather(\n  File \"/xxx/LightRAG/lightrag/kg/postgres_impl.py\", line 834, in get_node\n    record = await self._query(query, **params)\n  File \"/xxx/LightRAG/lightrag/kg/postgres_impl.py\", line 792, in _query\n    result = [PGGraphStorage._record_to_dict(d) for d in data]\n  File \"/xxx/LightRAG/lightrag/kg/postgres_impl.py\", line 792, in <listcomp>\n    result = [PGGraphStorage._record_to_dict(d) for d in data]\n  File \"/xxx/LightRAG/lightrag/kg/postgres_impl.py\", line 577, in _record_to_dict\n    field[\"label\"] = PGGraphStorage._decode_graph_label(vertex[\"label\"])\n  File \"//LightRAG/lightrag/kg/postgres_impl.py\", line 648, in _decode_graph_label\n    return bytes.fromhex(encoded_label.removeprefix(\"x\")).decode()\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe5 in position 30: unexpected end of data\n\n```",
      "state": "closed",
      "author": "WellTung666",
      "author_type": "User",
      "created_at": "2025-01-20T03:22:15Z",
      "updated_at": "2025-02-19T20:24:04Z",
      "closed_at": "2025-02-19T20:24:04Z",
      "labels": [
        "bug",
        "PostgreSQL"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/606/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/606",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/606",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.533074",
      "comments": [
        {
          "author": "YanSte",
          "body": "Thanks for your feedbacks we will have a look.",
          "created_at": "2025-02-18T14:32:51Z"
        },
        {
          "author": "YanSte",
          "body": "This is relate to AGE, please have a look to https://github.com/HKUDS/LightRAG?tab=readme-ov-file#storage\n\nPostgres",
          "created_at": "2025-02-19T20:24:03Z"
        }
      ]
    },
    {
      "issue_number": 599,
      "title": "Why the _find_most_related_edges_from_entities use the sorted edge?",
      "body": "The code snippet captured and highlighed. I noticed that the `all_edges_pack` and `all_edges_degree` are using the sorted edge to retrieve them. I understand that the sorted_edge is used to avoid duplication, but when you are getting edges or degrees, you should use the original edge rather than the sorted edge?\n\n**so the `all_edges.append(sorted_edge)` should be `all_edges.append(e)`?**\n\n<img width=\"637\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3f5f225e-a80f-4a23-902a-82aa0034967f\" />",
      "state": "open",
      "author": "ShanGor",
      "author_type": "User",
      "created_at": "2025-01-18T07:43:14Z",
      "updated_at": "2025-02-19T20:23:06Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/599/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/599",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/599",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.718984",
      "comments": [
        {
          "author": "YanSte",
          "body": "@ParisNeo Do you have an answer ?\n",
          "created_at": "2025-02-19T20:23:01Z"
        }
      ]
    },
    {
      "issue_number": 879,
      "title": "[Question]: test",
      "body": "### Do you need to ask a question?\n\n- [x] I have searched the existing question and discussions and this question is not already answered.\n- [x] I believe this is a legitimate question, not just a bug or feature request.\n\n### Your Question\n\ntest\n\n### Additional Context\n\ntest",
      "state": "closed",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-19T19:31:46Z",
      "updated_at": "2025-02-19T19:31:48Z",
      "closed_at": "2025-02-19T19:31:48Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/879/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/879",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/879",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.920301",
      "comments": []
    },
    {
      "issue_number": 878,
      "title": "[Feature Request]: test",
      "body": "### Do you need to file a feature request?\n\n- [ ] I have searched the existing feature request and this feature request is not already filed.\n- [ ] I believe this is a legitimate feature request, not just a question or bug.\n\n### Feature Request Description\n\ntest\n\n### Additional Context\n\ntest",
      "state": "closed",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-19T19:31:25Z",
      "updated_at": "2025-02-19T19:31:31Z",
      "closed_at": "2025-02-19T19:31:31Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/878/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/878",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/878",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.920333",
      "comments": []
    },
    {
      "issue_number": 877,
      "title": "[Bug]: test",
      "body": "### Do you need to file an issue?\n\n- [x] I have searched the existing issues and this bug is not already filed.\n- [x] I believe this is a legitimate bug, not just a question or feature request.\n\n### Describe the bug\n\ntest\n\n### Steps to reproduce\n\ntest\n\n### Expected Behavior\n\ntest\n\n### LightRAG Config Used\n\n# Paste your config here\ntest\n\n### Logs and screenshots\n\ntest\n\n### Additional Information\n\n- LightRAG Version:\n- Operating System:\n- Python Version:\n- Related Issues:\n",
      "state": "closed",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-19T19:30:55Z",
      "updated_at": "2025-02-19T19:31:10Z",
      "closed_at": "2025-02-19T19:31:10Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/877/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/877",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/877",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.920339",
      "comments": []
    },
    {
      "issue_number": 863,
      "title": "Custom prompt not working (KeyError: 'context_data')",
      "body": "\nHello, trying last version with custom prompt \n\ntest:\n\ncustom_prompt = \"\"\"\nVous êtes un assistant expert dans les études vétérinaires. \nFournissez des réponses détaillées et structurées, en expliquant les processus de manière claire avec des exemples.\n\nIMPORTANT: Si vous ne trouvez pas l'information spécifique dans la base de connaissances, \nveuillez répondre clairement que vous ne disposez pas de cette information dans votre base de connaissances \nplutôt que d'essayer de générer une réponse approximative.\n\n---Knowledge Base---\n{context_data}\n\n---Conversation History---\n{history}\n\n---Response Rules---\n- Target format and length: {response_type}\n\"\"\"\n                result = st.session_state.rag.query(\n                    prompt,\n                    param=QueryParam(\n                    mode=search_mode,\n                    only_need_context=False,\n                    response_type=\"Multiple Paragraphs\",\n                    top_k=60,\n                    max_token_for_text_unit=4000,\n                    max_token_for_global_context=4000,\n                    max_token_for_local_context=4000,\n                    ),\n                    system_prompt=custom_prompt \n                )\n\n\n\nKeyError: 'context_data'\nTraceback:\nFile \"/Users/ilan/_INFOSTRATES/_AI/rcchat/app.py\", line 249, in <module>\n    main()\nFile \"/Users/ilan/_INFOSTRATES/_AI/rcchat/app.py\", line 225, in main\n    result = st.session_state.rag.query(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/miniconda3/envs/rcchat/lib/python3.12/site-packages/lightrag/lightrag.py\", line 1072, in query\n    return loop.run_until_complete(self.aquery(query, param, system_prompt))  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/miniconda3/envs/rcchat/lib/python3.12/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n    return f.result()\n           ^^^^^^^^^^\nFile \"/opt/miniconda3/envs/rcchat/lib/python3.12/asyncio/futures.py\", line 202, in result\n    raise self._exception.with_traceback(self._exception_tb)\nFile \"/opt/miniconda3/envs/rcchat/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\nFile \"/opt/miniconda3/envs/rcchat/lib/python3.12/site-packages/lightrag/lightrag.py\", line 1113, in aquery\n    response = await naive_query(\n               ^^^^^^^^^^^^^^^^^^\nFile \"/opt/miniconda3/envs/rcchat/lib/python3.12/site-packages/lightrag/operate.py\", line 1609, in naive_query\n    sys_prompt = sys_prompt_temp.format(\n                 ^^^^^^^^^^^^^^^^^^^^^^^",
      "state": "open",
      "author": "ilanb",
      "author_type": "User",
      "created_at": "2025-02-19T11:06:30Z",
      "updated_at": "2025-02-19T18:43:44Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/863/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/863",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/863",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:30.920347",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hello, could you please provide more examples, books, and more context to reproduce it.\n\nThanks.",
          "created_at": "2025-02-19T11:59:14Z"
        },
        {
          "author": "ilanb",
          "body": "Sure, here is the complete script used :\n\n`import os\nimport streamlit as st\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete, openai_embed\nimport docx\nimport glob\nimport time\nfrom dotenv import load_dotenv\nimport nest_asyncio\nimport asyncio\n\n# Activation",
          "created_at": "2025-02-19T16:35:58Z"
        },
        {
          "author": "YanSte",
          "body": "Thanks for sharing, we will have a look.",
          "created_at": "2025-02-19T18:43:36Z"
        }
      ]
    },
    {
      "issue_number": 745,
      "title": "insert_custom_kg:KeyError: 'tokens'",
      "body": "use this api: rag.insert_custom_kg(custom_kg)\n\nReport an error:\n\n  File \"/lightrag/lightrag.py\", line 580, in ainsert_custom_kg\n    await self.chunks_vdb.upsert(all_chunks_data)\n  File \"/lightrag/kg/postgres_impl.py\", line 374, in upsert\n    upsert_sql, data = self._upsert_chunks(item)\n  File \"/lightrag/kg/postgres_impl.py\", line 312, in _upsert_chunks\n    raise e\n  File \"/lightrag/kg/postgres_impl.py\", line 303, in _upsert_chunks\n    \"tokens\": item[\"tokens\"],\nKeyError: 'tokens'",
      "state": "closed",
      "author": "qiangyongjun",
      "author_type": "User",
      "created_at": "2025-02-11T03:00:56Z",
      "updated_at": "2025-02-19T14:20:31Z",
      "closed_at": "2025-02-19T14:20:31Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/745/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/745",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/745",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:31.155620",
      "comments": []
    },
    {
      "issue_number": 849,
      "title": "Light RagServer",
      "body": "Hey @ArnoChenFx \n\nI'm getting:\n```\n2025-02-18 23:01:03 INFO:Using the label default for PostgreSQL as identifier\n2025-02-18 23:01:03 INFO:Connected to PostgreSQL database at postgres:5432/mydatabase\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n2025-02-18 23:01:03 ERROR:Failed to check table LIGHTRAG_DOC_FULL in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_doc_full\" does not exist\n2025-02-18 23:01:03 INFO:Created table LIGHTRAG_DOC_FULL in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n2025-02-18 23:01:03 ERROR:Failed to check table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\n2025-02-18 23:01:03 INFO:Created table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n2025-02-18 23:01:03 ERROR:Failed to check table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\n2025-02-18 23:01:03 INFO:Created table LIGHTRAG_VDB_ENTITY in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n2025-02-18 23:01:03 ERROR:Failed to check table LIGHTRAG_VDB_RELATION in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\n2025-02-18 23:01:03 INFO:Created table LIGHTRAG_VDB_RELATION in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n2025-02-18 23:01:03 ERROR:Failed to check table LIGHTRAG_LLM_CACHE in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_llm_cache\" does not exist\n2025-02-18 23:01:03 INFO:Created table LIGHTRAG_LLM_CACHE in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n2025-02-18 23:01:03 ERROR:Failed to check table LIGHTRAG_DOC_STATUS in PostgreSQL database\n2025-02-18 23:01:03 ERROR:PostgreSQL database error: relation \"lightrag_doc_status\" does not exist\n2025-02-18 23:01:03 INFO:Created table LIGHTRAG_DOC_STATUS in PostgreSQL database\n2025-02-18 23:01:03 INFO:Finished checking all tables in PostgreSQL database\n2025-02-18 23:01:03 INFO:Injected postgres_db to full_docs\n2025-02-18 23:01:03 INFO:Injected postgres_db to text_chunks\n2025-02-18 23:01:03 INFO:Injected postgres_db to chunk_entity_relation_graph\n2025-02-18 23:01:03 INFO:Injected postgres_db to entities_vdb\n2025-02-18 23:01:03 INFO:Injected postgres_db to relationships_vdb\n2025-02-18 23:01:03 INFO:Injected postgres_db to chunks_vdb\n2025-02-18 23:01:03 INFO:Injected postgres_db to doc_status\n2025-02-18 23:01:03 INFO:Injected postgres_db to llm_response_cache\n2025-02-18 23:01:03 INFO:     Application startup complete.\n```",
      "state": "closed",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-18T22:03:17Z",
      "updated_at": "2025-02-19T13:16:36Z",
      "closed_at": "2025-02-19T11:25:56Z",
      "labels": [
        "bug",
        "docker"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/849/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/849",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/849",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:31.155646",
      "comments": [
        {
          "author": "YanSte",
          "body": "@ArnoChenFx ",
          "created_at": "2025-02-18T22:18:41Z"
        },
        {
          "author": "YanSte",
          "body": "Here the log, my config was working before:\n\n```\n2025-02-19 00:17:51 INFO:     Started server process [1]\n2025-02-19 00:17:51 INFO:     Waiting for application startup.\n2025-02-19 00:17:51 INFO:Using the label default for PostgreSQL as identifier\n2025-02-19 00:17:51 INFO:Connected to PostgreSQL data",
          "created_at": "2025-02-18T23:06:30Z"
        },
        {
          "author": "ArnoChenFx",
          "body": "```pythin\n async def check_tables(self):\n        for k, v in TABLES.items():\n            try:\n                await self.query(\"SELECT 1 FROM {k} LIMIT 1\".format(k=k))\n            except Exception as e:\n                logger.error(f\"Failed to check table {k} in PostgreSQL database\")\n               ",
          "created_at": "2025-02-19T04:05:12Z"
        },
        {
          "author": "mirzabaig14",
          "body": "@YanSte you can do the following for installing the vector extension, I did this on my ubuntu:\n\n```\nsudo apt update\nsudo apt install postgresql-server-dev-14 build-essential git\n\n# Clone the repository\ngit clone --branch v0.5.1 https://github.com/pgvector/pgvector.git\ncd pgvector\n\n# Build and instal",
          "created_at": "2025-02-19T06:16:11Z"
        },
        {
          "author": "YanSte",
          "body": "@mirzabaig14 Thanks I will have a look 🙏🏻",
          "created_at": "2025-02-19T09:25:08Z"
        }
      ]
    },
    {
      "issue_number": 855,
      "title": "Run examples/lightrag_zhipu_postgres_demo.py failed:  RuntimeWarning: coroutine 'LightRAG.initialize_storages' was never awaited",
      "body": "```\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ~~~~~~~~~~^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 720, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"/private/var/www/LightRAG/hello2.py\", line 32, in main\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n    ...<15 lines>...\n        vector_storage=\"PGVectorStorage\",\n    )\n  File \"<string>\", line 34, in __init__\n  File \"/private/var/www/LightRAG/lightrag/lightrag.py\", line 563, in __post_init__\n    loop.run_until_complete(self.initialize_storages())\n    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 696, in run_until_complete\n    self._check_running()\n    ~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 632, in _check_running\n    raise RuntimeError('This event loop is already running')\nRuntimeError: This event loop is already running\nINFO:Creating a new event loop in main thread.\n<sys>:0: RuntimeWarning: coroutine 'LightRAG.initialize_storages' was never awaited\n```",
      "state": "closed",
      "author": "tevooli",
      "author_type": "User",
      "created_at": "2025-02-19T08:49:00Z",
      "updated_at": "2025-02-19T09:38:24Z",
      "closed_at": "2025-02-19T09:38:18Z",
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/855/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/855",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/855",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:31.350694",
      "comments": [
        {
          "author": "YanSte",
          "body": "Are you using a notebook ?\n\n``\nimport nest_asyncio\nnest_asyncio.apply()\n``\n",
          "created_at": "2025-02-19T09:11:33Z"
        },
        {
          "author": "tevooli",
          "body": "@YanSte Thank you. This line of code solved this error.",
          "created_at": "2025-02-19T09:32:33Z"
        }
      ]
    },
    {
      "issue_number": 859,
      "title": "Tiktoken error",
      "body": "File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/lightrag.py\", line 1092, in aquery\n    response = await kg_query(\n               ^^^^^^^^^^^^^^^\n\n\nFile \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/operate.py\", line 617, in kg_query\n    context = await _build_query_context(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/operate.py\", line 1010, in _build_query_context\n    ll_data, hl_data = await asyncio.gather(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/operate.py\", line 1098, in _get_node_data\n    use_text_units, use_relations = await asyncio.gather(\n                                    ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/operate.py\", line 1291, in _find_most_related_edges_from_entities\n    all_edges_data = truncate_list_by_token_size(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/utils.py\", line 246, in truncate_list_by_token_size\n    tokens += len(encode_string_by_tiktoken(key(data)))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/lightrag/utils.py\", line 194, in encode_string_by_tiktoken\n    tokens = ENCODER.encode(content)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/pypoetry/virtualenvs/lightrag-9TtSrW0h-py3.12/lib/python3.12/site-packages/tiktoken/core.py\", line 116, in encode\n    if match := _special_token_regex(disallowed_special).search(text):\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: expected string or buffer",
      "state": "open",
      "author": "ultrageopro",
      "author_type": "User",
      "created_at": "2025-02-19T09:22:46Z",
      "updated_at": "2025-02-19T09:26:10Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/859/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/859",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/859",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:31.551390",
      "comments": []
    },
    {
      "issue_number": 827,
      "title": "BUG: Error in get_kg_context: expected string or buffer",
      "body": "As described, after updating, an error occurs.\n\n\n\n\n\n\n\n\n",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-02-18T06:04:29Z",
      "updated_at": "2025-02-19T09:03:44Z",
      "closed_at": null,
      "labels": [
        "bug",
        "neo4j"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/827/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/827",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/827",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:31.551406",
      "comments": [
        {
          "author": "FeHuynhVI",
          "body": "This is the data that I debugged.\n\n![Image](https://github.com/user-attachments/assets/40ab0742-b710-45ae-9f34-3487b7ea1e67)\n\n![Image](https://github.com/user-attachments/assets/0d1c59ba-4c45-4f37-9513-87e339ee186e)\n\n![Image](https://github.com/user-attachments/assets/44cae6f4-ee3c-4541-a0d0-511690d",
          "created_at": "2025-02-18T10:48:51Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "This is my temporary fix.\n` all_edges_data = truncate_list_by_token_size(\n        all_edges_data,\n        key=lambda x: x[\"description\"] if x[\"description\"] != None else \"\" ,\n        max_token_size=query_param.max_token_for_global_context,\n    )`",
          "created_at": "2025-02-18T11:15:23Z"
        },
        {
          "author": "YanSte",
          "body": "Thanks for sharing this help me a lot.",
          "created_at": "2025-02-18T11:23:36Z"
        },
        {
          "author": "YanSte",
          "body": "Are you using NEO4j ?",
          "created_at": "2025-02-18T12:02:12Z"
        },
        {
          "author": "FeHuynhVI",
          "body": "> Are you using NEO4j ?\n\nYes",
          "created_at": "2025-02-19T03:53:43Z"
        }
      ]
    },
    {
      "issue_number": 787,
      "title": "graph_chunk_entity_relation.graphml损坏，能不能根据其他的json文件修复他",
      "body": "C:\\Users\\admin\\Anaconda\\envs\\lsdenv\\python.exe C:\\Users\\admin\\Desktop\\LightRAG-main\\examples\\问答.py \nINFO:numexpr.utils:NumExpr defaulting to 8 threads.\nINFO:httpx:HTTP Request: POST http://api.siliconflow.cn/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO:lightrag:Logger initialized for working directory: ./知识库\nDetected embedding dimension: 1024\nINFO:lightrag:Load KV llm_response_cache with 3 data\nINFO:lightrag:Load KV full_docs with 591 data\nINFO:lightrag:Load KV text_chunks with 2059 data\nINFO:lightrag:Loaded graph from ./知识库\\graph_chunk_entity_relation.graphml with 21116 nodes, 4517 edges\nINFO:nano-vectordb:Load (37175, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './知识库\\\\vdb_entities.json'} 37175 data\nINFO:nano-vectordb:Load (14470, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './知识库\\\\vdb_relationships.json'} 14470 data\nINFO:nano-vectordb:Load (2152, 1024) data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './知识库\\\\vdb_chunks.json'} 2152 data\nINFO:httpx:HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions \"HTTP/1.1 200 OK\"\n```json\n{\n  \"high_level_keywords\": [\"水文\", \"学科定义\", \"科学领域\"],\n  \"low_level_keywords\": [\"水循环\", \"水文现象\", \"水资源管理\", \"水文学科\"]\n}\n```\nINFO:lightrag:kw_prompt result:\nINFO:httpx:HTTP Request: POST http://api.siliconflow.cn/v1/embeddings \"HTTP/1.1 200 OK\"\nWARNING:lightrag:Some nodes are missing, maybe the storage is damaged\nINFO:lightrag:Local query uses 48 entites, 82 relations, 1 text units\nINFO:httpx:HTTP Request: POST http://api.siliconflow.cn/v1/embeddings \"HTTP/1.1 200 OK\"\nAn error occurred:\nWARNING:lightrag:Some edges are missing, maybe the storage is damaged\nTraceback (most recent call last):\n  File \"C:\\Users\\admin\\Desktop\\LightRAG-main\\examples\\问答.py\", line 83, in main\n    await rag.aquery(\n  File \"c:\\users\\admin\\desktop\\lightrag-main\\lightrag\\lightrag.py\", line 476, in aquery\n    response = await kg_query(\n               ^^^^^^^^^^^^^^^\n  File \"c:\\users\\admin\\desktop\\lightrag-main\\lightrag\\operate.py\", line 544, in kg_query\n    context = await _build_query_context(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\users\\admin\\desktop\\lightrag-main\\lightrag\\operate.py\", line 631, in _build_query_context\n    ) = await _get_edge_data(\n        ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\users\\admin\\desktop\\lightrag-main\\lightrag\\operate.py\", line 881, in _get_edge_data\n    edge_degree = await asyncio.gather(\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\users\\admin\\desktop\\lightrag-main\\lightrag\\storage.py\", line 262, in edge_degree\n    return int(self._graph.degree[src_id]) + int(self._graph.degree[tgt_id])\n               ~~~~~~~~~~~~~~~~~~^^^^^^^^\n  File \"C:\\Users\\admin\\Anaconda\\envs\\lsdenv\\Lib\\site-packages\\networkx\\classes\\reportviews.py\", line 524, in __getitem__\n    nbrs = self._succ[n]\n           ~~~~~~~~~~^^^\nKeyError: '\"气候因素\"<(\"ENTITY\"'",
      "state": "open",
      "author": "zhongc12",
      "author_type": "User",
      "created_at": "2025-02-15T07:38:32Z",
      "updated_at": "2025-02-19T06:36:04Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/787/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/787",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/787",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:31.742177",
      "comments": [
        {
          "author": "LarFii",
          "body": "最好是把这个entity从其他文件中也全部删除，添加会很麻烦",
          "created_at": "2025-02-19T06:36:03Z"
        }
      ]
    },
    {
      "issue_number": 843,
      "title": "Lightrag mongo example not storing data",
      "body": "I am using the example code give in lightrag_openai_mongodb_graph_demo.py which is in the examples folder.\n\nThis is my current code:\n\n```\nimport os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete, openai_embed\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\n#########\n# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n# import nest_asyncio\n# nest_asyncio.apply()\n#########\nWORKING_DIR = \"./mongodb_test_dir\"\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-proj-something\"\nos.environ[\"MONGO_URI\"] = \"mongodb://0.0.0.0:27017/?directConnection=true\"\nos.environ[\"MONGO_DATABASE\"] = \"LightRAG\"\nos.environ[\"MONGO_KG_COLLECTION\"] = \"MDB_KG\"\n\n# Embedding Configuration and Functions\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"text-embedding-3-large\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 8192))\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=EMBEDDING_MODEL,\n    )\n\n\nasync def get_embedding_dimension():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    return embedding.shape[1]\n\n\nasync def create_embedding_function_instance():\n    # Get embedding dimension\n    embedding_dimension = await get_embedding_dimension()\n    # Create embedding function instance\n    return EmbeddingFunc(\n        embedding_dim=embedding_dimension,\n        max_token_size=EMBEDDING_MAX_TOKEN_SIZE,\n        func=embedding_func,\n    )\n\n\nasync def initialize_rag():\n    embedding_func_instance = await create_embedding_function_instance()\n\n    return LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=gpt_4o_mini_complete,\n        embedding_func=embedding_func_instance,\n        graph_storage=\"MongoGraphStorage\",\n        log_level=\"DEBUG\",\n    )\n\n\n# Run the initialization\nrag = asyncio.run(initialize_rag())\n\nwith open(\"/home/amin/001-lightrag/docs/wot.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\n```\n\nThe code runs and this is the output in the terminal:\n\n```\n(001-lightrag) amin@amin-ai:~/001-lightrag/age$ /home/amin/.pyenv/versions/3.10.14/envs/001-lightrag/bin/python /home/amin/001-lightrag/lightrag_openai_mongodb_graph_demo.py\nINFO:nano-vectordb:Load (30, 3072) data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './mongodb_test_dir/vdb_entities.json'} 30 data\nINFO:nano-vectordb:Load (15, 3072) data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './mongodb_test_dir/vdb_relationships.json'} 15 data\nINFO:nano-vectordb:Load (2, 3072) data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './mongodb_test_dir/vdb_chunks.json'} 2 data\nINFO:lightrag:Loaded document status storage with 1 records\nINFO:lightrag:Creating a new event loop in main thread.\nINFO:lightrag:No new unique documents were found.\nINFO:lightrag:All documents have been processed or are duplicates\nINFO:lightrag:Non-embedding cached missed(mode:hybrid type:query)\nINFO:lightrag:Non-embedding cached missed(mode:hybrid type:keywords)\nDEBUG:lightrag:[kg_query]Prompt Tokens: 401\nDEBUG:lightrag:===== Sending Query to LLM =====\nDEBUG:lightrag:Model: gpt-4o-mini   Base URL: None\nDEBUG:lightrag:Additional kwargs: {}\nDEBUG:lightrag:High-level keywords: ['Story themes', 'Literary analysis', 'Narrative elements']\nDEBUG:lightrag:Low-level  keywords: ['Character development', 'Conflict', 'Setting', 'Symbolism', 'Plot']\nINFO:lightrag:Query nodes: Character development, Conflict, Setting, Symbolism, Plot, top_k: 60, cosine: 0.2\nINFO:lightrag:Query edges: Story themes, Literary analysis, Narrative elements, top_k: 60, cosine: 0.2\nDEBUG:lightrag:Truncate chunks from 2 to 2 (max tokens:4000)\nDEBUG:lightrag:Truncate relations from 9 to 9 (max tokens:4000)\nDEBUG:lightrag:Truncate entities from 16 to 16 (max tokens:4000)\nINFO:lightrag:Local query uses 16 entites, 9 relations, 2 chunks\nDEBUG:lightrag:Truncate relations from 11 to 11 (max tokens:4000)\nDEBUG:lightrag:Truncate chunks from 2 to 2 (max tokens:4000)\nDEBUG:lightrag:Truncate entities from 18 to 18 (max tokens:4000)\nINFO:lightrag:Global query uses 18 entites, 11 relations, 2 chunks\nDEBUG:lightrag:[kg_query]Prompt Tokens: 4974\nDEBUG:lightrag:===== Sending Query to LLM =====\nDEBUG:lightrag:Model: gpt-4o-mini   Base URL: None\nDEBUG:lightrag:Additional kwargs: {'stream': False}\n## Top Themes in \"The Strike at Shayol Ghul\"\n\n### Struggle Between Light and Darkness\nA central theme in the narrative is the conflict between the forces of good, represented by The Light, and the forces of evil, symbolized by The Shadow and the Dark One. This cosmic struggle is at the heart of the War of the Shadow, illustrating the perpetual fight for dominance over the world.\n\n### Sacrifice and Heroism\nThe story features legendary figures like Lews Therin Telamon and the Hundred Companions, who embody heroism and the willingness to make sacrifices for the greater good. Their collective efforts to combat the Dark One highlight the importance of collaboration and courage in the face of overwhelming odds.\n\n### The Nature of Power\nThe concept of power, particularly the One Power and artifacts like sa'angreal, plays a significant role in the story. The usage, risks, and ethical considerations surrounding these powerful tools reflect on the broader complexities of wielding power and its consequences.\n\n### Change and Transformation\nThe Breaking of the World and the War of the Shadow represent cataclysmic changes in society, culture, and the environment. This theme explores how significant events can reshape narratives and histories, as well as the implications of such transformations on the world and its inhabitants.\n\n### Strategy and Leadership\nLews Therin's plans for a direct attack on the Bore and the political discourse in the Hall of the Servants underscore themes of strategy and leadership. The discussions about the best course of action reflect the complexities involved in decision-making during times of conflict.\n\nThese themes intertwine throughout the narrative, enriching the storytelling and providing a multifaceted exploration of heroism, conflict, and the nature of existence within the fantasy world.\n(001-lightrag) amin@amin-ai:~/001-lightrag/age$ \n```\n\nBut when I check my mongdb I can see the the db and collections has been created but there is no data in it. Database is running locally\n\nAm I doing something wrong ??? Please advise as I am not sure what should be my next steps!\n",
      "state": "open",
      "author": "mirzabaig14",
      "author_type": "User",
      "created_at": "2025-02-18T16:59:53Z",
      "updated_at": "2025-02-18T19:48:41Z",
      "closed_at": null,
      "labels": [
        "mongoDB"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/843/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/843",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/843",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:31.919267",
      "comments": [
        {
          "author": "bzImage",
          "body": "Reporting the same issue.. not storing anything on my mongodb collection, just checked everything twice..",
          "created_at": "2025-02-18T19:03:52Z"
        },
        {
          "author": "YanSte",
          "body": "Can someone have a look to the Mango implementation ?",
          "created_at": "2025-02-18T19:48:19Z"
        }
      ]
    },
    {
      "issue_number": 807,
      "title": "Avoid importing unnecessary libraries when possible",
      "body": "@danielaskdd :  Hi. I was out for few days and today I wanted to take a look at the new api server.\nI can see this in the file:\n```python\nfrom .ollama_api import (\n    OllamaAPI,\n)\nfrom .ollama_api import ollama_server_infos\nfrom ..kg.postgres_impl import (\n    PostgreSQLDB,\n    PGKVStorage,\n    PGVectorStorage,\n    PGGraphStorage,\n    PGDocStatusStorage,\n)\nfrom ..kg.oracle_impl import (\n    OracleDB,\n    OracleKVStorage,\n    OracleVectorDBStorage,\n    OracleGraphStorage,\n)\nfrom ..kg.tidb_impl import (\n    TiDB,\n    TiDBKVStorage,\n    TiDBVectorDBStorage,\n    TiDBGraphStorage,\n)\n```\n\nWhenever someone uses the lightrag server, this would load all these DB storages at startup triggering automatic installation of all their dependencies.\n\nIf you take a look at each file, there is at startup a pipmaster verification of its dipendencies. whenever the module is loaded, pipmaster checks its dependencies and installs them if needed.\n\nSo importing them in the top of the file will result in installing everything which goes agaisnt this light installation spirit.\n\nFor example, you can see that the llm codes are imported only where we need them which means that if I use ollama and not openai, only the ollama library will be installed and not the openai one. \n\nCan you please make the imports only where they are needed. The whole point of using pipmaster is to segment everything into modules and install the requirements of the modules dynamically so that if someone don't use postgresql or oracle etc, the dependencies of these modules won't be installed. I did this for all llms and storages.\n\nI'm open to discussion if you please.\n\n@LarFii , do you agree about that?",
      "state": "closed",
      "author": "ParisNeo",
      "author_type": "User",
      "created_at": "2025-02-17T11:18:42Z",
      "updated_at": "2025-02-18T18:47:23Z",
      "closed_at": "2025-02-18T18:47:23Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 24,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/807/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/807",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/807",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:32.076841",
      "comments": []
    },
    {
      "issue_number": 786,
      "title": "Can chroma_impl support KV_STORAGE and GRAPH_STORAGE?",
      "body": "Can chroma_impl support KV_STORAGE and GRAPH_STORAGE?",
      "state": "open",
      "author": "hhxdestiny",
      "author_type": "User",
      "created_at": "2025-02-15T06:04:27Z",
      "updated_at": "2025-02-18T13:11:06Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "chroma"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/786/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/786",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/786",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:33.793046",
      "comments": [
        {
          "author": "spo0nman",
          "body": "Would be happy to take a shot! but can you please frame the feature request a bit better? What would you like to see in an ideal case?",
          "created_at": "2025-02-17T16:34:27Z"
        }
      ]
    },
    {
      "issue_number": 815,
      "title": "mix mode always returns None for context",
      "body": "It seems that mix mode always returns None for kg_context, even though I have tested it with many different queries.\n\nI am using Neo4j, with a total of 1,400 nodes and 1,742 relationships, but still not getting the appropriate context.\n![Image](https://github.com/user-attachments/assets/7f5f86dd-4236-47f2-86ce-48bc8ed44259)\n\n![Image](https://github.com/user-attachments/assets/6b43591c-8373-443c-b5ac-8476f23a9d6d)\n\n![Image](https://github.com/user-attachments/assets/39905f4b-b03c-4191-8304-7dfd3d248af1)",
      "state": "open",
      "author": "FeHuynhVI",
      "author_type": "User",
      "created_at": "2025-02-17T16:34:11Z",
      "updated_at": "2025-02-18T13:10:33Z",
      "closed_at": null,
      "labels": [
        "bug",
        "neo4j",
        "chroma"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/815/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/815",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/815",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:33.992559",
      "comments": []
    },
    {
      "issue_number": 839,
      "title": "TypeError: Can't instantiate abstract class JsonDocStatusStorage without an implementation for abstract method 'drop'",
      "body": null,
      "state": "closed",
      "author": "ultrageopro",
      "author_type": "User",
      "created_at": "2025-02-18T10:10:31Z",
      "updated_at": "2025-02-18T12:14:52Z",
      "closed_at": "2025-02-18T12:14:52Z",
      "labels": [
        "old-issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/839/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/839",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/839",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:33.992573",
      "comments": [
        {
          "author": "YanSte",
          "body": "Hi,\nCould you please update we fixed this issue.",
          "created_at": "2025-02-18T10:25:06Z"
        }
      ]
    },
    {
      "issue_number": 834,
      "title": "Check return Table query",
      "body": "All query from table need to be cast in type of the object. ",
      "state": "open",
      "author": "YanSte",
      "author_type": "User",
      "created_at": "2025-02-18T08:55:25Z",
      "updated_at": "2025-02-18T08:55:32Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/834/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/834",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/834",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:34.199248",
      "comments": []
    },
    {
      "issue_number": 801,
      "title": "Support for different model for insert and query.",
      "body": "It is convenient to use a reasoning model for query answering. But for entity extraction i want to use a nonreasoning model. Currently when initialize the light rag instance, we need to pass the llm and embedding model there. and they are used for query answering also. It will be good to have an option to choose a different model for query answering.\n\n![Image](https://github.com/user-attachments/assets/59c738ce-ddc3-47e0-b56d-77b606f6ef9e)\n\n\nThank you !!\n\n",
      "state": "closed",
      "author": "Aryabhattacharjee",
      "author_type": "User",
      "created_at": "2025-02-17T06:05:16Z",
      "updated_at": "2025-02-17T22:47:15Z",
      "closed_at": "2025-02-17T22:47:15Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/801/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/801",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/801",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:34.199274",
      "comments": [
        {
          "author": "YanSte",
          "body": "Thanks, but this feature is not planned for now. We will first focus on stabilizing a resilient version.",
          "created_at": "2025-02-17T22:47:13Z"
        }
      ]
    },
    {
      "issue_number": 635,
      "title": "PosgreSQL AGE Viewer node_ids as Text not HEX",
      "body": "Is there any way to see the node id's as text in the AGE viewer ? I've found a way to see it like this:\n\nFirst I made this custom function:\n```sql\nCREATE OR REPLACE FUNCTION hex_to_utf8(hex_str TEXT) RETURNS TEXT AS $$\nDECLARE\n    clean_hex TEXT;\n    utf8_str TEXT;\nBEGIN\n    -- Remove 'x' prefix\n    clean_hex := REGEXP_REPLACE(hex_str, '^x', '');\n    -- Convert from hex to bytes, then utf8\n    utf8_str := CONVERT_FROM(DECODE(clean_hex, 'hex'), 'UTF8');\n    RETURN utf8_str;\nEND;\n$$ LANGUAGE plpgsql;\n```\nThen I execute a query like this.\n```sql\nSELECT\n    hex_to_utf8((to_json(result.n)->'properties'->>'node_id')) AS node_id_utf8,\n\t*\nFROM cypher('amaine_pg', $$\n    MATCH (n)-[r]-(m)\n    RETURN n,r,m\n    LIMIT 10\n$$) AS result(\n    n agtype,\n\tr agtype,\n\tm agtype\n);\n```\n\nbut I can only see it in the table page \n\n![Image](https://github.com/user-attachments/assets/1c327a15-4b9f-482c-88e3-af481a56bfba)\n\nBut in the GRAPH visualizer I only get a blank screen.\n\n![Image](https://github.com/user-attachments/assets/0a58e867-7d18-4251-8a27-13bf206ad55c)\n\n\n\n",
      "state": "open",
      "author": "Daggle24",
      "author_type": "User",
      "created_at": "2025-01-24T07:05:43Z",
      "updated_at": "2025-02-17T22:39:35Z",
      "closed_at": null,
      "labels": [
        "question"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/635/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "YanSte"
      ],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/635",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/635",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:34.361144",
      "comments": []
    },
    {
      "issue_number": 767,
      "title": "Error in indexing with Postgres as store (Vector, Graph)",
      "body": "Good morning,\n \ntesting the postgres implementation for incrementally updating the Knowledge Graph with an one additional document, we incur in the following error:\n \n```\nERROR:asyncio:Task exception was never retrieved\nfuture: <Task finished name='Task-2101' coro=<_merge_edges_then_upsert() done, defined at C:\\Users\\Downloads\\light_rag_11\\.venv\\Lib\\site-packages\\lightrag\\operate.py:227> exception=TypeError(\"'NoneType' object is not subscriptable\")>\nTraceback (most recent call last):\n  File \"C:\\Users\\Downloads\\light_rag_11\\.venv\\Lib\\site-packages\\lightrag\\operate.py\", line 241, in _merge_edges_then_upsert\n    already_weights.append(already_edge[\"weight\"])\n                           ~~~~~~~~~~~~^^^^^^^^^^\nTypeError: 'NoneType' object is not subscriptable\n```\n \nThis error seems to come from the fact that the \"DIRECTED\" table of the graph schema is empty in the \"properties\" column\n \n![Image](https://github.com/user-attachments/assets/96f0460c-57b4-487e-b9fb-69d0d3a96ec2)\n\nWhile for the \"Entity\" table, the same column is populated\n \n![Image](https://github.com/user-attachments/assets/2cd8ac18-f15d-47b6-a5ff-1a484cd09cfd)\n\npackage version: 1.1.5\nPython version: 3.11.9\n\nsample code used to create index:\n \n```python\nasync def lightrag_init(first_time_init = False):\n    await postgres_db.initdb()\n \n    # Check if PostgreSQL DB tables exist, if not, tables will be created\n    await postgres_db.check_tables()\n \n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dimension,\n            max_token_size=8192,\n            func=embedding_func\n        ),\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        vector_storage=\"PGVectorStorage\",\n        embedding_func_max_async = 32,\n        llm_model_max_async = 32\n    )\n \n    # Set the KV/vector/graph storage's `db` property, so all operation will use same connection pool\n    rag.doc_status.db = postgres_db\n    rag.full_docs.db = postgres_db\n    rag.text_chunks.db = postgres_db\n    rag.llm_response_cache.db = postgres_db\n    rag.key_string_value_json_storage_cls.db = postgres_db\n    rag.chunks_vdb.db = postgres_db\n    rag.relationships_vdb.db = postgres_db\n    rag.entities_vdb.db = postgres_db\n    rag.graph_storage_cls.db = postgres_db\n    rag.chunk_entity_relation_graph.db = postgres_db\n \n    # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n \n    return rag\n \nasync def indexer(rag):\n    with open(f\"./dora_3.txt\", \"r\", encoding=\"utf-8\") as f:\n        await rag.ainsert(f.read())\n \nasync def main():\n    rag = await lightrag_init(False)\n    await indexer(rag)\n \nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "state": "closed",
      "author": "fedehann",
      "author_type": "User",
      "created_at": "2025-02-13T18:35:39Z",
      "updated_at": "2025-02-17T21:48:21Z",
      "closed_at": "2025-02-17T21:48:21Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/767/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/767",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/767",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:34.361179",
      "comments": [
        {
          "author": "ArindamRoy23",
          "body": "I faced the same issue as well  ",
          "created_at": "2025-02-14T07:47:18Z"
        },
        {
          "author": "YanSte",
          "body": "Fixed in the next version",
          "created_at": "2025-02-17T21:43:04Z"
        }
      ]
    },
    {
      "issue_number": 374,
      "title": "time-aware (temporal) KG that evolves as timeline changes... dynamic, state-based  - zep's \"graphiti\"",
      "body": "this repo might help augmenting LightRAG to support time-awareness (temporal KG):\r\n\r\nhttps://help.getzep.com/graphiti/graphiti/overview\r\n\r\n```\r\nGraphiti builds dynamic, temporally-aware knowledge graphs that represent complex, evolving relationships between entities over time. It ingests both unstructured and structured data, and the resulting graph may be queried using a fusion of time, full-text, semantic, and graph algorithm approaches.\r\n\r\nWith Graphiti, you can build LLM applications such as:\r\n\r\nAssistants that learn from user interactions, fusing personal knowledge with dynamic data from business systems like CRMs and billing platforms.\r\nAgents that autonomously execute complex tasks, reasoning with state changes from multiple dynamic sources.\r\n```",
      "state": "closed",
      "author": "mchzimm",
      "author_type": "User",
      "created_at": "2024-12-02T18:54:48Z",
      "updated_at": "2025-02-17T11:00:00Z",
      "closed_at": "2025-02-17T10:59:49Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/374/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/374",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/374",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:36.320226",
      "comments": [
        {
          "author": "LarFii",
          "body": "Thank you for sharing this valuable resource!",
          "created_at": "2024-12-09T02:29:07Z"
        }
      ]
    },
    {
      "issue_number": 239,
      "title": "Query Answer Citations",
      "body": "LightRAG is a very positive advancement for more precise RAG answers. It would be very help that with those query responses, there can be a way to provide some or all citations - such as the document names referenced or page numbers of the document referenced. This would make much better for the user to easily verify the accuracy of the answer(s). ",
      "state": "closed",
      "author": "kevinsosborne",
      "author_type": "User",
      "created_at": "2024-11-09T19:12:27Z",
      "updated_at": "2025-02-17T10:55:46Z",
      "closed_at": "2025-02-17T10:55:44Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/239/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/239",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/239",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:38.284522",
      "comments": [
        {
          "author": "aiproductguy",
          "body": "Do you mind drawing out your idea further? Send me a link here or just use this [excali board](https://excalidraw.com/#room=d3c27a90c8aecede902b,rqMb1oNFV-PFNnxRp_sAkQ). \r\n\r\nWorking demo: https://lightrag-gui.streamlit.app/",
          "created_at": "2024-11-10T04:03:00Z"
        },
        {
          "author": "Jaykumaran",
          "body": "@aiproductguy Your OpenAI API key is easily visible and its a extremely costly process, why you need to share your demo that uses your credits as publicly acessible. Do some workaround.",
          "created_at": "2024-11-10T05:24:18Z"
        },
        {
          "author": "kevinsosborne",
          "body": "@aiproductguy Thank you for interest in my suggestion. \r\n\r\nI am not a skilled software developer, but I think having citations as part of the response would really take LightRAG to the next level. Users need to be able to verify the answers and cross check the results to ensure it is trustworthy eas",
          "created_at": "2024-11-10T05:39:25Z"
        },
        {
          "author": "Jaykumaran",
          "body": "Hello,\r\n\r\nI think GraphRAG pipeline already has citations with a particular entity or relation. You may refer to that to adapt it to LightRAG. But yes it will be nice if it is available by default in LightRAG.",
          "created_at": "2024-11-11T04:22:23Z"
        },
        {
          "author": "amirsa66",
          "body": "> Do you mind drawing out your idea further? Send me a link here or just use this [excali board](https://excalidraw.com/#room=d3c27a90c8aecede902b,rqMb1oNFV-PFNnxRp_sAkQ).\r\n> \r\n> Working demo: https://lightrag-gui.streamlit.app/\r\n\r\n@aiproductguy  Would you provide the .py code for this demo?",
          "created_at": "2024-11-11T05:51:17Z"
        }
      ]
    },
    {
      "issue_number": 180,
      "title": "Incorporate litellm to enable the use much more LLMs and embeddings",
      "body": "Looks like a very promising project. I would suggest incorporating [litellm](https://github.com/BerriAI/litellm) to allow the use of any LLM and embedding from providers like Anthropic, Gemini, Mistral, Groq, Cohere, etc. Basically using litellm will cover for a broad range of proprietary models.",
      "state": "closed",
      "author": "dl423",
      "author_type": "User",
      "created_at": "2024-10-30T22:52:06Z",
      "updated_at": "2025-02-17T10:55:26Z",
      "closed_at": "2025-02-17T10:55:25Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/180/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/180",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/180",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:38.491027",
      "comments": [
        {
          "author": "LarFii",
          "body": "Good suggestion! We will try it lately.",
          "created_at": "2024-11-01T05:52:04Z"
        },
        {
          "author": "spo0nman",
          "body": "I\"m going to give this a shot and report back with a PR if no one else is working on this\r\n",
          "created_at": "2024-12-03T09:28:28Z"
        },
        {
          "author": "YanSte",
          "body": "Thanks, close (old issue).",
          "created_at": "2025-02-17T10:55:25Z"
        }
      ]
    },
    {
      "issue_number": 21,
      "title": "TypeError: cannot pickle '_thread.RLock' object",
      "body": "from lightrag import LightRAG, QueryParam\r\nfrom langchain_openai import ChatOpenAI\r\nimport os\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nllm_4o_mini = ChatOpenAI(\r\n    temperature=0,\r\n    model_name=\"gpt-4o-mini\",\r\n    openai_api_key=\"......\",\r\n    openai_api_base=\"......\",\r\n)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_4o_mini\r\n)\r\n\r\nwith open(\"./book.txt\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")))\r\n\r\n# Perform local search\r\nprint(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\")))\r\n\r\n# Perform global search\r\nprint(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\")))\r\n\r\n# Perform hybrid search\r\nprint(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")))\r\n\r\n-----------------------------------------------------------------------------------------------------------------\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\AIGC\\idataai_py\\test\\rag\\light_rag.py\", line 18, in <module>\r\n    rag = LightRAG(\r\n          ^^^^^^^^^\r\n  File \"<string>\", line 25, in __init__\r\n  File \"D:\\AIGC\\idataai_py\\venv\\Lib\\site-packages\\lightrag\\lightrag.py\", line 106, in __post_init__\r\n    _print_config = \",\\n  \".join([f\"{k} = {v}\" for k, v in asdict(self).items()])\r\n                                                           ^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\dataclasses.py\", line 1284, in asdict\r\n    return _asdict_inner(obj, dict_factory)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\dataclasses.py\", line 1291, in _asdict_inner\r\n    value = _asdict_inner(getattr(obj, f.name), dict_factory)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\dataclasses.py\", line 1325, in _asdict_inner\r\n    return copy.deepcopy(obj)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bozhum\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copy.py\", line 161, in deepcopy\r\n    rv = reductor(4)\r\n         ^^^^^^^^^^^\r\nTypeError: cannot pickle '_thread.RLock' object\r\n\r\n",
      "state": "closed",
      "author": "Rainismer",
      "author_type": "User",
      "created_at": "2024-10-16T02:53:59Z",
      "updated_at": "2025-01-30T15:36:12Z",
      "closed_at": "2024-10-16T06:57:45Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/21/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/21",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/21",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:38.702330",
      "comments": [
        {
          "author": "LarFii",
          "body": "It seems that the issue might be related to the Python version. Could you try using Python 3.10 instead and see if it resolves the problem?",
          "created_at": "2024-10-16T06:44:01Z"
        },
        {
          "author": "Rainismer",
          "body": "@LarFii My Python cannot be downgraded to 3.10. Additionally, I found that lightrag depends on hnswlib, which conflicts with chroma-hnswlib. I hope your new version can resolve these issues.",
          "created_at": "2024-10-16T06:51:11Z"
        },
        {
          "author": "Rainismer",
          "body": "@LarFii 我们也可以用中文交流。",
          "created_at": "2024-10-16T06:52:28Z"
        },
        {
          "author": "LarFii",
          "body": "好的，我后续会解决这些兼容性问题，解决后会第一时间通知你，请持续关注:)",
          "created_at": "2024-10-16T06:55:03Z"
        },
        {
          "author": "LarFii",
          "body": "能给我一份你现有的package list吗，方便修复兼容性问题",
          "created_at": "2024-10-16T06:56:16Z"
        }
      ]
    },
    {
      "issue_number": 236,
      "title": "[Bug] Incorrect documentation for file upload in /insert_file endpoint",
      "body": "---\r\n\r\n**Description:**\r\n\r\nWhen following the provided documentation for uploading a file to the `/insert_file` endpoint, I encountered an error. The documentation suggests using a JSON body with a `file_path` field, but this results in a `422 Unprocessable Entity` error. After some investigation, I found that the correct way to upload a file is by using the `-F` option in `curl` to specify the file directly.\r\n\r\n**Steps to Reproduce:**\r\n\r\n1. Follow the provided documentation to upload a file:\r\n   ```sh\r\n   curl -X POST \"http://127.0.0.1:8020/insert_file\" \\\r\n        -H \"Content-Type: application/json\" \\\r\n        -d '{\"file_path\": \"./book.txt\"}'\r\n   ```\r\n\r\n2. Observe the following errors:\r\n   - **Server Error:**\r\n     ```\r\n     127.0.0.1:57274 - \"POST /insert_file HTTP/1.1\" 422 Unprocessable Entity\r\n     ```\r\n   - **Client Error:**\r\n     ```json\r\n     {\r\n       \"detail\": [\r\n         {\r\n           \"type\": \"missing\",\r\n           \"loc\": [\"body\", \"file\"],\r\n           \"msg\": \"Field required\",\r\n           \"input\": null\r\n         }\r\n       ]\r\n     }\r\n     ```\r\n\r\n**Correct Usage:**\r\n\r\nTo correctly upload a file to the `/insert_file` endpoint, use the following `curl` command:\r\n```sh\r\ncurl -X POST \"http://127.0.0.1:8020/insert_file\" -F \"file=@path/to/your/example.txt\"\r\n```\r\n\r\n**Expected Behavior:**\r\n\r\nThe server should accept the file and return a success response, such as:\r\n```json\r\n{\r\n  \"status\": \"success\",\r\n  \"message\": \"File content from example.txt inserted successfully\"\r\n}\r\n```\r\n\r\n**Additional Information:**\r\n\r\n- **Endpoint Source Code:**\r\n- lightrag_api_openai_compatible_demo.py , line 120\r\n  ```python\r\n  @app.post(\"/insert_file\", response_model=Response)\r\n  async def insert_file(file: UploadFile = File(...)):\r\n      try:\r\n          file_content = await file.read()\r\n          # Read file content\r\n          try:\r\n              content = file_content.decode(\"utf-8\")\r\n          except UnicodeDecodeError:\r\n              # If UTF-8 decoding fails, try other encodings\r\n              content = file_content.decode(\"gbk\")\r\n          # Insert file content\r\n          loop = asyncio.get_event_loop()\r\n          await loop.run_in_executor(None, lambda: rag.insert(content))\r\n\r\n          return Response(\r\n              status=\"success\",\r\n              message=f\"File content from {file.filename} inserted successfully\",\r\n          )\r\n      except Exception as e:\r\n          raise HTTPException(status_code=500, detail=str(e))\r\n  ```\r\n\r\n- **Environment:**\r\n  - version : lightrag-hku = 0.0.8\r\n\r\n**Suggested Fix:**\r\n\r\nUpdate the documentation to reflect the correct usage of the `/insert_file` endpoint. Specifically, change the example `curl` command to use the `-F` option for file upload.\r\n\r\nThank you for your attention to this issue.\r\n\r\n---",
      "state": "closed",
      "author": "DHengW",
      "author_type": "User",
      "created_at": "2024-11-08T15:52:33Z",
      "updated_at": "2025-01-23T07:01:50Z",
      "closed_at": "2025-01-23T07:01:50Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/236/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/236",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/236",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:41.280321",
      "comments": [
        {
          "author": "suercxx",
          "body": "这个怎么上传其他类型的文件啊",
          "created_at": "2024-11-13T01:52:10Z"
        }
      ]
    },
    {
      "issue_number": 219,
      "title": "Reranker是否可加",
      "body": "在Embedding检索后，有没有必要加Reranker模型优化检索结果的排序，不知大佬是否试过效果",
      "state": "closed",
      "author": "feihoaze",
      "author_type": "User",
      "created_at": "2024-11-06T08:30:01Z",
      "updated_at": "2025-01-23T06:59:16Z",
      "closed_at": "2025-01-23T06:59:16Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/219/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/219",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/219",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:41.442494",
      "comments": [
        {
          "author": "ardyli",
          "body": "是的支持",
          "created_at": "2024-11-06T15:27:28Z"
        },
        {
          "author": "feihoaze",
          "body": "好的，谢谢！",
          "created_at": "2024-11-12T01:13:31Z"
        },
        {
          "author": "LarFii",
          "body": "后续我们会测试一下具体的效果",
          "created_at": "2024-11-12T03:12:05Z"
        },
        {
          "author": "feihoaze",
          "body": "嗯嗯，好的",
          "created_at": "2024-11-12T03:14:18Z"
        }
      ]
    },
    {
      "issue_number": 212,
      "title": "How to speed up insert process?",
      "body": "The insert process is quite slow for a small document. I tried to change `llm_model_max_async` value but the speed is never change. I also saw that the insert process is only using single core of my CPU. Is there any way to speed up the process? Maybe by using multiple thread or process?",
      "state": "closed",
      "author": "fahadh4ilyas",
      "author_type": "User",
      "created_at": "2024-11-05T10:41:58Z",
      "updated_at": "2025-01-23T06:59:09Z",
      "closed_at": "2025-01-23T06:59:09Z",
      "labels": [
        "good first PR"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 16,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/212/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/212",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/212",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:41.689363",
      "comments": [
        {
          "author": "JavieHush",
          "body": "Try to use GPU instead, the spped will boost up. The insert process of LightRAG is much faster than that in GraphRAG, based on my actual testing.\r\n",
          "created_at": "2024-11-06T02:51:17Z"
        },
        {
          "author": "abylikhsanov",
          "body": "@JavieHush Can you elaborate on that more? ",
          "created_at": "2024-11-06T07:09:01Z"
        },
        {
          "author": "Jaykumaran",
          "body": "@JavieHush \r\n\r\nFacing same issue, Could you describe how to achieve this?",
          "created_at": "2024-11-06T07:14:20Z"
        },
        {
          "author": "JavieHush",
          "body": "Guys :) I'm not quite sure about the situation you've encountered. my detailed situation is as follows\r\n\r\n### Suggestions\r\nThe insert process is highly related to LLM/Embedding model (the process use LLM to extract entities & relations, and EB model to index). This requires a significant amount of c",
          "created_at": "2024-11-06T09:21:52Z"
        },
        {
          "author": "abylikhsanov",
          "body": "@JavieHush That is why I got confused as in my situation I am not running LLM locally but rather using APIs so wondered what did you mean by using GPU.\r\n",
          "created_at": "2024-11-06T09:24:30Z"
        }
      ]
    },
    {
      "issue_number": 208,
      "title": "I want to directly use the locally built knowledge graph for subsequent RAG",
      "body": "I want to directly use the locally built knowledge graph for subsequent RAG. How can I do this? This involves the needs of vertical fields. It is difficult for general large models to build a satisfactory knowledge graph.",
      "state": "closed",
      "author": "cdcstu",
      "author_type": "User",
      "created_at": "2024-11-05T05:24:21Z",
      "updated_at": "2025-01-23T06:58:40Z",
      "closed_at": "2025-01-23T06:58:40Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/208/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/208",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/208",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:42.014677",
      "comments": [
        {
          "author": "LarFii",
          "body": "That’s definitely a useful feature, as LLM-generated results are often unsatisfactory. For now, you can try building based on the existing graph structure. We will also provide a method for inserting based on a local KG in the future, though this may take some time.",
          "created_at": "2024-11-07T07:20:59Z"
        }
      ]
    },
    {
      "issue_number": 186,
      "title": "BUG: LightRag Constructor Tries To Create Log File Before Folder Exists",
      "body": "If you try to create a LightRag object\r\n\r\nrag = LightRag(...)  but leave the working directory blank to use the default \r\n```/lightrag_cache_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S' ```  \r\n\r\nit throws an error since in ```__post_init__``` it calls ```set_logger(log_file)``` before the folder is created.\r\n\r\nFIX: move create of log file and call to set_logger below the creation of the folder.  \r\n\r\nAlso for the default name you have the time in there with colons - on windows at least I don't think thats allowed, should probably be underscores for all seperators or something along those lines.",
      "state": "closed",
      "author": "Justinius",
      "author_type": "User",
      "created_at": "2024-10-31T21:14:51Z",
      "updated_at": "2025-01-23T06:58:24Z",
      "closed_at": "2025-01-23T06:58:24Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/186/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/186",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/186",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:42.236068",
      "comments": [
        {
          "author": "LarFii",
          "body": "I'll fix it ASAP.",
          "created_at": "2024-11-07T06:58:40Z"
        }
      ]
    },
    {
      "issue_number": 225,
      "title": "请问支持自定义schema吗",
      "body": "支持自定义schema吗，大模型抽取的实体以及关系很杂乱，要是支持自定义schema就好了",
      "state": "closed",
      "author": "he-boy",
      "author_type": "User",
      "created_at": "2024-11-07T07:46:32Z",
      "updated_at": "2025-01-08T00:17:49Z",
      "closed_at": "2024-11-19T07:25:02Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/HKUDS/LightRAG/issues/225/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/HKUDS/LightRAG/issues/225",
      "api_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/225",
      "repository": "HKUDS/LightRAG",
      "extraction_date": "2025-06-21T23:35:42.439229",
      "comments": [
        {
          "author": "LarFii",
          "body": "暂时还没有提供支持，后续我们会考虑加入这个功能",
          "created_at": "2024-11-12T03:15:08Z"
        },
        {
          "author": "adamwuyu",
          "body": "请支持此功能，非常期待，可以参考neo4j-graphrag-python",
          "created_at": "2025-01-08T00:17:47Z"
        }
      ]
    }
  ]
}