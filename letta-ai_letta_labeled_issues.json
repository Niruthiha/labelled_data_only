{
  "repository": "letta-ai/letta",
  "repository_info": {
    "repo": "letta-ai/letta",
    "stars": 16962,
    "language": "Python",
    "description": "Letta (formerly MemGPT) is the stateful agents framework with memory, reasoning, and context management.",
    "url": "https://github.com/letta-ai/letta",
    "topics": [
      "ai",
      "ai-agents",
      "llm",
      "llm-agent"
    ],
    "created_at": "2023-10-11T07:38:37Z",
    "updated_at": "2025-06-22T02:16:57Z",
    "search_query": "ai agent language:python stars:>3 -framework",
    "total_issues_estimate": 66,
    "labeled_issues_estimate": 34,
    "labeling_rate": 52.8,
    "sample_labeled": 19,
    "sample_total": 36,
    "has_issues": true,
    "repo_id": 703411624,
    "default_branch": "main",
    "size": 44368
  },
  "extraction_date": "2025-06-21T23:36:13.445329",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 85,
  "issues": [
    {
      "issue_number": 2613,
      "title": "Code Injection Vulnerability Caused by eval() in function_message Function",
      "body": "### Description\nThis code is vulnerable to CWE - 94: Code Injection due to the use of the `eval()` function. The `function_message` function processes a string `msg`. When `msg` starts with \"Running \" and matches a specific regular expression, the function extracts the `function_name` and `function_args`. For certain `function_name` values, it uses `eval()` to execute the `function_args` string as a Python expression. \n\nImpacted code\n\nhttps://github.com/letta-ai/letta/blob/fdefb8a9cdaeeee72705a0660a9b9758e2e3ec59/letta/interface.py#L199\n\nhttps://github.com/letta-ai/letta/blob/fdefb8a9cdaeeee72705a0660a9b9758e2e3ec59/letta/interface.py#L225\n\nThe problem is that `eval()` can execute any valid Python code. If an attacker can control the `msg` input, they can craft malicious Python code within the `function_args` part of the string. Once `eval()` is called, this malicious code will be executed, potentially leading to unauthorized system access, data leakage, or other security risks.\n\n### Exploit\nAn attacker can exploit this vulnerability by providing a carefully crafted `msg` string. For example:\n```plaintext\nRunning archival_memory_insert(__import__('os').system('rm -rf /'))\n```\nIn this case, when the code reaches the `eval(function_args)` line, it will execute the malicious command `__import__('os').system('rm -rf /')` (on a Unix - like system, this command would delete all files in the root directory).\n\nThe attacker could use a similar approach for other vulnerable `function_name` values like `archival_memory_search`, `core_memory_replace`, `core_memory_append`, `conversation_search`, and `conversation_search_date`. They just need to construct a `msg` string that matches the expected format and contains malicious Python code in the `function_args` part.\n\n### Impacted\nAll versions of the code are affected because the vulnerability lies in the fundamental use of the `eval()` function, which is present throughout the codebase whenever the relevant conditions are met for processing the `msg` string. There is no version - specific mitigation implemented to address the code injection risk caused by `eval()`. \n\nFrom\n\nhttps://github.com/letta-ai/letta/blob/85faf5f474119b2684d1443c96ee34e09621cda5/letta/interface.py#L190\n\nto\n\nhttps://github.com/letta-ai/letta/blob/fdefb8a9cdaeeee72705a0660a9b9758e2e3ec59/letta/interface.py#L225\n",
      "state": "open",
      "author": "ybdesire",
      "author_type": "User",
      "created_at": "2025-05-08T14:30:25Z",
      "updated_at": "2025-06-22T02:17:25Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2613/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sarahwooders"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2613",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2613",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:47.298566",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-22T02:17:24Z"
        }
      ]
    },
    {
      "issue_number": 2579,
      "title": "vLLM Multiple providers Error",
      "body": "**Describe the bug**\nWhen using the vLLM as the provider, it raises multiple providers error as it adds two vLLM based providers here:\nhttps://github.com/letta-ai/letta/blob/79f4a7e94bce70ac9bc806ab12ea7231439b488a/letta/server/server.py#L291-L306\n\nTherefore, errors were raised with len(providers) > 1:\nhttps://github.com/letta-ai/letta/blob/79f4a7e94bce70ac9bc806ab12ea7231439b488a/letta/server/server.py#L1254-L1263\n\n\nI simply commented VLLMCompletionsProvider to use only v1/chat/completion.\n\n\n",
      "state": "closed",
      "author": "hbai98",
      "author_type": "User",
      "created_at": "2025-04-22T01:31:23Z",
      "updated_at": "2025-06-20T02:15:51Z",
      "closed_at": "2025-06-20T02:15:51Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2579/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cliandy"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2579",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2579",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:47.537473",
      "comments": [
        {
          "author": "cliandy",
          "body": "Thanks for reporting another issue @hbai98 Would you be open to making a contribution to change the names for the names of the providers?",
          "created_at": "2025-05-06T18:54:38Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-06T02:14:46Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-20T02:15:51Z"
        }
      ]
    },
    {
      "issue_number": 2580,
      "title": "vLLM Provider lacks v1 url",
      "body": "\nWhen setting \n`    export VLLM_API_BASE=\"http://0.0.0.0:8888\"  \n    # Run the Letta server (usually on a different port)\n    poetry run letta server --host 0.0.0.0 --port 80 # Add --ade if needed for development `\n\nor `export VLLM_API_BASE=\"http://localhost:8283`, the vLLM model can't be found.\n\n\nTo resolve it, adding a suffix like 'export VLLM_API_BASE=\"http://0.0.0.0:8888/v1', benefits finding the models available and latter functions' calls.\n\nIt may be helpful to set up like: https://github.com/letta-ai/letta/blob/79f4a7e94bce70ac9bc806ab12ea7231439b488a/letta/server/server.py#L313-L319\n\nFor adding this suffix automatically.\n",
      "state": "closed",
      "author": "hbai98",
      "author_type": "User",
      "created_at": "2025-04-22T03:00:51Z",
      "updated_at": "2025-06-20T02:15:50Z",
      "closed_at": "2025-06-20T02:15:50Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2580/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cliandy"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2580",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2580",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:47.769193",
      "comments": [
        {
          "author": "cliandy",
          "body": "Hi @hbai98 thanks for reporting this. Would you be open to making a contribution and add in this support?",
          "created_at": "2025-05-06T18:49:44Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-06T02:14:45Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-20T02:15:50Z"
        }
      ]
    },
    {
      "issue_number": 2467,
      "title": "Please add metadata field to messages",
      "body": "**Is your feature request related to a problem? Please describe.**\nI have a use case where message needs to include some metadata and currently I have to have a bad workaround to create a separate db table to manage the metadata for each message separately. It's made even more frustrating by how hard it is to get the correct user message since the only way you can differentiate multiple user messages is by the content.\n\nMy use case is similar to ChatGPT canvas feature where LLM agent helps me edit some document.\n\nIn my case, I'm sending two messages: 1) document context and 2) user request. Since both are type `user_message`, the only way I can pick the user request message to add metadata to is by examining the content which requires a hack like adding <user_request> in my user request content.\n\nHaving a metadata field in messages would make this much easier and give everyone flexibility to attach arbitrary data to messages.\n\n**Describe the solution you'd like**\nAdd a metadata field for messages",
      "state": "closed",
      "author": "seunggs",
      "author_type": "User",
      "created_at": "2025-03-04T04:03:17Z",
      "updated_at": "2025-06-18T02:15:51Z",
      "closed_at": "2025-06-18T02:15:51Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2467/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sarahwooders"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2467",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2467",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:48.005939",
      "comments": [
        {
          "author": "seunggs",
          "body": "@sarahwooders any ETA on this by any chance? This would make my use case a lot easier and more robust to work with. Thanks for adding this feature!",
          "created_at": "2025-03-12T14:31:00Z"
        },
        {
          "author": "seunggs",
          "body": "@sarahwooders FYI - it'd be good to also add tags and searchability (ideally through both tags and metadata fields)",
          "created_at": "2025-03-20T14:10:26Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-20T02:14:22Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-04T02:17:22Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-04T02:15:39Z"
        }
      ]
    },
    {
      "issue_number": 2608,
      "title": "Feature Request: Enhanced Support for OpenAI-Compatible Model Services and Improved Documentation",
      "body": "# Summary\nThe current version of Letta has significant limitations in terms of extensibility when it comes to integrating with large language model (LLM) and embedding services. While it supports a fixed set of providers like OpenAI and DeepSeek, it lacks the flexibility to seamlessly integrate with custom-deployed large model services that conform to the OpenAI-compatible API format. Additionally, the existing configuration options (llm_config, embedding_config, and base_url) are overly restrictive, with hard-coded prefixes in the code for endpoints, which forces users to write additional custom code for integration. This makes the process inconvenient and less user-friendly.\n\nMoreover, the documentation lacks practical examples for call parameters, which greatly increases the learning curve for new users. They often have to dig through the source code or make multiple guesses about the correct parameter formats, which is a major barrier to entry.\n\n# Feature Details\n## OpenAI-Compatible Model Service Support:\n* Enable Letta to natively support large model services that adhere to the OpenAI-compatible API format. This would allow users to integrate a wide range of custom-deployed models without the need for extensive custom coding.\n\n## Expanded Configuration Options:\n* Open up the llm_config, embedding_config, and base_url settings to be more flexible. This includes removing hard-coded prefixes for endpoints and allowing users to fully customize these configurations.\n* Take inspiration from the API design of Langchain, which is known for its extensibility and user-friendly integration. This would make it easier for users to configure and integrate different model services.\n\n## Improved Documentation:\n* Add practical examples for call parameters in the documentation. These examples should clearly illustrate how to use each parameter, including the correct format and typical use cases. This would significantly reduce the time and effort required for new users to get started with Letta.\n\n# Use Cases\nMany users may have their own custom-deployed large model services that are compatible with the OpenAI API format. Currently, integrating these services with Letta is a cumbersome process, requiring custom code to work around the limitations in the configuration options. By implementing the proposed features, users would be able to integrate these services more easily, making Letta a more versatile and user-friendly tool for a wider range of applications.\n\n***\nThank you for considering this feature request. I look forward to seeing these improvements in future releases of Letta.",
      "state": "closed",
      "author": "ClearLoving",
      "author_type": "User",
      "created_at": "2025-05-03T11:21:13Z",
      "updated_at": "2025-06-17T02:16:22Z",
      "closed_at": "2025-06-17T02:16:22Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2608/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2608",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2608",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:48.242148",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-03T02:15:30Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-17T02:16:22Z"
        }
      ]
    },
    {
      "issue_number": 2588,
      "title": "bug: Ollama has KeyError: 'description' in chatml.py",
      "body": "**Describe the bug**\n\n```\nletta                 | /app/letta/server/server.py:1567: UserWarning: Token streaming is only supported for models with type openai or anthropic or deepseek in the model_endpoint: agent has endpoint type ollama and http://host.docker.internal:11434/. Setting stream_tokens to False.\nletta                 |   warnings.warn(\nletta                 | 'description'\nletta                 | Letta.letta.agent - ERROR - step() failed\nletta                 | messages = [Message(created_by_id=None, last_updated_by_id=None, created_at=datetime.datetime(2025, 4, 25, 2, 20, 21, 939672, tzinfo=datetime.timezone.utc), updated_at=None, id='message-1baba3f7-c0a0-4e1e-8eaf-e668761b3eb2', organization_id=None, agent_id='agent-f6e3c998-3793-4092-99c6-031271e51968', model=None, role=<MessageRole.user: 'user'>, content=[TextContent(type=<MessageContentType.text: 'text'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"hello\",\\n  \"time\": \"2025-04-25 02:20:21 AM UTC+0000\"\\n}')], name=None, tool_calls=None, tool_call_id=None, step_id=None, otid='5ff373bb-842a-4021-b20f-e2dac77a4782', tool_returns=None, group_id=None, sender_id=None)]\nletta                 | error = Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff88ff6110> - error: 'description'\nletta                 | Letta.letta.agent - ERROR - step() failed with an unrecognized exception: 'Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff88ff6110> - error: 'description''\nletta                 | Traceback (most recent call last):\nletta                 |   File \"/app/letta/local_llm/chat_completion_proxy.py\", line 132, in get_chat_completion\nletta                 |     prompt = llm_wrapper.chat_completion_to_prompt(\nletta                 |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                 |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 200, in chat_completion_to_prompt\nletta                 |     system_block = self._compile_system_message(\nletta                 |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                 |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 108, in _compile_system_message\nletta                 |     prompt += self._compile_function_block(functions)\nletta                 |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                 |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 93, in _compile_function_block\nletta                 |     prompt += f\"\\n{self._compile_function_description(function_dict)}\"\nletta                 |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                 |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 81, in _compile_function_description\nletta                 |     func_str += f\"\\n    {param_k}: {param_v['description']}\"\nletta                 |                                     ~~~~~~~^^^^^^^^^^^^^^^\nletta                 | KeyError: 'description'\n```\n\n**Please describe your setup**\n- [X] How are you running Letta?  \n  - Docker 0.7.3\n- [X] Describe your setup\n  - MacOS 15.4\n\n```\n  # Letta is an agent building framework with built-in memory/vectordb support.\n  # https://docs.letta.com/quickstart/docker\n  letta:\n    image: letta/letta:0.7.3\n    container_name: letta\n    ports:\n      - 8283:8283\n    volumes:\n      # Mount the MCP configuration file (this points to Hayhooks)\n      - ./letta_mcp_config.json:/root/.letta/mcp_config.json\n    environment:\n      # https://docs.letta.com/guides/server/docker#setting-environment-variables\n      ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY\n      GEMINI_API_KEY: $GEMINI_API_KEY\n      # Adding Ollama just in case it's running\n      OLLAMA_BASE_URL: http://host.docker.internal:11434/\n    restart: on-failure\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://127.0.0.1:8283/v1/health/\"]\n      interval: 5s\n      timeout: 5s\n      retries: 18\n      start_period: 1s\n```\n\n---\n\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\n\n**Local LLM details**\n\nIf you are trying to run Letta with local LLMs, please provide the following information:\n\n- [X] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\n\ngemma3:12b-it-qat\n\n- [X] The local LLM backend you are using (web UI? LM Studio?)\n\nDirect against Letta Desktop 0.7.0\n\n- [x] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\n\nMacbook Pro M3.",
      "state": "open",
      "author": "wsargent",
      "author_type": "User",
      "created_at": "2025-04-25T02:28:14Z",
      "updated_at": "2025-06-16T02:17:19Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2588/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2588",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2588",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:48.479483",
      "comments": [
        {
          "author": "cliandy",
          "body": "Thanks for reporting this issue! To confirm, you are seeing similar issues in 0.7.7. Have you set up any custom functions or tools?",
          "created_at": "2025-05-01T23:39:24Z"
        },
        {
          "author": "wsargent",
          "body": "No, no custom tools in this case.",
          "created_at": "2025-05-16T04:00:16Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-16T02:17:18Z"
        }
      ]
    },
    {
      "issue_number": 2606,
      "title": "Letta 0.7.7 with Ollama streaming has exception",
      "body": "**Describe the bug**\n\nUsing streaming with Ollama (using Letta Desktop) causes this error:\n\n```\nl\n\n\n\nletta        | 'description'\nletta        | Letta.letta.agent - ERROR - step() failed\nletta        | messages = [Message(created_by_id=None, last_updated_by_id=None, created_at=datetime.datetime(2025, 5, 2, 15, 40, 10, 504109, tzinfo=datetime.timezone.utc), updated_at=None, id='message-dd776362-f6da-4a36-8242-58e253037041', organization_id=None, agent_id='agent-15245078-dfe4-4f18-9fc2-dc5503c43602', model=None, role=<MessageRole.user: 'user'>, content=[TextContent(type=<MessageContentType.text: 'text'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"test\",\\n  \"time\": \"2025-05-02 03:40:10 PM UTC+0000\"\\n}')], name=None, tool_calls=None, tool_call_id=None, step_id=None, otid='c07d915e-e4a0-454c-a30e-a747292ce053', tool_returns=None, group_id=None, sender_id=None)]\nletta        | error = Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description'\nletta        | Letta.letta.agent - ERROR - step() failed with an unrecognized exception: 'Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description''\nletta        | Traceback (most recent call last):\nletta        |   File \"/app/letta/local_llm/chat_completion_proxy.py\", line 132, in get_chat_completion\nletta        |     prompt = llm_wrapper.chat_completion_to_prompt(\nletta        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 200, in chat_completion_to_prompt\nletta        |     system_block = self._compile_system_message(\nletta        |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 108, in _compile_system_message\nletta        |     prompt += self._compile_function_block(functions)\nletta        |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 93, in _compile_function_block\nletta        |     prompt += f\"\\n{self._compile_function_description(function_dict)}\"\nletta        |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 81, in _compile_function_description\nletta        |     func_str += f\"\\n    {param_k}: {param_v['description']}\"\nletta        |                                     ~~~~~~~^^^^^^^^^^^^^^^\nletta        | KeyError: 'description'\nletta        | \nletta        | During handling of the above exception, another exception occurred:\nletta        | \nletta        | Traceback (most recent call last):\nletta        |   File \"/app/letta/agent.py\", line 860, in inner_step\nletta        |     response = self._get_ai_reply(\nletta        |                ^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/agent.py\", line 390, in _get_ai_reply\nletta        |     raise e\nletta        |   File \"/app/letta/agent.py\", line 348, in _get_ai_reply\nletta        |     response = create(\nletta        |                ^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 117, in wrapper\nletta        |     raise e\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 60, in wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 581, in create\nletta        |     return get_chat_completion(\nletta        |            ^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/chat_completion_proxy.py\", line 141, in get_chat_completion\nletta        |     raise LocalLLMError(\nletta        | letta.errors.LocalLLMError: Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description'\nletta        | Letta.letta.server.server - ERROR - Error in server._step: Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description'\nletta        | Traceback (most recent call last):\nletta        |   File \"/app/letta/local_llm/chat_completion_proxy.py\", line 132, in get_chat_completion\nletta        |     prompt = llm_wrapper.chat_completion_to_prompt(\nletta        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 200, in chat_completion_to_prompt\nletta        |     system_block = self._compile_system_message(\nletta        |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 108, in _compile_system_message\nletta        |     prompt += self._compile_function_block(functions)\nletta        |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 93, in _compile_function_block\nletta        |     prompt += f\"\\n{self._compile_function_description(function_dict)}\"\nletta        |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 81, in _compile_function_description\nletta        |     func_str += f\"\\n    {param_k}: {param_v['description']}\"\nletta        |                                     ~~~~~~~^^^^^^^^^^^^^^^\nletta        | KeyError: 'description'\nletta        | \nletta        | During handling of the above exception, another exception occurred:\nletta        | \nletta        | Traceback (most recent call last):\nletta        |   File \"/app/letta/server/server.py\", line 437, in _step\nletta        |     usage_stats = letta_agent.step(\nletta        |                   ^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/agent.py\", line 740, in step\nletta        |     step_response = self.inner_step(\nletta        |                     ^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/agent.py\", line 1026, in inner_step\nletta        |     raise e\nletta        | None\nletta        |   File \"/app/letta/agent.py\", line 860, in inner_step\nletta        |     response = self._get_ai_reply(\nletta        |                ^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/agent.py\", line 390, in _get_ai_reply\nletta        |     raise e\nletta        |   File \"/app/letta/agent.py\", line 348, in _get_ai_reply\nletta        |     response = create(\nletta        |                ^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 117, in wrapper\nletta        |     raise e\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 60, in wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 581, in create\nletta        |     return get_chat_completion(\nletta        |            ^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/chat_completion_proxy.py\", line 141, in get_chat_completion\nletta        |     raise LocalLLMError(\nletta        | letta.errors.LocalLLMError: Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description'\nletta        | Traceback (most recent call last):\nletta        |   File \"/app/letta/local_llm/chat_completion_proxy.py\", line 132, in get_chat_completion\nletta        |     prompt = llm_wrapper.chat_completion_to_prompt(\nletta        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 200, in chat_completion_to_prompt\nletta        |     system_block = self._compile_system_message(\nletta        |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 108, in _compile_system_message\nletta        |     prompt += self._compile_function_block(functions)\nletta        |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 93, in _compile_function_block\nletta        |     prompt += f\"\\n{self._compile_function_description(function_dict)}\"\nletta        |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/llm_chat_completion_wrappers/chatml.py\", line 81, in _compile_function_description\nletta        |     func_str += f\"\\n    {param_k}: {param_v['description']}\"\nletta        |                                     ~~~~~~~^^^^^^^^^^^^^^^\nletta        | KeyError: 'description'\nletta        | \nletta        | During handling of the above exception, another exception occurred:\nletta        | \nletta        | Traceback (most recent call last):\nletta        |   File \"/app/letta/server/rest_api/utils.py\", line 77, in sse_async_generator\nletta        |     usage = await usage_task\nletta        |             ^^^^^^^^^^^^^^^^\nletta        |   File \"/usr/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\nletta        |     return await loop.run_in_executor(None, func_call)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\nletta        |     result = self.fn(*self.args, **self.kwargs)\nletta        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/server/server.py\", line 702, in send_messages\nletta        |     return self._step(\nletta        |            ^^^^^^^^^^^\nletta        |   File \"/app/letta/server/server.py\", line 437, in _step\nletta        |     usage_stats = letta_agent.step(\nletta        |                   ^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/agent.py\", line 740, in step\nletta        |     step_response = self.inner_step(\nletta        |                     ^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/agent.py\", line 1026, in inner_step\nletta        |     raise e\nletta        |   File \"/app/letta/agent.py\", line 860, in inner_step\nletta        |     response = self._get_ai_reply(\nletta        |                ^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/agent.py\", line 390, in _get_ai_reply\nletta        |     raise e\nletta        |   File \"/app/letta/agent.py\", line 348, in _get_ai_reply\nletta        |     response = create(\nletta        |                ^^^^^^^\nletta        |   File \"/app/letta/tracing.py\", line 199, in sync_wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 117, in wrapper\nletta        |     raise e\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 60, in wrapper\nletta        |     return func(*args, **kwargs)\nletta        |            ^^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/llm_api/llm_api_tools.py\", line 581, in create\nletta        |     return get_chat_completion(\nletta        |            ^^^^^^^^^^^^^^^^^^^^\nletta        |   File \"/app/letta/local_llm/chat_completion_proxy.py\", line 141, in get_chat_completion\nletta        |     raise LocalLLMError(\nletta        | letta.errors.LocalLLMError: Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description'\nletta        | Letta.letta.server.rest_api.utils - ERROR - Caught unexpected Exception: Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description'\nletta        | /app/letta/server/rest_api/utils.py:133: UserWarning: SSE stream generator failed: Failed to convert ChatCompletion messages into prompt string with wrapper <letta.local_llm.llm_chat_completion_wrappers.chatml.ChatMLInnerMonologueWrapper object at 0xffff72940bd0> - error: 'description'\nletta        |   warnings.warn(f\"SSE stream generator failed: {e}\")\n```\n\n**Please describe your setup**\n- [X] How are you running Letta?  \n  - Docker \n- [X] Describe your setup\n   - https://github.com/wsargent/groundedllm\n   \n\nThis is happening on every ollama model I tried:\n\n- gemma3:4b\n- llama3.2\n- some gemma thing that has GGUF on it\n- qwen3 models",
      "state": "closed",
      "author": "wsargent",
      "author_type": "User",
      "created_at": "2025-05-02T15:45:06Z",
      "updated_at": "2025-06-16T02:17:17Z",
      "closed_at": "2025-06-16T02:17:17Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2606/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2606",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2606",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:48.670459",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-02T02:17:04Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-16T02:17:17Z"
        }
      ]
    },
    {
      "issue_number": 2615,
      "title": "Bugged pydantic_model_to_json_schema",
      "body": "**Describe the bug**\nThe \"pydantic model in custom tool\" feature is slightly bugged. \n\nWhile investigating why the custom tool generation fails when a complex object has pydantic.Optional[...] and object fields, I was building a minimal example, that failed with a different error. I drilled down to the code, that is obviously bugged:\n\n```\nFile \"/app/letta/functions/schema_generator.py\", line 321, in clean_schema\n    # Handle primitive types\n    return clean_property(schema_part)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n TypeError: pydantic_model_to_json_schema.<locals>.clean_property() missing 1 required positional argument: 'full_schema'\n```\n\nhttps://github.com/letta-ai/letta/blame/361529fe774e31c95618d3560735ac5c8e56b62d/letta/functions/schema_generator.py#L321C43-L321C43\n\n**Please describe your setup**\n- [x] How are you running Letta?  \n  - Docker \n- [x] Describe your setup\n  - MacOS, \n  - compose with letta image 0.7.12 \n  \n\n**Custom tool**\n\nCreated in ADE:\n\n```\nfrom typing import Optional, List\nfrom pydantic import BaseModel, Field\n\n\nclass ResponseSchema(BaseModel):\n    list_field: List[str] = Field(..., description=\"description\")\n\n    optional_string_field: Optional[str] = Field(None, description=\"description\")\n\n    object_field: object = Field(..., description=\"description\")\n\n\ndef respond_to_user(response: ResponseSchema):\n    \"\"\" \n    Respond to the user with a structured response\n    \n    Args:\n        response: structured response\n    \"\"\"\n    return response.model_dump()\n```\n\n\n---\n\nUsing OpenAI\n",
      "state": "open",
      "author": "siddie",
      "author_type": "User",
      "created_at": "2025-05-08T18:16:52Z",
      "updated_at": "2025-06-15T02:17:36Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2615/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2615",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2615",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:48.874679",
      "comments": [
        {
          "author": "dillibabu-subramani",
          "body": "Facing the same issue, @siddie. Solved?\n\n> Traceback (most recent call last):\n  File \"/app/letta/functions/functions.py\", line 45, in derive_openai_json_schema\n    schema = generate_schema(func, name=name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/letta/functions/schema_generator.py",
          "created_at": "2025-05-14T03:45:15Z"
        },
        {
          "author": "siddie",
          "body": "@dillibabu-subramani  From what I see, @cpacker or @mattzh72 are the main contributors to this feature. I believe, the most efficient would be if one them addresses the issue. \n\nIf I manage to allocate a day next week for this, I could start contributing to Letta by solving the issue. I cannot promi",
          "created_at": "2025-05-14T17:20:23Z"
        },
        {
          "author": "lemorage",
          "body": "umm seems it is, can any of u give a minimal reproducible example?",
          "created_at": "2025-05-15T07:55:23Z"
        },
        {
          "author": "siddie",
          "body": "@lemorage it is in the issue description. Launch the letta docker, open the ADE, create the custom tool with the specified code (see the ticket) and check the letta console - you will see the python error.",
          "created_at": "2025-05-15T09:36:56Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-15T02:17:36Z"
        }
      ]
    },
    {
      "issue_number": 2629,
      "title": "conversation_search Tool no access to agent's messages and also system prompt being included",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**Please describe your setup**\n- [X] How are you running Letta?  \n  - Docker \n- [X] Describe your setup\n - Linux Ubuntu (Latest) Server\n sudo docker run \\\n  -p 8284:8283 \\\n  -e OPENAI_API_KEY='sk-proj-key' \\\n  -e LETTA_PG_URI=\"postgresql://url here\" \\\n  -e SECURE=true \\\n  -e LETTA_SERVER_PASSWORD='password_here' \\\n  letta/letta:latest\n\n**Additional context**\nAdd any other context about the problem here.\n- What model you are using \nTried with 0.7.15 gpt-4.1-mini but also appears to be happening with all models and as far back as 0.6.49\n\nTo re-create: new docker install of 0.7.15 send a few messages back and forth to a new agent and examine carefully the messages table. Details below:\n\nI analyzed the output of the tool call conversation_search in the messages table and it does not appear to search the agent's responses ... it may not have access to them because they are stored in the tool_calls column. When an agent sends a message, this is what is stored in the content column: [{\"type\": \"text\", \"text\": \"{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2025-05-14 09:41:53 PM UTC+0000\"\\n}\"}]\n\nYou can see message: None there... that could be an issue ... that shows up for the send_message tool call row... the actual message the agent sent is stored in the previous row directly above the send_message tool call in the tool_calls column and looks like this: [{\"id\": \"call_EHa03sES6gmgSQEylzdHLNgm\", \"function\": {\"arguments\": \"{\\n  \"message\": \"Here's my message to the user\",\\n  \"request_heartbeat\": false\\n}\", \"name\": \"send_message\"}, \"type\": \"function\"}] \n\nI asked a new agent to summarize our brief conversation, analyzed conversation_search in the messages db and saw the content only included the user's messages - no agent messages.\n\nI think it would be a simple fix... basically take the agent's message content and make sure that None here in the content column is replaced with the agent's message (just like their thoughts appear in separate content column rows):  [{\"type\": \"text\", \"text\": \"{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2025-05-14 09:41:53 PM UTC+0000\"\\n}\"}]\n\nAnother strange thing I noticed which could be an issue is in the content column for conversation_search tool call in messages along with only the user's messages (not the agent's), the entire system prompt was in there for some reason... could be a bug pulling that in to that tool call.\n\n",
      "state": "open",
      "author": "bluestarforever",
      "author_type": "User",
      "created_at": "2025-05-14T22:21:51Z",
      "updated_at": "2025-06-15T02:17:35Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2629/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sarahwooders"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2629",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2629",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:49.068916",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-15T02:17:34Z"
        }
      ]
    },
    {
      "issue_number": 2634,
      "title": "Issue with bedrock Anthropic",
      "body": "**Describe the bug**\nI have created a bedrock -antrhopic based agent in the letta server.  whenever I am using the message/stream api,  I am getting the response like \n**[\"Traceback (most recent call last):\\n  File \\\"/app/letta/agent.py\\\", line 1270, in execute_tool_and_persist_state\\n    function_response = callable_func(**function_args)\\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: send_message() got an unexpected keyword argument 'id'\\n\"]**\nSeems like it keep on retrying then reverts with a failure message.\n\nAgent Creation Curl : \n**curl command : curl -X POST http://localhost:8283/v1/agents/       -H \"Authorization: Bearer \"      -H \"Content-Type: application/json\"      -d '{\n  \"name\": \"\",\n  \"memory_blocks\": [\n    {\n      \"value\": \"The human'\\''s name is Swarup Das.\",\n      \"label\": \"human\"\n    },\n    {\n      \"value\": \"My name is ERA, the all-knowing sentient AI.\",\n      \"label\": \"persona\"\n    }\n  ],\n  \"agent_type\": \"memgpt_agent\",\n  \"llm_config\": {\n    \"model\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    \"context_window\": 200000,\n    \"model_endpoint_type\": \"bedrock\",\n    \"put_inner_thoughts_in_kwargs\": true,\n    \"temperature\": 0.5,\n    \"enable_reasoner\": true\n  },\n  \"embedding_config\": {\n    \"embedding_model\": \"WhereIsAI/UAE-Large-V1\",\n    \"embedding_dim\": 1024,\n    \"embedding_endpoint_type\": \"hugging-face\",\n\"embedding_endpoint\":\"[127.0.0.1:5000/embed](http://127.0.0.1:5000/embed)\"\n  }\n}'**\n\n**Please describe your setup**\n- [ ] How are you running Letta?  \n  - Docker  (yes)\n  - pip (legacy) \n  - From source \n  - Desktop  \n- [ ] Describe your setup\n  - What's your OS (Windows/MacOS/Linux)? \n  - linux based ec2\n  - What is your `docker run ...` command (if applicable)\n  - sudo docker run   -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data   -p 8283:8283   -e AWS_ACCESS_KEY=\"###########\"   -e AWS_SECRET_ACCESS_KEY=\"#######\"   -e AWS_REGION=\"us-east-1\"   letta/letta:latest\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n![Image](https://github.com/user-attachments/assets/28e6824b-b5fb-44d2-a03b-fa3b175a786d)\n\n**Additional context**\nAdd any other context about the problem here.\n- What model you are using \n\n**Agent File (optional)**\nPlease attach your `.af` file, as this helps with reproducing issues. \n\n\n---\n\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\n\n**Local LLM details**\n\nIf you are trying to run Letta with local LLMs, please provide the following information:\n\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\n",
      "state": "open",
      "author": "dasswarup53",
      "author_type": "User",
      "created_at": "2025-05-15T13:28:56Z",
      "updated_at": "2025-06-15T02:17:33Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2634/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2634",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2634",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:49.253776",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-15T02:17:33Z"
        }
      ]
    },
    {
      "issue_number": 2625,
      "title": "Missing Endpoint type for bedrock",
      "body": "## No support for Bedrock and other endpoint types\n\nThe Letta documentation [embedding models endpoint](https://docs.letta.com/api-reference/embedding-models/list) lists `\"bedrock\"` as a valid `endpoint_type`, but the current source code does not support it. When attempting to use the Bedrock embedding model (Cohere English), the following error is raised:\n\nUpon inspecting the source code at [embeddings.py](https://github.com/letta-ai/letta/blob/main/letta/embeddings.py), I could not find any implementation or handling logic for `\"bedrock\"` under the available `endpoint_type` options, even though it is documented.\n\n---\n\n## Please describe your setup\n\n- [x] **Running Letta from source**\n- OS: macOS (Apple Silicon)\n\n---\n\n## Steps to reproduce\n\n1. Clone the Letta repo and install dependencies\n2. Configure Letta to use the Bedrock Cohere English embedding model\n3. Attempt to generate embeddings\n4. Observe the `ValueError` due to unrecognized `endpoint_type`\n\n---\n\n## Expected behavior\n\nThe `bedrock` endpoint should be recognized and properly handled during embedding generation, as advertised in the API reference.\n\n---\n\n## Screenshots\n\n![Image](https://github.com/user-attachments/assets/2c261e65-5db9-436e-b3a1-88f0509e7952)\n\n\n",
      "state": "open",
      "author": "shakeeb1998",
      "author_type": "User",
      "created_at": "2025-05-14T10:11:25Z",
      "updated_at": "2025-06-14T02:13:34Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2625/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2625",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2625",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:49.496091",
      "comments": [
        {
          "author": "shakeeb1998",
          "body": "I am working on a [pr](https://github.com/letta-ai/letta/pull/2626). For now, I've forked it to work on the \"cohere.embed-english-v3\" will make it more generic over the weekend ",
          "created_at": "2025-05-14T10:43:45Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-14T02:13:33Z"
        }
      ]
    },
    {
      "issue_number": 2631,
      "title": "Deleting Agent Deletes All Agent block entries (already deletes all blocks_agents entries)",
      "body": "**Is your feature request related to a problem? Please describe.**\nCurrently all messages, archival memories, blocks_agents entries are deleted when doing an API call to delete an agent. However, block entries remain.\n\n**Describe the solution you'd like**\nOption to delete block entries as well (if not done automatically) with that same single delete agent API call.\n\n**Describe alternatives you've considered**\nRunning multiple API calls.\n\n**Additional context**\nUnderstand some blocks could be used by more than one agent, etc. - that's why perhaps options can be added to delete agent API call.",
      "state": "open",
      "author": "bluestarforever",
      "author_type": "User",
      "created_at": "2025-05-14T23:10:57Z",
      "updated_at": "2025-06-14T02:13:33Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2631/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2631",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2631",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:49.721731",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-14T02:13:32Z"
        }
      ]
    },
    {
      "issue_number": 1926,
      "title": "Data Source Chat - The agent chat response broke after few interaction with uploaded source data",
      "body": "**Describe the bug**\r\nI uploaded data to source and attached the source to agent. After few interactions, the char chat conversations broke. Now agent chat functionality is not working.\r\n\r\nHere is the server side error\r\nSending request to https://inference.memgpt.ai/chat/completions\r\nResponse status code: 500\r\nHTTP error occurred: 500 Server Error: Internal Server Error for url: https://inference.memgpt.ai/chat/completions | Status code: 500, Message: {\"detail\":\"Internal server error (unpack): 400 Client Error: Bad Request for url: https://api.openai.com/v1/chat/completions\"}\r\nstep() failed\r\nuser_message = id='message-e2573d2d-6e00-4bf9-92ff-6093f9c09834' role=<MessageRole.user: 'user'> text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"Can you please summarise 5 Red Flags for Churn from Ultimate Customer Success Cheat Sheet which in archival memory?\",\\n  \"time\": \"2024-10-23 01:10:29 PM UTC+0000\"\\n}' user_id='user-00000000' agent_id='agent-7d3fc11f-aef8-4e33-b509-1f0aaed6dbac' model=None name=None created_at=datetime.datetime(2024, 10, 23, 13, 10, 29, 840415, tzinfo=datetime.timezone.utc) tool_calls=None tool_call_id=None\r\nerror = 'NoneType' object has no attribute 'status_code'\r\nstep() failed with an unrecognized exception: ''NoneType' object has no attribute 'status_code''\r\nLetta.letta.server.server - ERROR - Error in server._step: 'NoneType' object has no attribute 'status_code'\r\nTraceback (most recent call last):\r\n  File \"/letta/llm_api/helpers.py\", line 22, in make_post_request\r\n    response.raise_for_status()\r\n  File \"/app/.venv/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://inference.memgpt.ai/chat/completions\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/letta/llm_api/llm_api_tools.py\", line 70, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/llm_api/llm_api_tools.py\", line 166, in create\r\n    response = openai_chat_completions_request(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/llm_api/openai.py\", line 487, in openai_chat_completions_request\r\n    response_json = make_post_request(url, headers, data)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/llm_api/helpers.py\", line 49, in make_post_request\r\n    raise requests.exceptions.HTTPError(error_message) from http_err\r\nrequests.exceptions.HTTPError: HTTP error occurred: 500 Server Error: Internal Server Error for url: https://inference.memgpt.ai/chat/completions | Status code: 500, Message: {\"detail\":\"Internal server error (unpack): 400 Client Error: Bad Request for url: https://api.openai.com/v1/chat/completions\"}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/letta/server/server.py\", line 394, in _step\r\n    step_response = letta_agent.step(\r\n                    ^^^^^^^^^^^^^^^^^\r\n  File \"/letta/agent.py\", line 923, in step\r\n    raise e\r\n  File \"/letta/agent.py\", line 830, in step\r\n    response = self._get_ai_reply(\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/agent.py\", line 503, in _get_ai_reply\r\n    raise e\r\n  File \"/letta/agent.py\", line 472, in _get_ai_reply\r\n    response = create(\r\n               ^^^^^^^\r\n  File \"/letta/llm_api/llm_api_tools.py\", line 74, in wrapper\r\n    if http_err.response.status_code in error_codes:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'status_code'\r\nNone\r\nLetta.letta.server.server - DEBUG - Calling step_yield()\r\n\r\nAfter this error ...even after reloading the agent and sending hello results in this backend error\r\n\r\nSending request to https://inference.memgpt.ai/chat/completions\r\nResponse status code: 500\r\nHTTP error occurred: 500 Server Error: Internal Server Error for url: https://inference.memgpt.ai/chat/completions | Status code: 500, Message: {\"detail\":\"Internal server error (unpack): 400 Client Error: Bad Request for url: https://api.openai.com/v1/chat/completions\"}\r\nstep() failed\r\nuser_message = id='message-6bce21fe-6850-4d1a-8394-e9fba07aa05d' role=<MessageRole.user: 'user'> text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"Hello\",\\n  \"time\": \"2024-10-23 01:16:20 PM UTC+0000\"\\n}' user_id='user-00000000' agent_id='agent-7d3fc11f-aef8-4e33-b509-1f0aaed6dbac' model=None name=None created_at=datetime.datetime(2024, 10, 23, 13, 16, 20, 30532, tzinfo=datetime.timezone.utc) tool_calls=None tool_call_id=None\r\nerror = 'NoneType' object has no attribute 'status_code'\r\nstep() failed with an unrecognized exception: ''NoneType' object has no attribute 'status_code''\r\nLetta.letta.server.server - ERROR - Error in server._step: 'NoneType' object has no attribute 'status_code'\r\nTraceback (most recent call last):\r\n  File \"/letta/llm_api/helpers.py\", line 22, in make_post_request\r\n    response.raise_for_status()\r\n  File \"/app/.venv/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://inference.memgpt.ai/chat/completions\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/letta/llm_api/llm_api_tools.py\", line 70, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/llm_api/llm_api_tools.py\", line 166, in create\r\n    response = openai_chat_completions_request(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/llm_api/openai.py\", line 487, in openai_chat_completions_request\r\n    response_json = make_post_request(url, headers, data)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/llm_api/helpers.py\", line 49, in make_post_request\r\n    raise requests.exceptions.HTTPError(error_message) from http_err\r\nrequests.exceptions.HTTPError: HTTP error occurred: 500 Server Error: Internal Server Error for url: https://inference.memgpt.ai/chat/completions | Status code: 500, Message: {\"detail\":\"Internal server error (unpack): 400 Client Error: Bad Request for url: https://api.openai.com/v1/chat/completions\"}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/letta/server/server.py\", line 394, in _step\r\n    step_response = letta_agent.step(\r\n                    ^^^^^^^^^^^^^^^^^\r\n  File \"/letta/agent.py\", line 923, in step\r\n    raise e\r\n  File \"/letta/agent.py\", line 830, in step\r\n    response = self._get_ai_reply(\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"/letta/agent.py\", line 503, in _get_ai_reply\r\n    raise e\r\n  File \"/letta/agent.py\", line 472, in _get_ai_reply\r\n    response = create(\r\n               ^^^^^^^\r\n  File \"/letta/llm_api/llm_api_tools.py\", line 74, in wrapper\r\n    if http_err.response.status_code in error_codes:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'status_code'\r\nNone\r\nLetta.letta.server.server - DEBUG - Calling step_yield()\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "raolak",
      "author_type": "User",
      "created_at": "2024-10-23T13:18:17Z",
      "updated_at": "2025-06-13T17:26:06Z",
      "closed_at": "2025-02-03T02:01:44Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/1926/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sarahwooders"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/1926",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/1926",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:49.947716",
      "comments": [
        {
          "author": "RyanSept",
          "body": "I am also experiencing this. Happy to help contribute a fix if the maintainers can guide us",
          "created_at": "2024-11-27T00:22:42Z"
        },
        {
          "author": "lemorage",
          "body": "Yes, I got the exact same problem, and it happened time to time",
          "created_at": "2024-11-27T14:06:28Z"
        },
        {
          "author": "jannech",
          "body": "Same error here, after a failed function call.",
          "created_at": "2024-12-16T23:12:53Z"
        },
        {
          "author": "cpacker",
          "body": "@RyanSept @lemorage @jannech are you all running into the same error exactly?\r\n\r\nie: `error = 'NoneType' object has no attribute 'status_code'`?\r\n\r\nAre you able to share any more of your stack traces / screenshots / information about your Letta version? (are you on the latest build)\r\n\r\nThank you for",
          "created_at": "2024-12-17T02:08:54Z"
        },
        {
          "author": "RyanSept",
          "body": "Thanks @cpacker. I'm running letta 0.5.5 with the letta-free model and here is my error\r\n\r\n```\r\nINFO:     ::1:62931 - \"GET /v1/agents/agent-04ea0c4a-55fb-489a-91d8-a258b4d501b1 HTTP/1.1\" 200 OK\r\nINFO:     ::1:62930 - \"GET /v1/agents/agent-04ea0c4a-55fb-489a-91d8-a258b4d501b1/messages?limit=1000&msg_",
          "created_at": "2024-12-19T16:28:44Z"
        }
      ]
    },
    {
      "issue_number": 2594,
      "title": "feat: Add the option to specify trusted CA certificates",
      "body": "**Is your feature request related to a problem? Please describe.**\n\nAt the moment I cant use my vLLM instance because I it is setup with a self signed certificate. This throws the error message \n`[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain`\n\n**Describe the solution you'd like**\n\nIt would be great to have an option to specify trusted CA certificates (including self-signed ones) or a way to add custom certificates to the application's trust store. This would allow the application to connect to services using certificates that are not signed by publicly trusted Certificate Authorities, such as self-signed certificates or certificates from a private internal CA.\n\n**Additional context**\n\nI've attached the log from Letta Desktop as a reference.\n[desktop.log](https://github.com/user-attachments/files/19935573/desktop.log)",
      "state": "closed",
      "author": "HenryKleinschmidt",
      "author_type": "User",
      "created_at": "2025-04-28T07:13:59Z",
      "updated_at": "2025-06-12T02:15:23Z",
      "closed_at": "2025-06-12T02:15:23Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2594/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2594",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2594",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:50.158025",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-29T02:14:36Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-12T02:15:23Z"
        }
      ]
    },
    {
      "issue_number": 2589,
      "title": "Feature Request: Multi-Select and Bulk Delete for Archival Memory",
      "body": "**Problem:**\n\nCurrently, the system only supports deleting one passage at a time from the archival memory. While this works for small-scale use, it becomes very time-consuming and inefficient when dealing with larger texts, such as full textbooks. As I use the chatbot with multiple textbooks, I need a faster and more flexible way to manage stored passages.\n\n**Proposed Solution:**\n\nAdd support for **multi-select and bulk delete** functionality in the UI. This would allow users to:\n\n- Select multiple passages at once using checkboxes.\n- Perform a bulk delete operation to clear selected entries from the archival memory.\n- Optionally, include a \"Select All\" checkbox to quickly select all current entries.\n\n**Benefits:**\n\n- Significantly improves usability when handling large amounts of text.\n- Saves time and reduces repetitive manual effort.\n- Makes it easier to switch contexts between different documents or books.\n\n**Additional Suggestions (Optional):**\n\n- A confirmation modal before bulk deletion to prevent accidental data loss.\n- Tagging or grouping of passages to support better organization and easier selection in future enhancements.\n\nLet me know if this is feasible or if there are design considerations I should be aware of. Happy to provide more details or help test!\n\nThanks!",
      "state": "closed",
      "author": "dillibabu-subramani",
      "author_type": "User",
      "created_at": "2025-04-25T05:26:59Z",
      "updated_at": "2025-06-09T02:17:25Z",
      "closed_at": "2025-06-09T02:17:24Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2589/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2589",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2589",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:50.365473",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-26T02:15:55Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-09T02:17:24Z"
        }
      ]
    },
    {
      "issue_number": 1533,
      "title": " Letta 1.0 Stable Release ",
      "body": "# Letta (MemGPT) 1.0 Stable Release\n\n**Update 01/28/25: We expect only minor modifications to be made to our [existing API](https://docs.letta.com/api-reference/overview), and will be announcing a 1.0 soon. Please see our updated [Python](https://github.com/letta-ai/letta-python) and [Typescript](https://github.com/letta-ai/letta-node) SDKs to use this the latest API.** \n\nWe are working to release a stable version of MemGPT in a 1.0 release, which will be intended for production use. The release will include: \n\n- Refactoring of DB code (e.g. the `MetadataStore`) to solve scaleability and connection issues (e.g. #1488)\n- Consistent API interface for the REST API and the Python SDK, such as matching parameters and return types \n- Introduce stable database schemas and alembic migrations for future releases\n- Addition of production-grade testing, logging, and access control\n\nTo view progress on the release, you can check the [integration](https://github.com/cpacker/MemGPT/tree/integration) branch. \n\n### General Cleanup\n\n- Remove all nonessential code, dead or retired code, unofficially supported code from the codebase\n- Isolate non-core elements into package add-ons\n- Refactor MemGPT configuration to separate out server (e.g. DB) configuration and LLM/embedding configuration\n\n### Logging\n\n- Singular parent logger throughout codebase and removal of all `print` and `printd` calls (except for in the CLI)\n- Consistent use of [py logging best practices](https://docs.python.org/3/howto/logging-cookbook.html#adding-handlers-other-than-nullhandler-to-a-logger-in-a-library) for libraries and applications\n\n### Data Schemas\n\n- Consistent data schemas (e.g. `AgentState`) across CLI, Python Client, and REST API represented with pydantic models\n- Manage all persisted state (e.g. agent state) in an ORM\n- Supported DB migrations for all future versions using alembic\n\n### Auth + Access\n\n- Removal of authentication and access control (determined to be out-of-scope: developers should implement this within their own applications) \n\n### Testing\n\n- Automated testing for all officially supported integrations (LLM/embedding provider, storage provider) - remove any untested integrations (suggest migrating to adapters or fork)\n- SLA Support:\n    - Atomic unit and integration tests (should not depend on a live OpenAI API key for testing)\n    - Static code analysis threshold [prospector](https://prospector.landscape.io/en/master/) or components of)\n- Full CI run for each patch within all supported minors for Python client (versions 3.10, 3.11, 3.12)\n\n\n### Documentation \n- Add auto-documentation #1493\n\n## Timeline\nPlanned for end of January 2025: \n- [x]  Database refactor and creation of new schema models \n  - [x] Support auto-migrations with Alembic\n- [x]  #1579 \n    - [x]  Server\n    - [x]  Python Client\n    - [x]  REST API\n- [x] REST API versioning #1734 \n- [x] Autodocs for Python SDK \n- [x]  Refactor configuration\n- [x]  Move integrations to external adapters / package add-ons\n- [x]  Logging\n- [ ] Release 1.0 API \n",
      "state": "open",
      "author": "sarahwooders",
      "author_type": "User",
      "created_at": "2024-07-10T04:34:08Z",
      "updated_at": "2025-06-08T02:17:31Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/1533/reactions",
        "total_count": 22,
        "+1": 12,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 6,
        "rocket": 4,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/1533",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/1533",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:50.579450",
      "comments": [
        {
          "author": "a67793581",
          "body": "At present, I find that there is a problem with the generation of the open API. It cannot generate the correct parameters. Therefore, I can only manually modify the open API JSON file. The following is the /admin/users interface that I modified. It does not generate the body parameters for the GET r",
          "created_at": "2024-07-15T02:26:29Z"
        },
        {
          "author": "goetzrobin",
          "body": "Outstanding API issues to get dev portal chat UI fully functional:\r\n- [x] #1639 \r\n- [x] #1640\r\n- [x] #1641 \r\n- [x] #1642 \r\n- [x] #1643 \r\n- [x] #1644 \r\n- [x] #1645 \r\n- [x] #1646\r\n- [x] #1647 \r\n- [x] #1648 \r\n- [x] #1649 \r\n- [x] #1650",
          "created_at": "2024-08-15T17:43:46Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2024-12-09T02:11:43Z"
        },
        {
          "author": "Deng-Xian-Sheng",
          "body": "Are there any plans to publish something like the OpenAI API so it can be easily integrated into any existing project?",
          "created_at": "2024-12-22T17:51:46Z"
        },
        {
          "author": "sarahwooders",
          "body": "@Deng-Xian-Sheng yes we will be releasing documentation on our stable API release soon. You can see a preview here https://docs.letta.com/api-reference/agents/create",
          "created_at": "2025-01-07T00:18:10Z"
        }
      ]
    },
    {
      "issue_number": 2563,
      "title": "Can't view agents in ADE after creating an identity",
      "body": "**Describe the bug**\nThe title is pretty clear here. The ADE worked normally until I created an identity, and I got an error saying, \"**Something went wrong**\" when viewing my agents through ADE. I think this is gonna be reproducible, since I tested it multiple times. From Letta v0.6.50 ~ v0.6.53, the same problem exists for all.\n\nNote: There are no error logs found in `letta server`.\n\n**How to Reproduce**\n1. Run `letta server` locally and open ADE in browser.\n2. Create an identity in the **Identities** section in Letta ADE.\n3. After creation, click the **Agents** section, and the error will appear.\n\n**Please describe your setup**\n  - `pip install letta`\n  - `MacOS`\n  - `Terminal`\n\n**Screenshots**\nI've attached a video here for a clearer illustration.\n\nhttps://github.com/user-attachments/assets/96afd018-303c-4f13-89b2-4d4d6f7c03eb\n\n",
      "state": "open",
      "author": "lemorage",
      "author_type": "User",
      "created_at": "2025-04-14T03:45:55Z",
      "updated_at": "2025-06-08T02:17:30Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2563/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2563",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2563",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:50.774052",
      "comments": [
        {
          "author": "JerryJohnThomas",
          "body": "I dont think this is happening now",
          "created_at": "2025-05-08T17:59:36Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-06-08T02:17:29Z"
        }
      ]
    },
    {
      "issue_number": 2473,
      "title": "Custom Tools documentation issue: `args_schema` should be `args_json_schema`",
      "body": "**Describe the bug**\n\nThere is a small documentation issue in the `Guides and Concepts` section of the documentation.\n\nIn the Tools -> Custom Tools page (under \"STATEFUL AGENTS\"), there is a section called [Specifying tools via Pydantic models](https://docs.letta.com/guides/agents/custom-tools#specifying-tools-via-pydantic-models), where it says to use `args_schema` to provide the definition of the args for your tool. I think it should actually be `args_json_schema`.\n\n",
      "state": "closed",
      "author": "wooters",
      "author_type": "User",
      "created_at": "2025-03-07T03:00:47Z",
      "updated_at": "2025-06-05T02:15:20Z",
      "closed_at": "2025-06-05T02:15:19Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2473/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2473",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2473",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:50.977872",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-07T02:11:33Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-21T02:13:48Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-22T02:14:00Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-05T02:15:19Z"
        }
      ]
    },
    {
      "issue_number": 2494,
      "title": "MCP tools not appearing in ADE on Letta desktop (macOS)",
      "body": "**Describe the bug**\nModel Context Protocol (MCP) tools are not appearing in the Agent Development Environment (ADE) in Letta desktop for macOS. I've configured the MCP servers in the `~/.letta/mcp_config.json` file, but Letta doesn't seem to be detecting or loading these tools. I've tried restarting the application and killing all existing MCP processes, but the issue persists. Notably, this exact same MCP configuration works perfectly with my local Claude setup, suggesting the issue is specific to Letta's implementation.\n\n**Please describe your setup**\n- [x] How did you install letta?\n  - Downloaded Letta desktop application for macOS from the official website\n- [x] Describe your setup\n  - OS: macOS\n  - Running Letta: Using the desktop application launcher\n  - MCP tools attempted: sequential-thinking, openrpc, obsidian, mcp-perplexity-search, taskmanager\n\n**Steps to reproduce**\n1. Created `~/.letta/mcp_config.json` with proper configuration\n2. Launched Letta desktop application\n3. Checked Agent Development Environment for MCP tools\n4. No MCP tools appeared in the interface\n5. Tried killing existing MCP processes and restarting - no change\n\n**Additional context**\n- I've verified that the configuration file is properly formatted JSON\n- I've checked permissions on the configuration file\n- I've confirmed the MCP servers can be started manually outside of Letta\n- I've killed all existing MCP-related processes before restarting\n- This exact MCP configuration works correctly with my local Claude setup\n\n**Letta Config**\n```\n[defaults]\npreset = memgpt_chat\npersona = sam_pov\nhuman = basic\n\n[archival_storage]\ntype = postgres\npath = /Users/andrey/.letta\nuri = postgresql://postgres:@/postgres?host=/Users/andrey/.letta/desktop_data\n\n[recall_storage]\ntype = postgres\npath = /Users/andrey/.letta\nuri = postgresql://postgres:@/postgres?host=/Users/andrey/.letta/desktop_data\n\n[metadata_storage]\ntype = sqlite\npath = /Users/andrey/.letta\n\n[version]\nletta_version = 0.6.38\n```\n\n**MCP Config**\n```json\n{\n    \"mcpServers\": {\n        \"sequential-thinking\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"@modelcontextprotocol/server-sequential-thinking\"\n            ]\n        },\n        \"openrpc\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"openrpc-mcp-server\"\n            ]\n        },\n        \"obsidian\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"obsidian-mcp\",\n                \"/Users/andrey/repos/andrey-obsidian\"\n            ]\n        },\n        \"mcp-perplexity-search\": {\n            \"command\": \"/Users/andrey/.asdf/installs/nodejs/20.12.1/bin/node\",\n            \"args\": [\n                \"/Users/andrey/mcp-perplexity-search/dist/index.js\"\n            ],\n            \"env\": {\n                \"PERPLEXITY_API_KEY\": \"\"\n            }\n        },\n        \"taskmanager\": {\n            \"command\": \"/Users/andrey/.asdf/installs/nodejs/20.12.1/bin/node\",\n            \"args\": [\n                \"/Users/andrey/mcp-taskmanager/dist/index.js\"\n            ],\n            \"env\": {\n                \"TASK_MANAGER_FILE_PATH\": \"/Users/andrey/repos/andrey-obsidian/tasks.json\"\n            }\n        }\n    }\n}\n```\n\nMCP Server Details\n\nNode.js version: 20.12.1 (managed via asdf)\nMCP tools: Using standard packages from npm and custom local implementations\nCan start MCP servers manually: Yes",
      "state": "closed",
      "author": "AndreyLukin-SevenAI",
      "author_type": "User",
      "created_at": "2025-03-15T15:58:51Z",
      "updated_at": "2025-06-05T02:15:18Z",
      "closed_at": "2025-06-05T02:15:18Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2494/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2494",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2494",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:51.271774",
      "comments": [
        {
          "author": "AndreyLukin-SevenAI",
          "body": "Also, grepping for `MCP` or `mcp` in `.letta/logs/Letta.log` doesnt find anything, so I am assuming the file is never read",
          "created_at": "2025-03-15T16:11:50Z"
        },
        {
          "author": "leo-cheron",
          "body": "I have the same issue on self hosted docker version (v0.6.43). When initializing, the server successfully loads the MCP tools, but none appears in ADE.\n\n```back\nLetta.letta.server.server - INFO - MCP tools connected: get_workspace_hierarchy, create_task, get_task, get_tasks, update_task, move_task, ",
          "created_at": "2025-03-23T23:06:49Z"
        },
        {
          "author": "dchwong",
          "body": "I got 500 error when trying to load mcp tools programatically.\n\nhttpx - INFO - HTTP Request: GET http://localhost:8283/v1/tools/mcp/servers/gmail/tools \"HTTP/1.1 500 Internal Server Error\"\n2025-03-24 19:56:16.483 Uncaught app execution",
          "created_at": "2025-03-25T03:34:38Z"
        },
        {
          "author": "mark-veryawesome",
          "body": "@leo-cheron did you ever get this resolved? Encountering the same issue with self-hosted docker version (v0.6.53) using the Everything MCP server (SSE) example. \n\n```\nmcp.client.sse - INFO - Connecting to SSE endpoint: http://host.docker.internal:3001/sse\nhttpx - INFO - HTTP Request: GET http://host",
          "created_at": "2025-04-21T18:12:09Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-22T02:13:59Z"
        }
      ]
    },
    {
      "issue_number": 2571,
      "title": "Add a way to clear server logs in ADE",
      "body": "**Is your feature request related to a problem? Please describe.**\nIt is currently difficult to see where recent errors start in the logs.\n\n**Describe the solution you'd like**\nSome sort of \"Clear\" button next to the \"Restart\" button would be useful to clear the current log output in Letta Desktop. Note, this would only clear the displayed logs, not actual log file contents.\n\n**Describe alternatives you've considered**\nNone.\n\n**Additional context**\nThis could work similar to the \"Clear terminal\" (trash can icon) button in Docker Desktop.\n",
      "state": "closed",
      "author": "custompro12",
      "author_type": "User",
      "created_at": "2025-04-20T02:33:25Z",
      "updated_at": "2025-06-04T02:15:38Z",
      "closed_at": "2025-06-04T02:15:38Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2571/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2571",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2571",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:51.467948",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-21T02:14:15Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-04T02:15:37Z"
        }
      ]
    },
    {
      "issue_number": 2572,
      "title": "Add ability to manually specify model",
      "body": "**Is your feature request related to a problem? Please describe.**\nI'm unable to specify a tagged variant of the model that I want to use.\n\nMore specifically, I have Letta Desktop configured to fetch models from OpenRouter, however it doesn't pull/list all variants of each model.  Example, the Model dropdown lists `meta-llama/llama-4-maverick`, but doesn't list `meta-llama/llama-4-maverick:free`, so when trying to use the model I get an error \"Insufficient credits.\".\n\n**Describe the solution you'd like**\nEither list all variants of each model or let the user manually type in a model name (in addition to the dropdown / aka an editable dropdown).\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nNone\n",
      "state": "closed",
      "author": "custompro12",
      "author_type": "User",
      "created_at": "2025-04-20T04:25:35Z",
      "updated_at": "2025-06-04T02:15:37Z",
      "closed_at": "2025-06-04T02:15:36Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2572/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2572",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2572",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:51.656117",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-21T02:14:13Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-06-04T02:15:36Z"
        }
      ]
    },
    {
      "issue_number": 2557,
      "title": "chatbot personalit",
      "body": "![Image](https://github.com/user-attachments/assets/961ca744-99c5-473e-bcdb-7745af9d8191)\n\nlooks the entire comportment this chatbot is locked, change model or editing something just change nothing.",
      "state": "closed",
      "author": "RSerejo",
      "author_type": "User",
      "created_at": "2025-04-12T16:49:53Z",
      "updated_at": "2025-05-31T02:12:49Z",
      "closed_at": "2025-05-31T02:12:49Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2557/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2557",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2557",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:51.859085",
      "comments": [
        {
          "author": "RSerejo",
          "body": "letta prompt looks just like a loot of nothing",
          "created_at": "2025-04-13T01:43:11Z"
        },
        {
          "author": "4shub",
          "body": "Sorry, I am not sure what are you referring to for this bug? Are you unable to update the system instructions ?",
          "created_at": "2025-04-15T05:14:40Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-16T02:14:20Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-31T02:12:48Z"
        }
      ]
    },
    {
      "issue_number": 2567,
      "title": "response_data[\"candidates\"] not found in google_ai_client.py",
      "body": "image: letta/letta:0.6.53\n\n```\nletta                         | Letta.letta.agent - ERROR - step() failed\nletta                         | messages = [Message(created_by_id=None, last_updated_by_id=None, created_at=datetime.datetime(2025, 4, 15, 16, 50, 48, 1009, tzinfo=datetime.timezone.utc), updated_at=None, id='message-d4dedb3c-60d5-491c-8a95-c5891b4042c4', organization_id=None, agent_id='agent-0f391d56-0293-4286-b26d-aaeeb8a4c81d', model='gemini-2.5-pro-exp-03-25', role=<MessageRole.user: 'user'>, content=[TextContent(type=<MessageContentType.text: 'text'>, text='{\\n  \"type\": \"heartbeat\",\\n  \"reason\": \"[This is an automated system message hidden from the user] Function called using request_heartbeat=true, returning control\",\\n  \"time\": \"2025-04-15 04:50:48 PM UTC+0000\"\\n}')], name=None, tool_calls=None, tool_call_id=None, step_id=None, otid=None, tool_returns=None, group_id=None)]\nletta                         | error = 'candidates'\nletta                         | Letta.letta.agent - ERROR - step() failed with an unrecognized exception: ''candidates''\nletta                         | Traceback (most recent call last):\nletta                         |   File \"/app/letta/agent.py\", line 859, in inner_step\nletta                         |     response = self._get_ai_reply(\nletta                         |                ^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 392, in _get_ai_reply\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 342, in _get_ai_reply\nletta                         |     response = llm_client.send_llm_request(\nletta                         |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_client_base.py\", line 55, in send_llm_request\nletta                         |     return self.convert_response_to_chat_completion(response_data, messages)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/google_ai_client.py\", line 264, in convert_response_to_chat_completion\nletta                         |     raise e\nletta                         |   File \"/app/letta/llm_api/google_ai_client.py\", line 118, in convert_response_to_chat_completion\nletta                         |     for candidate in response_data[\"candidates\"]:\nletta                         |                      ~~~~~~~~~~~~~^^^^^^^^^^^^^^\nletta                         | KeyError: 'candidates'\nletta                         | Letta.letta.server.server - ERROR - Error in server._step: 'candidates'\nletta                         | Traceback (most recent call last):\nletta                         | None\nopen-webui                    | 2025-04-15 16:51:32,601 - Letta Pipe - ERROR - Error processing request: 500, message='Internal Server Error', url='http://letta:8283/v1/agents/agent-0f391d56-0293-4286-b26d-aaeeb8a4c81d/messages'\nletta                         |   File \"/app/letta/server/server.py\", line 409, in _step\nletta                         |     usage_stats = letta_agent.step(\nletta                         |                   ^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 743, in step\nletta                         |     step_response = self.inner_step(\nletta                         |                     ^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 1025, in inner_step\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 859, in inner_step\nletta                         |     response = self._get_ai_reply(\nletta                         |                ^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 392, in _get_ai_reply\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 342, in _get_ai_reply\nletta                         |     response = llm_client.send_llm_request(\nletta                         |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_client_base.py\", line 55, in send_llm_request\nletta                         |     return self.convert_response_to_chat_completion(response_data, messages)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/google_ai_client.py\", line 264, in convert_response_to_chat_completion\nletta                         |     raise e\nletta                         |   File \"/app/letta/llm_api/google_ai_client.py\", line 118, in convert_response_to_chat_completion\nletta                         |     for candidate in response_data[\"candidates\"]:\nletta                         |                      ~~~~~~~~~~~~~^^^^^^^^^^^^^^\nletta                         | KeyError: 'candidates'\nletta                         | 'candidates'\nletta                         | Traceback (most recent call last):\nletta                         |   File \"/app/letta/server/server.py\", line 1629, in send_message_to_agent\nletta                         |     usage = await task\nletta                         |             ^^^^^^^^^^\nletta                         |   File \"/usr/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\nletta                         |     return await loop.run_in_executor(None, func_call)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\nletta                         |     result = self.fn(*self.args, **self.kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/server/server.py\", line 692, in send_messages\nletta                         |     return self._step(\nletta                         |            ^^^^^^^^^^^\nletta                         |   File \"/app/letta/server/server.py\", line 409, in _step\nletta                         |     usage_stats = letta_agent.step(\nletta                         |                   ^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 743, in step\nletta                         |     step_response = self.inner_step(\nletta                         |                     ^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 1025, in inner_step\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 859, in inner_step\nletta                         |     response = self._get_ai_reply(\nletta                         |                ^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 392, in _get_ai_reply\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 342, in _get_ai_reply\nletta                         |     response = llm_client.send_llm_request(\nletta                         |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_client_base.py\", line 55, in send_llm_request\nletta                         |     return self.convert_response_to_chat_completion(response_data, messages)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/google_ai_client.py\", line 264, in convert_response_to_chat_completion\nletta                         |     raise e\nletta                         |   File \"/app/letta/llm_api/google_ai_client.py\", line 118, in convert_response_to_chat_completion\nletta                         |     for candidate in response_data[\"candidates\"]:\nletta                         |                      ~~~~~~~~~~~~~^^^^^^^^^^^^^^\nletta                         | KeyError: 'candidates'\n```\n",
      "state": "closed",
      "author": "wsargent",
      "author_type": "User",
      "created_at": "2025-04-15T16:53:45Z",
      "updated_at": "2025-05-31T02:12:48Z",
      "closed_at": "2025-05-31T02:12:48Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2567/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2567",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2567",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:52.080041",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-16T02:14:19Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-31T02:12:47Z"
        }
      ]
    },
    {
      "issue_number": 2517,
      "title": "Feature Request :Improve Responsiveness by Sending Message First Before Executing Tools",
      "body": "Is your feature request related to a problem? Please describe.\nCurrently, when executing tools such as core_memory_replace(), conversation_search(), or archival_memory_search(), users may experience a delay before receiving any response. This can make interactions feel slow and unresponsive.\n\nDescribe the solution you'd like\nTo improve user experience, introduce a mechanism where send_message() is triggered first to acknowledge the request before executing other tools. This ensures that users receive immediate feedback while background processes run asynchronously.\n\nDescribe alternatives you've considered\n\nUsing a loading indicator while waiting for tool execution, but this doesn't actively engage the user.\n\nRunning tools in the background without an initial response, but this creates a perception of lag.\n\nAdditional context\nThis approach can significantly enhance the perceived speed and responsiveness of interactions, making the system feel more natural and user-friendly.",
      "state": "closed",
      "author": "vukhaihoan",
      "author_type": "User",
      "created_at": "2025-03-25T02:25:45Z",
      "updated_at": "2025-05-29T02:14:41Z",
      "closed_at": "2025-05-29T02:14:41Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2517/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2517",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2517",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:52.318980",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "We will have a demo of this in about 1-2 weeks - please check back in then! ",
          "created_at": "2025-04-01T19:07:19Z"
        },
        {
          "author": "vukhaihoan",
          "body": "> We will have a demo of this in about 1-2 weeks - please check back in then!\nHello, have any update on this ? @sarahwooders \n",
          "created_at": "2025-04-14T05:16:43Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-15T02:12:45Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-29T02:14:40Z"
        }
      ]
    },
    {
      "issue_number": 2566,
      "title": "Performance Issue",
      "body": "The application is experiencing significant slowdowns. Please investigate this issue.",
      "state": "closed",
      "author": "Lorodn4x",
      "author_type": "User",
      "created_at": "2025-04-14T13:37:03Z",
      "updated_at": "2025-05-29T02:14:39Z",
      "closed_at": "2025-05-29T02:14:39Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2566/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2566",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2566",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:52.517405",
      "comments": [
        {
          "author": "cpacker",
          "body": "Hi @Lorodn4x happy to help - can you expand a bit on your setup and which things are slowing down? For example, are you using the server via Docker w/ OpenAI API, and are noticing that API calls to Letta are slowing down? Or are you using the server via Docker w/ local LLMs (eg LM Studio), and are n",
          "created_at": "2025-04-14T15:41:31Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-15T02:12:42Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-29T02:14:38Z"
        }
      ]
    },
    {
      "issue_number": 2522,
      "title": "Google_ai models error when a Composio tool is registered",
      "body": "**Describe the bug**\nWhen a Composio is added, any Gemini models will crash when on chat interaction.\nCan be tested on google_ai/gemini-2.0-flash, for instance.\n\n**Please describe your setup**\n- How did you install letta?\nfresh install of Letta Docker image, v0.6.44\n- Describe your setup\nLinux\n\n**Additional context**\nError:\n```bash\n Letta.letta.server.server - ERROR - Error in server._step: HTTP error occurred: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent | Status code: 400, Message: {\n \"error\": {\n   \"code\": 400,\n   \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\",\n   \"status\": \"INVALID_ARGUMENT\",\n   \"details\": [\n     {\n       \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n       \"fieldViolations\": [\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[2].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\"\n         },\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[3].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\"\n         }\n       ]\n     }\n   ]\n }\n\nraceback (most recent call last):\n File \"/app/letta/llm_api/helpers.py\", line 144, in make_post_request\n   response.raise_for_status()\n File \"/app/.venv/lib/python3.11/site-packages/requests/models.py\", line 1024, in raise_for_status\n   raise HTTPError(http_error_msg, response=self)\nequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\nhe above exception was the direct cause of the following exception:\nraceback (most recent call last):\n File \"/app/letta/server/server.py\", line 439, in _step\n   usage_stats = letta_agent.step(\n                 ^^^^^^^^^^^^^^^^^\n File \"/app/letta/tracing.py\", line 203, in sync_wrapper\n   result = func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/agent.py\", line 693, in step\n   step_response = self.inner_step(\n                   ^^^^^^^^^^^^^^^^\n File \"/app/letta/agent.py\", line 974, in inner_step\n   raise e\n File \"/app/letta/agent.py\", line 807, in inner_step\n   response = self._get_ai_reply(\n              ^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/tracing.py\", line 203, in sync_wrapper\n   result = func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/agent.py\", line 363, in _get_ai_reply\n   raise e\n File \"/app/letta/agent.py\", line 312, in _get_ai_reply\n   response = llm_client.send_llm_request(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/llm_api/llm_client_base.py\", line 52, in send_llm_request\n   response_data = self.request(request_data)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/llm_api/google_ai_client.py\", line 25, in request\n   return make_post_request(url, headers, request_data)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/llm_api/helpers.py\", line 171, in make_post_request\n   raise requests.exceptions.HTTPError(error_message) from http_err\nequests.exceptions.HTTPError: HTTP error occurred: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent | Status code: 400, Message: {\n \"error\": {\n   \"code\": 400,\n   \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\",\n   \"status\": \"INVALID_ARGUMENT\",\n   \"details\": [\n     {\n       \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n       \"fieldViolations\": [\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[2].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\"\n         },\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[3].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\"\n         }\n       ]\n     }\n   ]\n }\n\none\nraceback (most recent call last):\n File \"/app/letta/llm_api/helpers.py\", line 144, in make_post_request\n   response.raise_for_status()\n File \"/app/.venv/lib/python3.11/site-packages/requests/models.py\", line 1024, in raise_for_status\n   raise HTTPError(http_error_msg, response=self)\nequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\nhe above exception was the direct cause of the following exception:\nraceback (most recent call last):\n File \"/app/letta/server/rest_api/utils.py\", line 77, in sse_async_generator\n   usage = await usage_task\n           ^^^^^^^^^^^^^^^^\n File \"/usr/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n   return await loop.run_in_executor(None, func_call)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n   result = self.fn(*self.args, **self.kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/server/server.py\", line 722, in send_messages\n   return self._step(\n          ^^^^^^^^^^^\n File \"/app/letta/server/server.py\", line 439, in _step\n   usage_stats = letta_agent.step(\n                 ^^^^^^^^^^^^^^^^^\n File \"/app/letta/tracing.py\", line 203, in sync_wrapper\n   result = func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/agent.py\", line 693, in step\n   step_response = self.inner_step(\n                   ^^^^^^^^^^^^^^^^\n File \"/app/letta/agent.py\", line 974, in inner_step\n   raise e\n File \"/app/letta/agent.py\", line 807, in inner_step\n   response = self._get_ai_reply(\n              ^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/tracing.py\", line 203, in sync_wrapper\n   result = func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/agent.py\", line 363, in _get_ai_reply\n   raise e\n File \"/app/letta/agent.py\", line 312, in _get_ai_reply\n   response = llm_client.send_llm_request(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/llm_api/llm_client_base.py\", line 52, in send_llm_request\n   response_data = self.request(request_data)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/llm_api/google_ai_client.py\", line 25, in request\n   return make_post_request(url, headers, request_data)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n File \"/app/letta/llm_api/helpers.py\", line 171, in make_post_request\n   raise requests.exceptions.HTTPError(error_message) from http_err\nequests.exceptions.HTTPError: HTTP error occurred: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent | Status code: 400, Message: {\n \"error\": {\n   \"code\": 400,\n   \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\",\n   \"status\": \"INVALID_ARGUMENT\",\n   \"details\": [\n     {\n       \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n       \"fieldViolations\": [\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[2].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\"\n         },\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[3].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\"\n         }\n       ]\n     }\n   ]\n }\n\napp/letta/server/rest_api/utils.py:133: UserWarning: SSE stream generator failed: HTTP error occurred: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent | Status code: 400, Message: {\n \"error\": {\n   \"code\": 400,\n   \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\",\n   \"status\": \"INVALID_ARGUMENT\",\n   \"details\": [\n     {\n       \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n       \"fieldViolations\": [\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[2].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\"\n         },\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[3].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\"\n         }\n       ]\n     }\n   ]\n }\n\n warnings.warn(f\"SSE stream generator failed: {e}\")\netta.letta.server.rest_api.utils - ERROR - Caught unexpected Exception: HTTP error occurred: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent | Status code: 400, Message: {\n \"error\": {\n   \"code\": 400,\n   \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\",\n   \"status\": \"INVALID_ARGUMENT\",\n   \"details\": [\n     {\n       \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n       \"fieldViolations\": [\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[2].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[2].value': Cannot find field.\"\n         },\n         {\n           \"field\": \"tools[0].function_declarations[5].parameters.properties[3].value\",\n           \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[5].parameters.properties[3].value': Cannot find field.\"\n         }\n       ]\n     }\n   ]\n }\n```\n\n",
      "state": "closed",
      "author": "leo-cheron",
      "author_type": "User",
      "created_at": "2025-03-26T19:46:28Z",
      "updated_at": "2025-05-28T02:14:46Z",
      "closed_at": "2025-05-28T02:14:46Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2522/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2522",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2522",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:52.897096",
      "comments": [
        {
          "author": "lemorage",
          "body": "The cause should be due to the following function `generate_tool_schema_for_composio`, if there was a `default` field, we would include it in the `property_schema`; however, this is not compatible with the Gemini API.\n\nhttps://github.com/letta-ai/letta/blob/7f2dc80596e696b08c5e92343b5e4dd1b0298d2b/l",
          "created_at": "2025-03-27T05:27:33Z"
        },
        {
          "author": "gbanusi",
          "body": "@lemorage will you ever make it compatible? :)",
          "created_at": "2025-03-31T08:37:21Z"
        },
        {
          "author": "gbanusi",
          "body": "@lemorage Any news on this, will gemma 3 work with letta?",
          "created_at": "2025-04-08T12:33:14Z"
        },
        {
          "author": "lemorage",
          "body": "@gbanusi Hey, the pr hasn't been merged yet. I just skimmed through the latest versions of letta, and it seems like the gemini models still aren't quite compatible with the composio tool integration. BUT I haven't had a chance to test it out myself, so take that with a grain of salt.",
          "created_at": "2025-04-08T15:15:31Z"
        },
        {
          "author": "lemorage",
          "body": "This issue has been fixed in #2556 :)",
          "created_at": "2025-04-13T02:57:25Z"
        }
      ]
    },
    {
      "issue_number": 2549,
      "title": "Bad MCP tool can hang Letta",
      "body": "**Describe the bug**\n\nIt's possible to get Letta into a locked state with a bad MCP tool.  See https://github.com/wsargent/groundedllm/pull/35 for reproduction.\n\n* Install wikipedia-mcp-server as an MCP tool.\n* Add `getImagesForPage` to agent as a tool\n* Ask agent \"please find me cat pictures on wikipedia\"\n\nExpected result:\n\nCat pictures\n\nActual result:\n\nLetta hangs, no output.\n\n## Setup\n\n**Please describe your setup**\n- [X] How did you install letta?\n  - docker install: \n- [X] Describe your setup\n  - MacOS\n  - docker compose up --build from https://github.com/wsargent/groundedllm\n\n### MCP File\n\n```\n{\n  \"mcpServers\": {\n    \"hayhooks\": {\n      \"url\": \"http://hayhooks:1416/sse\",\n      \"disabled\": false,\n      \"autoApprove\": []\n    },\n    \"mermaid-mcp-server\": {\n      \"url\": \"http://mermaid-mcp-server:9711/sse\",\n      \"disabled\": false,\n      \"autoApprove\": []\n    },\n    \"aws-documentation-mcp-server\": {\n      \"url\": \"http://aws-documentation-mcp-server:9712/sse\",\n      \"disabled\": false,\n      \"autoApprove\": []\n    },\n    \"wikipedia\": {\n      \"url\": \"http://wikipedia-mcp-server:9713/sse\",\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## Logs\n\n```\nletta                         | Letta.letta.local_llm.utils - ERROR - Failed to encode field type with value {'type': ['string', 'number'], 'description': 'Maximum number of images to retrieve (default: 50)'}\nletta                         | Letta.letta.local_llm.utils - ERROR - Failed to encode field type with value {'type': ['string', 'number'], 'description': 'Maximum number of images to retrieve (default: 50)'}\nletta                         | Letta.letta.local_llm.utils - ERROR - Failed to encode field type with value {'type': ['string', 'number'], 'description': 'Maximum number of images to retrieve (default: 50)'}\nletta                         | Letta.letta.agent - ERROR - step() failed\nletta                         | messages = [Message(created_by_id=None, last_updated_by_id=None, created_at=datetime.datetime(2025, 4, 9, 14, 23, 49, 731406, tzinfo=datetime.timezone.utc), updated_at=None, id='message-01607e9b-b815-4e04-9f6f-521c3d33995c', role=<MessageRole.user: 'user'>, content=[TextContent(type=<MessageContentType.text: 'text'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"hello\",\\n  \"time\": \"2025-04-09 02:23:49 PM UTC+0000\"\\n}')], organization_id=None, agent_id='agent-1747c911-7c0d-4dc1-854d-2e0ecdea4c88', model=None, name=None, tool_calls=None, tool_call_id=None, step_id=None, otid=None, tool_returns=None, group_id=None)]\nletta                         | error = expected string or buffer\nletta                         | Letta.letta.agent - ERROR - step() failed with an unrecognized exception: 'expected string or buffer'\nletta                         | Traceback (most recent call last):\nletta                         |   File \"/app/letta/agent.py\", line 824, in inner_step\nletta                         |     response = self._get_ai_reply(\nletta                         |                ^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 365, in _get_ai_reply\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 323, in _get_ai_reply\nletta                         |     response = create(\nletta                         |                ^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 117, in wrapper\nletta                         |     raise e\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 60, in wrapper\nletta                         |     return func(*args, **kwargs)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 152, in create\nletta                         |     function_tokens = num_tokens_from_functions(functions=functions, model=llm_config.model) if functions else 0\nletta                         |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/local_llm/utils.py\", line 120, in num_tokens_from_functions\nletta                         |     function_tokens += len(encoding.encode(v[\"type\"]))\nletta                         |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/.venv/lib/python3.11/site-packages/tiktoken/core.py\", line 120, in encode\nletta                         |     if match := _special_token_regex(disallowed_special).search(text):\nletta                         |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         | TypeError: expected string or buffer\nletta                         | Letta.letta.server.server - ERROR - Error in server._step: expected string or buffer\nletta                         | Traceback (most recent call last):\nletta                         |   File \"/app/letta/server/server.py\", line 442, in _step\nletta                         |     usage_stats = letta_agent.step(\nletta                         |                   ^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 705, in step\nletta                         |     step_response = self.inner_step(\nletta                         |                     ^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 990, in inner_step\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 824, in inner_step\nletta                         |     response = self._get_ai_reply(\nletta                         |                ^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 365, in _get_ai_reply\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 323, in _get_ai_reply\nletta                         |     response = create(\nletta                         |                ^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 117, in wrapper\nletta                         |     raise e\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 60, in wrapper\nletta                         |     return func(*args, **kwargs)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 152, in create\nletta                         |     function_tokens = num_tokens_from_functions(functions=functions, model=llm_config.model) if functions else 0\nletta                         |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/local_llm/utils.py\", line 120, in num_tokens_from_functions\nletta                         |     function_tokens += len(encoding.encode(v[\"type\"]))\nletta                         |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/.venv/lib/python3.11/site-packages/tiktoken/core.py\", line 120, in encode\nletta                         |     if match := _special_token_regex(disallowed_special).search(text):\nletta                         |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         | TypeError: expected string or buffer\nletta                         | None\nletta                         | Traceback (most recent call last):\nletta                         |   File \"/app/letta/server/rest_api/utils.py\", line 77, in sse_async_generator\nletta                         |     usage = await usage_task\nletta                         |             ^^^^^^^^^^^^^^^^\nletta                         |   File \"/usr/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\nletta                         |     return await loop.run_in_executor(None, func_call)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\nletta                         |     result = self.fn(*self.args, **self.kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/server/server.py\", line 725, in send_messages\nletta                         |     return self._step(\nletta                         |            ^^^^^^^^^^^\nletta                         |   File \"/app/letta/server/server.py\", line 442, in _step\nletta                         |     usage_stats = letta_agent.step(\nletta                         |                   ^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 705, in step\nletta                         |     step_response = self.inner_step(\nletta                         |                     ^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 990, in inner_step\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 824, in inner_step\nletta                         |     response = self._get_ai_reply(\nletta                         |                ^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/agent.py\", line 365, in _get_ai_reply\nletta                         |     raise e\nletta                         |   File \"/app/letta/agent.py\", line 323, in _get_ai_reply\nletta                         |     response = create(\nletta                         |                ^^^^^^^\nletta                         |   File \"/app/letta/tracing.py\", line 203, in sync_wrapper\nletta                         |     result = func(*args, **kwargs)\nletta                         |              ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 117, in wrapper\nletta                         |     raise e\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 60, in wrapper\nletta                         |     return func(*args, **kwargs)\nletta                         |            ^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/llm_api/llm_api_tools.py\", line 152, in create\nletta                         |     function_tokens = num_tokens_from_functions(functions=functions, model=llm_config.model) if functions else 0\nletta                         |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/letta/local_llm/utils.py\", line 120, in num_tokens_from_functions\nletta                         |     function_tokens += len(encoding.encode(v[\"type\"]))\nletta                         |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         |   File \"/app/.venv/lib/python3.11/site-packages/tiktoken/core.py\", line 120, in encode\nletta                         |     if match := _special_token_regex(disallowed_special).search(text):\nletta                         |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nletta                         | TypeError: expected string or buffer\nletta                         | /app/letta/server/rest_api/utils.py:133: UserWarning: SSE stream generator failed: expected string or buffer\nletta                         |   warnings.warn(f\"SSE stream generator failed: {e}\")\nletta                         | Letta.letta.server.rest_api.utils - ERROR - Caught unexpected Exception: expected string or buffer\n```",
      "state": "closed",
      "author": "wsargent",
      "author_type": "User",
      "created_at": "2025-04-09T14:35:33Z",
      "updated_at": "2025-05-24T02:10:59Z",
      "closed_at": "2025-05-24T02:10:58Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2549/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2549",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2549",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:53.114397",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-10T02:10:04Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-24T02:10:57Z"
        }
      ]
    },
    {
      "issue_number": 2542,
      "title": "Google models don't have the correct config temperature.",
      "body": "**Describe the bug**\nGoogle gemini 1.5 and above models have the temperature slider ONLY at values from 0 to 1 in letta. These models, 1.5 and above support a temperature of 0 to 2.\n\n**Please describe your setup**\n- [ ] How did you install letta?\n  - Docker compose.\n- [ ] Describe your setup\n  - Windows 11\n  - Docker compose.\n\n",
      "state": "closed",
      "author": "T9es",
      "author_type": "User",
      "created_at": "2025-04-08T02:08:17Z",
      "updated_at": "2025-05-22T02:13:59Z",
      "closed_at": "2025-05-22T02:13:58Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2542/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2542",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2542",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:53.367241",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-08T02:13:55Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-22T02:13:58Z"
        }
      ]
    },
    {
      "issue_number": 2540,
      "title": "Letta desktop not moving beyond create agent",
      "body": "**Describe the bug**\nWhen I open letta desktop and I try to create a new agent or I try to import a agent I never get past the loading phase, I waited almost 20 minutes for it to complete loading.\n\n**Please describe your setup**\n- [ ] How did you install letta?\n  - I downloaded the letta app for windows.\n- [ ] Describe your setup\n  - What's your OS (Windows/MacOS/Linux)?\n  - I have a Windows 11 pc\n  - How are you running `letta`? (`cmd.exe`/Powershell/Anaconda Shell/Terminal)\n  - Letta desktop app\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/3e369b8c-b930-496a-b4a2-97a2d8ec79e1)\n\n**Additional context**\nAdd any other context about the problem here.\n\n**Letta Config**\nPlease attach your `~/.letta/config` file or copy paste it below.\n\n---\n\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\n\n**Local LLM details**\n\nIf you are trying to run Letta with local LLMs, please provide the following information:\n\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\n",
      "state": "closed",
      "author": "shivam13297",
      "author_type": "User",
      "created_at": "2025-04-06T05:49:15Z",
      "updated_at": "2025-05-21T02:14:20Z",
      "closed_at": "2025-05-21T02:14:19Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2540/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2540",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2540",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:53.589627",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Can you share the logs inside of the \"server logs\" tab? ",
          "created_at": "2025-04-06T16:44:56Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-07T02:13:15Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-21T02:14:18Z"
        }
      ]
    },
    {
      "issue_number": 2504,
      "title": "Agent export not working",
      "body": "**Describe the bug**\nThe export functionality is missing from the desktop client, throws an error in the cloud client, and returns 404 in the REST API.\n\n**Please describe your setup**\nI'm running the desktop client locally on my Mac, but I also tried to initiate the export with the browser client connected to my desktop instance as well as running the REST call through postman.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n![Image](https://github.com/user-attachments/assets/15900e3e-818a-49d3-b56b-63f20bb04f8c)\n\n",
      "state": "closed",
      "author": "tbashor",
      "author_type": "User",
      "created_at": "2025-03-19T22:45:04Z",
      "updated_at": "2025-05-17T02:12:16Z",
      "closed_at": "2025-05-17T02:12:16Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2504/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2504",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2504",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:53.810327",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Could you please try again on the latest version? The agent export is an alpha feature (not officially released yet) so has been a bit unstable. ",
          "created_at": "2025-04-01T19:08:53Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-02T02:12:21Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-17T02:12:16Z"
        }
      ]
    },
    {
      "issue_number": 2537,
      "title": "After downloading the latest version 0.6.48, I am unable to connect and use it properly. Could you please help me check? Thank you",
      "body": "After downloading the latest version 0.6.48, I am unable to connect and use it properly. Could you please help me check? Thank you\n[2025-04-02T06:17:29.961Z]\n\nStarting Letta Server...\n\n[2025-04-02T06:18:23.101Z]\n\nInitializing Letta Desktop Service... Connecting to Postgres at postgresql://postgres@localhost:54321/postgres Environment PATH: C:\\Users\\Admin\\AppData\\Local\\Temp\\_MEI45762\\pywin32_system32;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\Puppet Labs\\Puppet\\bin;C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps; Current working directory: C:\\Users\\Admin\\AppData\\Local\\Programs\\letta-desktop Attempt 1 to connect to Postgres failed: Can't create a connection to host localhost and port 54321 (timeout is None and source_address is None). Attempt 2 to connect to Postgres failed: Can't create a connection to host localhost and port 54321 (timeout is None and source_address is None). Attempt 3 to connect to Postgres failed: Can't create a connection to host localhost and port 54321 (timeout is None and source_address is None). Attempt 4 to connect to Postgres failed: Can't create a connection to host localhost and port 54321 (timeout is None and source_address is None). Attempt 5 to connect to Postgres failed: Can't create a connection to host localhost and port 54321 (timeout is None and source_address is None). FATAL: Could not connect to Postgres instance running at postgresql://postgres@localhost:54321/postgres after 5 attempts.\n\n[2025-04-02T06:18:24.751Z]\n\nchild process exited with code 1",
      "state": "closed",
      "author": "xiaoyacat",
      "author_type": "User",
      "created_at": "2025-04-02T06:24:10Z",
      "updated_at": "2025-05-17T02:12:15Z",
      "closed_at": "2025-05-17T02:12:14Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2537/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2537",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2537",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:54.078668",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Are you using Letta Desktop or docker? What was the last working version? ",
          "created_at": "2025-04-02T06:35:11Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-03T02:10:23Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-17T02:12:14Z"
        }
      ]
    },
    {
      "issue_number": 2538,
      "title": "Bug in AWS support for Letta due to incorrect variable name for access key.",
      "body": "**Describe the bug**\n\nHello all. I noticed a bug in AWS support. Line 17 of aws_bedrock.py is looking for the wrong access key variable. You have AWS_ACCESS_KEY. The correct variable names are AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and optionally AWS_SESSION_TOKEN for short-term creds. If the AWS_ACCESS_KEY_ID starts with AS then it's short-term and needs the session token.\n\nIt's easy enough to fix the variable name so it will work with Letta, but you probably meant to have the default names so people could paste in creds without having to change variable names.\n\nAlso this is not going to work with instance profiles, task roles, or execution roles easily. You'd be better off just doing a try and except to see if you can set up a session with boto3, and let the SDK find the creds via the normal path (environment variables as well as execution roles or creds files.\n\nIn general only using the access key id and secret access key means you're using long-lived creds, which is not good for security best practices. Better to support execution roles on AWS so they can manage short-term creds for you.\n\nDo you take pull requests for fixes?\n\nThanks for this great product.\n\n**Please describe your setup**\n\nI don't think you need the extra details given the nature of this issue, but this is in the main branch of letta that I just cloned.",
      "state": "closed",
      "author": "davetbo-amzn",
      "author_type": "User",
      "created_at": "2025-04-02T14:25:46Z",
      "updated_at": "2025-05-17T02:12:14Z",
      "closed_at": "2025-05-17T02:12:13Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2538/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2538",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2538",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:54.322827",
      "comments": [
        {
          "author": "davetbo-amzn",
          "body": "Even finding that and changing to the expected variable name isn't working. It seems like it's never calling the bedrock list models call. I've added debugging statements and it either never hits them or never logs them, despite enabling debug logging.\n\nI'd love to be able to help my customers use L",
          "created_at": "2025-04-02T14:50:51Z"
        },
        {
          "author": "sarahwooders",
          "body": "Thanks for reporting this @davetbo-amzn - The AWS keys/region are read from these settings: https://github.com/letta-ai/letta/blob/main/letta/settings.py (which is why they're being read from `model_settings` https://github.com/letta-ai/letta/blob/main/letta/llm_api/aws_bedrock.py#L28) so I think yo",
          "created_at": "2025-04-02T14:53:45Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-03T02:10:22Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-17T02:12:13Z"
        }
      ]
    },
    {
      "issue_number": 2515,
      "title": "Assistant Messages Incorrectly Embedded Within Reasoning Messages Instead of Separate Message Types",
      "body": "**Describe the bug**\nWhen using the Letta API with Claude models, assistant responses (JSON objects) are incorrectly embedded inside the \"reasoning\" field of reasoning_message objects instead of being returned as distinct assistant_message objects. This creates inconsistent behavior in the chat UI and prevents proper message type separation.\n\n**Please describe your setup**\n- [ ] How did you install letta?\n  - `pip install letta`\n- [ ] Describe your setup\n  - What's your OS \n    - MacOS (macOS Sonoma 14.5)\n    - Running Letta through Node.js/Next.js integration using letta-ai/letta-client package\n    - Claude 3.7 Sonnet model (anthropic/claude-3-7-sonnet-20250219)\n\n**Screenshots**\nNot attaching screenshots, but here's a clear example of the problem from our debug console:\n```\nType: user_message\n{\n  \"id\": \"user_messagemessage-0c45b940-6503-46e1-ac74-19daa19c5e18\",\n  \"date\": 1742823352000,\n  \"original_type\": \"user_message\",\n  \"user_message\": \"what is everest?\"\n}\nType: reasoning_message\n{\n  \"id\": \"reasoning_messagemessage-0879a146-46f0-4180-95dc-c034b03bf35c\",\n  \"date\": 1742823359000,\n  \"original_type\": \"reasoning_message\",\n  \"reasoning\": \"User is asking about Mount Everest. I should provide factual information.\\n\\n{\\\"model_response\\\": \\\"Here's what I know:\\\", \\\"document_md\\\": \\\"Mount Everest is Earth's highest mountain above sea level, located in the Mahalangur Himal sub-range of the Himalayas on the border between Nepal and China. Its elevation is 8,848.86 meters (29,031.7 feet). Known as Sagarmatha in Nepal and Chomolungma in Tibet, it was named after Sir George Everest, a British surveyor. The mountain continues to grow about 4mm per year due to geological uplift. First successfully summited by Edmund Hillary and Tenzing Norgay in 1953, it remains one of mountaineering's greatest challenges.\\\"}\"\n}\n```\n\n**Letta Config**\nUsing default agent configuration with the following settings:\n\n```json\n{\n  \"DEFAULT_MEMORY_BLOCKS\": [\n    {\n      \"label\": \"human\",\n      \"value\": \"The human's name is Ahsan\"\n    },\n    {\n      \"label\": \"persona\",\n      \"value\": \"My name is Khalid, the all-knowing sentient AI.\"\n    },\n    {\n      \"label\": \"response_format\",\n      \"value\": \"Your response must be a JSON object with exactly two fields: {model_response: string, document_md: string}...\"\n    }\n  ],\n  \"DEFAULT_LLM\": \"anthropic/claude-3-7-sonnet-20250219\",\n  \"DEFAULT_EMBEDDING\": \"openai/text-embedding-3-small\",\n  \"DEFAULT_CONTEXT_WINDOW_LIMIT\": 80000\n}\n```\n\n**Local LLM details**\nNot using a local LLM - we're using Claude 3.7 Sonnet via the Letta API.\n",
      "state": "closed",
      "author": "ahsan0608",
      "author_type": "User",
      "created_at": "2025-03-24T16:14:24Z",
      "updated_at": "2025-05-16T02:14:27Z",
      "closed_at": "2025-05-16T02:14:26Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2515/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2515",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2515",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:56.320480",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Hi @ahsan0608 could you please try again on the latest version? We made a bunch of changes to how we handle interactions with Claude 3.7 Sonnet. ",
          "created_at": "2025-04-01T19:08:06Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-02T02:12:19Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-16T02:14:26Z"
        }
      ]
    },
    {
      "issue_number": 2534,
      "title": "Google Models - 'role' Key Error on LLM Response",
      "body": "**Describe the bug**\nSeemingly at random points when a heartbeat message is issued, the LLM response is misconfigured and does not include a 'role' key.\n\nI suspect this is due to the rate limits of Gemini returning an unexpected message after the rate limit is exceeded on the free plan.\n\n**Please describe your setup**\n- [ ] How did you install letta?\n  - Official Docker image.\n- [ ] Describe your setup\n  - MacOS Sequoia 15.3.1\n  - Letta is ran as a Docker image, with a Google Gemini API key, and Postgresql persistence.\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/40abd4f9-d4c4-4505-a791-234ec400b0da)\n\n<img width=\"1438\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/00db9184-d2cb-40d5-98d2-9bc2906be80e\" />\n\n**Additional context**\nUnfortunately, I am unable to see the generated message. It could be that the message was not generated at all due to rate limits, and therefore the 'role' key is not there as the message is blank.\nThis is likely the cause, but I am unable to debug further currently.\n\n**LLM details**\n\nRunning with Google's gemini-2.0-flash-001 model, and the letta-free embedding model.\n",
      "state": "closed",
      "author": "RaspberryPicardBox",
      "author_type": "User",
      "created_at": "2025-04-01T11:33:44Z",
      "updated_at": "2025-05-16T02:14:25Z",
      "closed_at": "2025-05-16T02:14:24Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2534/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sarahwooders"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2534",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2534",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:56.517721",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Thanks for reporting this - looking into it!",
          "created_at": "2025-04-01T19:03:25Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-05-02T02:12:17Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-16T02:14:24Z"
        }
      ]
    },
    {
      "issue_number": 2425,
      "title": "Having user with thousands of messages blocks the entire server after 2/3 messages and slow down agent reply",
      "body": "**Describe the bug**\nAs the title suggests, when a user with almost ~2k messages sends a message, after 2/3 agent replies the entire server is stuck, no error, no logs, nothing.\nNeither the health endpoint can be reached.\n\nAttached to this, a single reply for these users takes more than 1 minute (compared to the 10/15 seconds of a new user with no message history)\n\nPlease let me know if you need further details.\n\n**Letta Config**\nLetta 0.6.22\n",
      "state": "closed",
      "author": "Sapessii",
      "author_type": "User",
      "created_at": "2025-02-09T14:47:34Z",
      "updated_at": "2025-05-12T02:15:34Z",
      "closed_at": "2025-05-12T02:15:34Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2425/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2425",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2425",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:56.746586",
      "comments": [
        {
          "author": "mattzh72",
          "body": "Hi! Sorry you are having this issue. \n\nThis is kind of hard to reproduce on our end - does this only happen at a specific number of messages?",
          "created_at": "2025-02-11T18:22:42Z"
        },
        {
          "author": "Sapessii",
          "body": "Hi @mattzh72, unfortunately we dont have a clear answer on that.\n\nIt looks like the issue isnt tied to the number of messages but rather to a high number of passages for the agent.\n\nIn fact, the error logs reference the agent_passages table. You might try adding several hundred entries to that tab",
          "created_at": "2025-02-11T19:01:40Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-03-14T02:05:43Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-28T02:08:10Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-28T02:13:42Z"
        }
      ]
    },
    {
      "issue_number": 2514,
      "title": "Memory Implementation Optimization",
      "body": "I really like your MCP and plugin mode, but I don't seem to have seen the implementation of memory. When there is more and more dialogue content, how can we ensure that it truly remembers things and has long-term memory? A good example is mem0, which has automatic allocation and creation, updating memory functions,memort api, and they have optimized the context to ensure that there is no very long context. However, they lack the support and ecological tools of Letta for plugins. I hope that both can develop more comprehensively and learn from each other's strengths.  the link :https://docs.mem0.ai/features/contextual-add\n\n\n",
      "state": "closed",
      "author": "kankanai",
      "author_type": "User",
      "created_at": "2025-03-24T07:38:07Z",
      "updated_at": "2025-05-09T02:13:08Z",
      "closed_at": "2025-05-09T02:13:08Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2514/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2514",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2514",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:56.986869",
      "comments": [
        {
          "author": "kankanai",
          "body": "Moreover, it's crucial to note that currently, memory is only triggered when I explicitly state my preference for \"xx.\" This is not ideal and lacks intelligence.\n\nmem0's approach, on the other hand, tends to record everything. Even if I don't say \"I want to go to China,\" but instead say \"How is th",
          "created_at": "2025-03-24T13:00:13Z"
        },
        {
          "author": "sarahwooders",
          "body": "Is it possible to connect to mem0 via tools? Then you can retrieve memories from mem0 into Letta. \n\nThe current memory management in Letta is a slightly more general version of that is described in the [MemGPT](https://arxiv.org/abs/2310.08560) paper. By default, all messages are stored in recall me",
          "created_at": "2025-03-25T17:01:50Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-25T02:11:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-09T02:13:07Z"
        }
      ]
    },
    {
      "issue_number": 2512,
      "title": "Summarizer ignores keep_last_n_messages",
      "body": "**Describe the bug.**\nOnce context window gets full the summarization of previous messages is triggered. As far as I can tell the planned behavior is to keep the last user messages of the block of messages to be summarized to keep context, e.g. if a user asks a question the agent should still know what question to answer after summarizing.\n\nNow we had the problem that exactly this was not the case and the agent wasn't able to refer to this very last user message/question. After debugging this behavior it seems like the agent keeps exactly two messages created **before** the system_alert:\n1. The _System instructions_, which doesn't make sense imho since we have it twice in our composed system prompt then and it already makes up for a big part of the context window.\nI can see [here](https://github.com/letta-ai/letta/blob/main/letta/agent.py#L1154) that this is expected behavior but I'm not sure why.\n\n2. The last user message which is (in our case) often a heartbeat message when the system_alert is triggered after a tool call.\n\nI tried fixing it by setting env variable `letta_summarizer_keep_last_n_messages=3` (also tried higher values) but it's always summarizing it down to only these 2 messages (see logs). Am I misunderstanding anything or do I need to set another env variable to get expected behavior?\n\n```2025-03-21 12:12:46 Letta.letta.agent - WARNING - context window exceeded with limit 7000, attempting to summarize (0/3\n2025-03-21 12:12:46 Letta.letta.agent - INFO - System message token count=1280\n2025-03-21 12:12:46 Letta.letta.agent - INFO - token_counts_no_system=[97, 87, 65, 62, 98, 86, 62, 100, 88, 62, 100, 85, 66, 115, 86, 81, 106, 85, 62, 100, 84, 67, 121, 87, 74, 435, 87, 68, 1049, 88, 63, 1051, 88, 66, 1050, 86]\n2025-03-21 12:12:46 Letta.letta.agent - INFO - desired_token_count_to_summarize=5275\n2025-03-21 12:12:46 Letta.letta.agent - WARNING - Breaking summary cutoff early on role=MessageRole.tool because we hit the `keep_last_n_messages`=3\n2025-03-21 12:12:46 Letta.letta.agent - INFO - Evicting 33/37 messages...\n2025-03-21 12:12:46 Letta.letta.agent - INFO - Attempting to summarize 33 messages of 37\n2025-03-21 12:12:48 httpx - INFO - HTTP Request: POST https://haimdall.dev.ella-lab.io/v1/chat/completions \"HTTP/1.1 200 OK\"\n2025-03-21 12:12:48 Letta.letta.agent - INFO - Got summary: I started by completing my bootup sequence and logged the user's first login. The user greeted me multiple times, and I responded with friendly messages to maintain a casual tone. They shared their favorite color, which I noted for personalization. Later, the user checked in on me, requested a lengthy message for testing, and asked for repetitions of that message several times. I engaged with their requests and kept the conversation flowing.\n2025-03-21 12:12:48 Letta.letta.agent - INFO - Packaged into message: {\n2025-03-21 12:12:48   \"type\": \"system_alert\",\n2025-03-21 12:12:48   \"message\": \"Note: prior messages (34 of 38 total messages) have been hidden from view due to conversation memory constraints.\\nThe following is a summary of the previous 33 messages:\\n I started by completing my bootup sequence and logged the user's first login. The user greeted me multiple times, and I responded with friendly messages to maintain a casual tone. They shared their favorite color, which I noted for personalization. Later, the user checked in on me, requested a lengthy message for testing, and asked for repetitions of that message several times. I engaged with their requests and kept the conversation flowing.\",\n2025-03-21 12:12:48   \"time\": \"2025-03-21 11:12:48 AM UTC+0000\"\n2025-03-21 12:12:48 }\n2025-03-21 12:12:49 Letta.letta.agent - INFO - Ran summarizer, messages length 37 -> 2\n2025-03-21 12:12:49 Letta.letta.agent - INFO - Summarizer brought down total token count from 7537 -> 1463\n```\n\n**Please describe your setup**\n- [X] How did you install letta?\n  - Docker deployment\n- [X] Describe your setup\n  - GCP\n\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n",
      "state": "closed",
      "author": "theocnrds",
      "author_type": "User",
      "created_at": "2025-03-21T11:35:48Z",
      "updated_at": "2025-05-08T02:13:59Z",
      "closed_at": "2025-05-08T02:13:58Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2512/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2512",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2512",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:57.179540",
      "comments": [
        {
          "author": "theocnrds",
          "body": "It seems like we're only keeping the system message and prepending the summary message. Shouldn't there also be a part where either `in_context_messages[cutoff:]` are prepended or all messages except of system_message (see comment above) plus `in_context_messages[cutoff:]` are trimmed in `trim_all_i",
          "created_at": "2025-03-21T11:46:29Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-24T02:11:24Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-08T02:13:58Z"
        }
      ]
    },
    {
      "issue_number": 2505,
      "title": "[Documentation] - Creating Custom tools via Pydantic models throws UnprocessableEntityError",
      "body": "My approach was to use a custom send message tool to do get a json response instead of just text response using pydantic models. \nreference - https://docs.letta.com/guides/agents/custom-tools\n\n```\nfrom letta_client import Letta\nfrom letta_client.client import BaseTool\nfrom pydantic import BaseModel\nfrom typing import List, Type, Dict, Any\n\nclass StructuredOutputSchema(BaseModel):\n    msg_type: str\n    message: str\n\nclass StructuredOutputTool(BaseTool):\n    name: str = \"structured_output\"\n    args_schema: Type[BaseModel] = StructuredOutputSchema\n    description: str = \"Return structured output with msg_type and message fields\"\n    tags: List[str] = [\"output\", \"formatting\"]\n    \n    def run(self, data: StructuredOutputSchema) -> Dict[str, Any]:\n        return {\n            \"msg_type\": data.msg_type,\n            \"message\": data.message\n        }\n\n# Create a client to connect to your local Letta Server\nclient = Letta(\n    base_url=\"http://localhost:8283\"\n)\n\n# Create and register the tool\nstructured_output_tool = client.tools.add(\n    tool=StructuredOutputTool(),\n)\n\nagent = client.agent.create(\n    model=\"anthropic/claude-3.5-sonnet\",\n    embedding=\"letta/letta-free\",\n    tools=[structured_output_tool],\n)\n\nprint(agent.id)\n```\n\n\nwhen I run this code I got the following error\n\n```\nTraceback (most recent call last):\n  File \"/Users/admin/Documents/grexa-gbp-whatsapp-agent/grexa-gbp-whatsapp-agent/grexa_gbp_chatbot/test.py\", line 28, in <module>\n    structured_output_tool = client.tools.add(\n                             ^^^^^^^^^^^^^^^^^\n  File \"/Users/admin/Documents/grexa-gbp-whatsapp-agent/grexa-gbp-whatsapp-agent/grexa-env/lib/python3.12/site-packages/letta_client/client.py\", line 374, in add\n    return self.upsert(\n           ^^^^^^^^^^^^\n  File \"/Users/admin/Documents/grexa-gbp-whatsapp-agent/grexa-gbp-whatsapp-agent/grexa-env/lib/python3.12/site-packages/letta_client/tools/client.py\", line 496, in upsert\n    raise UnprocessableEntityError(\nletta_client.errors.unprocessable_entity_error.UnprocessableEntityError: status_code: 422, body: detail=[ValidationError(loc=['body', 'args_json_schema'], msg='Extra inputs are not permitted', type='extra_forbidden', input={'properties': {'msg_type': {'title': 'Msg Type', 'type': 'string'}, 'message': {'title': 'Message', 'type': 'string'}}, 'required': ['msg_type', 'message'], 'title': 'StructuredOutputSchema', 'type': 'object'})]\n```\n\nTo recheck if the problem isn't due to my code I tried to run the code snippet provided in the official docs - https://docs.letta.com/guides/agents/custom-tools\n\n```\n\nfrom letta_client import Letta\nfrom letta_client.client import BaseTool\nfrom pydantic import BaseModel\nfrom typing import List, Type\n\nclass InventoryItem(BaseModel):\n    sku: str  # Unique product identifier\n    name: str  # Product name\n    price: float  # Current price\n    category: str  # Product category (e.g., \"Electronics\", \"Clothing\")\n\nclass InventoryEntry(BaseModel):\n    timestamp: int  # Unix timestamp of the transaction\n    item: InventoryItem  # The product being updated\n    transaction_id: str  # Unique identifier for this inventory update\n\nclass InventoryEntryData(BaseModel):\n    data: InventoryEntry\n    quantity_change: int  # Change in quantity (positive for additions, negative for removals)\n\n\nclass ManageInventoryTool(BaseTool):\n    name: str = \"manage_inventory\"\n    args_schema: Type[BaseModel] = InventoryEntryData\n    description: str = \"Update inventory catalogue with a new data entry\"\n    tags: List[str] = [\"inventory\", \"shop\"]\n\n    def run(self, data: InventoryEntry, quantity_change: int) -> bool:\n        print(f\"Updated inventory for {data.item.name} with a quantity change of {quantity_change}\")\n        return True\n\n# create a client to connect to your local Letta Server\nclient = Letta(\n  base_url=\"http://localhost:8283\"\n)\n# create the tool\ntool_from_class = client.tools.add(\n    tool=ManageInventoryTool(),\n)\n\n```\n\nI got the same *UnprocessableEntityError* - \n\n```\nTraceback (most recent call last):\n  File \"/Users/admin/Documents/grexa-gbp-whatsapp-agent/grexa-gbp-whatsapp-agent/grexa_gbp_chatbot/test.py\", line 78, in <module>\n    tool_from_class = client.tools.add(\n                      ^^^^^^^^^^^^^^^^^\n  File \"/Users/admin/Documents/grexa-gbp-whatsapp-agent/grexa-gbp-whatsapp-agent/grexa-env/lib/python3.12/site-packages/letta_client/client.py\", line 374, in add\n    return self.upsert(\n           ^^^^^^^^^^^^\n  File \"/Users/admin/Documents/grexa-gbp-whatsapp-agent/grexa-gbp-whatsapp-agent/grexa-env/lib/python3.12/site-packages/letta_client/tools/client.py\", line 496, in upsert\n    raise UnprocessableEntityError(\nletta_client.errors.unprocessable_entity_error.UnprocessableEntityError: status_code: 422, body: detail=[ValidationError(loc=['body', 'args_json_schema'], msg='Extra inputs are not permitted', type='extra_forbidden', input={'$defs': {'InventoryEntry': {'properties': {'timestamp': {'title': 'Timestamp', 'type': 'integer'}, 'item': {'$ref': '#/$defs/InventoryItem'}, 'transaction_id': {'title': 'Transaction Id', 'type': 'string'}}, 'required': ['timestamp', 'item', 'transaction_id'], 'title': 'InventoryEntry', 'type': 'object'}, 'InventoryItem': {'properties': {'sku': {'title': 'Sku', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'price': {'title': 'Price', 'type': 'number'}, 'category': {'title': 'Category', 'type': 'string'}}, 'required': ['sku', 'name', 'price', 'category'], 'title': 'InventoryItem', 'type': 'object'}}, 'properties': {'data': {'$ref': '#/$defs/InventoryEntry'}, 'quantity_change': {'title': 'Quantity Change', 'type': 'integer'}}, 'required': ['data', 'quantity_change'], 'title': 'InventoryEntryData', 'type': 'object'})]\n```\n\n## Can someone let me know how do I get my letta ai agent to return a structured output, like a custom JSON response instead of just text response? \n\n\n",
      "state": "closed",
      "author": "ranjeetsinh-grexa",
      "author_type": "User",
      "created_at": "2025-03-20T07:06:30Z",
      "updated_at": "2025-05-05T02:15:05Z",
      "closed_at": "2025-05-05T02:15:05Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2505/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2505",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2505",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:57.392127",
      "comments": [
        {
          "author": "lemorage",
          "body": "Which version are u using here? I can't reproduce your issue on macOS, with letta v0.6.43.",
          "created_at": "2025-03-20T08:44:03Z"
        },
        {
          "author": "ranjeetsinh-grexa",
          "body": "> Which version are u using here? I can't reproduce your issue on macOS, with letta v0.6.43.\n\nv0.6.27\nIs this new feature compatible with the version I am currently on?\n",
          "created_at": "2025-03-20T09:05:14Z"
        },
        {
          "author": "lemorage",
          "body": "well, I can't remember, but as far as I can tell, the pydantic model in tools, seemed to have a few bugs in old versions. Can u try the latest version first to see if it works?",
          "created_at": "2025-03-20T09:22:32Z"
        },
        {
          "author": "ranjeetsinh-grexa",
          "body": "> well, I can't remember, but as far as I can tell, the pydantic model in tools, seemed to have a few bugs in old versions. Can u try the latest version first to see if it works?\n\nI upgraded the pydantic module to pydantic==2.10.6 and also I tried creating the tool using letta v0.6.41\nNow I am getti",
          "created_at": "2025-03-20T09:27:34Z"
        },
        {
          "author": "lemorage",
          "body": "try to add docstrings to your tools, when creating them. The documentation is mandatory.",
          "created_at": "2025-03-20T09:31:32Z"
        }
      ]
    },
    {
      "issue_number": 2511,
      "title": "Creating a tool from a file failed when generating schema",
      "body": "**Describe the bug**\nThe following error will be raised using the \"**creating a tool from a file**\" functionality, however, there is no param 'id' in the tool. Please take a look at the reproduce steps for details.\n```\nletta_client.core.api_error.ApiError: status_code: 400, body: {'detail': \"400: Sc\nhema generation failed: Value error in schema generation: Parameter 'id' in funct\nion 'ManageInventoryTool' lacks a description in the docstring\", 'trace_id': '85d\n72535a2e0c1a3aba659a88844e49d'}\n```\n\n**How to reproduce**\n1. Using the following code snippet copied from [letta custom-tools docs](https://docs.letta.com/guides/agents/custom-tools), add some docstrings and put those into a separate file.\n```py\nfrom letta_client import Letta\nfrom letta_client.client import BaseTool\nfrom pydantic import BaseModel\nfrom typing import List, Type\n\nclass InventoryItem(BaseModel):\n    sku: str  # Unique product identifier\n    name: str  # Product name\n    price: float  # Current price\n    category: str  # Product category (e.g., \"Electronics\", \"Clothing\")\n\nclass InventoryEntry(BaseModel):\n    timestamp: int  # Unix timestamp of the transaction\n    item: InventoryItem  # The product being updated\n    transaction_id: str  # Unique identifier for this inventory update\n\nclass InventoryEntryData(BaseModel):\n    data: InventoryEntry\n    quantity_change: int  # Change in quantity (positive for additions, negative for removals)\n\n\nclass ManageInventoryTool(BaseTool):\n    name: str = \"manage_inventory\"\n    args_schema: Type[BaseModel] = InventoryEntryData\n    description: str = \"Update inventory catalogue with a new data entry\"\n    tags: List[str] = [\"inventory\", \"shop\"]\n\n    def run(self, data: InventoryEntry, quantity_change: int) -> bool:\n        print(f\"Updated inventory for {data.item.name} with a quantity change of {quantity_change}\")\n        return True\n```\n\n2. run the following code to create a tool from that file.\n```py\ntool = client.tools.create(\n    source_code = open(\"custom_tool.py\", \"r\").read()\n)\n```\n\n3. the server will complain about the following, but there is no param called `id` in the above tool.\n```\nTraceback (most recent call last):\n  File \"/Users/clannad/Library/Caches/pypoetry/virtualenvs/yanara-BBYGsHMK-py3.12/lib/python3.12/site-packages/letta/functions/functions.py\", line 51, in derive_openai_json_schema\n    raise LettaToolCreateError(f\"Value error in schema generation: {str(e)}\")\nletta.errors.LettaToolCreateError: Value error in schema generation: Parameter 'id' in function 'ManageInventoryTool' lacks a description in the docstring\nError occurred during tool creation: Schema generation failed: Value error in schema generation: Parameter 'id' in function 'ManageInventoryTool' lacks a description in the docstring\nTraceback (most recent call last):\n  File \"/Users/clannad/Library/Caches/pypoetry/virtualenvs/yanara-BBYGsHMK-py3.12/lib/python3.12/site-packages/letta/functions/functions.py\", line 45, in derive_openai_json_schema\n    schema = generate_schema(func, name=name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/clannad/Library/Caches/pypoetry/virtualenvs/yanara-BBYGsHMK-py3.12/lib/python3.12/site-packages/letta/functions/schema_generator.py\", line 352, in generate_schema\n    raise ValueError(f\"Parameter '{param.name}' in function '{function.__name__}' lacks a description in the docstring\")\nValueError: Parameter 'id' in function 'ManageInventoryTool' lacks a description in the docstring\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/clannad/Library/Caches/pypoetry/virtualenvs/yanara-BBYGsHMK-py3.12/lib/python3.12/site-packages/letta/functions/functions.py\", line 51, in derive_openai_json_schema\n    raise LettaToolCreateError(f\"Value error in schema generation: {str(e)}\")\nletta.errors.LettaToolCreateError: Value error in schema generation: Parameter 'id' in function 'ManageInventoryTool' lacks a description in the docstring\n```\n\n**Please describe your setup**\n- [X] How did you install letta?\n  - `pip install letta`\n- [X] Describe your setup\n  - **MacOS**\n  - **Terminal**\n\n**Letta Config**\n```cfg\n[defaults]\npreset = memgpt_chat\npersona = sam_pov\nhuman = basic\n\n[archival_storage]\ntype = postgres\npath = /Users/clannad/.letta\nuri = postgresql://postgres:@/postgres?host=/Users/clannad/.letta/desktop_data\n\n[recall_storage]\ntype = postgres\npath = /Users/clannad/.letta\nuri = postgresql://postgres:@/postgres?host=/Users/clannad/.letta/desktop_data\n\n[metadata_storage]\ntype = sqlite\npath = /Users/clannad/.letta\n\n[version]\nletta_version = 0.6.43\n```\n\n",
      "state": "closed",
      "author": "lemorage",
      "author_type": "User",
      "created_at": "2025-03-21T06:18:06Z",
      "updated_at": "2025-05-05T02:15:03Z",
      "closed_at": "2025-05-05T02:15:02Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2511/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2511",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2511",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:57.588283",
      "comments": [
        {
          "author": "lemorage",
          "body": "I think I get the point here. When creating a tool using the example code above, the `derive_openai_json_schema` function expects a function, but instead, it receives a class here. This class will be passed into `generate_schema` to get its signature out.\n```py\ngenerate_schema(func) # func will be t",
          "created_at": "2025-03-21T13:29:33Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-21T02:13:44Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-05-05T02:15:01Z"
        }
      ]
    },
    {
      "issue_number": 2410,
      "title": "openai.InternalServerError: Error code: 500:ADE(connect to local letta server) fails to send message after meets the max Context Window,details are below",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**Please describe your setup**  \n- [ ] Describe your setup\n  -  docker run letta server\n  -  use ADE connect local server:https://app.letta.com/development-servers/local/agents/agent-b0102eac-306f-4a8c-9016-0742b4b4836c\n\n**Screenshots**\nI can't send message to agent after the Context Window meets its MAX(set 4000 token)\n\n![Image](https://github.com/user-attachments/assets/4dbda49a-3162-4d52-8f71-b365ebdef8a2)\n\nI check the docker log:\n\n```python\n\n2025-02-03 16:33:13   File \"/app/letta/server/rest_api/utils.py\", line 62, in sse_async_generator\n2025-02-03 16:33:13     usage = await usage_task\n2025-02-03 16:33:13             ^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/usr/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n2025-02-03 16:33:13     return await loop.run_in_executor(None, func_call)\n2025-02-03 16:33:13            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n2025-02-03 16:33:13     result = self.fn(*self.args, **self.kwargs)\n2025-02-03 16:33:13              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/server/server.py\", line 772, in send_messages\n2025-02-03 16:33:13     return self._step(actor=actor, agent_id=agent_id, input_messages=message_objects, interface=interface)\n2025-02-03 16:33:13            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/server/server.py\", line 458, in _step\n2025-02-03 16:33:13     usage_stats = letta_agent.step(\n2025-02-03 16:33:13                   ^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/agent.py\", line 613, in step\n2025-02-03 16:33:13     step_response = self.inner_step(\n2025-02-03 16:33:13                     ^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/agent.py\", line 854, in inner_step\n2025-02-03 16:33:13     raise e\n2025-02-03 16:33:13   File \"/app/letta/agent.py\", line 721, in inner_step\n2025-02-03 16:33:13     response = self._get_ai_reply(\n2025-02-03 16:33:13                ^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/agent.py\", line 327, in _get_ai_reply\n2025-02-03 16:33:13     raise e\n2025-02-03 16:33:13   File \"/app/letta/agent.py\", line 290, in _get_ai_reply\n2025-02-03 16:33:13     response = create(\n2025-02-03 16:33:13                ^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/llm_api/llm_api_tools.py\", line 91, in wrapper\n2025-02-03 16:33:13     raise e\n2025-02-03 16:33:13   File \"/app/letta/llm_api/llm_api_tools.py\", line 58, in wrapper\n2025-02-03 16:33:13     return func(*args, **kwargs)\n2025-02-03 16:33:13            ^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/llm_api/llm_api_tools.py\", line 177, in create\n2025-02-03 16:33:13     response = openai_chat_completions_request(\n2025-02-03 16:33:13                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/letta/llm_api/openai.py\", line 413, in openai_chat_completions_request\n2025-02-03 16:33:13     chat_completion = client.chat.completions.create(**data)\n2025-02-03 16:33:13                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n2025-02-03 16:33:13     return func(*args, **kwargs)\n2025-02-03 16:33:13            ^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 863, in create\n2025-02-03 16:33:13     return self._post(\n2025-02-03 16:33:13            ^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1283, in post\n2025-02-03 16:33:13     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n2025-02-03 16:33:13                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 960, in request\n2025-02-03 16:33:13     return self._request(\n2025-02-03 16:33:13            ^^^^^^^^^^^^^^\n2025-02-03 16:33:13   File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1064, in _request\n2025-02-03 16:33:13     raise self._make_status_error_from_response(err.response) from None\n2025-02-03 16:33:13 openai.InternalServerError: Error code: 500 - {'detail': 'Internal server error (unpack): '}\n\n```\n\n",
      "state": "closed",
      "author": "nj-guiqi",
      "author_type": "User",
      "created_at": "2025-02-03T08:35:37Z",
      "updated_at": "2025-05-04T02:17:23Z",
      "closed_at": "2025-05-04T02:17:23Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2410/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2410",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2410",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:57.823966",
      "comments": [
        {
          "author": "wsargent",
          "body": "Same issue here.",
          "created_at": "2025-02-16T15:42:51Z"
        },
        {
          "author": "Autuk",
          "body": "Same issue.",
          "created_at": "2025-02-18T05:56:49Z"
        },
        {
          "author": "LixinLu42",
          "body": "now, my ADE is disapper Context Window Size option.\n\n![Image](https://github.com/user-attachments/assets/fd4becd1-b56b-4088-bf56-6e61caa9fe21)",
          "created_at": "2025-02-20T07:46:31Z"
        },
        {
          "author": "MeowLynxSea",
          "body": "same issue",
          "created_at": "2025-02-26T08:35:21Z"
        },
        {
          "author": "1803053530",
          "body": "Same issue here.",
          "created_at": "2025-03-20T09:23:53Z"
        }
      ]
    },
    {
      "issue_number": 2492,
      "title": "",
      "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n\n![Image](https://github.com/user-attachments/assets/19d6acd0-eb81-4bff-9cd2-3f5247156bd7)\n",
      "state": "closed",
      "author": "CPF471112809",
      "author_type": "User",
      "created_at": "2025-03-15T09:29:48Z",
      "updated_at": "2025-04-30T02:11:47Z",
      "closed_at": "2025-04-30T02:11:46Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2492/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2492",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2492",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:58.054599",
      "comments": [
        {
          "author": "CPF471112809",
          "body": "latta desktop\n .latta lattalatta\n1  .lattalatta.latta\ntemp\n2lattalattalatta",
          "created_at": "2025-03-16T04:21:06Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-16T02:11:03Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-30T02:11:46Z"
        }
      ]
    },
    {
      "issue_number": 2493,
      "title": "stream generator failed..  using openai compat api(s) ie: requesty.ai, openrouter ..",
      "body": "letta [0.6.41]\nTrying to use openai api services like \nrequesty.ai, openrouter .. using latest function call llms like qwq 32B  always generating \n\"stream generator failed.\" errors\n\n```\nconfig: using the sdk:\nagent = client.agents.create(\nname=\"thedude\", \nmemory_blocks=[\n\tCreateBlock(\n\t\tlabel=\"human\",\n\t\tvalue=\"Name: The Dude\",\n\t),\n\tCreateBlock(\n\t\tlabel=\"persona\",\n\t\tvalue=\"You are a helpful assistant\",\n\t),\n],\nllm_config=LlmConfig(\n    model=\"nebius/Qwen/QwQ-32B\",\n\tmodel_endpoint_type=\"openai\",\n\tmodel_endpoint=\"https://router.requesty.ai/v1\",\n\tapi_key=os.getenv(\"OPENAI_API_KEY\"),\n\tcontext_window=16384\n),\nembedding_config=EmbeddingConfig(embedding_endpoint_type='hugging-face', embedding_endpoint='https://embeddings.memgpt.ai', embedding_model='letta-free', embedding_dim=1024, embedding_chunk_size=300),\n)\n```\n\n### ERROR LOG:\n  File \"/app/letta/llm_api/llm_api_tools.py\", line 61, in wrapper\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/llm_api/llm_api_tools.py\", line 205, in create\n\n    response = openai_chat_completions_process_stream(\n\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/llm_api/openai.py\", line 387, in openai_chat_completions_process_stream\n\n    raise e\n\n  File \"/app/letta/llm_api/openai.py\", line 255, in openai_chat_completions_process_stream\n\n    for chat_completion_chunk in openai_chat_completions_request_stream(\n\n  File \"/app/letta/llm_api/openai.py\", line 424, in openai_chat_completions_request_stream\n\n    stream = client.chat.completions.create(**data)\n\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 914, in create\n\n    return self._post(\n\n           ^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1242, in post\n\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 919, in request\n\n    return self._request(\n\n           ^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1023, in _request\n\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'message': ''}}\n\nNone\n\nTraceback (most recent call last):\n\n  File \"/app/letta/server/rest_api/utils.py\", line 77, in sse_async_generator\n\n    usage = await usage_task\n\n            ^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n\n    return await loop.run_in_executor(None, func_call)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n\n    result = self.fn(*self.args, **self.kwargs)\n\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/server/server.py\", line 716, in send_messages\n\n    return self._step(\n\n           ^^^^^^^^^^^\n\n  File \"/app/letta/server/server.py\", line 399, in _step\n\n    usage_stats = letta_agent.step(\n\n                  ^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/tracing.py\", line 188, in sync_wrapper\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/agent.py\", line 779, in step\n\n    step_response = self.inner_step(\n\n                    ^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/agent.py\", line 1055, in inner_step\n\n    raise e\n\n  File \"/app/letta/agent.py\", line 892, in inner_step\n\n    response = self._get_ai_reply(\n\n               ^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/tracing.py\", line 188, in sync_wrapper\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/agent.py\", line 455, in _get_ai_reply\n\n    raise e\n\n  File \"/app/letta/agent.py\", line 414, in _get_ai_reply\n\n    response = create(\n\n               ^^^^^^^\n\n  File \"/app/letta/tracing.py\", line 188, in sync_wrapper\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/llm_api/llm_api_tools.py\", line 118, in wrapper\n\n    raise e\n\n  File \"/app/letta/llm_api/llm_api_tools.py\", line 61, in wrapper\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/llm_api/llm_api_tools.py\", line 205, in create\n\n    response = openai_chat_completions_process_stream(\n\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/letta/llm_api/openai.py\", line 387, in openai_chat_completions_process_stream\n\n    raise e\n\n  File \"/app/letta/llm_api/openai.py\", line 255, in openai_chat_completions_process_stream\n\n    for chat_completion_chunk in openai_chat_completions_request_stream(\n\n  File \"/app/letta/llm_api/openai.py\", line 424, in openai_chat_completions_request_stream\n\n    stream = client.chat.completions.create(**data)\n\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 914, in create\n\n    return self._post(\n\n           ^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1242, in post\n\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 919, in request\n\n    return self._request(\n\n           ^^^^^^^^^^^^^^\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1023, in _request\n\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'message': ''}}\n\n/app/letta/server/rest_api/utils.py:133: UserWarning: SSE stream generator failed: Error code: 400 - {'error': {'message': ''}}\n\n  warnings.warn(f\"SSE stream generator failed: {e}\")\n\nLetta.letta.server.rest_api.utils - ERROR - Caught unexpected Exception: Error code: 400 - {'error': {'message': ''}}\n\n# maybe these 'services' are not supporting streaming?, tool use, function calls? anyone successfully using any llms from requesty.ai, ?",
      "state": "closed",
      "author": "quantumalchemy",
      "author_type": "User",
      "created_at": "2025-03-15T13:49:10Z",
      "updated_at": "2025-04-29T02:11:54Z",
      "closed_at": "2025-04-29T02:11:54Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2493/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2493",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2493",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:58.289488",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-15T02:11:20Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-29T02:11:53Z"
        }
      ]
    },
    {
      "issue_number": 2481,
      "title": "LETTA-FREE model",
      "body": "Hi! \n\nI'm interested in power of letta free model,\nhow much vRAM does it take, how big is it, is it on hugging face?\n\nAlso I noticed that it doesnt condense the messages, it just runs into an error when the context is filled...\n\nAny documentation on it? Just some basic info?",
      "state": "closed",
      "author": "gbanusi",
      "author_type": "User",
      "created_at": "2025-03-13T09:15:58Z",
      "updated_at": "2025-04-28T02:13:42Z",
      "closed_at": "2025-04-28T02:13:42Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2481/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2481",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2481",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:58.504365",
      "comments": [
        {
          "author": "quantumalchemy",
          "body": "The free model -- its the free endpoint  from letta you can use by default just to get started\nthe actual LLM was a opensource llm -- I believe was based on: dolphin-2.7-mixtral-8x7b  but may have changed\nanyway its not really working anymore since like v0.6..",
          "created_at": "2025-03-14T17:08:43Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-14T02:12:21Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-28T02:13:41Z"
        }
      ]
    },
    {
      "issue_number": 2482,
      "title": "When using the DeepSeek v3 model, an exception is prompted: Exception: Retries exhausted and no valid response received. Final error: Failed to create valid JSON error.",
      "body": "The DeepSeek R1 model can be used normally.\n\nThe installation command used is: docker run --name letta -v C:/letta_data:/var/lib/postgresql/data -p 8283:8283 -e DEEPSEEK_API_KEY=\"xxx\" letta/letta:latest\n\ncontent window4317/64000 tokens\npersona1843/6000 CHARS\n\nlog\n2025-03-13 20:06:42 httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2025-03-13 20:06:48 Failed to interpret reasoning content () as JSON: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:06:51 Failed to interpret reasoning content () as JSON: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:06:51 Letta.letta.llm_api.openai - INFO - Finally ending streaming interface.\n2025-03-13 20:06:51 Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:06:52 httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2025-03-13 20:07:03 Failed to interpret reasoning content () as JSON: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:07 Failed to interpret reasoning content () as JSON: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:07 Letta.letta.llm_api.openai - INFO - Finally ending streaming interface.\n2025-03-13 20:07:07 Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:08 httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n2025-03-13 20:07:15 Failed to interpret reasoning content () as JSON: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:18 Failed to interpret reasoning content () as JSON: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:18 Letta.letta.llm_api.openai - INFO - Finally ending streaming interface.\n2025-03-13 20:07:18 Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:18 Letta.letta.agent - ERROR - step() failed\n2025-03-13 20:07:18 messages = [Message(created_by_id=None, last_updated_by_id=None, created_at=datetime.datetime(2025, 3, 13, 12, 6, 41, 731674, tzinfo=datetime.timezone.utc), updated_at=None, id='message-7c74556c-a0be-48b4-b44d-47b52a1dfddb', role=<MessageRole.user: 'user'>, content=[TextContent(type=<MessageContentType.text: 'text'>, text='{\\n  \"type\": \"user_message\",\\n  \"message\": \"xxxxxx\",\\n  \"time\": \"2025-03-13 12:06:41 PM UTC+0000\"\\n}')], organization_id=None, agent_id='agent-563d786e-51cb-473e-b09d-7da671d38929', model=None, name=None, tool_calls=None, tool_call_id=None, step_id=None)]\n2025-03-13 20:07:18 error = Retries exhausted and no valid response received. Final error: Failed to create valid JSON \n2025-03-13 20:07:18 Letta.letta.agent - ERROR - step() failed with an unrecognized exception: 'Retries exhausted and no valid response received. Final error: Failed to create valid JSON '\n2025-03-13 20:07:18 Letta.letta.server.server - ERROR - Error in server._step: Retries exhausted and no valid response received. Final error: Failed to create valid JSON \n2025-03-13 20:07:18 Traceback (most recent call last):\n2025-03-13 20:07:18   File \"/app/letta/llm_api/deepseek.py\", line 276, in convert_deepseek_response_to_chatcompletion\n2025-03-13 20:07:18     content_dict = json.loads(extract_json_block(content))\n2025-03-13 20:07:18                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n2025-03-13 20:07:18     return _default_decoder.decode(s)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n2025-03-13 20:07:18     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n2025-03-13 20:07:18                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n2025-03-13 20:07:18     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n2025-03-13 20:07:18 json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:18 \n2025-03-13 20:07:18 During handling of the above exception, another exception occurred:\n2025-03-13 20:07:18 \n2025-03-13 20:07:18 Traceback (most recent call last):\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 360, in _get_ai_reply\n2025-03-13 20:07:18     response = create(\n2025-03-13 20:07:18                ^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/tracing.py\", line 65, in sync_wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/llm_api/llm_api_tools.py\", line 118, in wrapper\n2025-03-13 20:07:18     raise e\n2025-03-13 20:07:18   File \"/app/letta/llm_api/llm_api_tools.py\", line 61, in wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/llm_api/llm_api_tools.py\", line 529, in create\n2025-03-13 20:07:18     response = convert_deepseek_response_to_chatcompletion(response)\n2025-03-13 20:07:18                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/llm_api/deepseek.py\", line 294, in convert_deepseek_response_to_chatcompletion\n2025-03-13 20:07:18     raise ValueError(f\"Failed to create valid JSON {content}\")\n2025-03-13 20:07:18 ValueError: Failed to create valid JSON \n2025-03-13 20:07:18 \n2025-03-13 20:07:18 During handling of the above exception, another exception occurred:\n2025-03-13 20:07:18 \n2025-03-13 20:07:18 Traceback (most recent call last):\n2025-03-13 20:07:18   File \"/app/letta/server/server.py\", line 365, in _step\n2025-03-13 20:07:18     usage_stats = letta_agent.step(\n2025-03-13 20:07:18                   ^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/tracing.py\", line 65, in sync_wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 666, in step\n2025-03-13 20:07:18     step_response = self.inner_step(\n2025-03-13 20:07:18                     ^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 924, in inner_step\n2025-03-13 20:07:18     raise e\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 779, in inner_step\n2025-03-13 20:07:18     response = self._get_ai_reply(\n2025-03-13 20:07:18                ^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/tracing.py\", line 65, in sync_wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 392, in _get_ai_reply\n2025-03-13 20:07:18     raise Exception(f\"Retries exhausted and no valid response received. Final error: {ve}\")\n2025-03-13 20:07:18 Exception: Retries exhausted and no valid response received. Final error: Failed to create valid JSON \n2025-03-13 20:07:18 None\n2025-03-13 20:07:18 Traceback (most recent call last):\n2025-03-13 20:07:18   File \"/app/letta/llm_api/deepseek.py\", line 276, in convert_deepseek_response_to_chatcompletion\n2025-03-13 20:07:18     content_dict = json.loads(extract_json_block(content))\n2025-03-13 20:07:18                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n2025-03-13 20:07:18     return _default_decoder.decode(s)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n2025-03-13 20:07:18     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n2025-03-13 20:07:18                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n2025-03-13 20:07:18     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n2025-03-13 20:07:18 json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n2025-03-13 20:07:18 \n2025-03-13 20:07:18 During handling of the above exception, another exception occurred:\n2025-03-13 20:07:18 \n2025-03-13 20:07:18 Traceback (most recent call last):\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 360, in _get_ai_reply\n2025-03-13 20:07:18     response = create(\n2025-03-13 20:07:18                ^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/tracing.py\", line 65, in sync_wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/llm_api/llm_api_tools.py\", line 118, in wrapper\n2025-03-13 20:07:18     raise e\n2025-03-13 20:07:18   File \"/app/letta/llm_api/llm_api_tools.py\", line 61, in wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/llm_api/llm_api_tools.py\", line 529, in create\n2025-03-13 20:07:18     response = convert_deepseek_response_to_chatcompletion(response)\n2025-03-13 20:07:18                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/llm_api/deepseek.py\", line 294, in convert_deepseek_response_to_chatcompletion\n2025-03-13 20:07:18     raise ValueError(f\"Failed to create valid JSON {content}\")\n2025-03-13 20:07:18 ValueError: Failed to create valid JSON \n2025-03-13 20:07:18 \n2025-03-13 20:07:18 During handling of the above exception, another exception occurred:\n2025-03-13 20:07:18 \n2025-03-13 20:07:18 Traceback (most recent call last):\n2025-03-13 20:07:18   File \"/app/letta/server/rest_api/utils.py\", line 77, in sse_async_generator\n2025-03-13 20:07:18     usage = await usage_task\n2025-03-13 20:07:18             ^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n2025-03-13 20:07:18     return await loop.run_in_executor(None, func_call)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n2025-03-13 20:07:18     result = self.fn(*self.args, **self.kwargs)\n2025-03-13 20:07:18              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/server/server.py\", line 681, in send_messages\n2025-03-13 20:07:18     return self._step(\n2025-03-13 20:07:18            ^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/server/server.py\", line 365, in _step\n2025-03-13 20:07:18     usage_stats = letta_agent.step(\n2025-03-13 20:07:18                   ^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/tracing.py\", line 65, in sync_wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 666, in step\n2025-03-13 20:07:18     step_response = self.inner_step(\n2025-03-13 20:07:18                     ^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 924, in inner_step\n2025-03-13 20:07:18     raise e\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 779, in inner_step\n2025-03-13 20:07:18     response = self._get_ai_reply(\n2025-03-13 20:07:18                ^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/tracing.py\", line 65, in sync_wrapper\n2025-03-13 20:07:18     return func(*args, **kwargs)\n2025-03-13 20:07:18            ^^^^^^^^^^^^^^^^^^^^^\n2025-03-13 20:07:18   File \"/app/letta/agent.py\", line 392, in _get_ai_reply\n2025-03-13 20:07:18     raise Exception(f\"Retries exhausted and no valid response received. Final error: {ve}\")\n2025-03-13 20:07:18 Exception: Retries exhausted and no valid response received. Final error: Failed to create valid JSON \n2025-03-13 20:07:18 Letta.letta.server.rest_api.utils - ERROR - Caught unexpected Exception: Retries exhausted and no valid response received. Final error: Failed to create valid JSON \n2025-03-13 20:08:36 2025-03-13 12:08:36.645 UTC [30] LOG:  checkpoint starting: time\n2025-03-13 20:08:39 2025-03-13 12:08:39.540 UTC [30] LOG:  checkpoint complete: wrote 29 buffers (0.2%); 0 WAL file(s) added, 0 removed, 0 recycled; write=2.846 s, sync=0.015 s, total=2.896 s; sync files=20, longest=0.003 s, average=0.001 s; distance=118 kB, estimate=118 kB",
      "state": "closed",
      "author": "z82134359",
      "author_type": "User",
      "created_at": "2025-03-13T12:20:25Z",
      "updated_at": "2025-04-28T02:13:41Z",
      "closed_at": "2025-04-28T02:13:40Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2482/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2482",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2482",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:58.718894",
      "comments": [
        {
          "author": "z82134359",
          "body": "When accessing the DeepSeek-v3 model directly through Open WebUI, it displays normally.",
          "created_at": "2025-03-13T12:28:07Z"
        },
        {
          "author": "lemorage",
          "body": "Are the curly quotes in the installation commands on purpose? Will change to straight quotes have any effect?",
          "created_at": "2025-03-14T06:09:22Z"
        },
        {
          "author": "z82134359",
          "body": "> Are the curly quotes in the installation commands on purpose? Will change to straight quotes have any effect?\n\nTranslation issue: I am using glm4 for translation, and it translated the quotation marks into Chinese. The original text was this quotation mark: \"",
          "created_at": "2025-03-14T12:05:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-14T02:12:19Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-28T02:13:40Z"
        }
      ]
    },
    {
      "issue_number": 2484,
      "title": "How to create an Agent via POST /v1/agents/?(request body)",
      "body": "I didn't use letta through docker and desktop.\nI tried to install letta using pip, and then started the service on the local wsl through OLLAMA_BASE_URL=http://127.0.0.1:11500 letta server --host 0.0.0.0 --port 8089 --ade. After I logged in through the browser, I wanted to create an agent to connect to the ollama model, but I encountered a \"POST /v1/agents/ HTTP/1.1\" 422 Unprocessable Entity error. I want to know how to write the Request body\n\n{\n  \"name\": \"test111\",\t#\n  \"memory_blocks\": [\n    {\n      \"value\": \"You are my personal AI assistant\",\n      \"limit\": 5000,\n      \"name\": \"deepseek_starter\",\n      \"is_template\": false,\n      \"label\": \"persona\",\n      \"description\": null,\t\n      \"metadata\": {}\n    }\n  ],\n  \"tools\": [\n    \"archival_memory_insert\",\n  \"core_memory_replace\",\n  \"send_message_to_agent_and_wait_for_reply\",\n  \"send_message\",\n  \"conversation_search\",\n  \"send_message_to_agents_matching_all_tags\",\n  \"send_message_to_agent_async\",\n  \"archival_memory_search\",\n  \"core_memory_append\"\n  ],\n  \"tool_ids\": [\n      \"tool-02a94396-8fa0-4424-a363-31ff8c84d2d1\",\n  \"tool-20358139-c108-410d-9bf4-7ceb6e9488ee\",\n  \"tool-37bc9c28-aa86-4b43-bbf8-a7ea720a230b\",\n  \"tool-741d523b-148a-4c27-be77-ba7857c86f18\",\n  \"tool-83ecd599-43e3-45d7-93c9-bb4829c562f4\",\n  \"tool-a0fbd13c-a062-4be7-83e9-1f85b03ba683\",\n  \"tool-b76b1f0a-d80d-4b7a-bb6e-10d52d98458f\",\n  \"tool-e1a41abb-e685-4bc9-830c-ff9330f16726\",\n  \"tool-fa5573ce-a4ec-4b33-8643-dbb5ea53d68d\"\n  ],\n  \"source_ids\": [\n    \"source-84d4729b-0b3c-4fec-af8f-64f479f2b7ac\"\n\t\"source-jjjjjjson\"\n  ],\n  \"block_ids\": [\n\t\"memory_update\",\n\t\"conversation_flow_1\"\n  ],\n  \"tool_rules\": [\n    {\n    \"tool_name\": \"archival_memory_insert\",\n    \"type\": \"constrain_child_tools\",\n    \"children\": [\"archival_memory_search\"]\n  },\n  {\n    \"tool_name\": \"conversation_search\",\n    \"type\": \"run_first\"\n  },\n  {\n    \"tool_name\": \"send_message\",\n    \"type\": \"exit_loop\"\n  },\n  {\n    \"tool_name\": \"archival_memory_search\",\n    \"type\": \"conditional\",\n    \"default_child\": \"send_message\",\n    \"child_output_mapping\": {\n      \"found\": \"send_message\",\n      \"not_found\": \"archival_memory_insert\"\n    },\n    \"require_output_mapping\": true\n  },\n  {\n    \"tool_name\": \"send_message_to_agent_async\",\n    \"type\": \"continue_loop\"\n  }\n  ],\n  \"tags\": [\n    \"daily_conversation\", \"chat_history\", \"personal_dialogue\"\n  ],\n  \"system\": \"You are a friendly and helpful assistant, designed to engage in casual daily conversations. Keep responses natural and engaging.\",\t\n  \"agent_type\": \"memgpt_agent\",\n  \"llm_config\": {\n    \"model\": \"deepseek-r1:7b\",\n    \"model_endpoint_type\": \"ollama\",\n    \"model_endpoint\": \"http://127.0.0.1:11500\",\n    \"model_wrapper\": \"ollama\",\n    \"context_window\": 4096,\n    \"put_inner_thoughts_in_kwargs\": true,\n    \"handle\": \"deepseek_handler\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 4096\n  },\n  \"embedding_config\": {\n    \"embedding_endpoint_type\": \"ollama\",\n    \"embedding_endpoint\": \"http://127.0.0.1:11500\",\n    \"embedding_model\": \"all-minilm:latest\",\n    \"embedding_dim\": 768,\n    \"embedding_chunk_size\": 300,\n    \"handle\": \"embedding_handler\"\n  },\n  \"initial_message_sequence\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"we are friends\",\n      \"name\": \"kyire\"\n    }\n  ],\n  \"include_base_tools\": true,\n  \"include_multi_agent_tools\": false,\n  \"include_base_tool_rules\": true,\n  \"description\": \"we are friends\",\n  \"metadata\": {  \"version\": \"1.0\",\n\t  \"author\": \"your name\",\n\t  \"description\": null,\n\t  \"created_at\": \"2025-03-13T20:20:20\",\n\t  \"tags\": [\"chat\", \"learn\"]\n\t  },\n  \"model\": \"deepseek-r1:7b\",\n  \"embedding\": \"all-minilm:latest\",\n  \"context_window_limit\": 4096,\n  \"embedding_chunk_size\": 300,\n  \"from_template\": null,\n  \"template\": false,\n  \"tool_exec_environment_variables\": {},\n  \"memory_variables\": {\n    \"favorite_color\": \"blue\",\n  },\n  \"project_id\": null,\n  \"template_id\": null,\n  \"base_template_id\": null,\n  \"identity_ids\": [],\n  \"message_buffer_autoclear\": false,\n  \"actor_id\": null\n}\n\nThis is what I have currently, but when I execute it I get a 422 error.\nIs there something I did wrong? Please help me solve the problem, I will be very grateful",
      "state": "closed",
      "author": "ASAmbitious",
      "author_type": "User",
      "created_at": "2025-03-14T03:17:20Z",
      "updated_at": "2025-04-28T02:13:40Z",
      "closed_at": "2025-04-28T02:13:39Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2484/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2484",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2484",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:58.909357",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-14T02:12:17Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-28T02:13:39Z"
        }
      ]
    },
    {
      "issue_number": 2368,
      "title": "FEATURE REQUEST: Support for Voyage Embedding Models",
      "body": "hello! would like to request for support for voyage embedding models\n\nhttps://docs.voyageai.com/docs/embeddings\n\nREST API Format:\n\ncurl https://api.voyageai.com/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $VOYAGE_API_KEY\" \\\n  -d '{\n    \"input\": \"Sample text\",\n    \"model\": \"voyage-3\",\n    \"input_type\": \"document\"\n  }'",
      "state": "closed",
      "author": "cloudrumbles",
      "author_type": "User",
      "created_at": "2025-01-21T14:11:47Z",
      "updated_at": "2025-04-25T02:11:41Z",
      "closed_at": "2025-04-25T02:11:40Z",
      "labels": [
        "feature request",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2368/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cpacker"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2368",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2368",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:59.141151",
      "comments": [
        {
          "author": "cpacker",
          "body": "Thank you for this feature request! Will keep you posted with any updates. If you have any additional details you can share on your usecase + experience using Voyage embeddings, that would help us understand the feature request better and prioritize it internally (you are of course also welcome to o",
          "created_at": "2025-01-24T23:41:15Z"
        },
        {
          "author": "cloudrumbles",
          "body": "hello! i went ahead and made a PR :) \n\nsee feat: add support for voyage embedding #2387 \n\n",
          "created_at": "2025-01-25T08:44:21Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-25T02:04:23Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-11T02:06:21Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-11T02:09:34Z"
        }
      ]
    },
    {
      "issue_number": 2455,
      "title": "Unclear how to set openai base url in Python",
      "body": "I think either the documentation needs to be tweaked or there's an underlying bug.\n\nIn https://docs.letta.com/guides/server/providers/openai#enabling-openai-models it says:\n\n`export OPENAI_API_KEY=...`\n\nBut it does not cover how to configure the `OPENAI_API_BASE` environment variable, or even if one exists -- it's not clear from https://github.com/letta-ai/letta/blob/a1a2dd44f57ff868d46e7e4bc517e4d299185771/letta/settings.py#L61 as it's hardcoded and there's no comment on where it comes from or if it can be overridden.\n\nMy use case is that I want to use https://docs.lambdalabs.com/public-cloud/lambda-inference-api/ which has a different API base:\n\n```python\nopenai_api_key = \"<API-KEY>\"\nopenai_api_base = \"https://api.lambdalabs.com/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n```",
      "state": "closed",
      "author": "wsargent",
      "author_type": "User",
      "created_at": "2025-02-23T07:28:30Z",
      "updated_at": "2025-04-25T02:11:39Z",
      "closed_at": "2025-04-25T02:11:38Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2455/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2455",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2455",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:59.347612",
      "comments": [
        {
          "author": "MeowLynxSea",
          "body": "save problem :(",
          "created_at": "2025-02-26T08:30:06Z"
        },
        {
          "author": "wsargent",
          "body": "Adding a bit more context: while there is https://docs.letta.com/guides/server/providers/openai-proxy which details `OPENAI_API_BASE`, the documentation does not show how to set this from Python.",
          "created_at": "2025-03-01T17:21:08Z"
        },
        {
          "author": "shanekong",
          "body": "Same problem, I change the variable in letta/letta/settings.py  line 63 directly. it works.  openai_api_base: str =xxx",
          "created_at": "2025-03-11T06:35:41Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-11T02:09:32Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-25T02:11:38Z"
        }
      ]
    },
    {
      "issue_number": 2463,
      "title": "Using Anthropic Claude, `assistant_messages` are not streamed, making the streaming endpoint unusable",
      "body": "**Describe the bug**\n\nUsing Anthropic Claude 3.7 Sonnet, assistant_messages are not streamed when using streaming tokens.\n``` \nmessage    {\"id\":\"message-68a02754-a26e-4ac2-b76d-83a0b20f15e9\",\"date\":\"2025-03-02T18:25:12+00:00\",\"message_type\":\"reasoning_message\",\"reasoning\":\"\"}    \n10:25:12.820\nmessage    {\"id\":\"message-68a02754-a26e-4ac2-b76d-83a0b20f15e9\",\"date\":\"2025-03-02T18:25:12+00:00\",\"message_type\":\"reasoning_message\",\"reasoning\":\"\\nUser\"}    \n10:25:12.820\nmessage    {\"id\":\"message-68a02754-a26e-4ac2-b76d-83a0b20f15e9\",\"date\":\"2025-03-02T18:25:12+00:00\",\"message_type\":\"reasoning_message\",\"reasoning\":\" wants a starting draft to work\"}    \n10:25:14.731\nmessage    {\"id\":\"message-68a02754-a26e-4ac2-b76d-83a0b20f15e9\",\"date\":\"2025-03-02T18:25:12+00:00\",\"message_type\":\"reasoning_message\",\"reasoning\":\" with. I'll create a comprehensive\"}    \n10:25:14.837\nmessage    {\"id\":\"message-68a02754-a26e-4ac2-b76d-83a0b20f15e9\",\"date\":\"2025-03-02T18:25:12+00:00\",\"message_type\":\"reasoning_message\",\"reasoning\":\" CS teacher persona with patience as a key\"}    \n10:25:14.941\nmessage    {\"id\":\"message-68a02754-a26e-4ac2-b76d-83a0b20f15e9\",\"date\":\"2025-03-02T18:25:12+00:00\",\"message_type\":\"reasoning_message\",\"reasoning\":\" trait.\\n\"}    \n10:25:15.156\nmessage    {\"id\":\"message-6507791c-9212-420d-ae46-8be227674b30\",\"date\":\"2025-03-02T18:25:30+00:00\",\"message_type\":\"tool_return_message\",\"tool_return\":\"None\",\"status\":\"success\",\"tool_call_id\":\"toolu_01C5Ru53pGsy8zKmUCXNs6TC\",\"stdout\":null,\"stderr\":null}    \n10:25:30.817\nmessage    {\"message_type\":\"usage_statistics\",\"completion_tokens\":391,\"prompt_tokens\":3460,\"total_tokens\":3851,\"step_count\":1}    \n10:25:30.960\nmessage    [DONE]\n```\n\nBut the message itself is created because once the streaming is over, I can see that assistant_message exists in `messages` endpoint. It's just not streamed real-time.\n\n**Please describe your setup**\n- [ ] How did you install letta?\nTypescript SDK\n- [ ] Describe your setup\nMacOS (M3 MAX Sequoia), letta server running in docker using the official image",
      "state": "closed",
      "author": "seunggs",
      "author_type": "User",
      "created_at": "2025-03-02T20:38:28Z",
      "updated_at": "2025-04-18T02:08:29Z",
      "closed_at": "2025-04-18T02:08:29Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2463/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2463",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2463",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:35:59.761398",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-04-04T02:08:24Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-18T02:08:28Z"
        }
      ]
    },
    {
      "issue_number": 1,
      "title": "Improve gpt-3.5-turbo performance",
      "body": "WIP\r\n\r\nCurrent gpt-3.5-turbo performance is not very good due primarily due to bad understanding of function set (functions called at the wrong times, messages sent w/o call to `send_message`, etc.). Goal is to upload specific variations of the key prompts (tailored towards worse instruction following ability of 3.5) that provide reasonable performance.",
      "state": "closed",
      "author": "cpacker",
      "author_type": "User",
      "created_at": "2023-10-15T22:05:28Z",
      "updated_at": "2025-04-14T13:36:59Z",
      "closed_at": "2023-10-20T21:34:42Z",
      "labels": [
        "roadmap",
        "feature request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/1/reactions",
        "total_count": 3,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 3,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cpacker"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/1",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/1",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:00.057545",
      "comments": [
        {
          "author": "tomsib2001",
          "body": "I'm not sure that this is possible for chatgpt-3.5 (or any version of chatGPT), but for open LLMs, there is this technique called grammar-based sampling which basically forces the LLM to answer according to some predetermined grammar, here is a Hacker News thread and link to a ggml PR describing it:",
          "created_at": "2023-10-17T10:07:09Z"
        }
      ]
    },
    {
      "issue_number": 2405,
      "title": "Attaching Composio tool (i.e filetool_write) causes non stop warning message which ends up filling the context window and floods server log",
      "body": "**Describe the bug**\nWhen attaching a Composio tool like filetool_write, there is a warning that gets dumped from https://github.com/letta-ai/letta/blob/main/letta/local_llm/utils.py#L130 and there is additional messages which i think are coming from the LLM backend being used when tool calling, look at the 1st screenshot for the nonstop messages `Please provide a value of type string`.\n\n```\n/app/letta/local_llm/utils.py:130: UserWarning: num_tokens_from_functions: Unsupported field default in function {'name': 'filetool_write', 'description': 'Write The Given Content To A File.\\n Note:     This Action Will Replace The Existing Content In The The File, And Completely Rewrite The File, If You Want To Edit A Specific Portion Of The File Use `Edit` Tool Instead.', 'parameters': {'type': 'object', 'properties': {'file_manager_id': {'type': 'string', 'description': 'ID of the file manager where the file will be opened, if not provided the recent file manager will be used to execute the action. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string. Please provide a value of type string\n```\n\nAbove is just a warning but floods the server logs since it gets printed nonstop and also fills the context window. It looks like the actual message that floods the server logs and fills the context window is coming from somewhere else (not from the open source framework). Below screenshot of my terminal shows the repeated messages.\n\n<img width=\"1952\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6ee0187d-3e0e-4b79-ba3d-17b5e073e3ba\" />\n\n\n**Please describe your setup**\n- [ ] How did you install letta?\n  - I installed with Docker, I followed https://docs.letta.com/guides/server/docker\n- [ ] Describe your setup\n  - MacOS with Apple chip\n  - I am running `letta` via:\n```\ndocker run \\\n  -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data \\\n  -p 8283:8283 \\\n  --env-file .env \\\n  letta/letta:latest\n```\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n<img width=\"980\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d54c64f8-966b-4017-9120-d563051f7bbd\" />\n\n**Additional context**\nAdd any other context about the problem here.\n\n**Letta Config**\nPlease attach your `~/.letta/config` file or copy paste it below.\n```\n[defaults]\npreset = memgpt_chat\npersona = sam_pov\nhuman = basic\n\n[archival_storage]\ntype = postgres\npath = /Users/alazarshenkute/.letta\nuri = postgresql://postgres:@/postgres?host=/Users/alazarshenkute/.letta/data\n\n[recall_storage]\ntype = postgres\npath = /Users/alazarshenkute/.letta\nuri = postgresql://postgres:@/postgres?host=/Users/alazarshenkute/.letta/data\n\n[metadata_storage]\ntype = sqlite\npath = /Users/alazarshenkute/.letta\n\n[version]\nletta_version = 0.6.13\n```\n\n---\n\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\n\n**Local LLM details**\n\nIf you are trying to run Letta with local LLMs, please provide the following information:\n\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\n",
      "state": "closed",
      "author": "a8nova",
      "author_type": "User",
      "created_at": "2025-01-31T08:17:44Z",
      "updated_at": "2025-04-12T02:07:11Z",
      "closed_at": "2025-04-12T02:07:11Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2405/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2405",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2405",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:00.333100",
      "comments": [
        {
          "author": "mattzh72",
          "body": "Digging into this actively!",
          "created_at": "2025-02-01T01:01:55Z"
        },
        {
          "author": "a8nova",
          "body": "thank you @mattzh72! I would be happy to contribute, is there anything i can help with? ",
          "created_at": "2025-02-26T07:35:23Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-03-29T02:07:09Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-12T02:07:11Z"
        }
      ]
    },
    {
      "issue_number": 2330,
      "title": "Tools dont work",
      "body": "**Describe the bug**\r\n\r\nIm trying to create any tool, but it doesnt work. I try this tool from the Deeplearning example:\r\n\r\n```\r\ndef task_queue_push(self: \"Agent\", task_description: str):\r\n    \"\"\"\r\n    Push to a task queue stored in core memory. \r\n\r\n    Args:\r\n        task_description (str): A description of the next task you must accomplish. \r\n\r\n    Returns:\r\n        Optional[str]: None is always returned as this function \r\n        does not produce a response.\r\n    \"\"\"\r\n    import json\r\n    tasks = json.loads(self.memory.get_block(\"tasks\").value)\r\n    tasks.append(task_description)\r\n    self.memory.update_block_value(\"tasks\", json.dumps(tasks))\r\n    return None\r\n```\r\n\r\nWhen agent is working it always return me error like this:\r\n\r\n```\r\nRequest\r\n\r\n{\r\n  \"task_description\": \"Create project directory and necessary files for Calendar app.\",\r\n  \"request_heartbeat\": true\r\n}\r\n\r\nResponse\r\n\r\n{\r\n  \"status\": \"Failed\",\r\n  \"message\": \"Error executing function task_queue_push: TypeError: task_queue_push() missing 1 required positional argument: 'self'\",\r\n  \"time\": \"2025-01-05 03:49:44 PM WET+0000\"\r\n}\r\n```\r\n\r\nWhenever I add any other tool, it always has issues with self:\r\n\r\n```\r\ndef create_file(self: \"Agent\", relative_path: str, initial_content: str = \"\") -> str:\r\n    \"\"\"\r\n    Creates a new file at '/Users/Shared/Projects/<project_name>/<relative_path>'\r\n    and optionally writes initial content.\r\n    If the file already exists, it will be overwritten.\r\n\r\n    Args:\r\n        relative_path (str): The path (relative to the project directory).\r\n        initial_content (str): Optional string content to write into the newly created file.\r\n\r\n    Returns:\r\n        str: A success or error message.\r\n    \"\"\"\r\n    import os\r\n\r\n    # Retrieve the project name from memory (adapt if your memory system differs).\r\n    project_name = self.memory.get_block(\"project_name\").value\r\n    base_dir = \"/Users/Shared/Projects\"\r\n    full_path = os.path.join(base_dir, project_name, relative_path)\r\n\r\n    try:\r\n        os.makedirs(os.path.dirname(full_path), exist_ok=True)\r\n        with open(full_path, \"w\", encoding=\"utf-8\") as f:\r\n            f.write(initial_content)\r\n        return f\"File created at '{full_path}' with initial content.\"\r\n    except Exception as e:\r\n        return f\"Failed to create file '{full_path}': {str(e)}\"\r\n```\r\n```\r\n\r\nRequest\r\n\r\n{\r\n  \"relative_path\": \"Main.storyboard\",\r\n  \"initial_content\": \"<storyboard version=\\\"3.0\\\"></storyboard>\",\r\n  \"request_heartbeat\": true\r\n}\r\n\r\nResponse\r\n\r\n{\r\n  \"status\": \"Failed\",\r\n  \"message\": \"Error executing function create_file: TypeError: create_file() missing 1 required positional argument: 'self'\",\r\n  \"time\": \"2025-01-05 03:49:49 PM WET+0000\"\r\n}\r\n```\r\n\r\n**Please describe your setup**\r\n- [ ] How did you install letta?\r\n  - `pip install letta`\r\n  - \r\n- [ ] Describe your setup\r\n  - What's your OS (Windows/MacOS/Linux)?\r\n  MacOS\r\n\r\n  - How are you running `letta`? (`cmd.exe`/Powershell/Anaconda Shell/Terminal)\r\nTerminal\r\n\r\n**Letta Config**\r\n[defaults]\r\npreset = memgpt_chat\r\npersona = sam_pov\r\nhuman = basic\r\n\r\n[archival_storage]\r\ntype = sqlite\r\npath = /Users/user/.letta\r\n\r\n[recall_storage]\r\ntype = sqlite\r\npath = /Users/user/.letta\r\n\r\n[metadata_storage]\r\ntype = sqlite\r\npath = /Users/user/.letta\r\n\r\n[version]\r\nletta_version = 0.6.7\r\n",
      "state": "closed",
      "author": "quantumcthulhu",
      "author_type": "User",
      "created_at": "2025-01-05T16:54:53Z",
      "updated_at": "2025-04-04T02:08:29Z",
      "closed_at": "2025-04-04T02:08:28Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2330/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2330",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2330",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:00.578991",
      "comments": [
        {
          "author": "lemorage",
          "body": "Sorry, lots of things changed. I think now for ver0.6.7, you don't need to include the param `self: \"Agent\"` anymore when defining the tool.",
          "created_at": "2025-01-10T05:44:48Z"
        },
        {
          "author": "quantumcthulhu",
          "body": "> Sorry, lots of things changed. I think now for ver0.6.7, you don't need to include the param `self: \"Agent\"` anymore when defining the tool.\r\n\r\nCan you please provide right format how to define tool",
          "created_at": "2025-01-11T18:30:37Z"
        },
        {
          "author": "sarahwooders",
          "body": "You should be able to do this on the lates 0.6.9 version: \r\n```python\r\nfrom letta import create_client\r\n\r\ndef create_file(relative_path: str, initial_content: str = \"\") -> str:\r\n    \"\"\"\r\n    Creates a new file at '/Users/Shared/Projects/<project_name>/<relative_path>'\r\n    and optionally writes init",
          "created_at": "2025-01-11T23:03:53Z"
        },
        {
          "author": "RadeenXALNW",
          "body": "Mine is showing name is not defined when I am executing my code as well as the above code also is showing same error.\n```\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/letta/client/client.py\", line 2999, in create_tool\n    name=name,\nNameError: name 'name' is not defined",
          "created_at": "2025-01-31T17:43:31Z"
        },
        {
          "author": "alaliaa",
          "body": "> Mine is showing name is not defined when I am executing my code as well as the above code also is showing same error.\n> \n> ```\n>   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/letta/client/client.py\", line 2999, in create_tool\n>     name=name,\n> NameError: name 'name' i",
          "created_at": "2025-02-17T08:42:58Z"
        }
      ]
    },
    {
      "issue_number": 2435,
      "title": "Tools access to environment variables",
      "body": "**Is your feature request related to a problem? Please describe.**\nWhen using tools, it would be useful to be able to access env vars. I have seen there is a way to pass this agents via the API, but I don't see why the tool scripts themselves can't access env vars directly to simplify this process.\n\n**Describe the solution you'd like**\nSimilar to how static site generators work like Astro or Gastby, being able to have specific env vars that start with \"TOOL_\" (or similar) accessible to the tool scripts will make integration with other systems much easier to manage.\n\n**Describe alternatives you've considered**\nI have looked at tool_exec_environment_variables, but this doesn't seem to be ideal when you potentially have multiple agents using the same key that needs to be replaced. \n",
      "state": "closed",
      "author": "asc2030",
      "author_type": "User",
      "created_at": "2025-02-14T08:34:48Z",
      "updated_at": "2025-04-04T02:08:27Z",
      "closed_at": "2025-04-04T02:08:26Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2435/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2435",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2435",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:00.773586",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Hi @asc2030 - we are working on adding improved configurability of the tool execution environment. However I believe you should be able to set env vars when you run the server, e.g. \n```\nexport MY_ENV_VAR=.... \nletta server\n```",
          "created_at": "2025-02-18T15:38:06Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-03-21T02:08:05Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-04-04T02:08:26Z"
        }
      ]
    },
    {
      "issue_number": 2434,
      "title": "I want to implement the LLM large model using openAI, and the embedding model using ollama privatization deployment, what should I do?",
      "body": "I want to implement the LLM large model using openAI, and the embedding model using ollama privatization deployment, what should I do?\n",
      "state": "closed",
      "author": "yunexuan",
      "author_type": "User",
      "created_at": "2025-02-14T02:15:38Z",
      "updated_at": "2025-03-31T02:12:04Z",
      "closed_at": "2025-03-31T02:12:04Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2434/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2434",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2434",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:00.975239",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-03-17T02:08:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-31T02:12:04Z"
        }
      ]
    },
    {
      "issue_number": 2424,
      "title": "After summarization the send_message is not called, hence including thinking in content reply",
      "body": "**Describe the bug**\nAs the title suggests, after upgrading from version 0.6.7 to 0.6.22, when summarization is triggered all the subsequent user message sent, the send_message is not called, hence including thinking in content reply\n\nthis is the ai reply (sorry for replacing text but they included sensitive data, the logic is the same btw)\n\nBetaMessage(id='msg_01AS6DKR5XWQqWqJBS6gYfy1', content=[BetaTextBlock(text=\"<thinking>here is the thinking</thinking>\\n\\nhere the reply\", type='text')], model='claude-3-5-sonnet-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=BetaUsage(cache_creation_input_tokens=10617, cache_read_input_tokens=0, input_tokens=2, output_tokens=253))\n\nand this the api response\n\n{\n    \"messages\": [\n        {\n            \"id\": \"message-23e7fc7e-333c-4b46-90fc-0805516a056e\",\n            \"date\": \"2025-02-09T13:36:13+00:00\",\n            \"message_type\": \"reasoning_message\",\n            \"reasoning\": \"here is the thinking.\\n\\nhere the reply\"\n        }\n    ],\n    \"usage\": {\n        \"message_type\": \"usage_statistics\",\n        \"completion_tokens\": 253,\n        \"prompt_tokens\": 10619,\n        \"total_tokens\": 10872,\n        \"step_count\": 1\n    }\n}\n\n\nwhile prior to summarization there was as always thinking separated from the send_message function\n\n**Please describe your setup**\nLetta 0.6.22\nAnthropic 3.5 sonnet latest",
      "state": "closed",
      "author": "Sapessii",
      "author_type": "User",
      "created_at": "2025-02-09T14:35:46Z",
      "updated_at": "2025-03-27T02:07:55Z",
      "closed_at": "2025-03-27T02:07:55Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2424/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "mattzh72"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2424",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2424",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:03.025724",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-03-13T02:07:03Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-27T02:07:54Z"
        }
      ]
    },
    {
      "issue_number": 1565,
      "title": "Add `memgpt benchmark` results to README",
      "body": "This would be useful for model comparisons, with a lot of new models coming out. ",
      "state": "closed",
      "author": "sarahwooders",
      "author_type": "User",
      "created_at": "2024-07-23T22:30:29Z",
      "updated_at": "2025-03-20T07:26:51Z",
      "closed_at": "2024-12-06T02:29:21Z",
      "labels": [
        "good first issue",
        "auto-closed"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/1565/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/1565",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/1565",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:03.244498",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically closed due to 60 days of inactivity.",
          "created_at": "2024-12-06T02:29:21Z"
        },
        {
          "author": "stocyr",
          "body": "This has been moved to TODO, however, it's still not solved, right?",
          "created_at": "2025-03-20T07:26:50Z"
        }
      ]
    },
    {
      "issue_number": 2409,
      "title": "Use agent_state = client.agents.create()  return:  letta_client.core.api_error.ApiError: status_code: 502, body:",
      "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**Please describe your setup**\n- [ ] How did you install letta?\n  - `pip install letta-client`\n- [ ] Describe your setup\n  - window docker run letta server\n  - vscode python code\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/ee95e752-5006-4768-b05d-64b601ff58e9)\n\ncode:\n```python\n# install letta_client with `pip install letta-client`\nfrom letta_client import Letta\n\n# create a client to connect to your local Letta Server\nclient = Letta(\n  base_url=\"http://localhost:8283\"\n)\n\n# create an agent with two basic self-editing memory blocks\nagent_state = client.agents.create(\n    memory_blocks=[\n        {\n          \"label\": \"human\",\n          \"value\": \"The human's name is Bob the Builder.\"\n        },\n        {\n          \"label\": \"persona\",\n          \"value\": \"My name is Sam, the all-knowing sentient AI.\"\n        }\n    ],\n    model=\"openai/gpt-4o-mini\",\n    context_window_limit=16000,\n    embedding=\"openai/text-embedding-3-small\"\n)\n\n# the AgentState object contains all the information about the agent\nprint(agent_state)\n```\nI use the request and curl, it works. It seem like something wrong with httpx.client:\n\n![Image](https://github.com/user-attachments/assets/7ae58b2d-a17c-4a67-8148-2f418836b674)\n\nit returns 502\n\n**Additional context**\nI use the Requests,it works\n\n![Image](https://github.com/user-attachments/assets/5ec13aa5-cf2c-40c9-a637-3b986aceeecd)\n\nI use the Postman, it works:\n\n![Image](https://github.com/user-attachments/assets/7e7fb68c-c227-4449-bd19-9693d135042e)\n",
      "state": "closed",
      "author": "nj-guiqi",
      "author_type": "User",
      "created_at": "2025-02-03T07:36:31Z",
      "updated_at": "2025-03-20T02:06:37Z",
      "closed_at": "2025-03-20T02:06:37Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2409/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2409",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2409",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:03.474468",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-03-06T02:05:47Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-20T02:06:36Z"
        }
      ]
    },
    {
      "issue_number": 2401,
      "title": "Can't create and use agent normaly.",
      "body": "**Describe the bug**\nCan't create and use agent normaly.\n\n**Please describe your setup**\ngit clone\nthen letta server\n- [ ] Describe your setup\n  - Mac\n  - Terminal\n\n**Screenshots**\nAfter creating agent and try to use it, the normal ade screen flashes, and then following screen:\n<img width=\"1297\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2a6d84c8-a2b0-4633-9a58-b2c0b7ef3794\" />\n\n**Additional context**\nseemed some requests also not normal?\nINFO:     ::1:53055 - \"GET /v1/agents/agent-79478aae-241e-452b-b2c1-54e86b926ad7/archival-memory HTTP/1.1\" 404 Not Found\nINFO:     ::1:53040 - \"GET /v1/agents/agent-79478aae-241e-452b-b2c1-54e86b926ad7 HTTP/1.1\" 200 OK\nINFO:     ::1:53041 - \"GET /v1/tools/composio/apps HTTP/1.1\" 400 Bad Request\nINFO:     ::1:53053 - \"GET /v1/agents/agent-79478aae-241e-452b-b2c1-54e86b926ad7/sources HTTP/1.1\" 200 OK\nINFO:     ::1:53054 - \"GET /v1/agents/agent-79478aae-241e-452b-b2c1-54e86b926ad7/context HTTP/1.1\" 200 OK\n\n**Letta Config**\n[defaults]\npreset = memgpt_chat\npersona = sam_pov\nhuman = basic\n\n[archival_storage]\ntype = postgres\npath = /Users/femtozheng/.letta\nuri = postgresql://letta:letta@localhost:5432/letta\n\n[recall_storage]\ntype = postgres\npath = /Users/femtozheng/.letta\nuri = postgresql://letta:letta@localhost:5432/letta\n\n[metadata_storage]\ntype = sqlite\npath = /Users/femtozheng/.letta\n\n[version]\nletta_version = 0.6.6\n\n---\n\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\n\n**Local LLM details**\n\nIf you are trying to run Letta with local LLMs, please provide the following information:\n\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\n",
      "state": "closed",
      "author": "femto",
      "author_type": "User",
      "created_at": "2025-01-30T04:53:44Z",
      "updated_at": "2025-03-16T02:10:38Z",
      "closed_at": "2025-03-16T02:10:37Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2401/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2401",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2401",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:03.691584",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Could you please upgrade to the latest version? For the web ADE, you need to have the latest server version or else there will be API mismatches. \n\nTo avoid having to upgrade regularly, you can also use [Letta desktop](https://docs.letta.com/install) if you're on Mac. ",
          "created_at": "2025-01-30T05:00:17Z"
        },
        {
          "author": "femto",
          "body": "will try [Letta desktop](https://docs.letta.com/install), but I want to learn letta code,now git pull,\nwith LETTA_PG_URI=postgresql://letta:letta@localhost:5432/letta\nalready alembic upgrade head\n```\nTraceback (most recent call last):\n  File \"/Users/femtozheng/python-project/letta/.venv/lib/python3.",
          "created_at": "2025-01-30T05:22:03Z"
        },
        {
          "author": "sarahwooders",
          "body": "If you want to build from source, I recommend doing: \n```\ndocker build . -t letta/letta:local\n\ndocker run \\                                                                                                                                                        \n -v ~/.letta/.persist/pgdata:/var/lib/",
          "created_at": "2025-01-30T16:02:21Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-03-02T02:08:00Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-16T02:10:36Z"
        }
      ]
    },
    {
      "issue_number": 1670,
      "title": "OpenRouter API support",
      "body": "Allow using OpenRouter models via the openai compatible API, allows access to Anthropic, Openai, Meta and other providers of open source and proprietary models through a single source, with a single point of payment and a single API key.\r\n\r\nThe api endpoint: https://openrouter.ai/api/v1 \r\nA json formatted list of models can be fetched via API if usefull: https://openrouter.ai/api/v1/models\r\n\r\nFull Docs: https://openrouter.ai/docs/\r\n\r\nI had a look at doing this myself, but without more familiarity with memgpt I cant figure it out all the places needed to be updated. It seems that endpoint providers and model config is widely spread throughout everything.  \r\n",
      "state": "closed",
      "author": "strueman",
      "author_type": "User",
      "created_at": "2024-08-19T05:01:36Z",
      "updated_at": "2025-03-14T17:01:41Z",
      "closed_at": "2024-12-06T02:23:33Z",
      "labels": [
        "auto-closed"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/1670/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/1670",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/1670",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:03.924689",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically closed due to 60 days of inactivity.",
          "created_at": "2024-12-06T02:23:32Z"
        },
        {
          "author": "hilmishah",
          "body": "Can this be revisited again? I prefer to use openrouter as it provides more models including free",
          "created_at": "2025-03-11T03:21:48Z"
        },
        {
          "author": "quantumalchemy",
          "body": "yeah! would be great to use openrouter.ai  with model_endpoint_type=\"openai\"\nIn theory should work with  as it uses openai endpoint -->\n---\nagent = client.agents.create(\nname=\"thedude\", \n#memory_blocks=[],\nllm_config=LlmConfig(\n\tmodel=\"qwen/qwq-32b:free\",\n\tmodel_endpoint_type=\"openai\",\n\tmodel_endpoi",
          "created_at": "2025-03-14T17:01:40Z"
        }
      ]
    },
    {
      "issue_number": 2389,
      "title": "client.agents.list() ",
      "body": "\n1 ADE  \n2localhost:8283 letta server\n\nExcuse me, why does it report an error here?\n1. I can use ADE normally.\n2. The local Lettuce server at localhost:8283 is also running normally. \n\n<img width=\"828\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9bc36b86-e6e4-4d45-b988-5a7fb37b28c8\" />",
      "state": "closed",
      "author": "njj10",
      "author_type": "User",
      "created_at": "2025-01-26T09:07:02Z",
      "updated_at": "2025-03-12T02:05:35Z",
      "closed_at": "2025-03-12T02:05:34Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2389/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2389",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2389",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:04.124206",
      "comments": [
        {
          "author": "njj10",
          "body": "According to the instructions in https://docs.letta.com/quickstart, when creating an agent using the Python API, the same error will also be reported. The error is as follows:\n\nhttps://docs.letta.com/quickstart \n  Python API  agent \n \n<img width=\"838\" alt=\"Image\" src=\"https://git",
          "created_at": "2025-01-26T09:47:09Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-26T02:04:13Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-12T02:05:34Z"
        }
      ]
    },
    {
      "issue_number": 2390,
      "title": "An unknown error occurred signing you in. Please contact support.",
      "body": "![Image](https://github.com/user-attachments/assets/4582e51d-3105-493d-b3c7-65e4fe527f12)",
      "state": "closed",
      "author": "njsgcs",
      "author_type": "User",
      "created_at": "2025-01-26T15:36:45Z",
      "updated_at": "2025-03-12T02:05:34Z",
      "closed_at": "2025-03-12T02:05:33Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2390/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2390",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2390",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:04.292268",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-26T02:04:11Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-12T02:05:33Z"
        }
      ]
    },
    {
      "issue_number": 2329,
      "title": "Inference doesnt work (both letta-free and openai)",
      "body": "**Describe the bug**\r\nwhen I try to use letta-free model it returns error: \r\n\"requests.exceptions.HTTPError: HTTP error occurred: 500 Server Error: Internal Server Error for url: https://inference.memgpt.ai/chat/completions | Status code: 500, Message: {\"detail\":\"Internal server error (unpack): \"}\r\n\"\r\n\r\nWhen I try to use gpt4o model it returns error:\r\n\r\nhttpx_sse._exceptions.SSEError: Expected response header Content-Type to contain 'text/event-stream', got 'application/json'\r\n\r\n**Please describe your setup**\r\n- [ ] How did you install letta?\r\n  - `pip install letta`\r\n- [ ] Describe your setup\r\n  - What's your OS (Windows/MacOS/Linux)?\r\n  MacOS\r\n\r\n  - How are you running `letta`? (`cmd.exe`/Powershell/Anaconda Shell/Terminal)\r\n  Terminal\r\n\r\n**Letta Config**\r\nPlease attach your `~/.letta/config` file or copy paste it below.\r\n\r\n[defaults]\r\npreset = memgpt_chat\r\npersona = sam_pov\r\nhuman = basic\r\n\r\n[archival_storage]\r\ntype = sqlite\r\npath = /Users/user/.letta\r\n\r\n[recall_storage]\r\ntype = sqlite\r\npath = /Users/user/.letta\r\n\r\n[metadata_storage]\r\ntype = sqlite\r\npath = /Users/user/.letta\r\n\r\n[version]\r\nletta_version = 0.6.7\r\n",
      "state": "closed",
      "author": "quantumcthulhu",
      "author_type": "User",
      "created_at": "2025-01-05T16:48:31Z",
      "updated_at": "2025-03-11T02:06:23Z",
      "closed_at": "2025-03-11T02:06:23Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2329/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2329",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2329",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:04.487880",
      "comments": [
        {
          "author": "pgiki",
          "body": "I have been also facing this issue for a couple of days now. After debugging I realized that this happens when one of the messages sent on the `make_post_request` doesn't have any tool_calls. Here is the patch way to solve it for now. I hope someone has a better solution but atleast this solves the ",
          "created_at": "2025-01-18T21:49:19Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-25T02:04:26Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-11T02:06:22Z"
        }
      ]
    },
    {
      "issue_number": 2376,
      "title": "v 0.1.16 SDK  ImportError: cannot import name 'Letta' from 'letta'  But installed",
      "body": "\nInstalled with docker 0.1.16\\new sdk not working..\ninstalled But >>\n\n>>  pip show letta-client\nName: letta-client\nVersion: 0.1.16\nSummary: \nHome-page: \nAuthor: \nAuthor-email: \nLicense: \nLocation: /app/.venv/lib/python3.11/site-packages\nRequires: httpx, httpx-sse, pydantic, pydantic-core, typing_extensions\nRequired-by: letta\nroot@4426541a1900:~# python\nPython 3.11.2 (main, Nov 30 2024, 21:22:50) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from letta import Letta\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: cannot import name 'Letta' from 'letta' (/app/letta/__init__.py)\n",
      "state": "closed",
      "author": "quantumalchemy",
      "author_type": "User",
      "created_at": "2025-01-22T19:20:33Z",
      "updated_at": "2025-03-11T02:06:20Z",
      "closed_at": "2025-03-11T02:06:20Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2376/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2376",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2376",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:04.709190",
      "comments": [
        {
          "author": "cpacker",
          "body": "can you try `from letta_client import Letta`?\n\n<img width=\"736\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a8456527-64b5-46fb-94b0-caf807a0757c\" />\n\n^from https://docs.letta.com/quickstart",
          "created_at": "2025-01-23T20:02:23Z"
        },
        {
          "author": "quantumalchemy",
          "body": "Only works with from letta import create_client \n\n",
          "created_at": "2025-01-24T16:50:05Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-24T02:05:06Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-11T02:06:19Z"
        }
      ]
    },
    {
      "issue_number": 2386,
      "title": "[Solved] Cannot be used with vLLM, connection error \"400\"",
      "body": "When you try to open a Letta services locally and according to the official documentation https://docs.letta.com/guides/server/providers/vllm steps to connect to the  vLLM as a backend, you may meet problem, cannot use and an error prompt \"400\".   Here's an unofficial solution:  \n\n\n* Local environment for testing:\n    + Letta deployed in docker\n    + Qwen-Coder-3B deployed in the vLLM service\n    + Windows 11 24H2 with WSL2 Ubuntu\n\n* Solution steps:\n    1. If your Letta is running in Docker, press Ctrl+C to stop the logs. Then, use the command `docker start [your-container]` to start this container, and enter the container's terminal by typing `docker exec -it [your-container] bash`.\n    2. Open the file `letta/llm_api/llm_api_tools.py`, locate the following code segment (around line 146-147):\n    ```\n    else:\n        functional_call = \"required\"\n    ```\n    Change it to:\n    ```\n    elif model_settings.vllm_api_base != None:\n        function_call = \"none\"\n        model_settings.openai_api_key='sk-placeholder'\n    else:\n        function_call = \"required\"\n    ```\n    3. Restart your container (or just restart the service), using `docker restart [your-container]`.\n    4. The issue should now be resolved.\n\nOr, you can directly use the modified version provided in the attachment to replace `letta/llm_api/llm_api_tools.py`. \n[llm_api_tools.py](https://github.com/user-attachments/files/18533409/llm_api_tools.txt)\n\nSince I do not have the local deployment of the required development environment, I have not submitted a Pull Request, but I hope this can help you.",
      "state": "closed",
      "author": "appleappstore102210",
      "author_type": "User",
      "created_at": "2025-01-24T09:36:21Z",
      "updated_at": "2025-03-11T02:06:19Z",
      "closed_at": "2025-03-11T02:06:18Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2386/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2386",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2386",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:04.921909",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-24T02:05:05Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-11T02:06:18Z"
        }
      ]
    },
    {
      "issue_number": 2388,
      "title": "Letta ignores the \"OLLAMA_BASE_URL\" env variable, and try to use OpenAI instead",
      "body": "**Describe the bug**\nLetta ignores the  `OLLAMA_BASE_URL` environment variable, and try to use OpenAI.\nThe only model available in the new ADE is \"letta-free\", despite the fact that I use `-e OLLAMA_BASE_URL=\"http://host.docker.internal:11434\"` when running Letta with docker.\n\nIs it possible to run Letta with Ollama only? Without an OpenAI API key?\n\n**Please describe your setup**\n- [ ] Describe your setup\n  - OS => Ubuntu 24.04.1\n- [ ] How did you install letta?\n  - I've followed the documentation [here](https://docs.letta.com/guides/server/providers/ollama#enabling-ollama-as-a-provider) and I've runned \n ```\nsudo docker run \\\n  -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data \\\n  -p 8283:8283 \\\n  -e OLLAMA_BASE_URL=\"http://host.docker.internal:11434\" \\\n  letta/letta:latest\n```\n\n**Logs**\nNo interesting logs when I start the server, but when I try to talk to the chatbot when creating an agent with the \"letta-free\" model, I have these errors:\n```\nLetta.letta.agent - ERROR - step() failed with an unrecognized exception: 'The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable'\nLetta.letta.server.server - ERROR - Error in server._step: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\nTraceback (most recent call last):\n\n...\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_client.py\", line 110, in __init__\n    raise OpenAIError(\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\nNone\nTraceback (most recent call last):\n\n...\n\n  File \"/app/.venv/lib/python3.11/site-packages/openai/_client.py\", line 110, in __init__\n    raise OpenAIError(\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n/app/letta/server/rest_api/utils.py:112: UserWarning: SSE stream generator failed: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n  warnings.warn(f\"SSE stream generator failed: {e}\")\n```\n\n**Additional context**\nWhen I run \n ```\nsudo docker run \\\n  -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data \\\n  -p 8283:8283 \\\n  -e OPENAI_API_KEY=\"aaaaaaaaaaaaaaaaaaaaaaaa\" \\\n  letta/letta:latest\n```\nWith a dummy OpenAI token, I now have the following error in the log\n```\n/app/letta/server/server.py:1049: UserWarning: An error occurred while listing LLM models for provider id=None name='openai' api_key='aaaaaaaaaaaaaaaaaaaaaaaa' organization_id=None updated_at=None base_url='https://api.openai.com/v1': 401 Client Error: Unauthorized for url: https://api.openai.com/v1/models\n  warnings.warn(f\"An error occurred while listing LLM models for provider {provider}: {e}\")\n```\nSo Letta can see my environment variables, but the OLLAMA_BASE_URL one is still ignored.\n\n\n**Letta Config**\nDefault initial config, runned letta for the first time.\n\n---\n\n**Local LLM details**\n- [ ] I have a working Ollama instance in ubuntu 24.04. (http://localhost:11434/ works)\n- [ ] I want to use llama3.2 but that not the problem here",
      "state": "closed",
      "author": "NeilSCGH",
      "author_type": "User",
      "created_at": "2025-01-24T21:56:14Z",
      "updated_at": "2025-03-11T02:06:17Z",
      "closed_at": "2025-03-11T02:06:17Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2388/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cpacker"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2388",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2388",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:05.101735",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Thanks for reporting this, we are looking into it now!",
          "created_at": "2025-01-24T22:02:34Z"
        },
        {
          "author": "cpacker",
          "body": "Hi @NeilSCGH - thank you so much for trying Letta and your bug report!!\n\nCan you confirm what version of Letta you're on? You should see a version message print out at the top of the server logs.\n\nI'm on the latest version and am not able to reproduce your bug.\n\nFor reference, this is the output of ",
          "created_at": "2025-01-24T23:38:59Z"
        },
        {
          "author": "NeilSCGH",
          "body": "Hello,\nThanks for your quick response.\n\n### Letta version: ok\nWhen running docker, it uses the latest version of Letta server: v0.6.15.\nHere are the startup logs:\n```\nNo external Postgres configuration detected, starting internal PostgreSQL...\n\nPostgreSQL Database directory appears to contain a data",
          "created_at": "2025-01-25T14:28:25Z"
        },
        {
          "author": "NeilSCGH",
          "body": "Edit:\nI've check the logs of ollama with `journalctl -u ollama --no-pager`, and the ollama was not getting any requests from Letta. So the `http://localhost:11434` address wasn't working, but with the direct ip of my computer, it worked.\n\n```\nsudo docker run \\\n -v ~/.letta/.persist/pgdata:/var/lib/p",
          "created_at": "2025-01-25T14:44:49Z"
        },
        {
          "author": "cpacker",
          "body": "Amazing, thank you for all the debugging notes!! Looks like we should definitely update our ollama docs to have special instructions for ubuntu/linux.\n\n> The llama3.2 models perfectly works with the terminal and with Open Webui. Does Letta need more ram to run the models? Any idea here?\n\nThe minimum",
          "created_at": "2025-01-25T19:20:14Z"
        }
      ]
    },
    {
      "issue_number": 2351,
      "title": "from letta_client import Letta  ModuleNotFoundError: No module named 'letta_client'",
      "body": "**Describe the bug**\r\nusing 0.6.9 Latest - docker\r\nusing the sdk\r\nfrom letta_client import Letta\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'letta_client'\r\n\r\n- so the  letta_client  is not installed with the docker version ?\r\n",
      "state": "closed",
      "author": "quantumalchemy",
      "author_type": "User",
      "created_at": "2025-01-13T17:59:34Z",
      "updated_at": "2025-03-10T01:54:09Z",
      "closed_at": "2025-03-10T01:54:07Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2351/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2351",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2351",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:05.357440",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Yes to use the new Python SDK you will need to do `pip install letta-client`. We will eventually add `letta-client` as a dependency to `letta` so they are all installed together. ",
          "created_at": "2025-01-13T18:01:31Z"
        },
        {
          "author": "quantumalchemy",
          "body": "will this still work without the rest api -- when updated?\r\n\r\n```\r\nfrom letta import create_client, LLMConfig, EmbeddingConfig, ChatMemory\r\nfrom letta.utils import get_human_text, get_persona_text\r\nclient = create_client() \r\n```\r\n\r\n\r\n\r\n",
          "created_at": "2025-01-13T18:27:43Z"
        },
        {
          "author": "quantumalchemy",
          "body": "Yes  but I thought this was the old sdk & to be replaced by Letta? So will this be good after updates?\n",
          "created_at": "2025-01-23T17:20:46Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-23T02:06:52Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-10T01:54:07Z"
        }
      ]
    },
    {
      "issue_number": 2355,
      "title": "Feat: Streaming support for All openai compatible api proxies Litellm, llama.cpp,  llamafile, lm studio ..",
      "body": "**Is your feature request related to a problem? Please describe.**\n\nAll openai compatible api proxies Litellm, llama.cpp, lm studio have support streaming\nIt would be Great if Letta could support this not just openai only.\n\nGreatest Advantage -- Would cut down the latency for Local small LLMs  via \nopenai compatible api proxies: like Litellm, llama.cpp,  llamafile, lm studio..\nopensource  LLM are getting faster better and smaller\nI have been testing this  last week with 1.5B model:  Dolphin3.0-Qwen2.5-1.5B.i1-Q6_K.gguf working with Letta ***\nfunction calling ect.  running on cpu / edge device served up by llamafile - https://github.com/Mozilla-Ocho/llamafile ( llama.cpp) \nIf we could only get streaming enabled would be fantastic! - Thanks!\n\n",
      "state": "closed",
      "author": "quantumalchemy",
      "author_type": "User",
      "created_at": "2025-01-16T16:32:26Z",
      "updated_at": "2025-03-10T01:54:06Z",
      "closed_at": "2025-03-10T01:54:06Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2355/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2355",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2355",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:05.560171",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Yes we are looking into this! ",
          "created_at": "2025-01-23T03:08:26Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-23T02:06:50Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-10T01:54:06Z"
        }
      ]
    },
    {
      "issue_number": 2371,
      "title": "Failure executing a tool with Letta pypi server 0.6.12",
      "body": "I am having problem getting an agent to execute a tool properly. The Letta server is executed from Windows command line with the 0.6.12 release. \n\nHere is the code for my python client, which simply copies the roll-dice tool from the quick-start guide:\n\n    from letta import create_client, agent, EmbeddingConfig, LLMConfig\n    client = create_client()\n    def roll_dice_20() -> str:\n        \"\"\"\n        Simulate the roll of a 20-sided die (d20).\n        This function generates a random integer between 1 and 20, inclusive,\n        which represents the outcome of a single roll of a d20.\n        Returns:\n            str: The result of the die roll.\n        \"\"\"\n        import random\n        dice_role_outcome = random.randint(1, 20)\n        output_string = f\"You rolled a {dice_role_outcome}\"\n        return output_string\n    \n    def create_agent(client):\n        try:\n            tool = client.create_tool(\n                func=roll_dice_20,\n                name = 'rd20f',\n            )\n            print(f\"...Created tool {tool.name}\")\n            agent = client.create_agent(\n                tool_ids=[tool.id]\n            )\n            return agent\n        except Exception as e:\n            print(f\"Error creating agent: {e}\")\n            return None\n\n    client.set_default_llm_config(LLMConfig.default_config(model_name=\"gpt-4\"))\n    client.set_default_embedding_config(EmbeddingConfig.default_config(model_name=\"text-embedding-ada-002\"))\n    agent = create_agent(client)\n  \nAfter opening up this newly created agent in ADE, the dialog goes as follows:\n\n    User: roll dice\n    Agent: It looks like the dice game isn't working at the moment. How about we chat instead? Do you have any burning questions that you've been pondering over recently?\n  \nWhen searching for this newly created tool in the ADE, it does appear in the \"All tools\" list.\n\n**Letta Config**\n\nHere is my `~/.letta/config` file:\n\n    [defaults]\n    preset = memgpt_chat\n    persona = sam_pov\n    human = basic\n    \n    [archival_storage]\n    type = sqlite\n    path = C:\\Users\\kaihu\\.letta\n    \n    [recall_storage]\n    type = sqlite\n    path = C:\\Users\\kaihu\\.letta\n    \n    [metadata_storage]\n    type = sqlite\n    path = C:\\Users\\kaihu\\.letta\n    \n    [version]\n    letta_version = 0.6.12",
      "state": "closed",
      "author": "kaihuchen",
      "author_type": "User",
      "created_at": "2025-01-21T19:26:39Z",
      "updated_at": "2025-03-09T01:53:48Z",
      "closed_at": "2025-03-09T01:53:47Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2371/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2371",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2371",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:05.777259",
      "comments": [
        {
          "author": "mattzh72",
          "body": "Interesting...can you go to the debug mode in the ADE and show me what the error with the tool is?\n\n<img width=\"448\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cc4510f9-3a44-49a2-ac0b-d5eb7c40c8b8\" />",
          "created_at": "2025-01-21T19:34:33Z"
        },
        {
          "author": "kaihuchen",
          "body": "Here you go:\n\n![Image](https://github.com/user-attachments/assets/2a1c810f-3f61-4a3a-a8f2-489e1013cb1a)\n\n![Image](https://github.com/user-attachments/assets/1e49f852-010e-42c2-b7da-887f0ac3bfb5)",
          "created_at": "2025-01-21T21:38:53Z"
        },
        {
          "author": "kaihuchen",
          "body": "I think I figured it out.\n\nSeems that the argument 'name' is the source of the problem.\n\nThe following call executed with no error, but the agent can't find the roll_dice_20 function. In addition, if I call it more than one then I get SQL error since name is an unique key in the DB. If I keep changi",
          "created_at": "2025-01-22T21:43:13Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-22T02:00:53Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-09T01:53:46Z"
        }
      ]
    },
    {
      "issue_number": 2372,
      "title": "pydantic_core._pydantic_core.ValidationError: 6 validation errors for AgentState",
      "body": "**Describe the bug**\nI get a pydantic error when I try to list agents.\n\n**Please describe your setup**\n- [ ] How did you install letta?\n  - `pip install letta`\n- [ ] Describe your setup\n  - What's your OS (Windows/MacOS/Linux)? Ubuntu 24.04\n  - How are you running `letta`? Docker\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/b92ed89c-4c1c-43c8-9f10-15957273a87f)\n\n**Additional context**\n```python\nfrom letta import create_client\n\nclient = create_client(base_url=\"http://localhost:8283\")\nclient.list_agents()\n```\n\n**Letta Config**\nPlease attach your `~/.letta/config` file or copy paste it below.\n\n---\n\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\n\n**Local LLM details**\n\nIf you are trying to run Letta with local LLMs, please provide the following information:\n\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\n",
      "state": "closed",
      "author": "KengoWada",
      "author_type": "User",
      "created_at": "2025-01-22T16:22:03Z",
      "updated_at": "2025-03-09T01:53:46Z",
      "closed_at": "2025-03-09T01:53:46Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2372/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2372",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2372",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:06.071334",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-22T02:00:52Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-09T01:53:45Z"
        }
      ]
    },
    {
      "issue_number": 2202,
      "title": "list_users throws Exception \"too many values to unpack\"",
      "body": "**Describe the bug**\r\nGET Endpoint `users` (func `list_users()`) [expects two values](https://github.com/letta-ai/letta/blob/0.6.1/letta/server/rest_api/routers/v1/users.py#L38) (`next_cursor` and `users`) from\u001b `server.user_manager.list_users(cursor=cursor, limit=limit)` which returns only a [single list](https://github.com/letta-ai/letta/blob/0.6.1/letta/services/user_manager.py#L101) of User instances. \r\n",
      "state": "closed",
      "author": "theocnrds",
      "author_type": "User",
      "created_at": "2024-12-09T12:37:55Z",
      "updated_at": "2025-03-08T01:50:20Z",
      "closed_at": "2025-03-08T01:50:19Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2202/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "mattzh72"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2202",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2202",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:06.356484",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "We don't recommend using the user objects provided by `letta` - if you have your own notion of a user in your application, you should use the `tags` field in agents to tag agents with your end user IDs. ",
          "created_at": "2024-12-11T05:05:34Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-01-11T02:03:20Z"
        },
        {
          "author": "Beetroit",
          "body": "This issue still persists in letta. Had to modify it manually. You are returning only the user list, while the route is expecting the next cursor and users list.\r\n![image](https://github.com/user-attachments/assets/40a980ca-73f3-4bf2-9213-4c73b0293120)\r\n![image](https://github.com/user-attachments/a",
          "created_at": "2025-01-12T15:37:04Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-13T02:02:40Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-08T01:50:19Z"
        }
      ]
    },
    {
      "issue_number": 2305,
      "title": "Request to add Microsoft and Apple to the login options",
      "body": "Request to add Microsoft and Apple to the login options to facilitate global users to log in, not just Google and GitHub.",
      "state": "closed",
      "author": "unlockInsight",
      "author_type": "User",
      "created_at": "2024-12-21T14:06:12Z",
      "updated_at": "2025-03-07T02:06:20Z",
      "closed_at": "2025-03-07T02:06:20Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2305/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2305",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2305",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:06.602360",
      "comments": [
        {
          "author": "cpacker",
          "body": "Microsoft is planned, should be coming soon. Will make a note of Apple login feature request ",
          "created_at": "2024-12-21T22:53:10Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-01-21T02:00:56Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-21T02:03:23Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-07T02:06:19Z"
        }
      ]
    },
    {
      "issue_number": 2358,
      "title": "Code version management",
      "body": "**Is your feature request related to a problem? Please describe.**\nWhen models edit a particular piece of code, there is a chance that harmful changes may be inadvertently made to the code. Oddly enough, this disease can affect even people. This is partly why tools like difftools\\were created. They can be used to carefully review all changes before committing, and then confirm or reject them.\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\nSo, it is proposed to introduce a system for tracking all changes to middle or large code fragments. Each code fragment should be marked with the specific id. If the model generated a change in the code fragment, it should also request the result of difftool, analyze them, and only if everything is in order  send the result to the user.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.",
      "state": "closed",
      "author": "palandovalex",
      "author_type": "User",
      "created_at": "2025-01-19T19:26:13Z",
      "updated_at": "2025-03-05T02:05:38Z",
      "closed_at": "2025-03-05T02:05:38Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2358/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2358",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2358",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:06.819166",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-19T02:02:51Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-05T02:05:37Z"
        }
      ]
    },
    {
      "issue_number": 2322,
      "title": "Memgpt sends all prev messages (chat history) to OPENAI everytime",
      "body": "I'm invoking memgpt agents using this following code\r\n\r\n```\r\nagent_state = client.load_agent(agent_config=get_agent_config(agent_name), project=project)\r\nresponse = client.user_message(agent_id=agent_state.get(\"id\"), message=message)\r\n```\r\nNow this `user_message` which is located in `memgpt.client.client.LocalClient` uses `._step` method that accumulates all the prev messages and sends them to OPENAI. \r\n\r\nI want to send only prev 3-4 messages max as sending all messages is very costly and my input token count goes upto 13k-14k for gpt-4o. Is there a way I can summarize messages early like whenever the token count exceeds 4k or 5k.\r\n\r\nI'm using pymemgpt==0.3.17.\r\n\r\nOne thing that I noticed was in `step()` method, the summarization is done based on the following code block\r\n\r\n```\r\nif current_total_tokens > MESSAGE_SUMMARY_WARNING_FRAC * int(self.agent_state.llm_config.context_window):\r\n                printd(\r\n                    f\"{CLI_WARNING_PREFIX}last response total_tokens ({current_total_tokens}) > {MESSAGE_SUMMARY_WARNING_FRAC * int(self.agent_state.llm_config.context_window)}\"\r\n                )\r\n                # Only deliver the alert if we haven't already (this period)\r\n                if not self.agent_alerted_about_memory_pressure:\r\n                    active_memory_warning = True\r\n                    self.agent_alerted_about_memory_pressure = True  # it's up to the outer loop to handle this\r\n```\r\n\r\nCan I also maybe change `MESSAGE_SUMMARY_WARNING_FRAC` variable from somewhere (given I'm using memgpt as a pip package, so I can't update it's code files directly)?",
      "state": "closed",
      "author": "ahmadbinshafiq",
      "author_type": "User",
      "created_at": "2024-12-31T15:07:14Z",
      "updated_at": "2025-03-04T02:05:29Z",
      "closed_at": "2025-03-04T02:05:28Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2322/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2322",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2322",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:07.017019",
      "comments": [
        {
          "author": "Shua1",
          "body": "Does not seem possible. It's a constant. Probably will need to change it to a environment variable or a flag to change it.\r\n\r\n\r\n",
          "created_at": "2025-01-08T01:44:32Z"
        },
        {
          "author": "mattzh72",
          "body": "We are making some new changes that will make the summarizer behavior configurable - stay tuned.",
          "created_at": "2025-01-16T18:42:30Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-16T02:06:43Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-04T02:05:28Z"
        }
      ]
    },
    {
      "issue_number": 2357,
      "title": "letta unable to query ollama - [ReadTimeout: timed out]",
      "body": "letta is unable to query to local ollama. I am using the letta-python sdk with ollama both on the same local server. The ollama server becomes unresponsive after requesting through letta and I have to restart the ollama server.\n```python\nfrom letta_client import Letta\nclient = Letta(base_url=\"http://localhost:8283\")\n```\nletta sees ollama when listing models using\n`client.models.list_llms()`\n```bash\n[LlmConfig(model='letta-free', model_endpoint_type='openai', model_endpoint='https://inference.memgpt.ai/', model_wrapper=None, context_window=16384, put_inner_thoughts_in_kwargs=True, handle='letta/letta-free'),\n LlmConfig(model='llama3.2:latest', model_endpoint_type='ollama', model_endpoint='http://localhost:11434/', model_wrapper='chatml', context_window=131072, put_inner_thoughts_in_kwargs=True, handle='ollama/llama3.2:latest')\n```\n\nI then created my agent using following code.\n```python\nfrom letta_client import LlmConfig, EmbeddingConfig\n\nllm_cfg = LlmConfig(\n    model=\"llama3.2\",\n    model_endpoint_type=\"ollama\",\n    model_endpoint=\"http://localhost:11434\",\n    context_window=131072\n)\n\nembedding_cfg = EmbeddingConfig(\n        embedding_endpoint_type=\"ollama\",\n        # embedding_endpoint=None,\n        embedding_model=\"llama3.2\",\n        embedding_dim=3072,\n        # embedding_chunk_size=300\n    )\nagent = client.agents.create(name='test', memory_blocks=[], llm_config=llm_cfg, embedding_config=embedding_cfg)\n```\n\nand then send a message by doing\n```python\nfrom letta_client import MessageCreate\nclient.agents.messages.send(\n    agent_id=agent.id,\n    messages=[\n        MessageCreate(\n            role=\"user\",\n            text=\"why is the sky blue?\",\n        )  \n    ],\n)\n```\n\nThe error trace I get is as follows:\n```bash\n{\n\t\"name\": \"ReadTimeout\",\n\t\"message\": \"timed out\",\n\t\"stack\": \"---------------------------------------------------------------------------\nReadTimeout                               Traceback (most recent call last)\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:101, in map_httpcore_exceptions()\n    100 try:\n--> 101     yield\n    102 except Exception as exc:\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:250, in HTTPTransport.handle_request(self, request)\n    249 with map_httpcore_exceptions():\n--> 250     resp = self._pool.handle_request(req)\n    252 assert isinstance(resp.stream, typing.Iterable)\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256, in ConnectionPool.handle_request(self, request)\n    255     self._close_connections(closing)\n--> 256     raise exc from None\n    258 # Return the response. Note that in this case we still have to manage\n    259 # the point at which the response is closed.\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236, in ConnectionPool.handle_request(self, request)\n    234 try:\n    235     # Send the request on the assigned connection.\n--> 236     response = connection.handle_request(\n    237         pool_request.request\n    238     )\n    239 except ConnectionNotAvailable:\n    240     # In some cases a connection may initially be available to\n    241     # handle a request, but then become unavailable.\n    242     #\n    243     # In this case we clear the connection and try again.\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:103, in HTTPConnection.handle_request(self, request)\n    101     raise exc\n--> 103 return self._connection.handle_request(request)\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:136, in HTTP11Connection.handle_request(self, request)\n    135         self._response_closed()\n--> 136 raise exc\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:106, in HTTP11Connection.handle_request(self, request)\n     97 with Trace(\n     98     \\\"receive_response_headers\\\", logger, request, kwargs\n     99 ) as trace:\n    100     (\n    101         http_version,\n    102         status,\n    103         reason_phrase,\n    104         headers,\n    105         trailing_data,\n--> 106     ) = self._receive_response_headers(**kwargs)\n    107     trace.return_value = (\n    108         http_version,\n    109         status,\n    110         reason_phrase,\n    111         headers,\n    112     )\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:177, in HTTP11Connection._receive_response_headers(self, request)\n    176 while True:\n--> 177     event = self._receive_event(timeout=timeout)\n    178     if isinstance(event, h11.Response):\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:217, in HTTP11Connection._receive_event(self, timeout)\n    216 if event is h11.NEED_DATA:\n--> 217     data = self._network_stream.read(\n    218         self.READ_NUM_BYTES, timeout=timeout\n    219     )\n    221     # If we feed this case through h11 we'll raise an exception like:\n    222     #\n    223     #     httpcore.RemoteProtocolError: can't handle event type\n   (...)\n    227     # perspective. Instead we handle this case distinctly and treat\n    228     # it as a ConnectError.\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:126, in SyncStream.read(self, max_bytes, timeout)\n    125 exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, OSError: ReadError}\n--> 126 with map_exceptions(exc_map):\n    127     self._sock.settimeout(timeout)\n\nFile ~/anaconda3/lib/python3.11/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    154 try:\n--> 155     self.gen.throw(typ, value, traceback)\n    156 except StopIteration as exc:\n    157     # Suppress StopIteration *unless* it's the same exception that\n    158     # was passed to throw().  This prevents a StopIteration\n    159     # raised inside the \\\"with\\\" statement from being suppressed.\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)\n     13     if isinstance(exc, from_exc):\n---> 14         raise to_exc(exc) from exc\n     15 raise\n\nReadTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nReadTimeout                               Traceback (most recent call last)\nCell In[100], line 2\n      1 from letta_client import MessageCreate\n----> 2 client.agents.messages.send(\n      3     agent_id=agent.id,\n      4     messages=[\n      5         MessageCreate(\n      6             role=\\\"user\\\",\n      7             text=\\\"hello\\\",\n      8         )  \n      9     ],\n     10 )\n\nFile ~/anaconda3/lib/python3.11/site-packages/letta_client/agents/messages/client.py:171, in MessagesClient.send(self, agent_id, messages, config, request_options)\n    124 def send(\n    125     self,\n    126     agent_id: str,\n   (...)\n    130     request_options: typing.Optional[RequestOptions] = None,\n    131 ) -> LettaResponse:\n    132     \\\"\\\"\\\"\n    133     Process a user message and return the agent's response.\n    134     This endpoint accepts a message from a user and processes it through the agent.\n   (...)\n    169     )\n    170     \\\"\\\"\\\"\n--> 171     _response = self._client_wrapper.httpx_client.request(\n    172         f\\\"v1/agents/{jsonable_encoder(agent_id)}/messages\\\",\n    173         method=\\\"POST\\\",\n    174         json={\n    175             \\\"messages\\\": convert_and_respect_annotation_metadata(\n    176                 object_=messages, annotation=typing.Sequence[MessageCreate], direction=\\\"write\\\"\n    177             ),\n    178             \\\"config\\\": convert_and_respect_annotation_metadata(\n    179                 object_=config, annotation=LettaRequestConfig, direction=\\\"write\\\"\n    180             ),\n    181         },\n    182         request_options=request_options,\n    183         omit=OMIT,\n    184     )\n    185     try:\n    186         if 200 <= _response.status_code < 300:\n\nFile ~/anaconda3/lib/python3.11/site-packages/letta_client/core/http_client.py:198, in HttpClient.request(self, path, method, base_url, params, json, data, content, files, headers, request_options, retries, omit)\n    190 timeout = (\n    191     request_options.get(\\\"timeout_in_seconds\\\")\n    192     if request_options is not None and request_options.get(\\\"timeout_in_seconds\\\") is not None\n    193     else self.base_timeout()\n    194 )\n    196 json_body, data_body = get_request_body(json=json, data=data, request_options=request_options, omit=omit)\n--> 198 response = self.httpx_client.request(\n    199     method=method,\n    200     url=urllib.parse.urljoin(f\\\"{base_url}/\\\", path),\n    201     headers=jsonable_encoder(\n    202         remove_none_from_dict(\n    203             {\n    204                 **self.base_headers(),\n    205                 **(headers if headers is not None else {}),\n    206                 **(request_options.get(\\\"additional_headers\\\", {}) or {} if request_options is not None else {}),\n    207             }\n    208         )\n    209     ),\n    210     params=encode_query(\n    211         jsonable_encoder(\n    212             remove_none_from_dict(\n    213                 remove_omit_from_dict(\n    214                     {\n    215                         **(params if params is not None else {}),\n    216                         **(\n    217                             request_options.get(\\\"additional_query_parameters\\\", {}) or {}\n    218                             if request_options is not None\n    219                             else {}\n    220                         ),\n    221                     },\n    222                     omit,\n    223                 )\n    224             )\n    225         )\n    226     ),\n    227     json=json_body,\n    228     data=data_body,\n    229     content=content,\n    230     files=(\n    231         convert_file_dict_to_httpx_tuples(remove_omit_from_dict(remove_none_from_dict(files), omit))\n    232         if (files is not None and files is not omit)\n    233         else None\n    234     ),\n    235     timeout=timeout,\n    236 )\n    238 max_retries: int = request_options.get(\\\"max_retries\\\", 0) if request_options is not None else 0\n    239 if _should_retry(response=response):\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:825, in Client.request(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\n    810     warnings.warn(message, DeprecationWarning, stacklevel=2)\n    812 request = self.build_request(\n    813     method=method,\n    814     url=url,\n   (...)\n    823     extensions=extensions,\n    824 )\n--> 825 return self.send(request, auth=auth, follow_redirects=follow_redirects)\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914, in Client.send(self, request, stream, auth, follow_redirects)\n    910 self._set_timeout(request)\n    912 auth = self._build_request_auth(request, auth)\n--> 914 response = self._send_handling_auth(\n    915     request,\n    916     auth=auth,\n    917     follow_redirects=follow_redirects,\n    918     history=[],\n    919 )\n    920 try:\n    921     if not stream:\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\n    939 request = next(auth_flow)\n    941 while True:\n--> 942     response = self._send_handling_redirects(\n    943         request,\n    944         follow_redirects=follow_redirects,\n    945         history=history,\n    946     )\n    947     try:\n    948         try:\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979, in Client._send_handling_redirects(self, request, follow_redirects, history)\n    976 for hook in self._event_hooks[\\\"request\\\"]:\n    977     hook(request)\n--> 979 response = self._send_single_request(request)\n    980 try:\n    981     for hook in self._event_hooks[\\\"response\\\"]:\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1014, in Client._send_single_request(self, request)\n   1009     raise RuntimeError(\n   1010         \\\"Attempted to send an async request with a sync Client instance.\\\"\n   1011     )\n   1013 with request_context(request=request):\n-> 1014     response = transport.handle_request(request)\n   1016 assert isinstance(response.stream, SyncByteStream)\n   1018 response.request = request\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:249, in HTTPTransport.handle_request(self, request)\n    235 import httpcore\n    237 req = httpcore.Request(\n    238     method=request.method,\n    239     url=httpcore.URL(\n   (...)\n    247     extensions=request.extensions,\n    248 )\n--> 249 with map_httpcore_exceptions():\n    250     resp = self._pool.handle_request(req)\n    252 assert isinstance(resp.stream, typing.Iterable)\n\nFile ~/anaconda3/lib/python3.11/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    153     value = typ()\n    154 try:\n--> 155     self.gen.throw(typ, value, traceback)\n    156 except StopIteration as exc:\n    157     # Suppress StopIteration *unless* it's the same exception that\n    158     # was passed to throw().  This prevents a StopIteration\n    159     # raised inside the \\\"with\\\" statement from being suppressed.\n    160     return exc is not value\n\nFile ~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:118, in map_httpcore_exceptions()\n    115     raise\n    117 message = str(exc)\n--> 118 raise mapped_exc(message) from exc\n\nReadTimeout: timed out\"\n}\n```\nMy ollama is serving on port 11434 and I can test it on a fresh serve by doing\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n```\n\nBut when I request from letta I don't get any response back and nothing is shown on letta logs. It seems the process somewhere fails in ollama request I'm guessing because of some issues in message formatting.\n\n```bash\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\nllama_model_loader: - kv   5:                         general.size_label str              = 3B\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   8:                          llama.block_count u32              = 28\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  18:                          general.file_type u32              = 15\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\ntime=2025-01-17T12:45:08.841+05:30 level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   58 tensors\nllama_model_loader: - type q4_K:  168 tensors\nllama_model_loader: - type q6_K:   29 tensors\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 3072\nllm_load_print_meta: n_layer          = 28\nllm_load_print_meta: n_head           = 24\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 3\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 8192\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 3B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 3.21 B\nllm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) \nllm_load_print_meta: general.name     = Llama 3.2 3B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\nllm_load_print_meta: LF token         = 128 ''\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nllm_load_tensors: offloading 19 repeating layers to GPU\nllm_load_tensors: offloaded 19/29 layers to GPU\nllm_load_tensors:   CPU_Mapped model buffer size =  1918.35 MiB\nllm_load_tensors:        CUDA0 model buffer size =  1096.05 MiB\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 131072\nllama_new_context_with_model: n_ctx_per_seq = 131072\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 500000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_kv_cache_init: kv_size = 131072, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\nllama_kv_cache_init:        CPU KV buffer size =  4608.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  9728.00 MiB\nllama_new_context_with_model: KV self size  = 14336.00 MiB, K (f16): 7168.00 MiB, V (f16): 7168.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.50 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =  7197.06 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   262.01 MiB\nllama_new_context_with_model: graph nodes  = 902\nllama_new_context_with_model: graph splits = 104 (with bs=512), 3 (with bs=1)\ntime=2025-01-17T12:45:11.604+05:30 level=INFO source=server.go:594 msg=\"llama runner started in 3.02 seconds\"\n```\nIt freezes at this point and I can't call ollama anymore. If I kill the ollama process by Ctrl+C then I get \n```bash\n[GIN] 2025/01/17 - 12:48:03 | 500 |         2m55s |       127.0.0.1 | POST     \"/api/generate\"\n```\n  - OS is a Ubuntu server\n  - Letta is running by command `sudo docker run --network=host -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data -e OLLAMA_BASE_URL=\"http://localhost:11434\" letta/letta:latest` which worked best for me since I wasn't able to link ollama to letta otherwise.",
      "state": "closed",
      "author": "ar5entum",
      "author_type": "User",
      "created_at": "2025-01-17T07:22:45Z",
      "updated_at": "2025-03-03T02:06:20Z",
      "closed_at": "2025-03-03T02:06:20Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2357/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2357",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2357",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:07.265577",
      "comments": [
        {
          "author": "ar5entum",
          "body": "Inside `https://github.com/letta-ai/letta/blob/main/letta/local_llm/ollama/api.py` I found out that the letta was making this request. It seemed to me that \"options\" parameter was causing the trouble. I commented out that part and it seemed to work but llama3.2 was unable to produce any responses (s",
          "created_at": "2025-01-17T12:29:47Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-17T02:04:33Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-03T02:06:19Z"
        }
      ]
    },
    {
      "issue_number": 639,
      "title": "Are there any examples demonstrating how to integrate and use LangChain?",
      "body": "Are there any examples demonstrating how to integrate and use LangChain?\r\n",
      "state": "closed",
      "author": "Casper-Mars",
      "author_type": "User",
      "created_at": "2023-12-18T07:20:04Z",
      "updated_at": "2025-03-02T00:25:54Z",
      "closed_at": "2024-12-06T02:30:51Z",
      "labels": [
        "auto-closed"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/639/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/639",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/639",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:07.469489",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically closed due to 60 days of inactivity.",
          "created_at": "2024-12-06T02:30:50Z"
        },
        {
          "author": "wsargent",
          "body": "https://github.com/letta-ai/letta/blob/main/examples/langchain_tool_usage.py",
          "created_at": "2025-03-02T00:25:53Z"
        }
      ]
    },
    {
      "issue_number": 2353,
      "title": "Stale example in Agentic RAG with Letta.ipynb",
      "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**Please describe your setup**\r\n- [x] How did you install letta?\r\n  - Cloned main, pip install -e .\r\n- [x] Describe your setup\r\n  - Mac\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\r\n<img width=\"790\" alt=\"image\" src=\"https://github.com/user-attachments/assets/29a7d5da-13c1-4b4c-a4af-6d5e73ad97b4\" />\r\n\r\nSpecifically this example code gives an error:\r\n\r\n```py\r\nno_tool_agent = client.create_agent(\r\n    tools=['send_message'], \r\n    include_base_tools=False\r\n)\r\nno_tool_agent.tools\r\n```\r\n\r\n`TypeError: LocalClient.create_agent() got an unexpected keyword argument 'tools'`\r\n\r\nAlso later in this code:\r\n\r\n```py\r\nfrom letta.schemas.memory import ChatMemory\r\n\r\n# delete agent if exists \r\nif client.get_agent_id(\"birthday_agent\"): \r\n    client.delete_agent(client.get_agent_id(\"birthday_agent\"))\r\n\r\nagent_state = client.create_agent(\r\n    name=\"birthday_agent\", \r\n    tools=[birthday_tool.name], \r\n    memory=ChatMemory(\r\n        human=\"My name is Sarah\", \r\n        persona=\"You are a agent with access to a birthday_db \" \\\r\n        + \"that you use to lookup information about users' birthdays.\"\r\n    )\r\n)\r\n```",
      "state": "closed",
      "author": "ianb",
      "author_type": "User",
      "created_at": "2025-01-14T22:57:37Z",
      "updated_at": "2025-03-01T02:08:46Z",
      "closed_at": "2025-03-01T02:08:45Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2353/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2353",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2353",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:07.652300",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Yes unfortunately we made major API changes recently -- for this example you will need to instead pass in `tool_ids` and provide the `birthday_tool.id`. \r\n\r\nWe are moving our Python client to a new auto-generated SDK, and with that will be releasing updated examples (hopefully around next week). Apo",
          "created_at": "2025-01-16T00:42:11Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-15T02:01:27Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-03-01T02:08:45Z"
        }
      ]
    },
    {
      "issue_number": 2352,
      "title": "Generate token for self hosted letta instances.",
      "body": "Currently when accessing a self hosted Letta instance, you have to pass the password in the headers. Could we have a way to generate tokens to use instead of the password.\r\n\r\n```diff\r\n- \"X-BARE-PASSWORD\": \"password <password>\"\r\n+ \"Authorization\": \"Bearer <token>\"\r\n```",
      "state": "closed",
      "author": "KengoWada",
      "author_type": "User",
      "created_at": "2025-01-14T15:50:50Z",
      "updated_at": "2025-02-28T02:04:38Z",
      "closed_at": "2025-02-28T02:04:38Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2352/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2352",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2352",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:07.894769",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-14T02:02:32Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-28T02:04:37Z"
        }
      ]
    },
    {
      "issue_number": 2255,
      "title": "\"API call didn't return a message\" error repeating very frequently in v5.5, with openai.",
      "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\nThe issue is \"API call didn't return a message\", error is repeating very frequently with letta v5.5. I am using openai's gpt 4o. The problem is that the inner monologue is displayed everytime on the ui but there is no actual user response given. I have been able to debug the issue. It is because in the normal response, when everything is correct, the message.choices contains tool calls for send message. But whenever this issue arises the message.choices doesn't contain any function call. It should have been fixed according to a previous closed bug. But it is still there and is very frustating. I am attaching the logs as well.\r\n```\r\ncorrect normal response:\r\noriginal response choices in unpack_all_inner_thoughts_from_kwargs: {\r\n    \"id\": \"message-fd4674bd-0943-4372-a1bd-55f84da2175d\",\r\n    \"choices\": [\r\n        {\r\n            \"finish_reason\": \"tool_calls\",\r\n            \"index\": 0,\r\n            \"message\": {\r\n                \"content\": null,\r\n                \"tool_calls\": [\r\n                    {\r\n                        \"id\": \"6b6fe6f1-3edb-4a3a-b775-e0edd\",\r\n                        \"type\": \"function\",\r\n                        \"function\": {\r\n                            \"arguments\": \"{\\\"inner_thoughts\\\":\\\"Keeping the interaction engaging. Ensuring consistency in responding to Harshit.\\\",\\\"message\\\":\\\"Hey there, Harshit! Let me know if there's anything specific you need or if you'd just like to chat.\\\"}\",\r\n                            \"name\": \"send_message\"\r\n                        }\r\n                    }\r\n                ],\r\n                \"role\": \"assistant\",\r\n                \"function_call\": null\r\n            },\r\n            \"logprobs\": null,\r\n            \"seed\": null\r\n        }\r\n    ],\r\n    \"created\": \"2024-12-15T19:25:37.356338Z\",\r\n    \"model\": \"gpt-4o-2024-08-06\",\r\n    \"system_fingerprint\": \"fp_a79d8dac1f\",\r\n    \"object\": \"chat.completion\",\r\n    \"usage\": {\r\n        \"completion_tokens\": 48,\r\n        \"prompt_tokens\": 15582,\r\n        \"total_tokens\": 15630\r\n    }\r\n}\r\n_get_ai_reply response: {\r\n    \"id\": \"message-fd4674bd-0943-4372-a1bd-55f84da2175d\",\r\n    \"choices\": [\r\n        {\r\n            \"finish_reason\": \"tool_calls\",\r\n            \"index\": 0,\r\n            \"message\": {\r\n                \"content\": \"Keeping the interaction engaging. Ensuring consistency in responding to Harshit.\",\r\n                \"tool_calls\": [\r\n                    {\r\n                        \"id\": \"6b6fe6f1-3edb-4a3a-b775-e0edd\",\r\n                        \"type\": \"function\",\r\n                        \"function\": {\r\n                            \"arguments\": \"{\\n  \\\"message\\\": \\\"Hey there, Harshit! Let me know if there's anything specific you need or if you'd just like to chat.\\\"\\n}\",\r\n                            \"name\": \"send_message\"\r\n                        }\r\n                    }\r\n                ],\r\n                \"role\": \"assistant\",\r\n                \"function_call\": null\r\n            },\r\n            \"logprobs\": null,\r\n            \"seed\": null\r\n        }\r\n    ],\r\n    \"created\": \"2024-12-15T19:25:37.356338Z\",\r\n    \"model\": \"gpt-4o-2024-08-06\",\r\n    \"system_fingerprint\": \"fp_a79d8dac1f\",\r\n    \"object\": \"chat.completion\",\r\n    \"usage\": {\r\n        \"completion_tokens\": 48,\r\n        \"prompt_tokens\": 15582,\r\n        \"total_tokens\": 15630\r\n    }\r\n}\r\n```\r\n\r\n```\r\nerror causing response:\r\noriginal response choices in unpack_all_inner_thoughts_from_kwargs: {\r\n    \"id\": \"message-4c44f4dd-95d5-45c1-ac8f-0c118a5b2967\",\r\n    \"choices\": [\r\n        {\r\n            \"finish_reason\": \"stop\",\r\n            \"index\": 0,\r\n            \"message\": {\r\n                \"content\": \"Hello again, Harshit!  How can I assist you today? \",\r\n                \"tool_calls\": null,\r\n                \"role\": \"assistant\",\r\n                \"function_call\": null\r\n            },\r\n            \"logprobs\": null,\r\n            \"seed\": null\r\n        }\r\n    ],\r\n    \"created\": \"2024-12-15T19:20:23.558565Z\",\r\n    \"model\": \"gpt-4o-2024-08-06\",\r\n    \"system_fingerprint\": \"fp_a79d8dac1f\",\r\n    \"object\": \"chat.completion\",\r\n    \"usage\": {\r\n        \"completion_tokens\": 17,\r\n        \"prompt_tokens\": 8289,\r\n        \"total_tokens\": 8306\r\n    }\r\n}\r\n_get_ai_reply response: {\r\n    \"id\": \"message-4c44f4dd-95d5-45c1-ac8f-0c118a5b2967\",\r\n    \"choices\": [\r\n        null\r\n    ],\r\n    \"created\": \"2024-12-15T19:20:23.558565Z\",\r\n    \"model\": \"gpt-4o-2024-08-06\",\r\n    \"system_fingerprint\": \"fp_a79d8dac1f\",\r\n    \"object\": \"chat.completion\",\r\n    \"usage\": {\r\n        \"completion_tokens\": 17,\r\n        \"prompt_tokens\": 8289,\r\n        \"total_tokens\": 8306\r\n    }\r\n}\r\n```\r\n\r\n**Please describe your setup**\r\n- [x] How did you install letta?\r\n  - `pip install letta`? `pip install letta-nightly`? `git clone`?\r\n  - git clone\r\n- [x] Describe your setup\r\n  - What's your OS (Windows/MacOS/Linux)?\r\n  - Ubuntu\r\n  - How are you running `letta`? (`cmd.exe`/Powershell/Anaconda Shell/Terminal)\r\n  - poetry run letta server\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\ncorrect response:\r\n<img width=\"815\" alt=\"Screenshot 2024-12-16 at 1 31 25AM\" src=\"https://github.com/user-attachments/assets/a8cc3cb8-9ba1-453e-8a02-37f22675927f\" />\r\nerror response:\r\n<img width=\"817\" alt=\"Screenshot 2024-12-16 at 1 32 19AM\" src=\"https://github.com/user-attachments/assets/ed4fcf41-80c2-4a3e-95f8-2cdb228066e1\" />\r\n\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\nI have also modified the parameters of _get_ai_reply, a little:\r\n```\r\ndef _get_ai_reply(\r\n        self,\r\n        message_sequence: List[Message],\r\n        function_call: str = \"auto\",\r\n        first_message: bool = False,  # hint\r\n        stream: bool = False,  # TODO move to config?\r\n        fail_on_empty_response: bool = True, # this was initially False\r\n        empty_response_retry_limit: int = 0 # this was initially 3\r\n    ) -> ChatCompletionResponse:\r\n```\r\n\r\n\r\n**Letta Config**\r\nPlease attach your `~/.letta/config` file or copy past it below.\r\n\r\n---\r\n\r\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\r\n\r\n**Local LLM details**\r\n\r\nIf you are trying to run Letta with local LLMs, please provide the following information:\r\n\r\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\r\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\r\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\r\n",
      "state": "closed",
      "author": "HG2407",
      "author_type": "User",
      "created_at": "2024-12-15T20:11:06Z",
      "updated_at": "2025-02-24T02:05:10Z",
      "closed_at": "2025-02-24T02:05:10Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 22,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2255/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2255",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2255",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:08.206718",
      "comments": []
    },
    {
      "issue_number": 2344,
      "title": "[Docs]: Letta AI Monitoring with OpenTelemetry",
      "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently there is no defined way to monitor a agent built with Letta\r\n\r\n**Describe the solution you'd like**\r\n\r\nAn OpenTelemetry-native integration that can help monitor AI Agents built with Letta\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNA\r\n\r\n**Additional context**\r\n\r\nHi Team, I am one of the maintainers of [OpenLIT](https://github.com/openlit/openlit) and we support Observability for LLM apps built using Letta AI, It is all OpenTelemetry-native so the traces and metrics can be sent to any platform like [Grafana](https://grafana.com/docs/grafana-cloud/monitor-applications/ai-observability/introduction/) or any [OSS OTel tools](https://opentelemetry.io/blog/2024/llm-observability/)\r\n\r\nOpenLIT helps when the users are using LLM + VectorDBs + Other framework actions as the consolidated monitoring gives them a complete picture. Plus since its OpenTelemetry which a lot of people are looking to use.\r\n\r\nI wanted to figure out a way to share this information to Letta AI users. I was looking to add a guide in docs or a blog, Can someone help me with a process?\r\n",
      "state": "closed",
      "author": "patcher9",
      "author_type": "User",
      "created_at": "2025-01-09T06:43:49Z",
      "updated_at": "2025-02-23T02:41:36Z",
      "closed_at": "2025-02-23T02:06:53Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2344/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2344",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2344",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:08.206739",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-09T02:04:31Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-23T02:06:53Z"
        },
        {
          "author": "carenthomas",
          "body": "Hi @patcher9 we introduced a basic otel integration in the latest release and will be iterating on the exact logging! Let me know if there's anything I can add that would make your project easier!",
          "created_at": "2025-02-23T02:41:35Z"
        }
      ]
    },
    {
      "issue_number": 2334,
      "title": "Creating data source stuck",
      "body": "Hi!\r\nI just created a letta instance with docker + using ollama. But if i want to create a new data source nothing happens. \r\nDid i miss something?\r\n![image](https://github.com/user-attachments/assets/48966136-3df6-439a-bd43-6fccb910066a)",
      "state": "closed",
      "author": "KeenMaron",
      "author_type": "User",
      "created_at": "2025-01-06T19:20:10Z",
      "updated_at": "2025-02-23T02:06:55Z",
      "closed_at": "2025-02-23T02:06:54Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2334/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2334",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2334",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:08.496848",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "What version of Letta are you using? Can you please make sure to use the latest docker image? ",
          "created_at": "2025-01-08T23:14:47Z"
        },
        {
          "author": "KeenMaron",
          "body": "Hi, i just installed the latest docker image few days ago. Nothing special",
          "created_at": "2025-01-09T13:27:46Z"
        },
        {
          "author": "cpacker",
          "body": "@KeenMaron what do the server logs look like (where you ran `docker run`) when you create the data source?\r\n\r\nAlso, just as a sanity check can you pull `docker pull letta/letta:latest` before doing `docker run` again? Sorry about the bug, will try and help you out here.",
          "created_at": "2025-01-09T17:52:27Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-09T02:04:32Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-23T02:06:54Z"
        }
      ]
    },
    {
      "issue_number": 2282,
      "title": "Letta does not see the ollama server (API call got non-200 response code)",
      "body": "# Bug description\r\n\r\n* Letta does not see the ollama server. It seems to me that it's not about ollama, since everything works with llama_index and langchain\r\n\r\n## Install\r\n```\r\npip install letta\r\n```\r\n\r\n## Agent setting\r\n\r\n```python\r\nfrom letta import create_client, LLMConfig, EmbeddingConfig\r\n\r\nclient = create_client()\r\n\r\nagent_state = client.create_agent(\r\n    llm_config=LLMConfig(\r\n        model=\"qwen2.5:0.5b\",\r\n        model_endpoint_type=\"ollama\",\r\n        model_endpoint=\"http://localhost:11434\",\r\n        context_window=128000\r\n    ), \r\n    embedding_config=EmbeddingConfig(\r\n        embedding_endpoint_type=\"ollama\",\r\n        embedding_endpoint=None,\r\n        embedding_model=\"all-minilm\",\r\n        embedding_dim=1536,\r\n        embedding_chunk_size=300\r\n    )\r\n)\r\n```\r\n\r\n## Launch ollama\r\n\r\n![image](https://github.com/user-attachments/assets/86334844-fb4c-45b4-9197-142dacc310ad)\r\n\r\n## Launch agent\r\n\r\n```python\r\nresponse = client.send_message(\r\n  agent_id=agent_state.id, \r\n  role=\"user\", \r\n  message=\"hello\"\r\n)\r\nprint(\"Usage\", response.usage)\r\nprint(\"Agent messages\", response.messages)\r\n```\r\n\r\n## Response\r\n\r\n```\r\nLetta.letta.server.server - ERROR - Error in server._step: API call got non-200 response code (code=500, msg={\"error\":\"llama runner process has terminated: exit status 2\"}) for address: http://localhost:11434/api/generate. Make sure that the ollama API server is running and reachable at http://localhost:11434/api/generate.\r\n```\r\n```python\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\server\\server.py\", line 450, in _step\r\n    usage_stats = letta_agent.step(\r\n                  ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py\", line 910, in step\r\n    step_response = self.inner_step(\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py\", line 1111, in inner_step\r\n    raise e\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py\", line 1026, in inner_step\r\n    response = self._get_ai_reply(\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py\", line 650, in _get_ai_reply\r\n    raise e\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py\", line 613, in _get_ai_reply\r\n    response = create(\r\n               ^^^^^^^\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\llm_api\\llm_api_tools.py\", line 100, in wrapper\r\n    raise e\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\llm_api\\llm_api_tools.py\", line 69, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\llm_api\\llm_api_tools.py\", line 389, in create\r\n    return get_chat_completion(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\local_llm\\chat_completion_proxy.py\", line 167, in get_chat_completion\r\n    result, usage = get_ollama_completion(endpoint, auth_type, auth_key, model, prompt, context_window)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\akidra\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\local_llm\\ollama\\api.py\", line 68, in get_ollama_completion\r\n    raise Exception(\r\nException: API call got non-200 response code (code=500, msg={\"error\":\"llama runner process has terminated: exit status 2\"}) for address: http://localhost:11434/api/generate. Make sure that the ollama API server is running and reachable at http://localhost:11434/api/generate.\r\nNone\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\nCell In[50], line 23\r\n      6 agent_state = client.create_agent(\r\n      7     llm_config=LLMConfig(\r\n      8         model=\"qwen2.5:0.5b\",\r\n   (...)\r\n     19     )\r\n     20 )\r\n     22 # Message an agent\r\n---> 23 response = client.send_message(\r\n     24   agent_id=agent_state.id, \r\n     25   role=\"user\", \r\n     26   message=\"hello\"\r\n     27 )\r\n     28 print(\"Usage\", response.usage)\r\n     29 print(\"Agent messages\", response.messages)\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\client\\client.py:2488, in LocalClient.send_message(self, message, role, name, agent_id, agent_name, stream_steps, stream_tokens)\r\n   2485     raise NotImplementedError\r\n   2486 self.interface.clear()\r\n-> 2488 usage = self.server.send_messages(\r\n   2489     actor=self.user,\r\n   2490     agent_id=agent_id,\r\n   2491     messages=[MessageCreate(role=MessageRole(role), text=message, name=name)],\r\n   2492 )\r\n   2494 ## TODO: need to make sure date/timestamp is propely passed\r\n   2495 ## TODO: update self.interface.to_list() to return actual Message objects\r\n   2496 ##       here, the message objects will have faulty created_by timestamps\r\n   (...)\r\n   2504 \r\n   2505 # format messages\r\n   2506 messages = self.interface.to_list()\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\server\\server.py:761, in SyncServer.send_messages(self, actor, agent_id, messages, wrap_user_message, wrap_system_message, interface)\r\n    758     raise ValueError(f\"All messages must be of type Message or MessageCreate, got {[type(message) for message in messages]}\")\r\n    760 # Run the agent state forward\r\n--> 761 return self._step(actor=actor, agent_id=agent_id, input_messages=message_objects, interface=interface)\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\server\\server.py:450, in SyncServer._step(self, actor, agent_id, input_messages, interface)\r\n    447 token_streaming = letta_agent.interface.streaming_mode if hasattr(letta_agent.interface, \"streaming_mode\") else False\r\n    449 logger.debug(f\"Starting agent step\")\r\n--> 450 usage_stats = letta_agent.step(\r\n    451     messages=input_messages,\r\n    452     chaining=self.chaining,\r\n    453     max_chaining_steps=self.max_chaining_steps,\r\n    454     stream=token_streaming,\r\n    455     skip_verify=True,\r\n    456 )\r\n    458 # save agent after step\r\n    459 save_agent(letta_agent)\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py:910, in Agent.step(self, messages, chaining, max_chaining_steps, **kwargs)\r\n    908 kwargs[\"first_message\"] = False\r\n    909 kwargs[\"step_count\"] = step_count\r\n--> 910 step_response = self.inner_step(\r\n    911     messages=next_input_message,\r\n    912     **kwargs,\r\n    913 )\r\n    914 heartbeat_request = step_response.heartbeat_request\r\n    915 function_failed = step_response.function_failed\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py:1111, in Agent.inner_step(self, messages, first_message, first_message_retry_limit, skip_verify, stream, step_count)\r\n   1109 else:\r\n   1110     printd(f\"step() failed with an unrecognized exception: '{str(e)}'\")\r\n-> 1111     raise e\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py:1026, in Agent.inner_step(self, messages, first_message, first_message_retry_limit, skip_verify, stream, step_count)\r\n   1023             raise Exception(f\"Hit first message retry limit ({first_message_retry_limit})\")\r\n   1025 else:\r\n-> 1026     response = self._get_ai_reply(\r\n   1027         message_sequence=input_message_sequence,\r\n   1028         first_message=first_message,\r\n   1029         stream=stream,\r\n   1030         step_count=step_count,\r\n   1031     )\r\n   1033 # Step 3: check if LLM wanted to call a function\r\n   1034 # (if yes) Step 4: call the function\r\n   1035 # (if yes) Step 5: send the info on the function call and function response to LLM\r\n   1036 response_message = response.choices[0].message\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py:650, in Agent._get_ai_reply(self, message_sequence, function_call, first_message, stream, empty_response_retry_limit, backoff_factor, max_delay, step_count)\r\n    646             time.sleep(delay)\r\n    648     except Exception as e:\r\n    649         # For non-retryable errors, exit immediately\r\n--> 650         raise e\r\n    652 raise Exception(\"Retries exhausted and no valid response received.\")\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\agent.py:613, in Agent._get_ai_reply(self, message_sequence, function_call, first_message, stream, empty_response_retry_limit, backoff_factor, max_delay, step_count)\r\n    611 for attempt in range(1, empty_response_retry_limit + 1):\r\n    612     try:\r\n--> 613         response = create(\r\n    614             llm_config=self.agent_state.llm_config,\r\n    615             messages=message_sequence,\r\n    616             user_id=self.agent_state.created_by_id,\r\n    617             functions=allowed_functions,\r\n    618             # functions_python=self.functions_python, do we need this?\r\n    619             function_call=function_call,\r\n    620             first_message=first_message,\r\n    621             force_tool_call=force_tool_call,\r\n    622             stream=stream,\r\n    623             stream_interface=self.interface,\r\n    624         )\r\n    626         # These bottom two are retryable\r\n    627         if len(response.choices) == 0 or response.choices[0] is None:\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\llm_api\\llm_api_tools.py:100, in retry_with_exponential_backoff.<locals>.wrapper(*args, **kwargs)\r\n     98 # Raise exceptions for any errors not specified\r\n     99 except Exception as e:\r\n--> 100     raise e\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\llm_api\\llm_api_tools.py:69, in retry_with_exponential_backoff.<locals>.wrapper(*args, **kwargs)\r\n     67 while True:\r\n     68     try:\r\n---> 69         return func(*args, **kwargs)\r\n     71     except requests.exceptions.HTTPError as http_err:\r\n     73         if not hasattr(http_err, \"response\") or not http_err.response:\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\llm_api\\llm_api_tools.py:389, in create(llm_config, messages, user_id, functions, functions_python, function_call, first_message, force_tool_call, use_tool_naming, stream, stream_interface, max_tokens, model_settings)\r\n    387 if stream:\r\n    388     raise NotImplementedError(f\"Streaming not yet implemented for {llm_config.model_endpoint_type}\")\r\n--> 389 return get_chat_completion(\r\n    390     model=llm_config.model,\r\n    391     messages=messages,\r\n    392     functions=functions,\r\n    393     functions_python=functions_python,\r\n    394     function_call=function_call,\r\n    395     context_window=llm_config.context_window,\r\n    396     endpoint=llm_config.model_endpoint,\r\n    397     endpoint_type=llm_config.model_endpoint_type,\r\n    398     wrapper=llm_config.model_wrapper,\r\n    399     user=str(user_id),\r\n    400     # hint\r\n    401     first_message=first_message,\r\n    402     # auth-related\r\n    403     auth_type=model_settings.openllm_auth_type,\r\n    404     auth_key=model_settings.openllm_api_key,\r\n    405 )\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\local_llm\\chat_completion_proxy.py:167, in get_chat_completion(model, messages, functions, functions_python, function_call, context_window, user, wrapper, endpoint, endpoint_type, function_correction, first_message, auth_type, auth_key)\r\n    165     result, usage = get_koboldcpp_completion(endpoint, auth_type, auth_key, prompt, context_window, grammar=grammar)\r\n    166 elif endpoint_type == \"ollama\":\r\n--> 167     result, usage = get_ollama_completion(endpoint, auth_type, auth_key, model, prompt, context_window)\r\n    168 elif endpoint_type == \"vllm\":\r\n    169     result, usage = get_vllm_completion(endpoint, auth_type, auth_key, model, prompt, context_window, user)\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\letta\\local_llm\\ollama\\api.py:68, in get_ollama_completion(endpoint, auth_type, auth_key, model, prompt, context_window, grammar)\r\n     66         result = result_full[\"response\"]\r\n     67     else:\r\n---> 68         raise Exception(\r\n     69             f\"API call got non-200 response code (code={response.status_code}, msg={response.text}) for address: {URI}.\"\r\n     70             + f\" Make sure that the ollama API server is running and reachable at {URI}.\"\r\n     71         )\r\n     73 except:\r\n     74     # TODO handle gracefully\r\n     75     raise\r\n\r\nException: API call got non-200 response code (code=500, msg={\"error\":\"llama runner process has terminated: exit status 2\"}) for address: http://localhost:11434/api/generate. Make sure that the ollama API server is running and reachable at http://localhost:11434/api/generate.\r\n```",
      "state": "closed",
      "author": "hherpa",
      "author_type": "User",
      "created_at": "2024-12-19T01:44:57Z",
      "updated_at": "2025-02-22T02:00:57Z",
      "closed_at": "2025-02-22T02:00:56Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2282/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2282",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2282",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:08.723483",
      "comments": [
        {
          "author": "Shua1",
          "body": "The error message says: Make sure that the ollama API server is running and reachable at http://localhost:11434/api/generate. Did you do that?",
          "created_at": "2025-01-08T02:07:08Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-08T02:00:00Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-22T02:00:56Z"
        }
      ]
    },
    {
      "issue_number": 2331,
      "title": "Error calling Get Agent Recall Memory Summary API endpoint ",
      "body": "**Describe the bug**\r\nGot this following error while calling `/v1/agents/:agent_id/memory/recall`\r\n\r\n```\r\n  File \"/app/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n    return await run_in_threadpool(dependant.call, **values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n    return await anyio.to_thread.run_sync(func, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n    return await future\r\n           ^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n    result = context.run(func, *args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 360, in get_agent_recall_memory_summary\r\n    return server.get_recall_memory_summary(agent_id=agent_id, actor=actor)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/letta/server/server.py\", line 909, in get_recall_memory_summary\r\n    return RecallMemorySummary(size=len(agent.message_manager))\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: object of type 'MessageManager' has no len()\r\n```\r\n\r\n**Setup**\r\n- Docker image letta/letta:latest (0.6.6)",
      "state": "closed",
      "author": "trananhnguyen97",
      "author_type": "User",
      "created_at": "2025-01-05T21:52:04Z",
      "updated_at": "2025-02-21T02:03:23Z",
      "closed_at": "2025-02-21T02:03:22Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2331/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2331",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2331",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:08.929253",
      "comments": [
        {
          "author": "Shua1",
          "body": "Looks like this might have been fixed by this commit [1]. So please update your local version and try again.\r\n\r\n[1] https://github.com/letta-ai/letta/commit/a6d09997e8b6d16f86def7832932ebefe9714cd4",
          "created_at": "2025-01-07T08:36:27Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-07T02:02:44Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-21T02:03:22Z"
        }
      ]
    },
    {
      "issue_number": 2328,
      "title": "Unable to start letta server in local setup ",
      "body": "**Describe the bug**\r\nUnable to start letta server in local setup . kindly assist on how to setup letta in local workstation . I followed the steps in https://www.youtube.com/watch?v=EOkpFDBNyEw as well but still got into below issue .\r\n\r\n**Please describe your setup**\r\n- [ ] How did you install letta?\r\n  - `pip install letta`\r\n- [ ] Describe your setup\r\n  - What's your OS (Windows/MacOS/Linux)? Windows\r\n  - How are you running `letta`? (`cmd.exe`/Powershell/Anaconda Shell/Terminal) cmd.exe\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\npython version \r\nPython 3.10.6\r\n\r\n(.venv) C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt>letta version\r\n0.1.14\r\nencountered same issue in 0.1.16 version as well while trying to follow the setup posted in this video  https://www.youtube.com/watch?v=EOkpFDBNyEw\r\n(.venv) C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt>letta quickstart\r\n(.venv) C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt>letta configure\r\n(.venv) C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt>letta run \r\n\r\nall above 3 commands return below error \r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\Scripts\\letta.exe\\__main__.py\", line 7, in <module>\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\typer\\main.py\", line 328, in __call__\r\n    raise e\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\typer\\main.py\", line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\click\\core.py\", line 1161, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\typer\\core.py\", line 783, in main\r\n    return _main(\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\typer\\core.py\", line 225, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\click\\core.py\", line 1697, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\click\\core.py\", line 1443, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\click\\core.py\", line 788, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\typer\\main.py\", line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\letta\\cli\\cli.py\", line 178, in quickstart\r\n    new_config, config_was_modified = set_config_with_dict(backup_config)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\letta\\cli\\cli.py\", line 63, in set_config_with_dict\r\n    old_config = LettaConfig.load()\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\letta\\config.py\", line 165, in load\r\n    llm_config = LLMConfig(**llm_config_dict)\r\n  File \"C:\\Users\\user\\Documents\\tutorial\\agents\\memgpt\\.venv\\lib\\site-packages\\pydantic\\main.py\", line 212, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\npydantic_core._pydantic_core.ValidationError: 4 validation errors for LLMConfig\r\nmodel\r\n  Field required [type=missing, input_value={}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\r\nmodel_endpoint_type\r\n  Field required [type=missing, input_value={}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\r\nmodel_endpoint\r\n  Field required [type=missing, input_value={}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\r\ncontext_window\r\n  Field required [type=missing, input_value={}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\r\n\r\n**Letta Config**\r\nPlease attach your `~/.letta/config` file or copy paste it below.\r\n\r\nThere is no documentation steps on how to to setup  letta/config  \r\nletta quickstart \r\nletta run\r\nletta configure \r\nall return the same error .\r\n\r\n---\r\n\r\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\r\nI am using Azure OpenAI and have setup the environmental variables \r\nAZURE_API_KEY=\"...\"\r\nAZURE_BASE_URL=\"...\"\r\nAZURE_API_VERSION=\"...\" \r\n\r\n**Local LLM details**\r\n\r\nIf you are trying to run Letta with local LLMs, please provide the following information:\r\n\r\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\r\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\r\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\r\n",
      "state": "closed",
      "author": "AjitAntony",
      "author_type": "User",
      "created_at": "2025-01-04T11:54:48Z",
      "updated_at": "2025-02-18T02:02:21Z",
      "closed_at": "2025-02-18T02:02:20Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2328/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2328",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2328",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:09.163934",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-02-04T02:01:17Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-18T02:02:20Z"
        }
      ]
    },
    {
      "issue_number": 2315,
      "title": "get_recall_memory_summary - TypeError: object of type 'MessageManager' has no len()",
      "body": "I get this error with the code below and also with the curl command through the swagger UI or from the terminal. After this error ollama went wild and my agents where all gone after that.\r\n\r\nclient.get_recall_memory_summary(agent_state.id)\r\n\r\nTypeError: object of type 'MessageManager' has no len()\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/app/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n    return await run_in_threadpool(dependant.call, **values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n    return await anyio.to_thread.run_sync(func, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n    return await future\r\n           ^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n    result = context.run(func, *args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 360, in get_agent_recall_memory_summary\r\n    return server.get_recall_memory_summary(agent_id=agent_id, actor=actor)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/letta/server/server.py\", line 909, in get_recall_memory_summary\r\n    return RecallMemorySummary(size=len(agent.message_manager))\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: object of type 'MessageManager' has no len()\r\n\r\n",
      "state": "closed",
      "author": "robstoof",
      "author_type": "User",
      "created_at": "2024-12-25T11:54:34Z",
      "updated_at": "2025-02-08T02:00:00Z",
      "closed_at": "2025-02-08T02:00:00Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2315/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2315",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2315",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:11.187203",
      "comments": [
        {
          "author": "cpacker",
          "body": "Hi @robstoof , thank you for the bug report!\r\n\r\nThis is fixed in the latest `main`, but isn't fixed yet in the latest release.\r\n\r\nE.g. the bug exists in [0.6.6](https://github.com/letta-ai/letta/blob/0.6.6/letta/server/server.py#L909), but doesn't exist [here](https://github.com/letta-ai/letta/blob/",
          "created_at": "2024-12-25T23:30:40Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-01-25T01:57:54Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-08T02:00:00Z"
        }
      ]
    },
    {
      "issue_number": 1776,
      "title": "MemGPT code is AAA+ unfortunately I cannot get it to work (no matter which LLM I try I cannot get it to work reliably)",
      "body": "Hi MemGPT Team,\r\n\r\nThank you for such a high quality codebase I'm pretty confident that, as LLMs will improve, MemGPT will become the \"standard\" between all products of its kind.\r\n\r\nI would recommend MemGPT team to put in place a webpage similar with \r\n\r\nhttps://aider.chat/docs/leaderboards/\r\n\r\nwhere people would immediately see (and with high confidence) what kind of quality to expect from any of the available LLMs.\r\n\r\nI want to ask the community if you were able to get MemGPT working in a \"day to day\" kind of way and, if yes, which LLMs are you using with and for which kind of scenario? (how long is your \"persona\" prompt? do you have instructions for the LLM to follow in your persona? How many? Are you using custom tools, if you are using custom tools how many and how complex?) If you are using \"day to day\" does MemGPT/LLM flawlesly work or you are used to see \"stacktraces\" and when you get a stacktrace you just click \"run again\" and it works next time? \r\n\r\nI'm very curious to understand how (and if) people are using MemGPT.\r\n\r\nI want to say that, with my own AI projects, I understood (before MemGPT) that it is incredible difficult to make LLMs to follow instructions no matter how well crafted and clear the prompt instructions are and it becomes even more difficult when the number of instructions to follow by the LLM increase and when you combine this situation with \"function calling\" ability of LLMs (where you can also have LLM to call functions and, similarly with instructions, the more functions you add the more confused the LLM becomes) => it becomes a very difficult problem, with the current ability of LLMs, to get anything more that \"hello world\" working. Even when the LLMs will follow the instructions the first time for the next two requests will not and will get a stacktrace (for anything more than \"hello world\").\r\n\r\nBecause of that I'm pretty sure what I will describe below it is happening because what the LLMs are (not) capable now and not because of MemGPT (which I already said has very well crafted source code).\r\n\r\nI tried MemGPT two times the first time 6 months ago and gave up because for 90% of requests I was getting stacktraces and 10% of requests were working.  For the past few days I tried again MemGPT and this time I also got familiar with the codebase. The situation is the same like 6 months ago.\r\n\r\nWith anthropic claude sonnet I could not get anything working.\r\n\r\nWith openai's gpt-4-1106-preview (which is advertised as 'featuring improved instruction following, JSON mode, reproducible outputs') I am able (from time to time) to get some requests processed but only when I start with `  --first --no-verify` - even so subsequent requests starts to fail and cannot recover. I also tried other openai models and I could not get any to work.\r\n\r\nHere is how I create my agent.\r\n\r\n```\r\nimport os\r\nimport sys\r\n\r\n# Determine the base directory two levels up from the current script\r\nbase_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\r\n\r\n# Add the path to the memgpt package\r\nmemgpt_path = os.path.join(base_dir, 'bkend-core-python', 'MemGPT')\r\nsys.path.append(memgpt_path)\r\n\r\nfrom memgpt import create_client\r\nfrom memgpt.memory import ChatMemory\r\nfrom memgpt.data_types import LLMConfig\r\n\r\nfrom memgpt.functions.function_sets.extras import workspace_list_files, read_file_content, write_file_content, ctrl_c, ctrl_v\r\n\r\ndef main():\r\n    # Create a `LocalClient`\r\n    client = create_client()\r\n\r\n    agent_name = \"Ada\"\r\n\r\n    # Check if the agent already exists\r\n    existing_agents = client.list_agents()\r\n    ada_agent = next((agent for agent in existing_agents if agent.name == agent_name), None)\r\n\r\n    if ada_agent is None:\r\n        \"\"\"Load text content from a file.\"\"\"\r\n        script_dir = os.path.dirname(os.path.abspath(__file__))\r\n        parent_script_dir = os.path.dirname(script_dir)\r\n        persona_file_path = os.path.join(script_dir, \"persona_ada.txt\")\r\n        human_file_path = os.path.join(f\"{parent_script_dir}/humans\", \"john.txt\")\r\n\r\n        with open(persona_file_path, 'r', encoding='utf-8') as file:\r\n            persona_content = file.read().strip()\r\n\r\n        with open(human_file_path, 'r', encoding='utf-8') as file:\r\n            human_content = file.read().strip()\r\n\r\n        persona_content = persona_content.replace(\"{workspace_folder_path}\", os.path.join(script_dir, \"workspace\"))\r\n        #print(f\"human_content: {human_content}\\npersona_content {persona_content}\")\r\n\r\n        # Create custom memory with the persona and human\r\n        memory_ada = ChatMemory(persona=persona_content, human=human_content)\r\n\r\n        # Create custom tools\r\n\r\n        # recreate tools from scratch\r\n        client.delete_tool('ada_simple_tool')\r\n        client.delete_tool('ada_workspace_list_files')\r\n        client.delete_tool('workspace_list_files')\r\n        client.delete_tool('read_from_text_file')\r\n        client.delete_tool('read_file_content')\r\n        client.delete_tool('write_file_content')\r\n        client.delete_tool('ctrl_c')\r\n        client.delete_tool('ctrl_v')\r\n               \r\n        client.create_tool(workspace_list_files, name=\"workspace_list_files\")\r\n        client.create_tool(read_file_content, name=\"read_file_content\")\r\n        client.create_tool(write_file_content, name=\"write_file_content\")\r\n        client.create_tool(ctrl_c, name=\"ctrl_c\")\r\n        client.create_tool(ctrl_v, name=\"ctrl_v\")\r\n\r\n        # Define the LLM configuration for the gpt-4o-2024-08-06 model\r\n        llm_config = LLMConfig(\r\n            #o1-preview (rate limited not usable) \r\n            #o1-mini (rate limited not usable) \r\n            #gpt-4o-2024-08-06 (TOP model #2)\r\n            #gpt-4-1106-preview (TOP model #1)\r\n            #claude-3-sonnet-20240229\r\n            model=\"claude-3-sonnet-20240229\",\r\n            #model_endpoint=\"https://api.openai.com/v1\",\r\n            model_endpoint=\"https://api.anthropic.com/v1\",\r\n            #model_endpoint_type=\"openai\",\r\n            model_endpoint_type=\"anthropic\",\r\n            #context_window=16384, #openai\r\n            context_window=200000 #anthropic\r\n        )\r\n\r\n        # Create an agent\r\n        ada_agent = client.create_agent(\r\n            llm_config=llm_config,\r\n            name=\"Ada\",\r\n            memory=memory_ada,\r\n            system_prompt=\"memgpt_chat\",\r\n            tools=['workspace_list_files'],\r\n            #include_base_tools=False,\r\n        )\r\n\r\n        print(f\"Created (agent) {ada_agent.name} with ID {str(ada_agent.id)}\")\r\n    else:\r\n        print(f\"Nothing to do because (agent) {ada_agent.name} already exists.\")\r\n\r\n    # Retrieve and print memory information\r\n    memory_info = client.get_agent_memory(agent_id=ada_agent.id)\r\n    print(f\"Agent Persona: {memory_info.core_memory.persona}\")\r\n    print(f\"Agent Human: {memory_info.core_memory.human}\")\r\n\r\nif __name__ == \"__main__\":\r\n   main()\r\n```\r\n\r\nThere is no point in attaching here long stacktraces. I pretty confident I have the setup/configuration correctly done.",
      "state": "closed",
      "author": "distributev",
      "author_type": "User",
      "created_at": "2024-09-23T15:32:27Z",
      "updated_at": "2025-02-01T02:03:47Z",
      "closed_at": "2025-02-01T02:03:47Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/1776/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sarahwooders"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/1776",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/1776",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:11.383209",
      "comments": [
        {
          "author": "sarahwooders",
          "body": "Can you try again with 0.5.0? There should be a lot of bugfixes to the configuration for LLMs and embedding models now. Please re-open if you are still having issues. ",
          "created_at": "2024-10-16T23:51:14Z"
        },
        {
          "author": "shivamatfigr",
          "body": "I have been facing similar issues with claude sonnet. The \"decision making\" part of managing memory and tools seem to be quite unusable in the sense it cannot decide when and which functions to call. Once I tell it to update and fetch from core/archive memory it does it but that becomes less practic",
          "created_at": "2024-12-11T07:39:12Z"
        },
        {
          "author": "distributev",
          "body": "@shivamatfigr From the MemGPT perspective I got pretty good results with gpt4o-mini which has also a very good price. As a basic assistant it works well - it still gives some stacktraces here and there related with memory updating but it is for sure usable (not like the other LLMs which I could not ",
          "created_at": "2024-12-18T12:53:42Z"
        },
        {
          "author": "mattzh72",
          "body": "@distributev quick question as we're looking into this - does gpt4o more reliably use the memory tools?",
          "created_at": "2024-12-18T19:07:20Z"
        },
        {
          "author": "distributev",
          "body": "Most of my testing I described above was done before the MemGPT to Letta project name change. \r\n\r\n**_At that time I could get only gpt4o-mini working_** \r\n\r\ngtp4o (directly from openai), claude sonnet (directly from anthropic), llama and few other LLMs from open router were all unusable and failing ",
          "created_at": "2024-12-18T19:16:43Z"
        }
      ]
    },
    {
      "issue_number": 2266,
      "title": "ADE: Delete Agent broken",
      "body": "**Describe the bug**\r\nI am trying to delete an agent from the ADE and it fails with message \"There was an error deleting this agent, try again or contact support.\" Stack trace shows indexerror.\r\n\r\n**Please describe your setup**\r\n- [x] How did you install letta?\r\n  - letta server docker setup\r\n- [x] Describe your setup\r\n  - What's your OS (Windows/MacOS/Linux)? Windows\r\n  - How are you running `letta`? (`cmd.exe`/Powershell/Anaconda Shell/Terminal) ADE\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n![image](https://github.com/user-attachments/assets/b9274e76-2ec8-4a68-a79c-5938eebd78c2)\r\n\r\n\r\n**Additional context**\r\nAdd any other context about the problem here. Adding stacktrace here:\r\n2024-12-17 23:11:33 ERROR:    Unhandled error: list index out of range\r\n2024-12-17 23:11:33 Traceback (most recent call last):\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:33     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:33     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:33     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:33     response = await f(request)\r\n2024-12-17 23:11:33                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:33     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:33                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:33     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:33     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:33     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:33     return await future\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:33     result = context.run(func, *args)\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 88, in get_agent_context_window\r\n2024-12-17 23:11:33     return server.get_agent_context_window(user_id=actor.id, agent_id=agent_id)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33 Stack trace: <traceback object at 0x7ff8e71c2a40>\r\n2024-12-17 23:11:33 INFO:     172.17.0.1:41818 - \"GET /v1/agents/agent-38b15714-91dc-46c5-83bb-b66299f804d7/context HTTP/1.1\" 500 Internal Server Error\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 1833, in get_agent_context_window\r\n2024-12-17 23:11:33     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:33                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:33     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:33   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:33     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:33                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:33 IndexError: list index out of range\r\n2024-12-17 23:11:33 ERROR:    Exception in ASGI application\r\n2024-12-17 23:11:33 Traceback (most recent call last):\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\r\n2024-12-17 23:11:33     result = await app(  # type: ignore[func-returns-value]\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n2024-12-17 23:11:33     return await self.app(scope, receive, send)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n2024-12-17 23:11:33     await super().__call__(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:33     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:33     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:33     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:33     response = await f(request)\r\n2024-12-17 23:11:33                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:33     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:33                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:33     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:33     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:33     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:33     return await future\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:33     result = context.run(func, *args)\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 88, in get_agent_context_window\r\n2024-12-17 23:11:33     return server.get_agent_context_window(user_id=actor.id, agent_id=agent_id)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 1833, in get_agent_context_window\r\n2024-12-17 23:11:33     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:33                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:33     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:33   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:33     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:33                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:33 IndexError: list index out of range\r\n2024-12-17 23:11:33 ERROR:    Unhandled error: list index out of range\r\n2024-12-17 23:11:33 Traceback (most recent call last):\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:33     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:33     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:33     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:33     response = await f(request)\r\n2024-12-17 23:11:33                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:33     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:33                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:33     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:33     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:33     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:33     return await future\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:33     result = context.run(func, *args)\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 434, in get_agent_messages\r\n2024-12-17 23:11:33     return server.get_agent_recall_cursor(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 1389, in get_agent_recall_cursor\r\n2024-12-17 23:11:33     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:33                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:33     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:33   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:33     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:33                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:33 IndexError: list index out of range\r\n2024-12-17 23:11:33 Stack trace: <traceback object at 0x7ff8e651a7c0>\r\n2024-12-17 23:11:33 INFO:     172.17.0.1:41804 - \"GET /v1/agents/agent-38b15714-91dc-46c5-83bb-b66299f804d7/messages?limit=50 HTTP/1.1\" 500 Internal Server Error\r\n2024-12-17 23:11:33 ERROR:    Exception in ASGI application\r\n2024-12-17 23:11:33 Traceback (most recent call last):\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\r\n2024-12-17 23:11:33     result = await app(  # type: ignore[func-returns-value]\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n2024-12-17 23:11:33     return await self.app(scope, receive, send)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n2024-12-17 23:11:33     await super().__call__(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:33     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:33     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:33     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:33     response = await f(request)\r\n2024-12-17 23:11:33                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:33     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:33                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:33     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:33     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:33     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:33     return await future\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:33     result = context.run(func, *args)\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 434, in get_agent_messages\r\n2024-12-17 23:11:33     return server.get_agent_recall_cursor(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 1389, in get_agent_recall_cursor\r\n2024-12-17 23:11:33     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:33                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:33     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:33   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:33     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:33                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:33 IndexError: list index out of range\r\n2024-12-17 23:11:33 ERROR:    Unhandled error: list index out of range\r\n2024-12-17 23:11:33 Traceback (most recent call last):\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:33     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:33     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:33     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:33     response = await f(request)\r\n2024-12-17 23:11:33                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:33     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:33                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:33     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:33     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:33     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:33     return await future\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:33     result = context.run(func, *args)\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 369, in get_agent_archival_memory\r\n2024-12-17 23:11:33     return server.get_agent_archival_cursor(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 1326, in get_agent_archival_cursor\r\n2024-12-17 23:11:33     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:33                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33 Stack trace: <traceback object at 0x7ff8e6581ac0>\r\n2024-12-17 23:11:33 INFO:     172.17.0.1:41820 - \"GET /v1/agents/agent-38b15714-91dc-46c5-83bb-b66299f804d7/archival HTTP/1.1\" 500 Internal Server Error\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:33     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:33   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:33     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:33                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:33 IndexError: list index out of range\r\n2024-12-17 23:11:33 ERROR:    Exception in ASGI application\r\n2024-12-17 23:11:33 Traceback (most recent call last):\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\r\n2024-12-17 23:11:33     result = await app(  # type: ignore[func-returns-value]\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n2024-12-17 23:11:33     return await self.app(scope, receive, send)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n2024-12-17 23:11:33     await super().__call__(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:33     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:33     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:33     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:33     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:33     await self.app(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:33     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:33     raise exc\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:33     await app(scope, receive, sender)\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:33     response = await f(request)\r\n2024-12-17 23:11:33                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:33     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:33                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:33     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:33     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:33     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:33     return await future\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:33     result = context.run(func, *args)\r\n2024-12-17 23:11:33              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 369, in get_agent_archival_memory\r\n2024-12-17 23:11:33     return server.get_agent_archival_cursor(\r\n2024-12-17 23:11:33            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 1326, in get_agent_archival_cursor\r\n2024-12-17 23:11:33     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:33                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:33   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:33     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:33   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:33     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:33                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:33 IndexError: list index out of range\r\n2024-12-17 23:11:34 2024-12-17 19:11:34.971 UTC [318] ERROR:  update or delete on table \"agents\" violates foreign key constraint \"passages_agent_id_fkey\" on table \"passages\"\r\n2024-12-17 23:11:34 2024-12-17 19:11:34.971 UTC [318] DETAIL:  Key (id)=(agent-38b15714-91dc-46c5-83bb-b66299f804d7) is still referenced from table \"passages\".\r\n2024-12-17 23:11:34 2024-12-17 19:11:34.971 UTC [318] STATEMENT:  DELETE FROM agents WHERE agents.id = 'agent-38b15714-91dc-46c5-83bb-b66299f804d7'\r\n2024-12-17 23:11:34 Letta.letta.server.server - ERROR - Failed to delete agent agent-38b15714-91dc-46c5-83bb-b66299f804d7 via ID with:\r\n2024-12-17 23:11:34 (psycopg2.errors.ForeignKeyViolation) update or delete on table \"agents\" violates foreign key constraint \"passages_agent_id_fkey\" on table \"passages\"\r\n2024-12-17 23:11:34 DETAIL:  Key (id)=(agent-38b15714-91dc-46c5-83bb-b66299f804d7) is still referenced from table \"passages\".\r\n2024-12-17 23:11:34 \r\n2024-12-17 23:11:34 [SQL: DELETE FROM agents WHERE agents.id = %(id_1)s]\r\n2024-12-17 23:11:34 [parameters: {'id_1': 'agent-38b15714-91dc-46c5-83bb-b66299f804d7'}]\r\n2024-12-17 23:11:34 (Background on this error at: https://sqlalche.me/e/20/gkpj)\r\n2024-12-17 23:11:34 Traceback (most recent call last):\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\r\n2024-12-17 23:11:34     self.dialect.do_execute(\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 941, in do_execute\r\n2024-12-17 23:11:34     cursor.execute(statement, parameters)\r\n2024-12-17 23:11:34 psycopg2.errors.ForeignKeyViolation: update or delete on table \"agents\" violates foreign key constraint \"passages_agent_id_fkey\" on table \"passages\"\r\n2024-12-17 23:11:34 DETAIL:  Key (id)=(agent-38b15714-91dc-46c5-83bb-b66299f804d7) is still referenced from table \"passages\".\r\n2024-12-17 23:11:34 \r\n2024-12-17 23:11:34 \r\n2024-12-17 23:11:34 The above exception was the direct cause of the following exception:\r\n2024-12-17 23:11:34 \r\n2024-12-17 23:11:34 Traceback (most recent call last):\r\n2024-12-17 23:11:34   File \"/app/letta/server/server.py\", line 1528, in delete_agent\r\n2024-12-17 23:11:34     self.ms.delete_agent(agent_id=agent_id, per_agent_lock_manager=self.per_agent_lock_manager)\r\n2024-12-17 23:11:34   File \"/app/letta/utils.py\", line 559, in wrapper\r\n2024-12-17 23:11:34     return func(*args, **kwargs)\r\n2024-12-17 23:11:34            ^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/letta/metadata.py\", line 331, in delete_agent\r\n2024-12-17 23:11:34     session.query(AgentModel).filter(AgentModel.id == agent_id).delete()\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/orm/query.py\", line 3162, in delete\r\n2024-12-17 23:11:34     result: CursorResult[Any] = self.session.execute(\r\n2024-12-17 23:11:34                                 ^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2362, in execute\r\n2024-12-17 23:11:34     return self._execute_internal(\r\n2024-12-17 23:11:34            ^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2247, in _execute_internal\r\n2024-12-17 23:11:34     result: Result[Any] = compile_state_cls.orm_execute_statement(\r\n2024-12-17 23:11:34                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/orm/bulk_persistence.py\", line 2021, in orm_execute_statement\r\n2024-12-17 23:11:34     return super().orm_execute_statement(\r\n2024-12-17 23:11:34            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/orm/context.py\", line 305, in orm_execute_statement\r\n2024-12-17 23:11:34     result = conn.execute(\r\n2024-12-17 23:11:34              ^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1418, in execute\r\n2024-12-17 23:11:34     return meth(\r\n2024-12-17 23:11:34            ^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/sql/elements.py\", line 515, in _execute_on_connection\r\n2024-12-17 23:11:34     return connection._execute_clauseelement(\r\n2024-12-17 23:11:34            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1640, in _execute_clauseelement\r\n2024-12-17 23:11:34     ret = self._execute_context(\r\n2024-12-17 23:11:34           ^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1846, in _execute_context\r\n2024-12-17 23:11:34     return self._exec_single_context(\r\n2024-12-17 23:11:34            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1986, in _exec_single_context\r\n2024-12-17 23:11:34     self._handle_dbapi_exception(\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 2355, in _handle_dbapi_exception\r\n2024-12-17 23:11:34     raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\r\n2024-12-17 23:11:34     self.dialect.do_execute(\r\n2024-12-17 23:11:34   File \"/app/.venv/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 941, in do_execute\r\n2024-12-17 23:11:34     cursor.execute(statement, parameters)\r\n2024-12-17 23:11:34 sqlalchemy.exc.IntegrityError: (psycopg2.errors.ForeignKeyViolation) update or delete on table \"agents\" violates foreign key constraint \"passages_agent_id_fkey\" on table \"passages\"\r\n2024-12-17 23:11:34 DETAIL:  Key (id)=(agent-38b15714-91dc-46c5-83bb-b66299f804d7) is still referenced from table \"passages\".\r\n2024-12-17 23:11:34 \r\n2024-12-17 23:11:34 [SQL: DELETE FROM agents WHERE agents.id = %(id_1)s]\r\n2024-12-17 23:11:34 [parameters: {'id_1': 'agent-38b15714-91dc-46c5-83bb-b66299f804d7'}]\r\n2024-12-17 23:11:34 (Background on this error at: https://sqlalche.me/e/20/gkpj)\r\n2024-12-17 23:11:34 INFO:     172.17.0.1:41826 - \"DELETE /v1/agents/agent-38b15714-91dc-46c5-83bb-b66299f804d7 HTTP/1.1\" 400 Bad Request\r\n2024-12-17 23:11:35 INFO:     172.17.0.1:41826 - \"GET /v1/jobs/active HTTP/1.1\" 200 OK\r\n2024-12-17 23:11:35 ERROR:    Unhandled error: list index out of range\r\n2024-12-17 23:11:35 Traceback (most recent call last):\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:35     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:35     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:35     await self.app(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:35     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:35     raise exc\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:35     await app(scope, receive, sender)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:35     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:35     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:35     await self.app(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:35     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:35     raise exc\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:35     await app(scope, receive, sender)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:35     response = await f(request)\r\n2024-12-17 23:11:35                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:35     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:35                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:35     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:35 Stack trace: <traceback object at 0x7ff8e6503580>\r\n2024-12-17 23:11:35 INFO:     172.17.0.1:41826 - \"GET /v1/agents/agent-38b15714-91dc-46c5-83bb-b66299f804d7/context HTTP/1.1\" 500 Internal Server Error\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:35     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:35     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:35     return await future\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:35     result = context.run(func, *args)\r\n2024-12-17 23:11:35              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 88, in get_agent_context_window\r\n2024-12-17 23:11:35     return server.get_agent_context_window(user_id=actor.id, agent_id=agent_id)\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/letta/server/server.py\", line 1833, in get_agent_context_window\r\n2024-12-17 23:11:35     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:35                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:35     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:35   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:35     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:35                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:35 IndexError: list index out of range\r\n2024-12-17 23:11:35 ERROR:    Exception in ASGI application\r\n2024-12-17 23:11:35 Traceback (most recent call last):\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\r\n2024-12-17 23:11:35     result = await app(  # type: ignore[func-returns-value]\r\n2024-12-17 23:11:35              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n2024-12-17 23:11:35     return await self.app(scope, receive, send)\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n2024-12-17 23:11:35     await super().__call__(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\r\n2024-12-17 23:11:35     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n2024-12-17 23:11:35     raise exc\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-12-17 23:11:35     await self.app(scope, receive, _send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n2024-12-17 23:11:35     await self.simple_response(scope, receive, send, request_headers=headers)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\r\n2024-12-17 23:11:35     await self.app(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-12-17 23:11:35     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:35     raise exc\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:35     await app(scope, receive, sender)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-12-17 23:11:35     await self.middleware_stack(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n2024-12-17 23:11:35     await route.handle(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-12-17 23:11:35     await self.app(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n2024-12-17 23:11:35     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-12-17 23:11:35     raise exc\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-12-17 23:11:35     await app(scope, receive, sender)\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n2024-12-17 23:11:35     response = await f(request)\r\n2024-12-17 23:11:35                ^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-12-17 23:11:35     raw_response = await run_endpoint_function(\r\n2024-12-17 23:11:35                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\r\n2024-12-17 23:11:35     return await run_in_threadpool(dependant.call, **values)\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\n2024-12-17 23:11:35     return await anyio.to_thread.run_sync(func, *args)\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n2024-12-17 23:11:35     return await get_async_backend().run_sync_in_worker_thread(\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n2024-12-17 23:11:35     return await future\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\r\n2024-12-17 23:11:35     result = context.run(func, *args)\r\n2024-12-17 23:11:35              ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/letta/server/rest_api/routers/v1/agents.py\", line 88, in get_agent_context_window\r\n2024-12-17 23:11:35     return server.get_agent_context_window(user_id=actor.id, agent_id=agent_id)\r\n2024-12-17 23:11:35            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/letta/server/server.py\", line 1833, in get_agent_context_window\r\n2024-12-17 23:11:35     letta_agent = self.load_agent(agent_id=agent_id)\r\n2024-12-17 23:11:35                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-12-17 23:11:35   File \"/app/letta/server/server.py\", line 476, in load_agent\r\n2024-12-17 23:11:35     agent.rebuild_system_prompt()\r\n2024-12-17 23:11:35   File \"/app/letta/agent.py\", line 1284, in rebuild_system_prompt\r\n2024-12-17 23:11:35     curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\r\n2024-12-17 23:11:35                           ~~~~~~~~~~~~~^^^\r\n2024-12-17 23:11:35 IndexError: list index out of range\r\n2024-12-17 23:11:36 INFO:     172.17.0.1:41832 - \"GET /v1/agents/agent-38b15714-91dc-46c5-83bb-b66299f804d7 HTTP/1.1\" 200 OK\r\n\r\n**Letta Config**\r\nPlease attach your `~/.letta/config` file or copy paste it below.\r\n\r\n[defaults]\r\npreset = memgpt_chat\r\npersona = sam_pov\r\nhuman = basic\r\n\r\n[archival_storage]\r\ntype = postgres\r\npath = /root/.letta\r\nuri = postgresql://letta:letta@localhost:5432/letta\r\n\r\n[recall_storage]\r\ntype = postgres\r\npath = /root/.letta\r\nuri = postgresql://letta:letta@localhost:5432/letta\r\n\r\n[metadata_storage]\r\ntype = sqlite\r\npath = /root/.letta\r\n\r\n[version]\r\nletta_version = 0.6.4\r\n\r\n\r\n---\r\n\r\nIf you're not using OpenAI, please provide additional information on your local LLM setup:\r\n\r\n**Local LLM details**\r\n\r\nIf you are trying to run Letta with local LLMs, please provide the following information:\r\n\r\n- [ ] The exact model you're trying to use (e.g. `dolphin-2.1-mistral-7b.Q6_K.gguf`)\r\n- [ ] The local LLM backend you are using (web UI? LM Studio?)\r\n- [ ] Your hardware for the local LLM backend (local computer? operating system? remote RunPod?)\r\n",
      "state": "closed",
      "author": "Lamiv",
      "author_type": "User",
      "created_at": "2024-12-17T19:18:08Z",
      "updated_at": "2025-02-01T02:03:46Z",
      "closed_at": "2025-02-01T02:03:45Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/letta-ai/letta/issues/2266/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "mattzh72"
      ],
      "milestone": null,
      "html_url": "https://github.com/letta-ai/letta/issues/2266",
      "api_url": "https://api.github.com/repos/letta-ai/letta/issues/2266",
      "repository": "letta-ai/letta",
      "extraction_date": "2025-06-21T23:36:11.575584",
      "comments": [
        {
          "author": "cpacker",
          "body": "Thanks for reporting @Lamiv , looking into it ",
          "created_at": "2024-12-17T21:23:09Z"
        },
        {
          "author": "mattzh72",
          "body": "Hi @Lamiv, could you share some details about your agent? Would like to know if you had any sources attached to this agent, etc. Also, are you using the local development server?",
          "created_at": "2024-12-18T01:38:29Z"
        },
        {
          "author": "Lamiv",
          "body": "Hi @mattzh72  - steps to repro:\r\n1. Started Docker\r\n2. Logged into ADE\r\n3. Connected local server\r\n4. Created Agent \r\n5. Chose Model : Letta-free\r\n6. Added PDF Documents to sources\r\n7. Archival memories show over 413 documents\r\n8. Tried renaming agent - renaming failed error.\r\n9. Then asked question",
          "created_at": "2024-12-18T07:40:24Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue is stale because it has been open for 30 days with no activity.",
          "created_at": "2025-01-18T01:58:48Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue was closed because it has been inactive for 14 days since being marked as stale.",
          "created_at": "2025-02-01T02:03:44Z"
        }
      ]
    }
  ]
}